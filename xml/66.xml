<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
<responseDate>2016-03-09T03:35:32Z</responseDate>
<request verb="ListRecords" resumptionToken="1122234|65001">http://export.arxiv.org/oai2</request>
<ListRecords>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5204</identifier>
 <datestamp>2014-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5204</id><created>2014-08-22</created><authors><author><keyname>Elkourdi</keyname><forenames>Tariq</forenames><affiliation>Shitz</affiliation></author><author><keyname>Simeone</keyname><forenames>Osvaldo</forenames><affiliation>Shitz</affiliation></author><author><keyname>Sahin</keyname><forenames>Onur</forenames><affiliation>Shitz</affiliation></author><author><keyname>Shamai</keyname><forenames>Shlomo</forenames><affiliation>Shitz</affiliation></author></authors><title>Signal and Interference Leakage Minimization in MIMO Uplink-Downlink
  Cellular Networks</title><categories>cs.IT math.IT</categories><comments>This is a slightly expanded version of a paper submitted to IEEE VTC
  2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linear processing in the spatial domain at the base stations (BSs) and at the
users of MIMO cellular systems enables the control of both inter-cell and
intra-cell interference. A number of iterative algorithms have been proposed
that allow the BSs and the users to calculate the transmit-side and the
receive-side linear processors in a distributed manner via message exchange
based only on local channel state information. In this paper, a novel such
strategy is proposed that requires the exchange of unitary matrices between BSs
and users. Specifically, focusing on a general both uplink- and
downlink-operated cells, the design of the linear processors is obtained as the
alternating optimization solution of the problem of minimizing the weighted sum
of the downlink and uplink inter-cell interference powers and of the signal
power leaked in the space orthogonal to the receive subspaces. Intra-cell
interference is handled via minimum mean square error (MMSE) or the
zero-forcing (ZF) precoding for downlink-operated cells and via joint decoding
for the uplink-operated cells. Numerical results validate the advantages of the
proposed technique with respect to existing similar techniques that account
only for the interference power in the optimization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5206</identifier>
 <datestamp>2014-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5206</id><created>2014-08-22</created><authors><author><keyname>Gahlawat</keyname><forenames>Aditya</forenames></author><author><keyname>Peet</keyname><forenames>Matthew. M.</forenames></author></authors><title>A Convex Approach to Output Feedback Control of Parabolic PDEs Using
  Sum-of-Squares</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we use optimization-based methods to design output-feedback
controllers for a class of one-dimensional parabolic partial differential
equations. The output may be distributed or point-measurements. The input may
be distributed or boundary actuation. We use Lyapunov operators, duality, and
the Luenberger observer framework to reformulate the synthesis problem as a
convex optimization problem expressed as a set of Linear-Operator-Inequalities
(LOIs). We then show how feasibility of these LOIs may be tested using
Semidefinite Programming (SDP) and the Sum-of-Squares methodology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5208</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5208</id><created>2014-08-22</created><updated>2015-05-12</updated><authors><author><keyname>Hao</keyname><forenames>Dong</forenames></author><author><keyname>Rong</keyname><forenames>Zhihai</forenames></author><author><keyname>Zhou</keyname><forenames>Tao</forenames></author></authors><title>Extortion under Uncertainty: Zero-Determinant Strategies in Noisy Games</title><categories>cs.GT physics.soc-ph q-bio.PE</categories><comments>8 pages, 4 figures</comments><journal-ref>Phys. Rev. E, 91:052803, May 2015</journal-ref><doi>10.1103/PhysRevE.91.052803</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Repeated game theory has been one of the most prevailing tools for
understanding the long-run relationships, which are footstones in building
human society. Recent works have revealed a new set of &quot;zero-determinant (ZD)&quot;
strategies, which is an important advance in repeated games. A ZD strategy
player can exert a unilaterally control on two players' payoffs. In particular
he can deterministically set the opponent's payoff, or enforce an unfair linear
relationship between the players' payoffs, thereby always seizing an
advantageous share of payoffs. One of the limitations of the original ZD
strategy, however, is that it does not capture the notion of robustness when
the game is subjected to stochastic errors. In this paper, we propose a general
model of ZD strategies for noisy repeated games, and find that ZD strategies
have high robustness against errors. We further derive the pinning strategy
under noise, by which the ZD strategy player coercively set the opponent's
expected payoff to his desired level, although his payoff control ability
declines with the increase of noise strength. Due to the uncertainty caused by
noise, the ZD strategy player cannot secure his payoff to be higher than the
opponent's, which implies strong extortions do not exist even under low noise.
While we show that the ZD strategy player can still establish a novel kind of
extortions, named weak extortions, where any increase of his own payoff always
exceeds that of the opponent's by a fixed percentage, and the conditions under
which the weak extortions can be realized are more stringent as the noise
becomes stronger.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5210</identifier>
 <datestamp>2014-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5210</id><created>2014-08-22</created><authors><author><keyname>Florian</keyname><forenames>Josef</forenames></author><author><keyname>Balkova</keyname><forenames>Lubomira</forenames></author></authors><title>On Periodicity and Complexity of Generalized Pseudostandard Words</title><categories>math.CO cs.FL</categories><msc-class>68R15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Generalized pseudostandard words have been introduced by de Luca and De Luca
in 2006. In comparison to the palindromic and pseudopalindromic closure, only
little is known about the generalized pseudopalindromic closure and the
associated generalized pseudostandard words. We present two new results
concerning these words. The first one is a necessary and sufficient condition
for their periodicity. The second result is a counterexample to Conjecture 43
from the paper: A. B. Masse, G.Paquin, H. Tremblay, and L. Vuillon, On
Generalized Pseudostandard Words over Binary Alphabet (Journal of Int.
Sequences, 16:Article 13.2.11, 2013) that estimated the complexity of binary
generalized pseudostandard words as C(n) being less than or equal to 4n for all
sufficiently large n.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5240</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5240</id><created>2014-08-22</created><authors><author><keyname>Miao</keyname><forenames>Lili</forenames></author><author><keyname>Zhang</keyname><forenames>Qian-Ming</forenames></author><author><keyname>Nie</keyname><forenames>Da-Chen</forenames></author><author><keyname>Cai</keyname><forenames>Shi-Min</forenames></author></authors><title>Whether Information Network Supplements Friendship Network</title><categories>physics.soc-ph cs.SI</categories><comments>8 pages, 5 figures</comments><journal-ref>Physica A 419, 301 (2015)</journal-ref><doi>10.1016/j.physa.2014.10.021</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Homophily is a significant mechanism for link prediction in complex network,
of which principle describes that people with similar profiles or experiences
tend to tie with each other. In a multi-relationship network, friendship among
people has been utilized to reinforce similarity of taste for recommendation
system whose basic idea is similar to homophily, yet how the taste inversely
affects friendship prediction is little discussed. This paper contributes to
address the issue by analyzing two benchmark datasets both including user's
behavioral information of taste and friendship based on the principle of
homophily. It can be found that the creation of friendship tightly associates
with personal taste. Especially, the behavioral information of taste involving
with popular objects is much more effective to improve the performance of
friendship prediction. However, this result seems to be contradictory to the
finding in [Q.M. Zhang, et al., PLoS ONE 8(2013)e62624] that the behavior
information of taste involving with popular objects is redundant in
recommendation system. We thus discuss this inconformity to comprehensively
understand the correlation between them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5241</identifier>
 <datestamp>2014-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5241</id><created>2014-08-22</created><authors><author><keyname>Nguyen</keyname><forenames>Duc-Hien</forenames></author><author><keyname>Le</keyname><forenames>Manh-Thanh</forenames></author></authors><title>A two-stage architecture for stock price forecasting by combining SOM
  and fuzzy-SVM</title><categories>cs.AI cs.LG</categories><comments>6 pages, 3 figures</comments><msc-class>68U35</msc-class><acm-class>I.2.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposed a model to predict the stock price based on combining
Self-Organizing Map (SOM) and fuzzy-Support Vector Machines (f-SVM). Extraction
of fuzzy rules from raw data based on the combining of statistical machine
learning models is base of this proposed approach. In the proposed model, SOM
is used as a clustering algorithm to partition the whole input space into the
several disjoint regions. For each partition, a set of fuzzy rules is extracted
based on a f-SVM combining model. Then fuzzy rules sets are used to predict the
test data using fuzzy inference algorithms. The performance of the proposed
approach is compared with other models using four data sets
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5245</identifier>
 <datestamp>2014-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5245</id><created>2014-08-22</created><authors><author><keyname>Kang</keyname><forenames>Xin</forenames></author><author><keyname>Chia</keyname><forenames>Yeow-Khiang</forenames></author><author><keyname>Sun</keyname><forenames>Sumei</forenames></author><author><keyname>Chong</keyname><forenames>Hon Fah</forenames></author></authors><title>Mobile Data Offloading through A Third-Party WiFi Access Point: An
  Operator's Perspective</title><categories>cs.IT math.IT</categories><comments>To appear in IEEE Trans. Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  WiFi offloading is regarded as one of the most promising techniques to deal
with the explosive data increase in cellular networks due to its high data
transmission rate and low requirement on devices. In this paper, we investigate
the mobile data offloading problem through a third-party WiFi access point (AP)
for a cellular mobile system. From the cellular operator's perspective, by
assuming a usage-based charging model, we formulate the problem as a utility
maximization problem. In particular, we consider three scenarios: (i)
successive interference cancellation (SIC) available at both the base station
(BS) and the AP; (ii) SIC available at neither the BS nor the AP; (iii) SIC
available at only the BS. For (i), we show that the utility maximization
problem can be solved by considering its relaxation problem, and we prove that
our proposed data offloading scheme is near-optimal when the number of users is
large. For (ii), we prove that with high probability the optimal solution is
One-One-Association, i.e., one user connects to the BS and one user connects to
the AP. For (iii), we show that with high probability there is at most one user
connecting to the AP, and all the other users connect to the BS. By comparing
these three scenarios, we prove that SIC decoders help the cellular operator
maximize its utility. To relieve the computational burden of the BS, we propose
a threshold-based distributed data offloading scheme. We show that the proposed
distributed scheme performs well if the threshold is properly chosen.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5246</identifier>
 <datestamp>2014-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5246</id><created>2014-08-22</created><authors><author><keyname>Nguyen</keyname><forenames>Duc-Hien</forenames></author><author><keyname>Le</keyname><forenames>Manh-Thanh</forenames></author></authors><title>Improving the Interpretability of Support Vector Machines-based Fuzzy
  Rules</title><categories>cs.LG cs.AI</categories><comments>8 pages, 2 figures</comments><msc-class>68U35</msc-class><acm-class>I.2.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Support vector machines (SVMs) and fuzzy rule systems are functionally
equivalent under some conditions. Therefore, the learning algorithms developed
in the field of support vector machines can be used to adapt the parameters of
fuzzy systems. Extracting fuzzy models from support vector machines has the
inherent advantage that the model does not need to determine the number of
rules in advance. However, after the support vector machine learning, the
complexity is usually high, and interpretability is also impaired. This paper
not only proposes a complete framework for extracting interpretable SVM-based
fuzzy modeling, but also provides optimization issues of the models.
Simulations examples are given to embody the idea of this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5248</identifier>
 <datestamp>2015-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5248</id><created>2014-08-22</created><updated>2015-06-09</updated><authors><author><keyname>Gawrychowski</keyname><forenames>Pawel</forenames></author><author><keyname>Straszak</keyname><forenames>Damian</forenames></author></authors><title>Strong inapproximability of the shortest reset word</title><categories>cs.FL</categories><comments>extended abstract to appear in MFCS 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The \v{C}ern\'y conjecture states that every $n$-state synchronizing
automaton has a reset word of length at most $(n-1)^2$. We study the hardness
of finding short reset words. It is known that the exact version of the
problem, i.e., finding the shortest reset word, is NP-hard and coNP-hard, and
complete for the DP class, and that approximating the length of the shortest
reset word within a factor of $O(\log n)$ is NP-hard [Gerbush and Heeringa,
CIAA'10], even for the binary alphabet [Berlinkov, DLT'13]. We significantly
improve on these results by showing that, for every $\epsilon&gt;0$, it is NP-hard
to approximate the length of the shortest reset word within a factor of
$n^{1-\epsilon}$. This is essentially tight since a simple $O(n)$-approximation
algorithm exists.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5250</identifier>
 <datestamp>2014-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5250</id><created>2014-08-22</created><authors><author><keyname>Mota</keyname><forenames>Joao F. C.</forenames></author><author><keyname>Deligiannis</keyname><forenames>Nikos</forenames></author><author><keyname>Rodrigues</keyname><forenames>Miguel R. D.</forenames></author></authors><title>Compressed Sensing with Prior Information: Optimal Strategies, Geometry,
  and Bounds</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of compressed sensing (CS) with prior information:
reconstruct a target CS signal with the aid of a similar signal that is known
beforehand, our prior information. We integrate the additional knowledge of the
similar signal into CS via L1-L1 and L1-L2 minimization. We then establish
bounds on the number of measurements required by these problems to successfully
reconstruct the original signal. Our bounds and geometrical interpretations
reveal that if the prior information has good enough quality, L1-L1
minimization improves the performance of CS dramatically. In contrast, L1-L2
minimization has a performance very similar to classical CS and brings no
significant benefits. All our findings are illustrated with experimental
results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5265</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5265</id><created>2014-08-22</created><updated>2014-08-25</updated><authors><author><keyname>Tziortziotis</keyname><forenames>Nikolaos</forenames></author><author><keyname>Papagiannis</keyname><forenames>Georgios</forenames></author><author><keyname>Blekas</keyname><forenames>Konstantinos</forenames></author></authors><title>A Bayesian Ensemble Regression Framework on the Angry Birds Game</title><categories>cs.AI</categories><comments>Angry Birds AI Symposium, ECAI 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An ensemble inference mechanism is proposed on the Angry Birds domain. It is
based on an efficient tree structure for encoding and representing game
screenshots, where it exploits its enhanced modeling capability. This has the
advantage to establish an informative feature space and modify the task of game
playing to a regression analysis problem. To this direction, we assume that
each type of object material and bird pair has its own Bayesian linear
regression model. In this way, a multi-model regression framework is designed
that simultaneously calculates the conditional expectations of several objects
and makes a target decision through an ensemble of regression models. Learning
procedure is performed according to an online estimation strategy for the model
parameters. We provide comparative experimental results on several game levels
that empirically illustrate the efficiency of the proposed methodology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5274</identifier>
 <datestamp>2014-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5274</id><created>2014-08-22</created><authors><author><keyname>Duch&#xea;ne</keyname><forenames>Eric</forenames></author><author><keyname>Parreau</keyname><forenames>Aline</forenames></author><author><keyname>Rigo</keyname><forenames>Michel</forenames></author></authors><title>Deciding game invariance</title><categories>cs.DM cs.CC math.CO</categories><msc-class>91A46, 91A05, 03D05, 03B25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Duch\^ene and Rigo introduced the notion of invariance for take-away games on
heaps. Roughly speaking, these are games whose rulesets do not depend on the
position. Given a sequence $S$ of positive tuples of integers, the question of
whether there exists an invariant game having $S$ as set of
$\mathcal{P}$-positions is relevant. In particular, it was recently proved by
Larsson et al. that if $S$ is a pair of complementary Beatty sequences, then
the answer to this question is always positive. In this paper, we show that for
a fairly large set of sequences (expressed by infinite words), the answer to
this question is decidable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5275</identifier>
 <datestamp>2014-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5275</id><created>2014-08-22</created><authors><author><keyname>Keshtkaran</keyname><forenames>Mohammad Reza</forenames></author><author><keyname>Yang</keyname><forenames>Zhi</forenames></author></authors><title>Unsupervised Spike Sorting Based on Discriminative Subspace Learning</title><categories>cs.CV physics.med-ph</categories><comments>EMBC14</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spike sorting is a fundamental preprocessing step for many neuroscience
studies which rely on the analysis of spike trains. In this paper, we present
two unsupervised spike sorting algorithms based on discriminative subspace
learning. The first algorithm simultaneously learns the discriminative feature
subspace and performs clustering. It uses histogram of features in the most
discriminative projection to detect the number of neurons. The second algorithm
performs hierarchical divisive clustering that learns a discriminative
1-dimensional subspace for clustering in each level of the hierarchy until
achieving almost unimodal distribution in the subspace. The algorithms are
tested on synthetic and in-vivo data, and are compared against two widely used
spike sorting methods. The comparative results demonstrate that our spike
sorting methods can achieve substantially higher accuracy in lower dimensional
feature space, and they are highly robust to noise. Moreover, they provide
significantly better cluster separability in the learned subspace than in the
subspace obtained by principal component analysis or wavelet transform.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5282</identifier>
 <datestamp>2015-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5282</id><created>2014-08-22</created><updated>2015-12-04</updated><authors><author><keyname>Salum</keyname><forenames>Latif</forenames></author></authors><title>On the Tractability of Un/Satisfiability</title><categories>cs.CC</categories><comments>13 pages, 4 figures</comments><acm-class>F.1.1; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Petri net approach proves to be effective to tackle the $P$ vs $NP$
problem. A safe acyclic Petri net (PN) is associated with some Exactly-1 3SAT
formula, in which a clause is an exactly-1 disjunction $\dot{\vee}$ of
literals. A clause also corresponds to a set of conflicting transitions in the
PN. Some 2SAT/XOR-SAT formula arisen in the inversed PN checks if the truth
assignment of a literal (a transition firing) $z_v$ is &quot;incompatible&quot; for the
satisfiability of the 3SAT formula (the reachability of the target state in the
inversed PN). If $z_v$ is incompatible, then $z_v$ is discarded and
$\overline{z}_v$ becomes true. Therefore, a clause $(\overline{z}_v \dot{\vee}
z_i \dot{\vee} z_j)$ reduces to the conjunction $(\overline{z}_v \wedge
\overline{z}_i \wedge \overline{z}_j)$, and a 3-literal clause $(z_v \dot{\vee}
z_u \dot{\vee} z_x)$ reduces to the 2-literal clause $(z_u \oplus z_x)$. This
reduction facilitates checking un/satisfiability; the 3SAT formula is
un/satisfiable iff the target state of the inversed PN is un/reachable. The
solution complexity is $O(n^5)$. Therefore, $P = NP = \text{co}NP$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5286</identifier>
 <datestamp>2016-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5286</id><created>2014-08-22</created><updated>2016-01-13</updated><authors><author><keyname>Livi</keyname><forenames>Lorenzo</forenames></author><author><keyname>Rizzi</keyname><forenames>Antonello</forenames></author><author><keyname>Sadeghian</keyname><forenames>Alireza</forenames></author></authors><title>Designing labeled graph classifiers by exploiting the R\'enyi entropy of
  the dissimilarity representation</title><categories>cs.CV cs.IT math.IT stat.ML</categories><comments>Revised version</comments><acm-class>I.2.6; H.1.1; E.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Representing patterns by complex relational structures, such as labeled
graphs, is becoming an increasingly common practice in the broad field of
computational intelligence. Accordingly, a wide repertoire of pattern
recognition tools, such as classifiers and knowledge discovery procedures, are
nowadays available and tested for various labeled graph data types. However,
the design of effective learning and mining procedures operating in the space
of labeled graphs is still a challenging problem, especially from the
computational complexity viewpoint. In this paper, we present a major
improvement of a general-purpose graph classification system, which is
conceived on an interplay among dissimilarity representation, clustering,
information-theoretic techniques, and evolutionary optimization. The
improvement focuses on a specific key subroutine of the system that performs
the compression of the input data. We prove different theorems which are
fundamental to the setting of such a compression operation. We demonstrate the
effectiveness of the resulting classifier by benchmarking the developed
variants on well-known datasets of labeled graphs, considering as distinct
performance indicators the classification accuracy, the computing time, and the
parsimony in terms of structural complexity of the synthesized classification
model. Overall, the results show state-of-the-art standards in terms of test
set accuracy, while achieving considerable reductions for what concerns both
the effective computing time and model complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5294</identifier>
 <datestamp>2014-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5294</id><created>2014-08-22</created><updated>2014-10-01</updated><authors><author><keyname>Simonetto</keyname><forenames>Andrea</forenames></author><author><keyname>Kester</keyname><forenames>Leon</forenames></author><author><keyname>Leus</keyname><forenames>Geert</forenames></author></authors><title>Distributed Time-Varying Stochastic Optimization and Utility-Based
  Communication</title><categories>math.OC cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We devise a distributed asynchronous stochastic epsilon-gradient-based
algorithm to enable a network of computing and communicating nodes to solve a
constrained discrete-time time-varying stochastic convex optimization problem.
Each node updates its own decision variable only once every discrete time step.
Under some assumptions (among which, strong convexity, Lipschitz continuity of
the gradient, persistent excitation), we prove the algorithm's asymptotic
convergence in expectation to an error bound whose size is related to the
constant stepsize choice alpha, the variability in time of the optimization
problem, and to the accuracy epsilon. Moreover, the convergence rate is linear.
Then, we show how to compute locally stochastic epsilon-gradients that depend
also on the time-varying noise probability density function (PDF) of the
neighboring nodes, without requiring the neighbors to send such PDFs at each
time step. We devise utility-based policies to allow each node to decide
whether to send or not the most up-to-date PDF, which guarantee a given
user-specified error level epsilon in the computation of the stochastic
epsilon-gradient. Numerical simulations display the added value of the proposed
approach and its relevance for estimation and control of time-varying processes
and networked systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5316</identifier>
 <datestamp>2014-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5316</id><created>2014-08-22</created><authors><author><keyname>Yang</keyname><forenames>Xin-She</forenames></author><author><keyname>Deb</keyname><forenames>Suash</forenames></author></authors><title>Cuckoo Search: Recent Advances and Applications</title><categories>math.OC cs.NE nlin.AO</categories><comments>9 pages</comments><msc-class>90-XX</msc-class><journal-ref>X. S. Yang and S. Deb, Cuckoo search: recent advances and
  applications, Neural Computing and Applications, vol. 24, No. 1, pp. 169-174
  (2014)</journal-ref><doi>10.1007/s00521-013-1367-1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cuckoo search (CS) is a relatively new algorithm, developed by Yang and Deb
in 2009, and CS is efficient in solving global optimization problems. In this
paper, we review the fundamental ideas of cuckoo search and the latest
developments as well as its applications. We analyze the algorithm and gain
insight into its search mechanisms and find out why it is efficient. We also
discuss the essence of algorithms and its link to self-organizing systems, and
finally we propose some important topics for further research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5320</identifier>
 <datestamp>2014-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5320</id><created>2014-08-22</created><authors><author><keyname>Yang</keyname><forenames>Xin-She</forenames></author><author><keyname>Karamanoglu</keyname><forenames>M.</forenames></author><author><keyname>Ting</keyname><forenames>T. O.</forenames></author><author><keyname>Zhao</keyname><forenames>Y. X.</forenames></author></authors><title>Applications and Analysis of Bio-Inspired Eagle Strategy for Engineering
  Optimization</title><categories>math.OC cs.NE nlin.AO</categories><comments>14 pages 1 figure</comments><msc-class>90C26</msc-class><journal-ref>Neural Computing and Applications, vol. 25, No. 2, pp. 411-420
  (2014)</journal-ref><doi>10.1007/s00521-013-1508-6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  All swarm-intelligence-based optimization algorithms use some stochastic
components to increase the diversity of solutions during the search process.
Such randomization is often represented in terms of random walks. However, it
is not yet clear why some randomization techniques (and thus why some
algorithms) may perform better than others for a given set of problems. In this
work, we analyze these randomization methods in the context of nature-inspired
algorithms. We also use eagle strategy to provide basic observations and relate
step sizes and search efficiency using Markov theory. Then, we apply our
analysis and observations to solve four design benchmarks, including the
designs of a pressure vessel, a speed reducer, a PID controller and a heat
exchanger. Our results demonstrate that eagle strategy with L\'evy flights can
perform extremely well in reducing the overall computational efforts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5328</identifier>
 <datestamp>2015-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5328</id><created>2014-08-22</created><updated>2015-04-16</updated><authors><author><keyname>Wilde</keyname><forenames>Mark M.</forenames></author><author><keyname>Renes</keyname><forenames>Joseph M.</forenames></author><author><keyname>Guha</keyname><forenames>Saikat</forenames></author></authors><title>Second-order coding rates for pure-loss bosonic channels</title><categories>quant-ph cs.IT math.IT</categories><comments>18 pages, 3 figures; v3: final version accepted for publication in
  Quantum Information Processing</comments><doi>10.1007/s11128-015-0997-x</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A pure-loss bosonic channel is a simple model for communication over
free-space or fiber-optic links. More generally, phase-insensitive bosonic
channels model other kinds of noise, such as thermalizing or amplifying
processes. Recent work has established the classical capacity of all of these
channels, and furthermore, it is now known that a strong converse theorem holds
for the classical capacity of these channels under a particular photon number
constraint. The goal of the present paper is to initiate the study of
second-order coding rates for these channels, by beginning with the simplest
one, the pure-loss bosonic channel. In a second-order analysis of
communication, one fixes the tolerable error probability and seeks to
understand the back-off from capacity for a sufficiently large yet finite
number of channel uses. We find a lower bound on the maximum achievable code
size for the pure-loss bosonic channel, in terms of the known expression for
its capacity and a quantity called channel dispersion. We accomplish this by
proving a general &quot;one-shot&quot; coding theorem for channels with classical inputs
and pure-state quantum outputs which reside in a separable Hilbert space. The
theorem leads to an optimal second-order characterization when the channel
output is finite-dimensional, and it remains an open question to determine
whether the characterization is optimal for the pure-loss bosonic channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5329</identifier>
 <datestamp>2014-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5329</id><created>2014-08-22</created><authors><author><keyname>Pauly</keyname><forenames>Arno</forenames></author></authors><title>The descriptive theory of represented spaces</title><categories>cs.LO math.GN math.LO</categories><comments>survey of work-in-progress</comments><msc-class>03E15, 26A21, 54H05, 18B25</msc-class><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is a survey on the ongoing development of a descriptive theory of
represented spaces, which is intended as an extension of both classical and
effective descriptive set theory to deal with both sets and functions between
represented spaces. Most material is from work-in-progress, and thus there may
be a stronger focus on projects involving the author than an objective survey
would merit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5332</identifier>
 <datestamp>2014-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5332</id><created>2014-08-22</created><authors><author><keyname>Yang</keyname><forenames>Xin-She</forenames></author><author><keyname>Karamanoglu</keyname><forenames>M.</forenames></author><author><keyname>He</keyname><forenames>X. S.</forenames></author></authors><title>Flower Pollination Algorithm: A Novel Approach for Multiobjective
  Optimization</title><categories>math.OC cs.NE nlin.AO</categories><comments>17 pages 8 figures. arXiv admin note: substantial text overlap with
  arXiv:1404.0695</comments><msc-class>90C26</msc-class><journal-ref>X. S. Yang, M. Karamanoglu, X. S. He, Flower Pollination
  Algorithm: A Novel Approach for Multiobjective Optimization, Engineering
  Optimization, 46 (9), pp. 1222 - 1237 (2014)</journal-ref><doi>10.1080/0305215X.2013.832237</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiobjective design optimization problems require multiobjective
optimization techniques to solve, and it is often very challenging to obtain
high-quality Pareto fronts accurately. In this paper, the recently developed
flower pollination algorithm (FPA) is extended to solve multiobjective
optimization problems. The proposed method is used to solve a set of
multobjective test functions and two bi-objective design benchmarks, and a
comparison of the proposed algorithm with other algorithms has been made, which
shows that FPA is efficient with a good convergence rate. Finally, the
importance for further parametric studies and theoretical analysis are
highlighted and discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5340</identifier>
 <datestamp>2014-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5340</id><created>2014-08-22</created><authors><author><keyname>Aldrich</keyname><forenames>Preston R.</forenames></author></authors><title>The curriculum prerequisite network: a tool for visualizing and
  analyzing academic curricula</title><categories>cs.SI cs.CY physics.soc-ph</categories><comments>10 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article advances the prerequisite network as a means to visualize the
hidden structure in an academic curriculum. Network technologies have been used
for some time now in social analyses and more recently in biology in the areas
of genomics and systems biology. Here I treat the curriculum as a complex
system with nodes representing courses and links between nodes the course
prerequisites as readily obtained from a course catalogue. The resulting
curriculum prerequisite network can be rendered as a directed acyclic graph,
which has certain desirable analytical features. The curriculum is seen as
partitioned into numerous isolated course groupings, the size of the groups
varying considerably. Individual courses are seen serving very different roles
in the overall organization, such as information sources, hubs, and bridges.
This network represents the intrinsic, hard-wired constraints on the flow of
information in a curriculum, and is the organizational context within which
learning occurs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5343</identifier>
 <datestamp>2014-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5343</id><created>2014-08-22</created><authors><author><keyname>Fister</keyname><forenames>I.</forenames><suffix>Jr.</suffix></author><author><keyname>Yang</keyname><forenames>X. S.</forenames></author><author><keyname>Fister</keyname><forenames>D.</forenames></author><author><keyname>Fister</keyname><forenames>I.</forenames></author></authors><title>Cuckoo Search: A Brief Literature Review</title><categories>math.OC cs.NE nlin.AO</categories><comments>14 pages 3 figures</comments><msc-class>90C26</msc-class><journal-ref>Cuckoo Search and Firefly Algorithm: Theory and Applications,
  Studies in Computational Intelligence, vol. 516, pp. 49-62 (2014)</journal-ref><doi>10.1007/978-3-319-02141-6_3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cuckoo search (CS) was introduced in 2009, and it has attracted great
attention due to its promising efficiency in solving many optimization problems
and real-world applications. In the last few years, many papers have been
published regarding cuckoo search, and the relevant literature has expanded
significantly. This chapter summarizes briefly the majority of the literature
about cuckoo search in peer-reviewed journals and conferences found so far.
These references can be systematically classified into appropriate categories,
which can be used as a basis for further research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5347</identifier>
 <datestamp>2014-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5347</id><created>2014-08-21</created><authors><author><keyname>Chai</keyname><forenames>Zhilei</forenames></author><author><keyname>Wang</keyname><forenames>Zhibin</forenames></author><author><keyname>Yang</keyname><forenames>Wenmin</forenames></author><author><keyname>Ding</keyname><forenames>Shuai</forenames></author><author><keyname>Zhang</keyname><forenames>Yuanpu</forenames></author></authors><title>OpenHEC: A Framework for Application Programmers to Design FPGA-based
  Systems</title><categories>cs.OH</categories><comments>Presented at First International Workshop on FPGAs for Software
  Programmers (FSP 2014) (arXiv:1408.4423)</comments><proxy>Frank Hannig</proxy><report-no>FSP/2014/12</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Today, there is a trend to incorporate more intelligence (e.g., vision
capabilities) into a wide range of devices, which makes high performance a
necessity for computing systems. Furthermore, for embedded systems, low power
consumption should be generally considered together with high computing
performance. FPGAs, as programmable logic devices able to support different
types of fine-grained parallelisms, their power and performance advantages were
recognized widely. However, designing applications on FPGA-based systems is
traditionally far from a task can be carried out by software programmers.
Generally, hardware engineers and even system-level software engineers have
more hardware/architectural knowledge but fewer algorithm and application
knowledge. Thus, it is critical for computing systems to allow
application-level programmers to realize their idea conveniently, which is
popular in computing systems based on the general processor. In this paper, the
OpenHEC (Open Framework for High-Efficiency Computing) framework is proposed to
provide a design framework for application-level software programmers to use
FPGA-based platforms. It frees users from hardware and architectural details to
let them focus more on algorithms/applications. This framework was integrated
with the commercial Xilinx ISE/Vivado to make it to be used immediately. After
implementing a widely-used feature detection algorithm on OpenHEC from the
perspective of software programmers, it shows this framework is applicable for
application programmers with little hardware knowledge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5348</identifier>
 <datestamp>2014-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5348</id><created>2014-08-22</created><authors><author><keyname>Yang</keyname><forenames>Xin-She</forenames></author><author><keyname>Deb</keyname><forenames>Suash</forenames></author><author><keyname>Fong</keyname><forenames>Simon</forenames></author></authors><title>Bat Algorithm is Better Than Intermittent Search Strategy</title><categories>math.OC cs.NE</categories><comments>11 pages 1 figure. Available as X. S. Yang, S. Deb, S. Fong, Bat
  Algorithm is Better Than Intermittent Search Strategy, Multiple-Valued Logic
  and Soft Computing, 22 (3), 223-237 (2014). arXiv admin note: substantial
  text overlap with arXiv:1308.3898</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The efficiency of any metaheuristic algorithm largely depends on the way of
balancing local intensive exploitation and global diverse exploration. Studies
show that bat algorithm can provide a good balance between these two key
components with superior efficiency. In this paper, we first review some
commonly used metaheuristic algorithms, and then compare the performance of bat
algorithm with the so-called intermittent search strategy. From simulations, we
found that bat algorithm is better than the optimal intermittent search
strategy. We also analyse the comparison results and their implications for
higher dimensional optimization problems. In addition, we also apply bat
algorithm in solving business optimization and engineering design problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5350</identifier>
 <datestamp>2014-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5350</id><created>2014-08-22</created><authors><author><keyname>Kononova</keyname><forenames>Anna V.</forenames></author><author><keyname>Corne</keyname><forenames>David W.</forenames></author><author><keyname>De Wilde</keyname><forenames>Philippe</forenames></author><author><keyname>Shneer</keyname><forenames>Vsevolod</forenames></author><author><keyname>Caraffini</keyname><forenames>Fabio</forenames></author></authors><title>Structural bias in population-based algorithms</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Challenging optimisation problems are abundant in all areas of science. Since
the 1950s, scientists have developed ever-diversifying families of black box
optimisation algorithms designed to address any optimisation problem, requiring
only that quality of a candidate solution is calculated via a fitness function
specific to the problem. For such algorithms to be successful, at least three
properties are required: an effective informed sampling strategy, that guides
generation of new candidates on the basis of fitnesses and locations of
previously visited candidates; mechanisms to ensure efficiency, so that same
candidates are not repeatedly visited; absence of structural bias, which, if
present, would predispose the algorithm towards limiting its search to some
regions of solution space. The first two of these properties have been
extensively investigated, however the third is little understood. In this
article we provide theoretical and empirical analyses that contribute to the
understanding of structural bias. We prove a theorem concerning dynamics of
population variance in the case of real-valued search spaces. This reveals how
structural bias can manifest as non-uniform clustering of population over time.
Theory predicts that structural bias is exacerbated with increasing population
size and problem difficulty. These predictions reveal two previously
unrecognised aspects of structural bias. Respectively, increasing population
size, though ostensibly promoting diversity, will magnify any inherent
structural bias, and effects of structural bias are more apparent when faced
with difficult problems. Our theoretical result also suggests that two commonly
used approaches to enhancing exploration, increasing population size and
increasing disruptiveness of search operators, have quite distinct implications
in terms of structural bias.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5352</identifier>
 <datestamp>2014-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5352</id><created>2014-08-22</created><authors><author><keyname>Wang</keyname><forenames>Zhaoran</forenames></author><author><keyname>Lu</keyname><forenames>Huanran</forenames></author><author><keyname>Liu</keyname><forenames>Han</forenames></author></authors><title>Nonconvex Statistical Optimization: Minimax-Optimal Sparse PCA in
  Polynomial Time</title><categories>stat.ML cs.LG</categories><comments>64 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse principal component analysis (PCA) involves nonconvex optimization for
which the global solution is hard to obtain. To address this issue, one popular
approach is convex relaxation. However, such an approach may produce suboptimal
estimators due to the relaxation effect. To optimally estimate sparse principal
subspaces, we propose a two-stage computational framework named &quot;tighten after
relax&quot;: Within the 'relax' stage, we approximately solve a convex relaxation of
sparse PCA with early stopping to obtain a desired initial estimator; For the
'tighten' stage, we propose a novel algorithm called sparse orthogonal
iteration pursuit (SOAP), which iteratively refines the initial estimator by
directly solving the underlying nonconvex problem. A key concept of this
two-stage framework is the basin of attraction. It represents a local region
within which the `tighten' stage has desired computational and statistical
guarantees. We prove that, the initial estimator obtained from the 'relax'
stage falls into such a region, and hence SOAP geometrically converges to a
principal subspace estimator which is minimax-optimal within a certain model
class. Unlike most existing sparse PCA estimators, our approach applies to the
non-spiked covariance models, and adapts to non-Gaussianity as well as
dependent data settings. Moreover, through analyzing the computational
complexity of the two stages, we illustrate an interesting phenomenon that
larger sample size can reduce the total iteration complexity. Our framework
motivates a general paradigm for solving many complex statistical problems
which involve nonconvex optimization with provable guarantees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5366</identifier>
 <datestamp>2015-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5366</id><created>2014-08-22</created><updated>2015-09-09</updated><authors><author><keyname>Barreto</keyname><forenames>Carlos</forenames></author><author><keyname>Mojica-Nava</keyname><forenames>Eduardo</forenames></author><author><keyname>Quijano</keyname><forenames>Nicanor</forenames></author></authors><title>Incentives-Based Mechanism for Efficient Demand Response Programs</title><categories>cs.GT</categories><comments>38 pages, 9 figures, submitted to journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we investigate the inefficiency of the electricity system with
strategic agents. Specifically, we prove that without a proper control the
total demand of an inefficient system is at most twice the total demand of the
optimal outcome. We propose an incentives scheme that promotes optimal outcomes
in the inefficient electricity market. The economic incentives can be seen as
an indirect revelation mechanism that allocates resources using a
one-dimensional message space per resource to be allocated. The mechanism does
not request private information from users and is valid for any concave
customer's valuation function. We propose a distributed implementation of the
mechanism using population games and evaluate the performance of four popular
dynamics methods in terms of the cost to implement the mechanism. We find that
the achievement of efficiency in strategic environments might be achieved at a
cost, which is dependent on both the users' preferences and the dynamic
evolution of the system. Some simulation results illustrate the ideas presented
throughout the paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5377</identifier>
 <datestamp>2014-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5377</id><created>2014-08-22</created><authors><author><keyname>Derrien</keyname><forenames>Alban</forenames></author><author><keyname>Petit</keyname><forenames>Thierry</forenames></author><author><keyname>Zampelli</keyname><forenames>Stephane</forenames></author></authors><title>Dynamic Sweep Filtering Algorithm for FlexC</title><categories>cs.AI</categories><report-no>TR-Mines Nantes: 14/1/INFO</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate cumulative scheduling in uncertain environments, using
constraint programming. We detail in this paper the dynamic sweep filtering
algorithm of the FlexC global constraint.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5378</identifier>
 <datestamp>2014-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5378</id><created>2014-08-19</created><authors><author><keyname>Hamdi</keyname><forenames>Maziyar</forenames></author><author><keyname>Solman</keyname><forenames>Grayden</forenames></author><author><keyname>Kingstone</keyname><forenames>Alan</forenames></author><author><keyname>Krishnamurthy</keyname><forenames>Vikram</forenames></author></authors><title>Social Learning in a Human Society: An Experimental Study</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an experimental study to investigate the learning and
decision making behavior of individuals in a human society. Social learning is
used as the mathematical basis for modelling interaction of individuals that
aim to perform a perceptual task interactively. A psychology experiment was
conducted on a group of undergraduate students at the University of British
Columbia to examine whether the decision (action) of one individual affects the
decision of the subsequent individuals. The major experimental observation that
stands out here is that the participants of the experiment (agents) were
affected by decisions of their partners in a relatively large fraction (60%) of
trials. We fit a social learning model that mimics the interactions between
participants of the psychology experiment. Misinformation propagation (also
known as data incest) within the society under study is further investigated in
this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5380</identifier>
 <datestamp>2014-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5380</id><created>2014-08-22</created><authors><author><keyname>Garcia-Bernardo</keyname><forenames>Javier</forenames></author><author><keyname>Eppstein</keyname><forenames>Margaret J.</forenames></author></authors><title>Evolving Modular Genetic Regulatory Networks with a Recursive, Top-Down
  Approach</title><categories>cs.CE q-bio.MN</categories><acm-class>J.3; I.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Being able to design genetic regulatory networks (GRNs) to achieve a desired
cellular function is one of the main goals of synthetic biology. However,
determining minimal GRNs that produce desired time-series behaviors is
non-trivial. In this paper, we propose a 'top-down' approach to evolving small
GRNs and then use these to recursively boot-strap the identification of larger,
more complex, modular GRNs. We start with relatively dense GRNs and then use
differential evolution (DE) to evolve interaction coefficients. When the target
dynamical behavior is found embedded in a dense GRN, we narrow the focus of the
search and begin aggressively pruning out excess interactions at the end of
each generation. We first show that the method can quickly rediscover known
small GRNs for a toggle switch and an oscillatory circuit. Next we include
these GRNs as non-evolvable subnetworks in the subsequent evolution of more
complex, modular GRNs. Successful solutions found in canonical DE where we
truncated small interactions to zero, with or without an interaction penalty
term, invariably contained many excess interactions. In contrast, by
incorporating aggressive pruning and the penalty term, the DE was able to find
minimal or nearly minimal GRNs in all test problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5383</identifier>
 <datestamp>2014-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5383</id><created>2014-08-21</created><authors><author><keyname>Weinhardt</keyname><forenames>Markus</forenames></author><author><keyname>H&#xf6;ckmann</keyname><forenames>Rainer</forenames></author><author><keyname>Kinder</keyname><forenames>Thomas</forenames></author></authors><title>High-Level Design of Portable and Scalable FPGA Accelerators</title><categories>cs.OH</categories><comments>Presented at First International Workshop on FPGAs for Software
  Programmers (FSP 2014) (arXiv:1408.4423)</comments><proxy>Frank Hannig</proxy><report-no>FSP/2014/07</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents our approach for making FPGA accelerators accessible to
software (SW) programmers. It is intended as a starting point for
collaborations with other groups pursuing similar objectives. We report on our
current SAccO platform (Scalable Accelerator platform Osnabr\&quot;uck) and the
planned project extending this platform.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5386</identifier>
 <datestamp>2014-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5386</id><created>2014-08-21</created><authors><author><keyname>Sano</keyname><forenames>Kentaro</forenames></author><author><keyname>Suzuki</keyname><forenames>Hayato</forenames></author><author><keyname>Ito</keyname><forenames>Ryo</forenames></author><author><keyname>Ueno</keyname><forenames>Tomohiro</forenames></author><author><keyname>Yamamoto</keyname><forenames>Satoru</forenames></author></authors><title>Stream Processor Generator for HPC to Embedded Applications on
  FPGA-based System Platform</title><categories>cs.OH</categories><comments>Presented at First International Workshop on FPGAs for Software
  Programmers (FSP 2014) (arXiv:1408.4423)</comments><proxy>Frank Hannig</proxy><report-no>FSP/2014/09</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a stream processor generator, called SPGen, for
FPGA-based system-on-chip platforms. In our research project, we use an FPGA as
a common platform for applications ranging from HPC to embedded/robotics
computing. Pipelining in application-specific stream processors brings FPGAs
power-efficient and high-performance computing. However, poor productivity in
developing custom pipelines prevents the reconfigurable platform from being
widely and easily used. SPGen aims at assisting developers to design and
implement high-throughput stream processors by generating their HDL codes with
our domain-specific high-level stream processing description, called SPD.With
an example of fluid dynamics computation, we validate SPD for describing a real
application and verify SPGen for synthesis with a pipelined data-flow graph. We
also demonstrate that SPGen allows us to easily explore a design space for
finding better implementation than a hand-designed one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5387</identifier>
 <datestamp>2014-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5387</id><created>2014-08-21</created><authors><author><keyname>Karras</keyname><forenames>Kimon</forenames></author><author><keyname>Blott</keyname><forenames>Michaela</forenames></author><author><keyname>Vissers</keyname><forenames>Kees</forenames></author></authors><title>High-Level Synthesis Case Study: Implementation of a Memcached Server</title><categories>cs.OH</categories><comments>Presented at First International Workshop on FPGAs for Software
  Programmers (FSP 2014) (arXiv:1408.4423)</comments><proxy>Frank Hannig</proxy><report-no>FSP/2014/15</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  High-Level Synthesis (HLS) aspires to raise the level of abstraction in
hardware design without sacrificing hardware efficiency. It has so far been
successfully employed in signal and video processing but has found only limited
use in other areas. This paper utilizes a commercial HLS tool, namely Vivado(R)
HLS, to implement the processing of a common data center application, the
Key-Value Store (KVS) application memcached, as a deeply pipelined dataflow
architecture. We compared our results to a fully equivalent RTL implementation
done previously in our group and found that it matches its performance, yields
tangible improvements in latency (between 7-30%) and resource consumption (22%
in LUTs and 35% in registers), all while requiring 3x less lines of code and 2x
less development time. The implementation was validated in hardware on a
Xilinx(R) VC709 development board, meeting timing requirements for 10Gbps line
rate processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5389</identifier>
 <datestamp>2014-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5389</id><created>2014-08-22</created><authors><author><keyname>Qian</keyname><forenames>Zhensong</forenames></author><author><keyname>Schulte</keyname><forenames>Oliver</forenames></author><author><keyname>Sun</keyname><forenames>Yan</forenames></author></authors><title>Computing Multi-Relational Sufficient Statistics for Large Databases</title><categories>cs.LG cs.DB</categories><comments>11pages, 8 figures, 8 tables, CIKM'14,November 3--7, 2014, Shanghai,
  China</comments><acm-class>H.2.8; H.2.4</acm-class><doi>10.1145/2661829.2662010</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Databases contain information about which relationships do and do not hold
among entities. To make this information accessible for statistical analysis
requires computing sufficient statistics that combine information from
different database tables. Such statistics may involve any number of {\em
positive and negative} relationships. With a naive enumeration approach,
computing sufficient statistics for negative relationships is feasible only for
small databases. We solve this problem with a new dynamic programming algorithm
that performs a virtual join, where the requisite counts are computed without
materializing join tables. Contingency table algebra is a new extension of
relational algebra, that facilitates the efficient implementation of this
M\&quot;obius virtual join operation. The M\&quot;obius Join scales to large datasets
(over 1M tuples) with complex schemas. Empirical evaluation with seven
benchmark datasets showed that information about the presence and absence of
links can be exploited in feature selection, association rule mining, and
Bayesian network learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5400</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5400</id><created>2014-08-22</created><authors><author><keyname>Xu</keyname><forenames>Jiaolong</forenames></author><author><keyname>Ramos</keyname><forenames>Sebastian</forenames></author><author><keyname>Vazquez</keyname><forenames>David</forenames></author><author><keyname>Lopez</keyname><forenames>Antonio M.</forenames></author></authors><title>Hierarchical Adaptive Structural SVM for Domain Adaptation</title><categories>cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A key topic in classification is the accuracy loss produced when the data
distribution in the training (source) domain differs from that in the testing
(target) domain. This is being recognized as a very relevant problem for many
computer vision tasks such as image classification, object detection, and
object category recognition. In this paper, we present a novel domain
adaptation method that leverages multiple target domains (or sub-domains) in a
hierarchical adaptation tree. The core idea is to exploit the commonalities and
differences of the jointly considered target domains.
  Given the relevance of structural SVM (SSVM) classifiers, we apply our idea
to the adaptive SSVM (A-SSVM), which only requires the target domain samples
together with the existing source-domain classifier for performing the desired
adaptation. Altogether, we term our proposal as hierarchical A-SSVM (HA-SSVM).
  As proof of concept we use HA-SSVM for pedestrian detection and object
category recognition. In the former we apply HA-SSVM to the deformable
part-based model (DPM) while in the latter HA-SSVM is applied to multi-category
classifiers. In both cases, we show how HA-SSVM is effective in increasing the
detection/recognition accuracy with respect to adaptation strategies that
ignore the structure of the target data. Since, the sub-domains of the target
data are not always known a priori, we shown how HA-SSVM can incorporate
sub-domain structure discovery for object category recognition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5401</identifier>
 <datestamp>2014-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5401</id><created>2014-08-21</created><authors><author><keyname>V&#xe9;stias</keyname><forenames>M&#xe1;rio</forenames></author><author><keyname>Neto</keyname><forenames>Hor&#xe1;cio</forenames></author></authors><title>A Many-Core Overlay for High-Performance Embedded Computing on FPGAs</title><categories>cs.AR</categories><comments>Presented at First International Workshop on FPGAs for Software
  Programmers (FSP 2014) (arXiv:1408.4423)</comments><proxy>Frank Hannig</proxy><report-no>FSP/2014/14</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we propose a configurable many-core overlay for
high-performance embedded computing. The size of internal memory, supported
operations and number of ports can be configured independently for each core of
the overlay. The overlay was evaluated with matrix multiplication, LU
decomposition and Fast-Fourier Transform (FFT) on a ZYNQ-7020 FPGA platform.
The results show that using a system-level many-core overlay avoids complex
hardware design and still provides good performance results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5403</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5403</id><created>2014-08-22</created><authors><author><keyname>Liu</keyname><forenames>Peilei</forenames></author><author><keyname>Wang</keyname><forenames>Ting</forenames></author></authors><title>Neural Mechanism of Language</title><categories>cs.NE cs.CL q-bio.NC</categories><comments>6 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is based on our previous work on neural coding. It is a
self-organized model supported by existing evidences. Firstly, we briefly
introduce this model in this paper, and then we explain the neural mechanism of
language and reasoning with it. Moreover, we find that the position of an area
determines its importance. Specifically, language relevant areas are in the
capital position of the cortical kingdom. Therefore they are closely related
with autonomous consciousness and working memories. In essence, language is a
miniature of the real world. Briefly, this paper would like to bridge the gap
between molecule mechanism of neurons and advanced functions such as language
and reasoning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5405</identifier>
 <datestamp>2015-11-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5405</id><created>2014-08-22</created><updated>2015-11-13</updated><authors><author><keyname>Raza</keyname><forenames>Khalid</forenames></author><author><keyname>Alam</keyname><forenames>Mansaf</forenames></author></authors><title>Recurrent Neural Network Based Hybrid Model of Gene Regulatory Network</title><categories>cs.NE cs.CE q-bio.MN</categories><comments>18 pages, 9 figures and 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Systems biology is an emerging interdisciplinary area of research that
focuses on study of complex interactions in a biological system, such as gene
regulatory networks. The discovery of gene regulatory networks leads to a wide
range of applications, such as pathways related to a disease that can unveil in
what way the disease acts and provide novel tentative drug targets. In
addition, the development of biological models from discovered networks or
pathways can help to predict the responses to disease and can be much useful
for the novel drug development and treatments. The inference of regulatory
networks from biological data is still in its infancy stage. This paper
proposes a recurrent neural network (RNN) based gene regulatory network (GRN)
model hybridized with generalized extended Kalman filter for weight update in
backpropagation through time training algorithm. The RNN is a complex neural
network that gives a better settlement between the biological closeness and
mathematical flexibility to model GRN. The RNN is able to capture complex,
non-linear and dynamic relationship among variables. Gene expression data are
inherently noisy and Kalman filter performs well for estimation even in noisy
data. Hence, non-linear version of Kalman filter, i.e., generalized extended
Kalman filter has been applied for weight update during network training. The
developed model has been applied on DNA SOS repair network, IRMA network, and
two synthetic networks from DREAM Challenge. We compared our results with other
state-of-the-art techniques that show superiority of our model. Further, 5%
Gaussian noise has been added in the dataset and result of the proposed model
shows negligible effect of noise on the results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5412</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5412</id><created>2014-08-14</created><authors><author><keyname>Purcell</keyname><forenames>Christopher</forenames></author><author><keyname>Rombach</keyname><forenames>M. Puck</forenames></author></authors><title>On the Complexity of Role Colouring Planar Graphs, Trees and Cographs</title><categories>cs.DS cs.CC cs.DM</categories><msc-class>68R10</msc-class><acm-class>F.2.2; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove several results about the complexity of the role colouring problem.
A role colouring of a graph $G$ is an assignment of colours to the vertices of
$G$ such that two vertices of the same colour have identical sets of colours in
their neighbourhoods. We show that the problem of finding a role colouring with
$1&lt; k &lt;n$ colours is NP-hard for planar graphs. We show that restricting the
problem to trees yields a polynomially solvable case, as long as $k$ is either
constant or has a constant difference with $n$, the number of vertices in the
tree. Finally, we prove that cographs are always $k$-role-colourable for
$1&lt;k\leq n$ and construct such a colouring in polynomial time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5418</identifier>
 <datestamp>2015-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5418</id><created>2014-08-11</created><updated>2015-08-04</updated><authors><author><keyname>Shi</keyname><forenames>Jianping</forenames></author><author><keyname>Yan</keyname><forenames>Qiong</forenames></author><author><keyname>Xu</keyname><forenames>Li</forenames></author><author><keyname>Jia</keyname><forenames>Jiaya</forenames></author></authors><title>Hierarchical Saliency Detection on Extended CSSD</title><categories>cs.CV</categories><comments>14 pages, 15 figures</comments><report-no>CUHK-CSE-201408</report-no><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Complex structures commonly exist in natural images. When an image contains
small-scale high-contrast patterns either in the background or foreground,
saliency detection could be adversely affected, resulting erroneous and
non-uniform saliency assignment. The issue forms a fundamental challenge for
prior methods. We tackle it from a scale point of view and propose a
multi-layer approach to analyze saliency cues. Different from varying patch
sizes or downsizing images, we measure region-based scales. The final saliency
values are inferred optimally combining all the saliency cues in different
scales using hierarchical inference. Through our inference model, single-scale
information is selected to obtain a saliency map. Our method improves detection
quality on many images that cannot be handled well traditionally. We also
construct an extended Complex Scene Saliency Dataset (ECSSD) to include complex
but general natural images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5420</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5420</id><created>2014-08-20</created><authors><author><keyname>Williams</keyname><forenames>Nathalie E.</forenames></author><author><keyname>Thomas</keyname><forenames>Timothy A.</forenames></author><author><keyname>Dunbar</keyname><forenames>Matthew</forenames></author><author><keyname>Eagle</keyname><forenames>Nathan</forenames></author><author><keyname>Dobra</keyname><forenames>Adrian</forenames></author></authors><title>Measures of Human Mobility Using Mobile Phone Records Enhanced with GIS
  Data</title><categories>physics.soc-ph cs.SI stat.AP</categories><comments>33 pages, 16 figures, 5 tables</comments><doi>10.1371/journal.pone.0133630</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the past decade, large scale mobile phone data have become available for
the study of human movement patterns. These data hold an immense promise for
understanding human behavior on a vast scale, and with a precision and accuracy
never before possible with censuses, surveys or other existing data collection
techniques. There is already a significant body of literature that has made key
inroads into understanding human mobility using this exciting new data source,
and there have been several different measures of mobility used. However,
existing mobile phone based mobility measures are inconsistent, inaccurate, and
confounded with social characteristics of local context. New measures would
best be developed immediately as they will influence future studies of mobility
using mobile phone data. In this article, we do exactly this. We discuss
problems with existing mobile phone based measures of mobility and describe new
methods for measuring mobility that address these concerns. Our measures of
mobility, which incorporate both mobile phone records and detailed GIS data,
are designed to address the spatial nature of human mobility, to remain
independent of social characteristics of context, and to be comparable across
geographic regions and time. We also contribute a discussion of the variety of
uses for these new measures in developing a better understanding of how human
mobility influences micro-level human behaviors and well-being, and macro-level
social organization and change.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5422</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5422</id><created>2014-08-11</created><authors><author><keyname>Stassiy</keyname><forenames>Igor</forenames></author></authors><title>Analysis of String Sorting using Heapsort</title><categories>cs.DS cs.CC</categories><comments>Master thesis</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this master thesis we analyze the complexity of sorting a set of strings.
It was shown that the complexity of sorting strings can be naturally expressed
in terms of the prefix trie induced by the set of strings. The model of
computation takes into account symbol comparisons and not just comparisons
between the strings. The analysis of upper and lower bounds for some classical
algorithms such as Quicksort and Mergesort in terms of such a model was shown.
Here we extend the analysis to another classical algorithm - Heapsort. We also
give analysis for the version of the algorithm that uses Binomial heaps as a
heap implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5425</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5425</id><created>2014-08-22</created><authors><author><keyname>Moore</keyname><forenames>Cristopher</forenames></author><author><keyname>Russell</keyname><forenames>Alexander</forenames></author></authors><title>Heat and Noise on Cubes and Spheres: The Sensitivity of Randomly Rotated
  Polynomial Threshold Functions</title><categories>cs.CC math.CO math.RT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We establish a precise relationship between spherical harmonics and Fourier
basis functions over a hypercube randomly embedded in the sphere. In
particular, we give a bound on the expected Boolean noise sensitivity of a
randomly rotated function in terms of its &quot;spherical sensitivity,&quot; which we
define according to its evolution under the spherical heat equation. As an
application, we prove an average case of the Gotsman-Linial conjecture,
bounding the sensitivity of polynomial threshold functions subjected to a
random rotation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5427</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5427</id><created>2014-08-21</created><authors><author><keyname>Godfrey</keyname><forenames>Daniel</forenames></author><author><keyname>Johns</keyname><forenames>Caley</forenames></author><author><keyname>Meyer</keyname><forenames>Carl</forenames></author><author><keyname>Race</keyname><forenames>Shaina</forenames></author><author><keyname>Sadek</keyname><forenames>Carol</forenames></author></authors><title>A Case Study in Text Mining: Interpreting Twitter Data From World Cup
  Tweets</title><categories>stat.ML cs.CL cs.IR cs.LG</categories><acm-class>I.5.4; I.2.7; H.2.8; H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cluster analysis is a field of data analysis that extracts underlying
patterns in data. One application of cluster analysis is in text-mining, the
analysis of large collections of text to find similarities between documents.
We used a collection of about 30,000 tweets extracted from Twitter just before
the World Cup started. A common problem with real world text data is the
presence of linguistic noise. In our case it would be extraneous tweets that
are unrelated to dominant themes. To combat this problem, we created an
algorithm that combined the DBSCAN algorithm and a consensus matrix. This way
we are left with the tweets that are related to those dominant themes. We then
used cluster analysis to find those topics that the tweets describe. We
clustered the tweets using k-means, a commonly used clustering algorithm, and
Non-Negative Matrix Factorization (NMF) and compared the results. The two
algorithms gave similar results, but NMF proved to be faster and provided more
easily interpreted results. We explored our results using two visualization
tools, Gephi and Wordle.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5445</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5445</id><created>2014-08-22</created><updated>2014-09-26</updated><authors><author><keyname>Fogarty</keyname><forenames>Neville</forenames></author><author><keyname>Gluesing-Luerssen</keyname><forenames>Heide</forenames></author></authors><title>A Circulant Approach to Skew-Constacyclic Codes</title><categories>cs.IT math.IT math.RA</categories><msc-class>11T71, 16S36, 94B05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce circulant matrices that capture the structure of a
skew-polynomial ring F[x;\theta] modulo the left ideal generated by a
polynomial of the type x^n-a. This allows us to develop an approach to
skew-constacyclic codes based on such circulants. Properties of these
circulants are derived, and in particular it is shown that the transpose of a
certain circulant is a circulant again. This recovers the well-known result
that the dual of a skew-constacyclic code is a constacyclic code again. Special
attention is paid to the case where x^n-a is two-sided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5449</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5449</id><created>2014-08-22</created><authors><author><keyname>Toh</keyname><forenames>Kar-Ann</forenames></author></authors><title>Stretchy Polynomial Regression</title><categories>cs.LG stat.ML</categories><comments>Article created in April and revised in August 2014. Submitted to
  ICARCV 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article proposes a novel solution for stretchy polynomial regression
learning. The solution comes in primal and dual closed-forms similar to that of
ridge regression. Essentially, the proposed solution stretches the covariance
computation via a power term thereby compresses or amplifies the estimation.
Our experiments on both synthetic data and real-world data show effectiveness
of the proposed method for compressive learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5456</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5456</id><created>2014-08-23</created><authors><author><keyname>Deng</keyname><forenames>Houtao</forenames></author></authors><title>Interpreting Tree Ensembles with inTrees</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tree ensembles such as random forests and boosted trees are accurate but
difficult to understand, debug and deploy. In this work, we provide the inTrees
(interpretable trees) framework that extracts, measures, prunes and selects
rules from a tree ensemble, and calculates frequent variable interactions. An
rule-based learner, referred to as the simplified tree ensemble learner (STEL),
can also be formed and used for future prediction. The inTrees framework can
applied to both classification and regression problems, and is applicable to
many types of tree ensembles, e.g., random forests, regularized random forests,
and boosted trees. We implemented the inTrees algorithms in the &quot;inTrees&quot; R
package.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5460</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5460</id><created>2014-08-23</created><authors><author><keyname>Verma</keyname><forenames>Priyanka</forenames></author><author><keyname>Kesswani</keyname><forenames>Nishtha</forenames></author></authors><title>Web Usage mining framework for Data Cleaning and IP address
  Identification</title><categories>cs.DB</categories><comments>4 pages, IJASCSE,online published by 5th sept 2014 at following link
  http://www.ijascse.org/publications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The World Wide Web is the most wide known information source that is easily
available and searchable. It consists of billions of interconnected documents
Web pages are authored by millions of people. Accesses made by various users to
pages are recorded inside web logs. These log files exist in various formats.
Because of increase in usage of web, size of web log files is increasing at a
much faster rate. Web mining is application of data mining technique to these
log files. It can be of three types Web usage mining, Web structure mining and
Web content mining. Web Usage mining is mining of usage patterns of users which
can then be used to personalize web sites and create attractive web sites. It
consists of three main phases: Preprocessing, Pattern discovery and Pattern
analysis. In this paper we focus on Data cleaning and IP Address identification
stages of preprocessing. Methodology has been proposed for both the stages. At
the end conclusion is made about number of users left after IP address
identification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5468</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5468</id><created>2014-08-23</created><authors><author><keyname>Yang</keyname><forenames>Bin</forenames></author><author><keyname>Tang</keyname><forenames>Xiaohu</forenames></author><author><keyname>Li</keyname><forenames>Jie</forenames></author></authors><title>A Sytematic Piggybacking Design for Minimum Storage Regenerating Codes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Piggybacking is an efficient method to decrease the repair bandwidth of
Maximum Distance Separable (MDS) codes or Minimum Storage Regenerating (MSR)
codes. In this paper, for minimizing the repair bandwidth of parity nodes of
the known MSR codes with high rate, which is usually the whole size of the
original data, i.e., the maximal, a new systematic piggybacking design is
proposed through an in-depth analysis of the design of piggybacking. As a
result, new MSR codes are obtained with almost optimal repair bandwidth of
parity nodes while retaining the optimal repair bandwidth of systematic nodes.
Furthermore, MSR codes with balanced download during node repair process are
presented based on the new piggybacking design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5488</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5488</id><created>2014-08-23</created><updated>2015-06-01</updated><authors><author><keyname>Morrison</keyname><forenames>Natasha</forenames></author><author><keyname>Noel</keyname><forenames>Jonathan A.</forenames></author><author><keyname>Scott</keyname><forenames>Alex</forenames></author></authors><title>Saturation in the Hypercube and Bootstrap Percolation</title><categories>math.CO cs.DM</categories><comments>21 pages, 2 figures. To appear in Combinatorics, Probability and
  Computing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $Q_d$ denote the hypercube of dimension $d$. Given $d\geq m$, a spanning
subgraph $G$ of $Q_d$ is said to be $(Q_d,Q_m)$-saturated if it does not
contain $Q_m$ as a subgraph but adding any edge of $E(Q_d)\setminus E(G)$
creates a copy of $Q_m$ in $G$. Answering a question of Johnson and Pinto, we
show that for every fixed $m\geq2$ the minimum number of edges in a
$(Q_d,Q_m)$-saturated graph is $\Theta(2^d)$.
  We also study weak saturation, which is a form of bootstrap percolation. A
spanning subgraph of $Q_d$ is said to be weakly $(Q_d,Q_m)$-saturated if the
edges of $E(Q_d)\setminus E(G)$ can be added to $G$ one at a time so that each
added edge creates a new copy of $Q_m$. Answering another question of Johnson
and Pinto, we determine the minimum number of edges in a weakly
$(Q_d,Q_m)$-saturated graph for all $d\geq m\geq1$. More generally, we
determine the minimum number of edges in a subgraph of the $d$-dimensional grid
$P_k^d$ which is weakly saturated with respect to `axis aligned' copies of a
smaller grid $P_r^m$. We also study weak saturation of cycles in the grid.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5490</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5490</id><created>2014-08-23</created><updated>2014-09-02</updated><authors><author><keyname>Greer</keyname><forenames>Kieran</forenames></author></authors><title>New Ideas for Brain Modelling 2</title><categories>cs.AI</categories><comments>This is an extended version of arXiv:1403.6274, with a different
  conclusions section as well</comments><journal-ref>K. Arai et al. (eds.), Intelligent Systems in Science and
  Information 2014, Studies in Computational Intelligence, Vol. 591, pp. 23 -
  39, Springer International Publishing Switzerland, 2015</journal-ref><doi>10.1007/978-3-319-14654-6_2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a relatively simple way of allowing a brain model to
self-organise its concept patterns through nested structures. For a simulation,
time reduction is helpful and it would be able to show how patterns may form
and then fire in sequence, as part of a search or thought process. It uses a
very simple equation to show how the inhibitors in particular, can switch off
certain areas, to allow other areas to become the prominent ones and thereby
define the current brain state. This allows for a small amount of control over
what appears to be a chaotic structure inside of the brain. It is attractive
because it is still mostly mechanical and therefore can be added as an
automatic process, or the modelling of that. The paper also describes how the
nested pattern structure can be used as a basic counting mechanism. Another
mathematical conclusion provides a basis for maintaining memory or concept
patterns. The self-organisation can space itself through automatic processes.
This might allow new neurons to be added in a more even manner and could help
to maintain the concept integrity. The process might also help with finding
memory structures afterwards. This extended version integrates further with the
existing cognitive model and provides some new conclusions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5492</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5492</id><created>2014-08-23</created><authors><author><keyname>Levin</keyname><forenames>Mark Sh.</forenames></author></authors><title>Towards Decision Support Technology Platform for Modular Systems</title><categories>cs.SY cs.AI math.OC</categories><comments>10 pages, 9 figures, 2 tables</comments><msc-class>68T20, 90C27, 90C39, 90C59, 90C90, 90-02, 93B50, 93B51, 93-2, 93A13</msc-class><acm-class>G.1.6; G.2.1; G.2.3; G.4; H.1.1; I.2.8; J.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The survey methodological paper addresses a glance to a general decision
support platform technology for modular systems (modular/composite
alterantives/solutions) in various applied domains. The decision support
platform consists of seven basic combinatorial engineering frameworks (system
synthesis, system modeling, evaluation, detection of bottleneck,
improvement/extension, multistage design, combinatorial evolution and
forecasting). The decision support platform is based on decision support
procedures (e.g., multicriteria selection/sorting, clustering), combinatorial
optimization problems (e.g., knapsack, multiple choice problem, clique,
assignment/allocation, covering, spanning trees), and their combinations. The
following is described: (1) general scheme of the decision support platform
technology; (2) brief descriptions of modular (composite) systems (or composite
alternatives); (3) trends in moving from chocie/selection of alternatives to
processing of composite alternatives which correspond to hierarchical modular
products/systems; (4) scheme of resource requirements (i.e., human,
information-computer); and (5) basic combinatorial engineering frameworks and
their applications in various domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5507</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5507</id><created>2014-08-23</created><authors><author><keyname>Smarandache</keyname><forenames>Florentin</forenames></author><author><keyname>Ali</keyname><forenames>Mumtaz</forenames></author><author><keyname>Shabir</keyname><forenames>Muhammad</forenames></author></authors><title>Soft Neutrosophic Algebraic Structures and Their Generalization</title><categories>cs.AI</categories><comments>264 pages</comments><acm-class>I.2.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Study of soft sets was first proposed by Molodtsov in 1999 to deal with
uncertainty in a non-parametric manner. The researchers did not pay attention
to soft set theory at that time but now the soft set theory has been developed
in many areas of mathematics. Algebraic structures using soft set theory are
very rapidly developed. In this book we developed soft neutrosophic algebraic
structures by using soft sets and neutrosophic algebraic structures. In this
book we study soft neutrosophic groups, soft neutrosophic semigroups, soft
neutrosophic loops, soft neutrosophic LA-semigroups, and their generalizations
respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5511</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5511</id><created>2014-08-23</created><authors><author><keyname>Lahlouhi</keyname><forenames>Ammar</forenames></author></authors><title>Validation of the development methodologies</title><categories>cs.SE</categories><comments>International Journal of Computational Science, Information
  Technology and Control Engineering (IJCSITCE) Vol.1, No.2, July 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper argues that modelling the development methodologies can improve
the multi-agents systems software engineering. Such modelling allows applying
methods, techniques and practices used in the software development to the
methodologies themselves. The paper discusses then the advantages of the
modelling of development methodologies. It describes a model of development
methodologies, uses such a model to develop a system of their partial
validation, and applies such a system to multi-agent methodologies. Several
benefits can be gained from such modelling, such as the improvement of the
works on the development, evaluation and comparison of multi-agent development
methodologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5512</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5512</id><created>2014-08-23</created><authors><author><keyname>Chen</keyname><forenames>Shaoshi</forenames></author><author><keyname>Kauers</keyname><forenames>Manuel</forenames></author><author><keyname>Singer</keyname><forenames>Michael F.</forenames></author></authors><title>Desingularization of Ore Operators</title><categories>cs.SC math.AC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that Ore operators can be desingularized by calculating a least
common left multiple with a random operator of appropriate order. Our result
generalizes a classical result about apparent singularities of linear
differential equations, and it gives rise to a surprisingly simple
desingularization algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5514</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5514</id><created>2014-08-23</created><authors><author><keyname>Kauers</keyname><forenames>Manuel</forenames></author></authors><title>Bounds for D-finite closure properties</title><categories>cs.SC math.CO</categories><acm-class>I.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide bounds on the size of operators obtained by algorithms for
executing D-finite closure properties. For operators of small order, we give
bounds on the degree and on the height (bit-size). For higher order operators,
we give degree bounds that are parameterized with respect to the order and
reflect the phenomenon that higher order operators may have lower degrees
(order-degree curves).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5516</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5516</id><created>2014-08-23</created><authors><author><keyname>Fidler</keyname><forenames>Sanja</forenames></author><author><keyname>Boben</keyname><forenames>Marko</forenames></author><author><keyname>Leonardis</keyname><forenames>Ales</forenames></author></authors><title>Learning a Hierarchical Compositional Shape Vocabulary for Multi-class
  Object Representation</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hierarchies allow feature sharing between objects at multiple levels of
representation, can code exponential variability in a very compact way and
enable fast inference. This makes them potentially suitable for learning and
recognizing a higher number of object classes. However, the success of the
hierarchical approaches so far has been hindered by the use of hand-crafted
features or predetermined grouping rules. This paper presents a novel framework
for learning a hierarchical compositional shape vocabulary for representing
multiple object classes. The approach takes simple contour fragments and learns
their frequent spatial configurations. These are recursively combined into
increasingly more complex and class-specific shape compositions, each exerting
a high degree of shape variability. At the top-level of the vocabulary, the
compositions are sufficiently large and complex to represent the whole shapes
of the objects. We learn the vocabulary layer after layer, by gradually
increasing the size of the window of analysis and reducing the spatial
resolution at which the shape configurations are learned. The lower layers are
learned jointly on images of all classes, whereas the higher layers of the
vocabulary are learned incrementally, by presenting the algorithm with one
object class after another. The experimental results show that the learned
multi-class object representation scales favorably with the number of object
classes and achieves a state-of-the-art detection performance at both, faster
inference as well as shorter training times.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5518</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5518</id><created>2014-08-23</created><updated>2014-09-14</updated><authors><author><keyname>Belazzougui</keyname><forenames>Djamal</forenames></author></authors><title>Faster construction of asymptotically good unit-cost error correcting
  codes in the RAM model</title><categories>cs.DS cs.IT math.IT</categories><comments>Manuscript (5 pages)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Assuming we are in a Word-RAM model with word size $w$, we show that we can
construct in $o(w)$ time an error correcting code with a constant relative
positive distance that maps numbers of $w$ bits into $\Theta(w)$-bit numbers,
and such that the application of the error-correcting code on any given number
$x\in[0,2^w-1]$ takes constant time. Our result improves on a previously
proposed error-correcting code with the same properties whose construction time
was exponential in $w$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5530</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5530</id><created>2014-08-23</created><authors><author><keyname>He</keyname><forenames>Dan</forenames></author><author><keyname>Wang</keyname><forenames>Zhanyong</forenames></author><author><keyname>Parida</keyname><forenames>Laxmi</forenames></author><author><keyname>Eskin</keyname><forenames>Eleazar</forenames></author></authors><title>IPED2: Inheritance Path based Pedigree Reconstruction Algorithm for
  Complicated Pedigrees</title><categories>cs.DS q-bio.PE</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reconstruction of family trees, or pedigree reconstruction, for a group of
individuals is a fundamental problem in genetics. The problem is known to be
NP-hard even for datasets known to only contain siblings. Some recent methods
have been developed to accurately and efficiently reconstruct pedigrees. These
methods, however, still consider relatively simple pedigrees, for example, they
are not able to handle half-sibling situations where a pair of individuals only
share one parent. In this work, we propose an efficient method, IPED2, based on
our previous work, which specifically targets reconstruction of complicated
pedigrees that include half-siblings. We note that the presence of
half-siblings makes the reconstruction problem significantly more challenging
which is why previous methods exclude the possibility of half-siblings. We
proposed a novel model as well as an efficient graph algorithm and experiments
show that our algorithm achieves relatively accurate reconstruction. To our
knowledge, this is the first method that is able to handle pedigree
reconstruction based on genotype data only when half-sibling exists in any
generation of the pedigree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5535</identifier>
 <datestamp>2015-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5535</id><created>2014-08-23</created><updated>2015-05-13</updated><authors><author><keyname>Wu</keyname><forenames>Lingfei</forenames></author><author><keyname>Stathopoulos</keyname><forenames>Andreas</forenames></author></authors><title>A Preconditioned Hybrid SVD Method for Computing Accurately Singular
  Triplets of Large Matrices</title><categories>cs.NA cs.MS</categories><comments>24 pages, 20 figures, and 8 tables. Accepted to SIAM Journal on
  Scientific Computing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The computation of a few singular triplets of large, sparse matrices is a
challenging task, especially when the smallest magnitude singular values are
needed in high accuracy. Most recent efforts try to address this problem
through variations of the Lanczos bidiagonalization method, but they are still
challenged even for medium matrix sizes due to the difficulty of the problem.
We propose a novel SVD approach that can take advantage of preconditioning and
of any well designed eigensolver to compute both largest and smallest singular
triplets. Accuracy and efficiency is achieved through a hybrid, two-stage
meta-method, PHSVDS. In the first stage, PHSVDS solves the normal equations up
to the best achievable accuracy. If further accuracy is required, the method
switches automatically to an eigenvalue problem with the augmented matrix. Thus
it combines the advantages of the two stages, faster convergence and accuracy,
respectively. For the augmented matrix, solving the interior eigenvalue is
facilitated by a proper use of the good initial guesses from the first stage
and an efficient implementation of the refined projection method. We also
discuss how to precondition PHSVDS and to cope with some issues that arise.
Numerical experiments illustrate the efficiency and robustness of the method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5539</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5539</id><created>2014-08-23</created><authors><author><keyname>Kuzu</keyname><forenames>Mehmet</forenames></author><author><keyname>Islam</keyname><forenames>Mohammad Saiful</forenames></author><author><keyname>Kantarcioglu</keyname><forenames>Murat</forenames></author></authors><title>A Distributed Framework for Scalable Search over Encrypted Documents</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, huge amount of documents are increasingly transferred to the remote
servers due to the appealing features of cloud computing. On the other hand,
privacy and security of the sensitive information in untrusted cloud
environment is a big concern. To alleviate such concerns, encryption of
sensitive data before its transfer to the cloud has become an important risk
mitigation option. Encrypted storage provides protection at the expense of a
significant increase in the data management complexity. For effective
management, it is critical to provide efficient selective document retrieval
capability on the encrypted collection. In fact, considerable amount of
searchable symmetric encryption schemes have been designed in the literature to
achieve this task. However, with the emergence of big data everywhere,
available approaches are insufficient to address some crucial real-world
problems such as scalability.
  In this study, we focus on practical aspects of a secure keyword search
mechanism over encrypted data on a real cloud infrastructure. First, we propose
a provably secure distributed index along with a parallelizable retrieval
technique that can easily scale to big data. Second, we integrate authorization
into the search scheme to limit the information leakage in multi-user setting
where users are allowed to access only particular documents. Third, we offer
efficient updates on the distributed secure index. In addition, we conduct
extensive empirical analysis on a real dataset to illustrate the efficiency of
the proposed practical techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5543</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5543</id><created>2014-08-23</created><authors><author><keyname>Cheng</keyname><forenames>Tao</forenames></author></authors><title>Restricted Conformal Property of Compressive Sensing</title><categories>cs.IT math.IT</categories><comments>11 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy and direction are tow basic properties of a vector. A discrete signal
is a vector in nature. RIP of compressive sensing can not show the direction
information of a signal but show the energy information of a signal. Hence, RIP
is not complete. Orthogonal matrices can preserve angles and lengths.
Preservation of length can show energies of signals like RIP do; and
preservation of angle can show directions of signals. Therefore, Restricted
Conformal Property (RCP) is proposed according to preservation of angle. RCP
can show the direction of a signal just as RIP shows the energy of a signal.
RCP is important supplement and development of RIP. Tow different proofs of RCP
are given, namely, RCP_JL and RCP_IP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5544</identifier>
 <datestamp>2014-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5544</id><created>2014-08-23</created><updated>2014-08-27</updated><authors><author><keyname>Pimentel-Alarc&#xf3;n</keyname><forenames>Daniel L.</forenames></author></authors><title>To lie or not to lie in a subspace</title><categories>stat.ML cs.LG</categories><comments>First author mistakenly listed advisors as co-authors in his research
  proposal. This is corrected in the current version. 59 pages, 19 figures.
  Subspace clustering, missing data, converse of matrix completion</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Give deterministic necessary and sufficient conditions to guarantee that if a
subspace fits certain partially observed data from a union of subspaces, it is
because such data really lies in a subspace.
  Furthermore, Give deterministic necessary and sufficient conditions to
guarantee that if a subspace fits certain partially observed data, such
subspace is unique.
  Do this by characterizing when and only when a set of incomplete vectors
behaves as a single but complete one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5552</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5552</id><created>2014-08-24</created><authors><author><keyname>Lee</keyname><forenames>Jaejun</forenames></author><author><keyname>Yun</keyname><forenames>Taeseon</forenames></author></authors><title>Fuzzy and entropy facial recognition</title><categories>cs.CV</categories><comments>5 pages</comments><msc-class>68T10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper suggests an effective method for facial recognition using fuzzy
theory and Shannon entropy. Combination of fuzzy theory and Shannon entropy
eliminates the complication of other methods. Shannon entropy calculates the
ratio of an element between faces, and fuzzy theory calculates the member ship
of the entropy with 1. More details will be mentioned in Section 3. The
learning performance is better than others as it is very simple, and only need
two data per learning. By using factors that don't usually change during the
life, the method will have a high accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5558</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5558</id><created>2014-08-24</created><authors><author><keyname>You</keyname><forenames>Zhi-Qiang</forenames></author><author><keyname>Han</keyname><forenames>Xiao-Pu</forenames></author><author><keyname>L&#xfc;</keyname><forenames>Linyuan</forenames></author><author><keyname>Yeung</keyname><forenames>Chi Ho</forenames></author></authors><title>Empirical studies on the network of social groups: the case of Tencent
  QQ</title><categories>physics.soc-ph cs.SI</categories><comments>18 pages, 9 figures</comments><doi>10.1371/journal.pone.0130538</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Participation in social groups are important but the collective behaviors of
human as a group are difficult to analyze due to the difficulties to quantify
ordinary social relation, group membership, and to collect a comprehensive
dataset. Such difficulties can be circumvented by analyzing online social
networks. In this paper, we analyze a comprehensive dataset obtained from
Tencent QQ, an instant messenger with the highest market share in China.
Specifically, we analyze three derivative networks involving groups and their
members -- the hypergraph of groups, the network of groups and the user network
-- to reveal social interactions at microscopic and mesoscopic level. Our
results uncover interesting behaviors on the growth of user groups, the
interactions between groups, and their relationship with member age and gender.
These findings lead to insights which are difficult to obtain in ordinary
social networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5560</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5560</id><created>2014-08-24</created><authors><author><keyname>Peron</keyname><forenames>Adriano</forenames><affiliation>Universit&#xe0; di Napoli Federico II</affiliation></author><author><keyname>Piazza</keyname><forenames>Carla</forenames><affiliation>Universit&#xe0; degli Studi di Udine</affiliation></author></authors><title>Proceedings Fifth International Symposium on Games, Automata, Logics and
  Formal Verification</title><categories>cs.GT cs.FL cs.LO</categories><proxy>EPTCS</proxy><acm-class>F.4.1; F.4.3; F.1.1; D.2.4</acm-class><journal-ref>EPTCS 161, 2014</journal-ref><doi>10.4204/EPTCS.161</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains the proceedings of the Fifth International Symposium on
Games, Automata, Logic and Formal Verification (GandALF 2014). The symposium
took place in Verona, Italy, from 10th to 12th of September 2014. The
proceedings of the symposium contain the abstracts of three invited talks and
19 papers that were accepted after a careful evaluation for presentation at the
conference. The topics of the accepted papers range over a wide spectrum,
including algorithmic and behavioral game theory, game semantics, formal
languages and automata theory, modal and temporal logics, software
verification, hybrid systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5564</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5564</id><created>2014-08-24</created><authors><author><keyname>Carbone</keyname><forenames>Marco</forenames><affiliation>IT University of Copenhagen</affiliation></author></authors><title>Proceedings Third Workshop on Behavioural Types</title><categories>cs.PL cs.LO</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 162, 2014</journal-ref><doi>10.4204/EPTCS.162</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains the proceedings of BEAT 2014, the third Workshop on
Behavioural Types. The workshop took place in Rome, Italy, on September 1st
2014, as a satellite even of CONCUR 2014, the 25th International Conference on
Concurrency Theory.
  The aim of this workshop is to bring together researchers in all aspects of
behavioural type theory and its applications, in order to share results,
consolidate the community, and discover opportunities for new collaborations
and future directions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5566</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5566</id><created>2014-08-24</created><authors><author><keyname>Chen</keyname><forenames>Jian</forenames></author><author><keyname>Chen</keyname><forenames>Xiaoming</forenames></author><author><keyname>Liu</keyname><forenames>Tao</forenames></author><author><keyname>Lei</keyname><forenames>Lei</forenames></author></authors><title>Energy-Efficient Power Allocation for Secure Communications in
  Large-Scale MIMO Relaying Systems</title><categories>cs.IT math.IT</categories><comments>6 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we address the problem of energy-efficient power allocation
for secure communications in an amplify-and-forward (AF) large-scale
multiple-input multiple-output (LS-MIMO) relaying system in presence of a
passive eavesdropper. The benefits of an AF LS-MIMO relay are exploited to
significantly improve the secrecy performance, especially the secrecy energy
efficiency (bit per Joule). We first analyze the impact of transmit power at
the relay on the secrecy outage capacity, and prove that the secrecy outage
capacity is a concave function of transmit power under very practical
assumptions, i.e. no eavesdropper channel state information (CSI) and imperfect
legitimate CSI. Then, we propose an energy-efficient power allocation scheme to
maximize the secrecy energy efficiency. Finally, simulation results validate
the advantage of the proposed energy-efficient scheme compared to the capacity
maximization scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5571</identifier>
 <datestamp>2014-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5571</id><created>2014-08-24</created><updated>2014-08-30</updated><authors><author><keyname>Fire</keyname><forenames>Michael</forenames></author><author><keyname>Chesney</keyname><forenames>Thomas</forenames></author><author><keyname>Elovici</keyname><forenames>Yuval</forenames></author></authors><title>Quantitative Analysis of Genealogy Using Digitised Family Trees</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Driven by the popularity of television shows such as Who Do You Think You
Are? many millions of users have uploaded their family tree to web projects
such as WikiTree. Analysis of this corpus enables us to investigate genealogy
computationally. The study of heritage in the social sciences has led to an
increased understanding of ancestry and descent but such efforts are hampered
by difficult to access data. Genealogical research is typically a tedious
process involving trawling through sources such as birth and death
certificates, wills, letters and land deeds. Decades of research have developed
and examined hypotheses on population sex ratios, marriage trends, fertility,
lifespan, and the frequency of twins and triplets. These can now be tested on
vast datasets containing many billions of entries using machine learning tools.
Here we survey the use of genealogy data mining using family trees dating back
centuries and featuring profiles on nearly 7 million individuals based in over
160 countries. These data are not typically created by trained genealogists and
so we verify them with reference to third party censuses. We present results on
a range of aspects of population dynamics. Our approach extends the boundaries
of genealogy inquiry to precise measurement of underlying human phenomena.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5573</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5573</id><created>2014-08-24</created><authors><author><keyname>Garcia-Constantino</keyname><forenames>Matias</forenames></author><author><keyname>Missier</keyname><forenames>Paolo</forenames></author><author><keyname>Guo</keyname><forenames>Phil Blytheand Amy Weihong</forenames></author></authors><title>Measuring the impact of cognitive distractions on driving performance
  using time series analysis</title><categories>cs.HC</categories><comments>IEEE ITS conference, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using current sensing technology, a wealth of data on driving sessions is
potentially available through a combination of vehicle sensors and drivers'
physiology sensors (heart rate, breathing rate, skin temperature, etc.). Our
hypothesis is that it should be possible to exploit the combination of time
series produced by such multiple sensors during a driving session, in order to
(i) learn models of normal driving behaviour, and (ii) use such models to
detect important and potentially dangerous deviations from the norm in
real-time, and thus enable the generation of appropriate alerts. Crucially, we
believe that such models and interventions should and can be personalised and
tailor-made for each individual driver. As an initial step towards this goal,
in this paper we present techniques for assessing the impact of cognitive
distraction on drivers, based on simple time series analysis. We have tested
our method on a rich dataset of driving sessions, carried out in a professional
simulator, involving a panel of volunteer drivers. Each session included a
different type of cognitive distraction, and resulted in multiple time series
from a variety of on-board sensors as well as sensors worn by the driver.
Crucially, each driver also recorded an initial session with no distractions.
In our model, such initial session provides the baseline times series that make
it possible to quantitatively assess driver performance under distraction
conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5574</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5574</id><created>2014-08-24</created><updated>2015-02-08</updated><authors><author><keyname>Lin</keyname><forenames>Guosheng</forenames></author><author><keyname>Shen</keyname><forenames>Chunhua</forenames></author><author><keyname>Hengel</keyname><forenames>Anton van den</forenames></author></authors><title>Supervised Hashing Using Graph Cuts and Boosted Decision Trees</title><categories>cs.LG cs.CV</categories><comments>15 pages. Appearing in IEEE T. Pattern Analysis &amp; Machine
  Intelligence. arXiv admin note: text overlap with arXiv:1404.1561</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Embedding image features into a binary Hamming space can improve both the
speed and accuracy of large-scale query-by-example image retrieval systems.
Supervised hashing aims to map the original features to compact binary codes in
a manner which preserves the label-based similarities of the original data.
Most existing approaches apply a single form of hash function, and an
optimization process which is typically deeply coupled to this specific form.
This tight coupling restricts the flexibility of those methods, and can result
in complex optimization problems that are difficult to solve. In this work we
proffer a flexible yet simple framework that is able to accommodate different
types of loss functions and hash functions. The proposed framework allows a
number of existing approaches to hashing to be placed in context, and
simplifies the development of new problem-specific hashing methods. Our
framework decomposes the into two steps: binary code (hash bits) learning, and
hash function learning. The first step can typically be formulated as a binary
quadratic problem, and the second step can be accomplished by training standard
binary classifiers. For solving large-scale binary code inference, we show how
to ensure that the binary quadratic problems are submodular such that an
efficient graph cut approach can be used. To achieve efficiency as well as
efficacy on large-scale high-dimensional data, we propose to use boosted
decision trees as the hash functions, which are nonlinear, highly descriptive,
and very fast to train and evaluate. Experiments demonstrate that our proposed
method significantly outperforms most state-of-the-art methods, especially on
high-dimensional data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5583</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5583</id><created>2014-08-24</created><authors><author><keyname>Zhou</keyname><forenames>Zhenyu</forenames></author><author><keyname>Zhou</keyname><forenames>Sheng</forenames></author><author><keyname>Gong</keyname><forenames>Jie</forenames></author><author><keyname>Niu</keyname><forenames>Zhisheng</forenames></author></authors><title>Energy-Efficient Antenna Selection and Power Allocation for Large-Scale
  Multiple Antenna Systems with Hybrid Energy Supply</title><categories>cs.IT math.IT</categories><comments>IEEE Globecom 2014 Selected Areas in Communications Symposium-Green
  Communications and Computing Track</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The combination of energy harvesting and large-scale multiple antenna
technologies provides a promising solution for improving the energy efficiency
(EE) by exploiting renewable energy sources and reducing the transmission power
per user and per antenna. However, the introduction of energy harvesting
capabilities into large-scale multiple antenna systems poses many new
challenges for energy-efficient system design due to the intermittent
characteristics of renewable energy sources and limited battery capacity.
Furthermore, the total manufacture cost and the sum power of a large number of
radio frequency (RF) chains can not be ignored, and it would be impractical to
use all the antennas for transmission. In this paper, we propose an
energy-efficient antenna selection and power allocation algorithm to maximize
the EE subject to the constraint of user's quality of service (QoS). An
iterative offline optimization algorithm is proposed to solve the non-convex EE
optimization problem by exploiting the properties of nonlinear fractional
programming. The relationships among maximum EE, selected antenna number,
battery capacity, and EE-SE tradeoff are analyzed and verified through computer
simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5592</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5592</id><created>2014-08-24</created><authors><author><keyname>Razavi</keyname><forenames>Seyed Basir Shariat</forenames></author><author><keyname>Tabrizi</keyname><forenames>Narjes Sadat Movahedi</forenames></author><author><keyname>Chitsaz</keyname><forenames>Hamidreza</forenames></author><author><keyname>Boucher</keyname><forenames>Christina</forenames></author></authors><title>HyDA-Vista: Towards Optimal Guided Selection of k-mer Size for Sequence
  Assembly</title><categories>cs.CE</categories><comments>11 pages, 1 figure, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivation: Intimately tied to assembly quality is the complexity of the de
Bruijn graph built by the assembler. Thus, there have been many paradigms
developed to decrease the complexity of the de Bruijn graph. One obvious
combinatorial paradigm for this is to allow the value of $k$ to vary; having a
larger value of $k$ where the graph is more complex and a smaller value of $k$
where the graph would likely contain fewer spurious edges and vertices. One
open problem that affects the practicality of this method is how to predict the
value of $k$ prior to building the de Bruijn graph. We show that optimal values
of $k$ can be predicted prior to assembly by using the information contained in
a phylogenetically-close genome and therefore, help make the use of multiple
values of $k$ practical for genome assembly.
  Results: We present HyDA-Vista, which is a genome assembler that uses
homology information to choose a value of $k$ for each read prior to the de
Bruijn graph construction. The chosen $k$ is optimal if there are no sequencing
errors and the coverage is sufficient. Fundamental to our method is the
construction of the {\em maximal sequence landscape}, which is a data structure
that stores for each position in the input string, the largest repeated
substring containing that position. In particular, we show the maximal sequence
landscape can be constructed in $O(n + n \log n)$-time and $O(n)$-space.
HyDA-Vista first constructs the maximal sequence landscape for a homologous
genome. The reads are then aligned to this reference genome, and values of $k$
are assigned to each read using the maximal sequence landscape and the
alignments. Eventually, all the reads are assembled by an iterative de Bruijn
graph construction method. Our results and comparison to other assemblers
demonstrate that HyDA-Vista achieves the best assembly of {\em E. coli} before
repeat resolution or scaffolding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5601</identifier>
 <datestamp>2014-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5601</id><created>2014-08-24</created><updated>2014-08-25</updated><authors><author><keyname>Yang</keyname><forenames>Jianwei</forenames></author><author><keyname>Lei</keyname><forenames>Zhen</forenames></author><author><keyname>Li</keyname><forenames>Stan Z.</forenames></author></authors><title>Learn Convolutional Neural Network for Face Anti-Spoofing</title><categories>cs.CV</categories><comments>8 pages, 9 figures, 7 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Though having achieved some progresses, the hand-crafted texture features,
e.g., LBP [23], LBP-TOP [11] are still unable to capture the most
discriminative cues between genuine and fake faces. In this paper, instead of
designing feature by ourselves, we rely on the deep convolutional neural
network (CNN) to learn features of high discriminative ability in a supervised
manner. Combined with some data pre-processing, the face anti-spoofing
performance improves drastically. In the experiments, over 70% relative
decrease of Half Total Error Rate (HTER) is achieved on two challenging
datasets, CASIA [36] and REPLAY-ATTACK [7] compared with the state-of-the-art.
Meanwhile, the experimental results from inter-tests between two datasets
indicates CNN can obtain features with better generalization ability. Moreover,
the nets trained using combined data from two datasets have less biases between
two datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5622</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5622</id><created>2014-08-24</created><authors><author><keyname>Parigi</keyname><forenames>Giacomo</forenames></author><author><keyname>Piastra</keyname><forenames>Marco</forenames></author></authors><title>Gradient of the Objective Function for an Anisotropic Centroidal Voronoi
  Tessellation (CVT) - A revised, detailed derivation</title><categories>cs.CG</categories><comments>14 pages</comments><msc-class>68U05 (Primary), 65D18 (Secondary)</msc-class><acm-class>I.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In their recent article (2010), Levy and Liu introduced a generalization of
Centroidal Voronoi Tessellation (CVT) - namely the Lp-CVT - that allows the
computation of an anisotropic CVT over a sound mathematical framework. In this
article a new objective function is defined, and both this function and its
gradient are derived in closed-form for surfaces and volumes. This method opens
a wide range of possibilities, also described in the paper, such as
quad-dominant surface remeshing, hex-dominant volume meshing or fully-automated
capturing of sharp features. However, in the same paper, the derivations of the
gradient and of the new objective function are only partially expanded in the
appendices, and some relevant requisites on the anisotropy field are left
implicit. In order to better harness the possibilities described there, in this
work the entire derivation process is made explicit. In the authors' opinion,
this also helps understanding the working conditions of the method and its
possible applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5634</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5634</id><created>2014-08-24</created><authors><author><keyname>Bowman</keyname><forenames>R. Sean</forenames></author><author><keyname>Heisterkamp</keyname><forenames>Douglas</forenames></author><author><keyname>Johnson</keyname><forenames>Jesse</forenames></author><author><keyname>O'Donnol</keyname><forenames>Danielle</forenames></author></authors><title>An application of topological graph clustering to protein function
  prediction</title><categories>cs.CE cs.LG q-bio.QM stat.ML</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We use a semisupervised learning algorithm based on a topological data
analysis approach to assign functional categories to yeast proteins using
similarity graphs. This new approach to analyzing biological networks yields
results that are as good as or better than state of the art existing
approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5636</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5636</id><created>2014-08-24</created><updated>2014-09-08</updated><authors><author><keyname>Cai</keyname><forenames>Mingzhong</forenames><affiliation>University of Wisconson, Madison</affiliation></author><author><keyname>Downey</keyname><forenames>Rodney G</forenames><affiliation>Victoria University Wellington</affiliation></author><author><keyname>Epstein</keyname><forenames>Rachel</forenames><affiliation>Swathmore College</affiliation></author><author><keyname>Lempp</keyname><forenames>Steffen</forenames><affiliation>University of Wisconson, Madison</affiliation></author><author><keyname>Miller</keyname><forenames>Joseph</forenames><affiliation>University of Wisconson, Madison</affiliation></author></authors><title>Random strings and tt-degrees of Turing complete C.E. sets</title><categories>cs.LO</categories><comments>25 pages</comments><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 10, Issue 3 (September
  10, 2014) lmcs:1126</journal-ref><doi>10.2168/LMCS-10(3:15)2014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the truth-table degrees of (co-)c.e.\ sets, in particular,
sets of random strings. It is known that the set of random strings with respect
to any universal prefix-free machine is Turing complete, but that truth-table
completeness depends on the choice of universal machine. We show that for such
sets of random strings, any finite set of their truth-table degrees do not meet
to the degree~0, even within the c.e. truth-table degrees, but when taking the
meet over all such truth-table degrees, the infinite meet is indeed~0. The
latter result proves a conjecture of Allender, Friedman and Gasarch. We also
show that there are two Turing complete c.e. sets whose truth-table degrees
form a minimal pair.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5661</identifier>
 <datestamp>2015-04-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5661</id><created>2014-08-25</created><updated>2015-04-17</updated><authors><author><keyname>Yamazaki</keyname><forenames>Keisuke</forenames></author></authors><title>Asymptotic Accuracy of Bayesian Estimation for a Single Latent Variable</title><categories>stat.ML cs.LG</categories><comments>28 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In data science and machine learning, hierarchical parametric models, such as
mixture models, are often used. They contain two kinds of variables: observable
variables, which represent the parts of the data that can be directly measured,
and latent variables, which represent the underlying processes that generate
the data. Although there has been an increase in research on the estimation
accuracy for observable variables, the theoretical analysis of estimating
latent variables has not been thoroughly investigated. In a previous study, we
determined the accuracy of a Bayes estimation for the joint probability of the
latent variables in a dataset, and we proved that the Bayes method is
asymptotically more accurate than the maximum-likelihood method. However, the
accuracy of the Bayes estimation for a single latent variable remains unknown.
In the present paper, we derive the asymptotic expansions of the error
functions, which are defined by the Kullback-Leibler divergence, for two types
of single-variable estimations when the statistical regularity is satisfied.
Our results indicate that the accuracies of the Bayes and maximum-likelihood
methods are asymptotically equivalent and clarify that the Bayes method is only
advantageous for multivariable estimations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5663</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5663</id><created>2014-08-25</created><authors><author><keyname>Matsuzono</keyname><forenames>Kazuhisa</forenames></author><author><keyname>Roca</keyname><forenames>Vincent</forenames></author><author><keyname>Asaeda</keyname><forenames>Hitoshi</forenames></author></authors><title>Structured Random Linear Codes (SRLC): Bridging the Gap between Block
  and Convolutional Codes</title><categories>cs.IT math.IT</categories><comments>7 pages, 12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several types of AL-FEC (Application-Level FEC) codes for the Packet Erasure
Channel exist. Random Linear Codes (RLC), where redundancy packets consist of
random linear combinations of source packets over a certain finite field, are a
simple yet efficient coding technique, for instance massively used for Network
Coding applications. However the price to pay is a high encoding and decoding
complexity, especially when working on $GF(2^8)$, which seriously limits the
number of packets in the encoding window. On the opposite, structured block
codes have been designed for situations where the set of source packets is
known in advance, for instance with file transfer applications. Here the
encoding and decoding complexity is controlled, even for huge block sizes,
thanks to the sparse nature of the code and advanced decoding techniques that
exploit this sparseness (e.g., Structured Gaussian Elimination). But their
design also prevents their use in convolutional use-cases featuring an encoding
window that slides over a continuous set of incoming packets.
  In this work we try to bridge the gap between these two code classes,
bringing some structure to RLC codes in order to enlarge the use-cases where
they can be efficiently used: in convolutional mode (as any RLC code), but also
in block mode with either tiny, medium or large block sizes. We also
demonstrate how to design compact signaling for these codes (for
encoder/decoder synchronization), which is an essential practical aspect.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5666</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5666</id><created>2014-08-25</created><authors><author><keyname>Kang</keyname><forenames>Wei</forenames></author><author><keyname>Liu</keyname><forenames>Nan</forenames></author></authors><title>Compressing Encrypted Data and Permutation Cipher</title><categories>cs.IT cs.CR math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a system that performs both encryption and lossy compression, the
conventional way is to compress first and then encrypt the compressed data.
This separation approach proves to be optimal. In certain applications where
sensitive information should be protected as early as possible, it is
preferable to perform encryption first and then compress the encrypted data,
which leads to the concept of the reversed system. Johnson et al. proposed an
achievability scheme for the reversed system that has a modulo-sum encryption
followed by a compression using Wyner-Ziv distributed source coding with side
information. However, this reversed system performs worse than the conventional
system in the sense that it requires more compression rate and secrecy key
rate. In this paper, we propose a new achievability scheme for the reverse
system where encryption is conducted by a permutation cipher and then the
encrypted data is compressed using the optimal rate-distortion code. The
proposed scheme can achieve the optimal compression rate and secret key rate,
and therefore shows that reversing the order of encryption and compression does
not necessarily compromise the performance of an encryption-compression system.
The proposed system attains weak secrecy, and we show that the information
leakage is mainly contributed by the type information of the sequence, which is
not concealed by the permutation cipher. Given the type of the sequence, the
rest of the information leakage vanishes exponentially.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5667</identifier>
 <datestamp>2015-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5667</id><created>2014-08-25</created><updated>2015-02-11</updated><authors><author><keyname>Zonoobi</keyname><forenames>Dornoosh</forenames></author><author><keyname>Roohi</keyname><forenames>Shahrooz Faghih</forenames></author><author><keyname>Kassim</keyname><forenames>Ashraf A.</forenames></author></authors><title>Dependent Nonparametric Bayesian Group Dictionary Learning for online
  reconstruction of Dynamic MR images</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce a dictionary learning based approach applied to
the problem of real-time reconstruction of MR image sequences that are highly
undersampled in k-space. Unlike traditional dictionary learning, our method
integrates both global and patch-wise (local) sparsity information and
incorporates some priori information into the reconstruction process. Moreover,
we use a Dependent Hierarchical Beta-process as the prior for the group-based
dictionary learning, which adaptively infers the dictionary size and the
sparsity of each patch; and also ensures that similar patches are manifested in
terms of similar dictionary atoms. An efficient numerical algorithm based on
the alternating direction method of multipliers (ADMM) is also presented.
Through extensive experimental results we show that our proposed method
achieves superior reconstruction quality, compared to the other state-of-the-
art DL-based methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5674</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5674</id><created>2014-08-25</created><authors><author><keyname>Li</keyname><forenames>Yong</forenames></author><author><keyname>Chen</keyname><forenames>Qianbin</forenames></author><author><keyname>Liu</keyname><forenames>Hongqing</forenames></author><author><keyname>Truong</keyname><forenames>Trieu-Kien</forenames></author></authors><title>Performance and analysis of Quadratic Residue Codes of lengths less than
  100</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the performance of quadratic residue (QR) codes of lengths
within 100 is given and analyzed when the hard decoding, soft decoding, and
linear programming decoding algorithms are utilized. We develop a simple method
to estimate the soft decoding performance, which avoids extensive simulations.
Also, a simulation-based algorithm is proposed to obtain the maximum likelihood
decoding performance of QR codes of lengths within 100. Moreover, four
important theorems are proposed to predict the performance of the hard decoding
and the maximum-likelihood decoding in which they can explore some internal
properties of QR codes. It is shown that such four theorems can be applied to
the QR codes with lengths less than 100 for predicting the decoding
performance. In contrast, they can be straightforwardly generalized to longer
QR codes. The result is never seen in the literature, to our knowledge.
Simulation results show that the estimated hard decoding performance is very
accurate in the whole signal-to-noise ratio (SNR) regimes, whereas the derived
upper bounds of the maximum likelihood decoding are only tight for moderate to
high SNR regions. For each of the considered QR codes, the soft decoding is
approximately 1.5 dB better than the hard decoding. By using powerful redundant
parity-check cuts, the linear programming-based decoding algorithm, i.e., the
ACG-ALP decoding algorithm performs very well for any QR code. Sometimes, it is
even superior to the Chase-based soft decoding algorithm significantly, and
hence is only a few tenths of dB away from the maximum likelihood decoding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5681</identifier>
 <datestamp>2015-08-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5681</id><created>2014-08-25</created><updated>2015-08-15</updated><authors><author><keyname>Bazzi</keyname><forenames>Louay</forenames></author></authors><title>Weight distribution of cosets of small codes with good dual properties</title><categories>cs.IT math.IT</categories><msc-class>94A24</msc-class><acm-class>E.4; H.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The {\em bilateral minimum distance} of a binary linear code is the maximum
$d$ such that all nonzero codewords have weights between $d$ and $n-d$. Let
$Q\subset \{0,1\}^n$ be a binary linear code whose dual has bilateral minimum
distance at least $d$, where $d$ is odd. Roughly speaking, we show that the
average $L_\infty$-distance -- and consequently the $L_1$-distance -- between
the weight distribution of a random cosets of $Q$ and the binomial distribution
decays quickly as the bilateral minimum distance $d$ of the dual of $Q$
increases. For $d = \Theta(1)$, it decays like $n^{-\Theta(d)}$. On the other
$d=\Theta(n)$ extreme, it decays like and $e^{-\Theta(d)}$. It follows that,
almost all cosets of $Q$ have weight distributions very close to the to the
binomial distribution. In particular, we establish the following bounds. If the
dual of $Q$ has bilateral minimum distance at least $d=2t+1$, where $t\geq 1$
is an integer, then the average $L_\infty$-distance is at most
$\min\{\left(e\ln{\frac{n}{2t}}\right)^{t}\left(\frac{2t}{n}\right)^{\frac{t}{2}
}, \sqrt{2} e^{-\frac{t}{10}}\}$. For the average $L_1$-distance, we conclude
the bound $\min\{(2t+1)\left(e\ln{\frac{n}{2t}}\right)^{t}
\left(\frac{2t}{n}\right)^{\frac{t}{2}-1},\sqrt{2}(n+1)e^{-\frac{t}{10}}\}$,
which gives nontrivial results for $t\geq 3$. We given applications to the
weight distribution of cosets of extended Hadamard codes and extended dual BCH
codes. Our argument is based on Fourier analysis, linear programming, and
polynomial approximation techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5690</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5690</id><created>2014-08-25</created><authors><author><keyname>Ringert</keyname><forenames>Jan Oliver</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author><author><keyname>Wortmann</keyname><forenames>Andreas</forenames></author></authors><title>From Software Architecture Structure and Behavior Modeling to
  Implementations of Cyber-Physical Systems</title><categories>cs.SE</categories><comments>16 pages, 7 figures</comments><journal-ref>Software Engineering 2013 Workshopband, LNI P-215, pages 155-170.
  GI, K\&quot;ollen Druck+Verlag GmbH, Bonn, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software development for Cyber-Physical Systems (CPS) is a sophisticated
activity as these systems are inherently complex. The engineering of CPS
requires composition and interaction of diverse distributed software modules.
Describing both, a systems architecture and behavior in integrated models,
yields many advantages to cope with this complexity: the models are platform
independent, can be decomposed to be developed independently by experts of the
respective fields, are highly reusable and may be subjected to formal analysis.
In this paper, we introduce a code generation framework for the
MontiArcAutomaton modeling language. CPS are modeled as Component &amp; Connector
architectures with embedded I/O! automata. During development, these models can
be analyzed using formal methods, graphically edited, and deployed to various
platforms. For this, we present four code generators based on the MontiCore
code generation framework, that implement the transformation from
MontiArcAutomaton models to Mona (formal analysis), EMF Ecore (graphical
editing), and Java and Python (deployment. Based on these prototypes, we
discuss their commonalities and differences as well as language and application
specific challenges focusing on code generator development.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5691</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5691</id><created>2014-08-25</created><authors><author><keyname>Berger</keyname><forenames>Christian</forenames></author><author><keyname>Block</keyname><forenames>Delf</forenames></author><author><keyname>Hons</keyname><forenames>Christian</forenames></author><author><keyname>K&#xfc;hnel</keyname><forenames>Stefan</forenames></author><author><keyname>Leschke</keyname><forenames>Andr&#xe9;</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author><author><keyname>Strutz</keyname><forenames>Torsten</forenames></author></authors><title>Meta-Metrics for Simulations in Software Engineering on the Example of
  Integral Safety Systems</title><categories>cs.SE</categories><report-no>13 pages, 5 figures</report-no><journal-ref>Braunschweiger Symposium AAET 2013, Automatisierungssysteme,
  Assistenzsysteme und eingebettete Systeme f\&quot;ur Transportmittel, 6.-7.2.2013,
  pp. 136-148, Niedersachsen e.V.(Hrsg.) 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vehicles passengers and other traffic participants are protected more and
more by integral safety systems. They continuously perceive the vehicles
environment to prevent dangerous situations by e.g. emergency braking systems.
Furthermore, increasingly intelligent vehicle functions are still of major
interest in research and development to reduce the risk of accidents. However,
the development and testing of these functions should not rely only on
validations on proving grounds and on long-term test-runs in real traffic;
instead, they should be extended by virtual testing approaches to model
potentially dangerous situations or to re-run specific traffic situations
easily. This article outlines meta-metrics as one of todays challenges for the
software engineering of these cyber-physical systems to provide guidance during
the system development: For example, unstable results of simulation test-runs
over the vehicle functions revision history are elaborated as an indicating
metric where to focus on with real or further virtual test-runs; furthermore,
varying acting time points for the same virtual traffic situation are
indicating problems with the reliability to interpret the specific situation.
In this article, several of such meta-metrics are discussed and assigned both
to different phases during the series development and to different levels of
detailedness of virtual testing approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5692</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5692</id><created>2014-08-25</created><authors><author><keyname>Ringert</keyname><forenames>Jan Oliver</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author><author><keyname>Wortmann</keyname><forenames>Andreas</forenames></author></authors><title>A Case Study on Model-Based Development of Robotic Systems using
  MontiArc with Embedded Automata</title><categories>cs.SE</categories><comments>14 pages, 7 figures</comments><journal-ref>In H. Giese, M. Huhn, J. Philipps, and B. Sch\&quot;atz, editors,
  Dagstuhl-Workshop MBEES: Modellbasierte Entwicklung eingebetteter Systeme.
  Pages 30-43, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software development for service robotics is inherently complex. Even a
single robot requires the composition of several sensors, actuators, and
software modules. The systems are usually developed by groups of domain
experts, rarely software engineering experts. Thus the resulting software
systems are monolithic programs solving a single problem on a single platform.
We claim modeling of both structure and behavior of robots in a modular way
leads to better reusable software. We report on a study about the modeling of
robotics software with the structure and behavior modeling language
MontiArcAutomaton. This study assesses the benefits and difficulties of
model-based robotics software development using MontiArc-Automaton. Our
findings are based on a survey, discussions with the participants, and key
figures from their development behavior. We present the project, our study,
lessons learned, and future work based on the insights gained
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5693</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5693</id><created>2014-08-25</created><authors><author><keyname>Pietsch</keyname><forenames>Pit</forenames></author><author><keyname>M&#xfc;ller</keyname><forenames>Klaus</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>Model Matching Challenge: Benchmarks for Ecore and BPMN Diagrams</title><categories>cs.SE</categories><comments>7 pages, 7 figures</comments><journal-ref>Softwaretechnik-Trends, Volume 33, Issue 2. Mai, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the last couple of years, Model Driven Engineering (MDE) gained a
prominent role in the context of software engineering. In the MDE paradigm,
models are considered first level artifacts which are iteratively developed by
teams of programmers over a period of time. Because of this, dedicated tools
for versioning and management of models are needed. A central functionality
within this group of tools is model comparison and differencing. In two
disjunct research projects, we identified a group of general matching problems
where state-of-the-art comparison algorithms delivered low quality results. In
this article, we will present five edit operations which are the cause for
these low quality results. The reasons why the algorithms fail, as well as
possible solutions, are also discussed. These examples can be used as
benchmarks by model developers to assess the quality and applicability of a
model comparison tool for a given model type.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5695</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5695</id><created>2014-08-25</created><authors><author><keyname>Rei&#xdf;</keyname><forenames>Dirk</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>Using Lightweight Activity Diagrams for Modeling and Generation of Web
  Information Systems</title><categories>cs.SE</categories><comments>12 pages, 6 figures</comments><journal-ref>Proceedings 4th International United Information Systems
  Conference, UNISCON 2012, Yalta, Ukraine, June 1-3, 2012. Lecture Notes in
  Business Information Processing, vol. 137, pages 61-73, Springer, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The development process of web information systems nowadays improved a lot
regarding effectiveness and tool support, but still contains many redundant
steps for similar tasks. In order to overcome this, we use a model-driven
approach to specify a web information system in an agile way and generate a
full- edged and runnable application from a set of models. The covered aspects
of the system comprise data structure, page structure including view on data,
page- and workflow within the system as well as overall application structure
and user rights management. Appropriate tooling allows transforming these
models to complete systems and thus gives us opportunity for a lightweight
development process based on models. In this paper, we describe how we approach
the page- and workflow aspect by using activity diagrams as part of the agile
modeling approach MontiWIS. We give an overview of the defined syntax, describe
the supported forms of action contents and finally explain how the behavior is
realized in the generated application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5696</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5696</id><created>2014-08-25</created><authors><author><keyname>Maoz</keyname><forenames>Shahar</forenames></author><author><keyname>Ringert</keyname><forenames>Jan Oliver</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>Synthesis of Component and Connector Models from Crosscutting Structural
  Views</title><categories>cs.SE</categories><comments>11 pages, 10 figures</comments><journal-ref>Joint Meeting of the European Software Engineering Conference and
  the ACM SIGSOFT Symposium on the Foundations of Software Engineering
  (ESEC/FSE'13), Eds.: B. Meyer, L. Baresi, M. Mezini, pages 444-454, ACM New
  York, 2013</journal-ref><doi>10.1145/2491411.2491414</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present component and connector (C&amp;C) views, which specify structural
properties of component and connector models in an expressive and intuitive
way. C&amp;C views provide means to abstract away direct hierarchy, direct
connectivity, port names and types, and thus can crosscut the traditional
boundaries of the implementation-oriented hierarchical decomposition of systems
and sub-systems, and reflect the partial knowledge available to different
stakeholders involved in a system's design. As a primary application for C&amp;C
views we investigate the synthesis problem: given a C&amp;C views specification,
consisting of mandatory, alternative, and negative views, construct a concrete
satisfying C&amp;C model, if one exists. We show that the problem is NP-hard and
solve it, in a bounded scope, using a reduction to SAT, via Alloy. We further
extend the basic problem with support for library components, specification
patterns, and architectural styles. The result of synthesis can be used for
further exploration, simulation, and refinement of the C&amp;C model or, as the
complete, final model itself, for direct code generation. A prototype tool and
an evaluation over four example systems with multiple specifications show
promising results and suggest interesting future research directions towards a
comprehensive development environment for the structure of component and
connector designs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5698</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5698</id><created>2014-08-25</created><authors><author><keyname>Brucker</keyname><forenames>Achim D.</forenames></author><author><keyname>Chiorean</keyname><forenames>Dan</forenames></author><author><keyname>Clark</keyname><forenames>Tony</forenames></author><author><keyname>Demuth</keyname><forenames>Birgit</forenames></author><author><keyname>Gogolla</keyname><forenames>Martin</forenames></author><author><keyname>Plotnikov</keyname><forenames>Dimitri</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author><author><keyname>Willink</keyname><forenames>Edward D.</forenames></author><author><keyname>Wolff</keyname><forenames>Burkhart</forenames></author></authors><title>Report on the Aachen OCL Meeting</title><categories>cs.SE</categories><comments>9 pages, 6 figures</comments><journal-ref>Proceedings of the MODELS 2013 OCL Workshop (OCL 2013), Miami,
  Florida (USA), Volume 1092 of CEUR Workshop Proceedings, Eds.: J. Cabot, M.
  Gogolla, I. Rath, E. Willink, pages 103-111, CEUR-WS.org, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As a continuation of the OCL workshop during the MODELS 2013 conference in
October 2013, a number of OCL experts decided to meet in November 2013 in
Aachen for two days to discuss possible short term improvements of OCL for an
upcoming OMG meeting and to envision possible future long-term developments of
the language. This paper is a sort of \minutes of the meeting&quot; and intended to
quickly inform the OCL community about the discussion topics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5699</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5699</id><created>2014-08-25</created><authors><author><keyname>Ganser</keyname><forenames>Andreas</forenames></author><author><keyname>Lichter</keyname><forenames>Horst</forenames></author><author><keyname>Roth</keyname><forenames>Alexander</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>Proactive Quality Guidance for Model Evolution in Model Libraries</title><categories>cs.SE</categories><comments>10 pages, figures. Appears in Models and Evolution Workshop
  Proceedings of the ACM/IEEE 16th International Conference on Model Driven
  Engineering Languages and Systems, Miami, Florida (USA), September 30, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Model evolution in model libraries differs from general model evolution. It
limits the scope to the manageable and allows to develop clear concepts,
approaches, solutions, and methodologies. Looking at model quality in evolving
model libraries, we focus on quality concerns related to reusability. In this
paper, we put forward our proactive quality guidance approach for model
evolution in model libraries. It uses an editing-time assessment linked to a
lightweight quality model, corresponding metrics, and simplified reviews. All
of which help to guide model evolution by means of quality gates fostering
model reusability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5700</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5700</id><created>2014-08-25</created><authors><author><keyname>Wildgaard</keyname><forenames>Lorna</forenames></author><author><keyname>Schneider</keyname><forenames>Jesper W.</forenames></author><author><keyname>Larsen</keyname><forenames>Birger</forenames></author></authors><title>A review of the characteristics of 108 author-level bibliometric
  indicators</title><categories>cs.DL</categories><comments>to be published in Scientometrics, 2014</comments><doi>10.1007/s11192-014-1423-3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An increasing demand for bibliometric assessment of individuals has led to a
growth of new bibliometric indicators as well as new variants or combinations
of established ones. The aim of this review is to contribute with objective
facts about the usefulness of bibliometric indicators of the effects of
publication activity at the individual level. This paper reviews 108 indicators
that can potentially be used to measure performance on the individual author
level, and examines the complexity of their calculations in relation to what
they are supposed to reflect and ease of end-user application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5703</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5703</id><created>2014-08-25</created><authors><author><keyname>Combemale</keyname><forenames>Benoit</forenames></author><author><keyname>De Antoni</keyname><forenames>Julien</forenames></author><author><keyname>France</keyname><forenames>Robert B.</forenames></author><author><keyname>Boulanger</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames></author><author><keyname>Mosser</keyname><forenames>Sebastien</forenames></author><author><keyname>Pantel</keyname><forenames>Marc</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author><author><keyname>Salay</keyname><forenames>Rick</forenames></author><author><keyname>Schindler</keyname><forenames>Martin</forenames></author></authors><title>Report on the First Workshop On the Globalization of Modeling Languages</title><categories>cs.SE</categories><comments>11 pages, 7 figures</comments><journal-ref>First Int. Workshop On the Globalization of Modeling Languages
  (GEMOC 2013): Towards the Model Driven Organization (AMINO 2013), Miami,
  Florida (USA), Volume 1102 of CEUR Workshop Proceedings, pages 3-13,
  CEUR-WS.org, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The first edition of GEMOC workshop was co-located with the MODELS 2013
conference in Miami, FL, USA. The workshop provided an open forum for sharing
experiences, problems and solutions related to the challenges of using of
multiple modeling languages in the development of complex software based
systems. During the workshop, concrete language composition artifacts,
approaches, and mechanisms were presented and discussed, ideas and opinions
exchanged, and constructive feedback provided to authors of accepted papers. A
major objective was to encourage collaborations and to start building a
community that focused on providing solutions that support what we refer to as
the globalization of domain-specific modeling languages, that is, support
coordinated use of multiple languages throughout the development of complex
systems. This report summarizes the presentations and discussions that took
place in the first GEMOC 2013 workshop.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5705</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5705</id><created>2014-08-25</created><authors><author><keyname>Perez</keyname><forenames>Antonio Navarro</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>Modeling Cloud Architectures as Interactive Systems</title><categories>cs.SE</categories><comments>10 pages, 5 figures</comments><journal-ref>Proceedings of the 2nd International Workshop on Model-Driven
  Engineering for High Performance and Cloud Computing, Miami. Volume 1118 of
  CEUR Workshop Proceedings, pages 15-24, CEUR-WS.org, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The development and maintenance of cloud software is complicated by complex
but crucial technological requirements that are tightly coupled with each other
and with the softwares actual business functionality. Consequently, the
complexity of design, implementation, deployment, and maintenance activities
increases. We present an architecture description language that raises the
level of technological abstraction by modeling cloud software as interactive
systems. We show how its models correspond to an architecture style that
particularly meets the requirements of cloud-based cyber-physical systems. The
result provides a basis for an architecture-driven model-based methodology for
engineering cloud software.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5707</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5707</id><created>2014-08-25</created><authors><author><keyname>Roth</keyname><forenames>Alexander</forenames></author><author><keyname>Ganser</keyname><forenames>Andreas</forenames></author><author><keyname>Lichter</keyname><forenames>Horst</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>Staged Evolution with Quality Gates for Model Libraries</title><categories>cs.SE</categories><comments>8 pages, 8 figures</comments><journal-ref>Proceedings of the International Workshop on Document Changes:
  Modeling, Detection, Storage and Visualization, Florence, Italy, September
  10, 2013, Volume 1008 of CEUR Workshop Proceedings. CEUR-WS.org, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Model evolution is widely considered as a subject under research. Despite its
role in research, common purpose concepts, approaches, solutions, and
methodologies are missing. Limiting the scope to model libraries makes model
evolution and related quality concerns manageable, as we show below. In this
paper, we put forward our quality staged model evolution theory for model
libraries. It is founded on evolution graphs, which offer a structure for model
evolution in model libraries through evolution steps. These evolution steps
eventually form a sequence, which can be partitioned into stages by quality
gates. Each quality gate is defined by a lightweight quality model and
respective characteristics fostering reusability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5710</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5710</id><created>2014-08-25</created><authors><author><keyname>Liu</keyname><forenames>Ya-Feng</forenames></author></authors><title>Complexity Analysis of Joint Subcarrier and Power Allocation for the
  Cellular Downlink OFDMA System</title><categories>cs.IT math.IT</categories><comments>Accepted for publication in IEEE Wireless Communications Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider the cellular downlink Orthogonal Frequency Division Multiple Access
(OFDMA) system where a single transmitter transmits signals to multiple
receivers on multiple discrete subcarriers. To adapt fast channel fluctuations,
the transmitter should be able to dynamically allocate subcarrier and power
resources. Assuming perfect channel knowledge, we formulate the joint
subcarrier and power allocation problem as two optimization problems: the first
is the one of minimizing the total transmission power subject to quality of
service constraints, and the second is the one of maximizing a system utility
function subject to power budget constraints. In this letter, we show that both
the aforementioned formulations of the joint subcarrier and power allocation
problem are generally NP-hard. We also identify several subclasses of the
problem which are polynomial time solvable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5715</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5715</id><created>2014-08-25</created><authors><author><keyname>Nagy</keyname><forenames>Zoltan</forenames></author><author><keyname>Nemes</keyname><forenames>Csaba</forenames></author><author><keyname>Hiba</keyname><forenames>Antal</forenames></author><author><keyname>Csik</keyname><forenames>Arpad</forenames></author><author><keyname>Kiss</keyname><forenames>Andras</forenames></author><author><keyname>Ruszinko</keyname><forenames>Miklos</forenames></author><author><keyname>Szolgay</keyname><forenames>Peter</forenames></author></authors><title>Accelerating unstructured finite volume computations on
  field-programmable gate arrays</title><categories>cs.PF</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Accurate simulations of various physical processes on digital computers
requires huge computing performance, therefore accelerating these scientific
and engineering applications has a great importance. Density of programmable
logic devices doubles in every 18 months according to Moore's Law. On the
recent devices around one hundred double precision floating-point adders and
multipliers can be implemented. In the paper an FPGA based framework is
described to efficiently utilize this huge computing power to accelerate
simulation of complex physical spatiotemporal phenomena. Simulating complicated
geometries requires unstructured spatial discretization which results in
irregular memory access patterns severely limiting computing performance. Data
locality is improved by mesh node renumbering technique which results in
predictable memory access pattern. Additionally storing a small window of node
data in the on-chip memory of the FPGA can increase data reuse and decrease
memory bandwidth requirements. Generation of the floating-point data path and
control structure of the arithmetic unit containing dozens of operators is a
very challenging task when the goal is high operating frequency. Long and high
fanout control lines and improper placement can severely affect computing
performance. In the paper an automatic data path generation and partitioning
algorithm is presented to eliminate long delays and aid placement of the
circuit. Efficiency and use of the framework is described by a case study
solving the Euler equations on an unstructured mesh using finite volume
technique. On the currently available largest FPGA the generated architecture
contains three processing elements working in parallel providing 90 times
speedup compared to a high performance microprocessor core.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5717</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5717</id><created>2014-08-25</created><authors><author><keyname>Varma</keyname><forenames>Vineeth S.</forenames></author><author><keyname>Lasaulce</keyname><forenames>Samson</forenames></author><author><keyname>Hayel</keyname><forenames>Yezekael</forenames></author><author><keyname>Elayoubi</keyname><forenames>Salah Eddine</forenames></author></authors><title>A Cross-Layer Approach for Distributed Energy-Efficient Power Control in
  Interference Networks</title><categories>cs.NI cs.IT math.IT</categories><comments>Accepted for publication in IEEE TVT</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In contrast with existing works which rely on the same type of
energy-efficiency measure to design distributed power control policies, the
present work takes into account the presence of a finite packet buffer at the
transmitter side and the impact of transport protocols. This approach is
relevant when the transmitters have a non-zero energy cost even when the
radiated power is zero. A generalized energy-efficiency performance metric
integrating these features is constructed under two different scenarios in
terms of transport layer protocols characterized by a constant or an adaptive
packet arrival rate. The derived performance metric is shown to have several
attractive properties in both scenarios, which ensures convergence of the used
distributed power control algorithm to a unique point. This point is the Nash
equilibrium of a game for which the equilibrium analysis is conducted. Although
the equilibrium analysis methodology is not new in itself, conducting it
requires several non-trivial proofs, including the proof of quasi-concavity of
the payoff functions. A thorough numerical analysis is provided to illustrate
the effects of the proposed approach, and provides several valuable insights in
terms of designing interference management policies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5733</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5733</id><created>2014-08-25</created><authors><author><keyname>Demleitner</keyname><forenames>Markus</forenames></author><author><keyname>Neves</keyname><forenames>Margarida Castro</forenames></author><author><keyname>Rothmaier</keyname><forenames>Florian</forenames></author><author><keyname>Wambsganss</keyname><forenames>Joachim</forenames></author></authors><title>Virtual Observatory Publishing with DaCHS</title><categories>astro-ph.IM cs.SE</categories><msc-class>68U35</msc-class><doi>10.1016/j.ascom.2014.08.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Data Center Helper Suite DaCHS is an integrated publication package for
building Virtual Observatory (VO) and Web services, supporting the entire
workflow from ingestion to data mapping to service definition. It implements
all major data discovery, data access, and registry protocols defined by the
VO. DaCHS in this sense works as glue between data produced by the data
providers and the standard protocols and formats defined by the VO. This paper
discusses central elements of the design of the package and gives two case
studies of how VO protocols are implemented using DaCHS' concepts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5737</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5737</id><created>2014-08-25</created><authors><author><keyname>Abdelrahim</keyname><forenames>Mahmoud</forenames></author><author><keyname>Postoyan</keyname><forenames>Romain</forenames></author><author><keyname>Daafouz</keyname><forenames>Jamal</forenames></author></authors><title>Event-triggered control of nonlinear singularly perturbed systems based
  only on the slow dynamics</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Controllers are often designed based on a reduced or simplified model of the
plant dynamics. In this context, we investigate whether it is possible to
synthesize a stabilizing event-triggered feedback law for networked control
systems (NCS) which have two time-scales, based only on an approximate model of
the slow dynamics. We follow an emulation-like approach as we assume that we
know how to solve the problem in the absence of sampling and then we study how
to design the event-triggering rule under communication constraints. The NCS is
modeled as a hybrid singularly perturbed system which exhibits the feature to
generate jumps for both the fast variable and the error variable induced by the
sampling. The first conclusion is that a triggering law which guarantees the
stability and the existence of a uniform minimum amount of time between two
transmissions for the slow model may not ensure the existence of such a time
for the overall system, which makes the controller not implementable in
practice. The objective of this contribution is twofold. We first show that
existing event-triggering conditions can be adapted to singularly perturbed
systems and semiglobal practical stability can be ensured in this case. Second,
we propose another technique that combines event-triggered and time-triggered
results in the sense that transmissions are only allowed after a predefined
amount of time has elapsed since the last transmission. This technique has the
advantage, under an additional assumption, to ensure a global asymptotic
stability property and to allow the user to directly tune the minimum
inter-transmission interval. We believe that this technique is of its own
interest independently of the two-time scale nature of the addressed problem.
The results are shown to be applicable to a class of globally Lipschitz
systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5738</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5738</id><created>2014-08-25</created><authors><author><keyname>Abdelrahim</keyname><forenames>Mahmoud</forenames></author><author><keyname>Postoyan</keyname><forenames>Romain</forenames></author><author><keyname>Daafouz</keyname><forenames>Jamal</forenames></author><author><keyname>Ne&#x161;i&#x107;</keyname><forenames>Dragan</forenames></author></authors><title>Stabilization of nonlinear systems using event-triggered output feedback
  controllers</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The objective is to design output feedback event-triggered controllers to
stabilize a class of nonlinear systems. One of the main difficulties of the
problem is to ensure the existence of a minimum amount of time between two
consecutive transmissions, which is essential in practice. We solve this issue
by combining techniques from event-triggered and time-triggered control. The
idea is to turn on the event-triggering mechanism only after a fixed amount of
time has elapsed since the last transmission. This time is computed based on
results on the stabilization of time-driven sampled-data systems. The overall
strategy ensures an asymptotic stability property for the closed-loop system.
The results are proved to be applicable to linear time-invariant (LTI) systems
as a particular case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5748</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5748</id><created>2014-08-25</created><authors><author><keyname>Hoffmann</keyname><forenames>Gottfried</forenames></author><author><keyname>Riehle</keyname><forenames>Dirk</forenames></author><author><keyname>Kolassa</keyname><forenames>Carsten</forenames></author><author><keyname>Mauerer</keyname><forenames>Wolfgang</forenames></author></authors><title>A Dual Model of Open Source License Growth</title><categories>cs.CY cs.SE</categories><comments>14 pages, 6 figures</comments><acm-class>D.2.8; D.2.9; D.m; K.4; K.4.3; K.4.m</acm-class><journal-ref>Proceedings of the 9th International Conference on Open Source
  Systems (OSS 2013). Page 245-256. Springer Verlag, 2013</journal-ref><doi>10.1007/978-3-642-38928-3_18</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Every open source project needs to decide on an open source license. This
decision is of high economic relevance: Just which license is the best one to
help the project grow and attract a community? The most common question is:
Should the project choose a restrictive (reciprocal) license or a more
permissive one? As an important step towards answering this question, this
paper analyses actual license choice and correlated project growth from ten
years of open source projects. It provides closed analytical models and finds
that around 2001 a reversal in license choice occurred from restrictive towards
permissive licenses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5750</identifier>
 <datestamp>2015-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5750</id><created>2014-08-25</created><updated>2015-10-16</updated><authors><author><keyname>Yang</keyname><forenames>Zai</forenames></author><author><keyname>Xie</keyname><forenames>Lihua</forenames></author></authors><title>Enhancing Sparsity and Resolution via Reweighted Atomic Norm
  Minimization</title><categories>cs.IT math.IT</categories><comments>12 pages, double column, 5 figures, to appear in IEEE Transactions on
  Signal Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The mathematical theory of super-resolution developed recently by Cand\`{e}s
and Fernandes-Granda states that a continuous, sparse frequency spectrum can be
recovered with infinite precision via a (convex) atomic norm technique given a
set of uniform time-space samples. This theory was then extended to the cases
of partial/compressive samples and/or multiple measurement vectors via atomic
norm minimization (ANM), known as off-grid/continuous compressed sensing (CCS).
However, a major problem of existing atomic norm methods is that the
frequencies can be recovered only if they are sufficiently separated,
prohibiting commonly known high resolution. In this paper, a novel (nonconvex)
sparse metric is proposed that promotes sparsity to a greater extent than the
atomic norm. Using this metric an optimization problem is formulated and a
locally convergent iterative algorithm is implemented. The algorithm
iteratively carries out ANM with a sound reweighting strategy which enhances
sparsity and resolution, and is termed as reweighted atomic-norm minimization
(RAM). Extensive numerical simulations are carried out to demonstrate the
advantageous performance of RAM with application to direction of arrival (DOA)
estimation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5751</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5751</id><created>2014-08-25</created><authors><author><keyname>Haber</keyname><forenames>Arne</forenames></author><author><keyname>Kolassa</keyname><forenames>Carsten</forenames></author><author><keyname>Manhart</keyname><forenames>Peter</forenames></author><author><keyname>Nazari</keyname><forenames>Pedram Mir Seyed</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author><author><keyname>Schaefer</keyname><forenames>Ina</forenames></author></authors><title>First-Class Variability Modeling in Matlab/Simulink</title><categories>cs.SE</categories><comments>8 pages, 8 figures. Workshop on Variability Modelling of
  Software-intensive Systems. VaMoS 2013 in Pisa, Italy</comments><acm-class>D.2.6; D.2.2</acm-class><journal-ref>Proceedings of the Seventh International Workshop on Variability
  Modelling of Software-intensive Systems, 23.-25.1.2013, pp. 11-18, ACM, New
  York, NY, USA. 2013</journal-ref><doi>10.1145/2430502.2430508</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern cars exist in an vast number of variants. Thus, variability has to be
dealt with in all phases of the development process, in particular during
model-based development of software-intensive functionality using
Matlab/Simulink. Currently, variability is often encoded within a functional
model leading to so called 150%-models which easily become very complex and do
not scale for larger product lines. To counter these problems, we propose a
modular variability modeling approach for Matlab/Simulink based on the concept
of delta modeling [8, 9, 24]. A functional variant is described by a delta
encapsulating a set of modifications. A sequence of deltas can be applied to a
core product to derive the desired variant. We present a prototypical
implementation, which is integrated into Matlab/Simulink and offers graphical
editing of delta models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5752</identifier>
 <datestamp>2014-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5752</id><created>2014-08-25</created><updated>2014-10-06</updated><authors><author><keyname>Torfah</keyname><forenames>Hazem</forenames></author><author><keyname>Zimmermann</keyname><forenames>Martin</forenames></author></authors><title>The Complexity of Counting Models of Linear-time Temporal Logic</title><categories>cs.LO cs.CC</categories><comments>A short version appears in Proceedings of FSTTCS 2014</comments><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We determine the complexity of counting models of bounded size of
specifications expressed in Linear-time Temporal Logic. Counting word models is
#P-complete, if the bound is given in unary, and as hard as counting accepting
runs of nondeterministic polynomial space Turing machines, if the bound is
given in binary. Counting tree models is as hard as counting accepting runs of
nondeterministic exponential time Turing machines, if the bound is given in
unary. For a binary encoding of the bound, the problem is at least as hard as
counting accepting runs of nondeterministic exponential space Turing machines.
On the other hand, it is not harder than counting accepting runs of
nondeterministic doubly-exponential time Turing machines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5756</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5756</id><created>2014-08-25</created><authors><author><keyname>Haber</keyname><forenames>Arne</forenames></author><author><keyname>H&#xf6;lldobler</keyname><forenames>Katrin</forenames></author><author><keyname>Kolassa</keyname><forenames>Carsten</forenames></author><author><keyname>Look</keyname><forenames>Markus</forenames></author><author><keyname>M&#xfc;ller</keyname><forenames>Klaus</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author><author><keyname>Schaefer</keyname><forenames>Ina</forenames></author></authors><title>Engineering Delta Modeling Languages</title><categories>cs.SE</categories><comments>10 pages, 8 figures. Proceedings of the 17th International Software
  Product Line Conference, Tokyo, September 2013, pp.22-31, ACM, 2013</comments><acm-class>D.2; D.2.2; D.2.3</acm-class><journal-ref>Proceedings of the 17th International Software Product Line
  Conference, Tokyo, September 2013, pp.22-31, ACM, 2013</journal-ref><doi>10.1145/2491627.2491632</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Delta modeling is a modular, yet flexible approach to capture spatial and
temporal variability by explicitly representing the differences between system
variants or versions. The conceptual idea of delta modeling is
language-independent. But, in order to apply delta modeling for a concrete
language, so far, a delta language had to be manually developed on top of the
base language leading to a large variety of heterogeneous language concepts. In
this paper, we present a process that allows deriving a delta language from the
grammar of a given base language. Our approach relies on an automatically
generated language extension that can be manually adapted to meet
domain-specific needs. We illustrate our approach using delta modeling on a
textual variant of statecharts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5777</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5777</id><created>2014-08-07</created><authors><author><keyname>Ahsan</keyname><forenames>Saba</forenames></author><author><keyname>Singh</keyname><forenames>Varun</forenames></author><author><keyname>Ott</keyname><forenames>J&#xf6;rg</forenames></author></authors><title>Characterizing Internet Video for Large-scale Active Measurements</title><categories>cs.MM cs.NI</categories><comments>15 pages, 18 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The availability of high definition video content on the web has brought
about a significant change in the characteristics of Internet video, but not
many studies on characterizing video have been done after this change. Video
characteristics such as video length, format, target bit rate, and resolution
provide valuable input to design Adaptive Bit Rate (ABR) algorithms, sizing
playout buffers in Dynamic Adaptive HTTP streaming (DASH) players, model the
variability in video frame sizes, etc. This paper presents datasets collected
in 2013 and 2014 that contains over 130,000 videos from YouTube's most viewed
(or most popular) video charts in 58 countries. We describe the basic
characteristics of the videos on YouTube for each category, format, video
length, file size, and data rate variation, observing that video length and
file size fit a log normal distribution. We show that three minutes of a video
suffice to represent its instant data rate fluctuation and that we can infer
data rate characteristics of different video resolutions from a single given
one. Based on our findings, we design active measurements for measuring the
performance of Internet video.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5780</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5780</id><created>2014-08-25</created><updated>2016-02-08</updated><authors><author><keyname>Olmez</keyname><forenames>Oktay</forenames></author><author><keyname>Ramamoorthy</keyname><forenames>Aditya</forenames></author></authors><title>Fractional repetition codes with flexible repair from combinatorial
  designs</title><categories>cs.IT math.IT</categories><comments>27 pages in IEEE two-column format. IEEE Transactions on Information
  Theory (to appear)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fractional repetition (FR) codes are a class of regenerating codes for
distributed storage systems with an exact (table-based) repair process that is
also uncoded, i.e., upon failure, a node is regenerated by simply downloading
packets from the surviving nodes. In our work, we present constructions of FR
codes based on Steiner systems and resolvable combinatorial designs such as
affine geometries, Hadamard designs and mutually orthogonal Latin squares. The
failure resilience of our codes can be varied in a simple manner. We construct
codes with normalized repair bandwidth ($\beta$) strictly larger than one;
these cannot be obtained trivially from codes with $\beta = 1$. Furthermore, we
present the Kronecker product technique for generating new codes from existing
ones and elaborate on their properties. FR codes with locality are those where
the repair degree is smaller than the number of nodes contacted for
reconstructing the stored file. For these codes we establish a tradeoff between
the local repair property and failure resilience and construct codes that meet
this tradeoff. Much of prior work only provided lower bounds on the FR code
rate. In our work, for most of our constructions we determine the code rate for
certain parameter ranges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5781</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5781</id><created>2014-08-25</created><authors><author><keyname>Perraudin</keyname><forenames>Nathana&#xeb;l</forenames></author><author><keyname>Paratte</keyname><forenames>Johan</forenames></author><author><keyname>Shuman</keyname><forenames>David</forenames></author><author><keyname>Kalofolias</keyname><forenames>Vassilis</forenames></author><author><keyname>Vandergheynst</keyname><forenames>Pierre</forenames></author><author><keyname>Hammond</keyname><forenames>David K.</forenames></author></authors><title>GSPBOX: A toolbox for signal processing on graphs</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this document we introduce a Matlab toolbox called the Graph Signal
Processing toolbox (GSPBox). This toolbox is based on spectral graph theory,
more specifically graph filtering. It includes fast filtering routines using
Chebychev polynomials as presented in [4]. This document is automatically
generated from the source files and includes the complete documen- tation of
the toolbox. However the most up-to-date documentation can be found on the
official website http://lts2research.epfl.ch/gsp/doc.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5782</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5782</id><created>2014-08-25</created><authors><author><keyname>Zhang</keyname><forenames>Guanghui</forenames></author><author><keyname>Chen</keyname><forenames>Bocong</forenames></author><author><keyname>Li</keyname><forenames>Liangchen</forenames></author></authors><title>A Construction of MDS Quantum Convolutional Codes</title><categories>cs.IT math.IT</categories><comments>11 pages</comments><doi>10.1007/s10773-015-2557-7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, two new families of MDS quantum convolutional codes are
constructed. The first one can be regarded as a generalization of \cite[Theorem
6.5]{GGGlinear}, in the sense that we do not assume that $q\equiv1\pmod{4}$.
More specifically, we obtain two classes of MDS quantum convolutional codes
with parameters: {\rm (i)}~ $[(q^2+1, q^2-4i+3,1;2,2i+2)]_q$, where $q\geq5$ is
an odd prime power and $2\leq i\leq(q-1)/2$; {\rm (ii)}~
$[(\frac{q^2+1}{10},\frac{q^2+1}{10}-4i,1;2,2i+3)]_q$, where $q$ is an odd
prime power with the form
  $q=10m+3$ or $10m+7$ ($m\geq2$), and $2\leq i\leq2m-1$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5786</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5786</id><created>2014-08-25</created><authors><author><keyname>Qureshi</keyname><forenames>M. Rizwan Jameel</forenames></author><author><keyname>Sabir</keyname><forenames>Fatima</forenames></author></authors><title>A comparison of model view controller and model view presenter</title><categories>cs.SE</categories><comments>3 pages, 5 figures</comments><journal-ref>Science International-Lahore, online Feb. 2013, Vol. 25, No. 1,
  2013, pp. 7-9</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Web application frameworks are managed by using different design strategies.
Design strategies are applied by using different design processes. In each
design process, requirement specifications are changed in to different design
model that describe the detail of different data structure, system
architecture, interface and components. Web application frame work is
implemented by using Model View Controller (MVC) and Model View Presenter
(MVP). These web application models are used to provide standardized view for
web applications. This paper mainly focuses on different design aspect of MVC
and MVP. Generally we present different methodologies that are related to the
implementation of MVC and MVP and implementation of appropriate platform and
suitable environment for MVC and MVP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5800</identifier>
 <datestamp>2015-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5800</id><created>2014-08-21</created><updated>2015-05-05</updated><authors><author><keyname>Kish</keyname><forenames>Laszlo B.</forenames></author></authors><title>Enhanced usage of keys obtained by physical, unconditionally secure
  distributions</title><categories>cs.CR</categories><comments>this revised version is published</comments><journal-ref>Fluctuation and Noise Letters 14 (2015) 1550007</journal-ref><doi>10.1142/S0219477515500078</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Unconditionally secure physical key distribution schemes are very slow, and
it is practically impossible to use a one-time-pad based cipher to guarantee
unconditional security for the encryption of data because using the key bits
more than once gives out statistical information, for example via the
known-plain-text-attack or by utilizing known components of the protocol and
language statistics. Here we outline a protocol that reduces this speed problem
and allows almost-one-time-pad based communication with an unconditionally
secure physical key of finite length. The physical, unconditionally secure key
is not used for data encryption but is employed in order to generate and share
a new software-based key without any known-plain-text component. The
software-only-based key distribution is then changed from computationally
secure to unconditionally secure, because the communicated key-exchange data
(algorithm parameters, one-way functions of random numbers, etc.) are encrypted
in an unconditionally secure way with a one-time-pad. For practical
applications, this combined physical/software key distribution based
communication looks favorable compared to the software-only and physical-only
key distribution based communication whenever the speed of the physical key
distribution is much lower than that of the software-based key distribution. A
mathematical security proof of this new scheme remains an open problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5806</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5806</id><created>2014-08-25</created><authors><author><keyname>Ramezanian</keyname><forenames>Rasoul</forenames></author><author><keyname>Salehi</keyname><forenames>Mostafa</forenames></author><author><keyname>Magnani</keyname><forenames>Matteo</forenames></author><author><keyname>Montesi</keyname><forenames>Danilo</forenames></author></authors><title>Diffusion of Innovations over Multiplex Social Networks</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ways in which an innovation (e.g., new behaviour, idea, technology,
product) diffuses among people can determine its success or failure. In this
paper, we address the problem of diffusion of innovations over multiplex social
networks where the neighbours of a person belong to one or multiple networks
(or layers) such as friends, families, or colleagues. To this end, we
generalise one of the basic game-theoretic diffusion models, called networked
coordination game, for multiplex networks. We present analytical results for
this extended model and validate them through a simulation study, finding among
other properties a lower bound for the success of an innovation.While simple
and leading to intuitively understandable results, to the best of our knowledge
this is the first extension of a game-theoretic innovation diffusion model for
multiplex networks and as such it provides a basic framework to study more
sophisticated innovation dynamics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5809</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5809</id><created>2014-08-25</created><updated>2014-09-02</updated><authors><author><keyname>Ahman</keyname><forenames>Danel</forenames><affiliation>University of Edinburgh</affiliation></author><author><keyname>Chapman</keyname><forenames>James</forenames><affiliation>Institute of Cybernetics at TUT</affiliation></author><author><keyname>Uustalu</keyname><forenames>Tarmo</forenames><affiliation>Institute of Cybernetics at TUT</affiliation></author></authors><title>When is a container a comonad?</title><categories>cs.PL cs.LO math.CT</categories><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 10, Issue 3 (September
  3, 2014) lmcs:894</journal-ref><doi>10.2168/LMCS-10(3:14)2014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Abbott, Altenkirch, Ghani and others have taught us that many parameterized
datatypes (set functors) can be usefully analyzed via container representations
in terms of a set of shapes and a set of positions in each shape. This paper
builds on the observation that datatypes often carry additional structure that
containers alone do not account for. We introduce directed containers to
capture the common situation where every position in a data-structure
determines another data-structure, informally, the sub-data-structure rooted by
that position. Some natural examples are non-empty lists and node-labelled
trees, and data-structures with a designated position (zippers). While
containers denote set functors via a fully-faithful functor, directed
containers interpret fully-faithfully into comonads. But more is true: every
comonad whose underlying functor is a container is represented by a directed
container. In fact, directed containers are the same as containers that are
comonads. We also describe some constructions of directed containers. We have
formalized our development in the dependently typed programming language Agda.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5823</identifier>
 <datestamp>2014-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5823</id><created>2014-08-25</created><updated>2014-12-22</updated><authors><author><keyname>Balcan</keyname><forenames>Maria-Florina</forenames></author><author><keyname>Kanchanapally</keyname><forenames>Vandana</forenames></author><author><keyname>Liang</keyname><forenames>Yingyu</forenames></author><author><keyname>Woodruff</keyname><forenames>David</forenames></author></authors><title>Improved Distributed Principal Component Analysis</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the distributed computing setting in which there are multiple
servers, each holding a set of points, who wish to compute functions on the
union of their point sets. A key task in this setting is Principal Component
Analysis (PCA), in which the servers would like to compute a low dimensional
subspace capturing as much of the variance of the union of their point sets as
possible. Given a procedure for approximate PCA, one can use it to
approximately solve $\ell_2$-error fitting problems such as $k$-means
clustering and subspace clustering. The essential properties of an approximate
distributed PCA algorithm are its communication cost and computational
efficiency for a given desired accuracy in downstream applications. We give new
algorithms and analyses for distributed PCA which lead to improved
communication and computational costs for $k$-means clustering and related
problems. Our empirical study on real world data shows a speedup of orders of
magnitude, preserving communication with only a negligible degradation in
solution quality. Some of these techniques we develop, such as a general
transformation from a constant success probability subspace embedding to a high
success probability subspace embedding with a dimension and sparsity
independent of the success probability, may be of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5825</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5825</id><created>2014-08-25</created><authors><author><keyname>Nayyar</keyname><forenames>Ashutosh</forenames></author><author><keyname>Negrete-Pincetic</keyname><forenames>Matias</forenames></author><author><keyname>Poolla</keyname><forenames>Kameshwar</forenames></author><author><keyname>Varaiya</keyname><forenames>Pravin</forenames></author></authors><title>Duration-differentiated Energy Services with a Continuum of Loads</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As the proportion of total power supplied by renewable sources increases, it
gets more costly to use reserve generation to compensate for the variability of
renewables like solar and wind. Hence attention has been drawn to exploiting
flexibility in demand as a substitute for reserve generation. Flexibility has
different attributes. In this paper we consider loads requiring a constant
power for a specified duration (within say one day), whose flexibility resides
in the fact that power may be delivered at any time so long as the total
duration of service equals the load's specified duration. We give conditions
under which a variable power supply is adequate to meet these flexible loads,
and describe how to allocate the power to the loads. We also characterize the
additional power needed when the supply is inadequate. We study the problem of
allocating the available power to loads to maximize welfare, and show that the
welfare optimum can be sustained as a competitive equilibrium in a forward
market in which electricity is sold as service contracts differentiated by the
duration of service and power level. We compare this forward market with a spot
market in their ability to capture the flexiblity inherent in
duration-differentiated loads.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5833</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5833</id><created>2014-08-25</created><updated>2015-03-07</updated><authors><author><keyname>Karafyllis</keyname><forenames>Iasson</forenames></author><author><keyname>Kontorinaki</keyname><forenames>Maria</forenames></author><author><keyname>Papageorgiou</keyname><forenames>Markos</forenames></author></authors><title>Global Exponential Stabilization of Freeway Models</title><categories>math.OC cs.SY</categories><comments>Generalization of previous versions. 32 pages, 9 figures, submitted
  to the International Journal of Robust and Nonlinear Control for possible
  publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work is devoted to the construction of feedback laws which guarantee the
robust global exponential stability of the uncongested equilibrium point for
general discrete-time freeway models. The feedback construction is based on a
control Lyapunov function approach and exploits certain important properties of
freeway models. The developed feedback laws are tested in simulation and a
detailed comparison is made with existing feedback laws in the literature. The
robustness properties of the corresponding closed-loop system with respect to
measurement errors are also studied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5845</identifier>
 <datestamp>2014-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5845</id><created>2014-08-25</created><updated>2014-12-04</updated><authors><author><keyname>Arablouei</keyname><forenames>Reza</forenames></author><author><keyname>Werner</keyname><forenames>Stefan</forenames></author><author><keyname>Do&#x11f;an&#xe7;ay</keyname><forenames>Kutluy&#x131;l</forenames></author><author><keyname>Huang</keyname><forenames>Yih-Fang</forenames></author></authors><title>Analysis of a Reduced-Communication Diffusion LMS Algorithm</title><categories>cs.DC cs.LG cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In diffusion-based algorithms for adaptive distributed estimation, each node
of an adaptive network estimates a target parameter vector by creating an
intermediate estimate and then combining the intermediate estimates available
within its closed neighborhood. We analyze the performance of a
reduced-communication diffusion least mean-square (RC-DLMS) algorithm, which
allows each node to receive the intermediate estimates of only a subset of its
neighbors at each iteration. This algorithm eases the usage of network
communication resources and delivers a trade-off between estimation performance
and communication cost. We show analytically that the RC-DLMS algorithm is
stable and convergent in both mean and mean-square senses. We also calculate
its theoretical steady-state mean-square deviation. Simulation results
demonstrate a good match between theory and experiment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5870</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5870</id><created>2014-08-20</created><authors><author><keyname>Matai</keyname><forenames>Janarbek</forenames></author><author><keyname>Richmond</keyname><forenames>Dustin</forenames></author><author><keyname>Lee</keyname><forenames>Dajung</forenames></author><author><keyname>Kastner</keyname><forenames>Ryan</forenames></author></authors><title>Enabling FPGAs for the Masses</title><categories>cs.SE cs.PL</categories><comments>Presented at First International Workshop on FPGAs for Software
  Programmers (FSP 2014) (arXiv:1408.4423)</comments><proxy>Frank Hannig</proxy><report-no>FSP/2014/03</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Implementing an application on a FPGA remains a difficult, non-intuitive task
that often requires hardware design expertise in a hardware description
language (HDL). High-level synthesis (HLS) raises the design abstraction from
HDL to languages such as C/C++/Scala/Java. Despite this, in order to get a good
quality of result (QoR), a designer must carefully craft the HLS code. In other
words, HLS designers must implement the application using an abstract language
in a manner that generates an efficient micro-architecture; we call this
process writing restructured code. This reduces the benefits of implementing
the application at a higher level of abstraction and limits the impact of HLS
by requiring explicit knowledge of the underlying hardware architecture.
Developers must know how to write code that reflects low level implementation
details of the application at hand as it is interpreted by HLS tools. As a
result, FPGA design still largely remains job of either hardware engineers or
expert HLS designers. In this work, we aim to take a step towards making HLS
tools useful for a broader set of programmers. To do this, we study
methodologies of restructuring software code for HLS tools; we provide examples
of designing different kernels in state-of-the art HLS tools; and we present a
list of challenges for developing a hardware programming model for software
programmers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5879</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5879</id><created>2014-08-25</created><updated>2015-04-12</updated><authors><author><keyname>Qin</keyname><forenames>Xiaolin</forenames></author><author><keyname>Sun</keyname><forenames>Zhi</forenames></author><author><keyname>Leng</keyname><forenames>Tuo</forenames></author><author><keyname>Feng</keyname><forenames>Yong</forenames></author></authors><title>Computing the determinant of a matrix with polynomial entries by
  approximation</title><categories>cs.SC</categories><comments>17 pages, 2 figures</comments><msc-class>15A15, 41A05, 65Y10, 68W30</msc-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Computing the determinant of a matrix with the univariate and multivariate
polynomial entries arises frequently in the scientific computing and
engineering fields. In this paper, an effective algorithm is presented for
computing the determinant of a matrix with polynomial entries using hybrid
symbolic and numerical computation. The algorithm relies on the Newton's
interpolation method with error control for solving Vandermonde systems. It is
also based on a novel approach for estimating the degree of variables, and the
degree homomorphism method for dimension reduction. Furthermore, the
parallelization of the method arises naturally.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5882</identifier>
 <datestamp>2014-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5882</id><created>2014-08-25</created><updated>2014-09-02</updated><authors><author><keyname>Kim</keyname><forenames>Yoon</forenames></author></authors><title>Convolutional Neural Networks for Sentence Classification</title><categories>cs.CL cs.NE</categories><comments>To appear in EMNLP 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We report on a series of experiments with convolutional neural networks (CNN)
trained on top of pre-trained word vectors for sentence-level classification
tasks. We show that a simple CNN with little hyperparameter tuning and static
vectors achieves excellent results on multiple benchmarks. Learning
task-specific vectors through fine-tuning offers further gains in performance.
We additionally propose a simple modification to the architecture to allow for
the use of both task-specific and static vectors. The CNN models discussed
herein improve upon the state of the art on 4 out of 7 tasks, which include
sentiment analysis and question classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5891</identifier>
 <datestamp>2014-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5891</id><created>2014-08-23</created><authors><author><keyname>Lahlouhi</keyname><forenames>Ammar</forenames></author></authors><title>Integration of Heterogeneous Systems as Multi-Agent Systems</title><categories>cs.SE cs.MA</categories><comments>Journal of Systems Integration, Vol 5, No 3 (2014)</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Systems integration is a difficult matter particularly when its components
are varied. The problem becomes even more difficult when such components are
heterogeneous such as humans, robots and software systems. Currently, the
humans are regarded as users of artificial systems (robots and software
systems). This has several disadvantages such as: (1) incoherence of artificial
systems exploitation where humans' roles are not clear and (2) vain research of
a user's universal model. In this paper, we adopted a cooperative approach
where the system's components are regarded as being of the same level and they
cooperate for the service of the global system. We concretized such approach by
considering humans, robots and software systems as autonomous agents assuming
roles in an organization. The latter will be implemented as a multi-agent
system developed using a multi-agent development methodology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5893</identifier>
 <datestamp>2014-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5893</id><created>2014-08-25</created><authors><author><keyname>Marx</keyname><forenames>Werner</forenames></author><author><keyname>Bornmann</keyname><forenames>Lutz</forenames></author></authors><title>On the causes of subject-specific citation rates in Web of Science</title><categories>cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well known in bibliometrics that the average number of citations per
paper differs greatly between the various disciplines. The differing citation
culture (in particular the different average number of references per paper and
thereby the different probability of being cited) is widely seen as the cause
of this variation. Based on all Web of Science (WoS) records published in 1990,
1995, 2000, 2005, and 2010 we demonstrate that almost all disciplines show
similar numbers of references in the appendices of their papers. Our results
suggest that the average citation rate is far more influenced by the extent to
which the papers (cited as references) are included in WoS as linked database
records. For example, the comparatively low citation rates in the humanities
are not at all the result of a lower average number of references per paper but
are caused by the low fraction of linked references which refer to papers
published in the core journals covered by WoS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5894</identifier>
 <datestamp>2014-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5894</id><created>2014-08-25</created><authors><author><keyname>Skoumas</keyname><forenames>Georgios</forenames></author><author><keyname>Pfoser</keyname><forenames>Dieter</forenames></author><author><keyname>Kyrillidis</keyname><forenames>Anastasios</forenames></author></authors><title>Location Estimation Using Crowdsourced Geospatial Narratives</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The &quot;crowd&quot; has become a very important geospatial data provider. Subsumed
under the term Volunteered Geographic Information (VGI), non-expert users have
been providing a wealth of quantitative geospatial data online. With spatial
reasoning being a basic form of human cognition, narratives expressing
geospatial experiences, e.g., travel blogs, would provide an even bigger source
of geospatial data. Textual narratives typically contain qualitative data in
the form of objects and spatial relationships. The scope of this work is (i) to
extract these relationships from user-generated texts, (ii) to quantify them
and (iii) to reason about object locations based only on this qualitative data.
We use information extraction methods to identify toponyms and spatial
relationships and to formulate a quantitative approach based on distance and
orientation features to represent the latter. Positional probability
distributions for spatial relationships are determined by means of a greedy
Expectation Maximization-based (EM) algorithm. These estimates are then used to
&quot;triangulate&quot; the positions of unknown object locations. Experiments using a
text corpus harvested from travel blog sites establish the considerable
location estimation accuracy of the proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5920</identifier>
 <datestamp>2014-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5920</id><created>2014-08-25</created><authors><author><keyname>Bekos</keyname><forenames>Michael A.</forenames></author><author><keyname>Gronemann</keyname><forenames>Martin</forenames></author><author><keyname>Kaufmann</keyname><forenames>Michael</forenames></author><author><keyname>Krug</keyname><forenames>Robert</forenames></author></authors><title>Planar Octilinear Drawings with One Bend Per Edge</title><categories>cs.DS</categories><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  In octilinear drawings of planar graphs, every edge is drawn as an
alternating sequence of horizontal, vertical and diagonal ($45^\circ$)
line-segments. In this paper, we study octilinear drawings of low edge
complexity, i.e., with few bends per edge. A $k$-planar graph is a planar graph
in which each vertex has degree less or equal to $k$. In particular, we prove
that every 4-planar graph admits a planar octilinear drawing with at most one
bend per edge on an integer grid of size $O(n^2) \times O(n)$. For 5-planar
graphs, we prove that one bend per edge still suffices in order to construct
planar octilinear drawings, but in super-polynomial area. However, for 6-planar
graphs we give a class of graphs whose planar octilinear drawings require at
least two bends per edge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5925</identifier>
 <datestamp>2014-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5925</id><created>2014-08-25</created><authors><author><keyname>Winter</keyname><forenames>F. T.</forenames></author><author><keyname>Clark</keyname><forenames>M. A.</forenames></author><author><keyname>Edwards</keyname><forenames>R. G.</forenames></author><author><keyname>Jo&#xf3;</keyname><forenames>B.</forenames></author></authors><title>A Framework for Lattice QCD Calculations on GPUs</title><categories>hep-lat cs.MS physics.comp-ph</categories><comments>10 pages, 6 figures, as published in the proceedings of IPDPS '14</comments><doi>10.1109/IPDPS.2014.112</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computing platforms equipped with accelerators like GPUs have proven to
provide great computational power. However, exploiting such platforms for
existing scientific applications is not a trivial task. Current GPU programming
frameworks such as CUDA C/C++ require low-level programming from the developer
in order to achieve high performance code. As a result porting of applications
to GPUs is typically limited to time-dominant algorithms and routines, leaving
the remainder not accelerated which can open a serious Amdahl's law issue. The
lattice QCD application Chroma allows to explore a different porting strategy.
The layered structure of the software architecture logically separates the
data-parallel from the application layer. The QCD Data-Parallel software layer
provides data types and expressions with stencil-like operations suitable for
lattice field theory and Chroma implements algorithms in terms of this
high-level interface. Thus by porting the low-level layer one can effectively
move the whole application in one swing to a different platform. The
QDP-JIT/PTX library, the reimplementation of the low-level layer, provides a
framework for lattice QCD calculations for the CUDA architecture. The complete
software interface is supported and thus applications can be run unaltered on
GPU-based parallel computers. This reimplementation was possible due to the
availability of a JIT compiler (part of the NVIDIA Linux kernel driver) which
translates an assembly-like language (PTX) to GPU code. The expression template
technique is used to build PTX code generators and a software cache manages the
GPU memory. This reimplementation allows us to deploy an efficient
implementation of the full gauge-generation program with dynamical fermions on
large-scale GPU-based machines such as Titan and Blue Waters which accelerates
the algorithm by more than an order of magnitude.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5928</identifier>
 <datestamp>2015-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5928</id><created>2014-08-25</created><updated>2015-10-22</updated><authors><author><keyname>Talarico</keyname><forenames>Salvatore</forenames></author><author><keyname>Valenti</keyname><forenames>Matthew C.</forenames></author><author><keyname>Halford</keyname><forenames>Thomas R.</forenames></author></authors><title>Unicast Barrage Relay Networks: Outage Analysis and Optimization</title><categories>cs.NI cs.IT math.IT</categories><comments>7 pages, 4 figures, 1 table, in IEEE Military Commun. Conf. (MILCOM),
  2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Barrage relays networks (BRNs) are ad hoc networks built on a rapid
cooperative flooding primitive as opposed to the traditional point-to-point
link abstraction. Controlled barrage regions (CBRs) can be used to contain this
flooding primitive for unicast and multicast, thereby enabling spatial reuse.
In this paper, the behavior of individual CBRs is described as a Markov process
that models the potential cooperative relay transmissions. The outage
probability for a CBR is found in closed form for a given topology, and the
probability takes into account fading and co-channel interference (CCI) between
adjacent CBRs. Having adopted this accurate analytical framework, this paper
proceeds to optimize a BRN by finding the optimal size of each CBR, the number
of relays contained within each CBR, the optimal relay locations when they are
constrained to lie on a straight line, and the code rate that maximizes the
transport capacity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5939</identifier>
 <datestamp>2015-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5939</id><created>2014-08-25</created><updated>2015-05-12</updated><authors><author><keyname>Borradaile</keyname><forenames>Glencora</forenames></author><author><keyname>Eppstein</keyname><forenames>David</forenames></author><author><keyname>Zhu</keyname><forenames>Pingan</forenames></author></authors><title>Planar Induced Subgraphs of Sparse Graphs</title><categories>cs.CG cs.DS math.CO</categories><comments>Accepted by Graph Drawing 2014. To appear in Journal of Graph
  Algorithms and Applications</comments><journal-ref>J. Graph Algorithms &amp; Applications 19(1): 281-297, 2015</journal-ref><doi>10.7155/jgaa.00358</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that every graph has an induced pseudoforest of at least $n-m/4.5$
vertices, an induced partial 2-tree of at least $n-m/5$ vertices, and an
induced planar subgraph of at least $n-m/5.2174$ vertices. These results are
constructive, implying linear-time algorithms to find the respective induced
subgraphs. We also show that the size of the largest $K_h$-minor-free graph in
a given graph can sometimes be at most $n-m/6+o(m)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5945</identifier>
 <datestamp>2014-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5945</id><created>2014-08-25</created><updated>2014-08-30</updated><authors><author><keyname>Mbaye</keyname><forenames>Abdoulaye</forenames></author><author><keyname>Ciss</keyname><forenames>Abdoul Aziz</forenames></author><author><keyname>Niang</keyname><forenames>Oumar</forenames></author></authors><title>A Lightweight Identification Protocol for Embedded Devices</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The task of this paper is to introduce a new lightweight identification
protocol based on biometric data and elliptic curves. In fact, we combine
biometric data and asymetric cryptography, namely elliptic curves and standard
tools to design a multifactor identification protocol. Our scheme is light,
very fast, secure and robust against all the known attacks on identification
protocol. Therefore, one can use it in any constraint device such as embedded
systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5946</identifier>
 <datestamp>2014-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5946</id><created>2014-08-25</created><updated>2014-12-02</updated><authors><author><keyname>Ascher</keyname><forenames>Uri</forenames></author><author><keyname>Roosta-Khorasani</keyname><forenames>Farbod</forenames></author></authors><title>Algorithms that satisfy a stopping criterion, probably</title><categories>cs.NA math.NA</categories><msc-class>65C20, 65C05, 65M32, 65L05, 65F10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Iterative numerical algorithms are typically equipped with a stopping
criterion, where the iteration process is terminated when some error or misfit
measure is deemed to be below a given tolerance. This is a useful setting for
comparing algorithm performance, among other purposes. However, in practical
applications a precise value for such a tolerance is rarely known; rather, only
some possibly vague idea of the desired quality of the numerical approximation
is at hand. We discuss four case studies from different areas of numerical
computation, where uncertainty in the error tolerance value and in the stopping
criterion is revealed in different ways. This leads us to think of approaches
to relax the notion of exactly satisfying a tolerance value. We then
concentrate on a {\em probabilistic} relaxation of the given tolerance. This
allows, for instance, derivation of proven bounds on the sample size of certain
Monte Carlo methods. We describe an algorithm that becomes more efficient in a
controlled way as the uncertainty in the tolerance increases, and demonstrate
this in the context of some particular applications of inverse problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5951</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5951</id><created>2014-08-25</created><updated>2015-06-30</updated><authors><author><keyname>Hota</keyname><forenames>Ashish R.</forenames></author><author><keyname>Garg</keyname><forenames>Siddharth</forenames></author><author><keyname>Sundaram</keyname><forenames>Shreyas</forenames></author></authors><title>Fragility of the Commons under Prospect-Theoretic Risk Attitudes</title><categories>cs.GT q-fin.EC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a common-pool resource game where the resource experiences failure
with a probability that grows with the aggregate investment in the resource. To
capture decision making under such uncertainty, we model each player's risk
preference according to the value function from prospect theory. We show the
existence and uniqueness of a pure strategy Nash equilibrium when the players
have arbitrary (potentially heterogeneous) risk preferences and under natural
assumptions on the rate of return and failure probability of the resource.
Greater competition, vis-a-vis the number of players, increases the failure
probability at the Nash equilibrium; we quantify this effect by obtaining tight
upper bounds on the ratio of the failure probability at the equilibrium when
the resource is shared among multiple players (common good) to the failure
probability under investment by a single user (private good). We further
examine the effects of heterogeneity in risk preferences of the players with
respect to two characteristics of the prospect-theoretic value function: loss
aversion and diminishing sensitivity. Heterogeneity in attitudes towards loss
aversion always leads to higher failure probability of the resource at the
equilibrium when compared to the case where players have identical risk
preferences, whereas there is no clear trend under heterogeneity in the
diminishing sensitivity parameter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5955</identifier>
 <datestamp>2014-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5955</id><created>2014-08-25</created><authors><author><keyname>Ben-Amram</keyname><forenames>Amir M.</forenames></author></authors><title>The Hardness of Finding Linear Ranking Functions for Lasso Programs</title><categories>cs.LO cs.CC</categories><comments>In Proceedings GandALF 2014, arXiv:1408.5560. I thank the organizers
  of the Dagstuhl Seminar 14141, &quot;Reachability Problems for Infinite-State
  Systems&quot;, for the opportunity to present an early draft of this work</comments><proxy>EPTCS</proxy><acm-class>F2.0; F3.1; F4.1</acm-class><journal-ref>EPTCS 161, 2014, pp. 32-45</journal-ref><doi>10.4204/EPTCS.161.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Finding whether a linear-constraint loop has a linear ranking function is an
important key to understanding the loop behavior, proving its termination and
establishing iteration bounds. If no preconditions are provided, the decision
problem is known to be in coNP when variables range over the integers and in
PTIME for the rational numbers, or real numbers. Here we show that deciding
whether a linear-constraint loop with a precondition, specifically with
partially-specified input, has a linear ranking function is EXPSPACE-hard over
the integers, and PSPACE-hard over the rationals. The precise complexity of
these decision problems is yet unknown. The EXPSPACE lower bound is derived
from the reachability problem for Petri nets (equivalently, Vector Addition
Systems), and possibly indicates an even stronger lower bound (subject to open
problems in VAS theory). The lower bound for the rationals follows from a novel
simulation of Boolean programs. Lower bounds are also given for the problem of
deciding if a linear ranking-function supported by a particular form of
inductive invariant exists. For loops over integers, the problem is PSPACE-hard
for convex polyhedral invariants and EXPSPACE-hard for downward-closed sets of
natural numbers as invariants.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5956</identifier>
 <datestamp>2014-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5956</id><created>2014-08-25</created><authors><author><keyname>Wurm</keyname><forenames>Christian</forenames></author></authors><title>Kleene Algebras, Regular Languages and Substructural Logics</title><categories>cs.LO cs.FL</categories><comments>In Proceedings GandALF 2014, arXiv:1408.5560</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 161, 2014, pp. 46-59</journal-ref><doi>10.4204/EPTCS.161.7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the two substructural propositional logics KL, KL+, which use
disjunction, fusion and a unary, (quasi-)exponential connective. For both we
prove strong completeness with respect to the interpretation in Kleene algebras
and a variant thereof. We also prove strong completeness for language models,
where each logic comes with a different interpretation. We show that for both
logics the cut rule is admissible and both have a decidable consequence
relation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5957</identifier>
 <datestamp>2014-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5957</id><created>2014-08-25</created><authors><author><keyname>Faymonville</keyname><forenames>Peter</forenames><affiliation>Saarland University</affiliation></author><author><keyname>Zimmermann</keyname><forenames>Martin</forenames><affiliation>Saarland University</affiliation></author></authors><title>Parametric Linear Dynamic Logic</title><categories>cs.LO</categories><comments>In Proceedings GandALF 2014, arXiv:1408.5560</comments><proxy>EPTCS</proxy><acm-class>F.4.1</acm-class><journal-ref>EPTCS 161, 2014, pp. 60-73</journal-ref><doi>10.4204/EPTCS.161.8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce Parametric Linear Dynamic Logic (PLDL), which extends Linear
Dynamic Logic (LDL) by temporal operators equipped with parameters that bound
their scope. LDL was proposed as an extension of Linear Temporal Logic (LTL)
that is able to express all $\omega$-regular specifications while still
maintaining many of LTL's desirable properties like an intuitive syntax and a
translation into non-deterministic B\&quot;uchi automata of exponential size. But
LDL lacks capabilities to express timing constraints. By adding parameterized
operators to LDL, we obtain a logic that is able to express all
$\omega$-regular properties and that subsumes parameterized extensions of LTL
like Parametric LTL and PROMPT-LTL. Our main technical contribution is a
translation of PLDL formulas into non-deterministic B\&quot;uchi word automata of
exponential size via alternating automata. This yields a PSPACE model checking
algorithm and a realizability algorithm with doubly-exponential running time.
Furthermore, we give tight upper and lower bounds on optimal parameter values
for both problems. These results show that PLDL model checking and
realizability are not harder than LTL model checking and realizability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5958</identifier>
 <datestamp>2014-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5958</id><created>2014-08-25</created><authors><author><keyname>Enea</keyname><forenames>Constantin</forenames></author><author><keyname>Habermehl</keyname><forenames>Peter</forenames></author><author><keyname>Inverso</keyname><forenames>Omar</forenames></author><author><keyname>Parlato</keyname><forenames>Gennaro</forenames></author></authors><title>On the Path-Width of Integer Linear Programming</title><categories>cs.LO cs.CC cs.FL</categories><comments>In Proceedings GandALF 2014, arXiv:1408.5560</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 161, 2014, pp. 74-87</journal-ref><doi>10.4204/EPTCS.161.9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the feasibility problem of integer linear programming (ILP). We
show that solutions of any ILP instance can be naturally represented by an
FO-definable class of graphs. For each solution there may be many graphs
representing it. However, one of these graphs is of path-width at most 2n,
where n is the number of variables in the instance. Since FO is decidable on
graphs of bounded path- width, we obtain an alternative decidability result for
ILP. The technique we use underlines a common principle to prove decidability
which has previously been employed for automata with auxiliary storage. We also
show how this new result links to automata theory and program verification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5959</identifier>
 <datestamp>2014-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5959</id><created>2014-08-25</created><authors><author><keyname>L&#xf6;ding</keyname><forenames>Christof</forenames><affiliation>RWTH Aachen University</affiliation></author><author><keyname>Winter</keyname><forenames>Sarah</forenames><affiliation>RWTH Aachen University</affiliation></author></authors><title>Synthesis of Deterministic Top-down Tree Transducers from Automatic Tree
  Relations</title><categories>cs.FL</categories><comments>In Proceedings GandALF 2014, arXiv:1408.5560</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 161, 2014, pp. 88-101</journal-ref><doi>10.4204/EPTCS.161.10</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the synthesis of deterministic tree transducers from automaton
definable specifications, given as binary relations, over finite trees. We
consider the case of specifications that are deterministic top-down tree
automatic, meaning the specification is recognizable by a deterministic
top-down tree automaton that reads the two given trees synchronously in
parallel. In this setting we study tree transducers that are allowed to have
either bounded delay or arbitrary delay. Delay is caused whenever the
transducer reads a symbol from the input tree but does not produce output. We
provide decision procedures for both bounded and arbitrary delay that yield
deterministic top-down tree transducers which realize the specification for
valid input trees. Similar to the case of relations over words, we use
two-player games to obtain our results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5960</identifier>
 <datestamp>2014-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5960</id><created>2014-08-25</created><authors><author><keyname>Montanari</keyname><forenames>Angelo</forenames><affiliation>Department of Mathematics and Computer Science University of Udine</affiliation></author><author><keyname>Sala</keyname><forenames>Pietro</forenames><affiliation>Department of Computer Science University of Verona</affiliation></author></authors><title>Interval-based Synthesis</title><categories>cs.LO cs.FL</categories><comments>In Proceedings GandALF 2014, arXiv:1408.5560</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 161, 2014, pp. 102-115</journal-ref><doi>10.4204/EPTCS.161.11</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the synthesis problem for Halpern and Shoham's modal logic of
intervals extended with an equivalence relation over time points, abbreviated
HSeq. In analogy to the case of monadic second-order logic of one successor,
the considered synthesis problem receives as input an HSeq formula phi and a
finite set Sigma of propositional variables and temporal requests, and it
establishes whether or not, for all possible evaluations of elements in Sigma
in every interval structure, there exists an evaluation of the remaining
propositional variables and temporal requests such that the resulting structure
is a model for phi. We focus our attention on decidability of the synthesis
problem for some meaningful fragments of HSeq, whose modalities are drawn from
the set A (meets), Abar (met by), B (begins), Bbar (begun by), interpreted over
finite linear orders and natural numbers. We prove that the fragment ABBbareq
is decidable (non-primitive recursive hard), while the fragment AAbarBBbar
turns out to be undecidable. In addition, we show that even the synthesis
problem for ABBbar becomes undecidable if we replace finite linear orders by
natural numbers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5961</identifier>
 <datestamp>2014-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5961</id><created>2014-08-25</created><authors><author><keyname>Bruse</keyname><forenames>Florian</forenames><affiliation>Universit&#xe4;t Kassel</affiliation></author><author><keyname>Falk</keyname><forenames>Michael</forenames><affiliation>Universit&#xe4;t Kassel</affiliation></author><author><keyname>Lange</keyname><forenames>Martin</forenames><affiliation>Universit&#xe4;t Kassel</affiliation></author></authors><title>The Fixpoint-Iteration Algorithm for Parity Games</title><categories>cs.LO</categories><comments>In Proceedings GandALF 2014, arXiv:1408.5560</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 161, 2014, pp. 116-130</journal-ref><doi>10.4204/EPTCS.161.12</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is known that the model checking problem for the modal mu-calculus reduces
to the problem of solving a parity game and vice-versa. The latter is realised
by the Walukiewicz formulas which are satisfied by a node in a parity game iff
player 0 wins the game from this node. Thus, they define her winning region,
and any model checking algorithm for the modal mu-calculus, suitably
specialised to the Walukiewicz formulas, yields an algorithm for solving parity
games. In this paper we study the effect of employing the most straight-forward
mu-calculus model checking algorithm: fixpoint iteration. This is also one of
the few algorithms, if not the only one, that were not originally devised for
parity game solving already. While an empirical study quickly shows that this
does not yield an algorithm that works well in practice, it is interesting from
a theoretical point for two reasons: first, it is exponential on virtually all
families of games that were designed as lower bounds for very particular
algorithms suggesting that fixpoint iteration is connected to all those.
Second, fixpoint iteration does not compute positional winning strategies. Note
that the Walukiewicz formulas only define winning regions; some additional work
is needed in order to make this algorithm compute winning strategies. We show
that these are particular exponential-space strategies which we call
eventually-positional, and we show how positional ones can be extracted from
them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5962</identifier>
 <datestamp>2014-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5962</id><created>2014-08-25</created><authors><author><keyname>Delzanno</keyname><forenames>Giorgio</forenames><affiliation>DIBRIS, University of Genova</affiliation></author><author><keyname>Tatarek</keyname><forenames>Michele</forenames><affiliation>DIBRIS, University of Genova</affiliation></author><author><keyname>Traverso</keyname><forenames>Riccardo</forenames><affiliation>FBK, Trento</affiliation></author></authors><title>Model Checking Paxos in Spin</title><categories>cs.LO cs.DC cs.DM</categories><comments>In Proceedings GandALF 2014, arXiv:1408.5560</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 161, 2014, pp. 131-146</journal-ref><doi>10.4204/EPTCS.161.13</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a formal model of a distributed consensus algorithm in the
executable specification language Promela extended with a new type of guards,
called counting guards, needed to implement transitions that depend on majority
voting. Our formalization exploits abstractions that follow from reduction
theorems applied to the specific case-study. We apply the model checker Spin to
automatically validate finite instances of the model and to extract
preconditions on the size of quorums used in the election phases of the
protocol.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5963</identifier>
 <datestamp>2014-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5963</id><created>2014-08-25</created><authors><author><keyname>Kuusisto</keyname><forenames>Antti</forenames></author></authors><title>Infinite Networks, Halting and Local Algorithms</title><categories>cs.DC cs.LO</categories><comments>In Proceedings GandALF 2014, arXiv:1408.5560</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 161, 2014, pp. 147-160</journal-ref><doi>10.4204/EPTCS.161.14</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The immediate past has witnessed an increased amount of interest in local
algorithms, i.e., constant time distributed algorithms. In a recent survey of
the topic (Suomela, ACM Computing Surveys, 2013), it is argued that local
algorithms provide a natural framework that could be used in order to
theoretically control infinite networks in finite time. We study a
comprehensive collection of distributed computing models and prove that if
infinite networks are included in the class of structures investigated, then
every universally halting distributed algorithm is in fact a local algorithm.
To contrast this result, we show that if only finite networks are allowed, then
even very weak distributed computing models can define nonlocal algorithms that
halt everywhere. The investigations in this article continue the studies in the
intersection of logic and distributed computing initiated in (Hella et al.,
PODC 2012) and (Kuusisto, CSL 2013).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5964</identifier>
 <datestamp>2014-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5964</id><created>2014-08-25</created><authors><author><keyname>Jaskolka</keyname><forenames>Jason</forenames><affiliation>McMaster University</affiliation></author><author><keyname>Khedri</keyname><forenames>Ridha</forenames><affiliation>McMaster University</affiliation></author></authors><title>A Formulation of the Potential for Communication Condition using C2KA</title><categories>cs.LO cs.CR</categories><comments>In Proceedings GandALF 2014, arXiv:1408.5560</comments><proxy>EPTCS</proxy><acm-class>D.2.4</acm-class><journal-ref>EPTCS 161, 2014, pp. 161-174</journal-ref><doi>10.4204/EPTCS.161.15</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An integral part of safeguarding systems of communicating agents from covert
channel communication is having the ability to identify when a covert channel
may exist in a given system and which agents are more prone to covert channels
than others. In this paper, we propose a formulation of one of the necessary
conditions for the existence of covert channels: the potential for
communication condition. Then, we discuss when the potential for communication
is preserved after the modification of system agents in a potential
communication path. Our approach is based on the mathematical framework of
Communicating Concurrent Kleene Algebra (C2KA). While existing approaches only
consider the potential for communication via shared environments, the approach
proposed in this paper also considers the potential for communication via
external stimuli.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5965</identifier>
 <datestamp>2014-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5965</id><created>2014-08-25</created><authors><author><keyname>Osada</keyname><forenames>Yuki</forenames><affiliation>The University of Western Australia</affiliation></author><author><keyname>French</keyname><forenames>Tim</forenames><affiliation>The University of Western Australia</affiliation></author><author><keyname>Reynolds</keyname><forenames>Mark</forenames><affiliation>The University of Western Australia</affiliation></author><author><keyname>Smallbone</keyname><forenames>Harry</forenames><affiliation>The University of Western Australia</affiliation></author></authors><title>Hourglass Automata</title><categories>cs.FL</categories><comments>In Proceedings GandALF 2014, arXiv:1408.5560</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 161, 2014, pp. 175-188</journal-ref><doi>10.4204/EPTCS.161.16</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we define the class of hourglass automata, which are timed
automata with bounded clocks that can be made to progress backwards as well as
forwards at a constant rate. We then introduce a new clock update for timed
automata that allows hourglass automata to be expressed. This allows us to show
that language emptiness remains decidable with this update when the number of
clocks is two or less. This is done by showing that we can construct a finite
untimed graph using clock regions from any timed automaton that use this new
update.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5966</identifier>
 <datestamp>2014-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5966</id><created>2014-08-25</created><authors><author><keyname>Boiret</keyname><forenames>Adrien</forenames><affiliation>University of Lille 1. Links</affiliation></author><author><keyname>Hugot</keyname><forenames>Vincent</forenames><affiliation>Inria. Links</affiliation></author><author><keyname>Niehren</keyname><forenames>Joachim</forenames><affiliation>Inria. Links</affiliation></author><author><keyname>Treinen</keyname><forenames>Ralf</forenames><affiliation>University Paris Diderot. PPS</affiliation></author></authors><title>Deterministic Automata for Unordered Trees</title><categories>cs.FL</categories><comments>In Proceedings GandALF 2014, arXiv:1408.5560</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 161, 2014, pp. 189-202</journal-ref><doi>10.4204/EPTCS.161.17</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automata for unordered unranked trees are relevant for defining schemas and
queries for data trees in Json or Xml format. While the existing notions are
well-investigated concerning expressiveness, they all lack a proper notion of
determinism, which makes it difficult to distinguish subclasses of automata for
which problems such as inclusion, equivalence, and minimization can be solved
efficiently. In this paper, we propose and investigate different notions of
&quot;horizontal determinism&quot;, starting from automata for unranked trees in which
the horizontal evaluation is performed by finite state automata. We show that a
restriction to confluent horizontal evaluation leads to polynomial-time
emptiness and universality, but still suffers from coNP-completeness of the
emptiness of binary intersections. Finally, efficient algorithms can be
obtained by imposing an order of horizontal evaluation globally for all
automata in the class. Depending on the choice of the order, we obtain
different classes of automata, each of which has the same expressiveness as
CMso.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5967</identifier>
 <datestamp>2014-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5967</id><created>2014-08-25</created><authors><author><keyname>Bresolin</keyname><forenames>Davide</forenames><affiliation>University of Bologna</affiliation></author><author><keyname>El-Fakih</keyname><forenames>Khaled</forenames><affiliation>American University of Sharjah</affiliation></author><author><keyname>Villa</keyname><forenames>Tiziano</forenames><affiliation>University of Verona</affiliation></author><author><keyname>Yevtushenko</keyname><forenames>Nina</forenames><affiliation>Tomsk State University</affiliation></author></authors><title>Deterministic Timed Finite State Machines: Equivalence Checking and
  Expressive Power</title><categories>cs.FL</categories><comments>In Proceedings GandALF 2014, arXiv:1408.5560</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 161, 2014, pp. 203-216</journal-ref><doi>10.4204/EPTCS.161.18</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There has been a growing interest in defining models of automata enriched
with time. For instance, timed automata were introduced as automata extended
with clocks. In this paper, we study models of timed finite state machines
(TFSMs), i.e., FSMs enriched with time, which accept timed input words and
generate timed output words. Here we discuss some models of TFSMs with a single
clock: TFSMs with timed guards, TFSMs with timeouts, and TFSMs with both timed
guards and timeouts. We solve the problem of equivalence checking for all three
models, and we compare their expressive power, characterizing subclasses of
TFSMs with timed guards and of TFSMs with timeouts that are equivalent to each
other.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5968</identifier>
 <datestamp>2014-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5968</id><created>2014-08-25</created><authors><author><keyname>Krishna</keyname><forenames>Shankara Narayanan</forenames><affiliation>IIT Bombay</affiliation></author><author><keyname>Manasa</keyname><forenames>Lakshmi</forenames><affiliation>IIT Bombay</affiliation></author><author><keyname>Trivedi</keyname><forenames>Ashutosh</forenames><affiliation>IIT Bombay</affiliation></author></authors><title>Improved Undecidability Results for Reachability Games on Recursive
  Timed Automata</title><categories>cs.FL cs.LO</categories><comments>In Proceedings GandALF 2014, arXiv:1408.5560</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 161, 2014, pp. 245-259</journal-ref><doi>10.4204/EPTCS.161.21</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study reachability games on recursive timed automata (RTA) that generalize
Alur-Dill timed automata with recursive procedure invocation mechanism similar
to recursive state machines. It is known that deciding the winner in
reachability games on RTA is undecidable for automata with two or more clocks,
while the problem is decidable for automata with only one clock. Ouaknine and
Worrell recently proposed a time-bounded theory of real-time verification by
claiming that restriction to bounded-time recovers decidability for several key
decision problem related to real-time verification. We revisited games on
recursive timed automata with time-bounded restriction in the hope of
recovering decidability. However, we found that the problem still remains
undecidable for recursive timed automata with three or more clocks. Using
similar proof techniques we characterize a decidability frontier for a
generalization of RTA to recursive stopwatch automata.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5969</identifier>
 <datestamp>2014-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5969</id><created>2014-08-25</created><authors><author><keyname>De Crescenzo</keyname><forenames>Ilaria</forenames></author><author><keyname>La Torre</keyname><forenames>Salvatore</forenames></author><author><keyname>Velner</keyname><forenames>Yaron</forenames></author></authors><title>Visibly Pushdown Modular Games</title><categories>cs.LO cs.CC cs.FL cs.GT</categories><comments>In Proceedings GandALF 2014, arXiv:1408.5560</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 161, 2014, pp. 260-274</journal-ref><doi>10.4204/EPTCS.161.22</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Games on recursive game graphs can be used to reason about the control flow
of sequential programs with recursion. In games over recursive game graphs, the
most natural notion of strategy is the modular strategy, i.e., a strategy that
is local to a module and is oblivious to previous module invocations, and thus
does not depend on the context of invocation. In this work, we study for the
first time modular strategies with respect to winning conditions that can be
expressed by a pushdown automaton.
  We show that such games are undecidable in general, and become decidable for
visibly pushdown automata specifications.
  Our solution relies on a reduction to modular games with finite-state
automata winning conditions, which are known in the literature.
  We carefully characterize the computational complexity of the considered
decision problem. In particular, we show that modular games with a universal
Buchi or co Buchi visibly pushdown winning condition are EXPTIME-complete, and
when the winning condition is given by a CARET or NWTL temporal logic formula
the problem is 2EXPTIME-complete, and it remains 2EXPTIME-hard even for simple
fragments of these logics.
  As a further contribution, we present a different solution for modular games
with finite-state automata winning condition that runs faster than known
solutions for large specifications and many exits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5971</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5971</id><created>2014-08-25</created><updated>2015-05-09</updated><authors><author><keyname>Kuzuoka</keyname><forenames>Shigeaki</forenames></author><author><keyname>Watanabe</keyname><forenames>Shun</forenames></author></authors><title>A Dichotomy of Functions in Distributed Coding: An Information Spectral
  Approach</title><categories>cs.IT math.IT</categories><comments>29 pages, 4 figures. In v2, results in Section 3.D are added. In v3,
  a terminology is changed. In v4, a typo is fixed</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of distributed data compression for function computation is
considered, where (i) the function to be computed is not necessarily
symbol-wise function and (ii) the information source has memory and may not be
stationary nor ergodic. We introduce the class of smooth sources and give a
sufficient condition on functions so that the achievable rate region for
computing coincides with the Slepian-Wolf region (i.e., the rate region for
reproducing the entire source) for any smooth sources. Moreover, for
symbol-wise functions, the necessary and sufficient condition for the
coincidence is established. Our result for the full side-information case is a
generalization of the result by Ahlswede and Csiszar to sources with memory;
our dichotomy theorem is different from Han and Kobayashi's dichotomy theorem,
which reveals an effect of memory in distributed function computation. All
results are given not only for fixed-length coding but also for variable-length
coding in a unified manner. Furthermore, for the full side-information case,
the error probability in the moderate deviation regime is also investigated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5977</identifier>
 <datestamp>2014-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5977</id><created>2014-08-25</created><authors><author><keyname>Ghilezan</keyname><forenames>Silvia</forenames><affiliation>Univerzitet u Novom Sadu, Serbia</affiliation></author><author><keyname>Jak&#x161;i&#x107;</keyname><forenames>Svetlana</forenames><affiliation>Univerzitet u Novom Sadu, Serbia</affiliation></author><author><keyname>Pantovi&#x107;</keyname><forenames>Jovanka</forenames><affiliation>Univerzitet u Novom Sadu, Serbia</affiliation></author><author><keyname>P&#xe9;rez</keyname><forenames>Jorge A.</forenames><affiliation>University of Groningen, The Netherlands</affiliation></author><author><keyname>Vieira</keyname><forenames>Hugo Torres</forenames><affiliation>LaSIGE, Faculdade de Ci&#xea;ncias, Universidade de Lisboa, Portugal</affiliation></author></authors><title>Dynamic Role Authorization in Multiparty Conversations</title><categories>cs.LO cs.PL</categories><comments>In Proceedings BEAT 2014, arXiv:1408.5564</comments><proxy>EPTCS</proxy><acm-class>D.3.1; F.3.2</acm-class><journal-ref>EPTCS 162, 2014, pp. 1-8</journal-ref><doi>10.4204/EPTCS.162.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Protocol specifications often identify the roles involved in communications.
In multiparty protocols that involve task delegation it is often useful to
consider settings in which different sites may act on behalf of a single role.
It is then crucial to control the roles that the different parties are
authorized to represent, including the case in which role authorizations are
determined only at runtime. Building on previous work on conversation types
with flexible role assignment, here we report initial results on a typed
framework for the analysis of multiparty communications with dynamic role
authorization and delegation. In the underlying process model, communication
prefixes are annotated with role authorizations and authorizations can be
passed around. We extend the conversation type system so as to statically
distinguish processes that never incur in authorization errors. The proposed
static discipline guarantees that processes are always authorized to
communicate on behalf of an intended role, also covering the case in which
authorizations are dynamically passed around in messages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5978</identifier>
 <datestamp>2014-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5978</id><created>2014-08-25</created><authors><author><keyname>Castellani</keyname><forenames>Ilaria</forenames><affiliation>INRIA Sophia-Antipolis, France</affiliation></author><author><keyname>Dezani-Ciancaglini</keyname><forenames>Mariangiola</forenames><affiliation>Universit&#xe0; di Torino, Italy</affiliation></author><author><keyname>P&#xe9;rez</keyname><forenames>Jorge A.</forenames><affiliation>University of Groningen, The Netherlands</affiliation></author></authors><title>Self-Adaptation and Secure Information Flow in Multiparty Structured
  Communications: A Unified Perspective</title><categories>cs.LO cs.CR cs.PL</categories><comments>In Proceedings BEAT 2014, arXiv:1408.5564</comments><proxy>EPTCS</proxy><acm-class>D.3.1; F.3.2</acm-class><journal-ref>EPTCS 162, 2014, pp. 9-18</journal-ref><doi>10.4204/EPTCS.162.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present initial results on a comprehensive model of structured
communications, in which self- adaptation and security concerns are jointly
addressed. More specifically, we propose a model of self-adaptive, multiparty
communications with secure information flow guarantees. In this model, security
violations occur when processes attempt to read or write messages of
inappropriate security levels within directed exchanges. Such violations
trigger adaptation mechanisms that prevent the violations to occur and/or to
propagate their effect in the choreography. Our model is equipped with local
and global mechanisms for reacting to security violations; type soundness
results ensure that global protocols are still correctly executed, while the
system adapts itself to preserve security.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5979</identifier>
 <datestamp>2014-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5979</id><created>2014-08-25</created><authors><author><keyname>Neykova</keyname><forenames>Rumyana</forenames><affiliation>Imperial College London</affiliation></author><author><keyname>Bocchi</keyname><forenames>Laura</forenames><affiliation>Imperial College London</affiliation></author><author><keyname>Yoshida</keyname><forenames>Nobuko</forenames><affiliation>Imperial College London</affiliation></author></authors><title>Timed Runtime Monitoring for Multiparty Conversations</title><categories>cs.DC cs.SE</categories><comments>In Proceedings BEAT 2014, arXiv:1408.5564</comments><proxy>EPTCS</proxy><acm-class>verification</acm-class><journal-ref>EPTCS 162, 2014, pp. 19-26</journal-ref><doi>10.4204/EPTCS.162.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a dynamic verification framework for protocols in real-time
distributed systems. The framework is based on Scribble, a tool-chain for
design and verification of choreographies based on multiparty session types,
developed with our industrial partners. Drawing from recent work on multiparty
session types for real-time interactions, we extend Scribble with clocks,
resets, and clock predicates constraining the times in which interactions
should occur. We present a timed API for Python to program distributed
implementations of Scribble specifications. A dynamic verification framework
ensures the safe execution of applications written with our timed API: we have
implemented dedicated runtime monitors that check that each interaction occurs
at a correct timing with respect to the corresponding Scribble specification.
The performance of our implementation and its practicability are analysed via
benchmarking.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5980</identifier>
 <datestamp>2014-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5980</id><created>2014-08-25</created><authors><author><keyname>Dardha</keyname><forenames>Ornela</forenames><affiliation>School of Computing Science, University of Glasgow</affiliation></author></authors><title>Recursive Session Types Revisited</title><categories>cs.PL</categories><comments>In Proceedings BEAT 2014, arXiv:1408.5564</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 162, 2014, pp. 27-34</journal-ref><doi>10.4204/EPTCS.162.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Session types model structured communication-based programming. In
particular, binary session types for the pi-calculus describe communication
between exactly two participants in a distributed scenario. Adding sessions to
the pi-calculus means augmenting it with type and term constructs. In a
previous paper, we tried to understand to which extent the session constructs
are more complex and expressive than the standard pi-calculus constructs. Thus,
we presented an encoding of binary session pi-calculus to the standard typed
pi-calculus by adopting linear and variant types and the continuation-passing
principle. In the present paper, we focus on recursive session types and we
present an encoding into recursive linear pi-calculus. This encoding is a
conservative extension of the former in that it preserves the results therein
obtained. Most importantly, it adopts a new treatment of the duality relation,
which in the presence of recursive types has been proven to be quite
challenging.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5981</identifier>
 <datestamp>2014-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5981</id><created>2014-08-25</created><authors><author><keyname>Barbanera</keyname><forenames>Franco</forenames><affiliation>Dipartimento di Matematica e Informatica, University of Catania</affiliation></author><author><keyname>Dezani-Ciancaglini</keyname><forenames>Mariangiola</forenames><affiliation>Dipartimento di Informatica, University of Torino</affiliation></author><author><keyname>Liguoro</keyname><forenames>Ugo de'</forenames><affiliation>Dipartimento di Informatica, University of Torino</affiliation></author></authors><title>Compliance for reversible client/server interactions</title><categories>cs.LO</categories><comments>In Proceedings BEAT 2014, arXiv:1408.5564</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 162, 2014, pp. 35-42</journal-ref><doi>10.4204/EPTCS.162.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the setting of session behaviours, we study an extension of the concept of
compliance when a disciplined form of backtracking is present. After adding
checkpoints to the syntax of session behaviours, we formalise the operational
semantics via a LTS, and define a natural notion of checkpoint compliance. We
then obtain a co-inductive characterisation of such compliance relation, and an
axiomatic presentation that is proved to be sound and complete. As a byproduct
we get a decision procedure for the new compliance, being the axiomatic system
algorithmic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5987</identifier>
 <datestamp>2015-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5987</id><created>2014-08-25</created><updated>2015-09-01</updated><authors><author><keyname>Polyakovskiy</keyname><forenames>Sergey</forenames></author><author><keyname>Berghammer</keyname><forenames>Rudolf</forenames></author><author><keyname>Neumann</keyname><forenames>Frank</forenames></author></authors><title>Solving Hard Control Problems in Voting Systems via Integer Programming</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Voting problems are central in the area of social choice. In this article, we
investigate various voting systems and types of control of elections. We
present integer linear programming (ILP) formulations for a wide range of
NP-hard control problems. Our ILP formulations are flexible in the sense that
they can work with an arbitrary number of candidates and voters. Using the
off-the-shelf solver Cplex, we show that our approaches can manipulate
elections with a large number of voters and candidates efficiently.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5990</identifier>
 <datestamp>2015-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5990</id><created>2014-08-25</created><authors><author><keyname>Liu</keyname><forenames>Jingchu</forenames></author><author><keyname>Zhou</keyname><forenames>Sheng</forenames></author><author><keyname>Gong</keyname><forenames>Jie</forenames></author><author><keyname>Niu</keyname><forenames>Zhisheng</forenames></author><author><keyname>Xu</keyname><forenames>Shugong</forenames></author></authors><title>On the Statistical Multiplexing Gain of Virtual Base Station Pools</title><categories>cs.IT cs.NI math.IT</categories><comments>Accepted by GlobeCom'14</comments><doi>10.1109/GLOCOM.2014.7037148</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Facing the explosion of mobile data traffic, cloud radio access network
(C-RAN) is proposed recently to overcome the efficiency and flexibility
problems with the traditional RAN architecture by centralizing baseband
processing. However, there lacks a mathematical model to analyze the
statistical multiplexing gain from the pooling of virtual base stations (VBSs)
so that the expenditure on fronthaul networks can be justified. In this paper,
we address this problem by capturing the session-level dynamics of VBS pools
with a multi-dimensional Markov model. This model reflects the constraints
imposed by both radio resources and computational resources. To evaluate the
pooling gain, we derive a product-form solution for the stationary distribution
and give a recursive method to calculate the blocking probabilities. For
comparison, we also derive the limit of resource utilization ratio as the pool
size approaches infinity. Numerical results show that VBS pools can obtain
considerable pooling gain readily at medium size, but the convergence to large
pool limit is slow because of the quickly diminishing marginal pooling gain. We
also find that parameters such as traffic load and desired Quality of Service
(QoS) have significant influence on the performance of VBS pools.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5995</identifier>
 <datestamp>2014-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5995</id><created>2014-08-25</created><authors><author><keyname>Li</keyname><forenames>Minming</forenames></author><author><keyname>Yao</keyname><forenames>Frances F.</forenames></author><author><keyname>Yuan</keyname><forenames>Hao</forenames></author></authors><title>An $O(n^2)$ Algorithm for Computing Optimal Continuous Voltage Schedules</title><categories>cs.DS</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dynamic Voltage Scaling techniques allow the processor to set its speed
dynamically in order to reduce energy consumption. In the continuous model, the
processor can run at any speed, while in the discrete model, the processor can
only run at finite number of speeds given as input. The current best algorithm
for computing the optimal schedules for the continuous model runs at $O(n^2\log
n)$ time for scheduling $n$ jobs. In this paper, we improve the running time to
$O(n^2)$ by speeding up the calculation of s-schedules using a more refined
data structure. For the discrete model, we improve the computation of the
optimal schedule from the current best $O(dn\log n)$ to $O(n\log \max\{d,n\})$
where $d$ is the number of allowed speeds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.5999</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.5999</id><created>2014-08-26</created><updated>2014-11-01</updated><authors><author><keyname>Su</keyname><forenames>Shenghui</forenames></author><author><keyname>Xie</keyname><forenames>Tao</forenames></author><author><keyname>Lv</keyname><forenames>Shuwang</forenames></author></authors><title>A New Non-MDS Hash Function Resisting Birthday Attack and
  Meet-in-the-middle Attack</title><categories>cs.CR</categories><comments>18 Pages</comments><acm-class>D.4.6; E.3; F.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To be paired with a lightweight digital signing scheme of which the modulus
length is between 80 and 160 bits, a new non-Merkle-Damgard structure (non-MDS)
hash function is proposed by the authors based on a multivariate permutation
problem (MPP) and an anomalous subset product problem (ASPP) to which no
subexponential time solutions are found so far. It includes an initialization
algorithm and a compression algorithm, and converts a short message of n bits
treated as only a block into a digest of m bits, where 80 &lt;= m &lt;= 232 and 80 &lt;=
m &lt;= n &lt;= 4096. Analysis shows that the new hash is one-way, weakly
collision-free, and strongly collision-free along with a proof, and its
security against existent attacks such as birthday attack and
meet-in-the-middle attack gets the O(2^m) magnitude. Running time of its
compression algorithm is analyzed to be O(n*m^2) bit operations. A comparison
with the Chaum-Heijst-Pfitzmann hash based on a discrete logarithm problem is
made. Especially, the new hash with short input and small computation may be
used to reform a classical hash with an m-bit output and an O(2^(m/2))
magnitude security into a compact hash with an m/2-bit output and the same
security. Thus, it opens a door to convenience for utilization of lightweight
digital signing schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6009</identifier>
 <datestamp>2015-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6009</id><created>2014-08-26</created><updated>2015-07-21</updated><authors><author><keyname>Lee</keyname><forenames>Byungju</forenames></author><author><keyname>Choi</keyname><forenames>Junil</forenames></author><author><keyname>Seol</keyname><forenames>Ji-yun</forenames></author><author><keyname>Love</keyname><forenames>David J.</forenames></author><author><keyname>Shim</keyname><forenames>Byonghyo</forenames></author></authors><title>Antenna Grouping based Feedback Compression for FDD-based Massive MIMO
  Systems</title><categories>cs.IT math.IT</categories><comments>13 pages, 14 figures, to appear in IEEE Transactions on
  Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent works on massive multiple-input multiple-output (MIMO) have shown that
a potential breakthrough in capacity gains can be achieved by deploying a very
large number of antennas at the basestation. In order to achieve the
performance that massive MIMO systems promise, accurate transmit-side channel
state information (CSI) should be available at the basestation. While
transmit-side CSI can be obtained by employing channel reciprocity in time
division duplexing (TDD) systems, explicit feedback of CSI from the user
terminal to the basestation is needed for frequency division duplexing (FDD)
systems. In this paper, we propose an antenna grouping based feedback reduction
technique for FDD-based massive MIMO systems. The proposed algorithm, dubbed
antenna group beamforming (AGB), maps multiple correlated antenna elements to a
single representative value using pre-designed patterns. The proposed method
modifies the feedback packet by introducing the concept of a header to select a
suitable group pattern and a payload to quantize the reduced dimension channel
vector. Simulation results show that the proposed method achieves significant
feedback overhead reduction over conventional approach performing the vector
quantization of whole channel vector under the same target sum rate
requirement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6010</identifier>
 <datestamp>2014-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6010</id><created>2014-08-26</created><authors><author><keyname>Clark</keyname><forenames>Gradeigh D.</forenames></author><author><keyname>Lindqvist</keyname><forenames>Janne</forenames></author></authors><title>Engineering Gesture-Based Authentication Systems</title><categories>cs.CR cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gestures are a topic of increasing interest in authentication and successful
implementation as a security layer requires reliable gesture recognition. So
far much work focuses on new ways to recognize gestures, leaving discussion on
the viability of recognition in an authentication scheme to the background.
  It is unclear how recognition should be deployed for practical and robust
real-world authentication. In this article, we analyze the effectiveness of
different approaches to recognizing gestures and the potential for use in
secure gesture-based authentication systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6011</identifier>
 <datestamp>2015-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6011</id><created>2014-08-26</created><updated>2015-03-26</updated><authors><author><keyname>Zhuang</keyname><forenames>Binnan</forenames></author><author><keyname>Guo</keyname><forenames>Dongning</forenames></author><author><keyname>Honig</keyname><forenames>Michael L.</forenames></author></authors><title>Traffic-Driven Spectrum Allocation in Heterogeneous Networks</title><categories>cs.IT math.IT</categories><comments>13 pages, 11 figures, accepted for publication by JSAC-HCN</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Next generation cellular networks will be heterogeneous with dense deployment
of small cells in order to deliver high data rate per unit area. Traffic
variations are more pronounced in a small cell, which in turn lead to more
dynamic interference to other cells. It is crucial to adapt radio resource
management to traffic conditions in such a heterogeneous network (HetNet). This
paper studies the optimization of spectrum allocation in HetNets on a
relatively slow timescale based on average traffic and channel conditions
(typically over seconds or minutes). Specifically, in a cluster with $n$ base
transceiver stations (BTSs), the optimal partition of the spectrum into $2^n$
segments is determined, corresponding to all possible spectrum reuse patterns
in the downlink. Each BTS's traffic is modeled using a queue with Poisson
arrivals, the service rate of which is a linear function of the combined
bandwidth of all assigned spectrum segments. With the system average packet
sojourn time as the objective, a convex optimization problem is first
formulated, where it is shown that the optimal allocation divides the spectrum
into at most $n$ segments. A second, refined model is then proposed to address
queue interactions due to interference, where the corresponding optimal
allocation problem admits an efficient suboptimal solution. Both allocation
schemes attain the entire throughput region of a given network. Simulation
results show the two schemes perform similarly in the heavy-traffic regime, in
which case they significantly outperform both the orthogonal allocation and the
full-frequency-reuse allocation. The refined allocation shows the best
performance under all traffic conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6012</identifier>
 <datestamp>2014-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6012</id><created>2014-08-26</created><updated>2014-09-04</updated><authors><author><keyname>Yoshikawa</keyname><forenames>Yuya</forenames></author><author><keyname>Iwata</keyname><forenames>Tomoharu</forenames></author><author><keyname>Sawada</keyname><forenames>Hiroshi</forenames></author></authors><title>Collaboration on Social Media: Analyzing Successful Projects on Social
  Coding</title><categories>cs.SI cs.SE physics.soc-ph</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social Coding Sites (SCSs) are social media services for sharing software
development projects on the Web, and many open source projects are currently
being developed on SCSs. One of the characteristics of SCSs is that they
provide a platform on social networks that encourages collaboration between
developers with the same interests and purpose. For example, external
developers can easily report bugs and improvements to the project members. In
this paper, we investigate keys to the success of projects on SCSs based on
large data consisting of more than three hundred thousand projects. We focus on
the following three perspectives: 1) the team structure, 2) social activity
with external developers, and 3) content developed by the project. To evaluate
the success quantitatively, we define activity, popularity and sociality as
success indexes. A summary of the findings we obtained by using the techniques
of correlation analysis, social network analysis and topic extraction is as
follows: the number of project members and the connectivity between the members
are positively correlated with success indexes. Second, projects that
faithfully tackle change requests from external developers are more likely to
be successful. Third, the success indexes differ between topics of softwares
developed by projects. Our analysis suggests how to be successful in various
projects, not limited to social coding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6019</identifier>
 <datestamp>2014-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6019</id><created>2014-08-26</created><authors><author><keyname>Athenst&#xe4;dt</keyname><forenames>Jan Christoph</forenames></author><author><keyname>Hartmann</keyname><forenames>Tanja</forenames></author><author><keyname>N&#xf6;llenburg</keyname><forenames>Martin</forenames></author></authors><title>Simultaneous Embeddability of Two Partitions</title><categories>cs.CG</categories><comments>17 pages, 7 figures, extended version of a paper to appear at GD 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the simultaneous embeddability of a pair of partitions of the same
underlying set into disjoint blocks. Each element of the set is mapped to a
point in the plane and each block of either of the two partitions is mapped to
a region that contains exactly those points that belong to the elements in the
block and that is bounded by a simple closed curve. We establish three main
classes of simultaneous embeddability (weak, strong, and full embeddability)
that differ by increasingly strict well-formedness conditions on how different
block regions are allowed to intersect. We show that these simultaneous
embeddability classes are closely related to different planarity concepts of
hypergraphs. For each embeddability class we give a full characterization. We
show that (i) every pair of partitions has a weak simultaneous embedding, (ii)
it is NP-complete to decide the existence of a strong simultaneous embedding,
and (iii) the existence of a full simultaneous embedding can be tested in
linear time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6027</identifier>
 <datestamp>2014-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6027</id><created>2014-08-26</created><authors><author><keyname>Geng</keyname><forenames>Xin</forenames></author><author><keyname>Zhao</keyname><forenames>Quan</forenames></author></authors><title>Label Distribution Learning</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although multi-label learning can deal with many problems with label
ambiguity, it does not fit some real applications well where the overall
distribution of the importance of the labels matters. This paper proposes a
novel learning paradigm named \emph{label distribution learning} (LDL) for such
kind of applications. The label distribution covers a certain number of labels,
representing the degree to which each label describes the instance. LDL is a
more general learning framework which includes both single-label and
multi-label learning as its special cases. This paper proposes six working LDL
algorithms in three ways: problem transformation, algorithm adaptation, and
specialized algorithm design. In order to compare their performance, six
evaluation measures are suggested for LDL algorithms, and the first batch of
label distribution datasets are collected and made publicly available.
Experimental results on one artificial and two real-world datasets show clear
advantage of the specialized algorithms, which indicates the importance of
special design for the characteristics of the LDL problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6030</identifier>
 <datestamp>2014-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6030</id><created>2014-08-26</created><authors><author><keyname>Lin</keyname><forenames>Jian-Hong</forenames></author><author><keyname>Liu</keyname><forenames>Jian-Guo</forenames></author><author><keyname>Guo</keyname><forenames>Qiang</forenames></author></authors><title>A general method for identifying node spreading influence via the
  adjacent matrix and spreading rate</title><categories>physics.soc-ph cs.SI physics.data-an</categories><comments>5 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With great theoretical and practical significance, identifying the node
spreading influence of complex network is one of the most promising domains. So
far, various topology-based centrality measures have been proposed to identify
the node spreading influence in a network. However, the node spreading
influence is a result of the interplay between the network topology structure
and spreading dynamics. In this paper, we build up the systematic method by
combining the network structure and spreading dynamics to identify the node
spreading influence. By combining the adjacent matrix $A$ and spreading
parameter $\beta$, we theoretical give the node spreading influence with the
eigenvector of the largest eigenvalue. Comparing with the
Susceptible-Infected-Recovered (SIR) model epidemic results for four real
networks, our method could identify the node spreading influence more
accurately than the ones generated by the degree, K-shell and eigenvector
centrality. This work may provide a systematic method for identifying node
spreading influence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6032</identifier>
 <datestamp>2014-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6032</id><created>2014-08-26</created><authors><author><keyname>Korsunsky</keyname><forenames>Ilya</forenames></author><author><keyname>Ramazzotti</keyname><forenames>Daniele</forenames></author><author><keyname>Caravagna</keyname><forenames>Giulio</forenames></author><author><keyname>Mishra</keyname><forenames>Bud</forenames></author></authors><title>Inference of Cancer Progression Models with Biological Noise</title><categories>stat.ML cs.LG q-bio.QM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many applications in translational medicine require the understanding of how
diseases progress through the accumulation of persistent events. Specialized
Bayesian networks called monotonic progression networks offer a statistical
framework for modeling this sort of phenomenon. Current machine learning tools
to reconstruct Bayesian networks from data are powerful but not suited to
progression models. We combine the technological advances in machine learning
with a rigorous philosophical theory of causation to produce Polaris, a
scalable algorithm for learning progression networks that accounts for causal
or biological noise as well as logical relations among genetic events, making
the resulting models easy to interpret qualitatively. We tested Polaris on
synthetically generated data and showed that it outperforms a widely used
machine learning algorithm and approaches the performance of the competing
special-purpose, albeit clairvoyant algorithm that is given a priori
information about the model parameters. We also prove that under certain rather
mild conditions, Polaris is guaranteed to converge for sufficiently large
sample sizes. Finally, we applied Polaris to point mutation and copy number
variation data in Prostate cancer from The Cancer Genome Atlas (TCGA) and found
that there are likely three distinct progressions, one major androgen driven
progression, one major non-androgen driven progression, and one novel minor
androgen driven progression.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6039</identifier>
 <datestamp>2014-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6039</id><created>2014-08-26</created><authors><author><keyname>Asadi</keyname><forenames>Behzad</forenames></author><author><keyname>Ong</keyname><forenames>Lawrence</forenames></author><author><keyname>Johnson</keyname><forenames>Sarah J.</forenames></author></authors><title>Coding Schemes for a Class of Receiver Message Side Information in AWGN
  Broadcast Channels</title><categories>cs.IT math.IT</categories><comments>accepted and to be presented at the 2014 IEEE Information Theory
  Workshop (ITW)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the three-receiver AWGN broadcast channel where the
receivers (i) have private-message requests and (ii) know some of the messages
requested by other receivers as side information. For this setup, all possible
side information configurations have been recently classified into eight groups
and the capacity of the channel has been established for six groups (Asadi et
al., ISIT 2014). We propose inner and outer bounds for the two remaining
groups, groups 4 and 7. A distinguishing feature of these two groups is that
the weakest receiver knows the requested message of the strongest receiver as
side information while the in-between receiver does not. For group 4, the inner
and outer bounds coincide at certain regions. For group 7, the inner and outer
bounds coincide, thereby establishing the capacity, for four members out of all
eight members of the group; for the remaining four members, the proposed bounds
reduce the gap between the best known inner and outer bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6042</identifier>
 <datestamp>2014-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6042</id><created>2014-08-26</created><authors><author><keyname>Abdelrahim</keyname><forenames>Mahmoud</forenames></author><author><keyname>Postoyan</keyname><forenames>Romain</forenames></author><author><keyname>Daafouz</keyname><forenames>Jamal</forenames></author><author><keyname>Ne&#x161;i&#x107;</keyname><forenames>Dragan</forenames></author></authors><title>Co-design of output feedback laws and event-triggering conditions for
  linear systems</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a procedure to simultaneously design the output feedback law and
the event-triggering condition to stabilize linear systems. The closed-loop
system is shown to satisfy a global asymptotic stability property and the
existence of a strictly positive minimum amount of time between two
transmissions is guaranteed. The event-triggered controller is obtained by
solving linear matrix inequalities (LMIs). We then exploit the flexibility of
the method to maximize the guaranteed minimum amount of time between two
transmissions. Finally, we provide a (heuristic) method to reduce the amount of
transmissions, which is supported by numerical simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6063</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6063</id><created>2014-08-26</created><updated>2015-08-31</updated><authors><author><keyname>Groshaus</keyname><forenames>Marina</forenames></author><author><keyname>Guedes</keyname><forenames>Andr&#xe9;</forenames></author><author><keyname>Montero</keyname><forenames>Leandro</forenames></author></authors><title>Almost every graph is divergent under the biclique operator</title><categories>cs.DM</categories><comments>24 pages, 13 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A biclique of a graph $G$ is a maximal induced complete bipartite subgraph of
$G$. The biclique graph of $G$ denoted by $KB(G)$, is the intersection graph of
all the bicliques of $G$. The biclique graph can be thought as an operator
between graphs. The iterated biclique graph of $G$ denoted by $KB^{k}(G)$, is
the graph obtained by applying the biclique operator $k$ successive times to
$G$. The associated problem is deciding whether an input graph converges,
diverges or is periodic under the biclique operator when $k$ grows to infinity.
All possible behaviors were characterized recently and an $O(n^4)$ algorithm
for deciding the behavior of any graph under the biclique operator was also
given. In this work we prove new structural results of biclique graphs. In
particular, we prove that every false-twin-free graph with at least $13$
vertices is divergent. These results lead to a linear time algorithm to solve
the same problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6074</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6074</id><created>2014-08-26</created><authors><author><keyname>Salnikov</keyname><forenames>Vladimir</forenames></author><author><keyname>Choi</keyname><forenames>Daniel</forenames></author><author><keyname>Karamian-Surville</keyname><forenames>Philippe</forenames></author></authors><title>On efficient and reliable stochastic generation of RVEs for analysis of
  composites within the framework of homogenization</title><categories>math.NA cond-mat.mtrl-sci cs.CE</categories><comments>20 pages</comments><doi>10.1007/s00466-014-1086-1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we describe efficient methods of generation of representative
volume elements (RVEs) suitable for producing the samples for analysis of
effective properties of composite materials via and for stochastic
homogenization. We are interested in composites reinforced by a mixture of
spherical and cylindrical inclusions. For these geometries we give explicit
conditions of intersection in a convenient form for verification. Based on
those conditions we present two methods to generate RVEs: one is based on the
Random Sequential Adsorption scheme, the other one on the time driven Molecular
Dynamics. We test the efficiency of these methods and show that the first one
is extremely powerful for low volume fraction of inclusions, while the second
one allows us to construct denser configurations. All the algorithms are given
explicitly so they can be implemented directly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6104</identifier>
 <datestamp>2014-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6104</id><created>2014-08-26</created><updated>2014-08-28</updated><authors><author><keyname>Fontana</keyname><forenames>Peter</forenames></author><author><keyname>Cleaveland</keyname><forenames>Rance</forenames></author></authors><title>The Power of Proofs: New Algorithms for Timed Automata Model Checking
  (with Appendix)</title><categories>cs.FL cs.SE</categories><comments>This is the preprint of the FORMATS 2014 paper, but this is the full
  version, containing the Appendix. The final publication is published from
  Springer, and is available at
  http://link.springer.com/chapter/10.1007%2F978-3-319-10512-3_9 on the
  Springer webpage</comments><journal-ref>Lecture Notes in Computer Science vol 8711 (Jan 2014) pp 115-129</journal-ref><doi>10.1007/978-3-319-10512-3_9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the first model-checking algorithm for an expressive
modal mu-calculus over timed automata, $L^{\mathit{rel},
\mathit{af}}_{\nu,\mu}$, and reports performance results for an implementation.
This mu-calculus contains extended time-modality operators and can express all
of TCTL. Our algorithmic approach uses an &quot;on-the-fly&quot; strategy based on proof
search as a means of ensuring high performance for both positive and negative
answers to model-checking questions. In particular, a set of proof rules for
solving model-checking problems are given and proved sound and complete; we
encode our algorithm in these proof rules and model-check a property by
constructing a proof (or showing none exists) using these rules. One noteworthy
aspect of our technique is that we show that verification performance can be
improved with \emph{derived rules}, whose correctness can be inferred from the
more primitive rules on which they are based. In this paper, we give the basic
proof rules underlying our method, describe derived proof rules to improve
performance, and compare our implementation of this model checker to the UPPAAL
tool.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6108</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6108</id><created>2014-08-26</created><authors><author><keyname>Buk&#xe1;&#x10d;ek</keyname><forenames>Marek</forenames></author><author><keyname>Hrab&#xe1;k</keyname><forenames>Pavel</forenames></author><author><keyname>Krb&#xe1;lek</keyname><forenames>Milan</forenames></author></authors><title>Experimental Study of Phase Transition in Pedestrian Flow</title><categories>cs.MA physics.soc-ph</categories><comments>To appear in proceedings of Pedestrian and Evacuation Dynamics 2014,
  Transportation Research Procedia, Elsevier</comments><journal-ref>Transportation Research Procedia, Volume 2, 2014, Pages 105-113</journal-ref><doi>10.1016/j.trpro.2014.09.014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The transition between low and high density phases is a typical feature of
systems with social interactions. This contribution focuses on simple
evacuation design of one room with one entrance and one exit; four
passing-through experiments were organized and evaluated by means of automatic
image processing. The phase of the system, determined by travel time and
occupancy, is evaluated with respect to the inflow, a controlled boundary
condition. Critical values of inflow and outflow were described with respect to
the transition from low density to congested state. Moreover, microscopic
analysis of travel time is provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6109</identifier>
 <datestamp>2014-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6109</id><created>2014-08-26</created><authors><author><keyname>Antonopoulos</keyname><forenames>Angelos</forenames></author><author><keyname>Lalos</keyname><forenames>Aris S.</forenames></author><author><keyname>Di Renzo</keyname><forenames>Marco</forenames></author><author><keyname>Verikoukis</keyname><forenames>Christos</forenames></author></authors><title>Cross-layer Theoretical Analysis of NC-aided Cooperative ARQ Protocols
  in Correlated Shadowed Environments (Extended Version)</title><categories>cs.NI</categories><comments>39 pages (including Appendices), 11 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a cross-layer analytical model for the study of
Network Coding (NC)-based Automatic Repeat reQuest (ARQ) Medium Access Control
(MAC) protocols in correlated slow faded (shadowed) environments, where two end
nodes are assisted by a cluster of relays to exchange data packets. The goal of
our work is threefold: i) to provide general Physical (PHY) layer theoretical
expressions for estimating crucial network parameters (i.e., network outage
probability and expected size of the active relay set), applicable in two-way
communications, ii) to demonstrate how these expressions are incorporated in
theoretical models of the upper layers (i.e., MAC), and iii) to study the
performance of a recently proposed NC-aided Cooperative ARQ (NCCARQ) MAC
protocol under correlated shadowing conditions. Extensive Monte Carlo
experiments have been carried out to validate the efficiency of the developed
analytical model and to investigate the realistic performance of NCCARQ. Our
results indicate that the number of active relays is independent of the
shadowing correlation in the wireless links and reveal intriguing trade-offs
between throughput and energy efficiency, highlighting the importance of
cross-layer approaches for the assessment of cooperative MAC protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6120</identifier>
 <datestamp>2014-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6120</id><created>2014-08-26</created><authors><author><keyname>Ahmad</keyname><forenames>Munib</forenames></author><author><keyname>Bajaber</keyname><forenames>Fuad</forenames></author><author><keyname>Qureshi</keyname><forenames>M. Rizwan Jameel</forenames></author></authors><title>The proposal of a novel software testing framework</title><categories>cs.SE</categories><comments>8 pages, 1 figure</comments><journal-ref>Life Science Journal-ACTA Zhengzhou University Overseas Edition,
  online October 2013; Vol. 10, No. 4, 2013, pp. 319-326</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software testing is normally used to check the validity of a program. Test
oracle performs an important role in software testing. The focus in this
research is to perform class level test by introducing a testing framework. A
technique is developed to generate test oracle for specification-based software
testing using Vienna Development Method (VDM++) formal language. A three stage
translation process, of VDM++ specifications of container classes to C++ test
oracle classes, is described in this paper. It is also presented that how
derived test oracle is integrated into a proposed functional testing framework.
This technique caters object oriented features such as inheritance and
aggregation, but concurrency is not considered in this work. Translation
issues, limitations and evaluation of the technique are also discussed. The
proposed approach is illustrated with the help of popular triangle problem case
study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6122</identifier>
 <datestamp>2015-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6122</id><created>2014-08-26</created><updated>2015-01-14</updated><authors><author><keyname>Bossy</keyname><forenames>Mireille</forenames></author><author><keyname>Maizi</keyname><forenames>Nadia</forenames></author><author><keyname>Pourtallier</keyname><forenames>Odile</forenames></author></authors><title>Game theory analysis for carbon auction market through electricity
  market coupling</title><categories>q-fin.MF cs.GT</categories><comments>arXiv admin note: text overlap with arXiv:1311.1535</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we analyze Nash equilibria between electricity producers
selling their production on an electricity market and buying CO2 emission
allowances on an auction carbon market. The producers' strategies integrate the
coupling of the two markets via the cost functions of the electricity
production. We set out a clear Nash equilibrium on the power market that can be
used to compute equilibrium prices on both markets as well as the related
electricity produced and CO2 emissions released.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6125</identifier>
 <datestamp>2014-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6125</id><created>2014-08-26</created><authors><author><keyname>Alghabban</keyname><forenames>Weam Gaoud</forenames></author><author><keyname>Qureshi</keyname><forenames>M. Rizwan Jameel</forenames></author></authors><title>The proposal of improved component selection framework</title><categories>cs.SE cs.IT math.IT</categories><comments>7 pages, 5 figures</comments><journal-ref>Life Science Journal-ACTA, Vol. 10, No. 4, 2013, pp. 3538-3544</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Component selection is considered one of hard tasks in Component Based
Software Engineering (CBSE). It is difficult to find the optimal component
selection. CBSE is an approach that is used to develop a software system from
pre-existing software components. Appropriate software component selection
plays an important role in CBSE. Many approaches were suggested to solve
component selection problem. In this paper the component selection is done by
improving the integrated component selection framework by including the
pliability metric. Pliability is a flexible measure that assesses software
quality in terms of its components quality. The validation of this proposed
solution is done through collecting a sample of people who answer an electronic
questionnaire that composed of 20 questions. The questionnaire is distributed
through social sites such as Twitter, Facebook and emails. The result of the
validation showed that using the integrated component selection framework with
pliability metric is suitable for component selection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6126</identifier>
 <datestamp>2014-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6126</id><created>2014-08-26</created><updated>2014-08-28</updated><authors><author><keyname>Pellegrino</keyname><forenames>Jacopo</forenames></author></authors><title>A Multi-agent Based Digital Preservation Model</title><categories>cs.MA cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Master's Degree Thesis: Department of Physics, University of Turin
  Supervisor: Prof. Marco Maggiora, Department of Physics, University of Turin;
email: marco.maggiora@unito.it
  Co-Supervisor: Prof. Walter Allasia, Innovation Department, EURIX; email:
allasia@eurix.it
  The thesis describes an agent-based model aimed to simulate those processes
in which a digital object faces the risk of obsolescence, a migration process
has to be performed and the most appropriate file format has to be adopted.
Agents have been designed in order to monitor and control the local system
where they reside and its environment. They are able to become aware of
obsolescent formats based on global parameters such as their diffusion. They
communicate as well with each other to find out the most suitable preservation
action to be performed. Agents request suggestions that are evaluated and
propagated according to a weighting based on the level of trust assigned to
both the agents who identified the problem and proposed the solution. In the
current research, the definition of the trust level has been chosen based on
the cultural and geographical distances, the expertise of the involved agents
and the file format numerosity. The level of trust between two agents is
automatically updated after every interaction by the mean of a feedback
mechanism profiting of an inter agent communication based on stigmergy. Summing
up, the thesis demonstrates how a multi-agent system can either perform an
autonomous preservation action or suggest a list of best candidate solutions to
the user. It benefits the management of several kinds of digital archive,
especially those with limited resources specifically dedicated to digital
preservation, such as small personal collections and many public institutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6127</identifier>
 <datestamp>2014-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6127</id><created>2014-08-26</created><authors><author><keyname>Boidot</keyname><forenames>Emmanuel</forenames></author><author><keyname>Marzuoli</keyname><forenames>Aude</forenames></author><author><keyname>Feron</keyname><forenames>Eric</forenames></author></authors><title>A Complete framework for ambush avoidance in realistic environments</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Operating vehicles in adversarial environments between a recurring
origin-destination pair requires new planning techniques. A two players
zero-sum game is introduced. The goal of the first player is to minimize the
expected casualties undergone by a convoy. The goal of the second player is to
maximize this damage. The outcome of the game is obtained via a linear program
that solves the corresponding minmax optimization problem over this outcome.
Different environment models are defined in order to compute routing strategies
over unstructured environments. To compare these methods for increasingly
accurate representations of the environment, a grid-based model is chosen to
represent the environment and the existence of a sufficient network size is
highlighted. A global framework for the generation of realistic routing
strategies between any two points is described. This framework requires a good
assessment of the potential casualties at any location, therefore the most
important parameters are identified. Finally the framework is tested on real
world environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6130</identifier>
 <datestamp>2014-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6130</id><created>2014-08-26</created><authors><author><keyname>Qureshi</keyname><forenames>M. Rizwan Jameel</forenames></author><author><keyname>Alshamat</keyname><forenames>Sohayp Abo</forenames></author><author><keyname>Sabir</keyname><forenames>Fatima</forenames></author></authors><title>Significance of the teamwork in agile software engineering</title><categories>cs.SE</categories><comments>4 pages, 4 figures</comments><journal-ref>Science International-Lahore, online Feb 2014; Vol. 26, No. 1,
  2014, pp. 117-120</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Software Engineering project depends significantly on team performance, as
does any activity that involves human interaction. In the last years, the
traditional perspective on software development is changing and agile methods
have received considerable attention. Among other attributes, the ageists claim
that fostering creativity is one of the keys to response to common problems and
challenges of software development today. The development of new software
products requires the generation of novel and useful ideas. It is a conceptual
framework introduced in the Agile Manifesto in 2001. This paper is written in
support of agile practices in terms of significance of teamwork for the success
of software projects. Survey is used as a research method to know the
significance of teamwork.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6141</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6141</id><created>2014-08-25</created><updated>2014-11-10</updated><authors><author><keyname>Arablouei</keyname><forenames>Reza</forenames></author><author><keyname>Do&#x11f;an&#xe7;ay</keyname><forenames>Kutluy&#x131;l</forenames></author><author><keyname>Werner</keyname><forenames>Stefan</forenames></author></authors><title>Recursive Total Least-Squares Algorithm Based on Inverse Power Method
  and Dichotomous Coordinate-Descent Iterations</title><categories>cs.SY cs.LG</categories><doi>10.1109/TSP.2015.2405492</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a recursive total least-squares (RTLS) algorithm for
errors-in-variables system identification utilizing the inverse power method
and the dichotomous coordinate-descent (DCD) iterations. The proposed
algorithm, called DCD-RTLS, outperforms the previously-proposed RTLS
algorithms, which are based on the line-search method, with reduced
computational complexity. We perform a comprehensive analysis of the DCD-RTLS
algorithm and show that it is asymptotically unbiased as well as being stable
in the mean. We also find a lower bound for the forgetting factor that ensures
mean-square stability of the algorithm and calculate the theoretical
steady-state mean-square deviation (MSD). We verify the effectiveness of the
proposed algorithm and the accuracy of the predicted steady-state MSD via
simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6142</identifier>
 <datestamp>2014-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6142</id><created>2014-08-26</created><authors><author><keyname>Qurashi</keyname><forenames>Saja Al</forenames></author><author><keyname>Qureshi</keyname><forenames>M. Rizwan Jameel</forenames></author></authors><title>Scrum of scrums solution for large size teams using scrum methodology</title><categories>cs.SE</categories><comments>7 pages, 7 figures</comments><journal-ref>Life Science Journal-ACTA Zhengzhou University Overseas Edition,
  online May 2014; Vol. 11, No. 8, 2014, pp. 443-449</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scrum is a structured framework to support complex product development.
However, Scrum methodology faces a challenge of managing large teams. To
address this challenge, in this paper we propose a solution called Scrum of
Scrums. In Scrum of Scrums, we divide the Scrum team into teams of the right
size, and then organize them hierarchically into a Scrum of Scrums. The main
goals of the proposed solution are to optimize communication between teams in
Scrum of Scrums; to make the system work after integration of all parts; to
reduce the dependencies between the parts of system; and to prevent the
duplication of parts in the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6143</identifier>
 <datestamp>2014-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6143</id><created>2014-08-21</created><authors><author><keyname>Pled</keyname><forenames>Florent Henri Marc R&#xe9;my</forenames><affiliation>LMT</affiliation></author><author><keyname>Chamoin</keyname><forenames>Ludovic</forenames><affiliation>LMT</affiliation></author><author><keyname>Ladev&#xe8;ze</keyname><forenames>Pierre</forenames><affiliation>LMT</affiliation></author></authors><title>An enhanced method with local energy minimization for the robust a
  posteriori construction of equilibrated stress fields in finite element
  analyses</title><categories>math.NA cs.NA</categories><comments>22 pages</comments><proxy>ccsd</proxy><journal-ref>Computational Mechanics 49, 3 (2012) 357-378</journal-ref><doi>10.1007/s00466-011-0645-y</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the context of global/goal-oriented error estimation applied to
computational mechanics, the need to obtain reliable and guaranteed bounds on
the discretization error has motivated the use of residual error estimators.
These estimators require the construction of admissible stress fields verifying
the equilibrium exactly. This article focuses on a recent method, based on a
flux-equilibration procedure and called the element equilibration + star-patch
technique (EESPT), that provides for such stress fields. The standard version
relies on a strong prolongation condition in order to calculate equilibrated
tractions along finite element boundaries. Here, we propose an enhanced
version, which is based on a weak prolongation condition resulting in a local
minimization of the complementary energy and leads to optimal tractions in
selected regions. Geometric and error estimate criteria are introduced to
select the relevant zones for optimizing the tractions. We demonstrate how this
optimization procedure is important and relevant to produce sharper estimators
at affordable computational cost, especially when the error estimate criterion
is used. Two- and three-dimensional numerical experiments demonstrate the
efficiency of the improved technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6147</identifier>
 <datestamp>2014-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6147</id><created>2014-08-26</created><authors><author><keyname>Ba-Brahem</keyname><forenames>Afnan Salem</forenames></author><author><keyname>Qureshi</keyname><forenames>M. Rizwan Jameel</forenames></author></authors><title>The proposal of improved inexact isomorphic graph algorithm to detect
  design patterns</title><categories>cs.SE</categories><comments>13 pages, 11 figures</comments><journal-ref>SYLWAN Journal, online June 2014; Vol. 158, No. 6, 2014, pp.
  90-102</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Design patterns being applied more and more to solve the software engineering
difficulties in the object oriented software design procedures. So, the design
pattern detection is widely used by software industries. Currently, many
solutions presented to detect the design pattern in the system design. In this
paper, we will propose a new one which first; we will use the graph
implementation to implement both the system design UML diagram and the design
pattern UML diagram. Second, we will implement the edges for each one of the
both two graphs in a set of 4-tuple elements. Then, we will apply a new inexact
graph isomorphic algorithm to detect the design pattern in the system design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6157</identifier>
 <datestamp>2014-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6157</id><created>2014-08-26</created><updated>2014-10-10</updated><authors><author><keyname>Cygan</keyname><forenames>Marek</forenames></author><author><keyname>Kociumaka</keyname><forenames>Tomasz</forenames></author></authors><title>Approximating Upper Degree-Constrained Partial Orientations</title><categories>cs.DS</categories><comments>12 pages, 1 figure</comments><msc-class>68W25, 05C85</msc-class><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the Upper Degree-Constrained Partial Orientation problem we are given an
undirected graph $G=(V,E)$, together with two degree constraint functions
$d^-,d^+ : V \to \mathbb{N}$. The goal is to orient as many edges as possible,
in such a way that for each vertex $v \in V$ the number of arcs entering $v$ is
at most $d^-(v)$, whereas the number of arcs leaving $v$ is at most $d^+(v)$.
This problem was introduced by Gabow [SODA'06], who proved it to be MAXSNP-hard
(and thus APX-hard). In the same paper Gabow presented an LP-based iterative
rounding $4/3$-approximation algorithm.
  Since the problem in question is a special case of the classic 3-Dimensional
Matching, which in turn is a special case of the $k$-Set Packing problem, it is
reasonable to ask whether recent improvements in approximation algorithms for
the latter two problems [Cygan, FOCS'13; Sviridenko &amp; Ward, ICALP'13] allow for
an improved approximation for Upper Degree-Constrained Partial Orientation. We
follow this line of reasoning and present a polynomial-time local search
algorithm with approximation ratio $5/4+\varepsilon$. Our algorithm uses a
combination of two types of rules: improving sets of bounded pathwidth from the
recent $4/3+\varepsilon$-approximation algorithm for 3-Set Packing [Cygan,
FOCS'13], and a simple rule tailor-made for the setting of partial
orientations. In particular, we exploit the fact that one can check in
polynomial time whether it is possible to orient all the edges of a given graph
[Gy\'arf\'as &amp; Frank, Combinatorics'76].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6179</identifier>
 <datestamp>2014-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6179</id><created>2014-08-26</created><authors><author><keyname>Milajevs</keyname><forenames>Dmitrijs</forenames></author><author><keyname>Kartsaklis</keyname><forenames>Dimitri</forenames></author><author><keyname>Sadrzadeh</keyname><forenames>Mehrnoosh</forenames></author><author><keyname>Purver</keyname><forenames>Matthew</forenames></author></authors><title>Evaluating Neural Word Representations in Tensor-Based Compositional
  Settings</title><categories>cs.CL</categories><comments>To be published in EMNLP 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a comparative study between neural word representations and
traditional vector spaces based on co-occurrence counts, in a number of
compositional tasks. We use three different semantic spaces and implement seven
tensor-based compositional models, which we then test (together with simpler
additive and multiplicative approaches) in tasks involving verb disambiguation
and sentence similarity. To check their scalability, we additionally evaluate
the spaces using simple compositional methods on larger-scale tasks with less
constrained language: paraphrase detection and dialogue act tagging. In the
more constrained tasks, co-occurrence vectors are competitive, although choice
of compositional method is important; on the larger-scale tasks, they are
outperformed by neural word embeddings, which show robust, stable performance
across the tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6181</identifier>
 <datestamp>2014-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6181</id><created>2014-08-26</created><authors><author><keyname>Kartsaklis</keyname><forenames>Dimitri</forenames></author><author><keyname>Kalchbrenner</keyname><forenames>Nal</forenames></author><author><keyname>Sadrzadeh</keyname><forenames>Mehrnoosh</forenames></author></authors><title>Resolving Lexical Ambiguity in Tensor Regression Models of Meaning</title><categories>cs.CL</categories><journal-ref>Proceedings of ACL 2014, Vol. 2:Short Papers, pp:212-217</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper provides a method for improving tensor-based compositional
distributional models of meaning by the addition of an explicit disambiguation
step prior to composition. In contrast with previous research where this
hypothesis has been successfully tested against relatively simple compositional
models, in our work we use a robust model trained with linear regression. The
results we get in two experiments show the superiority of the prior
disambiguation method and suggest that the effectiveness of this approach is
model-independent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6182</identifier>
 <datestamp>2015-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6182</id><created>2014-08-26</created><updated>2015-05-15</updated><authors><author><keyname>Babenko</keyname><forenames>Maxim</forenames></author><author><keyname>Gawrychowski</keyname><forenames>Pawe&#x142;</forenames></author><author><keyname>Kociumaka</keyname><forenames>Tomasz</forenames></author><author><keyname>Starikovskaya</keyname><forenames>Tatiana</forenames></author></authors><title>Wavelet Trees Meet Suffix Trees</title><categories>cs.DS</categories><comments>33 pages, 5 figures; preliminary version published at SODA 2015</comments><msc-class>68P05 (Primary), 68W05 (Secondary)</msc-class><acm-class>E.1; F.2.2</acm-class><doi>10.1137/1.9781611973730.39</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an improved wavelet tree construction algorithm and discuss its
applications to a number of rank/select problems for integer keys and strings.
  Given a string of length n over an alphabet of size $\sigma\leq n$, our
method builds the wavelet tree in $O(n \log \sigma/ \sqrt{\log{n}})$ time,
improving upon the state-of-the-art algorithm by a factor of $\sqrt{\log n}$.
As a consequence, given an array of n integers we can construct in $O(n
\sqrt{\log n})$ time a data structure consisting of $O(n)$ machine words and
capable of answering rank/select queries for the subranges of the array in
$O(\log n / \log \log n)$ time. This is a $\log \log n$-factor improvement in
query time compared to Chan and P\u{a}tra\c{s}cu and a $\sqrt{\log n}$-factor
improvement in construction time compared to Brodal et al.
  Next, we switch to stringological context and propose a novel notion of
wavelet suffix trees. For a string w of length n, this data structure occupies
$O(n)$ words, takes $O(n \sqrt{\log n})$ time to construct, and simultaneously
captures the combinatorial structure of substrings of w while enabling
efficient top-down traversal and binary search. In particular, with a wavelet
suffix tree we are able to answer in $O(\log |x|)$ time the following two
natural analogues of rank/select queries for suffixes of substrings: for
substrings x and y of w count the number of suffixes of x that are
lexicographically smaller than y, and for a substring x of w and an integer k,
find the k-th lexicographically smallest suffix of x.
  We further show that wavelet suffix trees allow to compute a
run-length-encoded Burrows-Wheeler transform of a substring x of w in $O(s \log
|x|)$ time, where s denotes the length of the resulting run-length encoding.
This answers a question by Cormode and Muthukrishnan, who considered an
analogous problem for Lempel-Ziv compression.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6186</identifier>
 <datestamp>2014-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6186</id><created>2014-08-26</created><authors><author><keyname>Das</keyname><forenames>Sujit</forenames></author><author><keyname>Kar</keyname><forenames>Samarjit</forenames></author></authors><title>Consensus and Consistency Level Optimization of Fuzzy Preference
  Relation: A Soft Computing Approach</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In group decision making (GDM) problems fuzzy preference relations (FPR) are
widely used for representing decision makers' opinions on the set of
alternatives. In order to avoid misleading solutions, the study of consistency
and consensus has become a very important aspect. This article presents a
simulated annealing (SA) based soft computing approach to optimize the
consistency/consensus level (CCL) of a complete fuzzy preference relation in
order to solve a GDM problem. Consistency level indicates as expert's
preference quality and consensus level measures the degree of agreement among
experts' opinions. This study also suggests the set of experts for the
necessary modifications in their prescribed preference structures without
intervention of any moderator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6196</identifier>
 <datestamp>2014-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6196</id><created>2014-08-26</created><authors><author><keyname>Xiao</keyname><forenames>Mingyu</forenames></author><author><keyname>Nagamochi</keyname><forenames>Hiroshi</forenames></author></authors><title>Exact Algorithms for Dominating Induced Matching Based on Graph
  Partition</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A dominating induced matching, also called an efficient edge domination, of a
graph $G=(V,E)$ with $n=|V|$ vertices and $m=|E|$ edges is a subset $F
\subseteq E$ of edges in the graph such that no two edges in $F$ share a common
endpoint and each edge in $E\setminus F$ is incident with exactly one edge in
$F$. It is NP-hard to decide whether a graph admits a dominating induced
matching or not. In this paper, we design a $1.1467^nn^{O(1)}$-time exact
algorithm for this problem, improving all previous results. This problem can be
redefined as a partition problem that is to partition the vertex set of a graph
into two parts $I$ and $F$, where $I$ induces an independent set (a 0-regular
graph) and $F$ induces a perfect matching (a 1-regular graph). After giving
several structural properties of the problem, we show that the problem always
contains some &quot;good vertices&quot;, branching on which by including them to either
$I$ or $F$ we can effectively reduce the graph. This leads to a fast exact
algorithm to this problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6197</identifier>
 <datestamp>2014-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6197</id><created>2014-08-23</created><authors><author><keyname>Turkoglu</keyname><forenames>Kamran</forenames></author></authors><title>Real-Time First Order Guidance Strategies for Trajectory Optimization in
  UAVs by Utilizing Wind Energy</title><categories>math.OC cs.SY</categories><comments>(under review in Journal of Aircraft)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents real-time guidance strategies for unmanned aerial
vehicles (UAVs) that can be used to enhance their flight endurance by utilizing
{\sl insitu} measurements of wind speeds and wind gradients. In these
strategies, periodic adjustments would be made in the airspeed and/or heading
angle command for the UAV to minimize a projected power requirement at some
future time. In this paper, UAV flights are described by a three-dimensional
dynamic point-mass. Onboard closed-loop trajectory tracking logics that follow
airspeed vector commands are modeled using the method of feedback
linearization. A generic wind field model is assumed that consists of a
constant term plus terms that vary sinusoidally with respect to the location.
To evaluate the benefits of these strategies in enhancing UAV flight endurance,
a reference strategy is introduced in which the UAV would seek to follow the
desired airspeed in a steady level flight under zero wind. A performance
measure is defined as the average power consumption both over a specified time
interval and over different initial heading angles of the UAV. A relative
benefit criterion is then defined as the percentage improvement of the
performance measure of a proposed strategy over that of the reference strategy.
Extensive numerical simulations are conducted. Results demonstrate the benefits
and trends of power savings of the proposed real-time guidance strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6198</identifier>
 <datestamp>2014-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6198</id><created>2014-08-18</created><authors><author><keyname>Kucherov</keyname><forenames>Gregory</forenames></author><author><keyname>No&#xe9;</keyname><forenames>Laurent</forenames></author><author><keyname>Roytberg</keyname><forenames>Mikhail</forenames></author></authors><title>Subset seed automaton</title><categories>cs.FL cs.DS q-bio.QM</categories><comments>12 pages, 2 figures, 2 tables, CIAA 2007,
  http://hal.inria.fr/inria-00170414/en/</comments><msc-class>20M35, 68Q45</msc-class><acm-class>F.1.1; F.4.3</acm-class><journal-ref>LNCS 4783 (2007), pp 180-191</journal-ref><doi>10.1007/978-3-540-76336-9_18</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the pattern matching automaton introduced in (A unifying framework
for seed sensitivity and its application to subset seeds) for the purpose of
seed-based similarity search. We show that our definition provides a compact
automaton, much smaller than the one obtained by applying the Aho-Corasick
construction. We study properties of this automaton and present an efficient
implementation of the automaton construction. We also present some experimental
results and show that this automaton can be successfully applied to more
general situations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6202</identifier>
 <datestamp>2014-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6202</id><created>2014-08-22</created><authors><author><keyname>Russell</keyname><forenames>Travis</forenames></author></authors><title>The exact synthesis of 1- and 2-qubit Clifford+T circuits</title><categories>quant-ph cs.ET</categories><comments>Honour's thesis, Dalhousie University, 20 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a new method for the decomposition of an arbitrary $n$ qubit
operator with entries in $\mathbb{Z}[i,\frac{1}{\sqrt{2}}]$, i.e., of the form
$(a+b\sqrt{2}+i(c+d\sqrt{2}))/{\sqrt{2}^{k}}$, into Clifford+$T$ operators
where $n\le 2$. This method achieves a bound of $O(k)$ gates using at most one
ancilla using decomposition into $1$- and $2$-level matrices which was first
proposed by Giles and Selinger.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6210</identifier>
 <datestamp>2015-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6210</id><created>2014-08-26</created><updated>2015-11-04</updated><authors><author><keyname>Cuel</keyname><forenames>Louis</forenames><affiliation>LAMA, LJK</affiliation></author><author><keyname>Lachaud</keyname><forenames>Jacques-Olivier</forenames><affiliation>LAMA</affiliation></author><author><keyname>M&#xe9;rigot</keyname><forenames>Quentin</forenames><affiliation>MGMI</affiliation></author><author><keyname>Thibert</keyname><forenames>Boris</forenames><affiliation>MGMI</affiliation></author></authors><title>Robust Geometry Estimation using the Generalized Voronoi Covariance
  Measure</title><categories>cs.CG</categories><proxy>ccsd</proxy><journal-ref>SIAM Journal on Imaging Sciences, Society for Industrial and
  Applied Mathematics, 2015, pp.1293-1314</journal-ref><doi>10.1137/140977552</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Voronoi Covariance Measure of a compact set K of R^d is a tensor-valued
measure that encodes geometric information on K and which is known to be
resilient to Hausdorff noise but sensitive to outliers. In this article, we
generalize this notion to any distance-like function delta and define the
delta-VCM. We show that the delta-VCM is resilient to Hausdorff noise and to
outliers, thus providing a tool to estimate robustly normals from a point cloud
approximation. We present experiments showing the robustness of our approach
for normal and curvature estimation and sharp feature detection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6214</identifier>
 <datestamp>2014-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6214</id><created>2014-08-26</created><authors><author><keyname>Rabenoro</keyname><forenames>Tsirizo</forenames><affiliation>SAMM</affiliation></author><author><keyname>Lacaille</keyname><forenames>J&#xe9;r&#xf4;me</forenames><affiliation>SAMM</affiliation></author><author><keyname>Cottrell</keyname><forenames>Marie</forenames><affiliation>SAMM</affiliation></author><author><keyname>Rossi</keyname><forenames>Fabrice</forenames><affiliation>SAMM</affiliation></author></authors><title>A Methodology for the Diagnostic of Aircraft Engine Based on Indicators
  Aggregation</title><categories>stat.ML cs.LG</categories><comments>Proceedings of the 14th Industrial Conference, ICDM 2014, St.
  Petersburg : Russian Federation (2014)</comments><proxy>ccsd</proxy><doi>10.1007/978-3-319-08976-8_11</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Aircraft engine manufacturers collect large amount of engine related data
during flights. These data are used to detect anomalies in the engines in order
to help companies optimize their maintenance costs. This article introduces and
studies a generic methodology that allows one to build automatic early signs of
anomaly detection in a way that is understandable by human operators who make
the final maintenance decision. The main idea of the method is to generate a
very large number of binary indicators based on parametric anomaly scores
designed by experts, complemented by simple aggregations of those scores. The
best indicators are selected via a classical forward scheme, leading to a much
reduced number of indicators that are tuned to a data set. We illustrate the
interest of the method on simulated data which contain realistic early signs of
anomalies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6226</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6226</id><created>2014-08-26</created><updated>2014-11-01</updated><authors><author><keyname>Su</keyname><forenames>Shenghui</forenames></author><author><keyname>Lv</keyname><forenames>Shuwang</forenames></author><author><keyname>Xu</keyname><forenames>Maozhi</forenames></author></authors><title>A Public Key Cryptoscheme Using Bit-pairs and Probabilistic Mazes</title><categories>cs.CR</categories><comments>16 Pages. arXiv admin note: text overlap with arXiv:1408.5999</comments><acm-class>D.4.6; E.3; F.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The authors give the definition and property of a bit-pair shadow, and design
the three algorithms of a public key cryptoscheme that is based on a
multivariate permutation problem (MPP) and an anomalous subset product problem
(ASPP) to which no subexponential time solutions are found so far, and regards
a bit-pair as an operation unit. Further, demonstrate that the decryption
algorithm is correct, deduce the probability that a plaintext solution is
nonunique is nearly zero, dissect the running times of the three algorithms,
analyze the security of the new scheme against extracting a private key from a
public key and recovering a related plaintext from a ciphertext by LLL lattice
basis reduction, meet-in-the-middle dichotomy, and adaptive-chosen-ciphertext
approach on the assumption that an integer factorization problem, a discrete
logarithm problem, and a low-density subset sum problem can be solved
efficiently, and prove that new scheme using random both padding and
permutation is semantically secure. Meantime, give a conversion from an ASPP to
an anomalous subset sum problem (ASSP). The analysis shows that the bit-pair
method increases the density of a related ASSP knapsack to D &gt; 1, and decreases
the modulus length of the new scheme to lgM = 464, 544, or 640 corresponding to
n = 80, 96, or 112 separately.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6228</identifier>
 <datestamp>2014-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6228</id><created>2014-08-26</created><authors><author><keyname>Qureshi</keyname><forenames>M. Rizwan Jameel</forenames></author></authors><title>Estimation of the new agile XP process model for medium-scale projects
  using industrial case studies</title><categories>cs.SE</categories><comments>3 pages, 1 figure</comments><journal-ref>International Journal of Machine Learning and Computing, online
  October 2013; Vol. 3, No. 5, 2013, pp. 393-395</journal-ref><doi>10.7763/IJMLC.2013.V3.346</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Agile is one of the terms with which software professionals are quite
familiar. Agile models promote fast development to develop high quality
software. XP process model is one of the most widely used and most documented
agile models. XP model is meant for small-scale projects. Since XP model is a
good model, therefore there is need of its extension for the development of
medium and large-scale projects. XP model has certain drawbacks such as weak
documentation and poor performance while adapting it for the development of
medium and large-scale projects having large teams. A new XP model is proposed
in this paper to cater the needs of software development companies for
medium-scale projects having large teams. This research may prove to be step
forward for adaptation of the proposed new XP model for the development of
large-scale projects. Two independent industrial case studies are conducted to
validate the proposed new XP model handling for small and medium scale software
projects, one case study for each type of project.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6229</identifier>
 <datestamp>2014-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6229</id><created>2014-08-26</created><authors><author><keyname>Qureshi</keyname><forenames>M. Rizwan Jameel</forenames></author></authors><title>IMS-based mobile learning system</title><categories>cs.CY</categories><comments>6 pages, 6 figures</comments><journal-ref>Life Science Journal-ACTA, Vol. 10, No. 4, 2013, pp. 2121-2126</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Electronic (E) learning management system is not a novel idea in the
educational domain. Learning management systems are used to deal with academic
activities such as course syllabi, time table scheduling, assessments and
project discussion forums. Almost, all the top universities of world are using
general purpose/customized solutions to manage learning management systems like
SAP, Oracle, Moodle and Blackboard. The aim of this paper i.e., Mobile (M)
Learning System (MLS) is not to substitute the traditional web based E learning
applications but to enhance it by amalgamating both web and mobile
technologies. This idea justifies the proposal of M learning system to use some
of the services of E learning system from mobiles. MLS will use
state-of-the-art IP Multimedia Sub System technology. The emphasis in this
research will be on the technical implementation of the Session Initiation
Protocol (SIP) using IP Multimedia Subsystem (IMS) to develop an MLS not only
for the students of the King Abdulaziz University but it will be beneficial for
the students of other universities at Kingdom of Saudi Arabia. A customized CBD
is proposed as per the nature of MLS project. MLS case study is used as a
research design to validate the customized CBD model. Multi-tier applications
architecture (client, web, and business) will be adopted during the development
of MLS case study. An MLS will be developed and tested using IMS platform to
check its practicality for the students of King Abdulaziz University. It is
anticipated that the proposed system will significantly facilitate to both the
students and teachers of KAU during their off campus activities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6252</identifier>
 <datestamp>2014-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6252</id><created>2014-08-24</created><updated>2014-10-08</updated><authors><author><keyname>Cao</keyname><forenames>Zhengjun</forenames></author><author><keyname>Cao</keyname><forenames>Zhenfu</forenames></author><author><keyname>Liu</keyname><forenames>Lihua</forenames></author></authors><title>Remarks on Quantum Modular Exponentiation and Some Experimental
  Demonstrations of Shor's Algorithm</title><categories>cs.DS</categories><comments>12 pages,5 figures. The original version has 6 pages. It did not
  point out the reason that some researchers took for granted that quantum
  modlar exponentiation is in polynomial time. In the new version, we indicate
  the reason and analyze some experimental demonstrations of Shor's algorithm.
  Besides, the author Zhenfu Cao is added to the version for his contribution.
  arXiv admin note: text overlap with arXiv:1409.7352</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An efficient quantum modular exponentiation method is indispensible for
Shor's factoring algorithm. But we find that all descriptions presented by
Shor, Nielsen and Chuang, Markov and Saeedi, et al., are flawed. We also remark
that some experimental demonstrations of Shor's algorithm are misleading,
because they violate the necessary condition that the selected number $q=2^s$,
where $s$ is the number of qubits used in the first register, must satisfy $n^2
\leq q &lt; 2n^2$, where $n$ is the large number to be factored.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6257</identifier>
 <datestamp>2014-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6257</id><created>2014-08-26</created><updated>2014-12-12</updated><authors><author><keyname>Huang</keyname><forenames>Sheng</forenames></author><author><keyname>Yang</keyname><forenames>Dan</forenames></author><author><keyname>Zhou</keyname><forenames>Jia</forenames></author><author><keyname>Huangfu</keyname><forenames>Luwen</forenames></author><author><keyname>Zhang</keyname><forenames>Xiaohong</forenames></author></authors><title>Sparse Graph-based Transduction for Image Classification</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by the remarkable successes of Graph-based Transduction (GT) and
Sparse Representation (SR), we present a novel Classifier named Sparse
Graph-based Classifier (SGC) for image classification. In SGC, SR is leveraged
to measure the correlation (similarity) of each two samples and a graph is
constructed for encoding these correlations. Then the Laplacian eigenmapping is
adopted for deriving the graph Laplacian of the graph. Finally, SGC can be
obtained by plugging the graph Laplacian into the conventional GT framework. In
the image classification procedure, SGC utilizes the correlations, which are
encoded in the learned graph Laplacian, to infer the labels of unlabeled
images. SGC inherits the merits of both GT and SR. Compared to SR, SGC improves
the robustness and the discriminating power of GT. Compared to GT, SGC
sufficiently exploits the whole data. Therefore it alleviates the undercomplete
dictionary issue suffered by SR. Four popular image databases are employed for
evaluation. The results demonstrate that SGC can achieve a promising
performance in comparison with the state-of-the-art classifiers, particularly
in the small training sample size case and the noisy sample case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6270</identifier>
 <datestamp>2014-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6270</id><created>2014-08-26</created><updated>2014-10-07</updated><authors><author><keyname>Mehta</keyname><forenames>Ruta</forenames></author><author><keyname>Panageas</keyname><forenames>Ioannis</forenames></author><author><keyname>Piliouras</keyname><forenames>Georgios</forenames></author></authors><title>Natural Selection as an Inhibitor of Genetic Diversity: Multiplicative
  Weights Updates Algorithm and a Conjecture of Haploid Genetics</title><categories>math.DS cs.CE q-bio.QM</categories><comments>18 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a recent series of papers a surprisingly strong connection was discovered
between standard models of evolution in mathematical biology and Multiplicative
Weights Updates Algorithm, a ubiquitous model of online learning and
optimization. These papers establish that mathematical models of biological
evolution are tantamount to applying discrete Multiplicative Weights Updates
Algorithm, a close variant of MWUA, on coordination games. This connection
allows for introducing insights from the study of game theoretic dynamics into
the field of mathematical biology. Using these results as a stepping stone, we
show that mathematical models of haploid evolution imply the extinction of
genetic diversity in the long term limit, a widely believed conjecture in
genetics. In game theoretic terms we show that in the case of coordination
games, under minimal genericity assumptions, discrete MWUA converges to pure
Nash equilibria for all but a zero measure of initial conditions. This result
holds despite the fact that mixed Nash equilibria can be exponentially (or even
uncountably) many, completely dominating in number the set of pure Nash
equilibria. Thus, in haploid organisms the long term preservation of genetic
diversity needs to be safeguarded by other evolutionary mechanisms such as
mutations and speciation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6271</identifier>
 <datestamp>2015-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6271</id><created>2014-08-26</created><authors><author><keyname>Ullah</keyname><forenames>Sajid</forenames></author></authors><title>Autonomous Surveying Boat</title><categories>cs.RO</categories><comments>7 pages,4 figures</comments><report-no>TADR-2014-0285</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The fresh water reservoirs are one of the main power resources of
Pakistan.These water reservoirs are in the form of Tarbela Dam, Mangla Dam,
Bhasha Dam,and Warsak Dam. To estimate the current power capability of the
Dams, the statistical information about the water in the dam has to be clear
and precise. For the purpose of water management monthly or yearly survey of
the dams required. One of the important parameter is to find the water level of
water, which can help us in finding the pressure and flow of water in dams. The
existing surveying systems have some problems, i.e., risky, errors in
measurement and sometimes expensive. Our project has tried a lot to overcome
these flaws and to develop more economical, safe and accurate system for
finding depth values of dams and ponds. The key purpose of Our Project
Autonomous Surveying Boat is to have it log water depths along a predefined set
of points. The Autonomous Surveying Boat floats in water according to
predefined path, getting the coordinates from GPS Sensor and direction is
controlled by using Magnetometer Sensor. It stores its data on SD card as a
text file for later readings. The boat can also be used to find the average
capacity of the dam. The average depth is calculated from the measured depth
values at different set points of the dam. The actual length of the dam is
determined by the magnetometer. The numbers of surveys over the time can help
us in finding the silting ratio in dams.For square dams the length and width of
the dam are measured and the average depth, then using these three parameters
we can estimate the average capacity of the dam.The boat is scalable for
furthered modification if needed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6273</identifier>
 <datestamp>2014-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6273</id><created>2014-08-26</created><authors><author><keyname>Burichenko</keyname><forenames>Vladimir P.</forenames></author></authors><title>On symmetries of the Strassen algorithm</title><categories>cs.CC math.GR math.RA math.RT</categories><comments>16 pp</comments><msc-class>68W40, 15A69, 20C15</msc-class><acm-class>F.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the famous Strassen algorithm for fast multiplication of
matrices. We show that this algorithm has a nontrivial finite group of
automorphisms of order 36 (namely the direct product of two copies of the
symmetric group on 3 symbols), or even 72, if we consider &quot;extended&quot; Strassen
algorithm. This is an indirect evidence that the (unknown at present) optimal
algorithm for multiplication of two size 3 by 3 matrices also may have a large
automorphism group, and this may be a fruitful idea for a search of such an
algorithm. In the beginning we give a brief introduction to the subject, to
make the text accessible for specialists in the representation theory of finite
groups.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6282</identifier>
 <datestamp>2014-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6282</id><created>2014-08-26</created><authors><author><keyname>Cohen</keyname><forenames>Edith</forenames></author><author><keyname>Delling</keyname><forenames>Daniel</forenames></author><author><keyname>Pajor</keyname><forenames>Thomas</forenames></author><author><keyname>Werneck</keyname><forenames>Renato F.</forenames></author></authors><title>Sketch-based Influence Maximization and Computation: Scaling up with
  Guarantees</title><categories>cs.DS cs.SI</categories><comments>10 pages, 5 figures. Appeared at the 23rd Conference on Information
  and Knowledge Management (CIKM 2014) in Shanghai, China</comments><acm-class>G.2.2; H.2.8</acm-class><doi>10.1145/2661829.2662077</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Propagation of contagion through networks is a fundamental process. It is
used to model the spread of information, influence, or a viral infection.
Diffusion patterns can be specified by a probabilistic model, such as
Independent Cascade (IC), or captured by a set of representative traces.
  Basic computational problems in the study of diffusion are influence queries
(determining the potency of a specified seed set of nodes) and Influence
Maximization (identifying the most influential seed set of a given size).
Answering each influence query involves many edge traversals, and does not
scale when there are many queries on very large graphs. The gold standard for
Influence Maximization is the greedy algorithm, which iteratively adds to the
seed set a node maximizing the marginal gain in influence. Greedy has a
guaranteed approximation ratio of at least (1-1/e) and actually produces a
sequence of nodes, with each prefix having approximation guarantee with respect
to the same-size optimum. Since Greedy does not scale well beyond a few million
edges, for larger inputs one must currently use either heuristics or
alternative algorithms designed for a pre-specified small seed set size.
  We develop a novel sketch-based design for influence computation. Our greedy
Sketch-based Influence Maximization (SKIM) algorithm scales to graphs with
billions of edges, with one to two orders of magnitude speedup over the best
greedy methods. It still has a guaranteed approximation ratio, and in practice
its quality nearly matches that of exact greedy. We also present influence
oracles, which use linear-time preprocessing to generate a small sketch for
each node, allowing the influence of any seed set to be quickly answered from
the sketches of its nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6285</identifier>
 <datestamp>2014-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6285</id><created>2014-08-26</created><authors><author><keyname>Prabhakaran</keyname><forenames>Manoj M.</forenames></author><author><keyname>Prabhakaran</keyname><forenames>Vinod M.</forenames></author></authors><title>Tension Bounds for Information Complexity</title><categories>cs.CC cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main contribution of this work is to relate information complexity to
&quot;tension&quot; [Prabhakaran and Prabhakaran, 2014] - an information-theoretic
quantity defined with no reference to protocols - and to illustrate that it
allows deriving strong lower-bounds on information complexity. In particular,
we use a very special case of this connection to give a quantitatively tighter
connection between information complexity and discrepancy than the one in the
work of Braverman and Weinstein (2012) (albeit, restricted to independent
inputs). Further, as tension is in fact a multi-dimensional notion, it enables
us to bound the 2-dimensional region that represents the trade-off between the
amounts of communication in the two directions in a 2-party protocol.
  This work is also intended to highlight tension as a fundamental measure of
correlation between a pair of random variables, with rich connections to a
variety of questions in computer science and information theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6290</identifier>
 <datestamp>2014-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6290</id><created>2014-08-26</created><authors><author><keyname>Kanaya</keyname><forenames>Ichiroh</forenames></author><author><keyname>Kanazawa</keyname><forenames>Mayuko</forenames></author><author><keyname>Imura</keyname><forenames>Masataka</forenames></author></authors><title>Function + Action = Interaction</title><categories>cs.HC</categories><acm-class>H.5.2; J.5</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This article presents the mathematical background of general interactive
systems. The first principle of designing a large system is to _divide and
conquer_, which implies that we could possibly reduce human error if we divided
a large system in smaller subsystems. Interactive systems are, however, often
composed of many subsystems that are _organically_ connected to one another and
thus difficult to divide. In other words, we cannot apply a framework of set
theory to the programming of interactive systems. We can overcome this
difficulty by applying a framework of category theory (Kleisli category) to the
programming, but this requires highly abstract mathematics, which is not very
popular. In this article we introduce the fundamental idea of category theory
using only lambda calculus, and then demonstrate how it can be used in the
practical design of an interactive system. Finally, we mention how this
discussion relates to category theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6292</identifier>
 <datestamp>2014-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6292</id><created>2014-08-26</created><authors><author><keyname>Gao</keyname><forenames>Yihan</forenames></author><author><keyname>Parameswaran</keyname><forenames>Aditya</forenames></author></authors><title>Finish Them!: Pricing Algorithms for Human Computation</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a batch of human computation tasks, a commonly ignored aspect is how
the price (i.e., the reward paid to human workers) of these tasks must be set
or varied in order to meet latency or cost constraints. Often, the price is set
up-front and not modified, leading to either a much higher monetary cost than
needed (if the price is set too high), or to a much larger latency than
expected (if the price is set too low). Leveraging a pricing model from prior
work, we develop algorithms to optimally set and then vary price over time in
order to meet a (a) user-specified deadline while minimizing total monetary
cost (b) user-specified monetary budget constraint while minimizing total
elapsed time. We leverage techniques from decision theory (specifically, Markov
Decision Processes) for both these problems, and demonstrate that our
techniques lead to upto 30\% reduction in cost over schemes proposed in prior
work. Furthermore, we develop techniques to speed-up the computation, enabling
users to leverage the price setting algorithms on-the-fly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6299</identifier>
 <datestamp>2015-05-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6299</id><created>2014-08-26</created><updated>2015-05-07</updated><authors><author><keyname>Mang</keyname><forenames>Andreas</forenames></author><author><keyname>Biros</keyname><forenames>George</forenames></author></authors><title>An inexact Newton-Krylov algorithm for constrained diffeomorphic image
  registration</title><categories>math.NA cs.CV cs.NA math.OC</categories><comments>32 pages; 10 figures; 9 tables</comments><msc-class>68U10, 49J20, 35Q93, 65K10, 76D55</msc-class><journal-ref>SIAM J. Imaging Sci., 8(2):1030-1069, 2015</journal-ref><doi>10.1137/140984002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose numerical algorithms for solving large deformation diffeomorphic
image registration problems. We formulate the nonrigid image registration
problem as a problem of optimal control. This leads to an infinite-dimensional
partial differential equation (PDE) constrained optimization problem.
  The PDE constraint consists, in its simplest form, of a hyperbolic transport
equation for the evolution of the image intensity. The control variable is the
velocity field. Tikhonov regularization on the control ensures well-posedness.
We consider standard smoothness regularization based on $H^1$- or
$H^2$-seminorms. We augment this regularization scheme with a constraint on the
divergence of the velocity field rendering the deformation incompressible and
thus ensuring that the determinant of the deformation gradient is equal to one,
up to the numerical error.
  We use a Fourier pseudospectral discretization in space and a Chebyshev
pseudospectral discretization in time. We use a preconditioned, globalized,
matrix-free, inexact Newton-Krylov method for numerical optimization. A
parameter continuation is designed to estimate an optimal regularization
parameter. Regularity is ensured by controlling the geometric properties of the
deformation field. Overall, we arrive at a black-box solver. We study spectral
properties of the Hessian, grid convergence, numerical accuracy, computational
efficiency, and deformation regularity of our scheme. We compare the designed
Newton-Krylov methods with a globalized preconditioned gradient descent. We
study the influence of a varying number of unknowns in time.
  The reported results demonstrate excellent numerical accuracy, guaranteed
local deformation regularity, and computational efficiency with an optional
control on local mass conservation. The Newton-Krylov methods clearly
outperform the Picard method if high accuracy of the inversion is required.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6303</identifier>
 <datestamp>2014-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6303</id><created>2014-08-26</created><authors><author><keyname>Wegner</keyname><forenames>Anatol E.</forenames></author></authors><title>Motif Conservation Laws for the Configuration Model</title><categories>q-bio.MN cs.SI physics.soc-ph</categories><comments>3 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The observation that some subgraphs, called motifs, appear more often in real
networks than in their randomized counterparts has attracted much attention in
the scientific community. In the prevalent approach the detection of motifs is
based on comparing subgraph counts in a network with their counterparts in the
configuration model with the same degree distribution as the network. In this
short note we derive conservation laws that relate motif counts in the
configuration model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6311</identifier>
 <datestamp>2014-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6311</id><created>2014-08-27</created><updated>2014-09-03</updated><authors><author><keyname>Xiong</keyname><forenames>Hai</forenames></author><author><keyname>Qu</keyname><forenames>Longjiang</forenames></author></authors><title>A Note on Cross Correlation Distribution of Ternary m-Sequences</title><categories>cs.IT math.IT</categories><comments>After posting our manuscript on arxiv, we recived an email from
  Yongbo Xia. He told us that they also got the same result as in our
  manuscript. And their paper was accepted by 2014 SETA on June 14th. And the
  information of their paper is&quot;Y. Xia, T. Helleseth and G. Wu, A note on
  cross-correlation distribution between a ternary m-sequence and its decimated
  sequence, to appear in SETA2014&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note, we prove a conjecture proposed by Tao Zhang, Shuxing Li, Tao
Feng and Gennian Ge, IEEE Transaction on Information Theory, vol. 60, no. 5,
May 2014. This conjecture is about the cross correlation distribution of
ternary $m$-sequences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6315</identifier>
 <datestamp>2014-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6315</id><created>2014-08-27</created><authors><author><keyname>Mehta</keyname><forenames>Rahul</forenames></author></authors><title>2048 is (PSPACE) Hard, but Sometimes Easy</title><categories>cs.CC</categories><comments>13 pages, 11 figures</comments><acm-class>F.1; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that a variant of 2048, a popular online puzzle game, is
PSPACE-Complete. Our hardness result holds for a version of the problem where
the player has oracle access to the computer player's moves. Specifically, we
show that for an $n \times n$ game board $\mathcal{G}$, computing a sequence of
moves to reach a particular configuration $\mathbb{C}$ from an initial
configuration $\mathbb{C}_0$ is PSPACE-Complete. Our reduction is from
Nondeterministic Constraint Logic (NCL). We also show that determining whether
or not there exists a fixed sequence of moves $\mathcal{S} \in \{\Uparrow,
\Downarrow, \Leftarrow, \Rightarrow\}^k$ of length $k$ that results in a
winning configuration for an $n \times n$ game board is fixed-parameter
tractable (FPT). We describe an algorithm to solve this problem in $O(4^k n^2)$
time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6321</identifier>
 <datestamp>2014-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6321</id><created>2014-08-27</created><authors><author><keyname>Bannister</keyname><forenames>Michael J.</forenames></author><author><keyname>Eppstein</keyname><forenames>David</forenames></author></authors><title>Crossing Minimization for 1-page and 2-page Drawings of Graphs with
  Bounded Treewidth</title><categories>cs.DS cs.DM math.CO</categories><comments>Graph Drawing 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate crossing minimization for 1-page and 2-page book drawings. We
show that computing the 1-page crossing number is fixed-parameter tractable
with respect to the number of crossings, that testing 2-page planarity is
fixed-parameter tractable with respect to treewidth, and that computing the
2-page crossing number is fixed-parameter tractable with respect to the sum of
the number of crossings and the treewidth of the input graph. We prove these
results via Courcelle's theorem on the fixed-parameter tractability of
properties expressible in monadic second order logic for graphs of bounded
treewidth.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6328</identifier>
 <datestamp>2014-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6328</id><created>2014-08-27</created><authors><author><keyname>Rossigneux</keyname><forenames>Francois</forenames></author><author><keyname>Gelas</keyname><forenames>Jean-Patrick</forenames></author><author><keyname>Lefevre</keyname><forenames>Laurent</forenames></author><author><keyname>de Assuncao</keyname><forenames>Marcos Dias</forenames></author></authors><title>A Generic and Extensible Framework for Monitoring Energy Consumption of
  OpenStack Clouds</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although cloud computing has been transformational to the IT industry, it is
built on large data centres that often consume massive amounts of electrical
power. Efforts have been made to reduce the energy clouds consume, with certain
data centres now approaching a Power Usage Effectiveness (PUE) factor of 1.08.
While this is an incredible mark, it also means that the IT infrastructure
accounts for a large part of the power consumed by a data centre. Hence, means
to monitor and analyse how energy is spent have never been so crucial. Such
monitoring is required not only for understanding how power is consumed, but
also for assessing the impact of energy management policies. In this article,
we draw lessons from experience on monitoring large-scale systems and introduce
an energy monitoring software framework called KiloWatt API (KWAPI), able to
handle OpenStack clouds. The framework --- whose architecture is scalable,
extensible, and completely integrated into OpenStack --- supports several
wattmeter devices, multiple measurement formats, and minimises communication
overhead.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6334</identifier>
 <datestamp>2014-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6334</id><created>2014-08-27</created><updated>2014-09-17</updated><authors><author><keyname>Daniels</keyname><forenames>Sunny</forenames></author></authors><title>A constructive proof presenting languages in $\Sigma_2^P$ that cannot be
  decided by circuit families of size $n^k$</title><categories>cs.CC</categories><comments>This is a corrected version of my previous article (of the same name)
  which attracted the attention of Professor Lance Fortnow at Georgia Institute
  of Technology (&quot;Sixteen Years in the Making&quot; in his Complexity Theory Blog).
  The original had a missing closing bracket in a footnote and a reference to
  the wrong step in the machine for $\Lambda_k$</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As far as I know, at the time that I originally devised this result (1998),
this was the first constructive proof that, for any integer $k$, there is a
language in $\Sigma_2^P$ that cannot be simulated by a family of logic circuits
of size $n^k$. However, this result had previously been proved
non-constructively: see Cai and Watanabe [CW08] for more information on the
history of this problem.
  This constructive proof is based upon constructing a language $\Gamma$
derived from the satisfiabiility problem, and a language $\Lambda_k$ defined by
an alternating Turing machine. We show that the union of $\Gamma$ and
$\Lambda_k$ cannot be simulated by circuits of size $n^k$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6335</identifier>
 <datestamp>2014-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6335</id><created>2014-08-27</created><authors><author><keyname>Yaroslavsky</keyname><forenames>Leonid</forenames></author></authors><title>Compression, Restoration, Re-sampling, Compressive Sensing: Fast
  Transforms in Digital Imaging</title><categories>cs.CV physics.optics</categories><comments>41 pages, 16 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Transform image processing methods are methods that work in domains of image
transforms, such as Discrete Fourier, Discrete Cosine, Wavelet and alike. They
are the basic tool in image compression, in image restoration, in image
re-sampling and geometrical transformations and can be traced back to early
1970-ths. The paper presents a review of these methods with emphasis on their
comparison and relationships, from the very first steps of transform image
compression methods to adaptive and local adaptive transform domain filters for
image restoration, to methods of precise image re-sampling and image
reconstruction from sparse samples and up to &quot;compressive sensing&quot; approach
that has gained popularity in last few years. The review has a tutorial
character and purpose.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6340</identifier>
 <datestamp>2014-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6340</id><created>2014-08-27</created><authors><author><keyname>Mahalanobis</keyname><forenames>Ayan</forenames></author><author><keyname>Singh</keyname><forenames>Anupam</forenames></author></authors><title>MOR Cryptosystem and classical Chevalley groups in odd characteristic</title><categories>math.GR cs.CR cs.IT math.IT</categories><msc-class>94A60, 20H30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the MOR cryptosystem using finite classical Chevalley
groups over a finite field of odd characteristic. In the process we develop an
algorithm for these Chevalley groups in the same spirit as the row-column
operation for special linear group. We focus our study on orthogonal and
symplectic groups. We find the hardness of the proposed MOR cryptosystem for
these groups.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6347</identifier>
 <datestamp>2014-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6347</id><created>2014-08-27</created><authors><author><keyname>Akhtar</keyname><forenames>Aleem</forenames></author><author><keyname>Shafi</keyname><forenames>Aamir</forenames></author><author><keyname>Jameel</keyname><forenames>Mohsan</forenames></author></authors><title>Design and Implementation of Parallel Debugger and Profiler for MPJ
  Express</title><categories>cs.DC cs.SE</categories><comments>6 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  MPJ Express is a messaging system that allows computational scientists to
write and execute parallel Java applications on High Performance Computing
(HPC) hardware. Despite its successful adoption in the Java HPC community, the
MPJ Express software currently does not provide any support for debugging and
profiling parallel applications and hence forces its users to rely on manual
and tedious debugging/profiling methods. Support for such tools is essential to
help application developers increase their overall productivity. To address
this we have developed debugging and profiling tools for MPJ Express, which are
the main topic of this paper. Key design goals for these tools include: 1)
maintain compatibility with existing logging, debugging, and visualizing tools,
2) build these tools by extending existing debugging/profiling tools instead of
reinventing the wheel. The first tool, named MPJDebug, builds on the
open-source Eclipse Integrated Development Environment (IDE). It provides an
Eclipse-based plugin developed using the Eclipse Plugin Development Environment
(PDE). The default Eclipse debugger currently does not support debugging
parallel applications running on a compute cluster. The second tool, named
MPJProf, is a utility based on Tuning and Analysis Utility (TAU)-an open-source
performance evaluation tool. Our goal here is to exploit TAU to profile Java
applications parallelized using MPJ Express by generating profiles and traces,
which can later be visualized using existing tools like paraprof and Jumpshot.
Towards the end of the paper, we quantify the overhead of using MPJProf, which
we found to be negligible in the profiling stage of parallel application
development.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6350</identifier>
 <datestamp>2014-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6350</id><created>2014-08-27</created><authors><author><keyname>Insa-Cabrera</keyname><forenames>Javier</forenames></author><author><keyname>Hern&#xe1;ndez-Orallo</keyname><forenames>Jos&#xe9;</forenames></author></authors><title>Definition and properties to assess multi-agent environments as social
  intelligence tests</title><categories>cs.MA cs.AI</categories><comments>53 pages + appendix</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social intelligence in natural and artificial systems is usually measured by
the evaluation of associated traits or tasks that are deemed to represent some
facets of social behaviour. The amalgamation of these traits is then used to
configure the intuitive notion of social intelligence. Instead, in this paper
we start from a parametrised definition of social intelligence as the expected
performance in a set of environments with several agents, and we assess and
derive tests from it. This definition makes several dependencies explicit: (1)
the definition depends on the choice (and weight) of environments and agents,
(2) the definition may include both competitive and cooperative behaviours
depending on how agents and rewards are arranged into teams, (3) the definition
mostly depends on the abilities of other agents, and (4) the actual difference
between social intelligence and general intelligence (or other abilities)
depends on these choices. As a result, we address the problem of converting
this definition into a more precise one where some fundamental properties
ensuring social behaviour (such as action and reward dependency and
anticipation on competitive/cooperative behaviours) are met as well as some
other more instrumental properties (such as secernment, boundedness, symmetry,
validity, reliability, efficiency), which are convenient to convert the
definition into a practical test. From the definition and the formalised
properties, we take a look at several representative multi-agent environments,
tests and games to see whether they meet these properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6351</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6351</id><created>2014-08-27</created><authors><author><keyname>Kaufman</keyname><forenames>Tali</forenames></author><author><keyname>Kazhdan</keyname><forenames>David</forenames></author><author><keyname>Lubotzky</keyname><forenames>Alexander</forenames></author></authors><title>Ramanujan Complexes and bounded degree topological expanders</title><categories>math.CO cs.CC math.GR math.GT</categories><comments>To appear in FOCS 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Expander graphs have been a focus of attention in computer science in the
last four decades. In recent years a high dimensional theory of expanders is
emerging. There are several possible generalizations of the theory of expansion
to simplicial complexes, among them stand out coboundary expansion and
topological expanders. It is known that for every d there are unbounded degree
simplicial complexes of dimension d with these properties. However, a major
open problem, formulated by Gromov, is whether bounded degree high dimensional
expanders, according to these definitions, exist for d &gt;= 2. We present an
explicit construction of bounded degree complexes of dimension d = 2 which are
high dimensional expanders. More precisely, our main result says that the
2-skeletons of the 3-dimensional Ramanujan complexes are topological expanders.
Assuming a conjecture of Serre on the congruence subgroup property, infinitely
many of them are also coboundary expanders.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6367</identifier>
 <datestamp>2014-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6367</id><created>2014-08-27</created><authors><author><keyname>Conradie</keyname><forenames>Willem</forenames></author><author><keyname>Craig</keyname><forenames>Andrew</forenames></author></authors><title>Canonicity results for mu-calculi: an algorithmic approach</title><categories>math.LO cs.LO</categories><msc-class>03B45</msc-class><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the canonicity of inequalities of the intuitionistic
mu-calculus. The notion of canonicity in the presence of fixed point operators
is not entirely straightforward. In the algebraic setting of canonical
extensions we examine both the usual notion of canonicity and what we will call
tame canonicity. This latter concept has previously been investigated for the
classical mu-calculus by Bezhanishvili and Hodkinson. Our approach is in the
spirit of Sahlqvist theory. That is, we identify syntactically-defined classes
of inequalities, namely the restricted inductive and tame inductive
inequalities, which are, respectively, canonical or tame canonical. Our
approach is to use an algorithm which processes inequalities with the aim of
eliminating propositional variables. The algorithm we introduce is closely
related to the algorithms ALBA and mu-ALBA studied by Conradie, Palmigiano, et
al. It is based on a calculus of rewrite rules, the soundness of which rests
upon the way in which algebras embed into their canonical extensions and the
order-theoretic properties of the latter. We show that the algorithm succeeds
on every restricted inductive inequality by means of a so-called proper run,
and that this is sufficient to guarantee their canonicity. Likewise, we are
able to show that the algorithm succeeds on every tame inductive inequality by
means of a so-called tame run. In turn, this guarantees their tame canonicity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6369</identifier>
 <datestamp>2014-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6369</id><created>2014-08-27</created><authors><author><keyname>Sanguinetti</keyname><forenames>Luca</forenames></author><author><keyname>Bjornson</keyname><forenames>Emil</forenames></author><author><keyname>Debbah</keyname><forenames>Merouane</forenames></author><author><keyname>Moustakas</keyname><forenames>Aris</forenames></author></authors><title>Optimal Linear Precoding in Multi-User MIMO Systems: A Large System
  Analysis</title><categories>cs.IT math.IT</categories><comments>6 pages, 2 figures, IEEE Global Communications Conference (GLOBECOM),
  Austin, Texas, Dec. 2014. An extended version of this work is available at
  http://arxiv.org/abs/1406.5988</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the downlink of a single-cell multi-user MIMO system in which the
base station makes use of $N$ antennas to communicate with $K$ single-antenna
user equipments (UEs) randomly positioned in the coverage area. In particular,
we focus on the problem of designing the optimal linear precoding for
minimizing the total power consumption while satisfying a set of target
signal-to-interference-plus-noise ratios (SINRs). To gain insights into the
structure of the optimal solution and reduce the computational complexity for
its evaluation, we analyze the asymptotic regime where $N$ and $K$ grow large
with a given ratio and make use of recent results from large system analysis to
compute the asymptotic solution. Then, we concentrate on the asymptotically
design of heuristic linear precoding techniques. Interestingly, it turns out
that the regularized zero-forcing (RZF) precoder is equivalent to the optimal
one when the ratio between the SINR requirement and the average channel
attenuation is the same for all UEs. If this condition does not hold true but
only the same SINR constraint is imposed for all UEs, then the RZF can be
modified to still achieve optimality if statistical information of the UE
positions is available at the BS. Numerical results are used to evaluate the
performance gap in the finite system regime and to make comparisons among the
precoding techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6373</identifier>
 <datestamp>2014-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6373</id><created>2014-08-27</created><authors><author><keyname>Hahn</keyname><forenames>T.</forenames></author></authors><title>Concurrent Cuba</title><categories>physics.comp-ph cs.MS hep-ph</categories><comments>LaTeX, 14 pages</comments><report-no>MPP-2014-327</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The parallel version of the multidimensional numerical integration package
Cuba is presented and achievable speed-ups discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6378</identifier>
 <datestamp>2014-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6378</id><created>2014-08-27</created><authors><author><keyname>Florian</keyname><forenames>Meier</forenames></author><author><keyname>Ueli</keyname><forenames>Peter</forenames></author></authors><title>Push is Fast on Sparse Random Graphs</title><categories>math.CO cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the classical push broadcast process on a large class of sparse
random multigraphs that includes random power law graphs and multigraphs. Our
analysis shows that for every $\varepsilon&gt;0$, whp $O(\log n)$ rounds are
sufficient to inform all but an $\varepsilon$-fraction of the vertices.
  It is not hard to see that, e.g. for random power law graphs, the push
process needs whp $n^{\Omega(1)}$ rounds to inform all vertices. Fountoulakis,
Panagiotou and Sauerwald proved that for random graphs that have power law
degree sequences with $\beta&gt;3$, the push-pull protocol needs $\Omega(\log n)$
to inform all but $\varepsilon n$ vertices whp. Our result demonstrates that,
for such random graphs, the pull mechanism does not (asymptotically) improve
the running time. This is surprising as it is known that, on random power law
graphs with $2&lt;\beta&lt;3$, push-pull is exponentially faster than pull.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6385</identifier>
 <datestamp>2015-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6385</id><created>2014-08-27</created><updated>2015-01-27</updated><authors><author><keyname>Doshi</keyname><forenames>Jainam</forenames></author><author><keyname>Vaze</keyname><forenames>Rahul</forenames></author></authors><title>Long term Throughput and Approximate Capacity of Transmitter-Receiver
  Energy Harvesting Channel with Fading</title><categories>cs.IT math.IT</categories><comments>To appear in ICCS 2014, Macau in Nov. 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We first consider an energy harvesting channel with fading, where only the
transmitter harvests energy from natural sources. We bound the optimal long
term throughput by a constant for a class of energy arrival distributions. The
proposed method also gives a constant approximation to the capacity of the
energy harvesting channel with fading. Next, we consider a more general system
where both the transmitter and the receiver employ energy harvesting to power
themselves. In this case, we show that finding an approximation to the optimal
long term throughput is far more difficult, and identify a special case of unit
battery capacity at both the transmitter and the receiver for which we obtain a
universal bound on the ratio of the upper and lower bound on the long term
throughput.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6388</identifier>
 <datestamp>2015-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6388</id><created>2014-08-27</created><updated>2015-01-14</updated><authors><author><keyname>Garnero</keyname><forenames>Valentin</forenames></author><author><keyname>Sau</keyname><forenames>Ignasi</forenames></author><author><keyname>Thilikos</keyname><forenames>Dimitrios M.</forenames></author></authors><title>A linear kernel for planar red-blue dominating set</title><categories>cs.DS</categories><comments>15 pages, 4 figures</comments><msc-class>05C85, 05C10</msc-class><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the Red-Blue Dominating Set problem, we are given a bipartite graph $G =
(V_B \cup V_R,E)$ and an integer $k$, and asked whether $G$ has a subset $D
\subseteq V_B$ of at most $k$ &quot;blue&quot; vertices such that each &quot;red&quot; vertex from
$V_R$ is adjacent to a vertex in $D$. We provide the first explicit linear
kernel for this problem on planar graphs, of size at most $46k$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6395</identifier>
 <datestamp>2014-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6395</id><created>2014-08-27</created><updated>2014-09-03</updated><authors><author><keyname>Darari</keyname><forenames>Fariz</forenames></author><author><keyname>Razniewski</keyname><forenames>Simon</forenames></author><author><keyname>Nutt</keyname><forenames>Werner</forenames></author></authors><title>Bridging the Semantic Gap between RDF and SPARQL using Completeness
  Statements [Extended Version]</title><categories>cs.DB</categories><comments>This paper is an extended version with proofs of a poster paper at
  ISWC 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  RDF data is often treated as incomplete, following the Open-World Assumption.
On the other hand, SPARQL, the standard query language over RDF, usually
follows the Closed-World Assumption, assuming RDF data to be complete. This
gives rise to a semantic gap between RDF and SPARQL. In this paper, we address
how to close the semantic gap between RDF and SPARQL in terms of certain
answers and possible answers using completeness statements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6418</identifier>
 <datestamp>2014-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6418</id><created>2014-08-09</created><authors><author><keyname>Barbu</keyname><forenames>Andrei</forenames></author><author><keyname>Bridge</keyname><forenames>Alexander</forenames></author><author><keyname>Burchill</keyname><forenames>Zachary</forenames></author><author><keyname>Coroian</keyname><forenames>Dan</forenames></author><author><keyname>Dickinson</keyname><forenames>Sven</forenames></author><author><keyname>Fidler</keyname><forenames>Sanja</forenames></author><author><keyname>Michaux</keyname><forenames>Aaron</forenames></author><author><keyname>Mussman</keyname><forenames>Sam</forenames></author><author><keyname>Narayanaswamy</keyname><forenames>Siddharth</forenames></author><author><keyname>Salvi</keyname><forenames>Dhaval</forenames></author><author><keyname>Schmidt</keyname><forenames>Lara</forenames></author><author><keyname>Shangguan</keyname><forenames>Jiangnan</forenames></author><author><keyname>Siskind</keyname><forenames>Jeffrey Mark</forenames></author><author><keyname>Waggoner</keyname><forenames>Jarrell</forenames></author><author><keyname>Wang</keyname><forenames>Song</forenames></author><author><keyname>Wei</keyname><forenames>Jinlian</forenames></author><author><keyname>Yin</keyname><forenames>Yifan</forenames></author><author><keyname>Zhang</keyname><forenames>Zhiqi</forenames></author></authors><title>Video In Sentences Out</title><categories>cs.CV cs.CL cs.IR</categories><comments>Appears in Proceedings of the Twenty-Eighth Conference on Uncertainty
  in Artificial Intelligence (UAI2012)</comments><proxy>auai</proxy><report-no>UAI-P-2012-PG-102-112</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a system that produces sentential descriptions of video: who did
what to whom, and where and how they did it. Action class is rendered as a
verb, participant objects as noun phrases, properties of those objects as
adjectival modifiers in those noun phrases, spatial relations between those
participants as prepositional phrases, and characteristics of the event as
prepositional-phrase adjuncts and adverbial modifiers. Extracting the
information needed to render these linguistic entities requires an approach to
event recognition that recovers object tracks, the trackto-role assignments,
and changing body posture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6427</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6427</id><created>2014-08-27</created><updated>2014-12-07</updated><authors><author><keyname>Alaa</keyname><forenames>Ahmed M.</forenames></author><author><keyname>Ismail</keyname><forenames>Mahmoud H.</forenames></author></authors><title>Degrees-of-Freedom of the K-user SISO Interference Channel with Blind
  Interference Alignment using Staggered Antenna Switching</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this letter, we present the first characterization for the achievable
Degrees-of-Freedom (DoF) by Blind Interference Alignment (BIA) using staggered
antenna switching in the $K$-user Gaussian Interference Channel. In such
scheme, each transmitter is equipped with one conventional antenna and each
receiver is equipped with one reconfigurable (multi-mode) antenna. Assuming
that the channel is known to the receivers only, we show that BIA can achieve
$\frac{2K}{K+2}$ DoF, which surpasses the sum DoF achieved by previously known
interference alignment schemes with delayed channel state information at
transmitters (CSIT). This result implies that the sum DoF is upper bounded by
2, which means that the best we can do with BIA is to double the DoF achieved
by orthogonal multiple access schemes. Moreover, we propose an algorithm to
generate the transmit beamforming vectors and the reconfigurable antenna
switching patterns, and apply this algorithm to the 4-user SISO Interference
Channel, showing that $\frac{4}{3}$ sum DoF is achievable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6434</identifier>
 <datestamp>2014-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6434</id><created>2014-08-27</created><authors><author><keyname>Allamaraju</keyname><forenames>Rakshit</forenames></author><author><keyname>Reish</keyname><forenames>Ben</forenames></author><author><keyname>Axelrod</keyname><forenames>Allan</forenames></author></authors><title>Sensor Noise Rejection</title><categories>stat.AP cs.RO</categories><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  An inaccessible control architecture caused an undesirable influence on a
UAV. The encountered noise in the performance was modeled using stochastic
methods and a corrective term was implemented on an external controller. Our
findings suggest that the sonar noise problem is unconventional may warrant the
development of a new methodology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6438</identifier>
 <datestamp>2014-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6438</id><created>2014-08-24</created><authors><author><keyname>Alanazi</keyname><forenames>Eisa</forenames></author></authors><title>A Note on the Ranking of Saudi Arabian Universities based on
  highlycited.com</title><categories>cs.DL</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, Thomson Reuters has published its 2014 list of highly cited
researchers (HCRs)[1]. Initial studies over the list [2] suggested that some
universities (for instance, King Abdulaziz University in Saudi Arabia) may have
been manipulating its world ranking by contracting with highly cited
researchers. In this work, we analyse the ranking of other Saudi universities
based solely on the list. Our analysis suggests that other universities in
Saudi Arabia do not follow the steps of King Abdulaziz University when it comes
to contracting with HCRs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6451</identifier>
 <datestamp>2014-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6451</id><created>2014-08-27</created><authors><author><keyname>Waldhauser</keyname><forenames>Christoph</forenames></author></authors><title>Public Spheres in Twitter- and Blogosphere. Evidence from the US</title><categories>cs.SI physics.soc-ph stat.AP</categories><comments>15 pages, 3 figures, presented at the European Political Science
  Conference EPSA 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The political requires a forum for its deliberation and Habermas has located
it in the public spheres. Originally, mass media's role was one of a
facilitator of these debates. However, under the immense pressures of free
market competition and mobile audiences, mass media prefers episodic over
thematic news. On the opposite end of the spectrum, social media has been
heralded as a new forum, a reincarnation of the ailing public spheres to
further the deliberation of the political. But do the followers of political
parties in social media endorse thematic or episodic content?
  To answer this question, I look at the most recent 3,200 tweets that were
broadcast from the Republican and Democratic Twitter accounts. By employing
Latent dirichlet allocation, I extract the prevailing topics of these tweets
and linked websites. Generalized linear models are used to describe the
relationship between episodicity, thematicity and the endorsement counts of the
posts analyzed.
  I find that there is a stark contrast between the behavior of Democratic and
Republican followers. In general, there seems to be a slight preference for
thematic messages. Interestingly, the distance to an election increases the
odds of a message to be endorsed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6466</identifier>
 <datestamp>2014-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6466</id><created>2014-08-26</created><authors><author><keyname>Lou</keyname><forenames>Vincent Yun</forenames></author><author><keyname>Bhagat</keyname><forenames>Smriti</forenames></author><author><keyname>Lakshmanan</keyname><forenames>Laks V. S.</forenames></author><author><keyname>Vaswani</keyname><forenames>Sharan</forenames></author></authors><title>Modeling Non-Progressive Phenomena for Influence Propagation</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent work on modeling influence propagation focus on progressive models,
i.e., once a node is influenced (active) the node stays in that state and
cannot become inactive. However, this assumption is unrealistic in many
settings where nodes can transition between active and inactive states. For
instance, a user of a social network may stop using an app and become inactive,
but again activate when instigated by a friend, or when the app adds a new
feature or releases a new version. In this work, we study such non-progressive
phenomena and propose an efficient model of influence propagation.
Specifically, we model in influence propagation as a continuous-time Markov
process with 2 states: active and inactive. Such a model is both highly
scalable (we evaluated on graphs with over 2 million nodes), 17-20 times
faster, and more accurate for estimating the spread of influence, as compared
with state-of-the-art progressive models for several applications where nodes
may switch states.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6474</identifier>
 <datestamp>2014-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6474</id><created>2014-08-24</created><authors><author><keyname>Hales</keyname><forenames>Thomas C.</forenames></author></authors><title>Developments in Formal Proofs</title><categories>cs.LO math.LO</categories><comments>Bourbaki seminar report 1086, June 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This report describes three particular technological advances in formal
proofs. The HOL Light proof assistant will be used to illustrate the design of
a highly reliable system. Today, proof assistants can verify large bodies of
advanced mathematics; and as an example, we turn to the formal proof in Coq of
the Feit-Thompson Odd Order theorem in group theory. Finally, we discuss
advances in the automation of formal proofs, as implemented in proof assistants
such as Mizar, Coq, Isabelle, and HOL Light.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6475</identifier>
 <datestamp>2014-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6475</id><created>2014-08-26</created><updated>2014-08-29</updated><authors><author><keyname>Doberkat</keyname><forenames>Ernst-Erich</forenames></author></authors><title>Sets, the Axiom of Choice, And All That: A Tutorial</title><categories>cs.LO cs.DM</categories><msc-class>03-01, 03E25, 91A99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This tutorial deal with the Axiom of Choice and some of its applications to
topics related to Computer Science. We will see that the Axiom of Choice is
equivalent to some well-known proof principles like Zorn's Lemma or Tuckey's
Maximality Principle. We try to touch upon some topics, which appear to be
important for developing mathematical structures within computer science.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6477</identifier>
 <datestamp>2014-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6477</id><created>2014-08-25</created><authors><author><keyname>Przyby&#x142;ko</keyname><forenames>Marcin</forenames><affiliation>University of New Caledonia, University of Warsaw</affiliation></author></authors><title>Tree games with regular objectives</title><categories>cs.LO cs.FL cs.GT</categories><comments>In Proceedings GandALF 2014, arXiv:1408.5560</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 161, 2014, pp. 231-244</journal-ref><doi>10.4204/EPTCS.161.20</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study tree games developed recently by Matteo Mio as a game interpretation
of the probabilistic $\mu$-calculus. With expressive power comes complexity.
Mio showed that tree games are able to encode Blackwell games and,
consequently, are not determined under deterministic strategies.
  We show that non-stochastic tree games with objectives recognisable by
so-called game automata are determined under deterministic, finite memory
strategies. Moreover, we give an elementary algorithmic procedure which, for an
arbitrary regular language L and a finite non-stochastic tree game with a
winning objective L decides if the game is determined under deterministic
strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6485</identifier>
 <datestamp>2014-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6485</id><created>2014-08-27</created><authors><author><keyname>McCreesh</keyname><forenames>Ciaran</forenames></author><author><keyname>Prosser</keyname><forenames>Patrick</forenames></author></authors><title>Finding Maximum k-Cliques Faster using Lazy Global Domination</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A clique in a graph is a set of vertices, each of which is adjacent to every
other vertex in this set. A k-clique relaxes this requirement, requiring
vertices to be within a distance k of each other, rather than directly
adjacent. In theory, a maximum clique algorithm can easily be adapted to solve
the maximum k-clique problem. We use a state of the art maximum clique
algorithm to show that this is feasible in practice, and introduce a lazy
global domination rule which sometimes vastly reduces the search space. We
include experimental results for a range of real-world and benchmark graphs,
and a detailed look at random graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6487</identifier>
 <datestamp>2014-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6487</id><created>2014-08-27</created><updated>2014-09-17</updated><authors><author><keyname>Hochman</keyname><forenames>Michael</forenames><affiliation>LACL</affiliation></author><author><keyname>Vanier</keyname><forenames>Pascal</forenames><affiliation>LACL</affiliation></author></authors><title>Turing degree spectra of minimal subshifts</title><categories>cs.FL cs.DM cs.LO</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Subshifts are shift invariant closed subsets of $\Sigma^{\mathbb{Z}^d}$ ,
minimal subshifts are subshifts in which all points contain the same patterns.
It has been proved by Jeandel and Vanier that the Turing degree spectra of
non-periodic minimal subshifts always contain the cone of Turing degrees above
any of its degree. It was however not known whether each minimal subshift's
spectrum was formed of exactly one cone or not. We construct inductively a
minimal subshift whose spectrum consists of an uncountable number of cones with
disjoint base.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6491</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6491</id><created>2014-08-27</created><updated>2015-03-16</updated><authors><author><keyname>Datta</keyname><forenames>Amit</forenames></author><author><keyname>Tschantz</keyname><forenames>Michael Carl</forenames></author><author><keyname>Datta</keyname><forenames>Anupam</forenames></author></authors><title>Automated Experiments on Ad Privacy Settings: A Tale of Opacity, Choice,
  and Discrimination</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To partly address people's concerns over web tracking, Google has created the
Ad Settings webpage to provide information about and some choice over the
profiles Google creates on users. We present AdFisher, an automated tool that
explores how user behaviors, Google's ads, and Ad Settings interact. AdFisher
can run browser-based experiments and analyze data using machine learning and
significance tests. Our tool uses a rigorous experimental design and
statistical analysis to ensure the statistical soundness of our results. We use
AdFisher to find that the Ad Settings was opaque about some features of a
user's profile, that it does provide some choice on ads, and that these choices
can lead to seemingly discriminatory ads. In particular, we found that visiting
webpages associated with substance abuse changed the ads shown but not the
settings page. We also found that setting the gender to female resulted in
getting fewer instances of an ad related to high paying jobs than setting it to
male. We cannot determine who caused these findings due to our limited
visibility into the ad ecosystem, which includes Google, advertisers, websites,
and users. Nevertheless, these results can form the starting point for deeper
investigations by either the companies themselves or by regulatory bodies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6493</identifier>
 <datestamp>2014-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6493</id><created>2014-08-27</created><authors><author><keyname>Gyongyosi</keyname><forenames>Laszlo</forenames></author></authors><title>Adaptive Quadrature Detection for Multicarrier Continuous-Variable
  Quantum Key Distribution</title><categories>quant-ph cs.IT math.IT</categories><comments>38 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose the adaptive quadrature detection for multicarrier
continuous-variable quantum key distribution (CVQKD). A multicarrier CVQKD
scheme uses Gaussian subcarrier continuous variables for the information
conveying and Gaussian sub-channels for the transmission. The proposed
multicarrier detection scheme dynamically adapts to the sub-channel conditions
using a corresponding statistics which is provided by our sophisticated
sub-channel estimation procedure. The sub-channel estimation phase determines
the transmittance coefficients of the sub-channels, which information are used
further in the adaptive quadrature decoding process. We define the technique
called subcarrier spreading to estimate the transmittance conditions of the
sub-channels with a theoretical error-minimum in the presence of a Gaussian
noise. We introduce the terms of single and collective adaptive quadrature
detection. We also extend the results for a multiuser multicarrier CVQKD
scenario. We prove the achievable error probabilities, the signal-to-noise
ratios, and quantify the attributes of the framework. The adaptive detection
scheme allows to utilize the extra resources of multicarrier CVQKD and to
maximize the amount of transmittable valuable information in diverse
measurement and transmission conditions. The framework is particularly
convenient for experimental CVQKD scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6497</identifier>
 <datestamp>2015-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6497</id><created>2014-08-27</created><updated>2015-03-12</updated><authors><author><keyname>Gholami</keyname><forenames>Amir</forenames></author><author><keyname>Malhotra</keyname><forenames>Dhairya</forenames></author><author><keyname>Sundar</keyname><forenames>Hari</forenames></author><author><keyname>Biros</keyname><forenames>George</forenames></author></authors><title>FFT, FMM, or Multigrid? A comparative study of state-of-the-art Poisson
  solvers in the unit cube</title><categories>math.NA cs.NA</categories><comments>25 pages; submitted journal paper</comments><msc-class>17B63, 65T50, 65T40, 78M16, 65N55, 65Y05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  From molecular dynamics and quantum chemistry, to plasma physics and
computational astrophysics, Poisson solvers in the unit cube are used in many
applications in computational science and engineering. In this work, we
benchmark and discuss the performance of the scalable methods for the Poisson
problem which are used widely in practice: the Fast Fourier Transform (FFT),
the Fast Multipole Method (FMM), the geometric multigrid (GMG) and algebraic
multigrid (AMG). All but the AMG code are developed in our group. The GMG and
FMM are novel parallel schemes using high-order approximation schemes for
Poisson problems with continuous forcing functions (the source or right-hand
side). Our FFT code is based on the FFTW for single node parallelism. The AMG
code is from the Trilinos library from the Sandia National Laboratory.
  We examine and report results for weak scaling, strong scaling, and time to
solution for uniform and highly refined grids. We present results on the
Stampede system at the Texas Advanced Computing Center and on the Titan system
at the Oak Ridge National Laboratory. In our largest test case, we solved a
problem with 600 billion unknowns on 229,379 cores of Titan. Overall, all
methods scale quite well to these problem sizes. We have tested all of the
methods with different source functions (the right hand side in the Poisson
problem). Our results indicate that FFT is the method of choice for smooth
source functions that require uniform resolution. However, FFT loses its
performance advantage when the source function has highly localized features
like internal sharp layers. FMM and GMG considerably outperform FFT for those
cases. The distinction between FMM and GMG is less clear.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6504</identifier>
 <datestamp>2014-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6504</id><created>2014-08-27</created><authors><author><keyname>Tutuncuoglu</keyname><forenames>Kaya</forenames></author><author><keyname>Ozel</keyname><forenames>Omur</forenames></author><author><keyname>Yener</keyname><forenames>Aylin</forenames></author><author><keyname>Ulukus</keyname><forenames>Sennur</forenames></author></authors><title>The Binary Energy Harvesting Channel with a Unit-Sized Battery</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory, August 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a binary energy harvesting communication channel with a
finite-sized battery at the transmitter. In this model, the channel input is
constrained by the available energy at each channel use, which is driven by an
external energy harvesting process, the size of the battery, and the previous
channel inputs. We consider an abstraction where energy is harvested in binary
units and stored in a battery with the capacity of a single unit, and the
channel inputs are binary. Viewing the available energy in the battery as a
state, this is a state-dependent channel with input-dependent states, memory in
the states, and causal state information available at the transmitter only. We
find an equivalent representation for this channel based on the timings of the
symbols, and determine the capacity of the resulting equivalent timing channel
via an auxiliary random variable. We give achievable rates based on certain
selections of this auxiliary random variable which resemble lattice coding for
the timing channel. We develop upper bounds for the capacity by using a
genie-aided method, and also by quantifying the leakage of the state
information to the receiver. We show that the proposed achievable rates are
asymptotically capacity achieving for small energy harvesting rates. We extend
the results to the case of ternary channel inputs. Our achievable rates give
the capacity of the binary channel within 0.03 bits/channel use, the ternary
channel within 0.05 bits/channel use, and outperform basic Shannon strategies
that only consider instantaneous battery states, for all parameter values.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6509</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6509</id><created>2014-08-27</created><updated>2015-08-10</updated><authors><author><keyname>Frenkel</keyname><forenames>Elizaveta</forenames></author><author><keyname>Nikolaev</keyname><forenames>Andrey</forenames></author><author><keyname>Ushakov</keyname><forenames>Alexander</forenames></author></authors><title>Knapsack problems in products of groups</title><categories>math.GR cs.CC</categories><comments>15 pages, 5 figures. Updated to include more general results, mostly
  in Section 4</comments><msc-class>03D15, 20F65, 20F10, 68Q45</msc-class><doi>10.1016/j.jsc.2015.05.006</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The classic knapsack and related problems have natural generalizations to
arbitrary (non-commutative) groups, collectively called knapsack-type problems
in groups. We study the effect of free and direct products on their time
complexity. We show that free products in certain sense preserve time
complexity of knapsack-type problems, while direct products may amplify it. Our
methods allow to obtain complexity results for rational subset membership
problem in amalgamated free products over finite subgroups.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6515</identifier>
 <datestamp>2015-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6515</id><created>2014-08-27</created><updated>2015-03-04</updated><authors><author><keyname>Zhang</keyname><forenames>Yuyu</forenames></author><author><keyname>Pang</keyname><forenames>Liang</forenames></author><author><keyname>Shi</keyname><forenames>Lei</forenames></author><author><keyname>Wang</keyname><forenames>Bin</forenames></author></authors><title>Large Scale Purchase Prediction with Historical User Actions on B2C
  Online Retail Platform</title><categories>cs.LG</categories><comments>Accepted by 2nd Large Scale Recommender Systems Workshop, RecSys 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes the solution of Bazinga Team for Tmall Recommendation
Prize 2014. With real-world user action data provided by Tmall, one of the
largest B2C online retail platforms in China, this competition requires to
predict future user purchases on Tmall website. Predictions are judged on
F1Score, which considers both precision and recall for fair evaluation. The
data set provided by Tmall contains more than half billion action records from
over ten million distinct users. Such massive data volume poses a big
challenge, and drives competitors to write every single program in MapReduce
fashion and run it on distributed cluster. We model the purchase prediction
problem as standard machine learning problem, and mainly employ regression and
classification methods as single models. Individual models are then aggregated
in a two-stage approach, using linear regression for blending, and finally a
linear ensemble of blended models. The competition is approaching the end but
still in running during writing this paper. In the end, our team achieves
F1Score 6.11 and ranks 7th (out of 7,276 teams in total).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6520</identifier>
 <datestamp>2014-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6520</id><created>2014-08-27</created><authors><author><keyname>Sohrabi</keyname><forenames>Shirin</forenames></author><author><keyname>Udrea</keyname><forenames>Octavian</forenames></author><author><keyname>Riabov</keyname><forenames>Anton V.</forenames></author></authors><title>Knowledge Engineering for Planning-Based Hypothesis Generation</title><categories>cs.AI</categories><comments>This paper appears in the Proceedings of the Automated Planning and
  Scheduling (ICAPS) Workshop on Knowledge Engineering for Planning and
  Scheduling (KEPS)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we address the knowledge engineering problems for hypothesis
generation motivated by applications that require timely exploration of
hypotheses under unreliable observations. We looked at two applications:
malware detection and intensive care delivery. In intensive care, the goal is
to generate plausible hypotheses about the condition of the patient from
clinical observations and further refine these hypotheses to create a recovery
plan for the patient. Similarly, preventing malware spread within a corporate
network involves generating hypotheses from network traffic data and selecting
preventive actions. To this end, building on the already established
characterization and use of AI planning for similar problems, we propose use of
planning for the hypothesis generation problem. However, to deal with
uncertainty, incomplete model description and unreliable observations, we need
to use a planner capable of generating multiple high-quality plans. To capture
the model description we propose a language called LTS++ and a web-based tool
that enables the specification of the LTS++ model and a set of observations. We
also proposed a 9-step process that helps provide guidance to the domain expert
in specifying the LTS++ model. The hypotheses are then generated by running a
planner on the translated LTS++ model and the provided trace. The hypotheses
can be visualized and shown to the analyst or can be further investigated
automatically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6552</identifier>
 <datestamp>2015-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6552</id><created>2014-08-27</created><updated>2015-07-08</updated><authors><author><keyname>Zhao</keyname><forenames>Shiyu</forenames></author><author><keyname>Zelazo</keyname><forenames>Daniel</forenames></author></authors><title>Bearing Rigidity and Almost Global Bearing-Only Formation Stabilization</title><categories>cs.SY</categories><comments>Accepted as a full paper by IEEE Transactions on Automatic Control.
  This is the final version before the official publication by IEEE</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A fundamental problem that the bearing rigidity theory studies is to
determine when a framework can be uniquely determined up to a translation and a
scaling factor by its inter-neighbor bearings. While many previous works
focused on the bearing rigidity of two-dimensional frameworks, a first
contribution of this paper is to extend these results to arbitrary dimensions.
It is shown that a framework in an arbitrary dimension can be uniquely
determined up to a translation and a scaling factor by the bearings if and only
if the framework is infinitesimally bearing rigid. In this paper, the proposed
bearing rigidity theory is further applied to the bearing-only formation
stabilization problem where the target formation is defined by inter-neighbor
bearings and the feedback control uses only bearing measurements. Nonlinear
distributed bearing-only formation control laws are proposed for the cases with
and without a global orientation. It is proved that the control laws can almost
globally stabilize infinitesimally bearing rigid formations. Numerical
simulations are provided to support the analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6566</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6566</id><created>2014-08-27</created><updated>2015-02-05</updated><authors><author><keyname>Liu</keyname><forenames>Sijia</forenames></author><author><keyname>Kar</keyname><forenames>Swarnendu</forenames></author><author><keyname>Fardad</keyname><forenames>Makan</forenames></author><author><keyname>Varshney</keyname><forenames>Pramod K.</forenames></author></authors><title>Sparsity-Aware Sensor Collaboration for Linear Coherent Estimation</title><categories>stat.ME cs.IT math.IT</categories><comments>IEEE Transactions on Signal Processing</comments><doi>10.1109/TSP.2015.2413381</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the context of distributed estimation, we consider the problem of sensor
collaboration, which refers to the act of sharing measurements with neighboring
sensors prior to transmission to a fusion center. While incorporating the cost
of sensor collaboration, we aim to find optimal sparse collaboration schemes
subject to a certain information or energy constraint. Two types of sensor
collaboration problems are studied: minimum energy with an information
constraint; and maximum information with an energy constraint. To solve the
resulting sensor collaboration problems, we present tractable optimization
formulations and propose efficient methods which render near-optimal solutions
in numerical experiments. We also explore the situation in which there is a
cost associated with the involvement of each sensor in the estimation scheme.
In such situations, the participating sensors must be chosen judiciously. We
introduce a unified framework to jointly design the optimal sensor selection
and collaboration schemes. For a given estimation performance, we show
empirically that there exists a trade-off between sensor selection and sensor
collaboration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6575</identifier>
 <datestamp>2014-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6575</id><created>2014-08-27</created><authors><author><keyname>Blum</keyname><forenames>Avrim</forenames></author><author><keyname>Mansour</keyname><forenames>Yishay</forenames></author><author><keyname>Morgenstern</keyname><forenames>Jamie</forenames></author></authors><title>Learning What's going on: reconstructing preferences and priorities from
  opaque transactions</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a setting where $n$ buyers, with combinatorial preferences over
$m$ items, and a seller, running a priority-based allocation mechanism,
repeatedly interact. Our goal, from observing limited information about the
results of these interactions, is to reconstruct both the preferences of the
buyers and the mechanism of the seller. More specifically, we consider an
online setting where at each stage, a subset of the buyers arrive and are
allocated items, according to some unknown priority that the seller has among
the buyers. Our learning algorithm observes only which buyers arrive and the
allocation produced (or some function of the allocation, such as just which
buyers received positive utility and which did not), and its goal is to predict
the outcome for future subsets of buyers. For this task, the learning algorithm
needs to reconstruct both the priority among the buyers and the preferences of
each buyer. We derive mistake bound algorithms for additive, unit-demand and
single minded buyers. We also consider the case where buyers' utilities for a
fixed bundle can change between stages due to different (observed) prices. Our
algorithms are efficient both in computation time and in the maximum number of
mistakes (both polynomial in the number of buyers and items).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6587</identifier>
 <datestamp>2015-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6587</id><created>2014-08-27</created><updated>2015-09-24</updated><authors><author><keyname>Chaudhari</keyname><forenames>Shailesh</forenames></author><author><keyname>Hu</keyname><forenames>Jingy</forenames></author><author><keyname>Daneshrad</keyname><forenames>Babak</forenames></author><author><keyname>Chen</keyname><forenames>Jesse</forenames></author></authors><title>Performance Comparison Between MIMO and SISO based on Indoor Field
  Measurements</title><categories>cs.NI cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we quantify performance gain achieved if SISO system is
replaced with 4x4 MIMO in WLAN setting compatible with IEEE 802.11n standard.
We compare throughput and power savings in MIMO by taking field measurements at
various indoor locations. Measurements are validated with simulations that
include different IEEE TGn channel models. For comparison between MIMO and
SISO, we select the Modulation and Coding Scheme (MCS) which yields the highest
throughput subject to QoS constraints (PER &lt;10%). We show that throughput gain
of 2.5x-3x and power saving of 5-15 dB are achievable in 4x4 MIMO system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6589</identifier>
 <datestamp>2014-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6589</id><created>2014-08-27</created><authors><author><keyname>Wu</keyname><forenames>Wentao</forenames></author><author><keyname>Wu</keyname><forenames>Xi</forenames></author><author><keyname>Hac&#x131;g&#xfc;m&#xfc;&#x15f;</keyname><forenames>Hakan</forenames></author><author><keyname>Naughton</keyname><forenames>Jeffrey F.</forenames></author></authors><title>Uncertainty Aware Query Execution Time Prediction</title><categories>cs.DB</categories><comments>This is the extended version of a paper with the same title and
  authors that appears in the Proceedings of the VLDB Endowment (PVLDB), Vol.
  7(14), 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Predicting query execution time is a fundamental issue underlying many
database management tasks. Existing predictors rely on information such as
cardinality estimates and system performance constants that are difficult to
know exactly. As a result, accurate prediction still remains elusive for many
queries. However, existing predictors provide a single, point estimate of the
true execution time, but fail to characterize the uncertainty in the
prediction. In this paper, we take a first step towards providing uncertainty
information along with query execution time predictions. We use the query
optimizer's cost model to represent the query execution time as a function of
the selectivities of operators in the query plan as well as the constants that
describe the cost of CPU and I/O operations in the system. By treating these
quantities as random variables rather than constants, we show that with low
overhead we can infer the distribution of likely prediction errors. We further
show that the estimated prediction errors by our proposed techniques are
strongly correlated with the actual prediction errors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6591</identifier>
 <datestamp>2014-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6591</id><created>2014-08-26</created><authors><author><keyname>Pietroni</keyname><forenames>Nico</forenames></author><author><keyname>Tonelli</keyname><forenames>Davide</forenames></author><author><keyname>Puppo</keyname><forenames>Enrico</forenames></author><author><keyname>Froli</keyname><forenames>Maurizio</forenames></author><author><keyname>Scopigno</keyname><forenames>Roberto</forenames></author><author><keyname>Cignoni</keyname><forenames>Paolo</forenames></author></authors><title>Voronoi Grid-Shell Structures</title><categories>cs.GR cs.CG</categories><comments>10 pages</comments><msc-class>68U05</msc-class><acm-class>I.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a framework for the generation of grid-shell structures that is
based on Voronoi diagrams and allows us to design tessellations that achieve
excellent static performances. We start from an analysis of stress on the input
surface and we use the resulting tensor field to induce an anisotropic
non-Euclidean metric over it. Then we compute a Centroidal Voronoi Tessellation
under the same metric. The resulting mesh is hex-dominant and made of cells
with a variable density, which depends on the amount of stress, and anisotropic
shape, which depends on the direction of maximum stress. This mesh is further
optimized taking into account symmetry and regularity of cells to improve
aesthetics. We demonstrate that our grid-shells achieve better static
performances with respect to quad-based grid shells, while offering an
innovative and aesthetically pleasing look.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6595</identifier>
 <datestamp>2014-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6595</id><created>2014-08-27</created><authors><author><keyname>Batra</keyname><forenames>Nipun</forenames></author><author><keyname>Parson</keyname><forenames>Oliver</forenames></author><author><keyname>Berges</keyname><forenames>Mario</forenames></author><author><keyname>Singh</keyname><forenames>Amarjeet</forenames></author><author><keyname>Rogers</keyname><forenames>Alex</forenames></author></authors><title>A comparison of non-intrusive load monitoring methods for commercial and
  residential buildings</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Non intrusive load monitoring (NILM), or energy disaggregation, is the
process of separating the total electricity consumption of a building as
measured at single point into the building's constituent loads. Previous
research in the field has mostly focused on residential buildings, and although
the potential benefits of applying this technology to commercial buildings have
been recognised since the field's conception, NILM in the commercial domain has
been largely unexplored by the academic community. As a result of the
heterogeneity of this section of the building stock (i.e., encompassing
buildings as diverse as airports, malls and coffee shops), and hence the loads
within them, many of the solutions developed for residential energy
disaggregation do not apply directly. In this paper we highlight some insights
for NILM in the commercial domain using data collected from a large smart meter
deployment within an educational campus in Delhi, India, of which a subset of
the data has been released for public use. We present an empirical
characterisation of loads in commercial buildings, highlighting the differences
in energy consumption and load characteristics between residential and
commercial buildings. We assess the validity of the assumptions generally made
by NILM solutions for residential buildings when applied to measurements from
commercial facilities. Based on our observations, we discuss the required
traits for a NILM system for commercial buildings, and run benchmark
residential NILM algorithms on our data set to confirm our observations. To
advance the research in commercial buildings energy disaggregation, we release
a subset of our data set, called COMBED (commercial building energy data set).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6596</identifier>
 <datestamp>2014-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6596</id><created>2014-08-27</created><updated>2014-11-12</updated><authors><author><keyname>Bhat</keyname><forenames>Uttam</forenames></author><author><keyname>Krapivsky</keyname><forenames>P. L.</forenames></author><author><keyname>Redner</keyname><forenames>S.</forenames></author></authors><title>Emergence of Clustering in an Acquaintance Model without Homophily</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI physics.data-an</categories><comments>16 pages, 14 figures, IOP format. Version 2: final form for
  publication in JSTAT</comments><journal-ref>J. Stat. Mech. P11035 (2014)</journal-ref><doi>10.1088/1742-5468/2014/11/P11035</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce an agent-based acquaintance model in which social links are
created by processes in which there is no explicit homophily. In spite of the
homogeneous nature of the social interactions, highly-clustered social networks
can arise. The crucial feature of our model is that of variable transitive
interactions. Namely, when an agent introduces two unconnected friends, the
rate at which a connection actually occurs between them depends on the number
of their mutual acquaintances. As this transitive interaction rate is varied,
the social network undergoes a dramatic clustering transition. Close to the
transition, the network consists of a collection of well-defined communities.
As a function of time, the network can also undergo an \emph{incomplete}
gelation transition, in which the gel, or giant cluster, does not constitute
the entire network, even at infinite time. Some of the clustering properties of
our model also arise, but in a more gradual manner, in Facebook networks.
Finally, we discuss a more realistic variant of our original model in which
there is a soft cutoff in the rate of transitive interactions. With this
variant, one can construct network realizations that quantitatively match
Facebook networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6605</identifier>
 <datestamp>2014-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6605</id><created>2014-08-27</created><authors><author><keyname>Li</keyname><forenames>Xu</forenames></author><author><keyname>Guo</keyname><forenames>Dongning</forenames></author><author><keyname>Grosspietsch</keyname><forenames>John</forenames></author><author><keyname>Yin</keyname><forenames>Huarui</forenames></author><author><keyname>Wei</keyname><forenames>Guo</forenames></author></authors><title>Maximizing Mobile Coverage via Optimal Deployment of Base Station and
  Relays</title><categories>cs.IT cs.NI math.IT</categories><comments>26 pages, 10 figures, submitted to transactions on wireless
  communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deploying relays and/or mobile base stations is a major means of extending
the coverage of a wireless network. This paper presents models, analytical
results, and algorithms to answer two related questions: The first is where to
deploy relays in order to extend the reach from a base station to the maximum;
the second is where to deploy a mobile base station and how many relays are
needed to reach any point in a given area. Simple time-division and
frequency-division scheduling schemes as well as an end-to-end data rate
requirement are assumed. An important use case of the results is in the Public
Safety Broadband Network, in which deploying relays and mobile base stations is
often crucial to provide coverage to an incident scene.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6610</identifier>
 <datestamp>2014-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6610</id><created>2014-08-27</created><authors><author><keyname>Cao</keyname><forenames>Zhengjun</forenames></author><author><keyname>Liu</keyname><forenames>Lihua</forenames></author></authors><title>The Barth-Boneh-Waters Private Broadcast Encryption Scheme Revisited</title><categories>cs.CR</categories><comments>7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The primitive of private broadcast encryption introduced by Barth, Boneh and
Waters, is used to encrypt a message to several recipients while hiding the
identities of the recipients. In their construction, a recipient has to first
decrypt the received ciphertext to extract the verification key for one-time
signature. He then uses the verification key to check whether the ciphertext is
malformed. The authors did not consider that information delivered over a
channel, especially over a broadcast channel, should be authenticated as to its
origin. We remark that the conventional public key signature suffices to
authenticate data origin and filter out all malformed ciphertexts. We also
discuss the disadvantages of the primitive of one-time signature used in their
construction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6615</identifier>
 <datestamp>2015-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6615</id><created>2014-08-27</created><updated>2015-02-11</updated><authors><author><keyname>Minaee</keyname><forenames>Shervin</forenames></author><author><keyname>Abdolrashidi</keyname><forenames>AmirAli</forenames></author></authors><title>Multispectral Palmprint Recognition Using Textural Features</title><categories>cs.CV</categories><comments>5 pages, Published in IEEE Signal Processing in Medicine and Biology
  Symposium 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to utilize identification to the best extent, we need robust and
fast algorithms and systems to process the data. Having palmprint as a reliable
and unique characteristic of every person, we extract and use its features
based on its geometry, lines and angles. There are countless ways to define
measures for the recognition task. To analyze a new point of view, we extracted
textural features and used them for palmprint recognition. Co-occurrence matrix
can be used for textural feature extraction. As classifiers, we have used the
minimum distance classifier (MDC) and the weighted majority voting system
(WMV). The proposed method is tested on a well-known multispectral palmprint
dataset of 6000 samples and an accuracy rate of 99.96-100% is obtained for most
scenarios which outperforms all previous works in multispectral palmprint
recognition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6617</identifier>
 <datestamp>2014-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6617</id><created>2014-08-27</created><authors><author><keyname>Zhang</keyname><forenames>Chao</forenames></author><author><keyname>Tao</keyname><forenames>Dacheng</forenames></author><author><keyname>Hu</keyname><forenames>Tao</forenames></author><author><keyname>Li</keyname><forenames>Xiang</forenames></author></authors><title>Task-group Relatedness and Generalization Bounds for Regularized
  Multi-task Learning</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the generalization performance of regularized
multi-task learning (RMTL) in a vector-valued framework, where MTL is
considered as a learning process for vector-valued functions. We are mainly
concerned with two theoretical questions: 1) under what conditions does RMTL
perform better with a smaller task sample size than STL? 2) under what
conditions is RMTL generalizable and can guarantee the consistency of each task
during simultaneous learning?
  In particular, we investigate two types of task-group relatedness: the
observed discrepancy-dependence measure (ODDM) and the empirical
discrepancy-dependence measure (EDDM), both of which detect the dependence
between two groups of multiple related tasks (MRTs). We then introduce the
Cartesian product-based uniform entropy number (CPUEN) to measure the
complexities of vector-valued function classes. By applying the specific
deviation and the symmetrization inequalities to the vector-valued framework,
we obtain the generalization bound for RMTL, which is the upper bound of the
joint probability of the event that there is at least one task with a large
empirical discrepancy between the expected and empirical risks. Finally, we
present a sufficient condition to guarantee the consistency of each task in the
simultaneous learning process, and we discuss how task relatedness affects the
generalization performance of RMTL. Our theoretical findings answer the
aforementioned two questions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6618</identifier>
 <datestamp>2014-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6618</id><created>2014-08-27</created><authors><author><keyname>Balduzzi</keyname><forenames>David</forenames></author></authors><title>Falsifiable implies Learnable</title><categories>cs.LG math.ST stat.ML stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper demonstrates that falsifiability is fundamental to learning. We
prove the following theorem for statistical learning and sequential prediction:
If a theory is falsifiable then it is learnable -- i.e. admits a strategy that
predicts optimally. An analogous result is shown for universal induction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6621</identifier>
 <datestamp>2014-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6621</id><created>2014-08-27</created><authors><author><keyname>Lasecki</keyname><forenames>Walter S.</forenames></author><author><keyname>Homan</keyname><forenames>Christopher M.</forenames></author><author><keyname>Bigham</keyname><forenames>Jeffrey P.</forenames></author></authors><title>Tuning the Diversity of Open-Ended Responses from the Crowd</title><categories>cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Crowdsourcing can solve problems that current fully automated systems cannot.
Its effectiveness depends on the reliability, accuracy, and speed of the crowd
workers that drive it. These objectives are frequently at odds with one
another. For instance, how much time should workers be given to discover and
propose new solutions versus deliberate over those currently proposed? How do
we determine if discovering a new answer is appropriate at all? And how do we
manage workers who lack the expertise or attention needed to provide useful
input to a given task? We present a mechanism that uses distinct payoffs for
three possible worker actions---propose,vote, or abstain---to provide workers
with the necessary incentives to guarantee an effective (or even optimal)
balance between searching for new answers, assessing those currently available,
and, when they have insufficient expertise or insight for the task at hand,
abstaining. We provide a novel game theoretic analysis for this mechanism and
test it experimentally on an image---labeling problem and show that it allows a
system to reliably control the balance betweendiscovering new answers and
converging to existing ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6661</identifier>
 <datestamp>2014-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6661</id><created>2014-08-28</created><authors><author><keyname>Polatidis</keyname><forenames>Nikolaos</forenames></author></authors><title>SFA Referee Allocation Scheme</title><categories>cs.OH</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  For many sports, the allocation of officials to matches is performed manually
and is a very time consuming procedure. For the Scottish Football Association
(SFA), the allocation of referees and other officials to matches is governed by
a number of rules specifying the expertise required from the different types of
official at each level, e.g. Scottish Premiership League referee must be a
grade 1 with high experience. The allocation requires an SFA secretary to
expend several hours to find suitable officials, contact them and assign them.
Most of the time, the secretary is a volunteer who performs the allocation as a
hobby and it would be useful to reduce his costs and time.
  The project aims to reduce the burden on SFA, and potentially other
secretaries, by developing a program to assign SFA officials. A suitable
algorithm must be devised to search through the set of data about matches and
officials and find a potential allocation. The program then updates the
database with the new data, and provides a web interface for both secretaries
and officials.
  A prototype system using the new greedy algorithm has been implemented and
evaluated with SFA secretaries. A final usable referee allocation system has
been designed that uses the greedy algorithm, and is extended after evaluation
of the prototype. The final allocation system based provides both a command
line and a web interface and has also been evaluated by SFA secretaries. In
their letters of recommendation in Appendix F the SFA secretaries indicate that
the final allocation system it will be used again in the future.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6676</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6676</id><created>2014-08-28</created><authors><author><keyname>Chaplick</keyname><forenames>Steven</forenames></author><author><keyname>Fiala</keyname><forenames>Ji&#x159;&#xed;</forenames></author><author><keyname>Hof</keyname><forenames>Pim van 't</forenames></author><author><keyname>Paulusma</keyname><forenames>Dani&#xeb;l</forenames></author><author><keyname>Tesa&#x159;</keyname><forenames>Marek</forenames></author></authors><title>Locally Constrained Homomorphisms on Graphs of Bounded Treewidth and
  Bounded Degree</title><categories>cs.CC cs.DM math.CO</categories><comments>An extended abstract of this paper appeared in the proceedings of FCT
  2013, LNCS 8070: 121-132. http://dx.doi.org/10.1007/978-3-642-40164-0_14</comments><journal-ref>Journal of Theoretical Computer Science: Fundamentals of
  Computation Theory (FCT 2013). 590: 86-95. 2015</journal-ref><doi>10.1016/j.tcs.2015.01.028</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A homomorphism from a graph G to a graph H is locally bijective, surjective,
or injective if its restriction to the neighborhood of every vertex of G is
bijective, surjective, or injective, respectively. We prove that the problems
of testing whether a given graph G allows a homomorphism to a given graph H
that is locally bijective, surjective, or injective, respectively, are
NP-complete, even when G has pathwidth at most 5, 4, or 2, respectively, or
when both G and H have maximum degree 3. We complement these hardness results
by showing that the three problems are polynomial-time solvable if G has
bounded treewidth and in addition G or H has bounded maximum degree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6686</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6686</id><created>2014-08-28</created><updated>2014-11-18</updated><authors><author><keyname>Song</keyname><forenames>Junxiao</forenames></author><author><keyname>Babu</keyname><forenames>Prabhu</forenames></author><author><keyname>Palomar</keyname><forenames>Daniel P.</forenames></author></authors><title>Sparse Generalized Eigenvalue Problem via Smooth Optimization</title><categories>stat.ML cs.LG</categories><doi>10.1109/TSP.2015.2394443</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider an $\ell_{0}$-norm penalized formulation of the
generalized eigenvalue problem (GEP), aimed at extracting the leading sparse
generalized eigenvector of a matrix pair. The formulation involves maximization
of a discontinuous nonconcave objective function over a nonconvex constraint
set, and is therefore computationally intractable. To tackle the problem, we
first approximate the $\ell_{0}$-norm by a continuous surrogate function. Then
an algorithm is developed via iteratively majorizing the surrogate function by
a quadratic separable function, which at each iteration reduces to a regular
generalized eigenvalue problem. A preconditioned steepest ascent algorithm for
finding the leading generalized eigenvector is provided. A systematic way based
on smoothing is proposed to deal with the &quot;singularity issue&quot; that arises when
a quadratic function is used to majorize the nondifferentiable surrogate
function. For sparse GEPs with special structure, algorithms that admit a
closed-form solution at every iteration are derived. Numerical experiments show
that the proposed algorithms match or outperform existing algorithms in terms
of computational complexity and support recovery.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6689</identifier>
 <datestamp>2014-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6689</id><created>2014-08-28</created><authors><author><keyname>Shan</keyname><forenames>Yuquan</forenames></author><author><keyname>Raghuram</keyname><forenames>Jayaram</forenames></author><author><keyname>Kesidis</keyname><forenames>George</forenames></author><author><keyname>Griffin</keyname><forenames>Christopher</forenames></author><author><keyname>Levitt</keyname><forenames>Karl</forenames></author><author><keyname>Miller</keyname><forenames>David J.</forenames></author><author><keyname>Rowe</keyname><forenames>Jeffry</forenames></author><author><keyname>Scaglione</keyname><forenames>Anna</forenames></author></authors><title>Generation bidding game with flexible demand</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a simple model of price-responsive demand, we consider a deregulated
electricity marketplace wherein the grid (ISO, retailer-distributor) accepts
bids per-unit supply from generators (simplified herein neither to consider
start-up/ramp-up expenses nor day-ahead or shorter-term load following) which
are then averaged (by supply allocations via an economic dispatch) to a common
&quot;clearing&quot; price borne by customers (irrespective of variations in
transmission/distribution or generation prices), i.e., the ISO does not
compensate generators based on their marginal costs. Rather, the ISO provides
sufficient information for generators to sensibly adjust their bids.
Notwithstanding our idealizations, the dispatch dynamics are complex. For a
simple benchmark power system, we find a price-symmetric Nash equilibrium
through numerical experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6691</identifier>
 <datestamp>2014-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6691</id><created>2014-08-28</created><authors><author><keyname>Matteis</keyname><forenames>Luca</forenames></author></authors><title>VoID-graph: Visualize Linked Datasets on the Web</title><categories>cs.DB cs.HC</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The Linked Open Data (LOD) cloud diagram is a picture that helps us grasp the
contents and the links of globally available data sets. Such diagram has been a
powerful dissemination method for the Linked Data movement, allowing people to
glance at the size and structure of this distributed, interconnected database.
However, generating such image for third-party datasets can be a quite complex
task as it requires the installation and understanding of a variety of tools
which are not easy to setup. In this paper we present VoID-graph
(http://lmatteis.github.io/void-graph/), a standalone web-tool that, given a
VoID description, can visualize a diagram similar to the LOD cloud. It is novel
because the diagram is autonomously shaped from VoID descriptions directly
within a Web-browser, which doesn't require any server cooperation. This makes
it not only easy to use, as no installation or configuration is required, but
also makes it more sustainable, as it is built using Open Web standards such as
JavaScript and SVG.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6695</identifier>
 <datestamp>2014-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6695</id><created>2014-08-28</created><authors><author><keyname>Salo</keyname><forenames>Ville</forenames></author><author><keyname>T&#xf6;rm&#xe4;</keyname><forenames>Ilkka</forenames></author></authors><title>Complexity of Conjugacy, Factoring and Embedding for Countable Sofic
  Shifts of Rank 2</title><categories>math.DS cs.CC cs.FL</categories><comments>14 pages, 1 figure. to appear in the postceedings of AUTOMATA 2014,
  published by Springer</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we study countable sofic shifts of Cantor-Bendixson rank at
most 2. We prove that their conjugacy problem is complete for GI, the
complexity class of graph isomorphism, and that the existence problems of block
maps, factor maps and embeddings are NP-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6701</identifier>
 <datestamp>2014-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6701</id><created>2014-08-28</created><authors><author><keyname>Salo</keyname><forenames>Ville</forenames></author><author><keyname>T&#xf6;rm&#xe4;</keyname><forenames>Ilkka</forenames></author></authors><title>Plane-Walking Automata</title><categories>cs.FL</categories><comments>17 pages, 3 figures. To appear in a shortened from in the
  postceedings of AUTOMATA 2014, published by Springer</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we study classes of multidimensional subshifts defined by
multihead finite automata, in particular the hierarchy of classes of subshifts
defined as the number of heads grows. The hierarchy collapses on the third
level, where all co-recursively enumerable subshifts are obtained in every
dimension. We also compare these classes to SFTs and sofic shifts. We are
unable to separate the second and third level of the hierarchy in one and two
dimensions, and suggest a related open problem for two-counter machines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6706</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6706</id><created>2014-08-28</created><updated>2015-03-18</updated><authors><author><keyname>Gabbay</keyname><forenames>D.</forenames></author><author><keyname>Rodrigues</keyname><forenames>O.</forenames></author></authors><title>Equilibrium States in Numerical Argumentation Networks</title><categories>cs.AI</categories><acm-class>I.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given an argumentation network with initial values to the arguments, we look
for algorithms which can yield extensions compatible with such initial values.
We find that the best way of tackling this problem is to offer an iteration
formula that takes the initial values and the attack relation and iterates a
sequence of intermediate values that eventually converges leading to an
extension. The properties surrounding the application of the iteration formula
and its connection with other numerical and non-numerical techniques proposed
by others are thoroughly investigated in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6716</identifier>
 <datestamp>2015-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6716</id><created>2014-08-28</created><updated>2015-01-16</updated><authors><author><keyname>Gallet</keyname><forenames>Matteo</forenames></author><author><keyname>Nawratil</keyname><forenames>Georg</forenames></author><author><keyname>Schicho</keyname><forenames>Josef</forenames></author></authors><title>M\&quot;obius Photogrammetry</title><categories>math.AG cs.RO</categories><comments>17 pages, 4 figures. updated references and fixed typographical
  errors</comments><doi>10.1007/s00022-014-0255-x</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by results on the mobility of mechanical devices called pentapods,
this paper deals with a mathematically freestanding problem, which we call
M\&quot;obius Photogrammetry. Unlike traditional photogrammetry, which tries to
recover a set of points in three-dimensional space from a finite set of central
projection, we consider the problem of reconstructing a vector of points in
$\mathbb{R}^3$ starting from its orthogonal parallel projections. Moreover, we
assume that we have partial information about these projections, namely that we
know them only up to M\&quot;obius transformations. The goal in this case is to
understand to what extent we can reconstruct the starting set of points, and to
prove that the result can be achieved if we allow some uncertainties in the
answer. Eventually, the techniques developed in the paper allow us to show that
for a pentapod with mobility at least two either some anchor points are
collinear, or platform and base are similar, or they are planar and affine
equivalent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6718</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6718</id><created>2014-08-28</created><authors><author><keyname>Zoller</keyname><forenames>J.</forenames></author><author><keyname>Montangero</keyname><forenames>S.</forenames></author></authors><title>Probing models of information spreading in social networks</title><categories>physics.soc-ph cs.SI</categories><comments>6 pages, 6 figures</comments><journal-ref>J. Phys. A: Math. Theor. 47 435102 (2014)</journal-ref><doi>10.1088/1751-8113/47/43/435102</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We apply signal processing analysis to the information spreading in
scale-free network. To reproduce typical behaviors obtained from the analysis
of information spreading in the world wide web we use a modified SIS model
where synergy effects and influential nodes are taken into account. This model
depends on a single free parameter that characterize the memory-time of the
spreading process. We show that by means of fractal analysis it is possible
-from aggregated easily accessible data- to gain information on the memory time
of the underlying mechanism driving the information spreading process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6721</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6721</id><created>2014-08-26</created><updated>2015-02-22</updated><authors><author><keyname>Arablouei</keyname><forenames>Reza</forenames></author><author><keyname>Do&#x11f;an&#xe7;ay</keyname><forenames>Kutluy&#x131;l</forenames></author></authors><title>Performance Analysis of Linear-Equality-Constrained Least-Squares
  Estimation</title><categories>cs.PF cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze the performance of a linear-equality-constrained least-squares
(CLS) algorithm and its relaxed version, called rCLS, that is obtained via the
method of weighting. The rCLS algorithm solves an unconstrained least-squares
problem that is augmented by incorporating a weighted form of the linear
constraints. As a result, unlike the CLS algorithm, the rCLS algorithm is
amenable to our approach to performance analysis presented here, which is akin
to the energy-conservation-based methodology. Therefore, we initially inspect
the convergence properties and evaluate the precision of estimation as well as
satisfaction of the constraints for the rCLS algorithm in both mean and
mean-square senses. Afterwards, we examine the performance of the CLS algorithm
by evaluating the limiting performance of the rCLS algorithm as the relaxation
parameter (weight) approaches infinity. Numerical examples verify the accuracy
of the theoretical findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6723</identifier>
 <datestamp>2014-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6723</id><created>2014-08-28</created><authors><author><keyname>Farina</keyname><forenames>Marcello</forenames></author><author><keyname>Giulioni</keyname><forenames>Luca</forenames></author><author><keyname>Magni</keyname><forenames>Lalo</forenames></author><author><keyname>Scattolini</keyname><forenames>Riccardo</forenames></author></authors><title>An MPC approach to output-feedback control of stochastic linear
  discrete-time systems</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose an output-feedback Model Predictive Control (MPC)
algorithm for linear discrete-time systems affected by a possibly unbounded
additive noise and subject to probabilistic constraints. In case the noise
distribution is unknown, the chance constraints on the input and state
variables are reformulated by means of the Chebyshev - Cantelli inequality. The
recursive feasibility of the proposed algorithm is guaranteed and the
convergence of the state to a suitable neighbor of the origin is proved under
mild assumptions. The implementation issues are thoroughly addressed showing
that, with a proper choice of the design parameters, its computational load can
be made similar to the one of a standard stabilizing MPC algorithm. Two
examples are discussed in details, with the aim of providing an insight on the
performance achievable by the proposed control scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6729</identifier>
 <datestamp>2014-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6729</id><created>2014-08-28</created><authors><author><keyname>Gerbessiotis</keyname><forenames>Alexandros V.</forenames></author><author><keyname>Siniolakis</keyname><forenames>Constantinos J.</forenames></author></authors><title>BSP Sorting: An experimental Study</title><categories>cs.DC</categories><comments>30 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Bulk-Synchronous Parallel model of computation has been used for the
architecture independent design and analysis of parallel algorithms whose
performance is expressed not only in terms of problem size n but also in terms
of parallel machine properties. In this paper the performance of
implementations of deterministic and randomized BSP sorting algorithms is
examined. The deterministic algorithm uses deterministic regular oversampling
and parallel sample sorting and is augmented to handle duplicate keys
transparently with optimal asymptotic efficiency. The randomized algorithm is
sample-sort based and uses oversampling and the ideas introduced with the
deterministic algorithm. The resulting randomized design, however, works
differently from traditional parallel sample-sort based algorithms and is also
augmented to transparently handle duplicate keys with optimal asymptotic
efficiency thus eliminating the need to tag all input keys and to double
communication/computation time. Both algorithms are shown to balance the
work-load evenly among the processors and the use and precise tuning of
oversampling that the BSP analysis allows combined with the transparent
duplicate-key handling insures regular and balanced communication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6736</identifier>
 <datestamp>2014-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6736</id><created>2014-08-28</created><authors><author><keyname>Shajaiah</keyname><forenames>Haya</forenames></author><author><keyname>Abdelhadi</keyname><forenames>Ahmed</forenames></author><author><keyname>Clancy</keyname><forenames>Charles</forenames></author></authors><title>Impact of Radar and Communication Coexistence on Radar's Detectable
  Target Parameters</title><categories>cs.NI cs.IT math.IT</categories><comments>Submitted to IEEE</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present our spectrum sharing algorithm between a
multi-input multi-output (MIMO) radar and Long Term Evolution (LTE) cellular
system with multiple base stations (BS)s. We analyze the performance of MIMO
radars in detecting the angle of arrival, propagation delay and Doppler angular
frequency by projecting orthogonal waveforms onto the null-space of
interference channel matrix. We compare and analyze the radar's detectable
target parameters in the case of the original radar waveform and the case of
null-projected radar waveform. Our proposed spectrum-sharing algorithm causes
minimum loss in radar performance by selecting the best interference channel
that does not cause interference to the i'th LTE base station due to the radar
signal. We show through our analytical and simulation results that the loss in
the radar performance in detecting the target parameters is minimal when our
proposed spectrum sharing algorithm is used to select the best channel onto
which radar signals are projected.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6741</identifier>
 <datestamp>2014-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6741</id><created>2014-08-28</created><authors><author><keyname>Pershin</keyname><forenames>Y. V.</forenames></author><author><keyname>Di Ventra</keyname><forenames>M.</forenames></author></authors><title>Memcomputing and Swarm Intelligence</title><categories>cs.NE cond-mat.mes-hall cs.ET</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explore the relation between memcomputing, namely computing with and in
memory, and swarm intelligence algorithms. In particular, we show that one can
design memristive networks to solve short-path optimization problems that can
also be solved by ant-colony algorithms. By employing appropriate memristive
elements one can demonstrate an almost one-to-one correspondence between
memcomputing and ant colony optimization approaches. However, the memristive
network has the capability of finding the solution in one deterministic step,
compared to the stochastic multi-step ant colony optimization. This result
paves the way for nanoscale hardware implementations of several swarm
intelligence algorithms that are presently explored, from scheduling problems
to robotics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6744</identifier>
 <datestamp>2014-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6744</id><created>2014-08-28</created><authors><author><keyname>Xie</keyname><forenames>Guodong</forenames></author><author><keyname>Li</keyname><forenames>Long</forenames></author><author><keyname>Ren</keyname><forenames>Yongxiong</forenames></author><author><keyname>Huang</keyname><forenames>Hao</forenames></author><author><keyname>Yan</keyname><forenames>Yan</forenames></author><author><keyname>Ahmed</keyname><forenames>Nisar</forenames></author><author><keyname>Zhao</keyname><forenames>Zhe</forenames></author><author><keyname>Lavery</keyname><forenames>Martin P. J.</forenames></author><author><keyname>Ashrafi</keyname><forenames>Nima</forenames></author><author><keyname>Ashrafi</keyname><forenames>Solyman</forenames></author><author><keyname>Tur</keyname><forenames>Moshe</forenames></author><author><keyname>Molisch</keyname><forenames>Andreas F.</forenames></author><author><keyname>Willner</keyname><forenames>Alan E.</forenames></author></authors><title>Performance Metrics and Design Parameters for a Free-space Communication
  Link Based on Multiplexing of Multiple Orbital-Angular-Momentum Beams</title><categories>physics.optics cs.IT math.IT</categories><comments>9 pages, 17 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the design parameters for an orbital angular momentum (OAM)
multiplexed free-space data link. Power loss, channel crosstalk and power
penalty of the link are analyzed in the case of misalignment between the
transmitter and receiver (lateral displacement, receiver angular error, or
transmitter pointing error). The relationship among the system power loss and
link distance, transmitted beam size and receiver aperture size are discussed
based on the beam divergence due to free space propagation. We also describe
the trade-offs for different receiver aperture sizes and mode spacing of the
transmitted OAM beams under given lateral displacements or receiver angular
errors. Through simulations and some experiments, we show that (1) a system
with a larger transmitted beam size and a larger receiver aperture is more
tolerant to the lateral displacement but less tolerant to the receiver angular
error; (2) a system with a larger mode spacing, which uses larger OAM charges,
suffers more system power loss but less channel crosstalk; thus, a system with
a small mode spacing shows lower system power penalty when system power loss
dominates (e.g., small lateral displacement or receiver angular error) while
that with a larger mode spacing shows lower power penalty when channel
crosstalk dominates (e.g., larger lateral displacement or receiver angular
error); (3) the effects of lateral displacement and receiver angular error are
not necessarily independent; as an example of them combined, the effects of the
transmitter pointing error on the system are also investigated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6746</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6746</id><created>2014-08-28</created><updated>2014-11-16</updated><authors><author><keyname>Beliga</keyname><forenames>Slobodan</forenames></author><author><keyname>Martin&#x10d;i&#x107;-Ip&#x161;i&#x107;</keyname><forenames>Sanda</forenames></author></authors><title>Non-Standard Words as Features for Text Categorization</title><categories>cs.CL cs.LG</categories><comments>IEEE 37th International Convention on Information and Communication
  Technology, Electronics and Microelectronics (MIPRO 2014), pp. 1415-1419,
  2014</comments><journal-ref>IEEE 37th International Convention on Information and
  Communication Technology, Electronics and Microelectronics (MIPRO 2014), pp.
  1415-1419, 2014</journal-ref><doi>10.1109/MIPRO.2014.6859744</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents categorization of Croatian texts using Non-Standard Words
(NSW) as features. Non-Standard Words are: numbers, dates, acronyms,
abbreviations, currency, etc. NSWs in Croatian language are determined
according to Croatian NSW taxonomy. For the purpose of this research, 390 text
documents were collected and formed the SKIPEZ collection with 6 classes:
official, literary, informative, popular, educational and scientific. Text
categorization experiment was conducted on three different representations of
the SKIPEZ collection: in the first representation, the frequencies of NSWs are
used as features; in the second representation, the statistic measures of NSWs
(variance, coefficient of variation, standard deviation, etc.) are used as
features; while the third representation combines the first two feature sets.
Naive Bayes, CN2, C4.5, kNN, Classification Trees and Random Forest algorithms
were used in text categorization experiments. The best categorization results
are achieved using the first feature set (NSW frequencies) with the
categorization accuracy of 87%. This suggests that the NSWs should be
considered as features in highly inflectional languages, such as Croatian. NSW
based features reduce the dimensionality of the feature space without standard
lemmatization procedures, and therefore the bag-of-NSWs should be considered
for further Croatian texts categorization experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6760</identifier>
 <datestamp>2014-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6760</id><created>2014-08-28</created><authors><author><keyname>van Asten</keyname><forenames>Benjamin J.</forenames></author><author><keyname>van Adrichem</keyname><forenames>Niels L. M.</forenames></author><author><keyname>Kuipers</keyname><forenames>Fernando A.</forenames></author></authors><title>Scalability and Resilience of Software-Defined Networking: An Overview</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software-Defined Networking (SDN) allows to control the available network
resources by an intelligent and centralized authority in order to optimize
traffic flows in a flexible manner. However, centralized control may face
scalability issues when the network size or the number of traffic flows
increases. Also, a centralized controller may form a single point of failure,
thereby affecting the network resilience.
  This article provides an overview of SDN that focuses on (1) scalability
concerning the increased control overhead faced by a central controller, and
(2) resiliency in terms of protection against controller failure, network
topology failure and security in terms of malicious attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6762</identifier>
 <datestamp>2014-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6762</id><created>2014-08-28</created><authors><author><keyname>Polatidis</keyname><forenames>Nikolaos</forenames></author></authors><title>Chatbot for admissions</title><categories>cs.CY cs.CL</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The communication of potential students with a university department is
performed manually and it is a very time consuming procedure. The opportunity
to communicate with on a one-to-one basis is highly valued. However with many
hundreds of applications each year, one-to-one conversations are not feasible
in most cases. The communication will require a member of academic staff to
expend several hours to find suitable answers and contact each student. It
would be useful to reduce his costs and time.
  The project aims to reduce the burden on the head of admissions, and
potentially other users, by developing a convincing chatbot. A suitable
algorithm must be devised to search through the set of data and find a
potential answer. The program then replies to the user and provides a relevant
web link if the user is not satisfied by the answer. Furthermore a web
interface is provided for both users and an administrator.
  The achievements of the project can be summarised as follows. To prepare the
background of the project a literature review was undertaken, together with an
investigation of existing tools, and consultation with the head of admissions.
The requirements of the system were established and a range of algorithms and
tools were investigated, including keyword and template matching. An algorithm
that combines keyword matching with string similarity has been developed. A
usable system using the proposed algorithm has been implemented. The system was
evaluated by keeping logs of questions and answers and by feedback received by
potential students that used it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6771</identifier>
 <datestamp>2014-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6771</id><created>2014-08-28</created><authors><author><keyname>Abel</keyname><forenames>Zachary</forenames></author><author><keyname>Demaine</keyname><forenames>Erik D.</forenames></author><author><keyname>Demaine</keyname><forenames>Martin L.</forenames></author><author><keyname>Eppstein</keyname><forenames>David</forenames></author><author><keyname>Lubiw</keyname><forenames>Anna</forenames></author><author><keyname>Uehara</keyname><forenames>Ryuhei</forenames></author></authors><title>Flat Foldings of Plane Graphs with Prescribed Angles and Edge Lengths</title><categories>cs.CG cs.DS</categories><comments>17 pages, 10 figures. To appear at Graph Drawing 2014</comments><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When can a plane graph with prescribed edge lengths and prescribed angles
(from among $\{0,180^\circ, 360^\circ\}$) be folded flat to lie in an
infinitesimally thick line, without crossings? This problem generalizes the
classic theory of single-vertex flat origami with prescribed mountain-valley
assignment, which corresponds to the case of a cycle graph. We characterize
such flat-foldable plane graphs by two obviously necessary but also sufficient
conditions, proving a conjecture made in 2001: the angles at each vertex should
sum to $360^\circ$, and every face of the graph must itself be flat foldable.
This characterization leads to a linear-time algorithm for testing flat
foldability of plane graphs with prescribed edge lengths and angles, and a
polynomial-time algorithm for counting the number of distinct folded states.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6788</identifier>
 <datestamp>2014-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6788</id><created>2014-08-28</created><updated>2014-08-29</updated><authors><author><keyname>Hough</keyname><forenames>Julian</forenames></author><author><keyname>Purver</keyname><forenames>Matthew</forenames></author></authors><title>Strongly Incremental Repair Detection</title><categories>cs.CL</categories><comments>12 pages, 6 figures, EMNLP conference long paper 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present STIR (STrongly Incremental Repair detection), a system that
detects speech repairs and edit terms on transcripts incrementally with minimal
latency. STIR uses information-theoretic measures from n-gram models as its
principal decision features in a pipeline of classifiers detecting the
different stages of repairs. Results on the Switchboard disfluency tagged
corpus show utterance-final accuracy on a par with state-of-the-art incremental
repair detection methods, but with better incremental accuracy, faster
time-to-detection and less computational overhead. We evaluate its performance
using incremental metrics and propose new repair processing evaluation
standards.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6800</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6800</id><created>2014-08-28</created><updated>2015-03-17</updated><authors><author><keyname>Choi</keyname><forenames>Jaehyung</forenames></author><author><keyname>Mullhaupt</keyname><forenames>Andrew P.</forenames></author></authors><title>Geometric shrinkage priors for K\&quot;ahlerian signal filters</title><categories>math.ST cs.IT math.DG math.IT stat.TH</categories><comments>10 pages, published version</comments><journal-ref>Entropy 17(3), 1347-1357 (2015)</journal-ref><doi>10.3390/e17031347</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We construct geometric shrinkage priors for K\&quot;ahlerian signal filters. Based
on the characteristics of K\&quot;ahler manifolds, an efficient and robust algorithm
for finding superharmonic priors which outperform the Jeffreys prior is
introduced. Several ans\&quot;atze for the Bayesian predictive priors are also
suggested. In particular, the ans\&quot;atze related to K\&quot;ahler potential are
geometrically intrinsic priors to the information manifold of which the
geometry is derived from the potential. The implication of the algorithm to
time series models is also provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6801</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6801</id><created>2014-08-27</created><updated>2014-11-20</updated><authors><author><keyname>Meister</keyname><forenames>Selina</forenames></author><author><keyname>Stockburger</keyname><forenames>J&#xfc;rgen T.</forenames></author><author><keyname>Schmidt</keyname><forenames>Rebecca</forenames></author><author><keyname>Ankerhold</keyname><forenames>Joachim</forenames></author></authors><title>Optimal control theory with arbitrary superpositions of waveforms</title><categories>math.OC cs.SY quant-ph</categories><comments>16 pages, 6 figures</comments><journal-ref>J. Phys. A: Math. Theor. 47 (2014) 495002</journal-ref><doi>10.1088/1751-8113/47/49/495002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Standard optimal control methods perform optimization in the time domain.
However, many experimental settings demand the expression of the control signal
as a superposition of given waveforms, a case that cannot easily be
accommodated using time-local constraints. Previous approaches [1,2] have
circumvented this difficulty by performing optimization in a parameter space,
using the chain rule to make a connection to the time domain. In this paper, we
present an extension to Optimal Control Theory which allows gradient-based
optimization for superpositions of arbitrary waveforms directly in a
time-domain subspace. Its key is the use of the Moore-Penrose pseudoinverse as
an efficient means of transforming between a time-local and waveform-based
descriptions. To illustrate this optimization technique, we study the
parametrically driven harmonic oscillator as model system and reduce its
energy, considering both Hamiltonian dynamics and stochastic dynamics under the
influence of a thermal reservoir. We demonstrate the viability and efficiency
of the method for these test cases and find significant advantages in the case
of waveforms which do not form an orthogonal basis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6804</identifier>
 <datestamp>2014-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6804</id><created>2014-08-28</created><updated>2014-11-18</updated><authors><author><keyname>Shah</keyname><forenames>Neel</forenames></author><author><keyname>Kolmogorov</keyname><forenames>Vladimir</forenames></author><author><keyname>Lampert</keyname><forenames>Christoph H.</forenames></author></authors><title>A Multi-Plane Block-Coordinate Frank-Wolfe Algorithm for Training
  Structural SVMs with a Costly max-Oracle</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Structural support vector machines (SSVMs) are amongst the best performing
models for structured computer vision tasks, such as semantic image
segmentation or human pose estimation. Training SSVMs, however, is
computationally costly, because it requires repeated calls to a structured
prediction subroutine (called \emph{max-oracle}), which has to solve an
optimization problem itself, e.g. a graph cut.
  In this work, we introduce a new algorithm for SSVM training that is more
efficient than earlier techniques when the max-oracle is computationally
expensive, as it is frequently the case in computer vision tasks. The main idea
is to (i) combine the recent stochastic Block-Coordinate Frank-Wolfe algorithm
with efficient hyperplane caching, and (ii) use an automatic selection rule for
deciding whether to call the exact max-oracle or to rely on an approximate one
based on the cached hyperplanes.
  We show experimentally that this strategy leads to faster convergence to the
optimum with respect to the number of requires oracle calls, and that this
translates into faster convergence with respect to the total runtime when the
max-oracle is slow compared to the other steps of the algorithm.
  A publicly available C++ implementation is provided at
http://pub.ist.ac.at/~vnk/papers/SVM.html .
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6806</identifier>
 <datestamp>2014-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6806</id><created>2014-08-28</created><updated>2014-08-29</updated><authors><author><keyname>Elizarov</keyname><forenames>Alexander</forenames></author><author><keyname>Kirillovich</keyname><forenames>Alexander</forenames></author><author><keyname>Lipachev</keyname><forenames>Evgeny</forenames></author><author><keyname>Nevzorova</keyname><forenames>Olga</forenames></author><author><keyname>Solovyev</keyname><forenames>Valery</forenames></author><author><keyname>Zhiltsov</keyname><forenames>Nikita</forenames></author></authors><title>Mathematical Knowledge Representation: Semantic Models and Formalisms</title><categories>cs.AI cs.DL cs.IR</categories><comments>10 pages, Lobachevskii J. of Mathematics, 2014, V.35, No 4</comments><msc-class>68T30</msc-class><acm-class>I.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper provides a survey of semantic methods for solution of fundamental
tasks in mathematical knowledge management. Ontological models and formalisms
are discussed. We propose an ontology of mathematical knowledge, covering a
wide range of fields of mathematics. We demonstrate applications of this
representation in mathematical formula search, and learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6809</identifier>
 <datestamp>2014-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6809</id><created>2014-08-28</created><authors><author><keyname>Peni</keyname><forenames>Tamas</forenames></author><author><keyname>Seiler</keyname><forenames>Peter J.</forenames></author></authors><title>Computation of lower bounds for the induced L2 norm of LPV systems</title><categories>cs.SY</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Determining the induced L2 norm of a linear, parameter-varying (LPV) system
is an integral part of many analysis and robust control design procedures. Most
prior work has focused on efficiently computing upper bounds for the induced L2
norm. The conditions for upper bounds are typically based on scaled small-gain
theorems with dynamic multipliers or dissipation inequalities with parameter
dependent Lyapunov functions. This paper presents a complementary algorithm to
compute lower bounds for the induced L2 norm. The proposed approach computes a
lower bound on the gain by restricting the parameter trajectory to be a
periodic signal. This restriction enables the use of recent results for exact
calculation of the L2 norm for a periodic linear time varying system. The
proposed lower bound algorithm has two benefits. First, the lower bound
complements standard upper bound techniques. Specifically, a small gap between
the bounds indicates that further computation, e.g. upper bounds with more
complex Lyapunov functions, is unnecessary. Second, the lower bound algorithm
returns a bad parameter trajectory for the LPV system that can be further
analyzed to provide insight into the system performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6812</identifier>
 <datestamp>2014-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6812</id><created>2014-08-28</created><updated>2014-11-04</updated><authors><author><keyname>Bose</keyname><forenames>Prosenjit</forenames></author><author><keyname>De Carufel</keyname><forenames>Jean-Lou</forenames></author></authors><title>Towards a General Framework for Searching on a Line and Searching on $m$
  Rays</title><categories>cs.DS</categories><comments>Submitted to ACM-SIAM Symposium on Discrete Algorithms (SODA 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider the following classical search problem: given a target point $p\in
\Re$, starting at the origin, find $p$ with minimum cost, where cost is defined
as the distance travelled. Let $D$ be the distance of $p$ from the origin. When
no lower bound on $D$ is given, no competitive search strategy exists. Demaine,
Fekete and Gal (Online searching with turn cost, Theor. Comput. Sci.,
361(2-3):342-355, 2006) considered the situation where no lower bound on $D$ is
given but a fixed \emph{turn cost} $t&gt;0$ is charged every time the searcher
changes direction. When the total cost is expressed as $c D+\phi$, where $c$
and $\phi$ are positive constants, they showed that if $c$ is set to $9$, then
the optimal search strategy has a cost of $9D+2t$. Although their strategy is
optimal for $c=9$, we prove that the minimum cost in their framework is
$5D+t+2\sqrt{2D(2D+t)} &lt; 9D+2t$. Note that the minimum cost requires knowledge
of $D$. However, given $D$, the optimal strategy has a smaller cost of $3D+t$.
Therefore, this problem cannot be solved optimally and exactly when no lower
bound on $D$ is given.
  To resolve this issue, we introduce a general framework where the cost of
moving distance $x$ away from the origin is $\alpha_1 x+\beta_1$ and the cost
of moving distance $y$ towards the origin is $\alpha_2 y+\beta_2$ for constants
$\alpha_1,\alpha_2,\beta_1,\beta_2$. Given a lower bound $\lambda$ on $D$, we
provide a provably optimal competitive search strategy when
$\alpha_1,\alpha_2,\beta_1,\beta_2 \geq 0$ and $\alpha_1+\alpha_2 &gt; 0$.
Finally, we address the problem of searching for a target lying on one of $m$
rays extending from the origin where the cost is measured as the total distance
travelled plus $t \geq 0$ times the number of turns. We provide a search
strategy and compute its cost. We prove our strategy is optimal for small
values of $t$ and conjecture it is always optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6821</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6821</id><created>2014-08-28</created><updated>2016-01-29</updated><authors><author><keyname>Frieze</keyname><forenames>Alan</forenames></author><author><keyname>Pegden</keyname><forenames>Wesley</forenames></author></authors><title>Looking for vertex number one</title><categories>math.CO cs.DS</categories><comments>improved theorem; errors corrected</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given an instance of the preferential attachment graph $G_n=([n],E_n)$, we
would like to find vertex 1, using only 'local' information about the graph;
that is, by exploring the neighborhoods of small sets of vertices. Borgs et. al
gave an an algorithm which runs in time $O(\log^4 n)$, which is local in the
sense that at each step, it needs only to search the neighborhood of a set of
vertices of size $O(\log^4 n)$. We give an algorithm to find vertex 1, which
w.h.p. runs in time $O(\omega\log n)$ and which is local in the strongest sense
of operating only on neighborhoods of single vertices. Here $\omega=\omega(n)$
is any function that goes to infinity with $n$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6822</identifier>
 <datestamp>2014-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6822</id><created>2014-08-28</created><authors><author><keyname>Song</keyname><forenames>Dongjin</forenames></author><author><keyname>Meyer</keyname><forenames>David A.</forenames></author></authors><title>A Model of Consistent Node Types in Signed Directed Social Networks</title><categories>cs.SI physics.soc-ph stat.AP</categories><comments>To appear in the IEEE/ACM International Conference on Advances in
  Social Network Analysis and Mining (ASONAM), 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Signed directed social networks, in which the relationships between users can
be either positive (indicating relations such as trust) or negative (indicating
relations such as distrust), are increasingly common. Thus the interplay
between positive and negative relationships in such networks has become an
important research topic. Most recent investigations focus upon edge sign
inference using structural balance theory or social status theory. Neither of
these two theories, however, can explain an observed edge sign well when the
two nodes connected by this edge do not share a common neighbor (e.g., common
friend). In this paper we develop a novel approach to handle this situation by
applying a new model for node types. Initially, we analyze the local node
structure in a fully observed signed directed network, inferring underlying
node types. The sign of an edge between two nodes must be consistent with their
types; this explains edge signs well even when there are no common neighbors.
We show, moreover, that our approach can be extended to incorporate directed
triads, when they exist, just as in models based upon structural balance or
social status theory. We compute Bayesian node types within empirical studies
based upon partially observed Wikipedia, Slashdot, and Epinions networks in
which the largest network (Epinions) has 119K nodes and 841K edges. Our
approach yields better performance than state-of-the-art approaches for these
three signed directed networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6824</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6824</id><created>2014-08-28</created><updated>2014-09-26</updated><authors><author><keyname>Ye</keyname><forenames>Min</forenames></author><author><keyname>Barg</keyname><forenames>Alexander</forenames></author></authors><title>Universal Source Polarization and an Application to a Multi-User Problem</title><categories>cs.IT math.IT</categories><comments>to be presented at Allerton 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a scheme that universally achieves the smallest possible
compression rate for a class of sources with side information, and develop an
application of this result for a joint source channel coding problem over a
broadcast channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6828</identifier>
 <datestamp>2014-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6828</id><created>2014-08-28</created><authors><author><keyname>Szolnoki</keyname><forenames>Attila</forenames></author><author><keyname>Mobilia</keyname><forenames>Mauro</forenames></author><author><keyname>Jiang</keyname><forenames>Luo-Luo</forenames></author><author><keyname>Szczesny</keyname><forenames>Bartosz</forenames></author><author><keyname>Rucklidge</keyname><forenames>Alastair M.</forenames></author><author><keyname>Perc</keyname><forenames>Matjaz</forenames></author></authors><title>Cyclic dominance in evolutionary games: A review</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI nlin.AO q-bio.PE</categories><comments>21 two-column pages, 16 figures; accepted for publication in Journal
  of the Royal Society Interface</comments><journal-ref>J. R. Soc. Interface 11 (2014) 20140735</journal-ref><doi>10.1098/rsif.2014.0735</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rock is wrapped by paper, paper is cut by scissors, and scissors are crushed
by rock. This simple game is popular among children and adults to decide on
trivial disputes that have no obvious winner, but cyclic dominance is also at
the heart of predator-prey interactions, the mating strategy of side-blotched
lizards, the overgrowth of marine sessile organisms, and the competition in
microbial populations. Cyclical interactions also emerge spontaneously in
evolutionary games entailing volunteering, reward, punishment, and in fact are
common when the competing strategies are three or more regardless of the
particularities of the game. Here we review recent advances on the
rock-paper-scissors and related evolutionary games, focusing in particular on
pattern formation, the impact of mobility, and the spontaneous emergence of
cyclic dominance. We also review mean-field and zero-dimensional
rock-paper-scissors models and the application of the complex Ginzburg-Landau
equation, and we highlight the importance and usefulness of statistical physics
for the successful study of large-scale ecological systems. Directions for
future research, related for example to dynamical effects of coevolutionary
rules and invasion reversals due to multi-point interactions, are outlined as
well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6829</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6829</id><created>2014-08-28</created><updated>2015-04-28</updated><authors><author><keyname>Li</keyname><forenames>Ke</forenames></author><author><keyname>Smith</keyname><forenames>Graeme</forenames></author></authors><title>Quantum de Finetti theorem under fully-one-way adaptive measurements</title><categories>quant-ph cs.CC</categories><comments>V2: minor changes. V3: new title, more discussions added,
  presentation improved. V4: minor changes, close to published version</comments><journal-ref>Phys. Rev. Lett. 114, 160503 (2015)</journal-ref><doi>10.1103/PhysRevLett.114.160503</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove a version of the quantum de Finetti theorem: permutation-invariant
quantum states are well approximated as a probabilistic mixture of multi-fold
product states. The approximation is measured by distinguishability under fully
one-way LOCC (local operations and classical communication) measurements. Our
result strengthens Brand\~{a}o and Harrow's de Finetti theorem where a kind of
partially one-way LOCC measurements was used for measuring the approximation,
with essentially the same error bound. As main applications, we show (i) a
quasipolynomial-time algorithm which detects multipartite entanglement with
amount larger than an arbitrarily small constant (measured with a variant of
the relative entropy of entanglement), and (ii) a proof that in quantum
Merlin-Arthur proof systems, polynomially many provers are not more powerful
than a single prover when the verifier is restricted to one-way LOCC
operations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6876</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6876</id><created>2014-08-28</created><updated>2014-11-09</updated><authors><author><keyname>Marzen</keyname><forenames>Sarah</forenames></author><author><keyname>Crutchfield</keyname><forenames>James P.</forenames></author></authors><title>Informational and Causal Architecture of Discrete-Time Renewal Processes</title><categories>cond-mat.stat-mech cs.IT math.IT math.ST nlin.CD stat.TH</categories><comments>18 pages, 9 figures, 1 table;
  http://csc.ucdavis.edu/~cmg/compmech/pubs/dtrp.htm</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Renewal processes are broadly used to model stochastic behavior consisting of
isolated events separated by periods of quiescence, whose durations are
specified by a given probability law. Here, we identify the minimal sufficient
statistic for their prediction (the set of causal states), calculate the
historical memory capacity required to store those states (statistical
complexity), delineate what information is predictable (excess entropy), and
decompose the entropy of a single measurement into that shared with the past,
future, or both. The causal state equivalence relation defines a new subclass
of renewal processes with a finite number of causal states despite having an
unbounded interevent count distribution. We use these formulae to analyze the
output of the parametrized Simple Nonunifilar Source, generated by a simple
two-state hidden Markov model, but with an infinite-state epsilon-machine
presentation. All in all, the results lay the groundwork for analyzing
processes with infinite statistical complexity and infinite excess entropy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6878</identifier>
 <datestamp>2014-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6878</id><created>2014-08-28</created><updated>2014-09-03</updated><authors><author><keyname>Biro</keyname><forenames>Peter</forenames></author><author><keyname>McBride</keyname><forenames>Iain</forenames></author></authors><title>Integer programming methods for special college admissions problems</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop Integer Programming (IP) solutions for some special college
admission problems arising from the Hungarian higher education admission
scheme. We focus on four special features, namely the solution concept of
stable score-limits, the presence of lower and common quotas, and paired
applications. We note that each of the latter three special feature makes the
college admissions problem NP-hard to solve. Currently, a heuristic based on
the Gale-Shapley algorithm is being used in the application. The IP methods
that we propose are not only interesting theoretically, but may also serve as
an alternative solution concept for this practical application, and also for
other ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6883</identifier>
 <datestamp>2014-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6883</id><created>2014-08-28</created><updated>2014-09-02</updated><authors><author><keyname>Yayla</keyname><forenames>O&#x11f;uz</forenames></author></authors><title>Nearly perfect sequences with arbitrary out-of-phase autocorrelation</title><categories>math.CO cs.IT math.IT</categories><comments>18 pages</comments><msc-class>05B10, 94A55</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study nearly perfect sequences (NPS) via their connection to
direct product difference sets (DPDS). We prove the connection between a
$p$-ary NPS of period $n$ and type $\gamma$ and a cyclic
$(n,p,n,\frac{n-\gamma}{p}+\gamma,0,\frac{n-\gamma}{p})$-DPDS for an arbitrary
integer $\gamma$. Next, we present the necessary conditions for the existence
of a $p$-ary NPS of type $\gamma$. We apply this result for excluding the
existence of some $p$-ary NPS of period $n$ and type $\gamma$ for $n \leq 100$
and $\vert \gamma \vert \leq 2$. We also prove the similar results for an
almost $p$-ary NPS of type $\gamma$. Finally, we show the non-existence of some
almost $p$-ary perfect sequences by showing the non-existence of equivalent
cyclic relative difference sets by using the notion of multipliers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6889</identifier>
 <datestamp>2014-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6889</id><created>2014-08-28</created><authors><author><keyname>Zamani</keyname><forenames>Mohsen</forenames></author><author><keyname>Helmke</keyname><forenames>Uwe</forenames></author><author><keyname>Anderson</keyname><forenames>Brian D. O.</forenames></author></authors><title>Zeros of Networked Systems with Time-invariant Interconnections</title><categories>cs.SY</categories><comments>Preprint submitted for possible publication in Automatica</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies zeros of networked linear systems with time-invariant
interconnection topology. While the characterization of zeros is given for both
heterogeneous and homogeneous networks, homogeneous networks are explored in
greater detail. In the current paper, for homogeneous networks with
time-invariant interconnection dynamics, it is illustrated how the zeros of
each individual agent's system description and zeros definable from the
interconnection dynamics contribute to generating zeros of the whole network.
We also demonstrate how zeros of networked systems and those of their
associated blocked versions are related.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6891</identifier>
 <datestamp>2015-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6891</id><created>2014-08-28</created><updated>2015-02-18</updated><authors><author><keyname>Buyya</keyname><forenames>Rajkumar</forenames></author><author><keyname>Calheiros</keyname><forenames>Rodrigo N.</forenames></author><author><keyname>Son</keyname><forenames>Jungmin</forenames></author><author><keyname>Dastjerdi</keyname><forenames>Amir Vahid</forenames></author><author><keyname>Yoon</keyname><forenames>Young</forenames></author></authors><title>Software-Defined Cloud Computing: Architectural Elements and Open
  Challenges</title><categories>cs.DC</categories><comments>Keynote Paper, 3rd International Conference on Advances in Computing,
  Communications and Informatics (ICACCI 2014), September 24-27, 2014, Delhi,
  India</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The variety of existing cloud services creates a challenge for service
providers to enforce reasonable Software Level Agreements (SLA) stating the
Quality of Service (QoS) and penalties in case QoS is not achieved. To avoid
such penalties at the same time that the infrastructure operates with minimum
energy and resource wastage, constant monitoring and adaptation of the
infrastructure is needed. We refer to Software-Defined Cloud Computing, or
simply Software-Defined Clouds (SDC), as an approach for automating the process
of optimal cloud configuration by extending virtualization concept to all
resources in a data center. An SDC enables easy reconfiguration and adaptation
of physical resources in a cloud infrastructure, to better accommodate the
demand on QoS through a software that can describe and manage various aspects
comprising the cloud environment. In this paper, we present an architecture for
SDCs on data centers with emphasis on mobile cloud applications. We present an
evaluation, showcasing the potential of SDC in two use cases-QoS-aware
bandwidth allocation and bandwidth-aware, energy-efficient VM placement-and
discuss the research challenges and opportunities in this emerging area.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6894</identifier>
 <datestamp>2016-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6894</id><created>2014-08-28</created><updated>2016-01-13</updated><authors><author><keyname>Hayashi</keyname><forenames>Masahito</forenames></author><author><keyname>Tomamichel</keyname><forenames>Marco</forenames></author></authors><title>Correlation Detection and an Operational Interpretation of the Renyi
  Mutual Information</title><categories>quant-ph cs.IT math-ph math.IT math.MP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, a variety of new measures of quantum Renyi mutual information and
quantum Renyi conditional entropy have been proposed, and some of their
mathematical properties explored. Here, we show that the Renyi mutual
information attains operational meaning in the context of composite hypothesis
testing, when the null hypothesis is a fixed bipartite state and the alternate
hypothesis consists of all product states that share one marginal with the null
hypothesis. This hypothesis testing problem occurs naturally in channel coding,
where it corresponds to testing whether a state is the output of a given
quantum channel or of a 'useless' channel whose output is decoupled from the
environment. Similarly, we establish an operational interpretation of Renyi
conditional entropy by choosing an alternative hypothesis that consists of
product states that are maximally mixed on one system. Specialized to classical
probability distributions, our results also establish an operational
interpretation of Renyi mutual information and Renyi conditional entropy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6908</identifier>
 <datestamp>2014-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6908</id><created>2014-08-28</created><updated>2014-10-09</updated><authors><author><keyname>Hernandez-Orallo</keyname><forenames>Jose</forenames></author></authors><title>AI Evaluation: past, present and future</title><categories>cs.AI</categories><comments>34 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Artificial intelligence develops techniques and systems whose performance
must be evaluated on a regular basis in order to certify and foster progress in
the discipline. We will describe and critically assess the different ways AI
systems are evaluated. We first focus on the traditional task-oriented
evaluation approach. We see that black-box (behavioural evaluation) is becoming
more and more common, as AI systems are becoming more complex and
unpredictable. We identify three kinds of evaluation: Human discrimination,
problem benchmarks and peer confrontation. We describe the limitations of the
many evaluation settings and competitions in these three categories and propose
several ideas for a more systematic and robust evaluation. We then focus on a
less customary (and challenging) ability-oriented evaluation approach, where a
system is characterised by its (cognitive) abilities, rather than by the tasks
it is designed to solve. We discuss several possibilities: the adaptation of
cognitive tests used for humans and animals, the development of tests derived
from algorithmic information theory or more general approaches under the
perspective of universal psychometrics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6911</identifier>
 <datestamp>2014-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6911</id><created>2014-08-28</created><authors><author><keyname>Adak</keyname><forenames>Chandranath</forenames></author><author><keyname>Chaudhuri</keyname><forenames>Bidyut B.</forenames></author></authors><title>Text Line Identification in Tagore's Manuscript</title><categories>cs.CV</categories><journal-ref>Proc. IEEE TechSym-2014, IEEE Conf. #32812, pp. 210-213,
  Kharagpur, India, 28 Feb.-2 Mar., 2014</journal-ref><doi>10.1109/TechSym.2014.6808048</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a text line identification method is proposed. The text lines
of printed document are easy to segment due to uniform straightness of the
lines and sufficient gap between the lines. But in handwritten documents, the
line is non-uniform and interline gaps are variable. We take Rabindranath
Tagore's manuscript as it is one of the most difficult manuscripts that contain
doodles. Our method consists of a pre-processing stage to clean the document
image. Then we separate doodles from the manuscript to get the textual region.
After that we identify the text lines on the manuscript. For text line
identification, we use window examination, black run-length smearing,
horizontal histogram and connected component analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6915</identifier>
 <datestamp>2014-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6915</id><created>2014-08-28</created><authors><author><keyname>Skirlo</keyname><forenames>Scott A.</forenames></author><author><keyname>Lu</keyname><forenames>Ling</forenames></author><author><keyname>Solja&#x10d;i&#x107;</keyname><forenames>Marin</forenames></author></authors><title>Binary matrices of optimal autocorrelations as alignment marks</title><categories>cs.CV cs.IT math.IT</categories><comments>8 pages, 6 figures and 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define a new class of binary matrices by maximizing the peak-sidelobe
distances in the aperiodic autocorrelations. These matrices can be used as
robust position marks for in-plane spatial alignment. The optimal square
matrices of dimensions up to 7 by 7 and optimal diagonally-symmetric matrices
of 8 by 8 and 9 by 9 were found by exhaustive searches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6916</identifier>
 <datestamp>2014-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6916</id><created>2014-08-28</created><updated>2014-09-26</updated><authors><author><keyname>Wang</keyname><forenames>Jiannan</forenames></author><author><keyname>Li</keyname><forenames>Guoliang</forenames></author><author><keyname>Kraska</keyname><forenames>Tim</forenames></author><author><keyname>Franklin</keyname><forenames>Michael J.</forenames></author><author><keyname>Feng</keyname><forenames>Jianhua</forenames></author></authors><title>Leveraging Transitive Relations for Crowdsourced Joins</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The development of crowdsourced query processing systems has recently
attracted a significant attention in the database community. A variety of
crowdsourced queries have been investigated. In this paper, we focus on the
crowdsourced join query which aims to utilize humans to find all pairs of
matching objects from two collections. As a human-only solution is expensive,
we adopt a hybrid human-machine approach which first uses machines to generate
a candidate set of matching pairs, and then asks humans to label the pairs in
the candidate set as either matching or non-matching. Given the candidate
pairs, existing approaches will publish all pairs for verification to a
crowdsourcing platform. However, they neglect the fact that the pairs satisfy
transitive relations. As an example, if $o_1$ matches with $o_2$, and $o_2$
matches with $o_3$, then we can deduce that $o_1$ matches with $o_3$ without
needing to crowdsource $(o_1, o_3)$. To this end, we study how to leverage
transitive relations for crowdsourced joins. We propose a hybrid
transitive-relations and crowdsourcing labeling framework which aims to
crowdsource the minimum number of pairs to label all the candidate pairs. We
prove the optimal labeling order in an ideal setting and propose a heuristic
labeling order in practice. We devise a parallel labeling algorithm to
efficiently crowdsource the pairs following the order. We evaluate our
approaches in both simulated environment and a real crowdsourcing platform.
Experimental results show that our approaches with transitive relations can
save much more money and time than existing methods, with a little loss in the
result quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6923</identifier>
 <datestamp>2014-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6923</id><created>2014-08-29</created><authors><author><keyname>Oancea</keyname><forenames>Bogdan</forenames></author><author><keyname>Andrei</keyname><forenames>Tudorel</forenames></author><author><keyname>Dragoescu</keyname><forenames>Raluca Mariana</forenames></author></authors><title>GPGPU Computing</title><categories>cs.DC</categories><journal-ref>CKS 2012</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Since the first idea of using GPU to general purpose computing, things have
evolved over the years and now there are several approaches to GPU programming.
GPU computing practically began with the introduction of CUDA (Compute Unified
Device Architecture) by NVIDIA and Stream by AMD. These are APIs designed by
the GPU vendors to be used together with the hardware that they provide. A new
emerging standard, OpenCL (Open Computing Language) tries to unify different
GPU general computing API implementations and provides a framework for writing
programs executed across heterogeneous platforms consisting of both CPUs and
GPUs. OpenCL provides parallel computing using task-based and data-based
parallelism. In this paper we will focus on the CUDA parallel computing
architecture and programming model introduced by NVIDIA. We will present the
benefits of the CUDA programming model. We will also compare the two main
approaches, CUDA and AMD APP (STREAM) and the new framwork, OpenCL that tries
to unify the GPGPU computing models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6924</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6924</id><created>2014-08-29</created><authors><author><keyname>Xu</keyname><forenames>Mingguang</forenames></author><author><keyname>Guo</keyname><forenames>Dongning</forenames></author><author><keyname>Honig</keyname><forenames>Michael L.</forenames></author></authors><title>Joint Bi-Directional Training of Nonlinear Precoders and Receivers in
  Cellular Networks</title><categories>cs.IT math.IT</categories><comments>12 pages, 9 figures, submitted to IEEE Trans. Signal Process., Aug.
  2014</comments><doi>10.1109/TSP.2015.2442959</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Joint optimization of nonlinear precoders and receive filters is studied for
both the uplink and downlink in a cellular system. For the uplink, the base
transceiver station (BTS) receiver implements successive interference
cancellation, and for the downlink, the BTS station pre-compensates for the
interference with Tomlinson-Harashima precoding (THP). Convergence of
alternating optimization of receivers and transmitters in a single cell is
established when filters are updated according to a minimum mean squared error
(MMSE) criterion, subject to appropriate power constraints. Adaptive algorithms
are then introduced for updating the precoders and receivers in the absence of
channel state information, assuming time-division duplex transmissions with
channel reciprocity. Instead of estimating the channels, the filters are
directly estimated according to a least squares criterion via bi-directional
training: Uplink pilots are used to update the feedforward and feedback
filters, which are then used as interference pre-compensation filters for
downlink training of the mobile receivers. Numerical results show that
nonlinear filters can provide substantial gains relative to linear filters with
limited forward-backward iterations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6926</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6926</id><created>2014-08-29</created><authors><author><keyname>Polatidis</keyname><forenames>Nikolaos</forenames></author><author><keyname>Georgiadis</keyname><forenames>Christos K.</forenames></author></authors><title>Factors Influencing the Quality of the User Experience in Ubiquitous
  Recommender Systems</title><categories>cs.HC cs.CR cs.IR</categories><comments>The final publication is available at www.springerlink.com
  Distributed, Ambient, and Pervasive Interactions Lecture Notes in Computer
  Science Volume 8530, 2014, pp 369-379</comments><journal-ref>Polatidis, N., &amp; Georgiadis, C. K. (2014). Factors Influencing the
  Quality of the User Experience in Ubiquitous Recommender Systems. In
  Distributed, Ambient, and Pervasive Interactions (pp. 369-379). Springer
  International Publishing</journal-ref><doi>10.1007/978-3-319-07788-8_35</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The use of mobile devices and the rapid growth of the internet and networking
infrastructure has brought the necessity of using Ubiquitous recommender
systems. However in mobile devices there are different factors that need to be
considered in order to get more useful recommendations and increase the quality
of the user experience. This paper gives an overview of the factors related to
the quality and proposes a new hybrid recommendation model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6927</identifier>
 <datestamp>2014-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6927</id><created>2014-08-29</created><authors><author><keyname>Krotov</keyname><forenames>Denis S.</forenames></author><author><keyname>&#xd6;sterg&#xe5;rd</keyname><forenames>Patric R. J.</forenames></author><author><keyname>Pottonen</keyname><forenames>Olli</forenames></author></authors><title>Non-existence of a ternary constant weight $(16, 5, 15; 2048)$ diameter
  perfect code</title><categories>cs.IT math.IT</categories><comments>9 pages. Submitted for publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ternary constant weight codes of length $n=2^m$, weight $n-1$, cardinality
$2^n$ and distance $5$ are known to exist for every $m$ for which there exists
an APN permutation of order $2^m$, that is, at least for all odd $m \geq 3$ and
for $m=6$. We show the non-existence of such codes for $m=4$ and prove that any
codes with the parameters above are diameter perfect.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6928</identifier>
 <datestamp>2014-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6928</id><created>2014-08-29</created><authors><author><keyname>Alam</keyname><forenames>Md. Jawaherul</forenames></author><author><keyname>Kobourov</keyname><forenames>Stephen G.</forenames></author><author><keyname>Pupyrev</keyname><forenames>Sergey</forenames></author><author><keyname>Toeniskoetter</keyname><forenames>Jackson</forenames></author></authors><title>Weak Unit Disk and Interval Representation of Planar Graphs</title><categories>cs.DM cs.CG math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a variant of intersection representations with unit balls, that is,
unit disks in the plane and unit intervals on the line. Given a planar graph
and a bipartition of the edges of the graph into near and far sets, the goal is
to represent the vertices of the graph by unit balls so that the balls
representing two adjacent vertices intersect if and only if the corresponding
edge is near. We consider the problem in the plane and prove that it is NP-hard
to decide whether such a representation exists for a given edge-partition. On
the other hand, every series-parallel graph admits such a representation with
unit disks for any near/far labeling of the edges. We also show that the
representation problem on the line is equivalent to a variant of a graph
coloring. We give examples of girth-4 planar and girth-3 outerplanar graphs
that have no such representation with unit intervals. On the other hand, all
triangle-free outerplanar graphs and all graphs with maximum average degree
less than 26/11 can always be represented. In particular, this gives a simple
proof of representability of all planar graphs with large girth.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6930</identifier>
 <datestamp>2014-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6930</id><created>2014-08-29</created><authors><author><keyname>Polatidis</keyname><forenames>Nikolaos</forenames></author><author><keyname>Georgiadis</keyname><forenames>Christos K.</forenames></author></authors><title>Mobile recommender systems: An overview of technologies and challenges</title><categories>cs.IR</categories><comments>The final publication is available at IEEE xplore
  http://ieeexplore.ieee.org/xpl/articleDetails.jsp?tp=&amp;arnumber=6650270&amp;url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D6650270</comments><journal-ref>Polatidis, N., &amp; Georgiadis, C. K. (2013, September). Mobile
  recommender systems: An overview of technologies and challenges. In
  Informatics and Applications (ICIA), 2013 Second International Conference on
  (pp. 282-287). IEEE</journal-ref><doi>10.1109/ICoIA.2013.6650270</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The use of mobile devices in combination with the rapid growth of the
internet has generated an information overload problem. Recommender systems is
a necessity to decide which of the data are relevant to the user. However in
mobile devices there are different factors who are crucial to information
retrieval, such as the location, the screen size and the processor speed. This
paper gives an overview of the technologies related to mobile recommender
systems and a more detailed description of the challenged faced.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6949</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6949</id><created>2014-08-29</created><updated>2015-02-25</updated><authors><author><keyname>Espinosa-Ortega</keyname><forenames>T.</forenames></author><author><keyname>Liew</keyname><forenames>T. C. H.</forenames></author></authors><title>Perceptrons with Hebbian learning based on wave ensembles in plastic
  potentials</title><categories>cond-mat.dis-nn cs.ET</categories><journal-ref>Phys. Rev. Lett. 114, 118101 (2015)</journal-ref><doi>10.1103/PhysRevLett.114.118101</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A general scheme to realize a perceptron for hardware neural networks is
presented, where multiple interconnections are achieved by a superposition of
Schrodinger waves. Spatially patterned potentials process information by
coupling different points of reciprocal space. The necessary potential shape is
obtained from the Hebbian learning rule, either through exact calculation or
construction from a superposition of known optical inputs. This allows
implementation in a wide range of compact optical systems, including: 1) any
non-linear optical system; 2) optical systems patterned by optical lithography;
and 3) exciton-polariton systems with phonon or nuclear spin interactions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6959</identifier>
 <datestamp>2014-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6959</id><created>2014-08-29</created><authors><author><keyname>Qu</keyname><forenames>Bo</forenames></author><author><keyname>Hanjalic</keyname><forenames>Alan</forenames></author><author><keyname>Wang</keyname><forenames>Huijuan</forenames></author></authors><title>Heterogeneous Recovery Rates against SIS Epidemics in Directed Networks</title><categories>physics.soc-ph cs.SI physics.data-an</categories><comments>6 figures, conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The nodes in communication networks are possibly and most likely equipped
with different recovery resources, which allow them to recover from a virus
with different rates. In this paper, we aim to understand know how to allocate
the limited recovery resources to efficiently prevent the spreading of
epidemics. We study the susceptible-infected-susceptible (SIS) epidemic model
on directed scale-free networks. In the classic SIS model, a susceptible node
can be infected by an infected neighbor with the infection rate $\beta$ and an
infected node can be recovered to be susceptible again with the recovery rate
$\delta$. In the steady state a fraction $y_\infty$ of nodes are infected,
which shows how severely the network is infected. We propose to allocate the
recovery rate $\delta_i$ for node $i$ according to its indegree and
outdegree-$\delta_i\scriptsize{\sim}k_{i,in}^{\alpha_{in}}k_{i,out}^{\alpha_{out}}$,
given the finite average recovery rate $\langle\delta\rangle$ representing the
limited recovery resources over the whole network. We find that, by tuning the
two scaling exponents $\alpha_{in}$ and $\alpha_{out}$, we can always reduce
the infection fraction $y_\infty$ thus reducing the extent of infections,
comparing to the homogeneous recovery rates allocation. Moreover, we can find
our optimal strategy via the optimal choice of the exponent $\alpha_{in}$ and
$\alpha_{out}$. Our optimal strategy indicates that when the recovery resources
are sufficient, more resources should be allocated to the nodes with a larger
indegree or outdegree, but when the recovery resource is very limited, only the
nodes with a larger outdegree should be equipped with more resources. We also
find that our optimal strategy works better when the recovery resources are
sufficient but not yet able to make the epidemic die out, and when the indegree
outdegree correlation is small.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6963</identifier>
 <datestamp>2014-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6963</id><created>2014-08-29</created><authors><author><keyname>Boix</keyname><forenames>Xavier</forenames></author><author><keyname>Roig</keyname><forenames>Gemma</forenames></author><author><keyname>Van Gool</keyname><forenames>Luc</forenames></author></authors><title>Comment on &quot;Ensemble Projection for Semi-supervised Image
  Classification&quot;</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a series of papers by Dai and colleagues [1,2], a feature map (or kernel)
was introduced for semi- and unsupervised learning. This feature map is build
from the output of an ensemble of classifiers trained without using the
ground-truth class labels. In this critique, we analyze the latest version of
this series of papers, which is called Ensemble Projections [2]. We show that
the results reported in [2] were not well conducted, and that Ensemble
Projections performs poorly for semi-supervised learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6966</identifier>
 <datestamp>2014-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6966</id><created>2014-08-29</created><authors><author><keyname>Zheng</keyname><forenames>Gan</forenames></author><author><keyname>Krikidis</keyname><forenames>Ioannis</forenames></author><author><keyname>Masouros</keyname><forenames>Christos</forenames></author><author><keyname>Timotheou</keyname><forenames>Stelios</forenames></author><author><keyname>Toumpakaris</keyname><forenames>Dimitris-Alexandros</forenames></author><author><keyname>Ding</keyname><forenames>Zhiguo</forenames></author></authors><title>Rethinking the Role of Interference in Wireless Networks</title><categories>cs.IT math.IT</categories><comments>6 figures, accepted in IEEE Communications Magazine</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article re-examines the fundamental notion of interference in wireless
networks by contrasting traditional approaches to new concepts that handle
interference in a creative way. Specifically, we discuss the fundamental limits
of the interference channel and present the interference alignment technique
and its extension of signal alignment techniques. Contrary to this traditional
view, which treats interference as a detrimental phenomenon, we introduce three
concepts that handle interference as a useful resource. The first concept
exploits interference at the modulation level and leads to simple multiuser
downlink precoding that provides significant energy savings. The second concept
uses radio frequency radiation for energy harvesting and handles interference
as a source of green energy. The last concept refers to a secrecy environment
and uses interference as an efficient means to jam potential eavesdroppers.
These three techniques bring a new vision about interference in wireless
networks and motivate a plethora of potential new applications and services.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6970</identifier>
 <datestamp>2014-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6970</id><created>2014-08-29</created><authors><author><keyname>Mohammadi</keyname><forenames>Mahdi Soodkhah</forenames></author><author><keyname>Bafghi</keyname><forenames>Abbas Ghaemi</forenames></author></authors><title>A Privacy-Preserving Electronic Payment System for DRM</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of major considerations in an online business is customer privacy.
Consumers are not interested in being monitored and identified by sellers. Some
solutions are proposed to hide selection of the customer but in the payment
phase, there will be a leakage of information as online shopper can infer some
information about customer's preference due to the price, which is paid by
customer. This is a big threat to customer privacy. Our solution to this
problem consists of a number of one-unit payment steps that cannot be linked to
each other or to customer's identity. At the end of purchase, content provider
will receive appropriate amount of money while customer will acquire a valid
license anonymously. Content provider will not be able to gain any information
about the customer or the content that is purchased. In addition, a dispute
resolution scheme is presented for cases of conflict between customer and
content provider. A series of analyses on the security, complexity and DRM
requirements are presented which indicate security and practicality of our
scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6974</identifier>
 <datestamp>2014-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6974</id><created>2014-08-29</created><authors><author><keyname>Choi</keyname><forenames>Pui Tung</forenames></author><author><keyname>Lui</keyname><forenames>Lok Ming</forenames></author></authors><title>Fast Disk Conformal Parameterization of Simply-connected Open Surfaces</title><categories>cs.CG cs.CV cs.GR cs.MM math.DG</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Surface parameterizations have been widely used in computer graphics and
geometry processing. In particular, as simply-connected open surfaces are
conformally equivalent to the unit disk, it is desirable to compute the disk
conformal parameterizations of the surfaces. In this paper, we propose a novel
algorithm for the conformal parameterization of a simply-connected open surface
onto the unit disk, which significantly speeds up the computation, enhances the
conformality and stability, and guarantees the bijectivity. The conformality
distortions at the inner region and on the boundary are corrected by two steps,
with the aid of an iterative scheme using quasi-conformal theories.
Experimental results demonstrate the effectiveness of our proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6986</identifier>
 <datestamp>2014-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6986</id><created>2014-08-29</created><authors><author><keyname>Sibomana</keyname><forenames>Louis</forenames></author><author><keyname>Tran</keyname><forenames>Hung</forenames></author><author><keyname>Tran</keyname><forenames>Quang Anh</forenames></author></authors><title>Impact of Secondary User Communication on Security Communication of
  Primary User</title><categories>cs.CR cs.IT cs.NI math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, spectrum sharing has been considered as a promising solution to
improve the spectrum utilization. It however may be vulnerable to security
problems as the primary and secondary network access the same resource.
Therefore, in this paper, we focus on the performance analysis of a cognitive
radio network in the presence of an eavesdropper (EAV) who illegally listens to
the primary user (PU) communication in which the transmit power of the
secondary transmitter (SU-Tx) is subject to the joint constraint of peak
transmit power of the SU-Tx and outage probability of the PU. Accordingly, an
adaptive transmit power policy and an analytical expression of symbol error
probability are derived for the SU. Most importantly, security evaluations of
primary network in terms of the probability of existence of non-zero secrecy
capacity and outage probability of secrecy capacity are obtained. Numerical
results reveal a fact that the security of the primary network does not only
depends on the channel mean powers between primary and secondary networks, but
also strongly depends on the channel condition of the SU-Tx to EAV link and
transmit power policy of the SU-Tx.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.6988</identifier>
 <datestamp>2014-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.6988</id><created>2014-08-29</created><authors><author><keyname>Ji</keyname><forenames>Zongcheng</forenames></author><author><keyname>Lu</keyname><forenames>Zhengdong</forenames></author><author><keyname>Li</keyname><forenames>Hang</forenames></author></authors><title>An Information Retrieval Approach to Short Text Conversation</title><categories>cs.IR cs.CL</categories><comments>21 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Human computer conversation is regarded as one of the most difficult problems
in artificial intelligence. In this paper, we address one of its key
sub-problems, referred to as short text conversation, in which given a message
from human, the computer returns a reasonable response to the message. We
leverage the vast amount of short conversation data available on social media
to study the issue. We propose formalizing short text conversation as a search
problem at the first step, and employing state-of-the-art information retrieval
(IR) techniques to carry out the task. We investigate the significance as well
as the limitation of the IR approach. Our experiments demonstrate that the
retrieval-based model can make the system behave rather &quot;intelligently&quot;, when
combined with a huge repository of conversation data from social media.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.7004</identifier>
 <datestamp>2015-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.7004</id><created>2014-08-29</created><updated>2015-04-28</updated><authors><author><keyname>Bacci</keyname><forenames>Giacomo</forenames></author><author><keyname>Belmega</keyname><forenames>E. Veronica</forenames></author><author><keyname>Mertikopoulos</keyname><forenames>Panayotis</forenames></author><author><keyname>Sanguinetti</keyname><forenames>Luca</forenames></author></authors><title>Energy-Aware Competitive Power Allocation for Heterogeneous Networks
  Under QoS Constraints</title><categories>cs.IT math.IT</categories><comments>37 pages, 12 figures, to appear IEEE Trans. Wireless Commun</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work proposes a distributed power allocation scheme for maximizing
energy efficiency in the uplink of orthogonal frequency-division multiple
access (OFDMA)-based heterogeneous networks (HetNets). The user equipment (UEs)
in the network are modeled as rational agents that engage in a non-cooperative
game where each UE allocates its available transmit power over the set of
assigned subcarriers so as to maximize its individual utility (defined as the
user's throughput per Watt of transmit power) subject to minimum-rate
constraints. In this framework, the relevant solution concept is that of Debreu
equilibrium, a generalization of Nash equilibrium which accounts for the case
where an agent's set of possible actions depends on the actions of its
opponents. Since the problem at hand might not be feasible, Debreu equilibria
do not always exist. However, using techniques from fractional programming, we
provide a characterization of equilibrial power allocation profiles when they
do exist. In particular, Debreu equilibria are found to be the fixed points of
a water-filling best response operator whose water level is a function of
minimum rate constraints and circuit power. Moreover, we also describe a set of
sufficient conditions for the existence and uniqueness of Debreu equilibria
exploiting the contraction properties of the best response operator. This
analysis provides the necessary tools to derive a power allocation scheme that
steers the network to equilibrium in an iterative and distributed manner
without the need for any centralized processing. Numerical simulations are then
used to validate the analysis and assess the performance of the proposed
algorithm as a function of the system parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.7016</identifier>
 <datestamp>2014-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.7016</id><created>2014-08-29</created><authors><author><keyname>De Florio</keyname><forenames>Vincenzo</forenames></author><author><keyname>Sun</keyname><forenames>Hong</forenames></author><author><keyname>Bakhouya</keyname><forenames>Mohamed</forenames></author></authors><title>Mutualistic Relationships in Service-Oriented Communities and Fractal
  Social Organizations</title><categories>cs.CY</categories><comments>Pre-camera-ready paper to appear in the Proceedings of WCCS 2014 (2nd
  World Conference on Complex Systems)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider two social organizations -- service-oriented
communities and fractal organizations -- and discuss how their main
characteristics provide an answer to several shortcomings of traditional
organizations. In particular, we highlight their ability to tap into the vast
basins of &quot;social energy&quot; of our societies. This is done through the
establishing of mutualistic relationships among the organizational components.
The paper also introduces a mathematical model of said mutualistic processes as
well as its translation in terms of semantic service description and matching.
Preliminary investigations of the resilience of fractal social organizations
are reported. Simulations show that fractal organizations outperform
non-fractal organizations and are able to quickly recover from disruptions and
changes characterizing dynamic environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.7019</identifier>
 <datestamp>2014-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.7019</id><created>2014-08-29</created><authors><author><keyname>Ebrahimi</keyname><forenames>Javad B.</forenames></author><author><keyname>Siavoshani</keyname><forenames>Mahdi Jafari</forenames></author></authors><title>On Index Coding and Graph Homomorphism</title><categories>cs.IT cs.DM math.IT</categories><comments>5 pages, to appear in &quot;IEEE Information Theory Workshop&quot;, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we study the problem of index coding from graph homomorphism
perspective. We show that the minimum broadcast rate of an index coding problem
for different variations of the problem such as non-linear, scalar, and vector
index code, can be upper bounded by the minimum broadcast rate of another index
coding problem when there exists a homomorphism from the complement of the side
information graph of the first problem to that of the second problem. As a
result, we show that several upper bounds on scalar and vector index code
problem are special cases of one of our main theorems.
  For the linear scalar index coding problem, it has been shown in [1] that the
binary linear index of a graph is equal to a graph theoretical parameter called
minrank of the graph. For undirected graphs, in [2] it is shown that
$\mathrm{minrank}(G) = k$ if and only if there exists a homomorphism from
$\bar{G}$ to a predefined graph $\bar{G}_k$. Combining these two results, it
follows that for undirected graphs, all the digraphs with linear index of at
most k coincide with the graphs $G$ for which there exists a homomorphism from
$\bar{G}$ to $\bar{G}_k$. In this paper, we give a direct proof to this result
that works for digraphs as well.
  We show how to use this classification result to generate lower bounds on
scalar and vector index. In particular, we provide a lower bound for the scalar
index of a digraph in terms of the chromatic number of its complement.
  Using our framework, we show that by changing the field size, linear index of
a digraph can be at most increased by a factor that is independent from the
number of the nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.7033</identifier>
 <datestamp>2015-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.7033</id><created>2014-08-29</created><updated>2015-07-01</updated><authors><author><keyname>Boyar</keyname><forenames>Joan</forenames></author><author><keyname>Favrholdt</keyname><forenames>Lene M.</forenames></author><author><keyname>Kudahl</keyname><forenames>Christian</forenames></author><author><keyname>Mikkelsen</keyname><forenames>Jesper W.</forenames></author></authors><title>The Advice Complexity of a Class of Hard Online Problems</title><categories>cs.DS</categories><comments>A preliminary version of this paper appeared in the proceedings of
  the 32nd International Symposium on Theoretical Aspects of Computer Science
  (STACS 2015), Leibniz International Proceedings in Informatics 30: 116-129,
  2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The advice complexity of an online problem is a measure of how much knowledge
of the future an online algorithm needs in order to achieve a certain
competitive ratio. We determine the advice complexity of a number of hard
online problems including independent set, vertex cover, dominating set and
several others. These problems are hard, since a single wrong answer by the
online algorithm can have devastating consequences. For each of these problems,
we show that $\log\left(1+\frac{(c-1)^{c-1}}{c^{c}}\right)n=\Theta (n/c)$ bits
of advice are necessary and sufficient (up to an additive term of $O(\log n)$)
to achieve a competitive ratio of $c$.
  The results are obtained by introducing a new string guessing problem related
to those of Emek et al. (TCS 2011) and B\&quot;ockenhauer et al. (TCS 2014). It
turns out that this gives a powerful but easy-to-use method for providing both
upper and lower bounds on the advice complexity of an entire class of online
problems.
  Previous results of Halld\'orsson et al. (TCS 2002) on online independent
set, in a related model, imply that the advice complexity of the problem is
$\Theta (n/c)$. Our results improve on this by providing an exact formula for
the higher-order term. For online disjoint path allocation, B\&quot;ockenhauer et
al. (ISAAC 2009) gave a lower bound of $\Omega (n/c)$ and an upper bound of
$O((n\log c)/c)$ on the advice complexity. We improve on the upper bound by a
factor of $\log c$. For the remaining problems, no bounds on their advice
complexity were previously known.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.7035</identifier>
 <datestamp>2015-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.7035</id><created>2014-08-29</created><updated>2015-02-26</updated><authors><author><keyname>Morales-Ponce</keyname><forenames>Oscar</forenames></author><author><keyname>Schiller</keyname><forenames>Elad M.</forenames></author><author><keyname>Falcone</keyname><forenames>Paolo</forenames></author></authors><title>Cooperation with Disagreement Correction in the Presence of
  Communication Failures</title><categories>cs.DC</categories><comments>Extended version of the paper with the same name that appears in 17th
  International IEEE Conference on Intelligent Transportation Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vehicle-to-vehicle communication is a fundamental requirement in cooperative
vehicular systems to achieve high performance while keeping high safety
standards. Vehicles periodically exchange critical information with nearby
vehicles to determine their maneuvers according to the information quality and
established strategies. However, wireless communication is prone to failures.
Thus, participants can be unaware that other participants have not received the
information on time resulting in conflicting trajectories that may not be safe.
We present a deterministic solution that allows all participants to use a
default strategy when other participants have not received on time the complete
information. We base our solution on a timed distributed protocol that adapts
its output according to the effect of message omission failures so that the
disagreement period occurs for no longer than a constant time (of the order of
milliseconds) that only depends on the message delay. We formally show the
correctness and perform experiments to corroborate its efficiency. We explain
how the proposed solution can be used on vehicular platooning to attain high
performance and still guarantee high safety standards despite communication
failures. We believe that this work can facilitate the implementation of
cooperative driving systems that have to deal with inherent (communication)
uncertainties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.7039</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.7039</id><created>2014-08-29</created><updated>2014-10-11</updated><authors><author><keyname>Goldberg</keyname><forenames>Eugene</forenames></author><author><keyname>Manolios</keyname><forenames>Panagiotis</forenames></author></authors><title>Bug Hunting By Computing Range Reduction</title><categories>cs.LO</categories><comments>The only difference of this version from the previous one is in
  Section 2. We added a comparison of the performance of the CRR method and
  other model checkers on an abstract counter</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a method of model checking called Computing Range Reduction
(CRR). The CRR method is based on derivation of clauses that reduce the set of
traces of reachable states in such a way that at least one counterexample
remains (if any). These clauses are derived by a technique called Partial
Quantifier Elimination (PQE). Given a number n, the CRR method finds a
counterexample of length less or equal to n or proves that such a
counterexample does not exist. We show experimentally that a PQE-solver we
developed earlier can be efficiently applied to derivation of constraining
clauses for transition relations of realistic benchmarks.
  One of the most appealing features of the CRR method is that it can
potentially find long counterexamples. This is the area where it can beat model
checkers computing reachable states (or their approximations as in IC3) or
SAT-based methods of bounded model checking. PQE cannot be efficiently
simulated by a SAT-solver. This is important because the current research in
model checking is dominated by SAT-based algorithms. The CRR method is a
reminder that one should not put all eggs in one basket.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.7070</identifier>
 <datestamp>2014-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.7070</id><created>2014-08-29</created><authors><author><keyname>Monnerat</keyname><forenames>Luiz</forenames></author><author><keyname>Amorim</keyname><forenames>Claudio L.</forenames></author></authors><title>An effective single-hop distributed hash table with high lookup
  performance and low traffic overhead</title><categories>cs.DC</categories><comments>This is the pre-peer reviewed version of the following article: Luiz
  Monnerat and Claudio L. Amorim, An effective single-hop distributed hash
  table with high lookup performance and low traffic overhead, Concurrency and
  Computation: Practice and Experience (CCPE), 2014, which has been published
  in final form at http://onlinelibrary.wiley.com/doi/10.1002/cpe.3342/abstract</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed Hash Tables (DHTs) have been used in several applications, but
most DHTs have opted to solve lookups with multiple hops, to minimize bandwidth
costs while sacrificing lookup latency. This paper presents D1HT, an original
DHT which has a peer-to-peer and self-organizing architecture and maximizes
lookup performance with reasonable maintenance traffic, and a Quarantine
mechanism to reduce overheads caused by volatile peers. We implemented both
D1HT and a prominent single-hop DHT, and we performed an extensive and highly
representative DHT experimental comparison, followed by complementary
analytical studies. In comparison with current single-hop DHTs, our results
showed that D1HT consistently had the lowest bandwidth requirements, with
typical reductions of up to one order of magnitude, and that D1HT could be used
even in popular Internet applications with millions of users. In addition, we
ran the first latency experiments comparing DHTs to directory servers, which
revealed that D1HT can achieve latencies equivalent to or better than a
directory server, and confirmed its greater scalability properties. Overall,
our extensive set of results allowed us to conclude that D1HT can provide a
very effective solution for a broad range of environments, from large-scale
corporate datacenters to widely deployed Internet applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.7071</identifier>
 <datestamp>2014-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.7071</id><created>2014-08-29</created><authors><author><keyname>Lan</keyname><forenames>Zhenzhong</forenames></author><author><keyname>Li</keyname><forenames>Xuanchong</forenames></author><author><keyname>Hauptmann</keyname><forenames>Alexandar G.</forenames></author></authors><title>Temporal Extension of Scale Pyramid and Spatial Pyramid Matching for
  Action Recognition</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Historically, researchers in the field have spent a great deal of effort to
create image representations that have scale invariance and retain spatial
location information. This paper proposes to encode equivalent temporal
characteristics in video representations for action recognition. To achieve
temporal scale invariance, we develop a method called temporal scale pyramid
(TSP). To encode temporal information, we present and compare two methods
called temporal extension descriptor (TED) and temporal division pyramid (TDP)
. Our purpose is to suggest solutions for matching complex actions that have
large variation in velocity and appearance, which is missing from most current
action representations. The experimental results on four benchmark datasets,
UCF50, HMDB51, Hollywood2 and Olympic Sports, support our approach and
significantly outperform state-of-the-art methods. Most noticeably, we achieve
65.0% mean accuracy and 68.2% mean average precision on the challenging HMDB51
and Hollywood2 datasets which constitutes an absolute improvement over the
state-of-the-art by 7.8% and 3.9%, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.7073</identifier>
 <datestamp>2014-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.7073</id><created>2014-08-27</created><authors><author><keyname>Mansuripur</keyname><forenames>Masud</forenames></author></authors><title>DNA, Human Memory, and the Storage Technology of the 21st Century</title><categories>q-bio.OT cs.ET</categories><comments>29 pages, 26 figures, 40 references</comments><journal-ref>Proceedings of SPIE, T. Hurst and S. Kobayashi, editors, Vol.
  4342, pp 1-29 (2002)</journal-ref><doi>10.1117/12.453368</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The sophisticated tools and techniques employed by Nature for purposeful
storage of information stand in stark contrast to the primitive and relatively
inefficient means used by man. We describe some impressive features of
biological data storage, and speculate on approaches to research and
development that could benefit the storage industry in the coming decades.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.7083</identifier>
 <datestamp>2014-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.7083</id><created>2014-08-29</created><authors><author><keyname>Hanebeck</keyname><forenames>Uwe D.</forenames></author></authors><title>Truncated Moment Problem for Dirac Mixture Densities with Entropy
  Regularization</title><categories>cs.SY</categories><comments>18 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We assume that a finite set of moments of a random vector is given. Its
underlying density is unknown. An algorithm is proposed for efficiently
calculating Dirac mixture densities maintaining these moments while providing a
homogeneous coverage of the state space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.7092</identifier>
 <datestamp>2014-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.7092</id><created>2014-08-29</created><authors><author><keyname>Gandon</keyname><forenames>Fabien Lucien</forenames><affiliation>INRIA Sophia Antipolis / Laboratoire I3S</affiliation></author><author><keyname>Buffa</keyname><forenames>Michel</forenames><affiliation>INRIA Sophia Antipolis / Laboratoire I3S</affiliation></author><author><keyname>Cabrio</keyname><forenames>Elena</forenames><affiliation>INRIA Sophia Antipolis / Laboratoire I3S</affiliation></author><author><keyname>Faron-Zucker</keyname><forenames>Catherine</forenames><affiliation>INRIA Sophia Antipolis / Laboratoire I3S</affiliation></author><author><keyname>Giboin</keyname><forenames>Alain</forenames><affiliation>INRIA Sophia Antipolis / Laboratoire I3S</affiliation></author><author><keyname>Thanh</keyname><forenames>Nhan Le</forenames><affiliation>INRIA Sophia Antipolis / Laboratoire I3S</affiliation></author><author><keyname>Mirbel</keyname><forenames>Isabelle</forenames><affiliation>INRIA Sophia Antipolis / Laboratoire I3S</affiliation></author><author><keyname>Sander</keyname><forenames>Peter</forenames><affiliation>INRIA Sophia Antipolis / Laboratoire I3S</affiliation></author><author><keyname>Tettamanzi</keyname><forenames>Andrea G. B.</forenames><affiliation>INRIA Sophia Antipolis / Laboratoire I3S</affiliation></author><author><keyname>Villata</keyname><forenames>Serena</forenames><affiliation>INRIA Sophia Antipolis / Laboratoire I3S</affiliation></author></authors><title>Challenges in Bridging Social Semantics and Formal Semantics on the Web</title><categories>cs.AI cs.IR</categories><proxy>ccsd</proxy><journal-ref>5h International Conference, ICEIS 2013 190 (2013) 3-15</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes several results of Wimmics, a research lab which names
stands for: web-instrumented man-machine interactions, communities, and
semantics. The approaches introduced here rely on graph-oriented knowledge
representation, reasoning and operationalization to model and support actors,
actions and interactions in web-based epistemic communities. The re-search
results are applied to support and foster interactions in online communities
and manage their resources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.7094</identifier>
 <datestamp>2014-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.7094</id><created>2014-08-29</created><authors><author><keyname>Figueiredo</keyname><forenames>Flavio</forenames></author><author><keyname>Gon&#xe7;alves</keyname><forenames>Marcos Andr&#xe9;</forenames></author><author><keyname>Almeida</keyname><forenames>Jussara M.</forenames></author></authors><title>Improving the Effectiveness of Content Popularity Prediction Methods
  using Time Series Trends</title><categories>cs.SI physics.soc-ph</categories><comments>Presented on the ECML/PKDD Discovery Challenge on Predictive
  Analytics. Winner of two out pf three tasks of the Predictive Analytics
  Discovery Challenge</comments><acm-class>H.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We here present a simple and effective model to predict the popularity of web
content. Our solution, which is the winner of two of the three tasks of the
ECML/PKDD 2014 Predictive Analytics Challenge, aims at predicting user
engagement metrics, such as number of visits and social network engagement,
that a web page will achieve 48 hours after its upload, using only information
available in the first hour after upload. Our model is based on two steps. We
first use time series clustering techniques to extract common temporal trends
of content popularity. Next, we use linear regression models, exploiting as
predictors both content features (e.g., numbers of visits and mentions on
online social networks) and metrics that capture the distance between the
popularity time series to the trends extracted in the first step. We discuss
why this model is effective and show its gains over state of the art
alternatives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1408.7114</identifier>
 <datestamp>2014-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1408.7114</id><created>2014-08-29</created><authors><author><keyname>Hupkens</keyname><forenames>Iris</forenames></author><author><keyname>Emmerich</keyname><forenames>Michael</forenames></author><author><keyname>Deutz</keyname><forenames>Andr&#xe9;</forenames></author></authors><title>Faster Computation of Expected Hypervolume Improvement</title><categories>cs.DS</categories><comments>LIACS Technical Report</comments><acm-class>G.1.6; F.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The expected improvement algorithm (or efficient global optimization) aims
for global continuous optimization with a limited budget of black-box function
evaluations. It is based on a statistical model of the function learned from
previous evaluations and an infill criterion - the expected improvement - used
to find a promising point for a new evaluation. The `expected improvement'
infill criterion takes into account the mean and variance of a predictive
multivariate Gaussian distribution.
  The expected improvement algorithm has recently been generalized to
multiobjective optimization. In order to measure the improvement of a Pareto
front quantitatively the gain in dominated (hyper-)volume is used. The
computation of the expected hypervolume improvement (EHVI) is a
multidimensional integration of a step-wise defined non-linear function related
to the Gaussian probability density function over an intersection of boxes.
This paper provides a new algorithm for the exact computation of the expected
improvement to more than two objective functions. For the bicriteria case it
has a time complexity in $O(n^2)$ with $n$ denoting the number of points in the
current best Pareto front approximation. It improves previously known
algorithms with time complexity $O(n^3 \log n)$. For tricriteria optimization
we devise an algorithm with time complexity of $O(n^3)$. Besides discussing the
new time complexity bounds the speed of the new algorithm is also tested
empirically on test data. It is shown that further improvements in speed can be
achieved by reusing data structures built up in previous iterations. The
resulting numerical algorithms can be readily used in existing implementations
of hypervolume-based expected improvement algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0031</identifier>
 <datestamp>2014-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0031</id><created>2014-08-29</created><updated>2014-09-03</updated><authors><author><keyname>Hall</keyname><forenames>Eric C.</forenames></author><author><keyname>Willett</keyname><forenames>Rebecca M.</forenames></author></authors><title>Tracking Dynamic Point Processes on Networks</title><categories>stat.ML cs.IT cs.SI math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cascading chains of events are a salient feature of many real-world social,
biological, and financial networks. In social networks, social reciprocity
accounts for retaliations in gang interactions, proxy wars in nation-state
conflicts, or Internet memes shared via social media. Neuron spikes stimulate
or inhibit spike activity in other neurons. Stock market shocks can trigger a
contagion of volatility throughout a financial network. In these and other
examples, only individual events associated with network nodes are observed,
usually without knowledge of the underlying dynamic relationships between
nodes. This paper addresses the challenge of tracking how events within such
networks stimulate or influence future events. The proposed approach is an
online learning framework well-suited to streaming data, using a multivariate
Hawkes point process model to encapsulate autoregressive features of observed
events within the social network. Recent work on online learning in dynamic
environments is leveraged not only to exploit the dynamics within the
underlying network, but also to track that network structure as it evolves.
Regret bounds and experimental results demonstrate that the proposed method
performs nearly as well as an oracle or batch algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0034</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0034</id><created>2014-08-29</created><updated>2015-08-31</updated><authors><author><keyname>Chiesa</keyname><forenames>Marco</forenames></author><author><keyname>Gurtov</keyname><forenames>Andrei</forenames></author><author><keyname>M&#x105;dry</keyname><forenames>Aleksander</forenames></author><author><keyname>Mitrovi&#x107;</keyname><forenames>Slobodan</forenames></author><author><keyname>Nikolaevkiy</keyname><forenames>Ilya</forenames></author><author><keyname>Panda</keyname><forenames>Aurojit</forenames></author><author><keyname>Schapira</keyname><forenames>Michael</forenames></author><author><keyname>Shenker</keyname><forenames>Scott</forenames></author></authors><title>Exploring the Limits of Static Failover Routing</title><categories>cs.NI</categories><comments>28 pages</comments><acm-class>C.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present and study the Static-Routing-Resiliency problem, motivated by
routing on the Internet: Given a graph $G$, a unique destination vertex $d$,
and an integer constant $c&gt;0$, does there exist a static and destination-based
routing scheme such that the correct delivery of packets from any source $s$ to
the destination $d$ is guaranteed so long as (1) no more than $c$ edges fail
and (2) there exists a physical path from $s$ to $d$? We embark upon a
systematic exploration of this fundamental question in a variety of models
(deterministic routing, randomized routing, with packet-duplication, with
packet-header-rewriting) and present both positive and negative results that
relate the edge-connectivity of a graph, i.e., the minimum number of edges
whose deletion partitions $G$, to its resiliency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0035</identifier>
 <datestamp>2014-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0035</id><created>2014-08-29</created><authors><author><keyname>Cohen</keyname><forenames>Edith</forenames></author><author><keyname>Delling</keyname><forenames>Daniel</forenames></author><author><keyname>Pajor</keyname><forenames>Thomas</forenames></author><author><keyname>Werneck</keyname><forenames>Renato F.</forenames></author></authors><title>Computing Classic Closeness Centrality, at Scale</title><categories>cs.DS</categories><comments>13 pages, 2 figures, appeared at the 2nd ACM Conference on Online
  Social Networks (COSN'14)</comments><acm-class>G.2.2; H.2.8</acm-class><doi>10.1145/2660460.2660465</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Closeness centrality, first considered by Bavelas (1948), is an importance
measure of a node in a network which is based on the distances from the node to
all other nodes. The classic definition, proposed by Bavelas (1950), Beauchamp
(1965), and Sabidussi (1966), is (the inverse of) the average distance to all
other nodes.
  We propose the first highly scalable (near linear-time processing and linear
space overhead) algorithm for estimating, within a small relative error, the
classic closeness centralities of all nodes in the graph. Our algorithm applies
to undirected graphs, as well as for centrality computed with respect to
round-trip distances in directed graphs.
  For directed graphs, we also propose an efficient algorithm that approximates
generalizations of classic closeness centrality to outbound and inbound
centralities. Although it does not provide worst-case theoretical approximation
guarantees, it is designed to perform well on real networks.
  We perform extensive experiments on large networks, demonstrating high
scalability and accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0052</identifier>
 <datestamp>2014-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0052</id><created>2014-08-29</created><authors><author><keyname>Alamoudi</keyname><forenames>Arwa</forenames></author><author><keyname>Alomar</keyname><forenames>Noura</forenames></author><author><keyname>Alabdulrahman</keyname><forenames>Rawan</forenames></author><author><keyname>Alkoblan</keyname><forenames>Sarah</forenames></author><author><keyname>Alrashed</keyname><forenames>Wea'am</forenames></author></authors><title>Usability Engineering of Games: A Comparative Analysis of Measuring
  Excitement Using Sensors, Direct Observations and Self-Reported Data</title><categories>cs.HC cs.SE</categories><comments>Wearable technology, Usability testing, Affective computing, Q
  Sensor, Xbox Kinect, International Journal Of UbiComp (IJU), July 2014,
  Volume 5, Number 3</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Usability engineering and usability testing are concepts that continue to
evolve. Interesting research studies and new ideas come up every now and then.
This paper tests the hypothesis of using an EDA based physiological
measurements as a usability testing tool by considering three measures which
are observers opinions, self reported data and EDA based physiological sensor
data. These data were analyzed comparatively and statistically. It concludes by
discussing the findings that has been obtained from those subjective and
objective measures, which partially supports the hypothesis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0060</identifier>
 <datestamp>2014-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0060</id><created>2014-08-29</created><authors><author><keyname>Isa</keyname><forenames>Mohd Anuar Mat</forenames></author><author><keyname>Hashim</keyname><forenames>Habibah</forenames></author><author><keyname>Adnan</keyname><forenames>Syed Farid Syed</forenames></author><author><keyname>Manan</keyname><forenames>Jamalul-lail Ab</forenames></author><author><keyname>Mahmod</keyname><forenames>Ramlan</forenames></author></authors><title>A Secure TFTP Protocol with Security Proofs</title><categories>cs.CR</categories><comments>Proceedings of the World Congress on Engineering 2014 Vol I, WCE 2014</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Advances in smart devices has witnessed major developments in many mobile
applications such as Android applications. These smart devices normally
interconnect to the internet using wireless technology and applications using
the TFTP protocol among these wireless devices are becoming commonplace. In
this work, we present an enhanced lightweight security protocol for smart
device and server communications using Trivial File Transfer Protocol (TFTP).
We suggest the use of lightweight symmetric encryption for data encryption and
asymmetric encryption for key exchange protocols in TFTP. The target
implementation of secure TFTP is for embedded devices such as Wi-Fi Access
Points (AP) and remote Base Stations (BS). In this paper we present the
security proofs based on an attack model (IND-CCA2) for securing TFTP protocol.
We also present the security reduction of SSW-ARQ protocol from Cramer-Shoup
encryption scheme and fixed-time side channel security. We have also introduced
a novel adversary model in IND-CCA2-(SC-TA) and it is considered a practical
model because the model incorporates the timing attack.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0065</identifier>
 <datestamp>2014-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0065</id><created>2014-08-29</created><authors><author><keyname>Isa</keyname><forenames>Mohd Anuar Mat</forenames></author><author><keyname>Hashim</keyname><forenames>Habibah</forenames></author><author><keyname>Manan</keyname><forenames>Jamalul-lail Ab</forenames></author><author><keyname>Adnan</keyname><forenames>Syed Farid Syed</forenames></author><author><keyname>Mahmod</keyname><forenames>Ramlan</forenames></author></authors><title>An Experimental Study of Cryptography Capability using Chained Key
  Exchange Scheme for Embedded Devices</title><categories>cs.CR</categories><comments>Proceedings of the World Congress on Engineering 2014 Vol I, WCE 2014</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  After 38 years of birthday Diffie-Hellman Key Exchange (DHKE), there are many
proposed improvements in the DHKE protocol to encounter modern security issues.
This protocol seems quite simple to be implemented, but it can be vulnerable to
many types of attacks. In this work, we propose the Chained Key Exchange scheme
as a case study to explore cryptographic computation capability of embedded
microcontroller. We choose ARM RaspberryPi board as hardware platform for
experimental setup. To enable RasberberryPi system on chip (SoC) to perform
cryptographic computation, we modified the GNU GMP Bignum library to support a
simple primitive cryptographic computation in the UBOOT firmware. The main
purpose of our study is to determine whether there is any gap between
cryptographic protocol-scheme (in term of theoretical) and its engineering
implementation. Our scheme will be integrated with Trivial File Transfer
Protocol (TFTP) application in the UBOOT firmware. Our proposed scheme in the
TFTP protocol will secure the sharing of secrets and symmetric keys (e.g.,
AES256). After that, the symmetric encryption algorithm can be used to encrypt
data in the cases of remote system updates, patching and upgrades (e.g.,
firmware, kernel or application).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0069</identifier>
 <datestamp>2014-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0069</id><created>2014-08-29</created><authors><author><keyname>Zidi</keyname><forenames>Amir</forenames></author><author><keyname>Bouhana</keyname><forenames>Amna</forenames></author><author><keyname>Fekih</keyname><forenames>Afef</forenames></author><author><keyname>Abed</keyname><forenames>Mourad</forenames></author></authors><title>Personalization of Itineraries search using Ontology and Rules to Avoid
  Congestion in Urban Areas</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is a relatively small amount of research covering urban freight
movements. Most research dealing with the subject of urban mobility focuses on
passenger vehicles, not commercial vehicles hauling freight. However, in many
ways, urban freight transport contributes to congestion, air pollution, noise,
accident and more fuel consumption which raises logistic costs, and hence the
price of products. The main focus of this paper is to propose a new solution
for congestion in order to improve the distribution process of goods in urban
areas and optimize transportation cost, time of delivery, fuel consumption, and
environmental impact, while guaranteeing the safety of goods and passengers. A
novel technique for personalization in itinerary search based on city logistics
ontology and rules is proposed to overcome this problem. The integration of
personalization plays a key role in capturing or inferring the needs of each
stakeholder (user), and then satisfying these needs in a given context. The
proposed approach is implemented to an itinerary search problem for freight
transportation in urban areas to demonstrate its ability in facilitating
intelligent decision support by retrieving the best itinerary that satisfies
the most users preferences (stakeholders).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0072</identifier>
 <datestamp>2014-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0072</id><created>2014-08-29</created><updated>2014-12-07</updated><authors><author><keyname>Yuan</keyname><forenames>Ye</forenames></author><author><keyname>Glover</keyname><forenames>Keith</forenames></author><author><keyname>Goncalvees</keyname><forenames>Jorge</forenames></author></authors><title>On minimal realisations of dynamical structure functions</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by the fact that transfer functions do not contain structural
information about networks, dynamical structure functions were introduced to
capture causal relationships between measured nodes in networks. From the
dynamical structure functions, a) we show that the actual number of hidden
states can be larger than the number of hidden states estimated from the
corresponding transfer function; b) we can obtain partial information about the
true state-space equation, which cannot in general be obtained from the
transfer function. Based on these properties, this paper proposes algorithms to
find minimal realisations for a given dynamical structure function. This helps
to estimate the minimal number of hidden states, to better understand the
complexity of the network, and to identify potential targets for new
measurements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0079</identifier>
 <datestamp>2014-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0079</id><created>2014-08-30</created><authors><author><keyname>Yang</keyname><forenames>Mao</forenames></author><author><keyname>Li</keyname><forenames>Yong</forenames></author><author><keyname>Jin</keyname><forenames>Depeng</forenames></author><author><keyname>Zeng</keyname><forenames>Lieguang</forenames></author><author><keyname>Wu</keyname><forenames>Xin</forenames></author><author><keyname>Vasilakos</keyname><forenames>Athanasios V.</forenames></author></authors><title>Software-Defined and Virtualized Future Mobile and Wireless Networks: A
  Survey</title><categories>cs.NI</categories><comments>12 pages, 3 figures, submitted to &quot;Mobile Networks and Applications&quot;
  (MONET)</comments><report-no>MONE-D-14-00029R1</report-no><doi>10.1007/s11036-014-0533-8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the proliferation of mobile demands and increasingly multifarious
services and applications, mobile Internet has been an irreversible trend.
Unfortunately, the current mobile and wireless network (MWN) faces a series of
pressing challenges caused by the inherent design. In this paper, we extend two
latest and promising innovations of Internet, software-defined networking and
network virtualization, to mobile and wireless scenarios. We first describe the
challenges and expectations of MWN, and analyze the opportunities provided by
the software-defined wireless network (SDWN) and wireless network
virtualization (WNV). Then, this paper focuses on SDWN and WNV by presenting
the main ideas, advantages, ongoing researches and key technologies, and open
issues respectively. Moreover, we interpret that these two technologies highly
complement each other, and further investigate efficient joint design between
them. This paper confirms that SDWN and WNV may efficiently address the crucial
challenges of MWN and significantly benefit the future mobile and wireless
network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0080</identifier>
 <datestamp>2015-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0080</id><created>2014-08-30</created><updated>2015-08-25</updated><authors><author><keyname>Lu</keyname><forenames>Wei</forenames></author><author><keyname>Chen</keyname><forenames>Shanshan</forenames></author><author><keyname>Li</keyname><forenames>Keqian</forenames></author><author><keyname>Lakshmanan</keyname><forenames>Laks V. S.</forenames></author></authors><title>Show Me the Money: Dynamic Recommendations for Revenue Maximization</title><categories>cs.DB cs.GT cs.IR</categories><comments>Conference version published in PVLDB 7(14). To be presented in the
  VLDB Conference 2015, in Hawaii. This version gives a detailed submodularity
  proof</comments><acm-class>H.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recommender Systems (RS) play a vital role in applications such as e-commerce
and on-demand content streaming. Research on RS has mainly focused on the
customer perspective, i.e., accurate prediction of user preferences and
maximization of user utilities. As a result, most existing techniques are not
explicitly built for revenue maximization, the primary business goal of
enterprises. In this work, we explore and exploit a novel connection between RS
and the profitability of a business. As recommendations can be seen as an
information channel between a business and its customers, it is interesting and
important to investigate how to make strategic dynamic recommendations leading
to maximum possible revenue. To this end, we propose a novel \model that takes
into account a variety of factors including prices, valuations, saturation
effects, and competition amongst products. Under this model, we study the
problem of finding revenue-maximizing recommendation strategies over a finite
time horizon. We show that this problem is NP-hard, but approximation
guarantees can be obtained for a slightly relaxed version, by establishing an
elegant connection to matroid theory. Given the prohibitively high complexity
of the approximation algorithm, we also design intelligent heuristics for the
original problem. Finally, we conduct extensive experiments on two real and
synthetic datasets and demonstrate the efficiency, scalability, and
effectiveness our algorithms, and that they significantly outperform several
intuitive baselines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0081</identifier>
 <datestamp>2014-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0081</id><created>2014-08-30</created><authors><author><keyname>Aichholzer</keyname><forenames>Oswin</forenames></author><author><keyname>Fabila-Monroy</keyname><forenames>Ruy</forenames></author><author><keyname>Gonz&#xe1;lez-Aguilar</keyname><forenames>Hern&#xe1;n</forenames></author><author><keyname>Hackl</keyname><forenames>Thomas</forenames></author><author><keyname>Heredia</keyname><forenames>Marco A.</forenames></author><author><keyname>Huemer</keyname><forenames>Clemens</forenames></author><author><keyname>Urrutia</keyname><forenames>Jorge</forenames></author><author><keyname>Valtr</keyname><forenames>Pavel</forenames></author><author><keyname>Vogtenhuber</keyname><forenames>Birgit</forenames></author></authors><title>On $k$-Gons and $k$-Holes in Point Sets</title><categories>cs.DM cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a variation of the classical Erd\H{o}s-Szekeres problems on the
existence and number of convex $k$-gons and $k$-holes (empty $k$-gons) in a set
of $n$ points in the plane. Allowing the $k$-gons to be non-convex, we show
bounds and structural results on maximizing and minimizing their numbers. Most
noteworthy, for any $k$ and sufficiently large $n$, we give a quadratic lower
bound for the number of $k$-holes, and show that this number is maximized by
sets in convex position.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0083</identifier>
 <datestamp>2014-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0083</id><created>2014-08-30</created><authors><author><keyname>Harandi</keyname><forenames>Mehrtash</forenames></author><author><keyname>Hartley</keyname><forenames>Richard</forenames></author><author><keyname>Lovell</keyname><forenames>Brian</forenames></author><author><keyname>Sanderson</keyname><forenames>Conrad</forenames></author></authors><title>Sparse Coding on Symmetric Positive Definite Manifolds using Bregman
  Divergences</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces sparse coding and dictionary learning for Symmetric
Positive Definite (SPD) matrices, which are often used in machine learning,
computer vision and related areas. Unlike traditional sparse coding schemes
that work in vector spaces, in this paper we discuss how SPD matrices can be
described by sparse combination of dictionary atoms, where the atoms are also
SPD matrices. We propose to seek sparse coding by embedding the space of SPD
matrices into Hilbert spaces through two types of Bregman matrix divergences.
This not only leads to an efficient way of performing sparse coding, but also
an online and iterative scheme for dictionary learning. We apply the proposed
methods to several computer vision tasks where images are represented by region
covariance matrices. Our proposed algorithms outperform state-of-the-art
methods on a wide range of classification tasks, including face recognition,
action recognition, material classification and texture categorization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0084</identifier>
 <datestamp>2014-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0084</id><created>2014-08-30</created><authors><author><keyname>Harandi</keyname><forenames>Mehrtash</forenames></author><author><keyname>Salzmann</keyname><forenames>Mathieu</forenames></author></authors><title>Kernel Coding: General Formulation and Special Cases</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Representing images by compact codes has proven beneficial for many visual
recognition tasks. Most existing techniques, however, perform this coding step
directly in image feature space, where the distributions of the different
classes are typically entangled. In contrast, here, we study the problem of
performing coding in a high-dimensional Hilbert space, where the classes are
expected to be more easily separable. To this end, we introduce a general
coding formulation that englobes the most popular techniques, such as bag of
words, sparse coding and locality-based coding, and show how this formulation
and its special cases can be kernelized. Importantly, we address several
aspects of learning in our general formulation, such as kernel learning,
dictionary learning and supervised kernel coding. Our experimental evaluation
on several visual recognition tasks demonstrates the benefits of performing
coding in Hilbert space, and in particular of jointly learning the kernel, the
dictionary and the classifier.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0085</identifier>
 <datestamp>2014-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0085</id><created>2014-08-30</created><authors><author><keyname>Mondal</keyname><forenames>Kaushik</forenames></author><author><keyname>Karmakar</keyname><forenames>Arindam</forenames></author><author><keyname>Mandal</keyname><forenames>Partha Sarathi</forenames></author></authors><title>Designing Path Planning Algorithms for Mobile Anchor towards Range-Free
  Localization</title><categories>cs.DC cs.NI</categories><comments>17 pages</comments><msc-class>68W15</msc-class><acm-class>C.2.1; C.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Localization is one of the most important factor in wireless sensor networks
as many applications demand position information of sensors. Recently there is
an increasing interest on the use of mobile anchors for localizing sensors.
Most of the works available in the literature either looks into the aspect of
reducing path length of mobile anchor or tries to increase localization
accuracy. The challenge is to design a movement strategy for a mobile anchor
that reduces path length while meeting the requirements of a good range-free
localization technique. In this paper we propose two cost-effective movement
strategies i.e., path planning for a mobile anchor so that localization can be
done using the localization scheme \cite{Lee2009}. In one strategy we use a
hexagonal movement pattern for the mobile anchor to localize all sensors inside
a bounded rectangular region with lesser movement compared to the existing
works in literature. In other strategy we consider a connected network in an
unbounded region where the mobile anchor moves in the hexagonal pattern to
localize the sensors. In this approach, we guarantee localization of all
sensors within $r/2$ error-bound where $r$ is the communication range of the
mobile anchor and sensors. Our simulation results support theoretical results
along with localization accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0088</identifier>
 <datestamp>2015-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0088</id><created>2014-08-30</created><updated>2015-05-19</updated><authors><author><keyname>SaiToh</keyname><forenames>Akira</forenames></author></authors><title>Quantum digital-to-analog conversion algorithm using decoherence</title><categories>quant-ph cs.DS</categories><comments>22 pages, no figure, v2: minor revision, style changed, minor changes
  of proofs in section 2, v3: revision with more detailed explanations, style
  changed, supplementary material added, v4: minor revision, v5: typos
  corrected, to appear in QIP</comments><msc-class>81P68</msc-class><acm-class>C.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of mapping digital data encoded on a quantum register
to analog amplitudes in parallel. It is shown to be unlikely that a fully
unitary polynomial-time quantum algorithm exists for this problem; NP becomes a
subset of BQP if it exists. In the practical point of view, we propose a
nonunitary linear-time algorithm using quantum decoherence. It tacitly uses an
exponentially large physical resource, which is typically a huge number of
identical molecules. Quantumness of correlation appearing in the process of the
algorithm is also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0089</identifier>
 <datestamp>2014-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0089</id><created>2014-08-30</created><authors><author><keyname>Roy</keyname><forenames>Partha Sarathi</forenames></author><author><keyname>Adhikari</keyname><forenames>Avishek</forenames></author></authors><title>Multi-Use Multi-Secret Sharing Scheme for General Access Structure</title><categories>cs.CR</categories><msc-class>94A62</msc-class><journal-ref>Annals of the University of Craiova, Mathematics and Computer
  Science Series Volume 37(4), 2010, Pages 50-57</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main aim of this paper is to construct a multi-secret sharing scheme for
general access structure in a trusted dealer model using suitable hash function
and Lagrange's interpolation method. Even though, the proposed scheme is a
multi-secret and multi-use one, each participant has to carry only one share.
The suitable use of collision resistant one way hash function makes the scheme
efficient and multi-use. Moreover, the scheme has a nice property that secrets,
participants or qualified sets of participants may be added to or even may be
made inactive dynamically by the dealer to get a new access structure without
altering the shares of the existing participants in the old access structure.
Finally, in the proposed scheme, both the combiner and the share holders can
verify the correctness of the information that they are receiving from each
other.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0090</identifier>
 <datestamp>2014-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0090</id><created>2014-08-30</created><authors><author><keyname>Weber</keyname><forenames>Steven</forenames></author><author><keyname>Guerin</keyname><forenames>Roch</forenames></author></authors><title>Facilitating adoption of network services with externalities via cost
  subsidization</title><categories>cs.SI</categories><comments>10 pages, 4 figures. Submitted to IEEE Transactions on Control of
  Network Systems (TCNS). Preliminary version presented at June, 2014 W-PIN +
  NetEcon workshop in Austin, TX</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the adoption level of a network service where the net
utility perceived by each user incorporates three key features, namely, user
service affinity heterogeneity, a network externality, and a subscription cost.
Services with network externality face a &quot;chicken and egg&quot; adoption problem in
that the service requires customers in order to attract customers. In this
paper we study cost subsidization as a means to &quot;reach the knee&quot; and thereby
change the equilibrium adoption level from zero to one. By focusing on a simple
subsidy structure and a simple model for user heterogeneity, we can derive
explicit expressions for quantities of natural interest, such as the minimum
subsidy required, the minimum subsidy duration, and the aggregate cost of the
subsidy to the service provider. We show that small or large subsidies are
inefficient, but that there is a Pareto efficient frontier for &quot;intermediate&quot;
subsidies wherein subsidy duration and aggregate cost are in tension with one
another.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0093</identifier>
 <datestamp>2014-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0093</id><created>2014-08-30</created><updated>2014-09-03</updated><authors><author><keyname>F&#xf6;rster</keyname><forenames>Anna</forenames></author><author><keyname>Sommer</keyname><forenames>Christoph</forenames></author><author><keyname>Steinbach</keyname><forenames>Till</forenames></author><author><keyname>W&#xe4;hlisch</keyname><forenames>Matthias</forenames></author></authors><title>Proceedings of the 1st OMNeT++ Community Summit, Hamburg, Germany,
  September 2, 2014</title><categories>cs.PF</categories><acm-class>I.6; C.2.0; C.4; D.4.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is the Proceedings of the 1st OMNeT++ Community Summit, which was held
in Hamburg, Germany, September 2, 2014.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0098</identifier>
 <datestamp>2015-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0098</id><created>2014-08-30</created><updated>2015-07-07</updated><authors><author><keyname>Wang</keyname><forenames>Hung-Lung</forenames></author></authors><title>An optimal algorithm for the weighted backup 2-center problem on a tree</title><categories>cs.DS</categories><comments>14 pages, 4 figures</comments><msc-class>68Q25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we are concerned with the weighted backup 2-center problem on
a tree. The backup 2-center problem is a kind of center facility location
problem, in which one is asked to deploy two facilities, with a given
probability to fail, in a network. Given that the two facilities do not fail
simultaneously, the goal is to find two locations, possibly on edges, that
minimize the expected value of the maximum distance over all vertices to their
closest functioning facility. In the weighted setting, each vertex in the
network is associated with a nonnegative weight, and the distance from vertex
$u$ to $v$ is weighted by the weight of $u$. With the strategy of
prune-and-search, we propose a linear time algorithm, which is asymptotically
optimal, to solve the weighted backup 2-center problem on a tree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0104</identifier>
 <datestamp>2014-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0104</id><created>2014-08-30</created><authors><author><keyname>Bauckhage</keyname><forenames>Christian</forenames></author></authors><title>Marginalizing over the PageRank Damping Factor</title><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note, we show how to marginalize over the damping parameter of the
PageRank equation so as to obtain a parameter-free version known as TotalRank.
Our discussion is meant as a reference and intended to provide a guided tour
towards an interesting result that has applications in information retrieval
and classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0107</identifier>
 <datestamp>2014-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0107</id><created>2014-08-30</created><authors><author><keyname>Barachant</keyname><forenames>Alexandre</forenames></author><author><keyname>Congedo</keyname><forenames>Marco</forenames></author></authors><title>A Plug&amp;Play P300 BCI Using Information Geometry</title><categories>cs.LG cs.HC stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new classification methods for Event Related Potentials
(ERP) based on an Information geometry framework. Through a new estimation of
covariance matrices, this work extend the use of Riemannian geometry, which was
previously limited to SMR-based BCI, to the problem of classification of ERPs.
As compared to the state-of-the-art, this new method increases performance,
reduces the number of data needed for the calibration and features good
generalisation across sessions and subjects. This method is illustrated on data
recorded with the P300-based game brain invaders. Finally, an online and
adaptive implementation is described, where the BCI is initialized with generic
parameters derived from a database and continuously adapt to the individual,
allowing the user to play the game without any calibration while keeping a high
accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0112</identifier>
 <datestamp>2015-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0112</id><created>2014-08-30</created><updated>2015-01-09</updated><authors><author><keyname>Breiling</keyname><forenames>Marco</forenames></author><author><keyname>Ng</keyname><forenames>Derrick Wing Kwan</forenames></author><author><keyname>Rohde</keyname><forenames>Christian</forenames></author><author><keyname>Burkhardt</keyname><forenames>Frank</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author></authors><title>Resource Allocation for Outdoor-to-Indoor Multicarrier Transmission with
  Shared UE-side Distributed Antenna Systems</title><categories>cs.IT math.IT</categories><comments>accepted for publication at the IEEE Vehicular Technology Conference
  (VTC) Spring, Glasgow, Scotland, UK, May 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the resource allocation algorithm design for downlink
multicarrier transmission with a shared user equipment (UE)-side distributed
antenna system (SUDAS) which utilizes both licensed and unlicensed frequency
bands for improving the system throughput. The joint UE selection and
transceiver processing matrix design is formulated as a non-convex optimization
problem for the maximization of the end-to-end system throughput (bits/s). In
order to obtain a tractable resource allocation algorithm, we first show that
the optimal transmitter precoding and receiver post-processing matrices jointly
diagonalize the end-to-end communication channel. Subsequently, the
optimization problem is converted to a scalar optimization problem for multiple
parallel channels, which is solved by using an asymptotically optimal iterative
algorithm. Simulation results illustrate that the proposed resource allocation
algorithm for the SUDAS achieves an excellent system performance and provides a
spatial multiplexing gain for single-antenna UEs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0117</identifier>
 <datestamp>2014-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0117</id><created>2014-08-30</created><authors><author><keyname>Dorman</keyname><forenames>A. M.</forenames></author></authors><title>Computerized Multi Microphone Test System</title><categories>cs.SD</categories><comments>13 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An acoustic testing approach based on the concept of a microphone sensor
surrounding the product under test is proposed. Microphone signals are
processed simultaneously by a test system computer, according to the objective
of the test. The spatial and frequency domain selectivity features of this
method are examined. Sound-spatial visualization algorithm is observed. A test
system design based on the concept of a microphone surrounding the tested
product has the potential to improve distortion measurement accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0119</identifier>
 <datestamp>2014-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0119</id><created>2014-08-30</created><authors><author><keyname>Bondarenko</keyname><forenames>Ievgen</forenames></author></authors><title>The word problem in Hanoi Towers groups</title><categories>math.GR cs.FL</categories><msc-class>20F10, 68R05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that elements of the Hanoi Towers groups $\mathcal{H}_m$ have depth
bounded from above by a poly-logarithmic function $O(\log^{m-2} n)$, where $n$
is the length of an element. Therefore the word problem in groups
$\mathcal{H}_m$ is solvable in subexponential time $\exp(O(\log^{m-2} n))$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0121</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0121</id><created>2014-08-30</created><updated>2014-09-05</updated><authors><author><keyname>Xu</keyname><forenames>Guangwu</forenames></author></authors><title>On Solving a Generalized Chinese Remainder Theorem in the Presence of
  Remainder Errors</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In estimating frequencies given that the signal waveforms are undersampled
multiple times, Xia and his collaborators proposed to use a generalized version
of Chinese remainder Theorem (CRT), where the moduli are $dm_1, dm_2, \cdots,
dm_k$ with $m_1, m_2, \cdots, m_k$ being pairwise coprime. If the errors of the
corrupted remainders are within $\frac{d}4$, their schemes are able to
construct an approximation of the solution to the generalized CRT with an error
smaller than $\frac{d}4$. One of the critical ingredients in their approach is
the clever idea of accurately finding the quotients. In this paper, we present
two treatments of this problem. The first treatment follows the route of Wang
and Xia to find the quotients, but with a simplified process. The second
treatment takes a different approach by working on the corrupted remainders
directly. This approach also reveals some useful information about the
remainders by inspecting extreme values of the erroneous remainders modulo $d$.
Both of our treatments produce efficient algorithms with essentially optimal
performance. This paper also provides a proof of the sharpness of the error
bound $\frac{d}4$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0128</identifier>
 <datestamp>2014-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0128</id><created>2014-08-30</created><authors><author><keyname>Renkema-Padmos</keyname><forenames>Arne</forenames></author><author><keyname>Baum</keyname><forenames>Jerome</forenames></author></authors><title>Through the Frosted Glass: Security Problems in a Translucent UI</title><categories>cs.HC cs.CR</categories><comments>10 pages</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Translucency is now a common design element in at least one popular mobile
operating system. This raises security concerns as it can make it harder for
users to correctly identify and interpret trusted interaction elements. In this
paper, we demonstrate this security problem using the example of the Safari
browser in the latest iOS version on Apple tablets and phones (iOS7), and
discuss technical challenges of an attack as well as solutions to these
challenges. We conclude with a survey-based user study, where we seek to
quantify the security impact, and find that further investigation is warranted.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0129</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0129</id><created>2014-08-30</created><authors><author><keyname>Mongeon</keyname><forenames>Philippe</forenames></author><author><keyname>Lariviere</keyname><forenames>Vincent</forenames></author></authors><title>Costly Collaborations: The Impact of Scientific Fraud on Co-authors'
  Careers</title><categories>cs.DL</categories><comments>Accepted for publication in the Journal of the Association for
  Information Science and Technology</comments><journal-ref>Journal of the Association for Information Science and Technology,
  67: 535-542 (2016)</journal-ref><doi>10.1002/asi.23421</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the last few years, several major scientific fraud cases have shocked
the scientific community. The number of retractions each year has also
increased tremendously, especially in the biomedical field, and scientific
misconduct accounts for approximately more than half of those retractions. It
is assumed that co-authors of retracted papers are affected by their
colleagues' misconduct, and the aim of this study is to provide empirical
evidence of the effect of retractions in biomedical research on co-authors'
research careers. Using data from the Web of Science (WOS), we measured the
productivity, impact and collaboration of 1,123 co-authors of 293 retracted
articles for a period of five years before and after the retraction. We found
clear evidence that collaborators do suffer consequences of their colleagues'
misconduct, and that a retraction for fraud has higher consequences than a
retraction for error. Our results also suggest that the extent of these
consequences is closely linked with the ranking of co-authors on the retracted
paper, being felt most strongly by first authors, followed by the last authors,
while the impact is less important for middle authors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0153</identifier>
 <datestamp>2014-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0153</id><created>2014-08-30</created><authors><author><keyname>Gupta</keyname><forenames>Varun</forenames></author><author><keyname>Zhang</keyname><forenames>Jiheng</forenames></author></authors><title>Approximations and Optimal Control for State-dependent Limited Processor
  Sharing Queues</title><categories>cs.SY math.OC math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper studies approximations and control of a processor sharing (PS)
server where the service rate depends on the number of jobs occupying the
server. The control of such a system is implemented by imposing a limit on the
number of jobs that can share the server concurrently, with the rest of the
jobs waiting in a first-in-first-out (FIFO) buffer. A desirable control scheme
should strike the right balance between efficiency (operating at a high service
rate) and parallelism (preventing small jobs from getting stuck behind large
ones).
  We employ the framework of heavy-traffic diffusion analysis to devise near
optimal control heuristics for such a queueing system. However, while the
literature on diffusion control of state-dependent queueing systems begins with
a sequence of systems and an exogenously defined drift function, we begin with
a finite discrete PS server and propose an axiomatic recipe to explicitly
construct a sequence of state-dependent PS servers which then yields a drift
function. We establish diffusion approximations and use them to obtain
insightful and closed-form approximations for the original system under a
static concurrency limit control policy.
  We extend our study to control policies that dynamically adjust the
concurrency limit. We provide two novel numerical algorithms to solve the
associated diffusion control problem. Our algorithms can be viewed as &quot;average
cost&quot; iteration: The first algorithm uses binary-search on the average cost and
can find an $\epsilon$-optimal policy in time $O\left( \log^2
\frac{1}{\epsilon} \right)$; the second algorithm uses the Newton-Raphson
method for root-finding and requires $O\left( \log \frac{1}{\epsilon} \log\log
\frac{1}{\epsilon}\right)$ time.
  Numerical experiments demonstrate the accuracy of our approximation for
choosing optimal or near-optimal static and dynamic concurrency control
heuristics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0158</identifier>
 <datestamp>2014-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0158</id><created>2014-08-30</created><authors><author><keyname>Titovets</keyname><forenames>Alexander</forenames></author><author><keyname>Mills</keyname><forenames>Philip</forenames></author><author><keyname>Kreinovich</keyname><forenames>Vladik</forenames></author></authors><title>Computers Should Be Uniters Not Dividers: A Vision of Computer-Enhanced
  Happy Future</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This manifesto provides a vision of how computers can be used to bring people
together, to enhance people's use of their natural creativity, and thus, make
them happier.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0166</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0166</id><created>2014-08-30</created><updated>2015-12-21</updated><authors><author><keyname>Kop</keyname><forenames>Cynthia</forenames></author><author><keyname>Nishida</keyname><forenames>Naoki</forenames></author><author><keyname>Fuhs</keyname><forenames>Carsten</forenames></author></authors><title>Verifying Procedural Programs via Constrained Rewriting Induction</title><categories>cs.LO</categories><acm-class>D.2.4; I.2.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper aims at developing a verification method for procedural programs
via a transformation into the recently introduced Logically Constrained Term
Rewriting Systems (LCTRSs). To this end, we introduce an extension of
transformation methods based on integer TRSs, which can also handle global
variables and arrays as well as encode safety checks. Then we adapt existing
rewriting induction methods to LCTRSs and propose a simple yet effective method
to generalize equations. We show that we can automatically verify memory safety
and prove correctness of realistic functions, involving for instance integers
and arrays. Our approach proves equivalence between two implementations, so in
contrast to other works, we do not require an explicit specification in a
separate specification language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0169</identifier>
 <datestamp>2015-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0169</id><created>2014-08-30</created><updated>2015-08-04</updated><authors><author><keyname>Bond</keyname><forenames>Benjamin</forenames></author><author><keyname>Levine</keyname><forenames>Lionel</forenames></author></authors><title>Abelian networks II. Halting on all inputs</title><categories>cs.FL cond-mat.stat-mech math.CO</categories><comments>Supersedes sections 5 and 6 of arXiv:1309.3445v1. To appear in
  Selecta Mathematica</comments><msc-class>68Q10, 37B15, 20M14, 20M35, 05C50</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Abelian networks are systems of communicating automata satisfying a local
commutativity condition. We show that a finite irreducible abelian network
halts on all inputs if and only if all eigenvalues of its production matrix lie
in the open unit disk.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0170</identifier>
 <datestamp>2015-11-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0170</id><created>2014-08-30</created><updated>2015-10-31</updated><authors><author><keyname>Bond</keyname><forenames>Benjamin</forenames></author><author><keyname>Levine</keyname><forenames>Lionel</forenames></author></authors><title>Abelian networks III. The critical group</title><categories>cs.FL cond-mat.stat-mech math.CO</categories><comments>supersedes sections 7 and 8 of arXiv:1309.3445v1. To appear in the
  Journal of Algebraic Combinatorics</comments><msc-class>68Q10, 37B15, 20M14, 20M35, 05C50</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The critical group of an abelian network is a finite abelian group that
governs the behavior of the network on large inputs. It generalizes the
sandpile group of a graph. We show that the critical group of an irreducible
abelian network acts freely and transitively on recurrent states of the
network. We exhibit the critical group as a quotient of a free abelian group by
a subgroup containing the image of the Laplacian, with equality in the case
that the network is rectangular. We generalize Dhar's burning algorithm to
abelian networks, and estimate the running time of an abelian network on an
arbitrary input up to a constant additive error.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0171</identifier>
 <datestamp>2014-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0171</id><created>2014-08-30</created><authors><author><keyname>Mayr</keyname><forenames>Christian</forenames></author><author><keyname>Schultz</keyname><forenames>Michael</forenames></author><author><keyname>Noack</keyname><forenames>Marko</forenames></author><author><keyname>Henker</keyname><forenames>Stephan</forenames></author><author><keyname>Partzsch</keyname><forenames>Johannes</forenames></author><author><keyname>Sch&#xfc;ffny</keyname><forenames>Rene</forenames></author></authors><title>OTA based 200 G{\Omega} resistance on 700 {\mu}m2 in 180 nm CMOS for
  neuromorphic applications</title><categories>q-bio.NC cs.ET</categories><comments>8 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Generating an exponential decay function with a time constant on the order of
hundreds of milliseconds is a mainstay for neuromorphic circuits. Usually,
either subthreshold circuits or RC-decays based on transconductance amplifiers
are used. In the latter case, transconductances in the 10 pS range are needed.
However, state-of-the-art low-transconductance amplifiers still require too
much circuit area to be applicable in neuromorphic circuits where &gt;100 of these
time constant circuits may be required on a single chip. We present a silicon
verified operational transconductance amplifier that achieves a gm of 5 pS in
only 700 {\mu}m2, a factor of 10-100 less area than current examples. This
allows a high-density integration of time constant circuits in target
appliations such as synaptic learning or as driving circuit for neuromorphic
memristor arrays.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0173</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0173</id><created>2014-08-30</created><updated>2014-09-28</updated><authors><author><keyname>Bandyapadhyay</keyname><forenames>Sayan</forenames></author></authors><title>A Variant of the Maximum Weight Independent Set Problem</title><categories>cs.DS cs.CG cs.DM</categories><comments>18 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a natural extension of the Maximum Weight Independent Set Problem
(MWIS), one of the most studied optimization problems in Graph algorithms. We
are given a graph $G=(V,E)$, a weight function $w: V \rightarrow \mathbb{R^+}$,
a budget function $b: V \rightarrow \mathbb{Z^+}$, and a positive integer $B$.
The weight (resp. budget) of a subset of vertices is the sum of weights (resp.
budgets) of the vertices in the subset. A $k$-budgeted independent set in $G$
is a subset of vertices, such that no pair of vertices in that subset are
adjacent, and the budget of the subset is at most $k$. The goal is to find a
$B$-budgeted independent set in $G$ such that its weight is maximum among all
the $B$-budgeted independent sets in $G$. We refer to this problem as MWBIS.
Being a generalization of MWIS, MWBIS also has several applications in
Scheduling, Wireless networks and so on. Due to the hardness results implied
from MWIS, we study the MWBIS problem in several special classes of graphs. We
design exact algorithms for trees, forests, cycle graphs, and interval graphs.
In unweighted case we design an approximation algorithm for $d+1$-claw free
graphs whose approximation ratio ($d$) is competitive with the approximation
ratio ($\frac{d}{2}$) of MWIS (unweighted). Furthermore, we extend Baker's
technique \cite{Baker83} to get a PTAS for MWBIS in planar graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0177</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0177</id><created>2014-08-30</created><updated>2015-03-09</updated><authors><author><keyname>Chung</keyname><forenames>Moo K.</forenames></author><author><keyname>Hanson</keyname><forenames>Jamie L.</forenames></author><author><keyname>Ye</keyname><forenames>Jieping</forenames></author><author><keyname>Davidson</keyname><forenames>Richard J.</forenames></author><author><keyname>Pollak</keyname><forenames>Seth D.</forenames></author></authors><title>Persistent Homology in Sparse Regression and Its Application to Brain
  Morphometry</title><categories>stat.ME cs.CV</categories><comments>submitted to IEEE Transactions on Medical Imaging</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse systems are usually parameterized by a tuning parameter that
determines the sparsity of the system. How to choose the right tuning parameter
is a fundamental and difficult problem in learning the sparse system. In this
paper, by treating the the tuning parameter as an additional dimension,
persistent homological structures over the parameter space is introduced and
explored. The structures are then further exploited in speeding up the
computation using the proposed soft-thresholding technique. The topological
structures are further used as multivariate features in the tensor-based
morphometry (TBM) in characterizing white matter alterations in children who
have experienced severe early life stress and maltreatment. These analyses
reveal that stress-exposed children exhibit more diffuse anatomical
organization across the whole white matter region.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0191</identifier>
 <datestamp>2014-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0191</id><created>2014-08-31</created><authors><author><keyname>Xie</keyname><forenames>Dong</forenames></author><author><keyname>Wang</keyname><forenames>An Min</forenames></author></authors><title>Continuous quantum measurement in spin environments</title><categories>quant-ph cs.IT math.IT</categories><comments>5pages,5 figures. arXiv admin note: text overlap with arXiv:1307.2101
  by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We derive a formalism of stochastic master equations (SME) which describes
the decoherence dynamics of a system in spin environments conditioned on the
measurement record. Markovian and non-Markovian nature of environment can be
revealed by a spectroscopy method based on weak quantum measurement (weak
spectroscopy). On account of that correlated environments can lead to a
nonlocal open system which exhibits strong non-Markovian effects although the
local dynamics are Markovian, the spectroscopy method can be used to
demonstrate that there is correlation between two environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0194</identifier>
 <datestamp>2015-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0194</id><created>2014-08-31</created><updated>2015-12-16</updated><authors><author><keyname>Garola</keyname><forenames>Claudio</forenames></author></authors><title>A Pragmatic Interpretation of Quantum Logic</title><categories>quant-ph cs.LO</categories><comments>Third version: 20 pages. Sects. 1, 2, and 4 rewritten and improved.
  Explanations added</comments><msc-class>81P05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scholars have wondered for a long time whether the language of quantum
mechanics introduces a quantum notion of truth which is formalized by quantum
logic (QL) and is incompatible with the classical (Tarskian) notion. We show
that QL can be interpreted as a pragmatic language of assertive formulas which
formalize statements about physical systems that are empirically justified or
unjustified in the framework of quantum mechanics. According to this
interpretation, QL formalizes properties of the metalinguistic notion of
empirical justification within quantum mechanics rather than properties of a
quantum notion of truth. This conclusion agrees with a general integrationist
perspective that interprets nonstandard logics as theories of metalinguistic
notions different from truth, thus avoiding incompatibility with classical
notions and preserving the globality of logic. By the way, some elucidations of
the standard notion of quantum truth are also obtained.
  Key words: pragmatics, quantum logic, quantum mechanics, justifiability,
global pluralism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0201</identifier>
 <datestamp>2014-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0201</id><created>2014-08-31</created><updated>2014-09-17</updated><authors><author><keyname>Ghari</keyname><forenames>Pouya Mollaebrahim</forenames></author><author><keyname>Shahbazian</keyname><forenames>Reza</forenames></author><author><keyname>Ghorashi</keyname><forenames>Seyed Ali</forenames></author></authors><title>Localization in Wireless Sensor Networks Using Quadratic Optimization</title><categories>math.OC cs.IT cs.NI math.IT</categories><comments>11 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The localization problem in a wireless sensor network is to determine the
coordination of sensor nodes using the known positions of some nodes (called
anchors) and corresponding noisy distance measurements. There is a variety of
different approaches to solve this problem such as semi-definite programming
(SDP) based, sum of squares and second order cone programming, and between
them, SDP-based approaches have shown good performance. In recent years, the
primary SDP approach has been investigated and a variety of approaches are
proposed in order to enhance its performance. In SDP approaches, errors in
approximating the given distances are minimized as an objective function. It is
desirable that the distribution of error in these problems would be a delta
distribution, which is practically impossible. Therefore, we may approximate
delta distribution by Gaussian distribution with very small variance. In this
paper, we define a new objective function which makes the error distribution as
similar as possible to a Gaussian distribution with a very small variance.
Simulation results show that our proposed method has higher accuracy compared
to the traditional SDP approach and other prevalent objective functions which
are used such as least squares. Our method is also faster than other popular
approaches which try to improve the accuracy of the primary SDP approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0203</identifier>
 <datestamp>2014-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0203</id><created>2014-08-31</created><authors><author><keyname>Taghizadeh</keyname><forenames>Mohammad J.</forenames></author><author><keyname>Parhizkar</keyname><forenames>Reza</forenames></author><author><keyname>Garner</keyname><forenames>Philip N.</forenames></author><author><keyname>Bourlard</keyname><forenames>Herve</forenames></author><author><keyname>Asaei</keyname><forenames>Afsaneh</forenames></author></authors><title>Ad Hoc Microphone Array Calibration: Euclidean Distance Matrix
  Completion Algorithm and Theoretical Guarantees</title><categories>cs.SD cs.LG</categories><comments>In Press, available online, August 1, 2014.
  http://www.sciencedirect.com/science/article/pii/S0165168414003508, Signal
  Processing, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of ad hoc microphone array calibration where
only partial information about the distances between microphones is available.
We construct a matrix consisting of the pairwise distances and propose to
estimate the missing entries based on a novel Euclidean distance matrix
completion algorithm by alternative low-rank matrix completion and projection
onto the Euclidean distance space. This approach confines the recovered matrix
to the EDM cone at each iteration of the matrix completion algorithm. The
theoretical guarantees of the calibration performance are obtained considering
the random and locally structured missing entries as well as the measurement
noise on the known distances. This study elucidates the links between the
calibration error and the number of microphones along with the noise level and
the ratio of missing distances. Thorough experiments on real data recordings
and simulated setups are conducted to demonstrate these theoretical insights. A
significant improvement is achieved by the proposed Euclidean distance matrix
completion algorithm over the state-of-the-art techniques for ad hoc microphone
array calibration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0205</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0205</id><created>2014-08-31</created><updated>2015-12-29</updated><authors><author><keyname>Wu</keyname><forenames>Tao</forenames></author><author><keyname>Guo</keyname><forenames>Yuxiao</forenames></author><author><keyname>Chen</keyname><forenames>LeiTing</forenames></author><author><keyname>Liu</keyname><forenames>YanBing</forenames></author></authors><title>Integrated structure investigation in complex networks by label
  propagation</title><categories>cs.SI physics.soc-ph</categories><comments>22 pages, 11 figures, 7 tables. arXiv admin note: text overlap with
  arXiv:physics/0607100 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The investigation of network structure has important significance to
understand the functions of various complex networks. The communities with
hierarchical and overlapping structures and the special nodes like hubs and
outliers are all common structure features to the networks. Network structure
investigation has attracted considerable research effort recently. However,
existing studies have only partially explored the structure features. In this
paper, a label propagation based integrated network structure investigation
algorithm (LINSIA) is proposed. The main novelty here is that LINSIA can
uncover hierarchical and overlapping communities, as well as hubs and outliers.
Moreover, LINSIA can provide insight into the label propagation mechanism and
propose a parameter-free solution that requires no prior knowledge. In
addition, LINSIA can give out a soft-partitioning result and depict the degree
of overlapping nodes belonging to each relevant community. The proposed
algorithm is validated on various synthetic and real-world networks.
Experimental results demonstrate that the algorithm outperforms several
state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0206</identifier>
 <datestamp>2014-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0206</id><created>2014-08-31</created><authors><author><keyname>Tavassoli</keyname><forenames>Babak</forenames></author></authors><title>A Computational Approach to Bisimulation of Hybrid Dynamical Systems</title><categories>cs.SY</categories><comments>10 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of finding a finite state symbolic model which is bisimilar to a
hybrid dynamical system (HDS) and has the minimum number of states is
considered. The considered class of HDS allows for discrete-valued inputs that
only affect the jumps (events) of the HDS. Representation of the HDS in the
form of a transition system is revisited in comparison with prior works. An
algorithm is proposed for solving the problem which gives the bisimulation with
the minimum number of states if it already exists and also a parameter of the
algorithm is properly tuned. There is no need for stability assumptions and no
time discretization is applied. The results are applied to an example
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0210</identifier>
 <datestamp>2014-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0210</id><created>2014-08-31</created><updated>2014-09-04</updated><authors><author><keyname>Zhang</keyname><forenames>Leihan</forenames></author><author><keyname>Zhao</keyname><forenames>Jichang</forenames></author><author><keyname>Xu</keyname><forenames>Ke</forenames></author></authors><title>Who creates trends in online social media: The crowd or opinion leaders?</title><categories>cs.SI cs.CY physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Trends in online social media always reflect the collective attention of a
vast number of individuals across the network. For example, Internet slang
words can be ubiquitous because of social memes and online contagions in an
extremely short period. From Weibo, a Twitter-like service in China, we find
that the adoption of popular Internet slang words experiences two peaks in its
temporal evolution, in which the former is relatively much lower than the
latter. This interesting phenomenon in fact provides a decent window to
disclose essential factors that drive the massive diffusion underlying trends
in online social media. Specifically, the in-depth comparison between
diffusions represented by different peaks suggests that more attention from the
crowd at early stage of the propagation produces large-scale coverage, while
the dominant participation of opinion leaders at the early stage just leads to
popularity of small scope. Our results quantificationally challenge the
conventional hypothesis of influentials. And the implications of these novel
findings for marketing practice and influence maximization in social networks
are also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0214</identifier>
 <datestamp>2014-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0214</id><created>2014-08-31</created><authors><author><keyname>J&#xf8;rgensen</keyname><forenames>Jakob S.</forenames></author><author><keyname>Kruschel</keyname><forenames>Christian</forenames></author><author><keyname>Lorenz</keyname><forenames>Dirk A.</forenames></author></authors><title>Testable uniqueness conditions for empirical assessment of undersampling
  levels in total variation-regularized x-ray CT</title><categories>math.OC cs.IT math.IT</categories><comments>18 pages, 7 figures, submitted</comments><doi>10.1080/17415977.2014.986724</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study recoverability in fan-beam computed tomography (CT) with sparsity
and total variation priors: how many underdetermined linear measurements
suffice for recovering images of given sparsity? Results from compressed
sensing (CS) establish such conditions for, e.g., random measurements, but not
for CT. Recoverability is typically tested by checking whether a computed
solution recovers the original. This approach cannot guarantee solution
uniqueness and the recoverability decision therefore depends on the
optimization algorithm. We propose new computational methods to test
recoverability by verifying solution uniqueness conditions. Using both
reconstruction and uniqueness testing we empirically study the number of CT
measurements sufficient for recovery on new classes of sparse test images. We
demonstrate an average-case relation between sparsity and sufficient sampling
and observe a sharp phase transition as known from CS, but never established
for CT. In addition to assessing recoverability more reliably, we show that
uniqueness tests are often the faster option.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0215</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0215</id><created>2014-08-31</created><updated>2015-09-23</updated><authors><author><keyname>B&#xe9;daride</keyname><forenames>Nicolas</forenames></author><author><keyname>Fernique</keyname><forenames>Thomas</forenames></author></authors><title>No Weak Local Rules for the 4p-Fold Tilings</title><categories>math.DS cs.DM math-ph math.MP</categories><comments>14 pages, 6 figures</comments><msc-class>37B50, 52C23</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  On the one hand, Socolar showed in 1990 that the n-fold planar tilings admit
weak local rules when n is not divisible by 4 (the n=10 case corresponds to the
Penrose tilings and is known since 1974). On the other hand, Burkov showed in
1988 that the 8-fold tilings do not admit weak local rules, and Le showed the
same for the 12-fold tilings (unpublished). We here show that this is actually
the case for all the 4p-fold tilings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0242</identifier>
 <datestamp>2014-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0242</id><created>2014-08-31</created><authors><author><keyname>Bianchi</keyname><forenames>Giuseppe</forenames></author><author><keyname>Bonola</keyname><forenames>Marco</forenames></author><author><keyname>Capone</keyname><forenames>Antonio</forenames></author><author><keyname>Cascone</keyname><forenames>Carmelo</forenames></author><author><keyname>Pontarelli</keyname><forenames>Salvatore</forenames></author></authors><title>Towards Wire-speed Platform-agnostic Control of OpenFlow Switches</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The possibility to offload, via a platform-agnostic specification, the
execution of (some/part of the) control functions down to the switch and
operate them at wire speed based on packet level events, would yield
significant benefits in terms of control latency and reaction times, meanwhile
retaining the SDN-type ability to program and instantiate a desired network
operation from a central controller. While programmability inside the switches,
at wire speed and using platform-independent abstractions, of &quot;any possible'&quot;
control function seems well beyond the OpenFlow capabilities, in this paper we
argue that a non trivial sub-class of stateful control functions, namely those
that can be abstracted in terms of Mealy (Finite State) Machines, is already
compatible with off-the-shelf OpenFlow version 1.1+ Hardware with marginal
architectural modifications. With minimal additional hardware circuitry, the
above sub-class can be extended to include support for bidirectional/cross-flow
state handling. We demonstrate the viability of our proposed approach via two
proof-of-concept implementations (hardware and software), and we show how some
stateful control functionalities frequently exploited in network protocols are
readily deployed using our application programming interface.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0252</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0252</id><created>2014-08-31</created><updated>2015-01-22</updated><authors><author><keyname>Nanz</keyname><forenames>Sebastian</forenames></author><author><keyname>Furia</keyname><forenames>Carlo A.</forenames></author></authors><title>A Comparative Study of Programming Languages in Rosetta Code</title><categories>cs.SE</categories><journal-ref>Proceedings of the 37th International Conference on Software
  Engineering (ICSE'15), pages 778-788. IEEE, 2015</journal-ref><doi>10.1109/ICSE.2015.90</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sometimes debates on programming languages are more religious than
scientific. Questions about which language is more succinct or efficient, or
makes developers more productive are discussed with fervor, and their answers
are too often based on anecdotes and unsubstantiated beliefs. In this study, we
use the largely untapped research potential of Rosetta Code, a code repository
of solutions to common programming tasks in various languages, to draw a fair
and well-founded comparison. Rosetta Code offers a large data set for analysis.
Our study is based on 7087 solution programs corresponding to 745 tasks in 8
widely used languages representing the major programming paradigms (procedural:
C and Go; object-oriented: C# and Java; functional: F# and Haskell; scripting:
Python and Ruby). Our statistical analysis reveals, most notably, that:
functional and scripting languages are more concise than procedural and
object-oriented languages; C is hard to beat when it comes to raw speed on
large inputs, but performance differences over inputs of moderate size are less
pronounced and allow even interpreted languages to be competitive; compiled
strongly-typed languages, where more defects can be caught at compile time, are
less prone to runtime failures than interpreted or weakly-typed languages. We
discuss implications of these results for developers, language designers, and
educators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0261</identifier>
 <datestamp>2014-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0261</id><created>2014-08-31</created><authors><author><keyname>Krikidis</keyname><forenames>Ioannis</forenames></author><author><keyname>Timotheou</keyname><forenames>Stelios</forenames></author><author><keyname>Nikolaou</keyname><forenames>Symeon</forenames></author><author><keyname>Zheng</keyname><forenames>Gan</forenames></author><author><keyname>Ng</keyname><forenames>Derrick Wing Kwan</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author></authors><title>Simultaneous Wireless Information and Power Transfer in Modern
  Communication Systems</title><categories>cs.IT math.IT</categories><comments>6 figures. Accepted in IEEE Communications Magazine, Green
  Communications and Computing Networks Series</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy harvesting for wireless communication networks is a new paradigm that
allows terminals to recharge their batteries from external energy sources in
the surrounding environment. A promising energy harvesting technology is
wireless power transfer where terminals harvest energy from electromagnetic
radiation. Thereby, the energy may be harvested opportunistically from ambient
electromagnetic sources or from sources that intentionally transmit
electromagnetic energy for energy harvesting purposes. A particularly
interesting and challenging scenario arises when sources perform simultaneous
wireless information and power transfer (SWIPT), as strong signals not only
increase power transfer but also interference. This paper provides an overview
of SWIPT systems with a particular focus on the hardware realization of
rectenna circuits and practical techniques that achieve SWIPT in the domains of
time, power, antennas, and space. The paper also discusses the benefits of a
potential integration of SWIPT technologies in modern communication networks in
the context of resource allocation and cooperative cognitive radio networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0264</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0264</id><created>2014-08-31</created><updated>2015-12-22</updated><authors><author><keyname>Lalley</keyname><forenames>Steven P.</forenames></author><author><keyname>Weyl</keyname><forenames>E. Glen</forenames></author></authors><title>Quadratic Voting</title><categories>cs.GT math.PR</categories><comments>Revision of our earlier article &quot;Nash Equilibria for a Quadratic
  Voting Game&quot;</comments><msc-class>Primary 91B12, Secondary 91B52, 60F99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Voters making a binary decision purchase votes from a centralized clearing
house, paying the square of the number of votes purchased. The net payoff to an
agent with utility $u$ who purchases $v$ votes is $\Psi (S_{n+1})u-v^{2}$,
where $\Psi$ is a monotone function taking values between -1 and +1 and
$S_{n+1}$ is the sum of all votes purchased by the $n+1$ voters participating
in the election. The utilities of the voters are assumed to arise by random
sampling from a probability distribution $F_{U}$ with compact support; each
voter knows her own utility, but not those of the other voters, although she
does know the sampling distribution $F_{U}$. Nash equilibria for this game are
described. These results imply that the expected inefficiency of any Nash
equilibrium decays like $1/n$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0266</identifier>
 <datestamp>2014-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0266</id><created>2014-08-31</created><authors><author><keyname>Constable</keyname><forenames>Robert L.</forenames></author></authors><title>Virtual Evidence: A Constructive Semantics for Classical Logics</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article presents a computational semantics for classical logic using
constructive type theory. Such semantics seems impossible because classical
logic allows the Law of Excluded Middle (LEM), not accepted in constructive
logic since it does not have computational meaning. However, the apparently
oracular powers expressed in the LEM, that for any proposition P either it or
its negation, not P, is true can also be explained in terms of constructive
evidence that does not refer to &quot;oracles for truth.&quot; Types with virtual
evidence and the constructive impossibility of negative evidence provide
sufficient semantic grounds for classical truth and have a simple computational
meaning. This idea is formalized using refinement types, a concept of
constructive type theory used since 1984 and explained here. A new axiom
creating virtual evidence fully retains the constructive meaning of the logical
operators in classical contexts.
  Key Words: classical logic, constructive logic, intuitionistic logic,
propositions-as-types, constructive type theory, refinement types, double
negation translation, computational content, virtual evidence
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0267</identifier>
 <datestamp>2014-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0267</id><created>2014-08-31</created><authors><author><keyname>Sabatini</keyname><forenames>Fabio</forenames></author><author><keyname>Sarracino</keyname><forenames>Francesco</forenames></author></authors><title>Online networks destroy social trust</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Studies in the social capital literature have documented two stylised facts:
first, a decline in measures of social participation has occurred in many OECD
countries. Second, and more recently, the success of social networking sites
(SNSs) has resulted in a steep rise in online social participation. Our study
adds to this body of research by conducting the first empirical assessment of
how online networking affects two economically relevant aspects of social
capital, i.e. trust and sociability. We address endogeneity in online
networking by exploiting technological characteristics of the pre-existing
voice telecommunication infrastructures that exogenously determined the
availability of broadband for high-speed Internet. We find that participation
in SNSs such as Facebook and Twitter has a positive effect on face-to-face
interactions. However, social trust decreases with online interactions. We
argue that the rising practice of hate speech may play a crucial role in the
destruction of trust.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0272</identifier>
 <datestamp>2014-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0272</id><created>2014-08-31</created><updated>2014-09-01</updated><authors><author><keyname>Goncalves</keyname><forenames>Andre R.</forenames></author><author><keyname>Das</keyname><forenames>Puja</forenames></author><author><keyname>Chatterjee</keyname><forenames>Soumyadeep</forenames></author><author><keyname>Sivakumar</keyname><forenames>Vidyashankar</forenames></author><author><keyname>Von Zuben</keyname><forenames>Fernando J.</forenames></author><author><keyname>Banerjee</keyname><forenames>Arindam</forenames></author></authors><title>Multi-task Sparse Structure Learning</title><categories>cs.LG stat.ML</categories><comments>23rd ACM International Conference on Information and Knowledge
  Management - CIKM 2014</comments><acm-class>I.5.1, J.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-task learning (MTL) aims to improve generalization performance by
learning multiple related tasks simultaneously. While sometimes the underlying
task relationship structure is known, often the structure needs to be estimated
from data at hand. In this paper, we present a novel family of models for MTL,
applicable to regression and classification problems, capable of learning the
structure of task relationships. In particular, we consider a joint estimation
problem of the task relationship structure and the individual task parameters,
which is solved using alternating minimization. The task relationship structure
learning component builds on recent advances in structure learning of Gaussian
graphical models based on sparse estimators of the precision (inverse
covariance) matrix. We illustrate the effectiveness of the proposed model on a
variety of synthetic and benchmark datasets for regression and classification.
We also consider the problem of combining climate model outputs for better
projections of future climate, with focus on temperature in South America, and
show that the proposed model outperforms several existing methods for the
problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0280</identifier>
 <datestamp>2014-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0280</id><created>2014-08-31</created><authors><author><keyname>Goudarzi</keyname><forenames>Alireza</forenames></author><author><keyname>Stefanovic</keyname><forenames>Darko</forenames></author></authors><title>Towards a Calculus of Echo State Networks</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reservoir computing is a recent trend in neural networks which uses the
dynamical perturbations on the phase space of a system to compute a desired
target function. We present how one can formulate an expectation of system
performance in a simple class of reservoir computing called echo state
networks. In contrast with previous theoretical frameworks, which only reveal
an upper bound on the total memory in the system, we analytically calculate the
entire memory curve as a function of the structure of the system and the
properties of the input and the target function. We demonstrate the precision
of our framework by validating its result for a wide range of system sizes and
spectral radii. Our analytical calculation agrees with numerical simulations.
To the best of our knowledge this work presents the first exact analytical
characterization of the memory curve in echo state networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0286</identifier>
 <datestamp>2014-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0286</id><created>2014-08-31</created><authors><author><keyname>Li</keyname><forenames>Tao</forenames></author><author><keyname>Fan</keyname><forenames>Pingyi</forenames></author><author><keyname>Letaief</keyname><forenames>Khaled Ben</forenames></author></authors><title>Outage Probability of Energy Harvesting Relay-aided Cooperative Networks
  Over Rayleigh Fading Channel</title><categories>cs.IT math.IT</categories><comments>5 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy harvesting technique is a potential way for relay node energy supply
in cooperative networks in terms of deployment flexibility and maintain charge
reduction. Unlike traditional power source, relay node in this case may run out
of energy with certain probability, which can degrade the benefit from
relay-aided cooperative transmission. In this paper, we concentrate on the
outage behavior of cooperative networks aided by energy harvesting relay node
in slow fading channel, and attempt to derive the closed-form expression of
outage probability of proposed cooperative protocol. Compared with traditional
direct transmission protocol, two conclusions are derived: 1) the diversity
gain cannot be increased excepting the extreme case that energy-exhausted
probability is zero; 2) a multiplicative gain for improving system performance
can be obtained in terms of minimizing outage probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0289</identifier>
 <datestamp>2014-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0289</id><created>2014-09-01</created><updated>2014-09-03</updated><authors><author><keyname>Fletcher</keyname><forenames>Alyson K.</forenames></author><author><keyname>Rangan</keyname><forenames>Sundeep</forenames></author></authors><title>Scalable Inference for Neuronal Connectivity from Calcium Imaging</title><categories>cs.IT math.IT</categories><comments>14 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fluorescent calcium imaging provides a potentially powerful tool for
inferring connectivity in neural circuits with up to thousands of neurons.
However, a key challenge in using calcium imaging for connectivity detection is
that current systems often have a temporal response and frame rate that can be
orders of magnitude slower than the underlying neural spiking process. Bayesian
inference methods based on expectation-maximization (EM) have been proposed to
overcome these limitations, but are often computationally demanding since the
E-step in the EM procedure typically involves state estimation for a
high-dimensional nonlinear dynamical system. In this work, we propose a
computationally fast method for the state estimation based on a hybrid of loopy
belief propagation and approximate message passing (AMP). The key insight is
that a neural system as viewed through calcium imaging can be factorized into
simple scalar dynamical systems for each neuron with linear interconnections
between the neurons. Using the structure, the updates in the proposed hybrid
AMP methodology can be computed by a set of one-dimensional state estimation
procedures and linear transforms with the connectivity matrix. This yields a
computationally scalable method for inferring connectivity of large neural
circuits. Simulations of the method on realistic neural networks demonstrate
good accuracy with computation times that are potentially significantly faster
than current approaches based on Markov Chain Monte Carlo methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0296</identifier>
 <datestamp>2014-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0296</id><created>2014-09-01</created><authors><author><keyname>Johnson</keyname><forenames>Thienne</forenames></author><author><keyname>Vergara</keyname><forenames>Jorge</forenames></author><author><keyname>Doll</keyname><forenames>Chelsea</forenames></author><author><keyname>Kramer</keyname><forenames>Madison</forenames></author><author><keyname>Sundararaman</keyname><forenames>Gayathri</forenames></author><author><keyname>Rajendran</keyname><forenames>Harsha</forenames></author><author><keyname>Efrat</keyname><forenames>Alon</forenames></author><author><keyname>Hingle</keyname><forenames>Melanie</forenames></author></authors><title>A Mobile Food Recommendation System Based on The Traffic Light Diet</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Innovative, real-time solutions are needed to address the mismatch between
the demand for and supply of critical information to inform and motivate diet
and health-related behavior change. Research suggests that interventions using
mobile health technologies hold great promise for influencing knowledge,
attitudes, and behaviors related to energy balance. The objective of this paper
is to present insights related to the development and testing of a mobile food
recommendation system targeting fast food restaurants. The system is designed
to provide consumers with information about energy density of food options
combined with tips for healthier choices when dining out, accessible through a
mobile phone.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0302</identifier>
 <datestamp>2014-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0302</id><created>2014-09-01</created><authors><author><keyname>Chandrasekaran</keyname><forenames>Muthukumaran</forenames></author><author><keyname>Doshi</keyname><forenames>Prashant</forenames></author><author><keyname>Zeng</keyname><forenames>Yifeng</forenames></author><author><keyname>Chen</keyname><forenames>Yingke</forenames></author></authors><title>Team Behavior in Interactive Dynamic Influence Diagrams with
  Applications to Ad Hoc Teams</title><categories>cs.MA cs.AI</categories><comments>8 pages, Appeared in the MSDM Workshop at AAMAS 2014, Extended
  Abstract version appeared at AAMAS 2014, France</comments><msc-class>68T37</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Planning for ad hoc teamwork is challenging because it involves agents
collaborating without any prior coordination or communication. The focus is on
principled methods for a single agent to cooperate with others. This motivates
investigating the ad hoc teamwork problem in the context of individual decision
making frameworks. However, individual decision making in multiagent settings
faces the task of having to reason about other agents' actions, which in turn
involves reasoning about others. An established approximation that
operationalizes this approach is to bound the infinite nesting from below by
introducing level 0 models. We show that a consequence of the finitely-nested
modeling is that we may not obtain optimal team solutions in cooperative
settings. We address this limitation by including models at level 0 whose
solutions involve learning. We demonstrate that the learning integrated into
planning in the context of interactive dynamic influence diagrams facilitates
optimal team behavior, and is applicable to ad hoc teamwork.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0308</identifier>
 <datestamp>2014-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0308</id><created>2014-09-01</created><authors><author><keyname>Gyarmati</keyname><forenames>Laszlo</forenames></author><author><keyname>Kwak</keyname><forenames>Haewoon</forenames></author><author><keyname>Rodriguez</keyname><forenames>Pablo</forenames></author></authors><title>Searching for a Unique Style in Soccer</title><categories>cs.SI physics.soc-ph</categories><comments>2014 KDD Workshop on Large-Scale Sports Analytics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Is it possible to have a unique, recognizable style in soccer nowadays? We
address this question by proposing a method to quantify the motif
characteristics of soccer teams based on their pass networks. We introduce the
the concept of &quot;flow motifs&quot; to characterize the statistically significant pass
sequence patterns. It extends the idea of the network motifs, highly
significant subgraphs that usually consists of three or four nodes. The
analysis of the motifs in the pass networks allows us to compare and
differentiate the styles of different teams. Although most teams tend to apply
homogenous style, surprisingly, a unique strategy of soccer exists.
Specifically, FC Barcelona's famous tiki-taka does not consist of uncountable
random passes but rather has a precise, finely constructed structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0309</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0309</id><created>2014-09-01</created><authors><author><keyname>Chong</keyname><forenames>Stephen</forenames></author><author><keyname>van der Meyden</keyname><forenames>Ron</forenames></author></authors><title>Using Architecture to Reason about Information Security</title><categories>cs.CR cs.LO</categories><doi>10.1145/2829949</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We demonstrate, by a number of examples, that information-flow security
properties can be proved from abstract architectural descriptions, that
describe only the causal structure of a system and local properties of trusted
components. We specify these architectural descriptions of systems by
generalizing intransitive noninterference policies to admit the ability to
filter information passed between communicating domains. A notion of refinement
of such system architectures is developed that supports top-down development of
architectural specifications and proofs by abstraction of information security
properties. We also show that, in a concrete setting where the causal structure
is enforced by access control, a static check of the access control setting
plus local verification of the trusted components is sufficient to prove that a
generalized intransitive noninterference policy is satisfied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0314</identifier>
 <datestamp>2014-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0314</id><created>2014-09-01</created><updated>2014-09-02</updated><authors><author><keyname>Rama</keyname><forenames>Taraka</forenames></author></authors><title>Empirical Evaluation of Tree distances for Parser Evaluation</title><categories>cs.CL</categories><comments>Submitted to satisfy partial requirements for Statistical Parsing
  course</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this empirical study, I compare various tree distance measures --
originally developed in computational biology for the purpose of tree
comparison -- for the purpose of parser evaluation. I will control for the
parser setting by comparing the automatically generated parse trees from the
state-of-the-art parser Charniak, 2000) with the gold-standard parse trees. The
article describes two different tree distance measures (RF and QD) along with
its variants (GRF and GQD) for the purpose of parser evaluation. The article
will argue that RF measure captures similar information as the standard EvalB
metric (Sekine and Collins, 1997) and the tree edit distance (Zhang and Shasha,
1989) applied by Tsarfaty et al. (2011). Finally, the article also provides
empirical evidence by reporting high correlations between the different tree
distances and EvalB metric's scores.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0315</identifier>
 <datestamp>2014-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0315</id><created>2014-09-01</created><updated>2014-12-04</updated><authors><author><keyname>N&#xf6;llenburg</keyname><forenames>Martin</forenames></author><author><keyname>Prutkin</keyname><forenames>Roman</forenames></author><author><keyname>Rutter</keyname><forenames>Ignaz</forenames></author></authors><title>On Self-Approaching and Increasing-Chord Drawings of 3-Connected Planar
  Graphs</title><categories>cs.CG</categories><comments>22 pages, 9 figures, full version of a paper appearing in Graph
  Drawing 2014. Compared to the previous version, contains a new result on area
  requirements of strongly monotone drawings</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An $st$-path in a drawing of a graph is self-approaching if during the
traversal of the corresponding curve from $s$ to any point $t'$ on the curve
the distance to $t'$ is non-increasing. A path has increasing chords if it is
self-approaching in both directions. A drawing is self-approaching
(increasing-chord) if any pair of vertices is connected by a self-approaching
(increasing-chord) path.
  We study self-approaching and increasing-chord drawings of triangulations and
3-connected planar graphs. We show that in the Euclidean plane, triangulations
admit increasing-chord drawings, and for planar 3-trees we can ensure
planarity. We prove that strongly monotone (and thus increasing-chord) drawings
of trees and binary cactuses require exponential resolution in the worst case,
answering an open question by Kindermann et al. [GD'14]. Moreover, we provide a
binary cactus that does not admit a self-approaching drawing. Finally, we show
that 3-connected planar graphs admit increasing-chord drawings in the
hyperbolic plane and characterize the trees that admit such drawings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0325</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0325</id><created>2014-09-01</created><authors><author><keyname>Lu&#x10d;anin</keyname><forenames>Dra&#x17e;en</forenames></author><author><keyname>Jrad</keyname><forenames>Foued</forenames></author><author><keyname>Brandic</keyname><forenames>Ivona</forenames></author><author><keyname>Streit</keyname><forenames>Achim</forenames></author></authors><title>Energy-Aware Cloud Management through Progressive SLA Specification</title><categories>cs.DC</categories><comments>14 pages, conference</comments><doi>10.1007/978-3-319-14609-6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Novel energy-aware cloud management methods dynamically reallocate
computation across geographically distributed data centers to leverage regional
electricity price and temperature differences. As a result, a managed VM may
suffer occasional downtimes. Current cloud providers only offer high
availability VMs, without enough flexibility to apply such energy-aware
management. In this paper we show how to analyse past traces of dynamic cloud
management actions based on electricity prices and temperatures to estimate VM
availability and price values. We propose a novel SLA specification approach
for offering VMs with different availability and price values guaranteed over
multiple SLAs to enable flexible energy-aware cloud management. We determine
the optimal number of such SLAs as well as their availability and price
guaranteed values. We evaluate our approach in a user SLA selection simulation
using Wikipedia and Grid'5000 workloads. The results show higher customer
conversion and 39% average energy savings per VM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0334</identifier>
 <datestamp>2014-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0334</id><created>2014-09-01</created><authors><author><keyname>Jiang</keyname><forenames>Xiaoran</forenames></author><author><keyname>Gripon</keyname><forenames>Vincent</forenames></author><author><keyname>Berrou</keyname><forenames>Claude</forenames></author><author><keyname>Rabbat</keyname><forenames>Michael</forenames></author></authors><title>Storing sequences in binary tournament-based neural networks</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An extension to a recently introduced architecture of clique-based neural
networks is presented. This extension makes it possible to store sequences with
high efficiency. To obtain this property, network connections are provided with
orientation and with flexible redundancy carried by both spatial and temporal
redundancy, a mechanism of anticipation being introduced in the model. In
addition to the sequence storage with high efficiency, this new scheme also
offers biological plausibility. In order to achieve accurate sequence
retrieval, a double layered structure combining hetero-association and
auto-association is also proposed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0347</identifier>
 <datestamp>2014-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0347</id><created>2014-09-01</created><authors><author><keyname>Li</keyname><forenames>Chao</forenames></author><author><keyname>Guo</keyname><forenames>Lili</forenames></author><author><keyname>Cichocki</keyname><forenames>Andrzej</forenames></author></authors><title>Multi-tensor Completion for Estimating Missing Values in Video Data</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many tensor-based data completion methods aim to solve image and video
in-painting problems. But, all methods were only developed for a single
dataset. In most of real applications, we can usually obtain more than one
dataset to reflect one phenomenon, and all the datasets are mutually related in
some sense. Thus one question raised whether such the relationship can improve
the performance of data completion or not? In the paper, we proposed a novel
and efficient method by exploiting the relationship among datasets for
multi-video data completion. Numerical results show that the proposed method
significantly improve the performance of video in-painting, particularly in the
case of very high missing percentage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0348</identifier>
 <datestamp>2015-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0348</id><created>2014-09-01</created><updated>2014-12-09</updated><authors><author><keyname>Kraker</keyname><forenames>Peter</forenames></author><author><keyname>Schl&#xf6;gl</keyname><forenames>Christian</forenames></author><author><keyname>Jack</keyname><forenames>Kris</forenames></author><author><keyname>Lindstaedt</keyname><forenames>Stefanie</forenames></author></authors><title>Visualization of Co-Readership Patterns from an Online Reference
  Management System</title><categories>cs.DL</categories><comments>Accepted for publication in the Journal of Informetrics</comments><journal-ref>Journal of Informetrics, Volume 9, Issue 1, January 2015, Pages
  169-182</journal-ref><doi>10.1016/j.joi.2014.12.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we analyze the adequacy and applicability of readership
statistics recorded in social reference management systems for creating
knowledge domain visualizations. First, we investigate the distribution of
subject areas in user libraries of educational technology researchers on
Mendeley. The results show that around 69% of the publications in an average
user library can be attributed to a single subject area. Then, we use
co-readership patterns to map the field of educational technology. The
resulting visualization prototype, based on the most read publications in this
field on Mendeley, reveals 13 topic areas of educational technology research.
The visualization is a recent representation of the field: 80% of the
publications included were published within ten years of data collection. The
characteristics of the readers, however, introduce certain biases to the
visualization. Knowledge domain visualizations based on readership statistics
are therefore multifaceted and timely, but it is important that the
characteristics of the underlying sample are made transparent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0351</identifier>
 <datestamp>2014-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0351</id><created>2014-09-01</created><authors><author><keyname>Michel</keyname><forenames>Laurent</forenames></author><author><keyname>Motch</keyname><forenames>Christian</forenames></author><author><keyname>Nguyen</keyname><forenames>Hoan Ngoc</forenames></author><author><keyname>Pineau</keyname><forenames>Fran&#xe7;ois-Xavier</forenames></author></authors><title>Building an Archive with Saada</title><categories>astro-ph.IM cs.DB</categories><comments>18 pages, 5 figures Special VO issue</comments><msc-class>68U35</msc-class><doi>10.1016/j.ascom.2014.08.001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Saada transforms a set of heterogeneous FITS files or VOTables of various
categories (images, tables, spectra ...) in a database without writing code.
Databases created with Saada come with a rich Web interface and an Application
Programming Interface (API). They support the four most common VO services.
Such databases can mix various categories of data in multiple collections. They
allow a direct access to the original data while providing a homogenous view
thanks to an internal data model compatible with the characterization axis
defined by the VO. The data collections can be bound to each other with
persistent links making relevant browsing paths and allowing data-mining
oriented queries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0367</identifier>
 <datestamp>2014-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0367</id><created>2014-09-01</created><updated>2014-10-14</updated><authors><author><keyname>Crick</keyname><forenames>Tom</forenames></author><author><keyname>Hall</keyname><forenames>Benjamin A.</forenames></author><author><keyname>Ishtiaq</keyname><forenames>Samin</forenames></author><author><keyname>Takeda</keyname><forenames>Kenji</forenames></author></authors><title>&quot;Share and Enjoy&quot;: Publishing Useful and Usable Scientific Models</title><categories>cs.CE</categories><comments>Accepted for the 1st International Workshop on Recomputability (part
  of UCC 2014); 5 pages, LaTeX</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The reproduction and replication of reported scientific results is a hot
topic within the academic community. The retraction of numerous studies from a
wide range of disciplines, from climate science to bioscience, has drawn the
focus of many commentators, but there exists a wider socio-cultural problem
that pervades the scientific community. Sharing code, data and models often
requires extra effort; this is currently seen as a significant overhead that
may not be worth the time investment.
  Automated systems, which allow easy reproduction of results, offer the
potential to incentivise a culture change and drive the adoption of new
techniques to improve the efficiency of scientific exploration. In this paper,
we discuss the value of improved access and sharing of the two key types of
results arising from work done in the computational sciences: models and
algorithms. We propose the development of an integrated cloud-based system
underpinning computational science, linking together software and data
repositories, toolchains, workflows and outputs, providing a seamless automated
infrastructure for the verification and validation of scientific models and in
particular, performance benchmarks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0375</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0375</id><created>2014-09-01</created><updated>2014-11-23</updated><authors><author><keyname>Panyukov</keyname><forenames>Anatoly</forenames></author></authors><title>Polynomial solvability of $NP$-complete problems</title><categories>cs.CC</categories><comments>11 pages</comments><msc-class>05C85</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A polynomial algorithm for solving &quot;Hamiltonian circuit&quot; problem is presented
in the paper. Computational complexity of the algorithm is equal
$O(n^{8}{\log_2}^2n)$ where $n$ is the cardinality of the observed graph vertex
set. Thus the polynomial solvability for ${\mathcal NP}$-complete problems is
proved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0384</identifier>
 <datestamp>2014-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0384</id><created>2014-09-01</created><authors><author><keyname>Maoz</keyname><forenames>Shahar</forenames></author><author><keyname>Ringert</keyname><forenames>Jan Oliver</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>An Interim Summary on Semantic Model Differencing</title><categories>cs.SE</categories><comments>3 pages, 5 figures, Softwaretechnik-Trends, Volume 32, Issue 4.
  November, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This position paper provides an interim summary on the goals and current
state of our ongoing research project on semantic model differencing for
software evolution. We describe the basics of semantic model differencing, give
two examples from our recent work, and discuss future challenges in taking full
advantage of the potential of semantic differencing techniques in the context
of models' evolution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0385</identifier>
 <datestamp>2014-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0385</id><created>2014-09-01</created><authors><author><keyname>Kowalewski</keyname><forenames>Stefan</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author><author><keyname>Stollenwerk</keyname><forenames>Andre</forenames></author></authors><title>Cyber-Physical Systems -- eine Herausforderung an die
  Automatisierungstechnik?</title><categories>cs.SE</categories><comments>12 pages, in German, 4 figures</comments><journal-ref>Proc. Automation 2012, VDI Berichte 2012, VDI-Verlag, pp. 113-116,
  Langfassung auf CD-ROM</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss challenges to control systems engineering arising from the advent
of cyber-physical systems (CPS). After discussing the terminology, general,
IT-related issues are treated which need cooperation with computer science, in
particular software engineering. Then we study those challenges that require
specific core competencies from control systems engineering. We sketch solution
approaches for the exemplary problem of dealing with changes in the physical
environment of a CPS.
  ----
  Der Beitrag befasst sich mit den methodischen Herausforderungen, die durch
die Verbreitung der Cyber-Physical Systems (CPS) in der Automatisierungstechnik
entstehen, und stellt L\&quot;osungsans\&quot;atze vor. Nach einer Behandlung des
Begriffs CPS werden zun\&quot;achst die allgemeinen, IT-bezogenen Fragestellungen
angesprochen, die gemeinsam mit der Informatik gel\&quot;ost werden m\&quot;ussen. Danach
gehen wir auf die Herausforderungen ein, deren Behandlung spezifisch
automatisierungstechnische Kernkompetenzen erfordern und skizzieren f\&quot;ur eine
beispielhafte Problemstellung, den Umgang mit \&quot;Anderungen in der
physikalischen Umgebung, wie entsprechende L\&quot;osungen aussehen k\&quot;onnen.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0393</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0393</id><created>2014-09-01</created><updated>2015-07-25</updated><authors><author><keyname>Hritcu</keyname><forenames>Catalin</forenames></author><author><keyname>Lampropoulos</keyname><forenames>Leonidas</forenames></author><author><keyname>Spector-Zabusky</keyname><forenames>Antal</forenames></author><author><keyname>de Amorim</keyname><forenames>Arthur Azevedo</forenames></author><author><keyname>D&#xe9;n&#xe8;s</keyname><forenames>Maxime</forenames></author><author><keyname>Hughes</keyname><forenames>John</forenames></author><author><keyname>Pierce</keyname><forenames>Benjamin C.</forenames></author><author><keyname>Vytiniotis</keyname><forenames>Dimitrios</forenames></author></authors><title>Testing Noninterference, Quickly</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information-flow control mechanisms are difficult both to design and to prove
correct. To reduce the time wasted on doomed proof attempts due to broken
definitions, we advocate modern random testing techniques for finding
counterexamples during the design process. We show how to use QuickCheck, a
property-based random-testing tool, to guide the design of increasingly complex
information-flow abstract machines, leading up to a sophisticated register
machine with a novel and highly permissive flow-sensitive dynamic enforcement
mechanism that is sound in the presence of first-class public labels. We find
that both sophisticated strategies for generating well-distributed random
programs and readily falsifiable formulations of noninterference properties are
critically important for efficient testing. We propose several approaches and
evaluate their effectiveness on a collection of injected bugs of varying
subtlety. We also present an effective technique for shrinking large
counterexamples to minimal, easily comprehensible ones. Taken together, our
best methods enable us to quickly and automatically generate simple
counterexamples for more than 45 bugs. Moreover, we show how testing guides the
discovery of the sophisticated invariants needed for the noninterference proof
of our most complex machine.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0394</identifier>
 <datestamp>2014-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0394</id><created>2014-09-01</created><authors><author><keyname>Ringert</keyname><forenames>Jan Oliver</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author><author><keyname>Wortmann</keyname><forenames>Andreas</forenames></author></authors><title>A Requirements Modeling Language for the Component Behavior of Cyber
  Physical Robotics Systems</title><categories>cs.SE</categories><comments>13 pges, 6 figures. In: Norbert Seyff and Anne Koziolek (eds.),
  Modelling and Quality in Requirements Engineering: Essays Dedicated to Martin
  Glinz on the Occasion of His 60th Birthday, M\&quot;unster: Monsenstein und
  Vannerdat, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software development for robotics applications is a sophisticated endeavor as
robots are inherently complex. Explicit modeling of the architecture and
behavior of robotics application yields many advantages to cope with this
complexity by identifying and separating logically and physically independent
components and by hierarchically structuring the system under development. On
top of component and connector models we propose modeling the requirements on
the behavior of robotics software components using I/O! automata. This approach
facilitates early simulation of requirements model, allows to subject these to
formal analysis and to generate the software from them. In this paper, we
introduce an extension of the architecture description language MontiArc to
model the requirements on components with I/O!automata, which are defined in
the spirit of Martin Glinz Statecharts for requirements modeling [10]. We
furthermore present a case study based on a robotics application generated for
the Lego NXT robotic platform.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0400</identifier>
 <datestamp>2014-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0400</id><created>2014-09-01</created><authors><author><keyname>Kurpick</keyname><forenames>Thomas</forenames></author><author><keyname>Look</keyname><forenames>Markus</forenames></author><author><keyname>Pinkernell</keyname><forenames>Claas</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>Modeling Cyber-Physical Systems: Model-Driven Specification of Energy
  Efficient Buildings</title><categories>cs.SE</categories><comments>6 pages, 6 figures. Proceedings of the Modelling of the Physical
  World Workshop MOTPW '12, Innsbruck, October 2012, pp.2:1-2:6, ACM Digital
  Library, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A lot of current buildings are operated energy inefficient and offer a great
potential to reduce the overall energy consumption and CO2 emission. Detecting
these inefficiencies is a complicated task and needs domain experts that are
able to identify them. Most approaches try to support detection by focussing on
monitoring the building's operation and visualizing data. Instead our approach
focuses on using techniques taken from the cyber-physical systems' modeling
domain. We create a model of the building and show how we constrain the model
by OCL-like rules to support a sound specification which can be matched against
monitoring results afterwards. The paper presents our domain-specific language
for modeling buildings and technical facilities that is implemented in a
software-based tool used by domain experts and thus hopefully providing a
suitable contribution to modeling the cyber-physical world.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0402</identifier>
 <datestamp>2014-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0402</id><created>2014-09-01</created><authors><author><keyname>Liskin</keyname><forenames>Olga</forenames></author><author><keyname>Herrmann</keyname><forenames>Christoph</forenames></author><author><keyname>Knauss</keyname><forenames>Eric</forenames></author><author><keyname>Kurpick</keyname><forenames>Thomas</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author><author><keyname>Schneider</keyname><forenames>Kurt</forenames></author></authors><title>Supporting acceptance testing in distributed software projects with
  integrated feedback systems: Experiences and requirements</title><categories>cs.SE</categories><comments>10 pages, 6 figures</comments><journal-ref>Proceedings of 7th International Conference on Global Software
  Engineering (ICGSE'12), Puerto Alegre, Brazil, pp.84-93, 2012</journal-ref><doi>10.1109/ICGSE.2012.34</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  During acceptance testing customers assess whether a system meets their
expectations and often identify issues that should be improved. These findings
have to be communicated to the developers a task we observed to be error prone,
especially in distributed teams. Here, it is normally not possible to have
developer representatives from every site attend the test. Developers who were
not present might misunderstand insufficiently documented findings. This
hinders fixing the issues and endangers customer satisfaction. Integrated
feedback systems promise to mitigate this problem. They allow to easily capture
findings and their context. Correctly applied, this technique could improve
feedback, while reducing customer effort. This paper collects our experiences
from comparing acceptance testing with and without feedback systems in a
distributed project. Our results indicate that this technique can improve
acceptance testing if certain requirements are met. We identify key
requirements feedback systems should meet to support acceptance testing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0405</identifier>
 <datestamp>2014-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0405</id><created>2014-09-01</created><authors><author><keyname>Witherden</keyname><forenames>F. D.</forenames></author><author><keyname>Vermeire</keyname><forenames>B. C.</forenames></author><author><keyname>Vincent</keyname><forenames>P. E.</forenames></author></authors><title>Heterogeneous Computing on Mixed Unstructured Grids with PyFR</title><categories>physics.flu-dyn cs.CE physics.comp-ph</categories><comments>21 pages, 9 figures, 6 tables</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  PyFR is an open-source high-order accurate computational fluid dynamics
solver for mixed unstructured grids that can target a range of hardware
platforms from a single codebase. In this paper we demonstrate the ability of
PyFR to perform high-order accurate unsteady simulations of flow on mixed
unstructured grids using heterogeneous multi-node hardware. Specifically, after
benchmarking single-node performance for various platforms, PyFR v0.2.2 is used
to undertake simulations of unsteady flow over a circular cylinder at Reynolds
number 3 900 using a mixed unstructured grid of prismatic and tetrahedral
elements on a desktop workstation containing an Intel Xeon E5-2697 v2 CPU, an
NVIDIA Tesla K40c GPU, and an AMD FirePro W9100 GPU. Both the performance and
accuracy of PyFR are assessed. PyFR v0.2.2 is freely available under a 3-Clause
New Style BSD license (see www.pyfr.org).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0413</identifier>
 <datestamp>2014-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0413</id><created>2014-09-01</created><authors><author><keyname>Berger</keyname><forenames>Christian</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>Autonomous Driving - 5 Years after the Urban Challenge: The Anticipatory
  Vehicle as a Cyber-Physical System</title><categories>cs.RO cs.SE</categories><comments>10 pages, 7 figures</comments><journal-ref>Proceedings of the 10th Workshop on Automotive Software
  Engineering (ASE 2012), pp. 789-798, Braunschweig, September 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In November 2007 the international competition DARPA Urban Challenge took
place on the former George Airforce Base in Victorville, California to
significantly promote the research and development on autonomously driving
vehicles for urban environments. In the final race only eleven out of initially
89 competitors participated and &quot;Boss&quot; from Carnegie Mellon University
succeeded. This paper summarizes results of the research carried out by all
finalists within the last five years after the competition and provides an
outlook where further investigation especially for software engineering is now
necessary to achieve the goal of driving safely and reliably through urban
environments with an anticipatory vehicle for the mass-market.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0415</identifier>
 <datestamp>2014-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0415</id><created>2014-09-01</created><authors><author><keyname>Herrmann</keyname><forenames>Christoph</forenames></author><author><keyname>Kurpick</keyname><forenames>Thomas</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>SSELab: A Plug-In-Based Framework for Web-Based Project Portals</title><categories>cs.SE</categories><comments>6 pages, 5 figures. Proceedings of the 2nd International Workshop on
  Developing Tools as Plug-Ins (TOPI 2012) at ICSE 2012, June 3, Zurich,
  Switzerland, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tools are an essential part of every software engineering project. But the
number of tools that are used in all phases of the software development
life-cycle and their complexity is growing continually. Consequently, the setup
and maintenance of current tool chains and development environments requires
much effort and consumes a lot of time. One approach to counter this, is to
employ web-based systems for development tasks, because centralized systems
simplify the administration and the deployment of new features. But desktop
IDEs play an important role in software development projects today, and will
not be replaced entirely by web-based environments in the near future.
Therefore, supporting a mixture of hosted tools and tools integrated into
desktop IDEs is a sensible approach. In this paper, we present the SSELab, a
framework for web- based project portals that attempts to migrate more software
development tools from desktop to server environments, but still allows their
integration into modern desktop IDEs. It supports the deployment of tools as
hosted services using plug-in systems on the server-side. Additionally, it
provides access to these tools by a set of clients that can be used in
different contexts, either from the command line, from within IDEs such as
Eclipse, or from web pages. In the paper, we discuss the architecture and the
extensibility of the SSELab framework. Furthermore, we share our experiences
with creating an instance of the framework and integrating various tools for
our own software development projects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0416</identifier>
 <datestamp>2014-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0416</id><created>2014-09-01</created><authors><author><keyname>Plesser</keyname><forenames>Stefan</forenames></author><author><keyname>Pinkernell</keyname><forenames>Claas</forenames></author><author><keyname>Fisch</keyname><forenames>M. Norbert</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>The Energy Navigator - A Web-Platform for Performance Design and
  Management</title><categories>cs.SE</categories><comments>9 pages, 5 figures. Proceedings of the 7th International Conference
  on Energy Efficiency in Commercial Buildings (IEECB), Frankfurt a. M.,
  Germany, April 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the last three decades comprehensive research has been carried out
trying to improve commissioning processes with powerful modeling tools and
methodologies for data analysis and visualization. Typically addressed
application scenarios are facilities management, contracting, special
consulting services and measurement &amp; verification as part of a certification
process. The results are all but convincing: Monitoring of building operation
has so far not become a regular service for buildings. We have identified a
lack of process integration as a significant barrier for market success. Most
methodologies have so far caused additional initial invest and transaction
cost: they added new services instead of improving existing ones. The Energy
Navigator, developed by synavision GmbH in cooperation with leading research
institutes of the Technical University Braunschweig and the RWTH Aachen
University, presents a new methodology with several new approaches. Its
software platform uses state graphs and a domain specific language to describe
building functions offering an alternative to the software that is so far most
widely used for this task: Microsoft Word. The Energy Navigators so called
Active Functional Specification (AFS) is used for the technical specification
of building services in the design phase. After construction it is completed by
the supplier of the BMS (Building Management System) with the relevant sensors
data as documentation of his service. Operation data can then automatically be
checked for initial and continuous commissioning on whether it meets the
criteria of the specification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0421</identifier>
 <datestamp>2015-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0421</id><created>2014-09-01</created><updated>2015-08-20</updated><authors><author><keyname>Gilboa</keyname><forenames>Shoni</forenames></author><author><keyname>Gueron</keyname><forenames>Shay</forenames></author><author><keyname>Nandi</keyname><forenames>Mridul</forenames></author></authors><title>Balanced permutations Even-Mansour ciphers</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The $r$-rounds Even-Mansour block cipher is a generalization of the well
known Even-Mansour block cipher to $r$ iterations. Attacks on this construction
were described by Nikoli\'c et al. and Dinur et al., for $r = 2, 3$. These
attacks are only marginally better than brute force, but are based on an
interesting observation (due to Nikoli\'c et al.): for a &quot;typical&quot; permutation
$P$, the distribution of $P(x) \oplus x$ is not uniform. This naturally raises
the following question. Call permutations for which the distribution of $P(x)
\oplus x$ is uniform &quot;balanced.&quot; Is there a sufficiently large family of
balanced permutations, and what is the security of the resulting Even-Mansour
block cipher?
  We show how to generate families of balanced permutations from the
Luby-Rackoff construction, and use them to define a $2n$-bit block cipher from
the $2$-rounds Even-Mansour scheme. We prove that this cipher is
indistinguishable from a random permutation of $\{0, 1\}^{2n}$, for any
adversary who has oracle access to the public permutations and to an
encryption/decryption oracle, as long as the number of queries is $o
(2^{n/2})$. As a practical example, we discuss the properties and the
performance of a $256$-bit block cipher that is based on our construction, and
uses AES as the public permutation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0428</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0428</id><created>2014-09-01</created><updated>2015-06-19</updated><authors><author><keyname>Ermann</keyname><forenames>Leonardo</forenames></author><author><keyname>Frahm</keyname><forenames>Klaus M.</forenames></author><author><keyname>Shepelyansky</keyname><forenames>Dima L.</forenames></author></authors><title>Google matrix analysis of directed networks</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI nlin.CD</categories><comments>56 pages, 58 figures. Small modifications, refs added, accepted to
  Rev. Mod. Phys</comments><journal-ref>Rev. Mod. Phys. 87, 1261 (2015)</journal-ref><doi>10.1103/RevModPhys.87.1261</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In past ten years, modern societies developed enormous communication and
social networks. Their classification and information retrieval processing
become a formidable task for the society. Due to the rapid growth of World Wide
Web, social and communication networks, new mathematical methods have been
invented to characterize the properties of these networks on a more detailed
and precise level. Various search engines are essentially using such methods.
It is highly important to develop new tools to classify and rank enormous
amount of network information in a way adapted to internal network structures
and characteristics. This review describes the Google matrix analysis of
directed complex networks demonstrating its efficiency on various examples
including World Wide Web, Wikipedia, software architecture, world trade, social
and citation networks, brain neural networks, DNA sequences and Ulam networks.
The analytical and numerical matrix methods used in this analysis originate
from the fields of Markov chains, quantum chaos and Random Matrix theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0436</identifier>
 <datestamp>2014-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0436</id><created>2014-09-01</created><authors><author><keyname>Hu</keyname><forenames>Yifan</forenames></author><author><keyname>Shi</keyname><forenames>Lei</forenames></author></authors><title>A Coloring Algorithm for Disambiguating Graph and Map Drawings</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Drawings of non-planar graphs always result in edge crossings. When there are
many edges crossing at small angles, it is often difficult to follow these
edges, because of the multiple visual paths resulted from the crossings that
slow down eye movements. In this paper we propose an algorithm that
disambiguates the edges with automatic selection of distinctive colors. Our
proposed algorithm computes a near optimal color assignment of a dual collision
graph, using a novel branch-and-bound procedure applied to a space
decomposition of the color gamut. We give examples demonstrating the
effectiveness of this approach in clarifying drawings of real world graphs and
maps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0440</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0440</id><created>2014-08-12</created><authors><author><keyname>Guo</keyname><forenames>Chunli</forenames></author><author><keyname>Davies</keyname><forenames>Mike E.</forenames></author></authors><title>Near optimal compressed sensing without priors: Parametric SURE
  Approximate Message Passing</title><categories>cs.IT math.IT</categories><comments>Part of the work will be presented at the European Signal Processing
  Conference, Lisbon, Portugal, September 2014</comments><doi>10.1109/TSP.2015.2408569</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Both theoretical analysis and empirical evidence confirm that the approximate
message passing (AMP) algorithm can be interpreted as recursively solving a
signal denoising problem: at each AMP iteration, one observes a Gaussian noise
perturbed original signal. Retrieving the signal amounts to a successive noise
cancellation until the noise variance decreases to a satisfactory level. In
this paper we incorporate the Stein's unbiased risk estimate (SURE) based
parametric denoiser with the AMP framework and propose the novel parametric
SURE-AMP algorithm. At each parametric SURE-AMP iteration, the denoiser is
adaptively optimized within the parametric class by minimizing SURE, which
depends purely on the noisy observation. In this manner, the parametric
SURE-AMP is guaranteed with the best-in-class recovery and convergence rate. If
the parameter family includes the families of the mimimum mean squared error
(MMSE) estimators, we are able to achieve the Bayesian optimal AMP performance
without knowing the signal prior. In the paper, we resort to the linear
parameterization of the SURE based denoiser and propose three different kernel
families as the base functions. Numerical simulations with the
Bernoulli-Gaussian, $k$-dense and Student's-t signals demonstrate that the
parametric SURE-AMP does not only achieve the state-of-the-art recovery but
also runs more than 20 times faster than the EM-GM-GAMP algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0451</identifier>
 <datestamp>2016-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0451</id><created>2014-09-01</created><updated>2016-01-20</updated><authors><author><keyname>Pouly</keyname><forenames>Amaury</forenames></author><author><keyname>Gra&#xe7;a</keyname><forenames>Daniel S.</forenames></author></authors><title>Computational complexity of solving polynomial differential equations
  over unbounded domains</title><categories>cs.CC</categories><msc-class>03D78, 65L05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we investigate the computational complexity of solving ordinary
differential equations (ODEs) $y^{\prime}=p(y)$ over \emph{unbounded time
domains}, where $p$ is a vector of polynomials. Contrarily to the bounded
(compact) time case, this problem has not been well-studied, apparently due to
the &quot;intuition&quot; that it can always be reduced to the bounded case by using
rescaling techniques. However, as we show in this paper, rescaling techniques
do not seem to provide meaningful insights on the complexity of this problem,
since the use of such techniques introduces a dependence on parameters which
are hard to compute.
  We present algorithms which numerically solve these ODEs over unbounded time
domains. These algorithms have guaranteed accuracy, i.e. given some arbitrarily
large time $t$ and error bound $\varepsilon$ as input, they will output a value
$\tilde{y}$ which satisfies $\|y(t)-\tilde{y}\|\leq\varepsilon$. We analyze the
complexity of these algorithms and show that they compute $\tilde{y}$ in time
polynomial in several quantities including the time $t$, the accuracy of the
output $\varepsilon$ and the length of the curve $y$ from $0$ to $t$, assuming
it exists until time $t$. We consider both algebraic complexity and bit
complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0454</identifier>
 <datestamp>2014-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0454</id><created>2014-09-01</created><authors><author><keyname>Zaidi</keyname><forenames>Abdellatif</forenames><affiliation>Shitz</affiliation></author><author><keyname>Shamai</keyname><forenames>Shlomo</forenames><affiliation>Shitz</affiliation></author></authors><title>On Cooperative Multiple Access Channels with Delayed CSI at Transmitters</title><categories>cs.IT math.IT</categories><comments>54 pages. To appear in IEEE Transactions on Information Theory. arXiv
  admin note: substantial text overlap with arXiv:1201.3278</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a cooperative two-user multiaccess channel in which the
transmission is controlled by a random state. Both encoders transmit a common
message and, one of the encoders also transmits an individual message. We study
the capacity region of this communication model for different degrees of
availability of the states at the encoders, causally or strictly causally. In
the case in which the states are revealed causally to both encoders but not to
the decoder we find an explicit characterization of the capacity region in the
discrete memoryless case. In the case in which the states are revealed only
strictly causally to both encoders, we establish inner and outer bounds on the
capacity region. The outer bound is non-trivial, and has a relatively simple
form. It has the advantage of incorporating only one auxiliary random variable.
We then introduce a class of cooperative multiaccess channels with states known
strictly causally at both encoders for which the inner and outer bounds agree;
and so we characterize the capacity region for this class. In this class of
channels, the state can be obtained as a deterministic function of the channel
inputs and output. We also study the model in which the states are revealed,
strictly causally, in an asymmetric manner, to only one encoder. Throughout the
paper, we discuss a number of examples; and compute the capacity region of some
of these examples. The results shed more light on the utility of delayed
channel state information for increasing the capacity region of state-dependent
cooperative multiaccess channels; and tie with recent progress in this
framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0461</identifier>
 <datestamp>2014-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0461</id><created>2014-09-01</created><authors><author><keyname>Bekos</keyname><forenames>Michael A.</forenames></author><author><keyname>Cornelsen</keyname><forenames>Sabine</forenames></author><author><keyname>Grilli</keyname><forenames>Luca</forenames></author><author><keyname>Hong</keyname><forenames>Seok-Hee</forenames></author><author><keyname>Kaufmann</keyname><forenames>Michael</forenames></author></authors><title>On the Recognition of Fan-Planar and Maximal Outer-Fan-Planar Graphs</title><categories>cs.CG cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fan-planar graphs were recently introduced as a generalization of 1-planar
graphs. A graph is fan-planar if it can be embedded in the plane, such that
each edge that is crossed more than once, is crossed by a bundle of two or more
edges incident to a common vertex. A graph is outer-fan-planar if it has a
fan-planar embedding in which every vertex is on the outer face. If, in
addition, the insertion of an edge destroys its outer-fan-planarity, then it is
maximal outer-fan-planar. In this paper, we present a polynomial-time algorithm
to test whether a given graph is maximal outer-fan-planar. The algorithm can
also be employed to produce an outer-fan-planar embedding, if one exists. On
the negative side, we show that testing fan-planarity of a graph is NP-hard,
for the case where the rotation system (i.e., the cyclic order of the edges
around each vertex) is given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0470</identifier>
 <datestamp>2014-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0470</id><created>2014-09-01</created><authors><author><keyname>Woodward</keyname><forenames>Alexander</forenames></author><author><keyname>Froese</keyname><forenames>Tom</forenames></author><author><keyname>Ikegami</keyname><forenames>Takashi</forenames></author></authors><title>Neural coordination can be enhanced by occasional interruption of normal
  firing patterns: A self-optimizing spiking neural network model</title><categories>nlin.AO cs.NE q-bio.NC</categories><comments>22 pages, 6 figures; Neural Networks, in press</comments><msc-class>92B20</msc-class><acm-class>I.2.6; F.1.1; C.1.3; I.5.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The state space of a conventional Hopfield network typically exhibits many
different attractors of which only a small subset satisfy constraints between
neurons in a globally optimal fashion. It has recently been demonstrated that
combining Hebbian learning with occasional alterations of normal neural states
avoids this problem by means of self-organized enlargement of the best basins
of attraction. However, so far it is not clear to what extent this process of
self-optimization is also operative in real brains. Here we demonstrate that it
can be transferred to more biologically plausible neural networks by
implementing a self-optimizing spiking neural network model. In addition, by
using this spiking neural network to emulate a Hopfield network with Hebbian
learning, we attempt to make a connection between rate-based and temporal
coding based neural systems. Although further work is required to make this
model more realistic, it already suggests that the efficacy of the
self-optimizing process is independent from the simplifying assumptions of a
conventional Hopfield network. We also discuss natural and cultural processes
that could be responsible for occasional alteration of neural firing patterns
in actual brains
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0472</identifier>
 <datestamp>2014-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0472</id><created>2014-09-01</created><authors><author><keyname>Guo</keyname><forenames>Qian</forenames></author><author><keyname>Johansson</keyname><forenames>Thomas</forenames></author><author><keyname>L&#xf6;ndahl</keyname><forenames>Carl</forenames></author></authors><title>A New Algorithm for Solving Ring-LPN with a Reducible Polynomial</title><categories>cs.CR cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The LPN (Learning Parity with Noise) problem has recently proved to be of
great importance in cryptology. A special and very useful case is the RING-LPN
problem, which typically provides improved efficiency in the constructed
cryptographic primitive. We present a new algorithm for solving the RING-LPN
problem in the case when the polynomial used is reducible. It greatly
outperforms previous algorithms for solving this problem. Using the algorithm,
we can break the Lapin authentication protocol for the proposed instance using
a reducible polynomial, in about 2^70 bit operations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0473</identifier>
 <datestamp>2015-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0473</id><created>2014-09-01</created><updated>2015-04-24</updated><authors><author><keyname>Bahdanau</keyname><forenames>Dzmitry</forenames></author><author><keyname>Cho</keyname><forenames>Kyunghyun</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author></authors><title>Neural Machine Translation by Jointly Learning to Align and Translate</title><categories>cs.CL cs.LG cs.NE stat.ML</categories><comments>Accepted at ICLR 2015 as oral presentation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neural machine translation is a recently proposed approach to machine
translation. Unlike the traditional statistical machine translation, the neural
machine translation aims at building a single neural network that can be
jointly tuned to maximize the translation performance. The models proposed
recently for neural machine translation often belong to a family of
encoder-decoders and consists of an encoder that encodes a source sentence into
a fixed-length vector from which a decoder generates a translation. In this
paper, we conjecture that the use of a fixed-length vector is a bottleneck in
improving the performance of this basic encoder-decoder architecture, and
propose to extend this by allowing a model to automatically (soft-)search for
parts of a source sentence that are relevant to predicting a target word,
without having to form these parts as a hard segment explicitly. With this new
approach, we achieve a translation performance comparable to the existing
state-of-the-art phrase-based system on the task of English-to-French
translation. Furthermore, qualitative analysis reveals that the
(soft-)alignments found by the model agree well with our intuition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0491</identifier>
 <datestamp>2014-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0491</id><created>2014-09-01</created><authors><author><keyname>G&#xf6;dert</keyname><forenames>Winfried</forenames></author></authors><title>Facets and Typed Relations as Tools for Reasoning Processes in
  Information Retrieval</title><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Faceted arrangement of entities and typed relations for representing
different associations between the entities are established tools in knowledge
representation. In this paper, a proposal is being discussed combining both
tools to draw inferences along relational paths. This approach may yield new
benefit for information retrieval processes, especially when modeled for
heterogeneous environments in the Semantic Web. Faceted arrangement can be used
as a se-lection tool for the semantic knowledge modeled within the knowledge
repre-sentation. Typed relations between the entities of different facets can
be used as restrictions for selecting them across the facets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0494</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0494</id><created>2014-09-01</created><updated>2015-05-26</updated><authors><author><keyname>Aguerri</keyname><forenames>I&#xf1;aki Estella</forenames></author><author><keyname>G&#xfc;nd&#xfc;z</keyname><forenames>Deniz</forenames></author></authors><title>Distortion Exponent in MIMO Fading Channels with Time-Varying Source
  Side Information</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Transmission of a Gaussian source over a time-varying multiple-input
multiple-output (MIMO) channel is studied under strict delay constraints.
Availability of a correlated side information at the receiver is assumed, whose
quality, i.e., correlation with the source signal, also varies over time. A
block-fading model is considered for the states of the time-varying channel and
the time-varying side information; and perfect state information at the
receiver is assumed, while the transmitter knows only the statistics. The high
SNR performance, characterized by the \textit{distortion exponent}, is studied
for this joint source-channel coding problem. An upper bound is derived and
compared with lowers based on list decoding, hybrid digital-analog
transmission, as well as multi-layer schemes which transmit successive
refinements of the source, relying on progressive and superposed transmission
with list decoding. The optimal distortion exponent is characterized for the
single-input multiple-output (SIMO) and multiple-input single-output (MISO)
scenarios by showing that the distortion exponent achieved by multi-layer
superpositon encoding with joint decoding meets the proposed upper bound. In
the MIMO scenario, the optimal distortion exponent is characterized in the low
bandwidth ratio regime, and it is shown that the multi-layer superposition
encoding performs very close to the upper bound in the high bandwidth expansion
regime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0496</identifier>
 <datestamp>2014-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0496</id><created>2014-09-01</created><authors><author><keyname>Teutsch</keyname><forenames>Jason</forenames></author><author><keyname>Zimand</keyname><forenames>Marius</forenames></author></authors><title>On approximate decidability of minimal programs</title><categories>math.LO cs.CC cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An index $e$ in a numbering of partial-recursive functions is called minimal
if every lesser index computes a different function from $e$. Since the 1960's
it has been known that, in any reasonable programming language, no effective
procedure determines whether or not a given index is minimal. We investigate
whether the task of determining minimal indices can be solved in an approximate
sense. Our first question, regarding the set of minimal indices, is whether
there exists an algorithm which can correctly label 1 out of $k$ indices as
either minimal or non-minimal. Our second question, regarding the function
which computes minimal indices, is whether one can compute a short list of
candidate indices which includes a minimal index for a given program. We give
some negative results and leave the possibility of positive results as open
questions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0499</identifier>
 <datestamp>2014-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0499</id><created>2014-09-01</created><authors><author><keyname>Aulbach</keyname><forenames>Maximilian</forenames></author><author><keyname>Fink</keyname><forenames>Martin</forenames></author><author><keyname>Schuhmann</keyname><forenames>Julian</forenames></author><author><keyname>Wolff</keyname><forenames>Alexander</forenames></author></authors><title>Drawing Graphs within Restricted Area</title><categories>cs.CG cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of selecting a maximum-weight subgraph of a given graph
such that the subgraph can be drawn within a prescribed drawing area subject to
given non-uniform vertex sizes. We develop and analyze heuristics both for the
general (undirected) case and for the use case of (directed) calculation graphs
which are used to analyze the typical mistakes that high school students make
when transforming mathematical expressions in the process of calculating, for
example, sums of fractions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0507</identifier>
 <datestamp>2014-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0507</id><created>2014-09-01</created><authors><author><keyname>Starnini</keyname><forenames>Michele</forenames></author><author><keyname>Baronchelli</keyname><forenames>Andrea</forenames></author><author><keyname>Pastor-Satorras</keyname><forenames>Romualdo</forenames></author></authors><title>Model reproduces individual, group and collective dynamics of human
  contact networks</title><categories>physics.soc-ph cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Empirical data on the dynamics of human face-to-face interactions across a
variety of social venues have recently revealed a number of context-independent
structural and temporal properties of human contact networks. This universality
suggests that some basic mechanisms may be responsible for the unfolding of
human interactions in the physical space. Here we discuss a simple model that
reproduces the empirical distributions for the individual, group and collective
dynamics of face-to-face contact networks. The model describes agents that move
randomly in a two-dimensional space and tend to stop when meeting &quot;attractive&quot;
peers, and reproduces accurately the empirical distributions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0517</identifier>
 <datestamp>2014-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0517</id><created>2014-09-01</created><authors><author><keyname>Borhani-fard</keyname><forenames>Zeinab</forenames></author><author><keyname>Esmaeili</keyname><forenames>Leila</forenames></author><author><keyname>Minaei-Bidgoli</keyname><forenames>Behrouz</forenames></author><author><keyname>Nasiri</keyname><forenames>Mehdi</forenames></author></authors><title>Experiments on Data Preprocessing of Persian Blog Networks</title><categories>cs.SI physics.soc-ph</categories><comments>International Journal of Advanced Studies in Computer Science &amp;
  Engineering (IJASCSE)- 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social networks analysis and exploring is important for researchers,
sociologists, academics, and various businesses due to their information
potential. Because of the large volume, diversity, and the data growth rate in
web 2.0, some challenges have been made in these data analysis. Based on
definitions, weblogs are a form of social networking. So far, the majority of
studies and researches in the field of weblog networks analysis and exploring
their stored data have been based on international data sets. In this paper, a
framework for preprocessing and data analysis in weblog networks is presented
and the results of applying it on a Persian weblog network, as a case study,
are expressed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0526</identifier>
 <datestamp>2014-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0526</id><created>2014-08-30</created><authors><author><keyname>Grinkrug</keyname><forenames>Efim</forenames></author></authors><title>Dynamic Component Composition</title><categories>cs.SE</categories><comments>19 pages</comments><journal-ref>International Journal of Software Engineering &amp; Applications
  (IJSEA), Vol.5, No.4, July 2014, pp. 83 - 101</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an approach to dynamic component composition that
facilitates creating new composed components using existing ones at runtime and
without any code generation. The dynamic abilities are supported by extended
type notion and implementation based on additional superstructure provided with
its Java API and corresponding JavaBeans components. The new component
composition is performed by building the composed prototype object that can be
dynamically transformed into the new instantiable type (component). That
approach demonstrates interrelations between prototype-based and class-based
component-oriented programming. The component model proposed can be used when
implementing user-defined types in declarative languages for event-driven
applications programming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0547</identifier>
 <datestamp>2014-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0547</id><created>2014-09-01</created><authors><author><keyname>Smets</keyname><forenames>Bart</forenames></author></authors><title>Investigation on Demand Side Management Techniques in the Smart Grid
  using Game Theory and ICT Concepts</title><categories>cs.DC cs.GT</categories><comments>26 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates how concepts from game theory and ICT can contribute
to solve challenges in demand side management, an important concept in the
upcoming smart grid. Demand side management is about modifying the energy load
distribution on the demand side, for example in order to reduce peaks in energy
usage. This can be done by shifting energy demands where possible. We start
with describing a number of smart grid concepts and assumptions (smart meters,
pricing, appliance scheduling) and explain the advantages demand side
management has. After the introduction of game theoretic concepts, it becomes
possible to mathematically describe the demand side management problem. Next
step is to solve the mathematical formulation, and show how complex demand side
management becomes if the number of energy users increases. By means of
distributed ICT algorithms however, it is possible to still find a solution.
Based on existing literature, different algorithms are studied. Though results
in literature looked promising, several conclusions on convergence of the
algorithm in general, and convergence towards the most optimal results in
particular are challenged.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0553</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0553</id><created>2014-09-01</created><updated>2015-09-09</updated><authors><author><keyname>Haesaert</keyname><forenames>Sofie</forenames></author><author><keyname>Babuska</keyname><forenames>Robert</forenames></author><author><keyname>Abate</keyname><forenames>Alessandro</forenames></author></authors><title>Sampling-based Approximations with Quantitative Performance for the
  Probabilistic Reach-Avoid Problem over General Markov Processes</title><categories>cs.SY cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article deals with stochastic processes endowed with the Markov
(memoryless) property and evolving over general (uncountable) state spaces. The
models further depend on a non-deterministic quantity in the form of a control
input, which can be selected to affect the probabilistic dynamics. We address
the computation of maximal reach-avoid specifications, together with the
synthesis of the corresponding optimal controllers. The reach-avoid
specification deals with assessing the likelihood that any finite-horizon
trajectory of the model enters a given goal set, while avoiding a given set of
undesired states. This article newly provides an approximate computational
scheme for the reach-avoid specification based on the Fitted Value Iteration
algorithm, which hinges on random sample extractions, and gives a-priori
computable formal probabilistic bounds on the error made by the approximation
algorithm: as such, the output of the numerical scheme is quantitatively
assessed and thus meaningful for safety-critical applications. Furthermore, we
provide tighter probabilistic error bounds that are sample-based. The overall
computational scheme is put in relationship with alternative approximation
algorithms in the literature, and finally its performance is practically
assessed over a benchmark case study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0561</identifier>
 <datestamp>2015-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0561</id><created>2014-09-01</created><updated>2015-02-24</updated><authors><author><keyname>Khanzadi</keyname><forenames>M. Reza</forenames></author><author><keyname>Durisi</keyname><forenames>Giuseppe</forenames></author><author><keyname>Eriksson</keyname><forenames>Thomas</forenames></author></authors><title>Capacity of SIMO and MISO Phase-Noise Channels with Common/Separate
  Oscillators</title><categories>cs.IT math.IT</categories><comments>IEEE Transactions on Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In multiple antenna systems, phase noise due to instabilities of the
radio-frequency (RF) oscillators, acts differently depending on whether the RF
circuitries connected to each antenna are driven by separate (independent)
local oscillators (SLO) or by a common local oscillator (CLO). In this paper,
we investigate the high-SNR capacity of single-input multiple-output (SIMO) and
multiple-output single-input (MISO) phase-noise channels for both the CLO and
the SLO configurations.
  Our results show that the first-order term in the high-SNR capacity expansion
is the same for all scenarios (SIMO/MISO and SLO/CLO), and equal to $0.5\ln
(\rho)$, where $\rho$ stands for the SNR. On the contrary, the second-order
term, which we refer to as phase-noise number, turns out to be
scenario-dependent. For the SIMO case, the SLO configuration provides a
diversity gain, resulting in a larger phase-noise number than for the CLO
configuration. For the case of Wiener phase noise, a diversity gain of at least
$0.5 \ln(M)$ can be achieved, where $M$ is the number of receive antennas. For
the MISO, the CLO configuration yields a higher phase-noise number than the SLO
configuration. This is because with the CLO configuration one can obtain a
coherent-combining gain through maximum ratio transmission (a.k.a. conjugate
beamforming). This gain is unattainable with the SLO configuration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0562</identifier>
 <datestamp>2014-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0562</id><created>2014-08-29</created><authors><author><keyname>Zebenaya</keyname><forenames>M.</forenames></author><author><keyname>Boge</keyname><forenames>T.</forenames></author><author><keyname>Choukroun</keyname><forenames>D.</forenames></author></authors><title>Modeling, Stability Analysis, and Testing of a Hybrid Docking Simulator</title><categories>cs.OH</categories><comments>30 papges</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A hybrid docking simulator is a hardware-in-the-loop (HIL) simulator that
includes a hardware element within a numerical simulation loop. One of the
goals of performing a HIL simulation at the European Proximity Operation
Simulator (EPOS) is the verification and validation of the docking phase in an
on-orbit servicing mission.....
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0575</identifier>
 <datestamp>2015-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0575</id><created>2014-09-01</created><updated>2015-01-29</updated><authors><author><keyname>Russakovsky</keyname><forenames>Olga</forenames></author><author><keyname>Deng</keyname><forenames>Jia</forenames></author><author><keyname>Su</keyname><forenames>Hao</forenames></author><author><keyname>Krause</keyname><forenames>Jonathan</forenames></author><author><keyname>Satheesh</keyname><forenames>Sanjeev</forenames></author><author><keyname>Ma</keyname><forenames>Sean</forenames></author><author><keyname>Huang</keyname><forenames>Zhiheng</forenames></author><author><keyname>Karpathy</keyname><forenames>Andrej</forenames></author><author><keyname>Khosla</keyname><forenames>Aditya</forenames></author><author><keyname>Bernstein</keyname><forenames>Michael</forenames></author><author><keyname>Berg</keyname><forenames>Alexander C.</forenames></author><author><keyname>Fei-Fei</keyname><forenames>Li</forenames></author></authors><title>ImageNet Large Scale Visual Recognition Challenge</title><categories>cs.CV</categories><comments>43 pages, 16 figures. v3 includes additional comparisons with PASCAL
  VOC (per-category comparisons in Table 3, distribution of localization
  difficulty in Fig 16), a list of queries used for obtaining object detection
  images (Appendix C), and some additional references</comments><acm-class>I.4.8; I.5.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ImageNet Large Scale Visual Recognition Challenge is a benchmark in
object category classification and detection on hundreds of object categories
and millions of images. The challenge has been run annually from 2010 to
present, attracting participation from more than fifty institutions.
  This paper describes the creation of this benchmark dataset and the advances
in object recognition that have been possible as a result. We discuss the
challenges of collecting large-scale ground truth annotation, highlight key
breakthroughs in categorical object recognition, provide a detailed analysis of
the current state of the field of large-scale image classification and object
detection, and compare the state-of-the-art computer vision accuracy with human
accuracy. We conclude with lessons learned in the five years of the challenge,
and propose future directions and improvements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0582</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0582</id><created>2014-09-01</created><updated>2015-06-02</updated><authors><author><keyname>McIver</keyname><forenames>Annabelle</forenames></author><author><keyname>Rabehaja</keyname><forenames>Tahiry</forenames></author><author><keyname>Struth</keyname><forenames>Georg</forenames></author></authors><title>Probabilistic Rely-guarantee Calculus</title><categories>cs.LO</categories><comments>Preprint submitted to TCS-QAPL</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Jones' rely-guarantee calculus for shared variable concurrency is extended to
include probabilistic behaviours. We use an algebraic approach which combines
and adapts probabilistic Kleene algebras with concurrent Kleene algebra.
Soundness of the algebra is shown relative to a general probabilistic event
structure semantics. The main contribution of this paper is a collection of
rely-guarantee rules built on top of that semantics. In particular, we show how
to obtain bounds on probabilities by deriving rely-guarantee rules within the
true-concurrent denotational semantics. The use of these rules is illustrated
by a detailed verification of a simple probabilistic concurrent program: a
faulty Eratosthenes sieve.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0584</identifier>
 <datestamp>2014-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0584</id><created>2014-09-01</created><authors><author><keyname>Kjos-Hanssen</keyname><forenames>Bj&#xf8;rn</forenames></author></authors><title>Kolmogorov structure functions for automatic complexity in computational
  statistics</title><categories>cs.FL math.LO</categories><comments>To appear in the Lecture Notes in Computer Science proceedings of the
  8th International Conference on Combinatorial Optimization and Applications
  (COCOA 2014)</comments><msc-class>03D</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a finite word $w$ of length $n$ and a class of finite automata $\mathcal
A$, we study the Kolmogorov structure function $h_w$ for automatic complexity
restricted to $\mathcal A$. We propose an approach to computational statistics
based on the minimum $p$-value of $h_w(m)$ over $0\le m\le n$. When $\mathcal
A$ is the class of all finite automata we give some upper bounds for $h_w$.
When $\mathcal A$ consists of automata that detect several success runs in $w$,
we give efficient algorithms to compute $h_w$. When $\mathcal A$ consists of
automata that detect one success run, we moreover give an efficient algorithm
to compute the $p$-values.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0585</identifier>
 <datestamp>2014-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0585</id><created>2014-09-01</created><authors><author><keyname>Yao</keyname><forenames>Li</forenames></author><author><keyname>Ozair</keyname><forenames>Sherjil</forenames></author><author><keyname>Cho</keyname><forenames>Kyunghyun</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author></authors><title>On the Equivalence Between Deep NADE and Generative Stochastic Networks</title><categories>stat.ML cs.LG</categories><comments>ECML/PKDD 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neural Autoregressive Distribution Estimators (NADEs) have recently been
shown as successful alternatives for modeling high dimensional multimodal
distributions. One issue associated with NADEs is that they rely on a
particular order of factorization for $P(\mathbf{x})$. This issue has been
recently addressed by a variant of NADE called Orderless NADEs and its deeper
version, Deep Orderless NADE. Orderless NADEs are trained based on a criterion
that stochastically maximizes $P(\mathbf{x})$ with all possible orders of
factorizations. Unfortunately, ancestral sampling from deep NADE is very
expensive, corresponding to running through a neural net separately predicting
each of the visible variables given some others. This work makes a connection
between this criterion and the training criterion for Generative Stochastic
Networks (GSNs). It shows that training NADEs in this way also trains a GSN,
which defines a Markov chain associated with the NADE model. Based on this
connection, we show an alternative way to sample from a trained Orderless NADE
that allows to trade-off computing time and quality of the samples: a 3 to
10-fold speedup (taking into account the waste due to correlations between
consecutive samples of the chain) can be obtained without noticeably reducing
the quality of the samples. This is achieved using a novel sampling procedure
for GSNs called annealed GSN sampling, similar to tempering methods that
combines fast mixing (obtained thanks to steps at high noise levels) with
accurate samples (obtained thanks to steps at low noise levels).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0586</identifier>
 <datestamp>2014-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0586</id><created>2014-09-01</created><authors><author><keyname>Wu</keyname><forenames>Hui</forenames></author><author><keyname>Zhang</keyname><forenames>Zhaoyang</forenames></author><author><keyname>Zhang</keyname><forenames>Huazi</forenames></author></authors><title>Faster Information Propagation on Highways: a Virtual MIMO Approach</title><categories>cs.IT cs.NI math.IT</categories><comments>IEEE 2014 Global Telecommunications Conference (GLOBECOM 2014) -
  Communication Theory Symposium</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In vehicular communications, traffic-related information should be spread
over the network as quickly as possible to maintain a safe and reliable
transportation system. This motivates us to develop more efficient information
propagation schemes. In this paper, we propose a novel cluster-based
cooperative information forwarding scheme, in which the vehicles
opportunistically form virtual antenna arrays to boost one-hop transmission
range and therefore accelerate information propagation along the highway. Both
closed-form results of the transmission range gain and the improved Information
Propagation Speed (IPS) are derived and verified by simulations. It is observed
that the proposed scheme demonstrates the most significant IPS gain in moderate
traffic scenarios, whereas too dense or too sparse vehicle density results in
less gain. Moreover, it is also shown that increased mobility offers more
contact opportunities and thus facilitates information propagation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0590</identifier>
 <datestamp>2015-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0590</id><created>2014-09-01</created><updated>2014-11-22</updated><authors><author><keyname>Adami</keyname><forenames>Christoph</forenames></author></authors><title>Information-theoretic considerations concerning the origin of life</title><categories>q-bio.PE cs.IT math.IT nlin.AO q-bio.BM</categories><comments>10 pages, one figure. Expanded discussion of experiments with
  biopolymers</comments><journal-ref>Origins of Life and Evolution of the Bioshperes 45 (2015) 9439</journal-ref><doi>10.1007/s11084-015-9439-0</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Research investigating the origins of life usually focuses on exploring
possible life-bearing chemistries in the pre-biotic Earth, or else on synthetic
approaches. Little work has been done exploring fundamental issues concerning
the spontaneous emergence of life using only concepts (such as information and
evolution) that are divorced from any particular chemistry. Here, I advocate
studying the probability of spontaneous molecular self-replication as a
function of the information contained in the replicator, and the environmental
conditions that might enable this emergence. I show that (under certain
simplifying assumptions) the probability to discover a self-replicator by
chance depends exponentially on the rate of formation of the monomers. If the
rate at which monomers are formed is somewhat similar to the rate at which they
would occur in a self-replicating polymer, the likelihood to discover such a
replicator by chance is increased by many orders of magnitude. I document such
an increase in searches for a self-replicator within the digital life system
avida
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0597</identifier>
 <datestamp>2014-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0597</id><created>2014-09-01</created><authors><author><keyname>Goodrich</keyname><forenames>Michael T.</forenames></author><author><keyname>Simons</keyname><forenames>Joseph A.</forenames></author></authors><title>Data-Oblivious Graph Algorithms in Outsourced External Memory</title><categories>cs.DS</categories><comments>20 pages</comments><msc-class>68</msc-class><acm-class>F.2.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by privacy preservation for outsourced data, data-oblivious
external memory is a computational framework where a client performs
computations on data stored at a semi-trusted server in a way that does not
reveal her data to the server. This approach facilitates collaboration and
reliability over traditional frameworks, and it provides privacy protection,
even though the server has full access to the data and he can monitor how it is
accessed by the client. The challenge is that even if data is encrypted, the
server can learn information based on the client data access pattern; hence,
access patterns must also be obfuscated. We investigate privacy-preserving
algorithms for outsourced external memory that are based on the use of
data-oblivious algorithms, that is, algorithms where each possible sequence of
data accesses is independent of the data values. We give new efficient
data-oblivious algorithms in the outsourced external memory model for a number
of fundamental graph problems. Our results include new data-oblivious
external-memory methods for constructing minimum spanning trees, performing
various traversals on rooted trees, answering least common ancestor queries on
trees, computing biconnected components, and forming open ear decompositions.
None of our algorithms make use of constant-time random oracles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0599</identifier>
 <datestamp>2014-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0599</id><created>2014-09-01</created><authors><author><keyname>Sun</keyname><forenames>Qifu Tyler</forenames></author><author><keyname>Li</keyname><forenames>Shuo-Yen Robert</forenames></author></authors><title>On Decoding of DVR-Based Linear Network Codes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The conventional theory of linear network coding (LNC) is only over acyclic
networks. Convolutional network coding (CNC) applies to all networks. It is
also a form of LNC, but the linearity is w.r.t. the ring of rational power
series rather than the field of data symbols. CNC has been generalized to LNC
w.r.t. any discrete valuation ring (DVR) in order for flexibility in
applications. For a causal DVR-based code, all possible source-generated
messages form a free module, while incoming coding vectors to a receiver span
the \emph{received submodule}. An existing \emph{time-invariant decoding}
algorithm is at a delay equal to the largest valuation among all invariant
factors of the received submodule. This intrinsic algebraic attribute is herein
proved to be the optimal decoding delay. Meanwhile, \emph{time-variant
decoding} is formulated. The meaning of time-invariant decoding delay gets a
new interpretation through being a special case of the time-variant
counterpart. The optimal delay turns out to be the same for time-variant
decoding, but the decoding algorithm is more flexible in terms of decodability
check and decoding matrix design. All results apply, in particular, to CNC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0602</identifier>
 <datestamp>2014-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0602</id><created>2014-09-01</created><authors><author><keyname>Zhu</keyname><forenames>Shizhan</forenames></author><author><keyname>Li</keyname><forenames>Cheng</forenames></author><author><keyname>Loy</keyname><forenames>Chen Change</forenames></author><author><keyname>Tang</keyname><forenames>Xiaoou</forenames></author></authors><title>Transferring Landmark Annotations for Cross-Dataset Face Alignment</title><categories>cs.CV</categories><comments>Shizhan Zhu and Cheng Li share equal contributions</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Dataset bias is a well known problem in object recognition domain. This
issue, nonetheless, is rarely explored in face alignment research. In this
study, we show that dataset plays an integral part of face alignment
performance. Specifically, owing to face alignment dataset bias, training on
one database and testing on another or unseen domain would lead to poor
performance. Creating an unbiased dataset through combining various existing
databases, however, is non-trivial as one has to exhaustively re-label the
landmarks for standardisation. In this work, we propose a simple and yet
effective method to bridge the disparate annotation spaces between databases,
making datasets fusion possible. We show extensive results on combining various
popular databases (LFW, AFLW, LFPW, HELEN) for improved cross-dataset and
unseen data alignment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0607</identifier>
 <datestamp>2014-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0607</id><created>2014-09-02</created><authors><author><keyname>Annamalai</keyname><forenames>Chidambaram</forenames></author><author><keyname>Kalaitzis</keyname><forenames>Christos</forenames></author><author><keyname>Svensson</keyname><forenames>Ola</forenames></author></authors><title>Combinatorial Algorithm for Restricted Max-Min Fair Allocation</title><categories>cs.DS</categories><comments>22 pages, 1 figure, submitted to SODA 2015</comments><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the basic allocation problem of assigning resources to players so as
to maximize fairness. This is one of the few natural problems that enjoys the
intriguing status of having a better estimation algorithm than approximation
algorithm. Indeed, a certain configuration-LP can be used to estimate the value
of the optimal allocation to within a factor of $4 + {\epsilon}$. In contrast,
however, the best known approximation algorithm for the problem has an
unspecified large constant guarantee.
  In this paper we significantly narrow this gap by giving a $13$-approximation
algorithm for the problem. Our approach develops a local search technique
introduced by Haxell [Hax95] for hypergraph matchings, and later used in this
context by Asadpour, Feige, and Saberi [AFS12]. For our local search procedure
to terminate in polynomial time, we introduce several new ideas such as lazy
updates and greedy players. Besides the improved approximation guarantee, the
highlight of our approach is that it is purely combinatorial and uses the
configuration-LP only in the analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0610</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0610</id><created>2014-09-02</created><updated>2016-01-12</updated><authors><author><keyname>Horlemann-Trautmann</keyname><forenames>Anna-Lena</forenames></author></authors><title>Message Encoding and Retrieval for Spread and Cyclic Orbit Codes</title><categories>cs.IT math.IT</categories><comments>This is an extension of the previous work &quot;Message Encoding for
  Spread and Orbit Codes&quot;, which appeared in the Proceedings of the 2014 IEEE
  International Symposium on Information Theory 2014 (Honolulu, USA)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spread codes and cyclic orbit codes are special families of constant
dimension subspace codes. These codes have been well-studied for their error
correction capability, transmission rate and decoding methods, but the question
of how to encode and retrieve messages has not been investigated. In this work
we show how a message set of consecutive integers can be encoded and retrieved
for these two code families.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0612</identifier>
 <datestamp>2014-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0612</id><created>2014-09-02</created><authors><author><keyname>Long</keyname><forenames>Ying</forenames></author><author><keyname>Shen</keyname><forenames>Zhenjiang</forenames></author></authors><title>Population spatialization and synthesis with open data</title><categories>cs.OH</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Individuals together with their locations &amp; attributes are essential to feed
micro-level applied urban models (for example, spatial micro-simulation and
agent-based modeling) for policy evaluation. Existed studies on population
spatialization and population synthesis are generally separated. In developing
countries like China, population distribution in a fine scale, as the input for
population synthesis, is not universally available. With the open-government
initiatives in China and the emerging Web 2.0 techniques, more and more open
data are becoming achievable. In this paper, we propose an automatic process
using open data for population spatialization and synthesis. Specifically, the
road network in OpenStreetMap is used to identify and delineate parcel
geometries, while crowd-sourced POIs are gathered to infer urban parcels with a
vector cellular automata model. Housing-related online Check-in records are
then applied to distinguish residential parcels from all of the identified
urban parcels. Finally the published census data, in which the sub-district
level of attributes distribution and relationships are available, is used for
synthesizing population attributes with a previously developed tool Agenter
(Long and Shen, 2013). The results are validated with ground truth
manually-prepared dataset by planners from Beijing Institute of City Planning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0650</identifier>
 <datestamp>2014-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0650</id><created>2014-09-02</created><authors><author><keyname>Furma&#x144;czyk</keyname><forenames>Hanna</forenames></author><author><keyname>Kubale</keyname><forenames>Marek</forenames></author></authors><title>Equitable coloring of corona products of cubic graphs is harder than
  ordinary coloring</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A graph is equitably $k$-colorable if its vertices can be partitioned into
$k$ independent sets in such a way that the number of vertices in any two sets
differ by at most one. The smallest $k$ for which such a coloring exists is
known as the \emph{equitable chromatic number} of $G$ and it is denoted by
$\chi_{=}(G)$. In this paper the problem of determinig $\chi_=$ for coronas of
cubic graphs is studied. Although the problem of ordinary coloring of coronas
of cubic graphs is solvable in polynomial time, the problem of equitable
coloring becomes NP-hard for these graphs. We provide polynomially solvable
cases of coronas of cubic graphs and prove the NP-hardness in a general case.
As a by-product we obtain a simple linear time algorithm for equitable coloring
of such graphs which uses $\chi_=(G)$ or $\chi_=(G)+1$ colors. Our algorithm is
best possible, unless $P=NP$. Consequently, cubical coronas seem to be the only
known class of graphs for which equitable coloring is harder than ordinary
coloring.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0651</identifier>
 <datestamp>2014-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0651</id><created>2014-09-02</created><authors><author><keyname>Pal</keyname><forenames>Koninika</forenames></author><author><keyname>Michel</keyname><forenames>Sebastian</forenames></author></authors><title>An LSH Index for Computing Kendall's Tau over Top-k Lists</title><categories>cs.DB</categories><comments>6 pages, 8 subfigures, presented in Seventeenth International
  Workshop on the Web and Databases (WebDB 2014) co-located with ACM SIGMOD2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of similarity search within a set of top-k lists
under the Kendall's Tau distance function. This distance describes how related
two rankings are in terms of concordantly and discordantly ordered items. As
top-k lists are usually very short compared to the global domain of possible
items to be ranked, creating an inverted index to look up overlapping lists is
possible but does not capture tight enough the similarity measure. In this
work, we investigate locality sensitive hashing schemes for the Kendall's Tau
distance and evaluate the proposed methods using two real-world datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0657</identifier>
 <datestamp>2014-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0657</id><created>2014-09-02</created><authors><author><keyname>Yu</keyname><forenames>Lei</forenames></author><author><keyname>Zhang</keyname><forenames>Tao</forenames></author><author><keyname>Siebers</keyname><forenames>Peer-Olaf</forenames></author><author><keyname>Aickelin</keyname><forenames>Uwe</forenames></author></authors><title>Modelling Electrical Car Diffusion Based on Agents</title><categories>cs.MA cs.CY</categories><journal-ref>International Journal of Digital Content Technology and its
  Applications (JDCTA), 6 (19), pp. 424-431, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Replacing traditional fossil fuel vehicles with innovative zero-emission
vehicles for the transport in ci ties is one of the major tactics to achieve
the UK government 2020 target of cutting emission. We are developing an
agent-based simulation model to study the possible impact of different
governmental interventions on the diffusion of such vehicles. Options that
could be studied with our what-if analysis to include things like car parking
charges, price of electrical car, energy awareness and word of mouth. In this
paper we present a first case study related to the introduction of a new car
park charging scheme at the University of Nottingham. We have developed an
agent based model to simulate theimpact of different car parking rates and
other incentives on the uptake of electrical cars. The goal of this case study
is to demonstrate the usefulness of agent-based modelling and simulation for
such investigations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0658</identifier>
 <datestamp>2014-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0658</id><created>2014-09-02</created><authors><author><keyname>liu</keyname><forenames>Yihui</forenames></author><author><keyname>Aickelin</keyname><forenames>Uwe</forenames></author></authors><title>Detect Adverse Drug Reactions for Drug Aspirin</title><categories>cs.CE</categories><comments>IEEE fifth International Conference on Advanced Computational
  Intelligence (ICACI), pp. 234-237, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Adverse drug reaction (ADR) is widely concerned for public health issue. In
this study we propose an original approach to detect the ADRs using feature
matrix and feature selection. The experiments are carried out on the drug
Aspirin. Major side effects for the drug are detected and better performance is
achieved compared to other computerized methods. The detected ADRs are based on
the computerized method, further investigation is needed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0669</identifier>
 <datestamp>2014-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0669</id><created>2014-09-02</created><authors><author><keyname>Rupp</keyname><forenames>Karl</forenames></author><author><keyname>Tillet</keyname><forenames>Philippe</forenames></author><author><keyname>Rudolf</keyname><forenames>Florian</forenames></author><author><keyname>Weinbub</keyname><forenames>Josef</forenames></author><author><keyname>Grasser</keyname><forenames>Tibor</forenames></author><author><keyname>J&#xfc;ngel</keyname><forenames>Ansgar</forenames></author></authors><title>Performance Portability Study of Linear Algebra Kernels in OpenCL</title><categories>cs.MS cs.DC cs.PF</categories><comments>11 pages, 8 figures, 2 tables, International Workshop on OpenCL 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The performance portability of OpenCL kernel implementations for common
memory bandwidth limited linear algebra operations across different hardware
generations of the same vendor as well as across vendors is studied. Certain
combinations of kernel implementations and work sizes are found to exhibit good
performance across compute kernels, hardware generations, and, to a lesser
degree, vendors. As a consequence, it is demonstrated that the optimization of
a single kernel is often sufficient to obtain good performance for a large
class of more complicated operations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0682</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0682</id><created>2014-09-02</created><updated>2014-12-15</updated><authors><author><keyname>Alexandropoulos</keyname><forenames>George C.</forenames></author><author><keyname>Barousis</keyname><forenames>Vlasis I.</forenames></author><author><keyname>Papadias</keyname><forenames>Constantinos B.</forenames></author></authors><title>Precoding for Multiuser MIMO Systems with Single-Fed Parasitic Antenna
  Arrays</title><categories>cs.IT math.IT</categories><comments>6 pages, 3 figures, IEEE GLOBECOM 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Transmitter (TX) cooperation at various levels has been shown to increase the
sum throughput of multiuser multiple-input multiple-output (MIMO) systems. In
this paper we consider a K-user MIMO system where TXs have only global channel
state knowledge. It has been theoretically shown that interference alignment
(IA) achieves the K/2 degrees of freedom of this K-user MIMO interference
channel. However, results on IA and all proposed transceiver techniques for
this channel up to date, assume conventional antenna arrays at the transceivers
with multiple radio-frequency (RF) chains, each connected to a different
antenna element. To reduce the consequent hardware burden and power dissipation
imposed by such arrays, we propose in this paper the utilization of compact
single-RF electronically steerable parasitic (passive) array radiators (ESPARs)
at the cooperating TXs. A signal model capable of capturing the characteristics
of the considered antenna arrays is first described and then a general
precoding design methodology for the tunable parasitic loads at the TXs' ESPARs
is introduced. Specific precoding techniques and an indicative ESPAR design are
presented for a 3-user 2x2 MIMO system with one ESPAR TX, and the obtained
performance evaluation results show that the gains of TX cooperation are still
feasible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0685</identifier>
 <datestamp>2014-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0685</id><created>2014-09-02</created><updated>2014-09-19</updated><authors><author><keyname>Zhu</keyname><forenames>Feiyun</forenames></author><author><keyname>Wang</keyname><forenames>Ying</forenames></author><author><keyname>Fan</keyname><forenames>Bin</forenames></author><author><keyname>Meng</keyname><forenames>Gaofeng</forenames></author><author><keyname>Pan</keyname><forenames>Chunhong</forenames></author></authors><title>Effective Spectral Unmixing via Robust Representation and Learning-based
  Sparsity</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hyperspectral unmixing (HU) plays a fundamental role in a wide range of
hyperspectral applications. It is still challenging due to the common presence
of outlier channels and the large solution space. To address the above two
issues, we propose a novel model by emphasizing both robust representation and
learning-based sparsity. Specifically, we apply the $\ell_{2,1}$-norm to
measure the representation error, preventing outlier channels from dominating
our objective. In this way, the side effects of outlier channels are greatly
relieved. Besides, we observe that the mixed level of each pixel varies over
image grids. Based on this observation, we exploit a learning-based sparsity
method to simultaneously learn the HU results and a sparse guidance map. Via
this guidance map, the sparsity constraint in the $\ell_{p}\!\left(\!0\!&lt;\!
p\!\leq\!1\right)$-norm is adaptively imposed according to the learnt mixed
level of each pixel. Compared with state-of-the-art methods, our model is
better suited to the real situation, thus expected to achieve better HU
results. The resulted objective is highly non-convex and non-smooth, and so it
is hard to optimize. As a profound theoretical contribution, we propose an
efficient algorithm to solve it. Meanwhile, the convergence proof and the
computational complexity analysis are systematically provided. Extensive
evaluations verify that our method is highly promising for the HU task---it
achieves very accurate guidance maps and much better HU results compared with
state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0697</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0697</id><created>2014-09-02</created><updated>2015-12-09</updated><authors><author><keyname>Chen</keyname><forenames>Bowei</forenames></author><author><keyname>Wang</keyname><forenames>Jun</forenames></author></authors><title>A lattice framework for pricing display advertisement options with the
  stochastic volatility underlying model</title><categories>cs.GT q-fin.CP</categories><comments>Bowei Chen and Jun Wang. A lattice framework for pricing display
  advertisement options with the stochastic volatility underlying model.
  Electronic Commerce Research and Applications, 2015, Volume 14, Issue 6,
  pages 465-479, ISSN: 1567-4223</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Advertisement (abbreviated ad) options are a recent development in online
advertising. Simply, an ad option is a first look contract in which a publisher
or search engine grants an advertiser a right but not obligation to enter into
transactions to purchase impressions or clicks from a specific ad slot at a
pre-specified price on a specific delivery date. Such a structure provides
advertisers with more flexibility of their guaranteed deliveries. The valuation
of ad options is an important topic and previous studies on ad options pricing
have been mostly restricted to the situations where the underlying prices
follow a geometric Brownian motion (GBM). This assumption is reasonable for
sponsored search; however, some studies have also indicated that it is not
valid for display advertising. In this paper, we address this issue by
employing a stochastic volatility (SV) model and discuss a lattice framework to
approximate the proposed SV model in option pricing. Our developments are
validated by experiments with real advertising data: (i) we find that the SV
model has a better fitness over the GBM model; (ii) we validate the proposed
lattice model via two sequential Monte Carlo simulation methods; (iii) we
demonstrate that advertisers are able to flexibly manage their guaranteed
deliveries by using the proposed options, and publishers can have an increased
revenue when some of their inventories are sold via ad options.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0703</identifier>
 <datestamp>2015-03-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0703</id><created>2014-08-29</created><updated>2015-03-29</updated><authors><author><keyname>Guinea</keyname><forenames>Alejandro Sanchez</forenames></author></authors><title>On computable abstractions (a conceptual introduction)</title><categories>cs.AI</categories><comments>17 pages; clearer and more precise motivation; clearer concepts
  presented; review on related works added</comments><acm-class>I.2.m; I.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces abstractions that are meaningful for computers and that
can be built and used according to computers' own criteria, i.e., computable
abstractions. It is analyzed how abstractions can be seen to serve as the
building blocks for the creation of one own's understanding of things, which is
essential in performing intellectual tasks. Thus, abstractional machines are
defined, which following a mechanical process can, based on computable
abstractions, build and use their own understanding of things. Abstractional
machines are illustrated through an example that outlines their application to
the task of natural language processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0706</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0706</id><created>2014-09-02</created><updated>2014-09-13</updated><authors><author><keyname>Zhong</keyname><forenames>Shiyan</forenames></author></authors><title>Efficient Scheme for Active Particle Selection in N-body Simulations</title><categories>cs.DS</categories><comments>8 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an efficient method for active particle selection, working with
Hermite Individual Time Steps (HITS) scheme in direct N-body simulation code
$\varphi$GRAPE. For a simulation with $N$ particles, this method can reduce the
computation complexity of active particle selection, from $O(N\cdot N_{step})$
to $O(\overline{N_{act}}\cdot N_{step})$, where $\overline{N_{act}}$ is the
average active particle number in every time step which is much smaller than
$N$ and $N_{step}$ is the total time steps integrated during the simulation.
Thus can save a lot of time spent on active particle selection part, especially
in the case of low $\overline{N_{act}}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0718</identifier>
 <datestamp>2014-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0718</id><created>2014-09-02</created><authors><author><keyname>Dent</keyname><forenames>Ian</forenames></author><author><keyname>Craig</keyname><forenames>Tony</forenames></author><author><keyname>Aickelin</keyname><forenames>Uwe</forenames></author><author><keyname>Rodden</keyname><forenames>Tom</forenames></author></authors><title>An Approach for Assessing Clustering of Households by Electricity Usage</title><categories>cs.CE</categories><comments>UKCI 2012, the 12th Annual Workshop on Computational Intelligence,
  Heriot-Watt University, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How a household varies their regular usage of electricity is useful
information for organisations to allow accurate targeting of behaviour
modification initiatives with the aim of improving the overall efficiency of
the electricity network. The variability of regular activities in a household
is one possible indication of that household's willingness to accept incentives
to change their behaviour.
  An approach is presented for identifying a way of representing the
variability of a household's behaviour and developing an efficient way of
clustering the households, using these measures of variability, into a few,
usable groupings.
  To evaluate the effectiveness of the variability measures, a number of
cluster validity indexes are explored with regard to how the indexes vary with
the number of clusters, the number of attributes, and the quality of the
attributes. The Cluster Dispersion Indicator (CDI) and the Davies-Boulden
Indicator (DBI) are selected for future work developing various indicators of
household behaviour variability.
  The approach is tested using data from 180 UK households monitored for over a
year at a sampling interval of 5 minutes. Data is taken from the evening peak
electricity usage period of 4pm to 8pm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0731</identifier>
 <datestamp>2014-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0731</id><created>2014-09-02</created><authors><author><keyname>Kiero&#x144;ski</keyname><forenames>Emanuel</forenames></author><author><keyname>Kuusisto</keyname><forenames>Antti</forenames></author></authors><title>Complexity and Expressivity of Uniform One-Dimensional Fragment with
  Equality</title><categories>math.LO cs.LO</categories><comments>preprint</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Uniform one-dimensional fragment UF1^= is a formalism obtained from
first-order logic by limiting quantification to applications of blocks of
existential (universal) quantifiers such that at most one variable remains free
in the quantified formula. The fragment is closed under Boolean operations, but
additional restrictions (called uniformity conditions) apply to combinations of
atomic formulas with two or more variables. The fragment can be seen as a
canonical generalization of two-variable logic, defined in order to be able to
deal with relations of arbitrary arities. The fragment was introduced recently,
and it was shown that the satisfiability problem of the equality-free fragment
of UF1^= is decidable. In this article we establish that the satisfiability and
finite satisfiability problems of UF1^= are NEXPTIME-complete. We also show
that the corresponding problems for the extension of UF1^= with counting
quantifiers are undecidable. In addition to decidability questions, we compare
the expressivities of UF1^= and two-variable logic with counting quantifiers
FOC^2. We show that while the logics are incomparable in general, UF1^= is
strictly contained in FOC^2 when attention is restricted to vocabularies with
the arity bound two.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0736</identifier>
 <datestamp>2014-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0736</id><created>2014-09-02</created><authors><author><keyname>Tazi</keyname><forenames>Fatima Zahra</forenames></author><author><keyname>Thibeault</keyname><forenames>Claude</forenames></author><author><keyname>Savaria</keyname><forenames>Yvon</forenames></author><author><keyname>Pichette</keyname><forenames>Simon</forenames></author><author><keyname>Audet</keyname><forenames>Yves</forenames></author></authors><title>On Delay Faults Affecting I/O Blocks of an SRAM-Based FPGA Due to
  Ionizing Radiations</title><categories>physics.space-ph cs.AR physics.pop-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Experimental means to characterize delay faults induced by bit flips and SEUs
in I/O blocks of SRAM-based FPGAs are proposed. A delay fault up to 6.2ns
sensitized by an events chain is reported.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0742</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0742</id><created>2014-09-02</created><updated>2015-08-10</updated><authors><author><keyname>Engels</keyname><forenames>Christian</forenames></author><author><keyname>Rao</keyname><forenames>B. V. Raghavendra</forenames></author></authors><title>New Algorithms and Hard Instances for Non-Commutative Computation</title><categories>cs.CC</categories><comments>Submitted to a conference</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Motivated by the recent developments on the complexity of
non-com\-mu\-ta\-tive determinant and permanent [Chien et al.\ STOC 2011,
Bl\&quot;aser ICALP 2013, Gentry CCC 2014] we attempt at obtaining a tight
characterization of hard instances of non-commutative permanent.
  We show that computing Cayley permanent and determinant on weight\-ed
adjacency matrices of graphs of component size six is $\#{\sf P}$ complete on
algebras that contain $2\times 2$ matrices and the permutation group $S_3$.
Also, we prove a lower bound of $2^{\Omega(n)}$ on the size of branching
programs computing the Cayley permanent on adjacency matrices of graphs with
component size bounded by two. Further, we observe that the lower bound holds
for almost all graphs of component size two.
  On the positive side, we show that the Cayley permanent on graphs of
component size $c$ can be computed in time $n^{c{\sf poly}(t)}$, where $t$ is a
parameter depending on the labels of the vertices.
  Finally, we exhibit polynomials that are equivalent to the Cayley permanent
polynomial but are easy to compute over commutative domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0745</identifier>
 <datestamp>2014-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0745</id><created>2014-09-02</created><authors><author><keyname>Zhang</keyname><forenames>Hongyang</forenames></author><author><keyname>Zamar</keyname><forenames>Ruben H.</forenames></author></authors><title>A natural framework for sparse hierarchical clustering</title><categories>stat.ML cs.LG</categories><comments>22 pages, 9 figures</comments><msc-class>62H30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There has been a surge in the number of large and flat data sets - data sets
containing a large number of features and a relatively small number of
observations - due to the growing ability to collect and store information in
medical research and other fi?elds. Hierarchical clustering is a widely used
clustering tool. In hierarchical clustering, large and flat data sets may allow
for a better coverage of clustering features (features that help explain the
true underlying clusters) but, such data sets usually include a large fraction
of noise features (non-clustering features) that may hide the underlying
clusters. Witten and Tibshirani (2010) proposed a sparse hierarchical
clustering framework to cluster the observations using an adaptively chosen
subset of the features, however, we show that this framework has some
limitations when the data sets contain clustering features with complex
structure. In this paper, another sparse hierarchical clustering (SHC)
framework is proposed. We show that, using simulation studies and real data
examples, the proposed framework produces superior feature selection and
clustering performance comparing to the classical (of-the-shelf) hierarchical
clustering and the existing sparse hierarchical clustering framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0748</identifier>
 <datestamp>2014-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0748</id><created>2014-09-02</created><authors><author><keyname>Reps</keyname><forenames>Jenna</forenames></author><author><keyname>Garibaldi</keyname><forenames>Jonathan M.</forenames></author><author><keyname>Aickelin</keyname><forenames>Uwe</forenames></author><author><keyname>Soria</keyname><forenames>Daniele</forenames></author><author><keyname>Gibson</keyname><forenames>Jack</forenames></author><author><keyname>Hubbard</keyname><forenames>Richard</forenames></author></authors><title>Comparison of algorithms that detect drug side effects using electronic
  healthcare databases</title><categories>cs.LG cs.CE</categories><comments>Soft Computing, 17(12) pp. 2381-2397, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The electronic healthcare databases are starting to become more readily
available and are thought to have excellent potential for generating adverse
drug reaction signals. The Health Improvement Network (THIN) database is an
electronic healthcare database containing medical information on over 11
million patients that has excellent potential for detecting ADRs. In this paper
we apply four existing electronic healthcare database signal detecting
algorithms (MUTARA, HUNT, Temporal Pattern Discovery and modified ROR) on the
THIN database for a selection of drugs from six chosen drug families. This is
the first comparison of ADR signalling algorithms that includes MUTARA and HUNT
and enabled us to set a benchmark for the adverse drug reaction signalling
ability of the THIN database. The drugs were selectively chosen to enable a
comparison with previous work and for variety. It was found that no algorithm
was generally superior and the algorithms' natural thresholds act at variable
stringencies. Furthermore, none of the algorithms perform well at detecting
rare ADRs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0749</identifier>
 <datestamp>2014-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0749</id><created>2014-09-02</created><authors><author><keyname>Verma</keyname><forenames>Vikas</forenames></author></authors><title>Image Retrieval And Classification Using Local Feature Vectors</title><categories>cs.IR cs.CV cs.MM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Content Based Image Retrieval(CBIR) is one of the important subfield in the
field of Information Retrieval. The goal of a CBIR algorithm is to retrieve
semantically similar images in response to a query image submitted by the end
user. CBIR is a hard problem because of the phenomenon known as $\textit
{semantic gap}$.
  In this thesis, we aim at analyzing the performance of a CBIR system build
using local feature vectors and Intermediate Matching Kernel. We also propose a
Two-Step Matching process for reducing the response time of the CBIR systems.
Further, we develop a Meta-Learning framework for improving the retrieval
performance of these systems. Our results show that the Two-Step Matching
process significantly reduces response time and the Meta-Learning Framework
improves the retrieval performance by more than two fold. We also analyze the
performance of various image classification systems that use different image
representations constructed from the local feature vectors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0753</identifier>
 <datestamp>2014-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0753</id><created>2014-09-02</created><authors><author><keyname>Torres-Salinas</keyname><forenames>Daniel</forenames></author><author><keyname>Jim&#xe9;nez-Contreras</keyname><forenames>Evaristo</forenames></author><author><keyname>Robinson-Garc&#xed;a</keyname><forenames>Nicolas</forenames></author></authors><title>How many citations are there in the Data Citation Index?</title><categories>cs.DL</categories><comments>Presented at the STI Conference held in Leiden, 3-5 september 2014</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Descriptive analysis on the citation distribution of the Thomson Reuters'
Data Citation Index by publication type and four broad areas: Science,
Engineering &amp; Technology, Humanities &amp; Arts and Social Sciences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0757</identifier>
 <datestamp>2015-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0757</id><created>2014-09-02</created><updated>2015-05-19</updated><authors><author><keyname>Barrett</keyname><forenames>Edd</forenames></author><author><keyname>Bolz</keyname><forenames>Carl Friedrich</forenames></author><author><keyname>Tratt</keyname><forenames>Laurence</forenames></author></authors><title>Approaches to Interpreter Composition</title><categories>cs.PL</categories><comments>33 pages, 1 figure, 9 tables</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this paper, we compose six different Python and Prolog VMs into 4 pairwise
compositions: one using C interpreters; one running on the JVM; one using
meta-tracing interpreters; and one using a C interpreter and a meta-tracing
interpreter. We show that programs that cross the language barrier frequently
execute faster in a meta-tracing composition, and that meta-tracing imposes a
significantly lower overhead on composed programs relative to mono-language
programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0758</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0758</id><created>2014-09-02</created><authors><author><keyname>Figueredo</keyname><forenames>Grazziela P</forenames></author><author><keyname>Siebers</keyname><forenames>Peer-Olaf</forenames></author><author><keyname>Owen</keyname><forenames>Markus R</forenames></author><author><keyname>Reps</keyname><forenames>Jenna</forenames></author><author><keyname>Aickelin</keyname><forenames>Uwe</forenames></author></authors><title>Comparing Stochastic Differential Equations and Agent-Based Modelling
  and Simulation for Early-stage Cancer</title><categories>cs.MA cs.CE</categories><comments>PLoS ONE, 9 (4), pp. e95150, 2014</comments><doi>10.1371/journal.pone.0095150</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is great potential to be explored regarding the use of agent-based
modelling and simulation as an alternative paradigm to investigate early-stage
cancer interactions with the immune system. It does not suffer from some
limitations of ordinary differential equation models, such as the lack of
stochasticity, representation of individual behaviours rather than aggregates
and individual memory. In this paper we investigate the potential contribution
of agent-based modelling and simulation when contrasted with stochastic
versions of ODE models using early-stage cancer examples. We seek answers to
the following questions: (1) Does this new stochastic formulation produce
similar results to the agent-based version? (2) Can these methods be used
interchangeably? (3) Do agent-based models outcomes reveal any benefit when
compared to the Gillespie results? To answer these research questions we
investigate three well-established mathematical models describing interactions
between tumour cells and immune elements. These case studies were
re-conceptualised under an agent-based perspective and also converted to the
Gillespie algorithm formulation. Our interest in this work, therefore, is to
establish a methodological discussion regarding the usability of different
simulation approaches, rather than provide further biological insights into the
investigated case studies. Our results show that it is possible to obtain
equivalent models that implement the same mechanisms; however, the incapacity
of the Gillespie algorithm to retain individual memory of past events affects
the similarity of some results. Furthermore, the emergent behaviour of ABMS
produces extra patters of behaviour in the system, which was not obtained by
the Gillespie algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0759</identifier>
 <datestamp>2015-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0759</id><created>2014-09-02</created><updated>2015-03-10</updated><authors><author><keyname>Enoiu</keyname><forenames>Eduard Paul</forenames></author><author><keyname>Causevic</keyname><forenames>Adnan</forenames></author></authors><title>Enablers and Impediments for Collaborative Research in Software Testing:
  An Empirical Exploration</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When it comes to industrial organizations, current collaboration efforts in
software engineering research are very often kept in-house, depriving these
organizations off the skills necessary to build independent collaborative
research. The current trend, towards empirical software engineering research,
requires certain standards to be established which would guide these
collaborative efforts in creating a strong partnership that promotes
independent, evidence-based, software engineering research. This paper examines
key enabling factors for an efficient and effective industry-academia
collaboration in the software testing domain. A major finding of the research
was that while technology is a strong enabler to better collaboration, it must
be complemented with industrial openness to disclose research results and the
use of a dedicated tooling platform. We use as an example an automated test
generation approach that has been developed in the last two years
collaboratively with Bombardier Transportation AB in Sweden.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0763</identifier>
 <datestamp>2014-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0763</id><created>2014-09-02</created><authors><author><keyname>Chen</keyname><forenames>Qi</forenames></author><author><keyname>Whitbrook</keyname><forenames>Amanda</forenames></author><author><keyname>Aickelin</keyname><forenames>Uwe</forenames></author><author><keyname>Roadknight</keyname><forenames>Chris</forenames></author></authors><title>Data classification using the Dempster-Shafer method</title><categories>cs.LG</categories><comments>Journal of Experimental &amp; Theoretical Artificial Intelligence,
  ahead-of-print, 2014</comments><doi>10.1080/0952813X.2014.886301</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the Dempster-Shafer method is employed as the theoretical
basis for creating data classification systems. Testing is carried out using
three popular (multiple attribute) benchmark datasets that have two, three and
four classes. In each case, a subset of the available data is used for training
to establish thresholds, limits or likelihoods of class membership for each
attribute, and hence create mass functions that establish probability of class
membership for each attribute of the test data. Classification of each data
item is achieved by combination of these probabilities via Dempster's Rule of
Combination. Results for the first two datasets show extremely high
classification accuracy that is competitive with other popular methods. The
third dataset is non-numerical and difficult to classify, but good results can
be achieved provided the system and mass functions are designed carefully and
the right attributes are chosen for combination. In all cases the
Dempster-Shafer method provides comparable performance to other more popular
algorithms, but the overhead of generating accurate mass functions increases
the complexity with the addition of new attributes. Overall, the results
suggest that the D-S approach provides a suitable framework for the design of
classification systems and that automating the mass function design and
calculation would increase the viability of the algorithm for complex
classification problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0768</identifier>
 <datestamp>2014-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0768</id><created>2014-09-02</created><authors><author><keyname>Reps</keyname><forenames>Jenna</forenames></author><author><keyname>Garibaldi</keyname><forenames>Jonathan M.</forenames></author><author><keyname>Aickelin</keyname><forenames>Uwe</forenames></author><author><keyname>Soria</keyname><forenames>Daniele</forenames></author><author><keyname>Gibson</keyname><forenames>Jack E.</forenames></author><author><keyname>Hubbard</keyname><forenames>Richard B.</forenames></author></authors><title>A Novel Semi-Supervised Algorithm for Rare Prescription Side Effect
  Discovery</title><categories>cs.LG cs.CE</categories><journal-ref>IEEE Journal of Biomedical and Health Informatics, 18 (2), pp.
  537-547, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Drugs are frequently prescribed to patients with the aim of improving each
patient's medical state, but an unfortunate consequence of most prescription
drugs is the occurrence of undesirable side effects. Side effects that occur in
more than one in a thousand patients are likely to be signalled efficiently by
current drug surveillance methods, however, these same methods may take decades
before generating signals for rarer side effects, risking medical morbidity or
mortality in patients prescribed the drug while the rare side effect is
undiscovered. In this paper we propose a novel computational meta-analysis
framework for signalling rare side effects that integrates existing methods,
knowledge from the web, metric learning and semi-supervised clustering. The
novel framework was able to signal many known rare and serious side effects for
the selection of drugs investigated, such as tendon rupture when prescribed
Ciprofloxacin or Levofloxacin, renal failure with Naproxen and depression
associated with Rimonabant. Furthermore, for the majority of the drug
investigated it generated signals for rare side effects at a more stringent
signalling threshold than existing methods and shows the potential to become a
fundamental part of post marketing surveillance to detect rare side effects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0772</identifier>
 <datestamp>2014-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0772</id><created>2014-09-02</created><authors><author><keyname>Reps</keyname><forenames>Jenna M.</forenames></author><author><keyname>Garibaldi</keyname><forenames>Jonathan M.</forenames></author><author><keyname>Aickelin</keyname><forenames>Uwe</forenames></author><author><keyname>Soria</keyname><forenames>Daniele</forenames></author><author><keyname>Gibson</keyname><forenames>Jack E.</forenames></author><author><keyname>Hubbard</keyname><forenames>Richard B.</forenames></author></authors><title>Signalling Paediatric Side Effects using an Ensemble of Simple Study
  Designs</title><categories>cs.LG cs.CE</categories><comments>Drug Safety, 37 (3), pp. 163-170, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Background: Children are frequently prescribed medication off-label, meaning
there has not been sufficient testing of the medication to determine its safety
or effectiveness. The main reason this safety knowledge is lacking is due to
ethical restrictions that prevent children from being included in the majority
of clinical trials. Objective: The objective of this paper is to investigate
whether an ensemble of simple study designs can be implemented to signal
acutely occurring side effects effectively within the paediatric population by
using historical longitudinal data. The majority of pharmacovigilance
techniques are unsupervised, but this research presents a supervised framework.
Methods: Multiple measures of association are calculated for each drug and
medical event pair and these are used as features that are fed into a
classiffier to determine the likelihood of the drug and medical event pair
corresponding to an adverse drug reaction. The classiffier is trained using
known adverse drug reactions or known non-adverse drug reaction relationships.
Results: The novel ensemble framework obtained a false positive rate of 0:149,
a sensitivity of 0:547 and a specificity of 0:851 when implemented on a
reference set of drug and medical event pairs. The novel framework consistently
outperformed each individual simple study design. Conclusion: This research
shows that it is possible to exploit the mechanism of causality and presents a
framework for signalling adverse drug reactions effectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0775</identifier>
 <datestamp>2014-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0775</id><created>2014-09-02</created><authors><author><keyname>Liu</keyname><forenames>Yihui</forenames></author><author><keyname>Aickelin</keyname><forenames>Uwe</forenames></author></authors><title>Feature selection in detection of adverse drug reactions from the Health
  Improvement Network (THIN) database</title><categories>cs.LG cs.CE</categories><comments>International Journal of Information Technology and Computer Science
  (IJITCS), in print, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Adverse drug reaction (ADR) is widely concerned for public health issue. ADRs
are one of most common causes to withdraw some drugs from market. Prescription
event monitoring (PEM) is an important approach to detect the adverse drug
reactions. The main problem to deal with this method is how to automatically
extract the medical events or side effects from high-throughput medical events,
which are collected from day to day clinical practice. In this study we propose
a novel concept of feature matrix to detect the ADRs. Feature matrix, which is
extracted from big medical data from The Health Improvement Network (THIN)
database, is created to characterize the medical events for the patients who
take drugs. Feature matrix builds the foundation for the irregular and big
medical data. Then feature selection methods are performed on feature matrix to
detect the significant features. Finally the ADRs can be located based on the
significant features. The experiments are carried out on three drugs:
Atorvastatin, Alendronate, and Metoclopramide. Major side effects for each drug
are detected and better performance is achieved compared to other computerized
methods. The detected ADRs are based on computerized methods, further
investigation is needed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0788</identifier>
 <datestamp>2014-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0788</id><created>2014-09-02</created><authors><author><keyname>Roadknight</keyname><forenames>Chris</forenames></author><author><keyname>Aickelin</keyname><forenames>Uwe</forenames></author><author><keyname>Scholefield</keyname><forenames>John</forenames></author><author><keyname>Durrant</keyname><forenames>Lindy</forenames></author></authors><title>Ensemble Learning of Colorectal Cancer Survival Rates</title><categories>cs.LG cs.CE</categories><comments>IEEE International Conference on Computational Intelligence and
  Virtual Environments for Measurement Systems and Applications (CIVEMSA) 2013,
  pp. 82 - 86, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we describe a dataset relating to cellular and physical
conditions of patients who are operated upon to remove colorectal tumours. This
data provides a unique insight into immunological status at the point of tumour
removal, tumour classification and post-operative survival. We build on
existing research on clustering and machine learning facets of this data to
demonstrate a role for an ensemble approach to highlighting patients with
clearer prognosis parameters. Results for survival prediction using 3 different
approaches are shown for a subset of the data which is most difficult to model.
The performance of each model individually is compared with subsets of the data
where some agreement is reached for multiple models. Significant improvements
in model accuracy on an unseen test set can be achieved for patients where
agreement between models is achieved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0791</identifier>
 <datestamp>2014-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0791</id><created>2014-09-02</created><authors><author><keyname>Yang</keyname><forenames>Jian</forenames></author><author><keyname>Meng</keyname><forenames>Liqiu</forenames></author></authors><title>Feature Selection in Conditional Random Fields for Map Matching of GPS
  Trajectories</title><categories>stat.ML cs.AI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Map matching of the GPS trajectory serves the purpose of recovering the
original route on a road network from a sequence of noisy GPS observations. It
is a fundamental technique to many Location Based Services. However, map
matching of a low sampling rate on urban road network is still a challenging
task. In this paper, the characteristics of Conditional Random Fields with
regard to inducing many contextual features and feature selection are explored
for the map matching of the GPS trajectories at a low sampling rate.
Experiments on a taxi trajectory dataset show that our method may achieve
competitive results along with the success of reducing model complexity for
computation-limited applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0792</identifier>
 <datestamp>2014-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0792</id><created>2014-09-01</created><authors><author><keyname>Jia</keyname><forenames>Zhen</forenames></author><author><keyname>Zhan</keyname><forenames>Jianfeng</forenames></author><author><keyname>Wang</keyname><forenames>Lei</forenames></author><author><keyname>Han</keyname><forenames>Rui</forenames></author><author><keyname>McKee</keyname><forenames>Sally A.</forenames></author><author><keyname>Yang</keyname><forenames>Qiang</forenames></author><author><keyname>Luo</keyname><forenames>Chunjie</forenames></author><author><keyname>Li</keyname><forenames>Jingwei</forenames></author></authors><title>Characterizing and Subsetting Big Data Workloads</title><categories>cs.PF</categories><comments>11 pages, 6 figures, 2014 IEEE International Symposium on Workload
  Characterization</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Big data benchmark suites must include a diversity of data and workloads to
be useful in fairly evaluating big data systems and architectures. However,
using truly comprehensive benchmarks poses great challenges for the
architecture community. First, we need to thoroughly understand the behaviors
of a variety of workloads. Second, our usual simulation-based research methods
become prohibitively expensive for big data. As big data is an emerging field,
more and more software stacks are being proposed to facilitate the development
of big data applications, which aggravates hese challenges. In this paper, we
first use Principle Component Analysis (PCA) to identify the most important
characteristics from 45 metrics to characterize big data workloads from
BigDataBench, a comprehensive big data benchmark suite. Second, we apply a
clustering technique to the principle components obtained from the PCA to
investigate the similarity among big data workloads, and we verify the
importance of including different software stacks for big data benchmarking.
Third, we select seven representative big data workloads by removing redundant
ones and release the BigDataBench simulation version, which is publicly
available from http://prof.ict.ac.cn/BigDataBench/simulatorversion/.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0797</identifier>
 <datestamp>2014-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0797</id><created>2014-09-02</created><authors><author><keyname>Yang</keyname><forenames>Jian</forenames></author><author><keyname>Meng</keyname><forenames>Liqiu</forenames></author></authors><title>Feature Engineering for Map Matching of Low-Sampling-Rate GPS
  Trajectories in Road Network</title><categories>stat.ML cs.LG</categories><comments>ECML/PKDD14 workshop on Machine Learning for Urban Sensor
  Data(SenseML)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Map matching of GPS trajectories from a sequence of noisy observations serves
the purpose of recovering the original routes in a road network. In this work
in progress, we attempt to share our experience of feature construction in a
spatial database by reporting our ongoing experiment of feature extrac-tion in
Conditional Random Fields (CRFs) for map matching. Our preliminary results are
obtained from real-world taxi GPS trajectories.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0798</identifier>
 <datestamp>2014-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0798</id><created>2014-09-02</created><authors><author><keyname>Bhardwaj</keyname><forenames>Anant</forenames></author><author><keyname>Bhattacherjee</keyname><forenames>Souvik</forenames></author><author><keyname>Chavan</keyname><forenames>Amit</forenames></author><author><keyname>Deshpande</keyname><forenames>Amol</forenames></author><author><keyname>Elmore</keyname><forenames>Aaron J.</forenames></author><author><keyname>Madden</keyname><forenames>Samuel</forenames></author><author><keyname>Parameswaran</keyname><forenames>Aditya G.</forenames></author></authors><title>DataHub: Collaborative Data Science &amp; Dataset Version Management at
  Scale</title><categories>cs.DB</categories><comments>7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Relational databases have limited support for data collaboration, where teams
collaboratively curate and analyze large datasets. Inspired by software version
control systems like git, we propose (a) a dataset version control system,
giving users the ability to create, branch, merge, difference and search large,
divergent collections of datasets, and (b) a platform, DataHub, that gives
users the ability to perform collaborative data analysis building on this
version control system. We outline the challenges in providing dataset version
control at scale.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0813</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0813</id><created>2014-09-02</created><updated>2014-09-03</updated><authors><author><keyname>Tegmark</keyname><forenames>Max</forenames><affiliation>MIT</affiliation></author></authors><title>Friendly Artificial Intelligence: the Physics Challenge</title><categories>cs.CY cs.AI</categories><comments>3 pages</comments><journal-ref>In proceedings of the AAAI 2015 Workshop On AI and Ethics, p87,
  Toby Walsh, Ed. (2015)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Relentless progress in artificial intelligence (AI) is increasingly raising
concerns that machines will replace humans on the job market, and perhaps
altogether. Eliezer Yudkowski and others have explored the possibility that a
promising future for humankind could be guaranteed by a superintelligent
&quot;Friendly AI&quot;, designed to safeguard humanity and its values. I argue that,
from a physics perspective where everything is simply an arrangement of
elementary particles, this might be even harder than it appears. Indeed, it may
require thinking rigorously about the meaning of life: What is &quot;meaning&quot; in a
particle arrangement? What is &quot;life&quot;? What is the ultimate ethical imperative,
i.e., how should we strive to rearrange the particles of our Universe and shape
its future? If we fail to answer the last question rigorously, this future is
unlikely to contain humans.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0814</identifier>
 <datestamp>2014-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0814</id><created>2014-09-02</created><authors><author><keyname>Karim</keyname><forenames>Rezaul</forenames></author><author><keyname>Aziz</keyname><forenames>Mohd. Momin Al</forenames></author><author><keyname>Shatabda</keyname><forenames>Swakkhar</forenames></author><author><keyname>Rahman</keyname><forenames>M. Sohel</forenames></author><author><keyname>Mia</keyname><forenames>Md. Abul Kashem</forenames></author><author><keyname>Zaman</keyname><forenames>Farhana</forenames></author><author><keyname>Rakin</keyname><forenames>Salman</forenames></author></authors><title>CoMOGrad and PHOG: From Computer Vision to Fast and Accurate Protein
  Tertiary Structure Retrieval</title><categories>cs.CV cs.CE cs.IR</categories><comments>draft</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to the advancements in technology number of entries in the structural
database of proteins are increasing day by day. Methods for retrieving protein
tertiary structures from this large database is the key to comparative analysis
of structures which plays an important role to understand proteins and their
function. In this paper, we present fast and accurate methods for the retrieval
of proteins from a large database with tertiary structures similar to a query
protein. Our proposed methods borrow ideas from the field of computer vision.
The speed and accuracy of our methods comes from the two newly introduced
features, the co-occurrence matrix of the oriented gradient and pyramid
histogram of oriented gradient and from the use of Euclidean distance as the
distance measure. Experimental results clearly indicate the superiority of our
approach in both running time and accuracy. Our method is readily available for
use from this website: http://research.buet.ac.bd:8080/Comograd/.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0820</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0820</id><created>2014-09-02</created><updated>2015-01-31</updated><authors><author><keyname>Kesidis</keyname><forenames>G.</forenames></author><author><keyname>Urgaonkar</keyname><forenames>B.</forenames></author><author><keyname>Shan</keyname><forenames>Y.</forenames></author><author><keyname>Kamarava</keyname><forenames>S.</forenames></author><author><keyname>Liebeherr</keyname><forenames>J.</forenames></author></authors><title>Network calculus for parallel processing</title><categories>cs.PF cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note, we present preliminary results on the use of &quot;network calculus&quot;
for parallel processing systems, specifically MapReduce.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0824</identifier>
 <datestamp>2014-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0824</id><created>2014-09-02</created><authors><author><keyname>Osherson</keyname><forenames>Daniel</forenames></author><author><keyname>Weinstein</keyname><forenames>Scott</forenames></author></authors><title>Deontic modality based on preference</title><categories>cs.LO cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deontic modalities are here defined in terms of the preference relation
explored in our previous work (Osherson and Weinstein, 2012). Some consequences
of the system are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0829</identifier>
 <datestamp>2014-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0829</id><created>2014-09-02</created><authors><author><keyname>Tebaa</keyname><forenames>Maha</forenames></author><author><keyname>Hajji</keyname><forenames>Said El</forenames></author></authors><title>Secure Cloud Computing through Homomorphic Encryption</title><categories>cs.CR</categories><comments>10 pages, 5 figures, International Journal of Advancements in
  Computing Technology (IJACT) 29 Volume 5, Number16, December 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Go to the cloud, has always been the dream of man. Cloud Computing offers a
number of benefits and services to its customers who pay the use of hardware
and software resources (servers hosted in data centers, applications,
software...) on demand which they can access via internet without the need of
expensive computers or a large storage system capacity and without paying any
equipment maintenance fees. But these cloud providers must provide guarantees
on the protection of privacy and sensitive data stored in their data centers
shared between multiple clients using the concept of virtualization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0838</identifier>
 <datestamp>2014-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0838</id><created>2014-09-02</created><updated>2014-09-10</updated><authors><author><keyname>Rasouli</keyname><forenames>Mohammad</forenames></author><author><keyname>Miehling</keyname><forenames>Erik</forenames></author><author><keyname>Teneketzis</keyname><forenames>Demosthenis</forenames></author></authors><title>A Supervisory Control Approach to Dynamic Cyber-Security</title><categories>cs.SY cs.CR</categories><comments>19 pages, 4 figures, GameSec 2014 (Conference on Decision and Game
  Theory for Security)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An analytical approach for a dynamic cyber-security problem that captures
progressive attacks to a computer network is presented. We formulate the
dynamic security problem from the defender's point of view as a supervisory
control problem with imperfect information, modeling the computer network's
operation by a discrete event system. We consider a min-max performance
criterion and use dynamic programming to determine, within a restricted set of
policies, an optimal policy for the defender. We study and interpret the
behavior of this optimal policy as we vary certain parameters of the
supervisory control problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0845</identifier>
 <datestamp>2014-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0845</id><created>2014-09-02</created><authors><author><keyname>&#x106;usti&#x107;</keyname><forenames>Ante</forenames></author><author><keyname>Klinz</keyname><forenames>Bettina</forenames></author><author><keyname>Woeginger</keyname><forenames>Gerhard J.</forenames></author></authors><title>Geometric versions of the 3-dimensional assignment problem under general
  norms</title><categories>math.OC cs.DM</categories><comments>21 pages, 9 figures</comments><msc-class>90C27, 90B80</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss the computational complexity of special cases of the 3-dimensional
(axial) assignment problem where the elements are points in a Cartesian space
and where the cost coefficients are the perimeters of the corresponding
triangles measured according to a certain norm. (All our results also carry
over to the corresponding special cases of the 3-dimensional matching problem.)
  The minimization version is NP-hard for every norm, even if the underlying
Cartesian space is 2-dimensional. The maximization version is polynomially
solvable, if the dimension of the Cartesian space is fixed and if the
considered norm has a polyhedral unit ball. If the dimension of the Cartesian
space is part of the input, the maximization version is NP-hard for every $L_p$
norm; in particular the problem is NP-hard for the Manhattan norm $L_1$ and the
Maximum norm $L_{\infty}$ which both have polyhedral unit balls.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0871</identifier>
 <datestamp>2014-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0871</id><created>2014-09-02</created><authors><author><keyname>B&#xe9;rard</keyname><forenames>B&#xe9;atrice</forenames></author><author><keyname>Mullins</keyname><forenames>John</forenames></author></authors><title>Verification of Information Flow Properties under Rational Observation</title><categories>cs.CR</categories><comments>19 pages, 7 figures, version extended from AVOCS'2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information flow properties express the capability for an agent to infer
information about secret behaviours of a partially observable system. In a
language-theoretic setting, where the system behaviour is described by a
language, we define the class of rational information flow properties (RIFP),
where observers are modeled by finite transducers, acting on languages in a
given family $\mathcal{L}$. This leads to a general decidability criterion for
the verification problem of RIFPs on $\mathcal{L}$, implying
PSPACE-completeness for this problem on regular languages. We show that most
trace-based information flow properties studied up to now are RIFPs, including
those related to selective declassification and conditional anonymity. As a
consequence, we retrieve several existing decidability results that were
obtained by ad-hoc proofs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0875</identifier>
 <datestamp>2015-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0875</id><created>2014-09-02</created><updated>2015-04-29</updated><authors><author><keyname>Bj&#xf6;rnson</keyname><forenames>Emil</forenames></author><author><keyname>Matthaiou</keyname><forenames>Michail</forenames></author><author><keyname>Debbah</keyname><forenames>M&#xe9;rouane</forenames></author></authors><title>Massive MIMO with Non-Ideal Arbitrary Arrays: Hardware Scaling Laws and
  Circuit-Aware Design</title><categories>cs.IT math.IT</categories><comments>Accepted for publication in IEEE Transactions on Wireless
  Communications, 16 pages, 8 figures. The results can be reproduced using the
  following Matlab code: https://github.com/emilbjornson/hardware-scaling-laws</comments><doi>10.1109/TWC.2015.2420095</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Massive multiple-input multiple-output (MIMO) systems are cellular networks
where the base stations (BSs) are equipped with unconventionally many antennas,
deployed on co-located or distributed arrays. Huge spatial degrees-of-freedom
are achieved by coherent processing over these massive arrays, which provide
strong signal gains, resilience to imperfect channel knowledge, and low
interference. This comes at the price of more infrastructure; the hardware cost
and circuit power consumption scale linearly/affinely with the number of BS
antennas $N$. Hence, the key to cost-efficient deployment of large arrays is
low-cost antenna branches with low circuit power, in contrast to today's
conventional expensive and power-hungry BS antenna branches. Such low-cost
transceivers are prone to hardware imperfections, but it has been conjectured
that the huge degrees-of-freedom would bring robustness to such imperfections.
We prove this claim for a generalized uplink system with multiplicative
phase-drifts, additive distortion noise, and noise amplification. Specifically,
we derive closed-form expressions for the user rates and a scaling law that
shows how fast the hardware imperfections can increase with $N$ while
maintaining high rates. The connection between this scaling law and the power
consumption of different transceiver circuits is rigorously exemplified. This
reveals that one can make the circuit power increase as $\sqrt{N}$, instead of
linearly, by careful circuit-aware system design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0876</identifier>
 <datestamp>2014-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0876</id><created>2014-09-02</created><updated>2014-09-17</updated><authors><author><keyname>Moghaddam</keyname><forenames>Reza Farrahi</forenames></author><author><keyname>Moghaddam</keyname><forenames>Fereydoun Farrahi</forenames></author><author><keyname>Cheriet</keyname><forenames>Mohamed</forenames></author></authors><title>A Graph-based Perspective to Total Carbon Footprint Assessment of
  Non-marginal Technology-driven Projects - Use case of OTT/IPTV</title><categories>cs.NI cs.CY</categories><comments>27 pages, 6 tables, and 6 figures. The preprint version of a paper
  submitted to Nova Science Publishers</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Life Cycle Assessment (LCA) of green and sustainable projects has been found
to be a necessary analysis in order to include all upstream, downstream, and
indirect impacts. Because of the complexity of interactions, the differential
impacts with respect to a baseline, i.e., a business-as-usual (BAU) scenario,
are commonly considered to relatively compare various projects. However, as the
degree of penetration of a project in the baseline increases, the popular
marginal assumption does no longer hold, and the differential impacts may
become inconsistent. Although various mythologies have been successfully
proposed and used to contain such a side effect, the bottom-up nature, which
initiates the assessment from the project itself and ultimately widens the
scope, could easily fail to acknowledge critical modifications to the baseline.
This is highly relevant in terms of ICT's disruptive and dynamic technologies
which push the baseline to become a marginal legacy. In this work, an analytic
formalism is presented to provide a means of comparison of such technologies
and projects. The core idea behind the proposed methodology is a
magnitude-insensitive graph-based distance function to differentially compare a
project with a baseline. The applicability of the proposed methodology is then
evaluated in a use case of OTT/IPTV online media distribution services.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0887</identifier>
 <datestamp>2014-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0887</id><created>2014-09-02</created><authors><author><keyname>Ouyang</keyname><forenames>Yi</forenames></author><author><keyname>Teneketzis</keyname><forenames>Demosthenis</forenames></author></authors><title>Signaling for Decentralized Routing in a Queueing Network</title><categories>cs.SY cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A discrete-time decentralized routing problem in a service system consisting
of two service stations and two controllers is investigated. Each controller is
affiliated with one station. Each station has an infinite size buffer.
Exogenous customer arrivals at each station occur with rate $\lambda$. Service
times at each station have rate $\mu$. At any time, a controller can route one
of the customers waiting in its own station to the other station. Each
controller knows perfectly the queue length in its own station and observes the
exogenous arrivals to its own station as well as the arrivals of customers sent
from the other station. At the beginning, each controller has a probability
mass function (PMF) on the number of customers in the other station. These PMFs
are common knowledge between the two controllers. At each time a holding cost
is incurred at each station due to the customers waiting at that station. The
objective is to determine routing policies for the two controllers that
minimize either the total expected holding cost over a finite horizon or the
average cost per unit time over an infinite horizon. In this problem there is
implicit communication between the two controllers; whenever a controller
decides to send or not to send a customer from its own station to the other
station it communicates information about its queue length to the other
station. This implicit communication through control actions is referred to as
signaling in decentralized control. Signaling results in complex communication
and decision problems. In spite of the complexity of signaling involved, it is
shown that an optimal signaling strategy is described by a threshold policy
which depends on the common information between the two controllers; this
threshold policy is explicitly determined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0893</identifier>
 <datestamp>2014-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0893</id><created>2014-09-02</created><authors><author><keyname>Ho&#xe0;ng</keyname><forenames>Ch&#xed;nh T.</forenames></author><author><keyname>Lazzarato</keyname><forenames>D. Adam</forenames></author></authors><title>Polynomial-time algorithms for minimum weighted colorings of ($P_5,
  \bar{P}_5$)-free graphs and related graph classes</title><categories>cs.DM</categories><msc-class>05C15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We design an $O(n^3)$ algorithm to find a minimum weighted coloring of a
($P_5, \bar{P}_5$)-free graph. Furthermore, the same technique can be used to
solve the same problem for several classes of graphs, defined by forbidden
induced subgraphs, such as (diamond, co-diamond)-free graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0908</identifier>
 <datestamp>2014-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0908</id><created>2014-09-02</created><authors><author><keyname>Tran</keyname><forenames>Anh</forenames></author><author><keyname>Guan</keyname><forenames>Jinyan</forenames></author><author><keyname>Pilantanakitti</keyname><forenames>Thanima</forenames></author><author><keyname>Cohen</keyname><forenames>Paul</forenames></author></authors><title>Action Recognition in the Frequency Domain</title><categories>cs.CV</categories><comments>Keywords: Artificial Intelligence, Computer Vision, Action
  Recognition</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we describe a simple strategy for mitigating variability in
temporal data series by shifting focus onto long-term, frequency domain
features that are less susceptible to variability. We apply this method to the
human action recognition task and demonstrate how working in the frequency
domain can yield good recognition features for commonly used optical flow and
articulated pose features, which are highly sensitive to small differences in
motion, viewpoint, dynamic backgrounds, occlusion and other sources of
variability. We show how these frequency-based features can be used in
combination with a simple forest classifier to achieve good and robust results
on the popular KTH Actions dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0911</identifier>
 <datestamp>2014-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0911</id><created>2014-09-02</created><authors><author><keyname>Usman</keyname><forenames>Muneer</forenames></author><author><keyname>Yang</keyname><forenames>Hong-Chuan</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author></authors><title>Extended Delivery Time Analysis for Non-work-preserving Packet
  Transmission in Cognitive Environment</title><categories>cs.IT cs.NI math.IT</categories><comments>22 single column pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cognitive radio transceiver can opportunistically access the underutilized
spectrum resource of primary systems for new wireless services. With interweave
cognitive implementation, the secondary transmission may be interrupted by the
primary user's transmission. To facilitate the packet delay analysis of such
secondary transmission, we study the resulting extended delivery time that
includes both transmission time and waiting time. In particular, we derive the
exact distribution function of extended delivery time of a fixed-size secondary
packet with non-work-preserving strategy i.e. interrupted packets will be
retransmitted. Both continuous sensing and periodic sensing with and without
missed detection cases are considered. Selected numerical and simulation
results are presented for verifying the mathematical formulation. Finally, we
apply the results to secondary queuing analysis with a generalized M/G/1 queue
set-up. The analytical results will greatly facilitate the design of the
secondary system for particular target application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0915</identifier>
 <datestamp>2014-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0915</id><created>2014-09-02</created><authors><author><keyname>Moraldo</keyname><forenames>H. Hernan</forenames></author></authors><title>An Approach for Text Steganography Based on Markov Chains</title><categories>cs.MM cs.CL</categories><comments>Presented at 41 JAIIO - WSegI 2012</comments><msc-class>68P25, 94A60</msc-class><acm-class>D.4.6</acm-class><journal-ref>41 JAIIO - WSegI 2012, ISSN: 2313-9110, pages 21 - 35</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A text steganography method based on Markov chains is introduced, together
with a reference implementation. This method allows for information hiding in
texts that are automatically generated following a given Markov model. Other
Markov - based systems of this kind rely on big simplifications of the language
model to work, which produces less natural looking and more easily detectable
texts. The method described here is designed to generate texts within a good
approximation of the original language model provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0919</identifier>
 <datestamp>2014-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0919</id><created>2014-09-02</created><authors><author><keyname>Hassanat</keyname><forenames>Ahmad Basheer</forenames></author><author><keyname>Abbadi</keyname><forenames>Mohammad Ali</forenames></author><author><keyname>Altarawneh</keyname><forenames>Ghada Awad</forenames></author><author><keyname>Alhasanat</keyname><forenames>Ahmad Ali</forenames></author></authors><title>Solving the Problem of the K Parameter in the KNN Classifier Using an
  Ensemble Learning Approach</title><categories>cs.LG</categories><journal-ref>International Journal of Computer Science and Information
  Security, International Journal of Computer Science and Information Security,
  Vol. 12, No. 8, August 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new solution for choosing the K parameter in the
k-nearest neighbor (KNN) algorithm, the solution depending on the idea of
ensemble learning, in which a weak KNN classifier is used each time with a
different K, starting from one to the square root of the size of the training
set. The results of the weak classifiers are combined using the weighted sum
rule. The proposed solution was tested and compared to other solutions using a
group of experiments in real life problems. The experimental results show that
the proposed classifier outperforms the traditional KNN classifier that uses a
different number of neighbors, is competitive with other classifiers, and is a
promising classifier with strong potential for a wide range of applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0921</identifier>
 <datestamp>2014-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0921</id><created>2014-09-02</created><authors><author><keyname>Zidi</keyname><forenames>Amir</forenames></author><author><keyname>Abed</keyname><forenames>Mourad</forenames></author></authors><title>A Generalized Framework for Ontology-Based Information Retrieval
  Application to a public-transportation system</title><categories>cs.IR</categories><doi>10.1109/ICAdLT.2013.6568453</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a generic framework for ontology-based information
retrieval. We focus on the recognition of semantic information extracted from
data sources and the mapping of this knowledge into ontology. In order to
achieve more scalability, we propose an approach for semantic indexing based on
entity retrieval model. In addition, we have used ontology of public
transportation domain in order to validate these proposals. Finally, we
evaluated our system using ontology mapping and real world data sources.
Experiments show that our framework can provide meaningful search results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0923</identifier>
 <datestamp>2014-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0923</id><created>2014-09-02</created><authors><author><keyname>Hassanat</keyname><forenames>Ahmad Basheer</forenames></author></authors><title>Dimensionality Invariant Similarity Measure</title><categories>cs.LG</categories><comments>(ISSN: 1545-1003). http://www.jofamericanscience.org</comments><journal-ref>J Am Sci 2014;10(8):221-226</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new similarity measure to be used for general tasks
including supervised learning, which is represented by the K-nearest neighbor
classifier (KNN). The proposed similarity measure is invariant to large
differences in some dimensions in the feature space. The proposed metric is
proved mathematically to be a metric. To test its viability for different
applications, the KNN used the proposed metric for classifying test examples
chosen from a number of real datasets. Compared to some other well known
metrics, the experimental results show that the proposed metric is a promising
distance measure for the KNN classifier with strong potential for a wide range
of applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0924</identifier>
 <datestamp>2014-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0924</id><created>2014-09-02</created><authors><author><keyname>Hassanat</keyname><forenames>Ahmad Basheer</forenames></author></authors><title>Visual Passwords Using Automatic Lip Reading</title><categories>cs.CV cs.CR</categories><journal-ref>International Journal of Sciences: Basic and Applied Research
  (IJSBAR) (2014) Volume 13, No 1, pp 218-231</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a visual passwords system to increase security. The
system depends mainly on recognizing the speaker using the visual speech signal
alone. The proposed scheme works in two stages: setting the visual password
stage and the verification stage. At the setting stage the visual passwords
system request the user to utter a selected password, a video recording of the
user face is captured, and processed by a special words-based VSR system which
extracts a sequence of feature vectors. In the verification stage, the same
procedure is executed, the features will be sent to be compared with the stored
visual password. The proposed scheme has been evaluated using a video database
of 20 different speakers (10 females and 10 males), and 15 more males in
another video database with different experiment sets. The evaluation has
proved the system feasibility, with average error rate in the range of 7.63% to
20.51% at the worst tested scenario, and therefore, has potential to be a
practical approach with the support of other conventional authentication
methods such as the use of usernames and passwords.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0925</identifier>
 <datestamp>2014-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0925</id><created>2014-09-02</created><authors><author><keyname>Hassanat</keyname><forenames>Ahmad B. A.</forenames></author></authors><title>Bypassing Captcha By Machine A Proof For Passing The Turing Test</title><categories>cs.CV cs.AI cs.HC</categories><comments>European Scientific Journal May 2014 edition vol.10, No.15 ISSN:
  1857-7881 (Print) e-ISSN 1857-7431</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For the last ten years, CAPTCHAs have been widely used by websites to prevent
their data being automatically updated by machines. By supposedly allowing only
humans to do so, CAPTCHAs take advantage of the reverse Turing test (TT),
knowing that humans are more intelligent than machines. Generally, CAPTCHAs
have defeated machines, but things are changing rapidly as technology improves.
Hence, advanced research into optical character recognition (OCR) is overtaking
attempts to strengthen CAPTCHAs against machine-based attacks. This paper
investigates the immunity of CAPTCHA, which was built on the failure of the TT.
We show that some CAPTCHAs are easily broken using a simple OCR machine built
for the purpose of this study. By reviewing other techniques, we show that even
more difficult CAPTCHAs can be broken using advanced OCR machines. Current
advances in OCR should enable machines to pass the TT in the image recognition
domain, which is exactly where machines are seeking to overcome CAPTCHAs. We
enhance traditional CAPTCHAs by employing not only characters, but also natural
language and multiple objects within the same CAPTCHA. The proposed CAPTCHAs
might be able to hold out against machines, at least until the advent of a
machine that passes the TT completely.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0926</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0926</id><created>2014-09-02</created><updated>2015-08-18</updated><authors><author><keyname>Simmons</keyname><forenames>David</forenames></author><author><keyname>Solomon</keyname><forenames>Yaar</forenames></author></authors><title>A Danzer set for Axis Parallel Boxes</title><categories>cs.CG cs.DM math.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present concrete constructions of discrete sets in $\mathbb{R}^d$ ($d\ge
2$) that intersect every aligned box of volume $1$ in $\mathbb{R}^d$, and which
have optimal growth rate $O(T^d)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0932</identifier>
 <datestamp>2015-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0932</id><created>2014-09-02</created><updated>2015-06-26</updated><authors><author><keyname>Wildman</keyname><forenames>Jeffrey</forenames></author><author><keyname>Weber</keyname><forenames>Steven</forenames></author></authors><title>On Characterizing the Local Pooling Factor of Greedy Maximal Scheduling
  in Random Graphs</title><categories>cs.IT cs.NI math.IT</categories><comments>16 pages, 7 figures, 1 table, 1 listing. Submitted on 2014-09-02 to
  IEEE/ACM Transactions on Networking. Accepted on 2015-05-29 to IEEE/ACM
  Transactions on Networking</comments><doi>10.1109/TNET.2015.2451090</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The study of the optimality of low-complexity greedy scheduling techniques in
wireless communications networks is a very complex problem. The Local Pooling
(LoP) factor provides a single-parameter means of expressing the achievable
capacity region (and optimality) of one such scheme, greedy maximal scheduling
(GMS). The exact LoP factor for an arbitrary network graph is generally
difficult to obtain, but may be evaluated or bounded based on the network
graph's particular structure. In this paper, we provide rigorous
characterizations of the LoP factor in large networks modeled as
Erd\H{o}s-R\'enyi (ER) and random geometric (RG) graphs under the primary
interference model. We employ threshold functions to establish critical values
for either the edge probability or communication radius to yield useful bounds
on the range and expectation of the LoP factor as the network grows large. For
sufficiently dense random graphs, we find that the LoP factor is between 1/2
and 2/3, while sufficiently sparse random graphs permit GMS optimality (the LoP
factor is 1) with high probability. We then place LoP within a larger context
of commonly studied random graph properties centered around connectedness. We
observe that edge densities permitting connectivity generally admit cycle
subgraphs which forms the basis for the LoP factor upper bound of 2/3. We
conclude with simulations to explore the regime of small networks, which
suggest the probability that an ER or RG graph satisfies LoP and is connected
decays quickly in network size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0934</identifier>
 <datestamp>2014-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0934</id><created>2014-09-02</created><authors><author><keyname>Kanamori</keyname><forenames>Takafumi</forenames></author><author><keyname>Fujiwara</keyname><forenames>Shuhei</forenames></author><author><keyname>Takeda</keyname><forenames>Akiko</forenames></author></authors><title>Breakdown Point of Robust Support Vector Machine</title><categories>stat.ML cs.LG</categories><comments>27 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The support vector machine (SVM) is one of the most successful learning
methods for solving classification problems. Despite its popularity, SVM has a
serious drawback, that is sensitivity to outliers in training samples. The
penalty on misclassification is defined by a convex loss called the hinge loss,
and the unboundedness of the convex loss causes the sensitivity to outliers. To
deal with outliers, robust variants of SVM have been proposed, such as the
robust outlier detection algorithm and an SVM with a bounded loss called the
ramp loss. In this paper, we propose a robust variant of SVM and investigate
its robustness in terms of the breakdown point. The breakdown point is a
robustness measure that is the largest amount of contamination such that the
estimated classifier still gives information about the non-contaminated data.
The main contribution of this paper is to show an exact evaluation of the
breakdown point for the robust SVM. For learning parameters such as the
regularization parameter in our algorithm, we derive a simple formula that
guarantees the robustness of the classifier. When the learning parameters are
determined with a grid search using cross validation, our formula works to
reduce the number of candidate search points. The robustness of the proposed
method is confirmed in numerical experiments. We show that the statistical
properties of the robust SVM are well explained by a theoretical analysis of
the breakdown point.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0938</identifier>
 <datestamp>2014-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0938</id><created>2014-09-02</created><authors><author><keyname>Rana</keyname><forenames>Rajib</forenames></author><author><keyname>Austin</keyname><forenames>Daniel</forenames></author><author><keyname>Jacob</keyname><forenames>Peter G.</forenames></author><author><keyname>Karunanithi</keyname><forenames>Mohanraj</forenames></author><author><keyname>Kaye</keyname><forenames>Jeffrey</forenames></author></authors><title>Continuous Gait Velocity Estimation using Houseohld Motion Detectors</title><categories>cs.SY</categories><comments>arXiv admin note: substantial text overlap with arXiv:1310.4880</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gait velocity has been consistently shown to be an important indicator and
predictor of health status, especially in older adults. Gait velocity is often
assessed clinically, but the assessments occur infrequently and thus do not
allow optimal detection of key health changes when they occur. In this paper,
we show the time it takes a person to move between rooms in their home denoted
'transition times' can predict gait velocity when estimated from passive
infrared motion detectors installed in a patient's own home. Using a support
vector regression approach to model the relationship between transition times
and gait velocities, we show that velocity can be predicted with an average
error less than 2.5 cm/sec. This is demonstrated with data collected over a 5
year period from 74 older adults monitored in their own homes. This method is
simple and cost effective, and has advantages over competing approaches such
as: obtaining 20 to100x more gait velocity measurements per day, and offering
the fusion of location specific information with time stamped gait estimates.
These advantages allow stable estimates of gait parameters (maximum or average
speed, variability) at shorter time scales than current approaches. This also
provides a pervasive in home method for context aware gait velocity sensing
that allows for monitoring of gait trajectories in space and time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0940</identifier>
 <datestamp>2015-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0940</id><created>2014-09-02</created><updated>2015-04-16</updated><authors><author><keyname>Sindhwani</keyname><forenames>Vikas</forenames></author><author><keyname>Avron</keyname><forenames>Haim</forenames></author></authors><title>High-performance Kernel Machines with Implicit Distributed Optimization
  and Randomization</title><categories>stat.ML cs.DC cs.LG</categories><comments>Work presented at MMDS 2014 (June 2014) and JSM 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to fully utilize &quot;big data&quot;, it is often required to use &quot;big
models&quot;. Such models tend to grow with the complexity and size of the training
data, and do not make strong parametric assumptions upfront on the nature of
the underlying statistical dependencies. Kernel methods fit this need well, as
they constitute a versatile and principled statistical methodology for solving
a wide range of non-parametric modelling problems. However, their high
computational costs (in storage and time) pose a significant barrier to their
widespread adoption in big data applications.
  We propose an algorithmic framework and high-performance implementation for
massive-scale training of kernel-based statistical models, based on combining
two key technical ingredients: (i) distributed general purpose convex
optimization, and (ii) the use of randomization to improve the scalability of
kernel methods. Our approach is based on a block-splitting variant of the
Alternating Directions Method of Multipliers, carefully reconfigured to handle
very large random feature matrices, while exploiting hybrid parallelism
typically found in modern clusters of multicore machines. Our implementation
supports a variety of statistical learning tasks by enabling several loss
functions, regularization schemes, kernels, and layers of randomized
approximations for both dense and sparse datasets, in a highly extensible
framework. We evaluate the ability of our framework to learn models on data
from applications, and provide a comparison against existing sequential and
parallel libraries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0952</identifier>
 <datestamp>2014-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0952</id><created>2014-09-03</created><authors><author><keyname>Wang</keyname><forenames>Anyu</forenames></author><author><keyname>Zhang</keyname><forenames>Zhifang</forenames></author></authors><title>An Integer Programming Based Bound for Locally Repairable Codes</title><categories>cs.IT math.IT</categories><comments>23 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The locally repairable code (LRC) studied in this paper is an $[n,k]$ linear
code of which the value at each coordinate can be recovered by a linear
combination of at most $r$ other coordinates. The central problem in this work
is to determine the largest possible minimum distance for LRCs. First, an
integer programming based upper bound is derived for any LRC. Then by solving
the programming problem under certain conditions, an explicit upper bound is
obtained for LRCs with parameters $n_1&gt;n_2$, where $n_1 = \left\lceil
\frac{n}{r+1} \right\rceil$ and $n_2 = n_1 (r+1) - n$. Finally, an explicit
construction for LRCs attaining this upper bound is presented over the finite
field $\mathbb{F}_{2^m}$, where $m\geq n_1r$. Based on these results, the
largest possible minimum distance for all LRCs with $r \le \sqrt{n}-1$ has been
definitely determined, which is of great significance in practical use.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0964</identifier>
 <datestamp>2014-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0964</id><created>2014-09-03</created><authors><author><keyname>Zhuang</keyname><forenames>Liansheng</forenames></author><author><keyname>Gao</keyname><forenames>Shenghua</forenames></author><author><keyname>Tang</keyname><forenames>Jinhui</forenames></author><author><keyname>Wang</keyname><forenames>Jingjing</forenames></author><author><keyname>Lin</keyname><forenames>Zhouchen</forenames></author><author><keyname>Ma</keyname><forenames>Yi</forenames></author></authors><title>Constructing a Non-Negative Low Rank and Sparse Graph with Data-Adaptive
  Features</title><categories>cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper aims at constructing a good graph for discovering intrinsic data
structures in a semi-supervised learning setting. Firstly, we propose to build
a non-negative low-rank and sparse (referred to as NNLRS) graph for the given
data representation. Specifically, the weights of edges in the graph are
obtained by seeking a nonnegative low-rank and sparse matrix that represents
each data sample as a linear combination of others. The so-obtained NNLRS-graph
can capture both the global mixture of subspaces structure (by the low
rankness) and the locally linear structure (by the sparseness) of the data,
hence is both generative and discriminative. Secondly, as good features are
extremely important for constructing a good graph, we propose to learn the data
embedding matrix and construct the graph jointly within one framework, which is
termed as NNLRS with embedded features (referred to as NNLRS-EF). Extensive
experiments on three publicly available datasets demonstrate that the proposed
method outperforms the state-of-the-art graph construction method by a large
margin for both semi-supervised classification and discriminative analysis,
which verifies the effectiveness of our proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0966</identifier>
 <datestamp>2014-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0966</id><created>2014-09-03</created><authors><author><keyname>Liu</keyname><forenames>Chun-Hao</forenames></author><author><keyname>Pawe&#x142;czak</keyname><forenames>Przemys&#x142;aw</forenames></author><author><keyname>Cabric</keyname><forenames>Danijela</forenames></author></authors><title>Primary User Traffic Classification in Dynamic Spectrum Access Networks</title><categories>cs.PF cs.NI</categories><comments>Accepted to IEEE Journal on Selected Areas in Communications;
  Preliminary version appeared in Proc. IEEE GLOBECOM, Dec. 9-13, 2013,
  Atlanta, GA, USA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper focuses on analytical studies of the primary user (PU) traffic
classification problem. Observing that the gamma distribution can represent
positively skewed data and exponential distribution (popular in communication
networks performance analysis literature) it is considered here as the PU
traffic descriptor. We investigate two PU traffic classifiers utilizing
perfectly measured PU activity (busy) and inactivity (idle) periods: (i)
maximum likelihood classifier (MLC) and (ii) multi-hypothesis sequential
probability ratio test classifier (MSPRTC). Then, relaxing the assumption on
perfect period measurement, we consider a PU traffic observation through
channel sampling. For a special case of negligible probability of PU state
change in between two samplings, we propose a minimum variance PU busy/idle
period length estimator. Later, relaxing the assumption of the complete
knowledge of the parameters of the PU period length distribution, we propose
two PU traffic classification schemes: (i) estimate-then-classify (ETC), and
(ii) average likelihood function (ALF) classifiers considering time domain
fluctuation of the PU traffic parameters. Numerical results show that both MLC
and MSPRTC are sensitive to the periods measurement errors when the distance
among distribution hypotheses is small, and to the distribution parameter
estimation errors when the distance among hypotheses is large. For PU traffic
parameters with a partial prior knowledge of the distribution, the ETC
outperforms ALF when the distance among hypotheses is small, while the opposite
holds when the distance is large.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0973</identifier>
 <datestamp>2014-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0973</id><created>2014-09-03</created><authors><author><keyname>Lai</keyname><forenames>Xiangjing</forenames></author><author><keyname>Lu</keyname><forenames>Zhipeng</forenames></author><author><keyname>Hao</keyname><forenames>Jin-Kao</forenames></author><author><keyname>Glover</keyname><forenames>Fred</forenames></author><author><keyname>Xu</keyname><forenames>Liping</forenames></author></authors><title>Path Relinking for Bandwidth Coloring Problem</title><categories>cs.DM cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Path Relinking algorithm is proposed for the Bandwidth Coloring problem and
the Bandwidth MultiColoring problem. It combines a population based relinking
method and a tabu search based local search procedure. The proposed algorithm
is assessed on two sets of 66 benchmark instances commonly used in the
literature. Computational results demonstrate that the proposed algorithm is
highly competitive in terms of both solution quality and efficiency compared to
the best performing algorithms in the literature. Specifically, it improves the
previous best known results for 15 out of 66 instances, while matching the
previous best known results for 47 cases. Some key elements of the proposed
algorithm are investigated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0979</identifier>
 <datestamp>2014-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0979</id><created>2014-09-03</created><authors><author><keyname>Esmaeilzadeh</keyname><forenames>Mohammad</forenames></author><author><keyname>Aboutorab</keyname><forenames>Neda</forenames></author></authors><title>Inter-session Network Coding for Transmitting Multiple Layered Streams
  over Single-hop Wireless Networks</title><categories>cs.IT math.IT</categories><comments>Accepted to be presented at 2014 IEEE Information Theory Workshop
  (ITW), 5 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the problem of transmitting multiple independent layered
video streams over single-hop wireless networks using network coding (NC). We
combine feedback-free random linear NC (RLNC) with unequal error protection
(UEP) and our goal is to investigate the benefits of coding across streams,
i.e. inter session NC. To this end, we present a transmission scheme that in
addition to mixing packets of different layers of each stream (intra-session
NC), mixes packets of different streams as well. Then, we propose the
analytical formulation of the layer decoding probabilities for each user and
utilize it to define a theoretical performance metric. Assessing this
performance metric under various scenarios, it is observed that inter-session
NC improves the trade-off among the performances of users. Furthermore, the
analytical results show that the throughput gain of inter-session NC over
intra-session NC increases with the number of independent streams and also by
increasing packet error rate, but degrades as network becomes more
heterogeneous.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0980</identifier>
 <datestamp>2015-07-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0980</id><created>2014-09-03</created><updated>2015-07-03</updated><authors><author><keyname>Vychodil</keyname><forenames>Vilem</forenames></author></authors><title>Monoidal functional dependencies</title><categories>cs.DB</categories><msc-class>68P15, 03B52, 03G10</msc-class><acm-class>H.2.4; F.4.1</acm-class><journal-ref>Journal of Computer and System Sciences 81(7) (2015) 1357-1372</journal-ref><doi>10.1016/j.jcss.2015.03.006</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a complete logic for reasoning with functional dependencies (FDs)
with semantics defined over classes of commutative integral partially ordered
monoids and complete residuated lattices. The dependencies allow us to express
stronger relationships between attribute values than the ordinary FDs. In our
setting, the dependencies not only express that certain values are determined
by others but also express that similar values of attributes imply similar
values of other attributes. We show complete axiomatization using a system of
Armstrong-like rules, comment on related computational issues, and the
relational vs. propositional semantics of the dependencies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0982</identifier>
 <datestamp>2014-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0982</id><created>2014-09-03</created><authors><author><keyname>Vainer</keyname><forenames>Evgeny</forenames></author><author><keyname>Yehudai</keyname><forenames>Amiram</forenames></author></authors><title>Taming the Concurrency: Controlling Concurrent Behavior while Testing
  Multithreaded Software</title><categories>cs.SE</categories><acm-class>D.2.5; D.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Developing multithreaded software is an extremely challenging task, even for
experienced programmers. The challenge does not end after the code is written.
There are other tasks associated with a development process that become
exceptionally hard in a multithreaded environment. A good example of this is
creating unit tests for concurrent data structures. In addition to the desired
test logic, such a test contains plenty of synchronization code that makes it
hard to understand and maintain.
  In our work we propose a novel approach for specifying and executing
schedules for multithreaded tests. It allows explicit specification of desired
thread scheduling for some unit test and enforces it during the test execution,
giving the developer an ability to construct deterministic and repeatable unit
tests. This goal is achieved by combining a few basic tools available in every
modern runtime/IDE and does not require dedicated runtime environment, new
specification language or code under test modifications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0988</identifier>
 <datestamp>2014-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0988</id><created>2014-09-03</created><authors><author><keyname>Frey</keyname><forenames>Michael</forenames></author><author><keyname>G&#xfc;nes</keyname><forenames>Mesut</forenames></author></authors><title>Attack of the Ants: Studying Ant Routing Algorithms in Simulation and
  Wireless Testbeds</title><categories>cs.NI</categories><comments>Published in: A. F\&quot;orster, C. Sommer, T. Steinbach, M. W\&quot;ahlisch
  (Eds.), Proc. of 1st OMNeT++ Community Summit, Hamburg, Germany, September 2,
  2014, arXiv:1409.0093, 2014</comments><report-no>OMNET/2014/08</report-no><acm-class>D.4.8; C.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless networks are becoming the key building block of our communications
infrastructure. Examples range from cellular networks to ad hoc and sensor
networks in wildlife monitoring and environmental scenarios. With the rise of
the Internet of Things (IoT) millions of physical and virtual objects will
communicate wireless and enhance the daily life. The adaptivity and scalability
of wireless networks in the IoT is one of the most challenging tasks.
Bio-inspired networking algorithms are a way to tackle these issues. In this
paper we present a simulation framework based on OMNeT++ to implement ant
routing algorithms to study and compare them on the algorithmic level and an
approach to run large simulation studies in a comprehensive way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0991</identifier>
 <datestamp>2014-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0991</id><created>2014-09-03</created><authors><author><keyname>Nguyen</keyname><forenames>Van-Thiep</forenames></author><author><keyname>Gautier</keyname><forenames>Matthieu</forenames></author><author><keyname>Berder</keyname><forenames>Olivier</forenames></author></authors><title>Implementation of an adaptive energy-efficient MAC protocol in
  OMNeT++/MiXiM</title><categories>cs.NI</categories><comments>Published in: A. F\&quot;orster, C. Sommer, T. Steinbach, M. W\&quot;ahlisch
  (Eds.), Proc. of 1st OMNeT++ Community Summit, Hamburg, Germany, September 2,
  2014, arXiv:1409.0093, 2014</comments><report-no>OMNET/2014/07</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, many MAC protocols for wireless sensor networks have been
proposed and most of them are evaluated using Matlab simulator and/or network
simulators (OMNeT++, NS2, etc). However, most of them have a static behavior
and few network simulations are available for adaptive protocols. Specially, in
OMNeT++/MiXiM, there are few energy-efficient MAC protocols for WSNs (B-MAC &amp;
L-MAC) and no adaptive ones. To this end, the TAD-MAC (Traffic Aware Dynamic
MAC) protocol has been simulated in OMNeT++ with the MiXiM framework and
implementation details are given in this paper. The simulation results have
been used to evaluate the performance of TAD-MAC through comparisons with B-MAC
and L-MAC protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0994</identifier>
 <datestamp>2014-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0994</id><created>2014-09-03</created><authors><author><keyname>Stoffers</keyname><forenames>Mirko</forenames></author><author><keyname>Bettermann</keyname><forenames>Ralf</forenames></author><author><keyname>Gross</keyname><forenames>James</forenames></author><author><keyname>Wehrle</keyname><forenames>Klaus</forenames></author></authors><title>Enabling Distributed Simulation of OMNeT++ INET Models</title><categories>cs.NI</categories><comments>Published in: A. F\&quot;orster, C. Sommer, T. Steinbach, M. W\&quot;ahlisch
  (Eds.), Proc. of 1st OMNeT++ Community Summit, Hamburg, Germany, September 2,
  2014, arXiv:1409.0093, 2014</comments><report-no>OMNET/2014/04</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Parallel and distributed simulation have been extensively researched for a
long time. Nevertheless, many simulation models are still executed
sequentially. We attribute this to the fact that many of those models are
simply not capable of being executed in parallel since they violate particular
constraints. In this paper, we analyze the INET model suite, which enables
network simulation in OMNeT++, with regard to parallelizability. We uncovered
several issues preventing parallel execution of INET models. We analyzed those
issues and developed solutions allowing INET models to be run in parallel. A
case study shows the feasibility of our approach. Though there are parts of the
model suite that we didn't investigate yet and the performance can still be
improved, the results show parallelization speedup for most configurations. The
source code of our implementation is available through our web site at
code.comsys.rwth-aachen.de.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.0998</identifier>
 <datestamp>2014-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.0998</id><created>2014-09-03</created><authors><author><keyname>Kawahara</keyname><forenames>Keigo</forenames></author><author><keyname>Matsubara</keyname><forenames>Yutaka</forenames></author><author><keyname>Takada</keyname><forenames>Hiroaki</forenames></author></authors><title>A Simulation Environment and preliminary evaluation for Automotive
  CAN-Ethernet AVB Networks</title><categories>cs.NI</categories><comments>Published in: A. F\&quot;orster, C. Sommer, T. Steinbach, M. W\&quot;ahlisch
  (Eds.), Proc. of 1st OMNeT++ Community Summit, Hamburg, Germany, September 2,
  2014, arXiv:1409.0093, 2014</comments><report-no>OMNET/2014/02</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ethernet is being considered as the backbone network protocol for
next-generation automotive control networks. In such networks, Controller Area
Network (CAN) messages related to automotive control can be sent from a CAN
network to other sub-networks via the backbone Ethernet bus and, if the CAN
messages have real-time constraints, these have to be guaranteed. This paper
presents a simulation environment for CAN--Ethernet Audio Video Bridging (AVB)
mixed networks based on OMNeT++. We use Ethernet AVB, which can guarantee
network bandwidth, to improve the real-time property of CAN messages through
the backbone Ethernet bus. To simulate the networks, we also developed a
CAN--Ethernet AVB gateway (GW) model. To verify the efficacy of our model, we
measured the latency of CAN messages sent from a CAN bus to an Ethernet AVB
node via the backbone Ethernet AVB bus in both bandwidth-guaranteed and
best-effort queue scenarios. The results indicate that the latency of Ethernet
AVB frames containing CAN messages is minimized and limited by the
bandwidth-guaranteed mechanism of Ethernet AVB.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1001</identifier>
 <datestamp>2014-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1001</id><created>2014-09-03</created><authors><author><keyname>Dias</keyname><forenames>Gabriel Martins</forenames></author><author><keyname>Bellalta</keyname><forenames>Boris</forenames></author><author><keyname>Oechsner</keyname><forenames>Simon</forenames></author></authors><title>Towards information-centric WSN simulations</title><categories>cs.NI</categories><comments>Published in: A. F\&quot;orster, C. Sommer, T. Steinbach, M. W\&quot;ahlisch
  (Eds.), Proc. of 1st OMNeT++ Community Summit, Hamburg, Germany, September 2,
  2014, arXiv:1409.0093, 2014</comments><report-no>OMNET/2014/09</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In pursuance of integrating Wireless Sensor Networks (WSNs) with other
systems, the use of techniques from other fields, such as machine learning and
information processing, are becoming more common. Therefore, we faced the
problem of missing network simulations that are not only focused on the packet
exchange between network elements, but also in the data that is transmitted
between them. In other words, we needed a tool that evaluated the WSNs on how
they evolve and react to the environmental changes. To illustrate the benefits
of having such perspective, we explain the kind of simulation problems that we
solved in our last work. Moreover, we outline the next steps in the direction
of creating an extension to support this approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1002</identifier>
 <datestamp>2015-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1002</id><created>2014-09-03</created><updated>2015-10-07</updated><authors><author><keyname>Pierzchlewski</keyname><forenames>Jacek</forenames></author><author><keyname>Arildsen</keyname><forenames>Thomas</forenames></author></authors><title>Generation and Analysis of Constrained Random Sampling Patterns</title><categories>cs.DS</categories><comments>29 pages, 12 figures, submitted to Circuits, Systems and Signal
  Processing journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Random sampling is a technique for signal acquisition which is gaining
popularity in practical signal processing systems. Nowadays, event-driven
analog-to-digital converters make random sampling feasible in practical
applications. A process of random sampling is defined by a sampling pattern,
which indicates signal sampling points in time. Practical random sampling
patterns are constrained by ADC characteristics and application requirements.
In this paper authors introduce statistical methods which evaluate random
sampling pattern generators with emphasis on practical applications.
Furthermore, the authors propose a new random pattern generator which copes
with strict practical limitations imposed on patterns, with possibly minimal
loss in randomness of sampling. The proposed generator is compared with
existing sampling pattern generators using the introduced statistical methods.
It is shown that the proposed algorithm generates random sampling patterns
dedicated for event-driven-ADCs better than existed sampling pattern
generators. Finally, implementation issues of random sampling patterns are
discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1003</identifier>
 <datestamp>2014-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1003</id><created>2014-09-03</created><authors><author><keyname>Schellenberg</keyname><forenames>Sebastian</forenames></author><author><keyname>Berndt</keyname><forenames>R&#xfc;diger</forenames></author><author><keyname>German</keyname><forenames>Reinhard</forenames></author><author><keyname>Eckhoff</keyname><forenames>David</forenames></author></authors><title>Evaluating the Electrification of Vehicle Fleets Using the Veins
  Framework</title><categories>cs.OH</categories><comments>Published in: A. F\&quot;orster, C. Sommer, T. Steinbach, M. W\&quot;ahlisch
  (Eds.), Proc. of 1st OMNeT++ Community Summit, Hamburg, Germany, September 2,
  2014, arXiv:1409.0093, 2014</comments><report-no>OMNET/2014/03</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The case study discussed in this paper involves a company maintaining a
vehicle fleet of one hundred vehicles. In this article we will discuss how we
extend and deploy the Veins framework, which couples OMNeT++ and SUMO, to help
in the process of electrifying this vehicle fleet, i.e., replacing combustion
engine cars with electric vehicles to save money and lower CO$_2$ emissions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1005</identifier>
 <datestamp>2014-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1005</id><created>2014-09-03</created><authors><author><keyname>Setzer</keyname><forenames>Alexander</forenames></author></authors><title>The planar minimum linear arrangement problem is different from the
  minimum linear arrangement problem</title><categories>cs.DS cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In various research papers, such as [2], one will find the claim that the
minLA is optimally solvable on outerplanar graphs, with a reference to [1].
However, the problem solved in that publication, which we refer to as the
planar minimum linear arrangement problem (planar minLA), is different from the
minimum linear arrangement problem (minLA), as we show in this article.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1006</identifier>
 <datestamp>2014-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1006</id><created>2014-09-03</created><authors><author><keyname>Argumanez</keyname><forenames>Humberto Escudero</forenames></author><author><keyname>Tschauner</keyname><forenames>Matthias</forenames></author></authors><title>Tactical communication systems based on civil standards: Modeling in the
  MiXiM framework</title><categories>cs.NI</categories><comments>Published in: A. F\&quot;orster, C. Sommer, T. Steinbach, M. W\&quot;ahlisch
  (Eds.), Proc. of 1st OMNeT++ Community Summit, Hamburg, Germany, September 2,
  2014, arXiv:1409.0093, 2014</comments><report-no>OMNET/2014/06</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, new work is presented belonging to an ongoing study, which
evaluates civil communication standards as potential candidates for the future
military Wide Band Waveforms (WBWFs). After an evaluation process of possible
candidates presented in [2], the selection process in [1] showed that the IEEE
802.11n OFDM could be a possible military WBWF candidate, but it should be
further investigated first in order to enhance or even replace critical
modules. According to this, some critical modules of the physical layer has
been further analyzed in [3] regarding the susceptibility of the OFDM signal
under jammer influences. However, the critical modules of the MAC layer (e.g.,
probabilistic medium access CSMA/CA) have not been analysed. In fact, it was
only suggested in [2] to replace this medium access by the better suited
Unified Slot Allocation Protocol - Multiple Access (USAP-MA) [4]. In this
regard, the present contribution describes the design paradigms of the new MAC
layer and explains how the proposed WBWF candidate has been modelled within the
MiXiM Framework of the OMNeT++ simulator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1023</identifier>
 <datestamp>2014-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1023</id><created>2014-09-03</created><updated>2014-09-09</updated><authors><author><keyname>Nyman</keyname><forenames>Thomas</forenames></author><author><keyname>Ekberg</keyname><forenames>Jan-Erik</forenames></author><author><keyname>Asokan</keyname><forenames>N.</forenames></author></authors><title>Citizen Electronic Identities using TPM 2.0</title><categories>cs.CR</categories><comments>This work is based on an earlier work: Citizen Electronic Identities
  using TPM 2.0, to appear in the Proceedings of the 4th international workshop
  on Trustworthy embedded devices, TrustED'14, November 3, 2014, Scottsdale,
  Arizona, USA, http://dx.doi.org/10.1145/2666141.2666146</comments><acm-class>K.6.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Electronic Identification (eID) is becoming commonplace in several European
countries. eID is typically used to authenticate to government e-services, but
is also used for other services, such as public transit, e-banking, and
physical security access control. Typical eID tokens take the form of physical
smart cards, but successes in merging eID into phone operator SIM cards show
that eID tokens integrated into a personal device can offer better usability
compared to standalone tokens. At the same time, trusted hardware that enables
secure storage and isolated processing of sensitive data have become
commonplace both on PC platforms as well as mobile devices.
  Some time ago, the Trusted Computing Group (TCG) released the version 2.0 of
the Trusted Platform Module (TPM) specification. We propose an eID architecture
based on the new, rich authorization model introduced in the TCGs TPM 2.0. The
goal of the design is to improve the overall security and usability compared to
traditional smart card-based solutions. We also provide, to the best our
knowledge, the first accessible description of the TPM 2.0 authorization model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1026</identifier>
 <datestamp>2014-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1026</id><created>2014-09-03</created><authors><author><keyname>Wunner</keyname><forenames>Patrick</forenames></author><author><keyname>May</keyname><forenames>Stefan</forenames></author><author><keyname>May</keyname><forenames>Stefan</forenames></author><author><keyname>Dengler</keyname><forenames>Sebastian</forenames></author></authors><title>Development and Testing of Automotive Ethernet-Networks together in one
  Tool - OMNeT++</title><categories>cs.NI</categories><comments>Published in: A. F\&quot;orster, C. Sommer, T. Steinbach, M. W\&quot;ahlisch
  (Eds.), Proc. of 1st OMNeT++ Community Summit, Hamburg, Germany, September 2,
  2014, arXiv:1409.0093, 2014</comments><report-no>OMNET/2014/01</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the network simulation framework OMNeT++ is used for
development and testing of automotive Ethernet-Networks. Therefore OMNeT++ is
extended by the INET framework. It is augmented by an implementation of the
protocol SOME/IP (-SD) and an connector to the middleware Gamma V. The
middleware is used to configure the network by initialization. Additionally
data, which is sent over the network, can be changed on the fly.
  The contribution of this work regards three main aspects: First, the use of
OMNeT++ for network development in automotive industry. Second, the employment
of an existing simulation model and using it as restbus simulation for Hardware
in the Loop (HiL) testing or rapid prototyping. Finally, the implementation of
SOME/IP(-SD) into OMNeT++.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1039</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1039</id><created>2014-09-03</created><updated>2014-09-14</updated><authors><author><keyname>Murtagh</keyname><forenames>Fionn</forenames></author><author><keyname>Pianosi</keyname><forenames>Monica</forenames></author><author><keyname>Bull</keyname><forenames>Richard</forenames></author></authors><title>Visualizing and Quantifying Impact and Effect in Twitter Narrative using
  Geometric Data Analysis</title><categories>cs.SI cs.CY physics.soc-ph</categories><comments>34 pages, 11 figures</comments><msc-class>66H25, 62H30, 91F99</msc-class><acm-class>I.7; I.5.3; H.3.1; H.2.8; G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We use geometric multivariate data analysis which has been termed a
methodology for both the visualization and verbalization of data. The general
objectives are data mining and knowledge discovery. In the first case study, we
use the narrative surrounding very highly profiled tweets, and thus a Twitter
event of significance and importance. In the second case study, we use eight
carefully planned Twitter campaigns relating to environmental issues. The aim
of these campaigns was to increase environmental awareness and behaviour.
Unlike current marketing, political and other communication campaigns using
Twitter, we develop an innovative approach to measuring bevavioural change. We
show also how we can assess statistical significance of social media behaviour.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1043</identifier>
 <datestamp>2014-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1043</id><created>2014-09-03</created><authors><author><keyname>Dent</keyname><forenames>Ian</forenames></author><author><keyname>Craig</keyname><forenames>Tony</forenames></author><author><keyname>Aickelin</keyname><forenames>Uwe</forenames></author><author><keyname>Rodden</keyname><forenames>Tom</forenames></author></authors><title>Variability of Behaviour in Electricity Load Profile Clustering; Who
  Does Things at the Same Time Each Day?</title><categories>cs.LG cs.CE</categories><comments>Advances in Data Mining, pp. 70-84, Springer, Heidelberg, 2014, ISBN
  978-3-319-08975-1</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  UK electricity market changes provide opportunities to alter households'
electricity usage patterns for the benefit of the overall electricity network.
Work on clustering similar households has concentrated on daily load profiles
and the variability in regular household behaviours has not been considered.
Those households with most variability in regular activities may be the most
receptive to incentives to change timing.
  Whether using the variability of regular behaviour allows the creation of
more consistent groupings of households is investigated and compared with daily
load profile clustering. 204 UK households are analysed to find repeating
patterns (motifs). Variability in the time of the motif is used as the basis
for clustering households. Different clustering algorithms are assessed by the
consistency of the results.
  Findings show that variability of behaviour, using motifs, provides more
consistent groupings of households across different clustering algorithms and
allows for more efficient targeting of behaviour change interventions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1045</identifier>
 <datestamp>2014-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1045</id><created>2014-09-03</created><authors><author><keyname>McCullochy</keyname><forenames>Josie C.</forenames></author><author><keyname>Hinde</keyname><forenames>Chris J.</forenames></author><author><keyname>Wagner</keyname><forenames>Christian</forenames></author><author><keyname>Aickelin</keyname><forenames>Uwe</forenames></author></authors><title>A Fuzzy Directional Distance Measure</title><categories>cs.AI</categories><comments>Proceedings of the 2014 World Congress on Computational Intelligence
  (WCCI 2014), pp. 141-148, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The measure of distance between two fuzzy sets is a fundamental tool within
fuzzy set theory, however, distance measures currently within the literature
use a crisp value to represent the distance between fuzzy sets. A real valued
distance measure is developed into a fuzzy distance measure which better
reflects the uncertainty inherent in fuzzy sets and a fuzzy directional
distance measure is presented, which accounts for the direction of change
between fuzzy sets. A multiplicative version is explored as a full maximal
assignment is computationally intractable so an intermediate solution is
offered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1046</identifier>
 <datestamp>2014-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1046</id><created>2014-09-03</created><authors><author><keyname>McCulloch</keyname><forenames>Josie</forenames></author><author><keyname>Wagner</keyname><forenames>Christian</forenames></author><author><keyname>Aickelin</keyname><forenames>Uwe</forenames></author></authors><title>Analysing Fuzzy Sets Through Combining Measures of Similarity and
  Distance</title><categories>cs.AI</categories><comments>Proceedings of the 2014 World Congress on Computational Intelligence
  (WCCI 2014), pp. 155-162, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reasoning with fuzzy sets can be achieved through measures such as similarity
and distance. However, these measures can often give misleading results when
considered independently, for example giving the same value for two different
pairs of fuzzy sets. This is particularly a problem where many fuzzy sets are
generated from real data, and while two different measures may be used to
automatically compare such fuzzy sets, it is difficult to interpret two
different results. This is especially true where a large number of fuzzy sets
are being compared as part of a reasoning system. This paper introduces a
method for combining the results of multiple measures into a single measure for
the purpose of analysing and comparing fuzzy sets. The combined measure
alleviates ambiguous results and aids in the automatic comparison of fuzzy
sets. The properties of the combined measure are given, and demonstrations are
presented with discussions on the advantages over using a single measure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1052</identifier>
 <datestamp>2014-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1052</id><created>2014-09-03</created><authors><author><keyname>Zhou</keyname><forenames>Yuzhe</forenames></author><author><keyname>Ai</keyname><forenames>Bo</forenames></author></authors><title>Provide High-QoS of the High-Speed Railway Mobile Communications in
  Cyber-Physical Systems</title><categories>cs.NI</categories><comments>6 pages, 5 figures, The IEEE International Conference on Cyber,
  Physical and Social Computing (CPSCom 2012), accepted for publication,
  November, 2012. arXiv admin note: text overlap with arXiv:0805.2854 by other
  authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Technical advances in networks, embedded computing, and wireless
communications are leading to the next generation of complex intelligent
systems called Cyber-Physical Systems (CPS). CPS promises to transform the way
we interact with the physical world. Efficient and reliable operation of the
CTCS-3 (Chinese Train Control System Level 3) is great protection of the
national economy and public safety. The CTCS-3 is based on GSM-R (GSM for
Railway) to achieve a continuous and two-way transmission of information
between ground and the train. To ensure the growing needs of safety, fastness
and service diversity of China's railway, the pursuit of high-QoS has been the
key of the relative study. This paper examines the main characteristics of
GSM-R and the requirements of all-layers' QoS indicators for GSM-R. Several
main technologies of improving QoS indicators of all-layers are summarized. As
a solution, a comprehensive scheme is proposed to improve the delay and packet
loss indicators. An example is also presented that illustrates the real-time
features of the proposed solution. Based on the CPS characteristics are highly
correlated with the QoS indicators, conclusions are made that GSM-R can provide
a reliable and real-time way for message.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1053</identifier>
 <datestamp>2014-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1053</id><created>2014-09-03</created><authors><author><keyname>Reps</keyname><forenames>Jenna M.</forenames></author><author><keyname>Aickelin</keyname><forenames>Uwe</forenames></author><author><keyname>Garibaldi</keyname><forenames>Jonathan M.</forenames></author></authors><title>Tuning a Multiple Classifier System for Side Effect Discovery using
  Genetic Algorithms</title><categories>cs.LG cs.CE</categories><comments>Proceedings of the 2014 World Congress on Computational Intelligence
  (WCCI 2014), pp. 910-917, IEEE, Beijing, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In previous work, a novel supervised framework implementing a binary
classifier was presented that obtained excellent results for side effect
discovery. Interestingly, unique side effects were identified when different
binary classifiers were used within the framework, prompting the investigation
of applying a multiple classifier system. In this paper we investigate tuning a
side effect multiple classifying system using genetic algorithms. The results
of this research show that the novel framework implementing a multiple
classifying system trained using genetic algorithms can obtain a higher partial
area under the receiver operating characteristic curve than implementing a
single classifier. Furthermore, the framework is able to detect side effects
efficiently and obtains a low false positive rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1055</identifier>
 <datestamp>2014-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1055</id><created>2014-09-03</created><authors><author><keyname>Hassan</keyname><forenames>Diman</forenames></author><author><keyname>Aickelin</keyname><forenames>Uwe</forenames></author><author><keyname>Wagner</keyname><forenames>Christian</forenames></author></authors><title>Comparison of Distance Metrics for Hierarchical Data in Medical
  Databases</title><categories>cs.DB cs.CE</categories><comments>Proceedings of the 2014 World Congress on Computational Intelligence
  (WCCI 2014), pp. 3636-3643, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distance metrics are broadly used in different research areas and
applications, such as bio-informatics, data mining and many other fields.
However, there are some metrics, like pq-gram and Edit Distance used
specifically for data with a hierarchical structure. Other metrics used for
non-hierarchical data are the geometric and Hamming metrics. We have applied
these metrics to The Health Improvement Network (THIN) database which has some
hierarchical data. The THIN data has to be converted into a tree-like structure
for the first group of metrics. For the second group of metrics, the data are
converted into a frequency table or matrix, then for all metrics, all distances
are found and normalised. Based on this particular data set, our research
question: which of these metrics is useful for THIN data? This paper compares
the metrics, particularly the pq-gram metric on finding the similarities of
patients' data. It also investigates the similar patients who have the same
close distances as well as the metrics suitability for clustering the whole
patient population. Our results show that the two groups of metrics perform
differently as they represent different structures of the data. Nevertheless,
all the metrics could represent some similar data of patients as well as
discriminate sufficiently well in clustering the patient population using
$k$-means clustering algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1057</identifier>
 <datestamp>2014-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1057</id><created>2014-09-03</created><authors><author><keyname>Ladas</keyname><forenames>Alexandros</forenames></author><author><keyname>Garibaldi</keyname><forenames>Jonathan M.</forenames></author><author><keyname>Scarpel</keyname><forenames>Rodrigo</forenames></author><author><keyname>Aickelin</keyname><forenames>Uwe</forenames></author></authors><title>Augmented Neural Networks for Modelling Consumer Indebtness</title><categories>cs.CE cs.LG cs.NE</categories><comments>Proceedings of the 2014 World Congress on Computational Intelligence
  (WCCI 2014), pp. 3086-3093, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consumer Debt has risen to be an important problem of modern societies,
generating a lot of research in order to understand the nature of consumer
indebtness, which so far its modelling has been carried out by statistical
models. In this work we show that Computational Intelligence can offer a more
holistic approach that is more suitable for the complex relationships an
indebtness dataset has and Linear Regression cannot uncover. In particular, as
our results show, Neural Networks achieve the best performance in modelling
consumer indebtness, especially when they manage to incorporate the significant
and experimentally verified results of the Data Mining process in the model,
exploiting the flexibility Neural Networks offer in designing their topology.
This novel method forms an elaborate framework to model Consumer indebtness
that can be extended to any other real world application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1059</identifier>
 <datestamp>2014-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1059</id><created>2014-09-03</created><authors><author><keyname>Liu</keyname><forenames>Yihui</forenames></author><author><keyname>Aickelin</keyname><forenames>Uwe</forenames></author></authors><title>Detecting adverse drug reactions for the drug Simvastatin</title><categories>cs.CE</categories><comments>Fourth International Conference on Multimedia Information Networking
  and Security (MINES), pp. 246-249, 2012. arXiv admin note: substantial text
  overlap with arXiv:1409.0658</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Adverse drug reactions (ADR) are widely concerning for public health issue.
In this study we propose an original approach to detect ADRs using a feature
matrix and feature selection. The experiments are carried out on the drug
Simvastatin. Major side effects for the drug are detected and better
performance is achieved compared to other computerized methods. Because
currently the detected ADRs are based solely on computerized methods, further
expert investigation is needed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1060</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1060</id><created>2014-09-03</created><authors><author><keyname>Fouche</keyname><forenames>Willem L.</forenames></author></authors><title>Kolmogorov complexity and the geometry of Brownian motion</title><categories>cs.CC</categories><comments>Paper accepted by Mathematical Structures in Computer Science</comments><journal-ref>Math. Struct. in Comp. Science 25 (2014) 1590-1606</journal-ref><doi>10.1017/S0960129513000273</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we continue the study of the geometry of Brownian motions
which are encoded by Kolmogorov-Chaitin random reals (complex oscillations). We
unfold Kolmogorov-Chaitin complexity in the context of Brownian motion and
specifically to phenomena emerging from the random geometric patterns generated
by a Brownian motion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1062</identifier>
 <datestamp>2014-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1062</id><created>2014-09-03</created><authors><author><keyname>Shang</keyname><forenames>Fanhua</forenames></author><author><keyname>Liu</keyname><forenames>Yuanyuan</forenames></author><author><keyname>Tong</keyname><forenames>Hanghang</forenames></author><author><keyname>Cheng</keyname><forenames>James</forenames></author><author><keyname>Cheng</keyname><forenames>Hong</forenames></author></authors><title>Structured Low-Rank Matrix Factorization with Missing and Grossly
  Corrupted Observations</title><categories>cs.LG cs.CV stat.ML</categories><comments>28 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recovering low-rank and sparse matrices from incomplete or corrupted
observations is an important problem in machine learning, statistics,
bioinformatics, computer vision, as well as signal and image processing. In
theory, this problem can be solved by the natural convex joint/mixed
relaxations (i.e., l_{1}-norm and trace norm) under certain conditions.
However, all current provable algorithms suffer from superlinear per-iteration
cost, which severely limits their applicability to large-scale problems. In
this paper, we propose a scalable, provable structured low-rank matrix
factorization method to recover low-rank and sparse matrices from missing and
grossly corrupted data, i.e., robust matrix completion (RMC) problems, or
incomplete and grossly corrupted measurements, i.e., compressive principal
component pursuit (CPCP) problems. Specifically, we first present two
small-scale matrix trace norm regularized bilinear structured factorization
models for RMC and CPCP problems, in which repetitively calculating SVD of a
large-scale matrix is replaced by updating two much smaller factor matrices.
Then, we apply the alternating direction method of multipliers (ADMM) to
efficiently solve the RMC problems. Finally, we provide the convergence
analysis of our algorithm, and extend it to address general CPCP problems.
Experimental results verified both the efficiency and effectiveness of our
method compared with the state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1064</identifier>
 <datestamp>2015-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1064</id><created>2014-09-03</created><updated>2015-01-22</updated><authors><author><keyname>Gabora</keyname><forenames>Liane</forenames></author></authors><title>Physical Light as a Metaphor for Inner Light</title><categories>q-bio.NC cs.CG cs.MM</categories><comments>17 pages; In a special issue of Aisthesis
  (http://www.fupress.net/index.php/aisthesis/index) on &quot;Giving form through
  metaphors&quot;; seeking collaborators in computer graphics, visualization, and
  optics for further development of these projects. arXiv admin note:
  substantial text overlap with arXiv:1501.00029</comments><journal-ref>Aisthesis, 7(2), 43-61 (2014)</journal-ref><doi>10.13128/Aisthesis-15289</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The metaphor between physical light and inner light has a long history that
permeates diverse languages and cultures. This paper outlines a system for
using basic principles from optics to visually represent psychological states
and processes such as ideation, enlightenment, mindfulness, and fragmentation
versus integrity, as well as situations that occur between people involving
phenomena such as honest versus deceptive communication, and understanding
versus misunderstanding. The paper summarizes two ongoing projects based on
this system: The Light and Enlightenment art installation project, and the
Soultracker virtual reality project. These projects enable people to depict
their inner lives and external worlds including situations and relationships
with others, both as they are and as they could be, and explore alternative
paths for navigating challenges and living to their fullest potential. The
projects aim to be of clinical value as therapeutic tools, as well as of
pedagogical value by providing a concrete language for depicting aspects of
human nature that can otherwise seem elusive and intangible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1066</identifier>
 <datestamp>2014-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1066</id><created>2014-09-03</created><authors><author><keyname>Falahrastegar</keyname><forenames>Marjan</forenames></author><author><keyname>Haddadi</keyname><forenames>Hamed</forenames></author><author><keyname>Uhlig</keyname><forenames>Steve</forenames></author><author><keyname>Mortier</keyname><forenames>Richard</forenames></author></authors><title>Anatomy of the Third-Party Web Tracking Ecosystem</title><categories>cs.SI cs.CY</categories><acm-class>K.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The presence of third-party tracking on websites has become customary.
However, our understanding of the third-party ecosystem is still very
rudimentary. We examine third-party trackers from a geographical perspective,
observing the third-party tracking ecosystem from 29 countries across the
globe. When examining the data by region (North America, South America, Europe,
East Asia, Middle East, and Oceania), we observe significant geographical
variation between regions and countries within regions. We find trackers that
focus on specific regions and countries, and some that are hosted in countries
outside their expected target tracking domain. Given the differences in
regulatory regimes between jurisdictions, we believe this analysis sheds light
on the geographical properties of this ecosystem and on the problems that these
may pose to our ability to track and manage the different data silos that now
store personal data about us all.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1073</identifier>
 <datestamp>2014-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1073</id><created>2014-09-03</created><authors><author><keyname>Lai</keyname><forenames>Xinsheng</forenames></author><author><keyname>Zhou</keyname><forenames>Yuren</forenames></author><author><keyname>He</keyname><forenames>Jun</forenames></author><author><keyname>Zhang</keyname><forenames>Jun</forenames></author></authors><title>Performance Analysis on Evolutionary Algorithms for the Minimum Label
  Spanning Tree Problem</title><categories>cs.NE cs.DS</categories><doi>10.1109/TEVC.2013.2291790</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Some experimental investigations have shown that evolutionary algorithms
(EAs) are efficient for the minimum label spanning tree (MLST) problem.
However, we know little about that in theory. As one step towards this issue,
we theoretically analyze the performances of the (1+1) EA, a simple version of
EAs, and a multi-objective evolutionary algorithm called GSEMO on the MLST
problem. We reveal that for the MLST$_{b}$ problem the (1+1) EA and GSEMO
achieve a $\frac{b+1}{2}$-approximation ratio in expected polynomial times of
$n$ the number of nodes and $k$ the number of labels. We also show that GSEMO
achieves a $(2ln(n))$-approximation ratio for the MLST problem in expected
polynomial time of $n$ and $k$. At the same time, we show that the (1+1) EA and
GSEMO outperform local search algorithms on three instances of the MLST
problem. We also construct an instance on which GSEMO outperforms the (1+1) EA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1078</identifier>
 <datestamp>2014-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1078</id><created>2014-09-02</created><authors><author><keyname>Wu</keyname><forenames>Yuanxin</forenames></author></authors><title>Versatile Land Navigation Using Inertial Sensors and Odometry:
  Self-calibration, In-motion Alignment and Positioning</title><categories>cs.RO</categories><journal-ref>Inertial Sensors and Systems - Symposium Gyro Technology
  (ISS-SGT), Karlsruhe, September 16 to 17, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inertial measurement unit (IMU) and odometer have been commonly-used sensors
for autonomous land navigation in the global positioning system (GPS)-denied
scenarios. This paper systematically proposes a versatile strategy for
self-contained land vehicle navigation using the IMU and an odometer.
Specifically, the paper proposes a self-calibration and refinement method for
IMU/odometer integration that is able to overcome significant variation of the
misalignment parameters, which are induced by many inevitable and adverse
factors such as load changing, refueling and ambient temperature. An
odometer-aided IMU in-motion alignment algorithm is also devised that enables
the first-responsive functionality even when the vehicle is running freely. The
versatile strategy is successfully demonstrated and verified via long-distance
real tests.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1102</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1102</id><created>2014-09-03</created><authors><author><keyname>Han</keyname><forenames>Qiwei</forenames></author><author><keyname>Ferreira</keyname><forenames>Pedro</forenames></author></authors><title>The Role of Peer Influence in Churn in Wireless Networks</title><categories>cs.SI cs.CY physics.soc-ph</categories><comments>Accepted in Seventh ASE International Conference on Social Computing
  (Socialcom 2014), Best Paper Award Winner</comments><acm-class>H.4</acm-class><doi>10.1145/2639968.2640057</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Subscriber churn remains a top challenge for wireless carriers. These
carriers need to understand the determinants of churn to confidently apply
effective retention strategies to ensure their profitability and growth. In
this paper, we look at the effect of peer influence on churn and we try to
disentangle it from other effects that drive simultaneous churn across friends
but that do not relate to peer influence. We analyze a random sample of roughly
10 thousand subscribers from large dataset from a major wireless carrier over a
period of 10 months. We apply survival models and generalized propensity score
to identify the role of peer influence. We show that the propensity to churn
increases when friends do and that it increases more when many strong friends
churn. Therefore, our results suggest that churn managers should consider
strategies aimed at preventing group churn. We also show that survival models
fail to disentangle homophily from peer influence over-estimating the effect of
peer influence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1122</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1122</id><created>2014-09-03</created><updated>2014-09-08</updated><authors><author><keyname>Limmer</keyname><forenames>Steffen</forenames></author><author><keyname>Stanczak</keyname><forenames>Slawomir</forenames></author></authors><title>On $\ell_p$-norm Computation over Multiple-Access Channels</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Information Theory Workshop (ITW 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses some aspects of the general problem of information
transfer and distributed function computation in wireless networks. Many
applications of wireless technology foresee networks of autonomous devices
executing tasks that can be posed as distributed function computation. In
today's wireless networks, the tasks of communication and (distributed)
computation are performed separately, although an efficient network operation
calls for approaches in which the information transfer is dynamically adapted
to time-varying computation objectives. Thus, wireless communications and
function computation must be tightly coupled and it is shown in this paper that
information theory may play a crucial role in the design of efficient
computation-aware wireless communication and networking strategies. This is
explained in more detail by considering the problem of computing $\ell_p$-norms
over multiple access channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1134</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1134</id><created>2014-09-03</created><updated>2014-09-05</updated><authors><author><keyname>Jain</keyname><forenames>Rishabh</forenames></author><author><keyname>S.</keyname><forenames>Abhishek B.</forenames></author><author><keyname>Jagannath</keyname><forenames>Satvik</forenames></author></authors><title>Mining and Analyzing Twitter trends: Frequency based ranking of
  descriptive Tweets</title><categories>cs.SI</categories><msc-class>68T02</msc-class><doi>10.5120/18279-9200</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the major sources of trending news, events and opinion in the current
age is micro blogging. Twitter, being one of them, is extensively used to mine
data about public responses and event updates. This paper intends to propose
methods to filter tweets to obtain the most accurately descriptive tweets,
which communicates the content of the trend. It also potentially ranks the
tweets according to relevance. The principle behind the ranking mechanism would
be the assumed tendencies in the natural language used by the users. The
mapping frequencies of occurrence of words and related hash tags is used to
create a weighted score for each tweet in the sample space obtained from
twitter on a particular trend.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1136</identifier>
 <datestamp>2014-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1136</id><created>2014-09-03</created><updated>2014-11-28</updated><authors><author><keyname>Cotton-Barratt</keyname><forenames>Conrad</forenames></author><author><keyname>Murawski</keyname><forenames>Andrzej</forenames></author><author><keyname>Ong</keyname><forenames>Luke</forenames></author></authors><title>Weak and Nested Class Memory Automata</title><categories>cs.FL</categories><comments>Preprint of LATA'15 paper</comments><acm-class>F.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automata over infinite alphabets have recently come to be studied extensively
as potentially useful tools for solving problems in verification and database
theory. One popular model of automata studied is the Class Memory Automata
(CMA), for which the emptiness problem is equivalent to Petri Net Reachability.
We identify a restriction - which we call weakness - of CMA, and show that
their emptiness problem is equivalent to Petri Net Coverability. Further, we
show that in the deterministic case they are closed under all Boolean
operations. We clarify the connections between weak CMA and existing automata
over data languages. We also extend CMA to operate over multiple levels of
nested data values, and show that while these have undecidable emptiness in
general, adding the weakness constraint recovers decidability of emptiness, via
reduction to coverability in well-structured transition systems. We also
examine connections with existing automata over nested data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1143</identifier>
 <datestamp>2014-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1143</id><created>2014-09-03</created><authors><author><keyname>Manukyan</keyname><forenames>Narine</forenames></author><author><keyname>Eppstein</keyname><forenames>Margaret J.</forenames></author><author><keyname>Buzas</keyname><forenames>Jeffrey S.</forenames></author></authors><title>Tunably Rugged Landscapes with Known Maximum and Minimum</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose NM landscapes as a new class of tunably rugged benchmark problems.
NM landscapes are well-defined on alphabets of any arity, including both
discrete and real-valued alphabets, include epistasis in a natural and
transparent manner, are proven to have known value and location of the global
maximum and, with some additional constraints, are proven to also have a known
global minimum. Empirical studies are used to illustrate that, when
coefficients are selected from a recommended distribution, the ruggedness of NM
landscapes is smoothly tunable and correlates with several measures of search
difficulty. We discuss why these properties make NM landscapes preferable to
both NK landscapes and Walsh polynomials as benchmark landscape models with
tunable epistasis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1144</identifier>
 <datestamp>2014-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1144</id><created>2014-09-03</created><authors><author><keyname>Zaidi</keyname><forenames>Abdellatif</forenames></author></authors><title>Achievable Regions for Interference Channels with Generalized and
  Intermittent Feedback</title><categories>cs.IT math.IT</categories><comments>To appear in Proc. of the 2014 IEEE International Symposium on
  Information Theory, 6 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we first study a two-user interference channel with
generalized feedback. We establish an inner bound on its capacity region. The
coding scheme that we employ for the inner bound is based on an appropriate
combination of Han-Kobayash rate splitting and compress-and-forward at the
senders. Each sender compresses the channel output that is observes using a
compression scheme that is \`a-la Lim et al. noisy network coding and
Avestimeher et al. quantize-map-and-forward. Next, we study an injective
deterministic model in which the senders obtain output feedback only
intermittently. Specializing the coding scheme of the model with generalized
feedback to this scenario, we obtain useful insights onto effective ways of
combining noisy network coding with interference alignment techniques. We also
apply our results to linear deterministic interference channels with
intermittent feedback.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1147</identifier>
 <datestamp>2015-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1147</id><created>2014-09-03</created><updated>2015-02-24</updated><authors><author><keyname>Hwang</keyname><forenames>S.</forenames></author><author><keyname>Choi</keyname><forenames>S.</forenames></author><author><keyname>Lee</keyname><forenames>Deokjae</forenames></author><author><keyname>Kahng</keyname><forenames>B.</forenames></author></authors><title>Efficient algorithm to compute mutually connected components in
  interdependent networks</title><categories>cond-mat.stat-mech cs.SI physics.soc-ph</categories><comments>6 pages 5 figures</comments><journal-ref>Phys. Rev. E 91, 022814 (2015)</journal-ref><doi>10.1103/PhysRevE.91.022814</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mutually connected components (MCCs) play an important role as a measure of
resilience in the study of interdependent networks. Despite their importance,
an efficient algorithm to obtain the statistics of all MCCs during the removal
of links has thus far been absent. Here, using a well-known fully dynamic graph
algorithm, we propose an efficient algorithm to accomplish this task. We show
that the time complexity of this algorithm is approximately $O({N^{1.2} })$ for
random graphs, which is more efficient than $O(N^{2})$ of the brute-force
algorithm. We confirm the correctness of our algorithm by comparing the
behavior of the order parameter as links are removed with existing results for
three types of double-layer multiplex networks. We anticipate that this
algorithm will be used for simulations of large-size systems that have been
previously inaccessible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1148</identifier>
 <datestamp>2014-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1148</id><created>2014-09-03</created><authors><author><keyname>Abou-zeid</keyname><forenames>Hatem</forenames></author><author><keyname>Hassenein</keyname><forenames>Hosssam S.</forenames></author></authors><title>Toward Green Media Delivery: Location-Aware Opportunities and Approaches</title><categories>cs.MM cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile media has undoubtedly become the predominant source of traffic in
wireless networks. The result is not only congestion and poor
Quality-of-Experience, but also an unprecedented energy drain at both the
network and user devices. In order to sustain this continued growth, novel
disruptive paradigms of media delivery are urgently needed. We envision that
two key contemporary advancements can be leveraged to develop greener media
delivery platforms: 1) the proliferation of navigation hardware and software in
mobile devices has created an era of location-awareness, where both the current
and future user locations can be predicted; and 2) the rise of context-aware
network architectures and self-organizing functionalities is enabling context
signaling and in-network adaptation. With these developments in mind, this
article investigates the opportunities of exploiting location-awareness to
enable green end-to-end media delivery. In particular, we discuss and propose
approaches for location-based adaptive video quality planning, in-network
caching, content prefetching, and long-term radio resource management. To
provide insights on the energy savings, we then present a cross-layer framework
that jointly optimizes resource allocation and multi-user video quality using
location predictions. Finally, we highlight some of the future research
directions for location-aware media delivery in the conclusion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1152</identifier>
 <datestamp>2014-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1152</id><created>2014-09-02</created><authors><author><keyname>Saha</keyname><forenames>Tanay Kumar</forenames></author><author><keyname>Hasan</keyname><forenames>Mohammad Al</forenames></author></authors><title>FS^3: A Sampling based method for top-k Frequent Subgraph Mining</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mining labeled subgraph is a popular research task in data mining because of
its potential application in many different scientific domains. All the
existing methods for this task explicitly or implicitly solve the subgraph
isomorphism task which is computationally expensive, so they suffer from the
lack of scalability problem when the graphs in the input database are large. In
this work, we propose FS^3, which is a sampling based method. It mines a small
collection of subgraphs that are most frequent in the probabilistic sense. FS^3
performs a Markov Chain Monte Carlo (MCMC) sampling over the space of a
fixed-size subgraphs such that the potentially frequent subgraphs are sampled
more often. Besides, FS^3 is equipped with an innovative queue manager. It
stores the sampled subgraph in a finite queue over the course of mining in such
a manner that the top-k positions in the queue contain the most frequent
subgraphs. Our experiments on database of large graphs show that FS^3 is
efficient, and it obtains subgraphs that are the most frequent amongst the
subgraphs of a given size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1170</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1170</id><created>2014-09-03</created><updated>2014-09-13</updated><authors><author><keyname>Latif</keyname><forenames>Kamran</forenames></author></authors><title>Hybrid Systems Knowledge Representation Using Modelling Environment
  System Techniques Artificial Intelligence</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Knowledge-based or Artificial Intelligence techniques are used increasingly
as alternatives to more classical techniques to model ENVIRONMENTAL SYSTEMS.
Use of Artificial Intelligence (AI) in environmental modelling has increased
with recognition of its potential. In this paper we examine the DIFFERENT
TECHNIQUES of Artificial intelligence with profound examples of human
perception, learning and reasoning to solve complex problems. However with the
increase of complexity better methods are required. Keeping in view of the
above some researchers introduced the idea of hybrid mechanism in which two or
more methods can be combined which seems to be a positive effort for creating a
more complex; advanced and intelligent system which has the capability to in-
cooperate human decisions thus driving the landscape changes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1177</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1177</id><created>2014-09-03</created><updated>2015-09-13</updated><authors><author><keyname>Kirsche</keyname><forenames>Michael</forenames></author><author><keyname>Schnurbusch</keyname><forenames>Matti</forenames></author></authors><title>A New IEEE 802.15.4 Simulation Model for OMNeT++ / INET</title><categories>cs.NI</categories><comments>Published in: A. F\&quot;orster, C. Sommer, T. Steinbach, M. W\&quot;ahlisch
  (Eds.), Proc. of 1st OMNeT++ Community Summit, Hamburg, Germany, September 2,
  2014, arXiv:1409.0093, 2014</comments><proxy>Michael Kirsche</proxy><report-no>OMNET/2014/05</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a new IEEE 802.15.4 simulation model for OMNeT++ /
INET. 802.15.4 is an important underlying standard for wireless sensor networks
and Internet of Things scenarios. The presented implementation is designed to
be compatible with OMNeT++ 4.x and INET 2.x and laid-out to be expandable for
newer revisions of the 802.15.4 standard. The source code is available online
https://github.com/michaelkirsche/IEEE802154INET-Standalone
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1184</identifier>
 <datestamp>2014-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1184</id><created>2014-09-03</created><authors><author><keyname>Fang</keyname><forenames>Z.</forenames></author><author><keyname>Wang</keyname><forenames>X.</forenames></author><author><keyname>Yuan</keyname><forenames>X.</forenames></author></authors><title>Spectral Efficiency of the Cellular Two-Way Relaying with Large Antenna
  Arrays</title><categories>cs.IT math.IT</categories><comments>submitted to ICC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers a multiuser cellular two-way relay network (cTWRN) where
multiple users exchange information with a base station (BS) via a relay
station (RS). Each user is equipped with a single antenna, while both the BS
and the RS are equipped with a very large antenna array. We investigate the
performance of the cTWRN with amplify-and-forward (AF) based physical-layer
network coding, and derive closed-form expression for the asymptotic spectral
efficiency when both the number of antennas at the BS and the RS grow large. It
is shown that the noise propagation of the non-regenerative relaying protocol
can be greatly suppressed, and the AF relaying scheme can approach the cut-set
bound under certain conditions. We also investigate the performance of the AF
relaying scheme under two power-scaling cases, and show that the transmit power
of the BS and each user can be made inversely proportional to the number of
relay antennas while maintaining a given quality-of-service. Numerical results
are presented to verify the analytical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1194</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1194</id><created>2014-09-03</created><authors><author><keyname>Govindarajan</keyname><forenames>Sathish</forenames></author><author><keyname>Nivasch</keyname><forenames>Gabriel</forenames></author></authors><title>A variant of the Hadwiger-Debrunner (p,q)-problem in the plane</title><categories>cs.CG cs.DM math.CO</categories><comments>10 pages, 1 figure</comments><msc-class>68U05, 52C99</msc-class><journal-ref>Discrete Comput. Geom. 54:637-646, 2015</journal-ref><doi>10.1007/s00454-015-9723-9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $X$ be a convex curve in the plane (say, the unit circle), and let
$\mathcal S$ be a family of planar convex bodies, such that every two of them
meet at a point of $X$. Then $\mathcal S$ has a transversal $N\subset\mathbb
R^2$ of size at most $1.75\cdot 10^9$.
  Suppose instead that $\mathcal S$ only satisfies the following
&quot;$(p,2)$-condition&quot;: Among every $p$ elements of $\mathcal S$ there are two
that meet at a common point of $X$. Then $\mathcal S$ has a transversal of size
$O(p^8)$. For comparison, the best known bound for the Hadwiger--Debrunner $(p,
q)$-problem in the plane, with $q=3$, is $O(p^6)$.
  Our result generalizes appropriately for $\mathbb R^d$ if $X\subset \mathbb
R^d$ is, for example, the moment curve.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1199</identifier>
 <datestamp>2014-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1199</id><created>2014-09-03</created><authors><author><keyname>Plaza</keyname><forenames>Stephen M.</forenames></author></authors><title>Focused Proofreading: Efficiently Extracting Connectomes from Segmented
  EM Images</title><categories>q-bio.QM cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Identifying complex neural circuitry from electron microscopic (EM) images
may help unlock the mysteries of the brain. However, identifying this circuitry
requires time-consuming, manual tracing (proofreading) due to the size and
intricacy of these image datasets, thus limiting state-of-the-art analysis to
very small brain regions. Potential avenues to improve scalability include
automatic image segmentation and crowd sourcing, but current efforts have had
limited success. In this paper, we propose a new strategy, focused
proofreading, that works with automatic segmentation and aims to limit
proofreading to the regions of a dataset that are most impactful to the
resulting circuit. We then introduce a novel workflow, which exploits
biological information such as synapses, and apply it to a large dataset in the
fly optic lobe. With our techniques, we achieve significant tracing speedups of
3-5x without sacrificing the quality of the resulting circuit. Furthermore, our
methodology makes the task of proofreading much more accessible and hence
potentially enhances the effectiveness of crowd sourcing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1200</identifier>
 <datestamp>2014-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1200</id><created>2014-09-03</created><authors><author><keyname>Wang</keyname><forenames>Jim Jing-Yan</forenames></author></authors><title>Domain Transfer Structured Output Learning</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose the problem of domain transfer structured output
learn- ing and the first solution to solve it. The problem is defined on two
different data domains sharing the same input and output spaces, named as
source domain and target domain. The outputs are structured, and for the data
samples of the source domain, the corresponding outputs are available, while
for most data samples of the target domain, the corresponding outputs are
missing. The input distributions of the two domains are significantly
different. The problem is to learn a predictor for the target domain to predict
the structured outputs from the input. Due to the limited number of outputs
available for the samples form the target domain, it is difficult to directly
learn the predictor from the target domain, thus it is necessary to use the
output information available in source domain. We propose to learn the target
domain predictor by adapting a auxiliary predictor trained by using source
domain data to the target domain. The adaptation is implemented by adding a
delta function on the basis of the auxiliary predictor. An algorithm is
developed to learn the parameter of the delta function to minimize loss
functions associat- ed with the predicted outputs against the true outputs of
the data samples with available outputs of the target domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1257</identifier>
 <datestamp>2014-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1257</id><created>2014-09-03</created><updated>2014-10-07</updated><authors><author><keyname>Pouget-Abadie</keyname><forenames>Jean</forenames></author><author><keyname>Bahdanau</keyname><forenames>Dzmitry</forenames></author><author><keyname>van Merrienboer</keyname><forenames>Bart</forenames></author><author><keyname>Cho</keyname><forenames>Kyunghyun</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author></authors><title>Overcoming the Curse of Sentence Length for Neural Machine Translation
  using Automatic Segmentation</title><categories>cs.CL cs.LG cs.NE stat.ML</categories><comments>Eighth Workshop on Syntax, Semantics and Structure in Statistical
  Translation (SSST-8)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The authors of (Cho et al., 2014a) have shown that the recently introduced
neural network translation systems suffer from a significant drop in
translation quality when translating long sentences, unlike existing
phrase-based translation systems. In this paper, we propose a way to address
this issue by automatically segmenting an input sentence into phrases that can
be easily translated by the neural network translation model. Once each segment
has been independently translated by the neural machine translation model, the
translated clauses are concatenated to form a final translation. Empirical
results show a significant improvement in translation quality for long
sentences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1259</identifier>
 <datestamp>2014-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1259</id><created>2014-09-03</created><updated>2014-10-07</updated><authors><author><keyname>Cho</keyname><forenames>Kyunghyun</forenames></author><author><keyname>van Merrienboer</keyname><forenames>Bart</forenames></author><author><keyname>Bahdanau</keyname><forenames>Dzmitry</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author></authors><title>On the Properties of Neural Machine Translation: Encoder-Decoder
  Approaches</title><categories>cs.CL stat.ML</categories><comments>Eighth Workshop on Syntax, Semantics and Structure in Statistical
  Translation (SSST-8)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neural machine translation is a relatively new approach to statistical
machine translation based purely on neural networks. The neural machine
translation models often consist of an encoder and a decoder. The encoder
extracts a fixed-length representation from a variable-length input sentence,
and the decoder generates a correct translation from this representation. In
this paper, we focus on analyzing the properties of the neural machine
translation using two models; RNN Encoder--Decoder and a newly proposed gated
recursive convolutional neural network. We show that the neural machine
translation performs relatively well on short sentences without unknown words,
but its performance degrades rapidly as the length of the sentence and the
number of unknown words increase. Furthermore, we find that the proposed gated
recursive convolutional network learns a grammatical structure of a sentence
automatically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1272</identifier>
 <datestamp>2014-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1272</id><created>2014-09-03</created><authors><author><keyname>Shafie</keyname><forenames>Ahmed El</forenames></author><author><keyname>Ashour</keyname><forenames>Mahmoud</forenames></author><author><keyname>Mohamed</keyname><forenames>Amr</forenames></author><author><keyname>Khattab</keyname><forenames>Tamer</forenames></author></authors><title>Optimal Spectrum Access for a Rechargeable Cognitive Radio User Based on
  Energy Buffer State</title><categories>cs.IT cs.NI math.IT</categories><comments>arXiv admin note: text overlap with arXiv:1407.7267</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the maximum throughput for a rechargeable secondary
user (SU) sharing the spectrum with a primary user (PU) plugged to a reliable
power supply. The SU maintains a finite energy queue and harvests energy from
natural resources, e.g., solar, wind and acoustic noise. We propose a
probabilistic access strategy by the SU based on the number of packets at its
energy queue. We investigate the effect of the energy arrival rate, the amount
of energy per energy packet, and the capacity of the energy queue on the SU
throughput under fading channels. Results reveal that the proposed access
strategy can enhance the performance of the SU.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1282</identifier>
 <datestamp>2014-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1282</id><created>2014-09-03</created><updated>2014-10-12</updated><authors><author><keyname>Thompson</keyname><forenames>Andrew M.</forenames></author><author><keyname>Szymanski</keyname><forenames>Boleslaw K.</forenames></author><author><keyname>Lim</keyname><forenames>Chjan C.</forenames></author></authors><title>Propensity and stickiness in the naming game: Tipping fractions of
  minorities</title><categories>physics.soc-ph cs.SI</categories><comments>9 pages, 10 figures</comments><journal-ref>Phys. Rev. E 90(4):042809, Oct. 2014</journal-ref><doi>10.1103/PhysRevE.90.042809</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Agent-based models of the binary naming game are generalized here to
represent a family of models parameterized by the introduction of two
continuous parameters. These parameters define varying listener-speaker
interactions on the individual level with one parameter controlling the speaker
and the other controlling the listener of each interaction. The major finding
presented here is that the generalized naming game preserves the existence of
critical thresholds for the size of committed minorities. Above such threshold,
a committed minority causes a fast (in time logarithmic in size of the network)
convergence to consensus, even when there are other parameters influencing the
system. Below such threshold, reaching consensus requires time exponential in
the size of the network. Moreover, the two introduced parameters cause
bifurcations in the stabilities of the system's fixed points and may lead to
changes in the system's consensus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1284</identifier>
 <datestamp>2014-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1284</id><created>2014-09-03</created><authors><author><keyname>Alam</keyname><forenames>Sawood</forenames></author><author><keyname>Mehmood</keyname><forenames>Fateh ud din B</forenames></author><author><keyname>Nelson</keyname><forenames>Michael L.</forenames></author></authors><title>Improving Accessibility of Archived Raster Dictionaries of Complex
  Script Languages</title><categories>cs.DL cs.IR</categories><comments>11 pages, 5 images, 2 codes, 1 table</comments><acm-class>H.3.3</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We propose an approach to index raster images of dictionary pages which in
turn would require very little manual effort to enable direct access to the
appropriate pages of the dictionary for lookup. Accessibility is further
improved by feedback and crowdsourcing that enables highlighting of the
specific location on the page where the lookup word is found, annotation,
digitization, and fielded searching. This approach is equally applicable on
simple scripts as well as complex writing systems. Using our proposed approach,
we have built a Web application called &quot;Dictionary Explorer&quot; which supports
word indexes in various languages and every language can have multiple
dictionaries associated with it. Word lookup gives direct access to appropriate
pages of all the dictionaries of that language simultaneously. The application
has exploration features like searching, pagination, and navigating the word
index through a tree-like interface. The application also supports feedback,
annotation, and digitization features. Apart from the scanned images,
&quot;Dictionary Explorer&quot; aggregates results from various sources and user
contributions in Unicode. We have evaluated the time required for indexing
dictionaries of different sizes and complexities in the Urdu language and
examined various trade-offs in our implementation. Using our approach, a single
person can make a dictionary of 1,000 pages searchable in less than an hour.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1292</identifier>
 <datestamp>2014-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1292</id><created>2014-09-03</created><authors><author><keyname>Yang</keyname><forenames>Mohan</forenames></author><author><keyname>Ding</keyname><forenames>Bolin</forenames></author><author><keyname>Chaudhuri</keyname><forenames>Surajit</forenames></author><author><keyname>Chakrabarti</keyname><forenames>Kaushik</forenames></author></authors><title>Finding Patterns in a Knowledge Base using Keywords to Compose Table
  Answers</title><categories>cs.DB</categories><comments>VLDB 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We aim to provide table answers to keyword queries against knowledge bases.
For queries referring to multiple entities, like &quot;Washington cities population&quot;
and &quot;Mel Gibson movies&quot;, it is better to represent each relevant answer as a
table which aggregates a set of entities or entity-joins within the same table
scheme or pattern. In this paper, we study how to find highly relevant patterns
in a knowledge base for user-given keyword queries to compose table answers. A
knowledge base can be modeled as a directed graph called knowledge graph, where
nodes represent entities in the knowledge base and edges represent the
relationships among them. Each node/edge is labeled with type and text. A
pattern is an aggregation of subtrees which contain all keywords in the texts
and have the same structure and types on node/edges. We propose efficient
algorithms to find patterns that are relevant to the query for a class of
scoring functions. We show the hardness of the problem in theory, and propose
path-based indexes that are affordable in memory. Two query-processing
algorithms are proposed: one is fast in practice for small queries (with small
patterns as answers) by utilizing the indexes; and the other one is better in
theory, with running time linear in the sizes of indexes and answers, which can
handle large queries better. We also conduct extensive experimental study to
compare our approaches with a naive adaption of known techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1300</identifier>
 <datestamp>2014-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1300</id><created>2014-09-03</created><authors><author><keyname>Zhou</keyname><forenames>Yuzhe</forenames></author><author><keyname>Ai</keyname><forenames>Bo</forenames></author></authors><title>Quality of Service Improvement for High-Speed Railway Communications</title><categories>cs.IT cs.NI cs.PF math.IT</categories><comments>9 pages, 7 figures, China Communications, accepted for publication,
  August, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the fast development of high-speed railways, a call for fulfilling the
notion of communication at &quot;anytime, anywhere&quot; for high-speed train passengers
in the Train Operating Control System is on the way. In order to make a
realization of that, new railway wireless communication networks are needed.
The most promising one is the Long Term Evolution for Railway which will
provide broadband access, fast handover, and reliable communication for high
mobility users. However, with the increase of speed, the system is subjected to
high bit error rate, Doppler frequency shift and handover failure just like
other system does. This paper is trying to solve these problems by employing
MIMO technique. Specifically, the goal is to provide higher data rate, higher
reliability, less delay, and other relative quality of services for passengers.
MIMO performance analysis, resource allocation, and access control for handover
and various services in a two-hop model are proposed in this paper. Analytical
results and simulation results show that the proposed model and schemes perform
well in improving the system performances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1320</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1320</id><created>2014-09-04</created><updated>2014-09-05</updated><authors><author><keyname>Ping</keyname><forenames>Wei</forenames></author><author><keyname>Liu</keyname><forenames>Qiang</forenames></author><author><keyname>Ihler</keyname><forenames>Alexander</forenames></author></authors><title>Marginal Structured SVM with Hidden Variables</title><categories>stat.ML cs.LG</categories><comments>Accepted by the 31st International Conference on Machine Learning
  (ICML 2014). 12 pages version with supplement</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we propose the marginal structured SVM (MSSVM) for structured
prediction with hidden variables. MSSVM properly accounts for the uncertainty
of hidden variables, and can significantly outperform the previously proposed
latent structured SVM (LSSVM; Yu &amp; Joachims (2009)) and other state-of-art
methods, especially when that uncertainty is large. Our method also results in
a smoother objective function, making gradient-based optimization of MSSVMs
converge significantly faster than for LSSVMs. We also show that our method
consistently outperforms hidden conditional random fields (HCRFs; Quattoni et
al. (2007)) on both simulated and real-world datasets. Furthermore, we propose
a unified framework that includes both our and several other existing methods
as special cases, and provides insights into the comparison of different models
in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1323</identifier>
 <datestamp>2015-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1323</id><created>2014-09-04</created><updated>2015-03-13</updated><authors><author><keyname>Ziv</keyname><forenames>Jacob</forenames></author></authors><title>A Constrained-Dictionary version of LZ78 asymptotically achieves the
  Finite-State Compressibility for any Individual Sequence with a Distortion
  measure</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The unrestricted LZ78 universal data-compression algorithm (as well as the
LZ77 and LZW versions) achieves asymptotically, as the block-length tends to
infinity, the FS compressibility, namely the best compression-ratio that may be
achieved by any Information-lossless(IL) block-to-variable finite-state(FS)
algorithm, for any infinitely-long individual sequence. The encoder parses the
sequence into distinct phrases where each newly generated phrase is a past
phrase which is already stored in a dictionary, extended by one letter. The
newly generated phrase is then added to the updated, ever-growing dictionary.
One heuristic approach is the &quot;Least Recently Utilized&quot; (LRU) deletion
approach, where only the most recent D entries are kept in the dictionary, thus
yielding a constrained-dictionary version of LZ78 denoted by LZ78(LRU). In this
note, for the sake of completeness, it is demonstrated again via a simple proof
that the unrestricted LZ78 algorithm asymptotically achieves the
FS-Compressibility. Then, it is demonstrated that the LZ78(LRU)
information-lossless data-compression algorithm also achieves the FS
compressibility, as the dictionary size D tends to infinity. Although this is
perhaps not surprising, it does nevertheless yield a theoretical optimality
argument for the popular LZ78(LRU) algorithm (and similarly, for the LZW(LRU)
algorithm). In addition, the finite-state compressibility of an individual
sequence under a constrained allowable distance measure between the original
sequence and the decompressed sequence is defined. It is demonstrated that a
particular adaptive vector-quantizer that sequentially replaces clusters of
L-vectors onto a single, cluster-representative L-vector, followed by a
constrained D-entries-dictionary version of LZ78(LRU) as above, is
asymptotically optimal as D tends to infinity and L= log D .
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1348</identifier>
 <datestamp>2014-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1348</id><created>2014-09-04</created><authors><author><keyname>Dross</keyname><forenames>Fran&#xe7;ois</forenames></author><author><keyname>Montassier</keyname><forenames>Mickael</forenames></author><author><keyname>Pinlou</keyname><forenames>Alexandre</forenames></author></authors><title>Large induced forests in planar graphs with girth 4 or 5</title><categories>cs.DM math.CO</categories><comments>35 pages, 15 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give here some new lower bounds on the order of a largest induced forest
in planar graphs with girth $4$ and $5$. In particular we prove that a
triangle-free planar graph of order $n$ admits an induced forest of order at
least $\frac{6n+7}{11}$ , improving the lower bound of Salavatipour [M. R.
Salavatipour, Large induced forests in triangle-free planar graphs, Graphs and
Combinatorics, 22:113-126, 2006]. We also prove that a planar graph of order
$n$ and girth at least $5$ admits an induced forest of order at least
$\frac{44n+50}{69}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1354</identifier>
 <datestamp>2014-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1354</id><created>2014-09-04</created><updated>2014-09-10</updated><authors><author><keyname>Li</keyname><forenames>Ming-Hua</forenames></author><author><keyname>Wang</keyname><forenames>Ping</forenames></author><author><keyname>Chang</keyname><forenames>Zhe</forenames></author><author><keyname>Zhao</keyname><forenames>Dong</forenames></author></authors><title>CosmoMC Installation and Running Guidelines</title><categories>astro-ph.IM astro-ph.CO cs.MS hep-th</categories><comments>10 pages, 0 figures. Publicly distributed and available</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  CosmoMC is a Fortran 95 Markov-Chain Monte-Carlo (MCMC) engine to explore the
cosmological parameter space, plus a Python suite for plotting and presenting
results (see http://cosmologist.info/cosmomc/). This document describes the
installation of the CosmoMC on a Linux system (Ubuntu 14.04.1 LTS 64-bit
version). It is written for those who want to use it in their scientific
research but without much training on Linux and the program. Besides a
step-by-step installation guide, we also give a brief introduction of how to
run the program on both a desktop and a cluster. We share our way to generate
the plots that are commonly used in the references of cosmology. For more
information, one can refer to the CosmoCoffee forum
(http://cosmocoffee.info/viewforum.php?f=11) or contact the authors of this
document. Questions and comments would be much appreciated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1357</identifier>
 <datestamp>2014-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1357</id><created>2014-09-04</created><authors><author><keyname>Kern</keyname><forenames>Roman</forenames></author><author><keyname>Jack</keyname><forenames>Kris</forenames></author><author><keyname>Granitzer</keyname><forenames>Michael</forenames></author></authors><title>Recommending Scientific Literature: Comparing Use-Cases and Algorithms</title><categories>cs.IR</categories><comments>12 pages, 2 figures, 5 tables</comments><acm-class>H.3.3; H.3.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An important aspect of a researcher's activities is to find relevant and
related publications. The task of a recommender system for scientific
publications is to provide a list of papers that match these criteria. Based on
the collection of publications managed by Mendeley, four data sets have been
assembled that reflect different aspects of relatedness. Each of these
relatedness scenarios reflect a user's search strategy. These scenarios are
public groups, venues, author publications and user libraries. The first three
of these data sets are being made publicly available for other researchers to
compare algorithms against. Three recommender systems have been implemented: a
collaborative filtering system; a content-based filtering system; and a hybrid
of these two systems. Results from testing demonstrate that collaborative
filtering slightly outperforms the content-based approach, but fails in some
scenarios. The hybrid system, that combines the two recommendation methods,
provides the best performance, achieving a precision of up to 70%. This
suggests that both techniques contribute complementary information in the
context of recommending scientific literature and different approaches suite
for different information needs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1365</identifier>
 <datestamp>2014-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1365</id><created>2014-09-04</created><authors><author><keyname>Korpi</keyname><forenames>Dani</forenames></author><author><keyname>Anttila</keyname><forenames>Lauri</forenames></author><author><keyname>Valkama</keyname><forenames>Mikko</forenames></author></authors><title>Feasibility of In-band Full-Duplex Radio Transceivers with Imperfect RF
  Components: Analysis and Enhanced Cancellation Algorithms</title><categories>cs.IT math.IT</categories><comments>7 pages, presented in the CROWNCOM 2014 conference</comments><journal-ref>Proceedings of the 9th International Conference on Cognitive Radio
  Oriented Wireless Networks and Communications (CROWNCOM), pp. 532-538, June
  2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we provide an overview regarding the feasibility of in-band
full-duplex transceivers under imperfect RF components. We utilize results and
findings from the recent research on full-duplex communications, while
introducing also transmitter-induced thermal noise into the analysis. This
means that the model of the RF impairments used in this paper is the most
comprehensive thus far. By assuming realistic parameter values for the
different transceiver components, it is shown that IQ imaging and
transmitter-induced nonlinearities are the most significant sources of
distortion in in-band full-duplex transceivers, in addition to linear
self-interference. Motivated by this, we propose a novel augmented nonlinear
digital self-interference canceller that is able to model and hence suppress
all the essential transmitter imperfections jointly. This is also verified and
demonstrated by extensive waveform simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1372</identifier>
 <datestamp>2014-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1372</id><created>2014-09-04</created><authors><author><keyname>Korpi</keyname><forenames>Dani</forenames></author><author><keyname>Anttila</keyname><forenames>Lauri</forenames></author><author><keyname>Valkama</keyname><forenames>Mikko</forenames></author></authors><title>Impact of Received Signal on Self-interference Channel Estimation and
  Achievable Rates in In-band Full-duplex Transceivers</title><categories>cs.IT math.IT</categories><comments>8 pages, to be presented in the Asilomar Conference on Signals,
  Systems, and Computers in November 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we analyze the effect of the calibration period, or lack of, on
self-interference channel estimation in the digital domain of in-band
full-duplex radio transceivers. In particular, we consider a scenario where the
channel estimation must be performed without a separate calibration period,
which means that the received signal of interest will act as an additional
noise source from the estimation perspective. We will explicitly analyze its
effect, and quantify the increase in the parameter estimation variance, or
sample size, if similar accuracy for the channel estimate is to be achieved as
with a separate calibration period. In addition, we will analyze how the
calibration period, or its absence, affects the overall achievable rates. Full
waveform simulations are then used to determine the validity of the obtained
results, as well as provide numerical results regarding the achievable rates.
It is shown that, even though a substantial increase in the parameter sample
size is required if there is no calibration period, the achievable rates are
still comparable for the two scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1397</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1397</id><created>2014-09-04</created><authors><author><keyname>Kaufman</keyname><forenames>Tali</forenames></author><author><keyname>Kazhdan</keyname><forenames>David</forenames></author><author><keyname>Lubotzky</keyname><forenames>Alexander</forenames></author></authors><title>Isoperimetric Inequalities for Ramanujan Complexes and Topological
  Expanders</title><categories>math.CO cs.CC math.GR math.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Expander graphs have been intensively studied in the last four decades. In
recent years a high dimensional theory of expanders has emerged, and several
variants have been studied. Among them stand out coboundary expansion and
topological expansion. It is known that for every $d$ there are unbounded
degree simplicial complexes of dimension $d$ with these properties. However, a
major open problem, formulated by Gromov, is whether bounded degree high
dimensional expanders exist for $d \geq 2$.
  We present an explicit construction of bounded degree complexes of dimension
$d=2$ which are topological expanders, thus answering Gromov's question in the
affirmative. Conditional on a conjecture of Serre on the congruence subgroup
property, infinite sub-family of these give also a family of bounded degree
coboundary expanders.
  The main technical tools are new isoperimetric inequalities for Ramanujan
Complexes. We prove linear size bounds on $F_2$ systolic invariants of these
complexes, which seem to be the first linear $F_2$ systolic bounds. The
expansion results are deduced from these isoperimetric inequalities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1399</identifier>
 <datestamp>2015-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1399</id><created>2014-09-04</created><updated>2015-11-23</updated><authors><author><keyname>Ward</keyname><forenames>Justin</forenames></author><author><keyname>Zivny</keyname><forenames>Stanislav</forenames></author></authors><title>Maximizing k-Submodular Functions and Beyond</title><categories>cs.DS cs.DM</categories><comments>Full version of a SODA'14 paper, to appear in ACM Transactions on
  Algorithms (TALG)</comments><acm-class>F.2.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the maximization problem in the value oracle model of functions
defined on $k$-tuples of sets that are submodular in every orthant and $r$-wise
monotone, where $k\geq 2$ and $1\leq r\leq k$. We give an analysis of a
deterministic greedy algorithm that shows that any such function can be
approximated to a factor of $1/(1+r)$. For $r=k$, we give an analysis of a
randomised greedy algorithm that shows that any such function can be
approximated to a factor of $1/(1+\sqrt{k/2})$.
  In the case of $k=r=2$, the considered functions correspond precisely to
bisubmodular functions, in which case we obtain an approximation guarantee of
$1/2$. We show that, as in the case of submodular functions, this result is the
best possible in both the value query model, and under the assumption that
$NP\neq RP$.
  Extending a result of Ando et al., we show that for any $k\geq 3$
submodularity in every orthant and pairwise monotonicity (i.e. $r=2$) precisely
characterize $k$-submodular functions. Consequently, we obtain an approximation
guarantee of $1/3$ (and thus independent of $k$) for the maximization problem
of $k$-submodular functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1411</identifier>
 <datestamp>2014-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1411</id><created>2014-09-02</created><authors><author><keyname>Hassanat</keyname><forenames>Ahmad B. A.</forenames></author></authors><title>Visual Speech Recognition</title><categories>cs.CV</categories><comments>Speech and Language Technologies (Book), Prof. Ivo Ipsic (Ed.), ISBN:
  978-953-307-322-4, InTech (2011)</comments><doi>10.5772/19361</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lip reading is used to understand or interpret speech without hearing it, a
technique especially mastered by people with hearing difficulties. The ability
to lip read enables a person with a hearing impairment to communicate with
others and to engage in social activities, which otherwise would be difficult.
Recent advances in the fields of computer vision, pattern recognition, and
signal processing has led to a growing interest in automating this challenging
task of lip reading. Indeed, automating the human ability to lip read, a
process referred to as visual speech recognition (VSR) (or sometimes speech
reading), could open the door for other novel related applications. VSR has
received a great deal of attention in the last decade for its potential use in
applications such as human-computer interaction (HCI), audio-visual speech
recognition (AVSR), speaker recognition, talking heads, sign language
recognition and video surveillance. Its main aim is to recognise spoken word(s)
by using only the visual signal that is produced during speech. Hence, VSR
deals with the visual domain of speech and involves image processing,
artificial intelligence, object detection, pattern recognition, statistical
modelling, etc.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1412</identifier>
 <datestamp>2014-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1412</id><created>2014-09-02</created><authors><author><keyname>Kumar</keyname><forenames>Surender</forenames></author><author><keyname>Prateek</keyname><forenames>Manish</forenames></author><author><keyname>Ahuja</keyname><forenames>N. J.</forenames></author><author><keyname>Bhushan</keyname><forenames>Bharat</forenames></author></authors><title>MEEP: Multihop Energy Efficient Protocol For Heterogeneous Wireless
  Sensor Network</title><categories>cs.NI</categories><comments>9 Pages, 10 Figures, http://www.ijcst.org/Volume5/Issue3/2014. arXiv
  admin note: substantial text overlap with arXiv:1408.3110</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Energy conservation of sensor nodes for increasing the network life is the
most crucial design goal while developing efficient routing protocol for
wireless sensor networks. Recent technological advances help in the development
of wide variety of sensor nodes. Heterogeneity takes the advantage of different
types of sensor nodes and improves the energy efficiency and network life.
Generally sensors are deployed randomly and densely in a sensing region so
short distance multihop communication reduces the long distance transmission in
the sensor network. In this research paper MEEP (multihop energy efficient
protocol for heterogeneous sensor network) is proposed. The proposed protocol
combines the idea of clustering and multihop communication. Heterogeneity is
created in the network by using some nodes of high energy. Low energy nodes use
a residual energy based scheme to become a cluster head. High energy nodes act
as the relay nodes for low energy cluster head when they are not performing the
duty of a cluster head to save their energy further. Protocol also suggests a
sleep state for nodes in the cluster formation process for saving energy and
increasing the life of sensor network. Simulation result shows that the
proposed scheme is better than other two level heterogeneous sensor network
protocol like SEP in energy efficiency and network life.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1415</identifier>
 <datestamp>2014-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1415</id><created>2014-09-04</created><authors><author><keyname>Daniel</keyname><forenames>Uchenna Peter</forenames></author><author><keyname>Agbanusi</keyname><forenames>Nneka Chikazo</forenames></author><author><keyname>Danjuma</keyname><forenames>Kwetishe Joro</forenames></author></authors><title>A Survey of Bandwidth Optimization Techniques and Patterns in VoIP
  Services and Applications</title><categories>cs.NI</categories><comments>8 pages, 7 figures. ISSN (Print): 1694-0814 | ISSN (Online):
  1694-0784</comments><journal-ref>IJCSI, International Journal of Computer Science Issues, Vol. 11,
  Issue 2, No 2: pp.211 -218, March 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article surveys the various techniques adopted for optimising bandwidth
for VoIP services over the period 1999-2014. The improvement of bandwidth can
be realized through; silence suppression measure of repressing the silent
portions (packets) in a voice conversation using Voice Activity Detection
algorithm; by so doing, the transmission rate during the inactive periods of
speech is reduced, and thus, the mean transmission rate can be reduced. A
second measure is packet header reduction which defines a process of
multiplexing and de-multiplexing packet headers to curb excesses. Voice/ Packet
Header compression is considered the most productive of all the techniques,
offering a scheme where VoIP packets are compressed from the 40 bytes of size
to a smaller byte size of 2 bytes. When combined with aggregation, compression
potentially yields a compressed size of up to 1 byte. In either case, bandwidth
save is reached using compression and decompression codecs of varying data and
bit rates. It is envisaged that an improvement in the performance of codecs
would yield a better result in terms of enhancing results favourably in Voice
over broadband networks
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1416</identifier>
 <datestamp>2015-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1416</id><created>2014-09-04</created><updated>2015-01-20</updated><authors><author><keyname>Li</keyname><forenames>Hong-Wei</forenames></author><author><keyname>Yang</keyname><forenames>Li</forenames></author></authors><title>A quantum algorithm for approximating the influences of Boolean
  functions and its applications</title><categories>cs.DS quant-ph</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the influences of variables on a Boolean function $f$ based on
the quantum Bernstein-Vazirani algorithm. A previous paper (Floess et al. in
Math. Struct. in Comp. Science 23: 386, 2013) has proved that if a $n$-variable
Boolean function $f(x_1,\ldots,x_n)$ does not depend on an input variable
$x_i$, using the Bernstein-Vazirani circuit to $f$ will always obtain an output
$y$ that has a $0$ in the $i$th position. We generalize this result and show
that after one time running the algorithm, the probability of getting a 1 in
each position $i$ is equal to the dependence degree of $f$ on the variable
$x_i$, i.e. the influence of $x_i$ on $f$. On this foundation, we give an
approximation algorithm to evaluate the influence of any variable on a Boolean
function. Next, as an application, we use it to study the Boolean functions
with juntas, and construct probabilistic quantum algorithms to learn certain
Boolean functions. Compared with the deterministic algorithms given by Floess
et al., our probabilistic algorithms are faster.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1427</identifier>
 <datestamp>2014-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1427</id><created>2014-09-04</created><updated>2014-10-17</updated><authors><author><keyname>Soussi</keyname><forenames>Mohieddine El</forenames></author><author><keyname>Zaidi</keyname><forenames>Abdellatif</forenames></author><author><keyname>Vandendorpe</keyname><forenames>Luc</forenames></author></authors><title>Compute-and-Forward on a Multi-User Multi-Relay Channel</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider a system in which multiple users communicate with
a destination with the help of multiple half-duplex relays. Based on the
compute-and-forward scheme, each relay, instead of decoding the users'
messages, decodes an integer-valued linear combination that relates the
transmitted messages. Then, it forwards the linear combination towards the
destination. Given these linear combinations, the destination may or may not
recover the transmitted messages since the linear combinations are not always
full rank. Therefore, we propose an algorithm where we optimize the precoding
factor at the users such that the probability that the equations are full rank
is increased and that the transmission rate is maximized. We show, through some
numerical examples, the effectiveness of our algorithm and the advantage of
performing precoding allocation at the users. Also, we show that this scheme
can outperform standard relaying techniques in certain regimes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1434</identifier>
 <datestamp>2014-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1434</id><created>2014-09-04</created><updated>2014-10-30</updated><authors><author><keyname>Willms</keyname><forenames>J&#xfc;rgen</forenames></author></authors><title>Run Vector Analysis and Barker Sequences of Odd Length</title><categories>cs.IT math.CO math.IT</categories><comments>11 pages, no figures; v2: typos corrected and some minor changes</comments><msc-class>11B83, 94A55, 68R15, 68P30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The run vector of a binary sequence reflects the run structure of the
sequence, which is given by the set of all substrings of the run length
encoding. The run vector and the aperiodic autocorrelations of a binary
sequence are strongly related. In this paper, we analyze the run vector of
skew-symmetric binary sequences. Using the derived results we present a new and
different proof that there exists no Barker sequence of odd length n &gt; 13.
Barker sequences are binary sequences whose off-peak aperiodic autocorrelations
are all in magnitude at most 1.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1436</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1436</id><created>2014-09-04</created><updated>2015-03-17</updated><authors><author><keyname>Jo</keyname><forenames>Hang-Hyun</forenames></author><author><keyname>Moon</keyname><forenames>Eunyoung</forenames></author></authors><title>Coevolution of a network and perception</title><categories>physics.soc-ph cs.SI</categories><comments>32 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How does an individual's cognition change a system which is a collective
behavior of individuals? Or, how does a system affect an individual's
cognition? To examine the interplay between a system and individuals, we study
a cognition-based network formation. When a network is not fully observable,
individuals' perception of a network plays an important role in decision
making. Assuming that a communication link is costly, and more accurate
perception yields higher network utility, an agent decides whether to form a
link in order to get better information or not. Changes in a network with newly
added links affect individuals' perception accuracy, which may cause further
changes in a network. We characterize the early stage of network dynamics and
information dispersion. Network structures in a steady state are also examined.
Additionally, we discuss local interactions and a link concentration in a
frequently changing network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1437</identifier>
 <datestamp>2014-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1437</id><created>2014-09-04</created><authors><author><keyname>Stein</keyname><forenames>Manuel</forenames></author><author><keyname>Theiler</keyname><forenames>Sebastian</forenames></author><author><keyname>Nossek</keyname><forenames>Josef A.</forenames></author></authors><title>Overdemodulation for High-Performance Receivers with Low-Resolution ADC</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The design of the analog demodulator for receivers with low-resolution
analog-to-digital converters (ADC) is investigated. For infinite ADC
resolution, demodulation to baseband with M = 2 orthogonal sinusoidal functions
(quadrature demodulation) is an optimum design choice. For receive systems
which are restricted to ADC with low amplitude resolution we show here that
this classical demodulation approach is suboptimal. To this end we analyze the
theoretical channel parameter estimation performance based on a simple
pessimistic characterization of the Fisher information measure when forming M &gt;
2 analog demodulation channels prior to an ADC with 1-bit amplitude resolution.
In order to emphasize that this inside is also true for communication problems,
we provide an additional discussion on the behavior of the Shannon information
measure under overdemodulation and 1-bit quantization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1444</identifier>
 <datestamp>2014-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1444</id><created>2014-09-04</created><updated>2014-09-11</updated><authors><author><keyname>Bastug</keyname><forenames>Mert</forenames></author><author><keyname>Petreczky</keyname><forenames>Mihaly</forenames></author><author><keyname>Wisniewski</keyname><forenames>Rafael</forenames></author><author><keyname>Leth</keyname><forenames>John</forenames></author></authors><title>Model Reduction of Linear Switched Systems by Restricting Discrete
  Dynamics</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a procedure for reducing the number of continuous states of
discrete-time linear switched systems, such that the reduced system has the
same behavior as the original system for a subset of switching sequences. The
proposed method is expected to be useful for abstraction based control
synthesis methods for hybrid systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1451</identifier>
 <datestamp>2014-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1451</id><created>2014-09-04</created><authors><author><keyname>Peters</keyname><forenames>Gareth W.</forenames></author><author><keyname>Chapelle</keyname><forenames>Ariane</forenames></author><author><keyname>Panayi</keyname><forenames>Efstathios</forenames></author></authors><title>Opening discussion on banking sector risk exposures and vulnerabilities
  from virtual currencies: An operational risk perspective</title><categories>q-fin.RM cs.CR q-fin.EC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop the first basic Operational Risk perspective on key risk
management issues associated with the development of new forms of electronic
currency in the real economy. In particular, we focus on understanding the
development of new risks types and the evolution of current risk types as new
components of financial institutions arise to cater for an increasing demand
for electronic money, micro-payment systems, Virtual money and cryptographic
(Crypto) currencies. In particular, this paper proposes a framework of risk
identification and assessment applied to Virtual and Crypto currencies from a
banking regulation perspective. In doing so, it addresses the topical issues of
understanding important key Operational Risk vulnerabilities and exposure risk
drivers under the framework of the Basel II/III banking regulation,
specifically associated with Virtual and Crypto currencies. This is critical to
consider should such alternative currencies continue to grow in utilisation to
the point that they enter into the banking sector, through commercial banks and
financial institutions who are beginning to contemplate their recognition in
terms of deposits, transactions and exchangeability for fiat currencies.
  We highlight how some of the features of Virtual and Crypto currencies are
important drivers of Operational Risk, posing both management and regulatory
challenges that must start to be considered and addressed both by regulators,
central banks and security exchanges. In this paper we focus purely on the
Operational Risk perspective of banks operating in an environment where such
electronic Virtual currencies are available. Some aspects of this discussion
are directly relevant now, whilst others can be understood as discussions to
raise awareness of issues in Operational Risk that will arise as Virtual
currency start to interact more widely in the real economy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1455</identifier>
 <datestamp>2014-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1455</id><created>2014-09-04</created><authors><author><keyname>Raman</keyname><forenames>Vasumathi</forenames></author><author><keyname>Kress-Gazit</keyname><forenames>Hadas</forenames></author></authors><title>Unsynthesizable Cores - Minimal Explanations for Unsynthesizable
  High-Level Robot Behaviors</title><categories>cs.RO cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the increasing ubiquity of multi-capable, general-purpose robots arises
the need for enabling non-expert users to command these robots to perform
complex high-level tasks. To this end, high-level robot control has seen the
application of formal methods to automatically synthesize
correct-by-construction controllers from user-defined specifications; synthesis
fails if and only if there exists no controller that achieves the specified
behavior. Recent work has also addressed the challenge of providing
easy-to-understand feedback to users when a specification fails to yield a
corresponding controller. Existing techniques provide feedback on portions of
the specification that cause the failure, but do so at a coarse granularity.
This work presents techniques for refining this feedback, extracting minimal
explanations of unsynthesizability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1456</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1456</id><created>2014-09-04</created><updated>2014-09-07</updated><authors><author><keyname>Ravanbakhsh</keyname><forenames>Siamak</forenames></author><author><keyname>Liu</keyname><forenames>Philip</forenames></author><author><keyname>Bjorndahl</keyname><forenames>Trent</forenames></author><author><keyname>Mandal</keyname><forenames>Rupasri</forenames></author><author><keyname>Grant</keyname><forenames>Jason R.</forenames></author><author><keyname>Wilson</keyname><forenames>Michael</forenames></author><author><keyname>Eisner</keyname><forenames>Roman</forenames></author><author><keyname>Sinelnikov</keyname><forenames>Igor</forenames></author><author><keyname>Hu</keyname><forenames>Xiaoyu</forenames></author><author><keyname>Luchinat</keyname><forenames>Claudio</forenames></author><author><keyname>Greiner</keyname><forenames>Russell</forenames></author><author><keyname>Wishart</keyname><forenames>David S.</forenames></author></authors><title>Accurate, fully-automated NMR spectral profiling for metabolomics</title><categories>cs.AI cs.CE q-bio.QM</categories><journal-ref>PLoS ONE 10(5): e0124219, 2015</journal-ref><doi>10.1371/journal.pone.0132873</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many diseases cause significant changes to the concentrations of small
molecules (aka metabolites) that appear in a person's biofluids, which means
such diseases can often be readily detected from a person's &quot;metabolic
profile&quot;. This information can be extracted from a biofluid's NMR spectrum.
Today, this is often done manually by trained human experts, which means this
process is relatively slow, expensive and error-prone. This paper presents a
tool, Bayesil, that can quickly, accurately and autonomously produce a complex
biofluid's (e.g., serum or CSF) metabolic profile from a 1D1H NMR spectrum.
This requires first performing several spectral processing steps then matching
the resulting spectrum against a reference compound library, which contains the
&quot;signatures&quot; of each relevant metabolite. Many of these steps are novel
algorithms and our matching step views spectral matching as an inference
problem within a probabilistic graphical model that rapidly approximates the
most probable metabolic profile. Our extensive studies on a diverse set of
complex mixtures, show that Bayesil can autonomously find the concentration of
all NMR-detectable metabolites accurately (~90% correct identification and ~10%
quantification error), in &lt;5minutes on a single CPU. These results demonstrate
that Bayesil is the first fully-automatic publicly-accessible system that
provides quantitative NMR spectral profiling effectively -- with an accuracy
that meets or exceeds the performance of trained experts. We anticipate this
tool will usher in high-throughput metabolomics and enable a wealth of new
applications of NMR in clinical settings. Available at http://www.bayesil.ca.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1458</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1458</id><created>2014-09-04</created><updated>2014-09-29</updated><authors><author><keyname>Jaggi</keyname><forenames>Martin</forenames></author><author><keyname>Smith</keyname><forenames>Virginia</forenames></author><author><keyname>Tak&#xe1;&#x10d;</keyname><forenames>Martin</forenames></author><author><keyname>Terhorst</keyname><forenames>Jonathan</forenames></author><author><keyname>Krishnan</keyname><forenames>Sanjay</forenames></author><author><keyname>Hofmann</keyname><forenames>Thomas</forenames></author><author><keyname>Jordan</keyname><forenames>Michael I.</forenames></author></authors><title>Communication-Efficient Distributed Dual Coordinate Ascent</title><categories>cs.LG math.OC stat.ML</categories><comments>NIPS 2014 version, including proofs. Published in Advances in Neural
  Information Processing Systems 27 (NIPS 2014)</comments><msc-class>90C25, 68W15</msc-class><acm-class>G.1.6; C.1.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Communication remains the most significant bottleneck in the performance of
distributed optimization algorithms for large-scale machine learning. In this
paper, we propose a communication-efficient framework, CoCoA, that uses local
computation in a primal-dual setting to dramatically reduce the amount of
necessary communication. We provide a strong convergence rate analysis for this
class of algorithms, as well as experiments on real-world distributed datasets
with implementations in Spark. In our experiments, we find that as compared to
state-of-the-art mini-batch versions of SGD and SDCA algorithms, CoCoA
converges to the same .001-accurate solution quality on average 25x as quickly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1461</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1461</id><created>2014-09-04</created><updated>2015-02-01</updated><authors><author><keyname>Flatow</keyname><forenames>David</forenames></author><author><keyname>Naaman</keyname><forenames>Mor</forenames></author><author><keyname>Xie</keyname><forenames>Ke Eddie</forenames></author><author><keyname>Volkovich</keyname><forenames>Yana</forenames></author><author><keyname>Kanza</keyname><forenames>Yaron</forenames></author></authors><title>On the Accuracy of Hyper-local Geotagging of Social Media Content</title><categories>cs.IR cs.SI</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social media users share billions of items per year, only a small fraction of
which is geotagged. We present a data- driven approach for identifying
non-geotagged content items that can be associated with a hyper-local
geographic area by modeling the location distributions of hyper-local n-grams
that appear in the text. We explore the trade-off between accuracy, precision
and coverage of this method. Further, we explore differences across content
received from multiple platforms and devices, and show, for example, that
content shared via different sources and applications produces significantly
different geographic distributions, and that it is best to model and predict
location for items according to their source. Our findings show the potential
and the bounds of a data-driven approach to geotag short social media texts,
and offer implications for all applications that use data-driven approaches to
locate content.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1465</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1465</id><created>2014-09-04</created><authors><author><keyname>Gleich</keyname><forenames>David F.</forenames></author><author><keyname>Lim</keyname><forenames>Lek-Heng</forenames></author><author><keyname>Yu</keyname><forenames>Yongyang</forenames></author></authors><title>Multilinear PageRank</title><categories>cs.NA math.NA</categories><doi>10.1137/140985160</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we first extend the celebrated PageRank modification to a
higher-order Markov chain. Although this system has attractive theoretical
properties, it is computationally intractable for many interesting problems. We
next study a computationally tractable approximation to the higher-order
PageRank vector that involves a system of polynomial equations called
multilinear PageRank, which is a type of tensor PageRank vector. It is
motivated by a novel &quot;spacey random surfer&quot; model, where the surfer remembers
bits and pieces of history and is influenced by this information. The
underlying stochastic process is an instance of a vertex-reinforced random
walk. We develop convergence theory for a simple fixed-point method, a shifted
fixed-point method, and a Newton iteration in a particular parameter regime. In
marked contrast to the case of the PageRank vector of a Markov chain where the
solution is always unique and easy to compute, there are parameter regimes of
multilinear PageRank where solutions are not unique and simple algorithms do
not converge. We provide a repository of these non-convergent cases that we
encountered through exhaustive enumeration and randomly sampling that we
believe is useful for future study of the problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1466</identifier>
 <datestamp>2014-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1466</id><created>2014-09-04</created><authors><author><keyname>Levit</keyname><forenames>Vadim E.</forenames></author><author><keyname>Tankus</keyname><forenames>David</forenames></author></authors><title>Well-dominated graphs without cycles of lengths 4 and 5</title><categories>cs.DM math.CO</categories><comments>10 pages, 2 figures</comments><msc-class>05C69 (Primary), 05C85 (Secondary)</msc-class><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $G$ be a graph. A set $S$ of vertices in $G$ dominates the graph if every
vertex of $G$ is either in $S$ or a neighbor of a vertex in $S$. Finding a
minimal cardinality set which dominates the graph is an NP-complete problem.
The graph $G$ is well-dominated if all its minimal dominating sets are of the
same cardinality. The complexity status of recognizing well-dominated graphs is
not known. We show that recognizing well-dominated graphs can be done
polynomially for graphs without cycles of lengths $4$ and $5$, by proving that
a graph belonging to this family is well-dominated if and only if it is
well-covered.
  Assume that a weight function $w$ is defined on the vertices of $G$. Then $G$
is $w$-well-dominated} if all its minimal dominating sets are of the same
weight. We prove that the set of weight functions $w$ such that $G$ is
$w$-well-dominated is a vector space, and denote that vector space by $WWD(G)$.
We prove that $WWD(G)$ is a subspace of $WCW(G)$, the vector space of weight
functions $w$ such that $G$ is $w$-well-covered. We provide a polynomial
characterization of $WWD(G)$ for the case that $G$ does not contain cycles of
lengths $4$, $5$, and $6$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1467</identifier>
 <datestamp>2014-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1467</id><created>2014-09-04</created><authors><author><keyname>Leitinger</keyname><forenames>Erik</forenames></author><author><keyname>Meissner</keyname><forenames>Paul</forenames></author><author><keyname>R&#xfc;disser</keyname><forenames>Christoph</forenames></author><author><keyname>Dumphart</keyname><forenames>Gregor</forenames></author><author><keyname>Witrisal</keyname><forenames>Klaus</forenames></author></authors><title>Evaluation of Position-related Information in Multipath Components for
  Indoor Positioning</title><categories>cs.IT math.IT</categories><comments>14 pages, 10 figures, submitted to the IEEE Journal on Selected Areas
  in Communications: Localization-Awareness for Radios and Networks</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Location awareness is a key factor for a wealth of wireless indoor
applications. Its provision requires the careful fusion of diverse information
sources. For agents that use radio signals for localization, this information
may either come from signal transmissions with respect to fixed anchors, from
cooperative transmissions in between agents, or from radar-like monostatic
transmissions. Using a-priori knowledge of a floor plan of the environment,
specular multipath components can be exploited, based on a geometric-stochastic
channel model. In this paper, a unified framework is presented for the
quantification of this type of position-related information, using the concept
of equivalent Fisher information. We derive analytical results for the
Cram\'er-Rao lower bound of multipath-assisted positioning, considering
bistatic transmissions between agents and fixed anchors, monostatic
transmissions from agents, cooperative measurements in-between agents, and
combinations thereof, including the effect of clock offsets and missing
synchronization. Awareness of this information enables highly accurate and
robust indoor positioning. Computational results show the applicability of the
framework for the characterization of the localization capabilities of some
environment, quantifying the influence of different system setups, signal
parameters, and the impact of path overlap.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1484</identifier>
 <datestamp>2015-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1484</id><created>2014-09-04</created><updated>2015-04-03</updated><authors><author><keyname>Betancourt</keyname><forenames>Alejandro</forenames></author><author><keyname>Morerio</keyname><forenames>Pietro</forenames></author><author><keyname>Regazzoni</keyname><forenames>Carlo S.</forenames></author><author><keyname>Rauterberg</keyname><forenames>Matthias</forenames></author></authors><title>The Evolution of First Person Vision Methods: A Survey</title><categories>cs.CV</categories><comments>First Person Vision, Egocentric Vision, Wearable Devices, Smart
  Glasses, Computer Vision, Video Analytics, Human-machine Interaction</comments><journal-ref>Betancourt, A., Morerio, P., Regazzoni, C. S., &amp; Rauterberg, M.
  (2015). The Evolution of First Person Vision Methods: A Survey. IEEE
  Transactions on Circuits and Systems for Video Technology,
  doi:10.1109/TCSVT.2015.2409731</journal-ref><doi>10.1109/TCSVT.2015.2409731</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The emergence of new wearable technologies such as action cameras and
smart-glasses has increased the interest of computer vision scientists in the
First Person perspective. Nowadays, this field is attracting attention and
investments of companies aiming to develop commercial devices with First Person
Vision recording capabilities. Due to this interest, an increasing demand of
methods to process these videos, possibly in real-time, is expected. Current
approaches present a particular combinations of different image features and
quantitative methods to accomplish specific objectives like object detection,
activity recognition, user machine interaction and so on. This paper summarizes
the evolution of the state of the art in First Person Vision video analysis
between 1997 and 2014, highlighting, among others, most commonly used features,
methods, challenges and opportunities within the field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1487</identifier>
 <datestamp>2016-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1487</id><created>2014-09-04</created><updated>2016-01-06</updated><authors><author><keyname>Gradwohl</keyname><forenames>Ronen</forenames></author><author><keyname>Smorodinsky</keyname><forenames>Rann</forenames></author></authors><title>Perception Games and Privacy</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Players (people, firms, states, etc.) have privacy concerns that may affect
their choice of actions in strategic settings. We use a variant of signaling
games to model this effect and study its relation to pooling behavior,
misrepresentation of information, and inefficiency. We discuss these issues and
show that common intuitions may lead to inaccurate conclusions about the
implications of privacy concerns.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1496</identifier>
 <datestamp>2014-10-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1496</id><created>2014-09-04</created><authors><author><keyname>Ciampaglia</keyname><forenames>Giovanni Luca</forenames></author><author><keyname>Taraborelli</keyname><forenames>Dario</forenames></author></authors><title>MoodBar: Increasing new user retention in Wikipedia through lightweight
  socialization</title><categories>cs.SI cs.HC physics.soc-ph</categories><comments>9 pages, 5 figures, accepted for presentation at CSCW'15</comments><doi>10.1145/2675133.2675181</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Socialization in online communities allows existing members to welcome and
recruit newcomers, introduce them to community norms and practices, and sustain
their early participation. However, socializing newcomers does not come for
free: in large communities, socialization can result in a significant workload
for mentors and is hard to scale. In this study we present results from an
experiment that measured the effect of a lightweight socialization tool on the
activity and retention of newly registered users attempting to edit for the
first time Wikipedia. Wikipedia is struggling with the retention of newcomers
and our results indicate that a mechanism to elicit lightweight feedback and to
provide early mentoring to newcomers improves their chances of becoming
long-term contributors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1510</identifier>
 <datestamp>2014-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1510</id><created>2014-09-04</created><authors><author><keyname>Kaczmarek</keyname><forenames>O.</forenames></author><author><keyname>Schmidt</keyname><forenames>C.</forenames></author><author><keyname>Steinbrecher</keyname><forenames>P.</forenames></author><author><keyname>Mukherjee</keyname><forenames>Swagato</forenames></author><author><keyname>Wagner</keyname><forenames>M.</forenames></author></authors><title>HISQ inverter on Intel Xeon Phi and NVIDIA GPUs</title><categories>cs.DC hep-lat</categories><comments>7 pages, proceedings, presented at the 32nd International Symposium
  on Lattice Field Theory (Lattice 2014), June 23 to June 28, 2014, New York,
  USA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The runtime of a Lattice QCD simulation is dominated by a small kernel, which
calculates the product of a vector by a sparse matrix known as the &quot;Dslash&quot;
operator. Therefore, this kernel is frequently optimized for various HPC
architectures. In this contribution we compare the performance of the Intel
Xeon Phi to current Kepler-based NVIDIA Tesla GPUs running a conjugate gradient
solver. By exposing more parallelism to the accelerator through inverting
multiple vectors at the same time we obtain a performance 250 GFlop/s on both
architectures. This more than doubles the performance of the inversions. We
give a short overview of both architectures, discuss some details of the
implementation and the effort required to obtain the achieved performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1513</identifier>
 <datestamp>2014-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1513</id><created>2014-09-04</created><authors><author><keyname>Xie</keyname><forenames>Ronggui</forenames></author><author><keyname>Yin</keyname><forenames>Huarui</forenames></author><author><keyname>Chen</keyname><forenames>Xiaohui</forenames></author><author><keyname>Wang</keyname><forenames>Zhengdao</forenames></author></authors><title>Multiple Access for Small Packets Based on Precoding and Sparsity-Aware
  Detection</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Transactions on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern mobile terminals often produce a large number of small data packets.
For these packets, it is inefficient to follow the conventional medium access
control protocols because of poor utilization of service resources. We propose
a novel multiple access scheme that employs block-spreading based precoding at
the transmitters and sparsity-aware detection schemes at the base station. The
proposed scheme is well suited for the emerging massive multiple-input
multiple-output (MIMO) systems, as well as conventional cellular systems with a
small number of base-station antennas. The transmitters employ precoding in
time domain to enable the simultaneous transmissions of many users, which could
be even more than the number of receive antennas at the base station. The
system is modeled as a linear system of equations with block-sparse unknowns.
We first adopt the block orthogonal matching pursuit (BOMP) algorithm to
recover the transmitted signals. We then develop an improved algorithm, named
interference cancellation BOMP (ICBOMP), which takes advantage of error
correction and detection coding to perform perfect interference cancellation
during each iteration of BOMP algorithm. Conditions for guaranteed data
recovery are identified. The simulation results demonstrate that the proposed
scheme can accommodate more simultaneous transmissions than conventional
schemes in typical small-packet transmission scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1515</identifier>
 <datestamp>2014-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1515</id><created>2014-09-04</created><authors><author><keyname>Mostafaei</keyname><forenames>Habib</forenames></author><author><keyname>Esnaashari</keyname><forenames>Mehdi</forenames></author><author><keyname>Meybodi</keyname><forenames>Mohammad Reza</forenames></author></authors><title>A Coverage Monitoring algorithm based on Learning Automata for Wireless
  Sensor Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To cover a set of targets with known locations within an area with limited or
prohibited ground access using a wireless sensor network, one approach is to
deploy the sensors remotely, from an aircraft. In this approach, the lack of
precise sensor placement is compensated by redundant de-ployment of sensor
nodes. This redundancy can also be used for extending the lifetime of the
network, if a proper scheduling mechanism is available for scheduling the
active and sleep times of sensor nodes in such a way that each node is in
active mode only if it is required to. In this pa-per, we propose an efficient
scheduling method based on learning automata and we called it LAML, in which
each node is equipped with a learning automaton, which helps the node to select
its proper state (active or sleep), at any given time. To study the performance
of the proposed method, computer simulations are conducted. Results of these
simulations show that the pro-posed scheduling method can better prolong the
lifetime of the network in comparison to similar existing method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1527</identifier>
 <datestamp>2014-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1527</id><created>2014-09-04</created><authors><author><keyname>Garnatz</keyname><forenames>Chris</forenames></author><author><keyname>Gu</keyname><forenames>Xiaoyi</forenames></author><author><keyname>Kingman</keyname><forenames>Alison</forenames></author><author><keyname>LaManna</keyname><forenames>James</forenames></author><author><keyname>Needell</keyname><forenames>Deanna</forenames></author><author><keyname>Tu</keyname><forenames>Shenyinying</forenames></author></authors><title>Practical approximate projection schemes in greedy signal space methods</title><categories>math.NA cs.IT math.IT</categories><msc-class>41A46, 68Q25, 68W20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compressive sensing (CS) is a new signal acquisition paradigm which shows
that far fewer samples are required to reconstruct sparse signals than
previously thought. Although most of the literature focuses on signals sparse
in a fixed orthonormal basis, recently the Signal Space CoSaMP (SSCoSaMP)
greedy method was developed for the reconstruction of signals compressible in
arbitrary redundant dictionaries. The algorithm itself needs access to
approximate sparse projection schemes, which have been difficult to obtain and
analyze. This paper investigates the use of several different projection
schemes and catalogs for what types of signals each scheme can successfully be
utilized. In addition, we present novel hybrid projection methods which
outperform all other schemes on a wide variety of signal classes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1533</identifier>
 <datestamp>2014-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1533</id><created>2014-09-02</created><authors><author><keyname>Oka</keyname><forenames>Mizuki</forenames></author><author><keyname>Abe</keyname><forenames>Hirotake</forenames></author><author><keyname>Ikegami</keyname><forenames>Takashi</forenames></author></authors><title>Dynamic Homeostasis in Packet Switching Networks</title><categories>cs.NI physics.soc-ph</categories><comments>18 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this study, we investigate the adaptation and robustness of a packet
switching network (PSN), the fundamental architecture of the Internet. We claim
that the adaptation introduced by a transmission control protocol (TCP)
congestion control mechanism is interpretable as the self-organization of
multiple attractors and stability to switch from one attractor to another. To
discuss this argument quantitatively, we study the adaptation of the Internet
by simulating a PSN using ns-2. Our hypothesis is that the robustness and
fragility of the Internet can be attributed to the inherent dynamics of the PSN
feedback mechanism called the congestion window size, or \textit{cwnd}. By
varying the data input into the PSN system, we investigate the possible
self-organization of attractors in cwnd temporal dynamics and discuss the
adaptability and robustness of PSNs. The present study provides an example of
Ashby's Law of Requisite Variety in action.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1534</identifier>
 <datestamp>2014-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1534</id><created>2014-09-04</created><authors><author><keyname>Basu</keyname><forenames>Saugata</forenames></author></authors><title>Algorithms in Real Algebraic Geometry: A Survey</title><categories>math.AG cs.CC cs.CG cs.SC</categories><comments>41 pages, 4 figures. Based on survey talk given at the Real Algebraic
  Geometry Conference, Rennes, June 20-24, 2011. Some references updated and
  some newer material added</comments><msc-class>Primary 14P10, 14P25, Secondary 68W30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We survey both old and new developments in the theory of algorithms in real
algebraic geometry -- starting from effective quantifier elimination in the
first order theory of reals due to Tarski and Seidenberg, to more recent
algorithms for computing topological invariants of semi-algebraic sets. We
emphasize throughout the complexity aspects of these algorithms and also
discuss the computational hardness of the underlying problems. We also describe
some recent results linking the computational hardness of decision problems in
the first order theory of the reals, with that of computing certain topological
invariants of semi-algebraic sets. Even though we mostly concentrate on exact
algorithms, we also discuss some numerical approaches involving semi-definite
programming that have gained popularity in recent times.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1544</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1544</id><created>2014-09-04</created><updated>2014-09-10</updated><authors><author><keyname>Streicher</keyname><forenames>Thomas</forenames></author><author><keyname>Keimel</keyname><forenames>K.</forenames></author><author><keyname>Streicher</keyname><forenames>T.</forenames></author></authors><title>Observationally-induced algebras in Domain Theory</title><categories>cs.LO</categories><comments>26 pages</comments><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 10, Issue 3 (September
  11, 2014) lmcs:963</journal-ref><doi>10.2168/LMCS-10(3:18)2014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we revise and simplify the notion of observationally induced
algebra introduced by Simpson and Schroeder for the purpose of modelling
computational effects in the particular case where the ambient category is
given by classical domain theory. As examples of the general framework we
consider the various powerdomains. For the particular case of the Plotkin
powerdomain the general recipe leads to a somewhat unexpected result which,
however, makes sense from a Computer Science perspective. We analyze this
&quot;deviation&quot; and show how to reobtain the original Plotkin powerdomain by
imposing further conditions previously considered by R.~Heckmann and
J.~Goubault-Larrecq.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1551</identifier>
 <datestamp>2014-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1551</id><created>2014-09-04</created><authors><author><keyname>Rouayheb</keyname><forenames>Salim El</forenames></author><author><keyname>Goparaju</keyname><forenames>Sreechakra</forenames></author><author><keyname>Kiah</keyname><forenames>Han Mao</forenames></author><author><keyname>Milenkovic</keyname><forenames>Olgica</forenames></author></authors><title>Synchronizing Edits in Distributed Storage Networks</title><categories>cs.IT cs.DC math.IT</categories><comments>This draft contains 33 pages. The authors are listed according to the
  Hardy-Littlewood rule (alphabetical in last name)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of synchronizing data in distributed storage networks
under an edit model that includes deletions and insertions. We present two
modifications of MDS, regenerating and locally repairable codes that allow
updates in the parity-check values to be performed with one round of
communication at low bit rates and using small storage overhead. Our main
contributions are novel protocols for synchronizing both hot and semi-static
data and protocols for data deduplication applications, based on intermediary
permutation, Vandermonde and Cauchy matrix coding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1556</identifier>
 <datestamp>2015-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1556</id><created>2014-09-04</created><updated>2015-04-10</updated><authors><author><keyname>Simonyan</keyname><forenames>Karen</forenames></author><author><keyname>Zisserman</keyname><forenames>Andrew</forenames></author></authors><title>Very Deep Convolutional Networks for Large-Scale Image Recognition</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we investigate the effect of the convolutional network depth on
its accuracy in the large-scale image recognition setting. Our main
contribution is a thorough evaluation of networks of increasing depth using an
architecture with very small (3x3) convolution filters, which shows that a
significant improvement on the prior-art configurations can be achieved by
pushing the depth to 16-19 weight layers. These findings were the basis of our
ImageNet Challenge 2014 submission, where our team secured the first and the
second places in the localisation and classification tracks respectively. We
also show that our representations generalise well to other datasets, where
they achieve state-of-the-art results. We have made our two best-performing
ConvNet models publicly available to facilitate further research on the use of
deep visual representations in computer vision.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1560</identifier>
 <datestamp>2014-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1560</id><created>2014-09-04</created><authors><author><keyname>Chen</keyname><forenames>Xiaojie</forenames></author><author><keyname>Perc</keyname><forenames>Matjaz</forenames></author></authors><title>Optimal distribution of incentives for public cooperation in
  heterogeneous interaction environments</title><categories>q-bio.PE cs.GT cs.SI physics.soc-ph</categories><comments>19 pages, 8 figures; accepted for publication in Frontiers in
  Behavioral Neuroscience</comments><journal-ref>Front. Behav. Neurosci. 8 (2014) 248</journal-ref><doi>10.3389/fnbeh.2014.00248</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the framework of evolutionary games with institutional reciprocity,
limited incentives are at disposal for rewarding cooperators and punishing
defectors. In the simplest case, it can be assumed that, depending on their
strategies, all players receive equal incentives from the common pool. The
question arises, however, what is the optimal distribution of institutional
incentives? How should we best reward and punish individuals for cooperation to
thrive? We study this problem for the public goods game on a scale-free
network. We show that if the synergetic effects of group interactions are weak,
the level of cooperation in the population can be maximized simply by adopting
the simplest &quot;equal distribution&quot; scheme. If synergetic effects are strong,
however, it is best to reward high-degree nodes more than low-degree nodes.
These distribution schemes for institutional rewards are independent of payoff
normalization. For institutional punishment, however, the same optimization
problem is more complex, and its solution depends on whether absolute or
degree-normalized payoffs are used. We find that degree-normalized payoffs
require high-degree nodes be punished more lenient than low-degree nodes.
Conversely, if absolute payoffs count, then high-degree nodes should be
punished stronger than low-degree nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1564</identifier>
 <datestamp>2014-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1564</id><created>2014-09-03</created><authors><author><keyname>Zhou</keyname><forenames>Yuzhe</forenames></author><author><keyname>Ai</keyname><forenames>Bo</forenames></author></authors><title>Evaluation of High-speed Train Communication Handover Models Based on
  DEA</title><categories>cs.NI</categories><comments>5 pages, accepted for publication, The IEEE 79th Vehicular Technology
  Conference (VTC 2014 Spring), May, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Broadband communications for high speed train is becoming a main trend in
high mobility communications. The main bottleneck of this communication network
is handover, since the handover occurs so frequently and the delays are so long
that broadband real-time communication cannot apply. Various handover models
have been developed and studied recently. However, no comprehensive evaluation
method for these models is employed. To this end, we borrow Data Envelopment
Analysis (DEA) method to evaluate six typical handover system models. Handover
models that to be evaluated are introduced. A brief presentation of DEA and its
characters is provided. A specific procedure of the evaluation is proposed.
Then the results of the evaluation are obtained by running the DEA. Finally, we
give our comments and conclusions to all the handover models. We hope our work
will supply a gap in the system evaluation area.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1568</identifier>
 <datestamp>2014-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1568</id><created>2014-09-03</created><updated>2014-09-09</updated><authors><author><keyname>Wang</keyname><forenames>Shuang</forenames></author><author><keyname>Chen</keyname><forenames>Wei</forenames></author><author><keyname>Yin</keyname><forenames>Zhen-Qiang</forenames></author><author><keyname>Li</keyname><forenames>Hong-Wei</forenames></author><author><keyname>He</keyname><forenames>De-Yong</forenames></author><author><keyname>Li</keyname><forenames>Yu-Hu</forenames></author><author><keyname>Zhou</keyname><forenames>Zheng</forenames></author><author><keyname>Song</keyname><forenames>Xiao-Tian</forenames></author><author><keyname>Li</keyname><forenames>Fang-Yi</forenames></author><author><keyname>Wang</keyname><forenames>Dong</forenames></author><author><keyname>Chen</keyname><forenames>Hua</forenames></author><author><keyname>Han</keyname><forenames>Yun-Guang</forenames></author><author><keyname>Huang</keyname><forenames>Jing-Zheng</forenames></author><author><keyname>Guo</keyname><forenames>Jun-Fu</forenames></author><author><keyname>Hao</keyname><forenames>Peng-Lei</forenames></author><author><keyname>Li</keyname><forenames>Mo</forenames></author><author><keyname>Zhang</keyname><forenames>Chun-Mei</forenames></author><author><keyname>Liu</keyname><forenames>Dong</forenames></author><author><keyname>Liang</keyname><forenames>Wen-Ye</forenames></author><author><keyname>Miao</keyname><forenames>Chun-Hua</forenames></author><author><keyname>Wu</keyname><forenames>Ping</forenames></author><author><keyname>Guo</keyname><forenames>Guang-Can</forenames></author><author><keyname>Han</keyname><forenames>Zheng-Fu</forenames></author></authors><title>Field and long-term demonstration of a wide area quantum key
  distribution network</title><categories>quant-ph cs.CR physics.optics</categories><comments>Ref.[65] is updated</comments><journal-ref>Optics Express, Vol. 22, Issue 18, pp. 21739-21756 (2014)</journal-ref><doi>10.1364/OE.22.021739</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A wide area quantum key distribution (QKD) network deployed on communication
infrastructures provided by China Mobile Ltd. is demonstrated. Three cities and
two metropolitan area QKD networks were linked up to form the Hefei-Chaohu-Wuhu
wide area QKD network with over 150 kilometers coverage area, in which Hefei
metropolitan area QKD network was a typical full-mesh core network to offer
all-to-all interconnections, and Wuhu metropolitan area QKD network was a
representative quantum access network with point-to-multipoint configuration.
The whole wide area QKD network ran for more than 5000 hours, from 21 December
2011 to 19 July 2012, and part of the network stopped until last December. To
adapt to the complex and volatile field environment, the Faraday-Michelson QKD
system with several stability measures was adopted when we designed QKD
devices. Through standardized design of QKD devices, resolution of symmetry
problem of QKD devices, and seamless switching in dynamic QKD network, we
realized the effective integration between point-to-point QKD techniques and
networking schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1576</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1576</id><created>2014-09-04</created><updated>2015-01-21</updated><authors><author><keyname>Hajian</keyname><forenames>Amir</forenames></author><author><keyname>Alvarez</keyname><forenames>Marcelo</forenames></author><author><keyname>Bond</keyname><forenames>J. Richard</forenames></author></authors><title>Machine Learning Etudes in Astrophysics: Selection Functions for Mock
  Cluster Catalogs</title><categories>astro-ph.CO astro-ph.IM cs.LG stat.ML</categories><comments>Matches version to appear in JCAP. Discussions expanded, 2 figures
  added. 16 pages, 8 figures</comments><doi>10.1088/1475-7516/2015/01/038</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Making mock simulated catalogs is an important component of astrophysical
data analysis. Selection criteria for observed astronomical objects are often
too complicated to be derived from first principles. However the existence of
an observed group of objects is a well-suited problem for machine learning
classification. In this paper we use one-class classifiers to learn the
properties of an observed catalog of clusters of galaxies from ROSAT and to
pick clusters from mock simulations that resemble the observed ROSAT catalog.
We show how this method can be used to study the cross-correlations of thermal
Sunya'ev-Zeldovich signals with number density maps of X-ray selected cluster
catalogs. The method reduces the bias due to hand-tuning the selection function
and is readily scalable to large catalogs with a high-dimensional space of
astrophysical features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1606</identifier>
 <datestamp>2014-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1606</id><created>2014-09-04</created><authors><author><keyname>Islam</keyname><forenames>Muhammad Nazmul</forenames></author><author><keyname>Mandayam</keyname><forenames>Narayan B.</forenames></author><author><keyname>Kompella</keyname><forenames>Sastry</forenames></author><author><keyname>Seskar</keyname><forenames>Ivan</forenames></author></authors><title>Power Optimal Non-contiguous Spectrum Access in Multi Front End Radio
  Enabled Point-to-Point Link</title><categories>cs.NI cs.IT math.IT</categories><comments>5 pages, 3 figure. To be submitted to IEEE Communication Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Non-contiguous spectrum chunks allow wireless links to flexibly access a wide
amount of bandwidth. Multi- Channel Multi-Radio (MC-MR) and Non-Contiguous
Orthogonal Frequency Division Multiplexing (NC-OFDM) are the two commercially
viable strategies to access non-contiguous spectrum chunks. MC-MR accesses
multiple non-contiguous chunks by activating multiple front ends which, in
turn, increases the circuit power consumption of each of the activated front
ends. NC-OFDM accesses non-contiguous spectrum chunks with a single front end
by nulling remaining subchannels but increases spectrum span which, in turn,
increases the power consumption of ADC and DAC. This work focuses on a
point-to-point link where transmitter and receiver have multiple front ends and
can employ NC-OFDM technology. We investigate optimal spectrum fragmentation in
each front end from a system power (summation of transmit power and circuit
power) perspective. We formulate a mixed integer non-linear program (MINLP) to
perform power control and scheduling, and minimize system power by providing a
greedy algorithm (O(M^3 I)) where M and I denote the number of channels and
radio front ends respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1612</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1612</id><created>2014-09-04</created><updated>2014-10-26</updated><authors><author><keyname>Kutuzov</keyname><forenames>Andrey</forenames></author></authors><title>Semantic clustering of Russian web search results: possibilities and
  problems</title><categories>cs.CL cs.IR</categories><comments>Presented at Russian Summer School in Information Retrieval (RuSSIR
  2014). To be published in Springer Communications in Computer and Information
  Science series</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The paper deals with word sense induction from lexical co-occurrence graphs.
We construct such graphs on large Russian corpora and then apply this data to
cluster Mail.ru Search results according to meanings of the query. We compare
different methods of performing such clustering and different source corpora.
Models of applying distributional semantics to big linguistic data are
described.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1619</identifier>
 <datestamp>2014-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1619</id><created>2014-09-04</created><authors><author><keyname>Johnsen</keyname><forenames>Aleck</forenames></author><author><keyname>Kao</keyname><forenames>Ming-Yang</forenames></author><author><keyname>Seki</keyname><forenames>Shinnosuke</forenames></author></authors><title>A manually-checkable proof for the NP-hardness of 11-color pattern
  self-assembly tile set synthesis</title><categories>cs.DM</categories><msc-class>68Q17, 92B05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Patterned self-assembly tile set synthesis (PATS) aims at finding a minimum
tile set to uniquely self-assemble a given rectangular (color) pattern. For k
&gt;= 1, k-PATS is a variant of PATS that restricts input patterns to those with
at most $k$ colors. A computer-assisted proof has been recently proposed for
2-PATS by Kari et al. [arXiv:1404.0967 (2014)]. In contrast, the best known
manually-checkable proof is for the NP-hardness of 29-PATS by Johnsen, Kao, and
Seki [ISAAC 2013, LNCS 8283, pp.~699-710]. We propose a manually-checkable
proof for the NP-hardness of 11-PATS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1628</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1628</id><created>2014-09-04</created><updated>2015-06-01</updated><authors><author><keyname>Usman</keyname><forenames>Muneer</forenames></author><author><keyname>Yang</keyname><forenames>Hong-Chuan</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author></authors><title>Extended Delivery Time Analysis for Cognitive Packet Transmission with
  Application to Secondary Queuing Analysis</title><categories>cs.IT cs.NI math.IT</categories><comments>28 pages, 10 figures. arXiv admin note: substantial text overlap with
  arXiv:1409.0911</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cognitive radio transceiver can opportunistically access the underutilized
spectrum resource of primary systems for new wireless services. With interleave
implementation, the secondary transmission may be interrupted by the primary
user's transmission. To facilitate the delay analysis of such secondary
transmission for a fixed-size secondary packet, we study the resulting extended
delivery time that includes both transmission time and waiting time. In
particular, we derive the exact distribution function of extended delivery time
of secondary transmission for both continuous sensing and periodic sensing
cases. Selected numerical and simulation results are presented for illustrating
the mathematical formulation. Finally, we consider a generalized M/G/1 queue
set-up at the secondary user and formulate the closed-form expressions for the
expected delay with Poisson traffic. The analytical results will greatly
facilitate the design of the secondary system for particular target
application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1636</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1636</id><created>2014-09-04</created><updated>2014-09-13</updated><authors><author><keyname>Liu</keyname><forenames>Xiufeng</forenames></author></authors><title>Two-level Data Staging ETL for Transaction Data</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In data warehousing, Extract-Transform-Load (ETL) extracts the data from data
sources into a central data warehouse regularly for the support of business
decision-makings. The data from transaction processing systems are featured
with the high frequent changes of insertion, update, and deletion. It is
challenging for ETL to propagate the changes to the data warehouse, and
maintain the change history. Moreover, ETL jobs typically run in a sequential
order when processing the data with dependencies, which is not optimal, \eg,
when processing early-arriving data. In this paper, we propose a two-level data
staging ETL for handling transaction data. The proposed method detects the
changes of the data from transactional processing systems, identifies the
corresponding operation codes for the changes, and uses two staging databases
to facilitate the data processing in an ETL process. The proposed ETL provides
the &quot;one-stop&quot; method for fast-changing, slowly-changing and early-arriving
data processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1639</identifier>
 <datestamp>2014-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1639</id><created>2014-09-04</created><authors><author><keyname>Liu</keyname><forenames>Xiufeng</forenames></author></authors><title>Optimizing ETL Dataflow Using Shared Caching and Parallelization Methods</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Extract-Transform-Load (ETL) handles large amount of data and manages
workload through dataflows. ETL dataflows are widely regarded as complex and
expensive operations in terms of time and system resources. In order to
minimize the time and the resources required by ETL dataflows, this paper
presents a framework to optimize dataflows using shared cache and
parallelization techniques. The framework classifies the components in an ETL
dataflow into different categories based on their data operation properties.
The framework then partitions the dataflow based on the classification at
different granularities. Furthermore, the framework applies optimization
techniques such as cache re-using, pipelining and multi-threading to the
already-partitioned dataflows. The proposed techniques reduce system memory
footprint and the frequency of copying data between different components, and
also take full advantage of the computing power of multi-core processors. The
experimental results show that the proposed optimization framework is 4.7 times
faster than the ordinary ETL dataflows (without using the proposed optimization
techniques), and outperforms the similar tool (Kettle).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1654</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1654</id><created>2014-09-04</created><updated>2014-09-12</updated><authors><author><keyname>Shahin</keyname><forenames>Ashraf A.</forenames></author></authors><title>Polymorphic Worms Collection in Cloud Computing</title><categories>cs.DC cs.CR</categories><comments>International Journal of Computer Science and Mobile Computing, Vol.3
  Issue.8, August- 2014, pg. 645-652</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the past few years, computer worms are seen as one of significant
challenges of cloud computing. Worms are rapidly changing and getting more
sophisticated to evade detection. One major issue to defend against computer
worms is collecting worms' payloads to generate their signature and study their
behavior. To collect worms' payloads, we identified challenges for detecting
and collecting worms' payloads and proposed high-interactive honeypot to
collect payloads of zero-day polymorphic worms in homogeneous and heterogeneous
cloud computing platforms. Virtual machine (VM) memory and VM disk image are
inspected from outside using open-source forensics tools and VMWare Virtual
Disk Development Kit. Our experiments show that the proposed approach overcomes
the identified challenges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1656</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1656</id><created>2014-09-04</created><updated>2014-09-12</updated><authors><author><keyname>Shahin</keyname><forenames>Ashraf A.</forenames></author><author><keyname>Samir</keyname><forenames>Areeg</forenames></author><author><keyname>Khamis</keyname><forenames>Abdelaziz</forenames></author></authors><title>An Aspect-Oriented Approach for SaaS Application Customization</title><categories>cs.SE</categories><comments>48th Conference on Statistics, Computer Science and Operations
  Research, Cairo University, Egypt, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-tenancy is one of the most important concepts for any Software as a
Service (SaaS) application. Multi-tenant SaaS application serves a large number
of tenants with one single application instance. Complex SaaS application that
serves significant number of tenants could have a huge number of customizations
with complicated relationships, which increases the customization complexity
and reduces the customization understandability. Modeling such customizations,
validating each tenant's customization, and adapting SaaS applications on the
fly based on each tenant's requirements become very complex tasks. To mitigate
these challenges, we propose an aspect-oriented approach that makes use of the
Orthogonal Variability Model (OVM) and Metagraphs. The OVM is used to provide
the tenants with simple and understandable customization model. A
Metagraph-based algorithm has been developed to validate tenants'
customizations. On the other hand, the aspect-oriented approach offers a high
level of runtime adaptability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1657</identifier>
 <datestamp>2014-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1657</id><created>2014-09-04</created><authors><author><keyname>Jiang</keyname><forenames>Shaoquan</forenames></author></authors><title>On the Optimality of Keyless Authentication in a Noisy Model</title><categories>cs.IT cs.CR math.IT</categories><comments>10 pages</comments><msc-class>94A60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We further study the keyless authentication problem in a noisy model in our
previous work, where no secret setup is available for sender Alice and receiver
Bob while there is DMC $W_1$ from Alice to Bob and a two-way noiseless but
insecure channel between them. We propose a construction such that the message
length over DMC $W_1$ does not depend on the size of the source space. If the
source space is ${\cal S}$ and the number of channel $W_1$ uses is $n$, then
our protocol only has a round complexity of $\log^*|{\cal S}|-\log^*n+4.$ In
addition, we show that the round complexity of any secure protocol in our model
is lower bounded by $\log^*|{\cal S}|-\log^* n-5$. We also obtain a lower bound
on the success probability when the message size on DMC $W_1$ is given.
Finally, we derive the capacity for a non-interactive authentication protocol
under general DMCs, which extends the result under BSCs in our previous work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1660</identifier>
 <datestamp>2014-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1660</id><created>2014-09-05</created><authors><author><keyname>Weekly</keyname><forenames>Kevin</forenames></author><author><keyname>Jin</keyname><forenames>Ming</forenames></author><author><keyname>Zou</keyname><forenames>Han</forenames></author><author><keyname>Hsu</keyname><forenames>Christopher</forenames></author><author><keyname>Bayen</keyname><forenames>Alexandre</forenames></author><author><keyname>Spanos</keyname><forenames>Costas</forenames></author></authors><title>Building-in-Briefcase (BiB)</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A building's environment has profound influence on occupant comfort and
health. Continuous monitoring of building occupancy and environment is
essential to fault detection, intelligent control, and building commissioning.
Though many solutions for environmental measuring based on wireless sensor
networks exist, they are not easily accessible to households and building
owners who may lack time or technical expertise needed to set up a system and
get quick and detailed overview of environmental conditions.
  Building-in-Briefcase (BiB) is a portable sensor network platform that is
trivially easy to deploy in any building environment. Once the sensors are
distributed, the environmental data is collected and communicated to the BiB
router via TCP/IP protocol and WiFi technology which then forwards the data to
the central database securely over the internet through a 3G radio. The user,
with minimal effort, can access the aggregated data and visualize the trends in
real time on the BiB web portal. Paramount to the adoption and continued
operation of an indoor sensing platform is battery lifetime. This design has
achieved a multi-year lifespan by careful selection of components, an efficient
binary communications protocol and data compression. Our BiB sensor is capable
of collecting a rich set of environmental parameters, and is expandable to
measure others, such as CO2. This paper describes the power characteristics of
BiB sensors and their occupancy estimation and activity recognition
functionality. Our vision is large-scale deployment of BiB in thousands of
buildings, which would provide ample research opportunities and opportunities
to identify ways to improve the building environment and energy efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1661</identifier>
 <datestamp>2015-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1661</id><created>2014-09-05</created><updated>2015-05-05</updated><authors><author><keyname>Kumbhkar</keyname><forenames>Ratnesh</forenames></author><author><keyname>Islam</keyname><forenames>Muhammad Nazmul</forenames></author><author><keyname>Mandayam</keyname><forenames>Narayan B.</forenames></author><author><keyname>Seskar</keyname><forenames>Ivan</forenames></author></authors><title>Rate Optimal design of a Wireless Backhaul Network using TV White Space</title><categories>cs.NI</categories><doi>10.1109/COMSNETS.2015.7098691</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The penetration of wireless broadband services in remote areas has primarily
been limited due to the lack of economic incentives that service providers
encounter in sparsely populated areas. Besides, wireless backhaul links like
satellite and microwave are either expensive or require strict line of sight
communication making them unattractive. TV white space channels with their
desirable radio propagation characteristics can provide an excellent
alternative for engineering backhaul networks in areas that lack abundant
infrastructure. Specifically, TV white space channels can provide &quot;free
wireless backhaul pipes&quot; to transport aggregated traffic from broadband sources
to fiber access points. In this paper, we investigate the feasibility of
multi-hop wireless backhaul in the available white space channels by using
noncontiguous Orthogonal Frequency Division Multiple Access (NC-OFDMA)
transmissions between fixed backhaul towers. Specifically, we consider joint
power control, scheduling and routing strategies to maximize the minimum rate
across broadband towers in the network. Depending on the population density and
traffic demands of the location under consideration, we discuss the suitable
choice of cell size for the backhaul network. Using the example of available TV
white space channels in Wichita, Kansas (a small city located in central USA),
we provide illustrative numerical examples for designing such wireless backhaul
network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1662</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1662</id><created>2014-09-05</created><updated>2016-01-23</updated><authors><author><keyname>Yang</keyname><forenames>Shengtian</forenames></author></authors><title>Separate Random Number Generation from Correlated Sources</title><categories>cs.IT math.IT</categories><comments>v1.2.1-48340b, no.201601241110, 22 pages, 2 figures, revised</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work studies the problem of separate random number generation from
correlated general sources with side information at the tester under the
criterion of statistical distance. Tight one-shot lower and upper performance
bounds, as well as the achievable rate region, are obtained using the
random-bin approach and the information-spectrum approach. A refined analysis
is further performed for two important random-bin maps. One is the
pure-random-bin map that is uniformly distributed over the set of all maps
(with the same domain and codomain). The other is the equal-random-bin map that
is uniformly distributed over the set of all surjective maps that induce an
equal or quasi-equal partition of the domain. Both of them are proved to have a
doubly-exponential concentration of the performance of their sample maps. As an
application, an open and transparent lottery scheme, using a random number
generator on a public data source, is proposed to solve the social problem of
scarce resource allocation. The core of the proposed framework of lottery
algorithms is a permutation, a good rateless randomness extractor, whose
existence is confirmed by the theoretical performance of equal-random-bin maps.
This extractor, together with other important details of the scheme, ensures
that the lottery scheme is immune to all kinds of fraud under some reasonable
assumptions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1666</identifier>
 <datestamp>2014-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1666</id><created>2014-09-05</created><authors><author><keyname>Nakkiran</keyname><forenames>Preetum</forenames></author><author><keyname>Shah</keyname><forenames>Nihar B.</forenames></author><author><keyname>Rashmi</keyname><forenames>K. V.</forenames></author></authors><title>Fundamental Limits on Communication for Oblivious Updates in Storage
  Networks</title><categories>cs.IT cs.DC cs.NI math.IT</categories><comments>IEEE Global Communications Conference (GLOBECOM) 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In distributed storage systems, storage nodes intermittently go offline for
numerous reasons. On coming back online, nodes need to update their contents to
reflect any modifications to the data in the interim. In this paper, we
consider a setting where no information regarding modified data needs to be
logged in the system. In such a setting, a 'stale' node needs to update its
contents by downloading data from already updated nodes, while neither the
stale node nor the updated nodes have any knowledge as to which data symbols
are modified and what their value is. We investigate the fundamental limits on
the amount of communication necessary for such an &quot;oblivious&quot; update process.
  We first present a generic lower bound on the amount of communication that is
necessary under any storage code with a linear encoding (while allowing
non-linear update protocols). This lower bound is derived under a set of
extremely weak conditions, giving all updated nodes access to the entire
modified data and the stale node access to the entire stale data as side
information. We then present codes and update algorithms that are optimal in
that they meet this lower bound. Next, we present a lower bound for an
important subclass of codes, that of linear Maximum-Distance-Separable (MDS)
codes. We then present an MDS code construction and an associated update
algorithm that meets this lower bound. These results thus establish the
capacity of oblivious updates in terms of the communication requirements under
these settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1668</identifier>
 <datestamp>2014-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1668</id><created>2014-09-05</created><updated>2014-09-11</updated><authors><author><keyname>Cheng</keyname><forenames>Fan</forenames></author></authors><title>Generalization of Mrs. Gerber's Lemma</title><categories>cs.IT math.IT</categories><comments>Accepted by Communications in Information and Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mrs. Gerber's Lemma (MGL) hinges on the convexity of $H(p*H^{-1}(u))$, where
$H(u)$ is the binary entropy function. In this work, we prove that $H(p*f(u))$
is convex in $u$ for every $p\in [0,1]$ provided $H(f(u))$ is convex in $u$,
where $f(u) : (a, b) \to [0, \frac12]$. Moreover, our result subsumes MGL and
simplifies the original proof. We show that the generalized MGL can be applied
in binary broadcast channel to simplify some discussion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1673</identifier>
 <datestamp>2014-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1673</id><created>2014-09-05</created><authors><author><keyname>Mishra</keyname><forenames>Kumar Vijay</forenames></author><author><keyname>Cho</keyname><forenames>Myung</forenames></author><author><keyname>Kruger</keyname><forenames>Anton</forenames></author><author><keyname>Xu</keyname><forenames>Weiyu</forenames></author></authors><title>Spectral Super-resolution With Prior Knowledge</title><categories>cs.IT math.IT</categories><comments>13 pages, 8 figures. arXiv admin note: text overlap with
  arXiv:1404.7041, arXiv:1311.0950</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of super-resolution frequency recovery using prior
knowledge of the structure of a spectrally sparse, undersampled signal. In many
applications of interest, some structure information about the signal spectrum
is often known. The prior information might be simply knowing precisely some
signal frequencies or the likelihood of a particular frequency component in the
signal. We devise a general semidefinite program to recover these frequencies
using theories of positive trigonometric polynomials. Our theoretical analysis
shows that, given sufficient prior information, perfect signal reconstruction
is possible using signal samples no more than thrice the number of signal
frequencies. Numerical experiments demonstrate great performance enhancements
using our method. We show that the nominal resolution necessary for the
grid-free results can be improved if prior information is suitably employed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1676</identifier>
 <datestamp>2014-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1676</id><created>2014-09-05</created><authors><author><keyname>Karthick</keyname><forenames>T.</forenames></author></authors><title>New polynomial case for efficient domination in $P_6$-free graphs</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a graph $G$, an {\it efficient dominating set} is a subset $D$ of vertices
such that $D$ is an independent set and each vertex outside $D$ has exactly one
neighbor in $D$. The {\textsc{Efficient Dominating Set}} problem (EDS) asks for
the existence of an efficient dominating set in a given graph $G$. The EDS is
known to be $NP$-complete for $P_7$-free graphs, and is known to be polynomial
time solvable for $P_5$-free graphs. However, the computational complexity of
the EDS problem is unknown for $P_6$-free graphs. In this paper, we show that
the EDS problem can be solved in polynomial time for a subclass of $P_6$-free
graphs, namely ($P_6$, banner)-free graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1686</identifier>
 <datestamp>2014-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1686</id><created>2014-09-05</created><authors><author><keyname>Go&#xeb;ffon</keyname><forenames>Adrien</forenames></author><author><keyname>Lardeux</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames></author><author><keyname>Saubion</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames></author></authors><title>Simulating Non Stationary Operators in Search Algorithms</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a model for simulating search operators whose
behaviour often changes continuously during the search. In these scenarios, the
performance of the operators decreases when they are applied. This is motivated
by the fact that operators for optimization problems are often roughly
classified into exploitation operators and exploration operators. Our
simulation model is used to compare the different performances of operator
selection policies and clearly identify their ability to adapt to such specific
operators behaviours. The experimental study provides interesting results on
the respective behaviours of operator selection policies when faced to such non
stationary search scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1693</identifier>
 <datestamp>2014-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1693</id><created>2014-09-05</created><authors><author><keyname>Yassine</keyname><forenames>Benjelloun Touimi</forenames></author></authors><title>The Homogeneity Indicator of Learners in Project-based Learning</title><categories>cs.CY</categories><journal-ref>IJCSI-2014-11-1-8120</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The development of ICT has emerged a new way of learning using electronic
platforms: E-learning. In addition, pedagogical approaches have been adopted in
teaching based on group learning, such as the project-based teaching. The
project-based teaching is an active learning method, based on group work to
develop skills and acquire knowledge. However, the group of students is facing
several challenges throughout the project, such as the decision-making group.
The group decision generates convergences and divergences among members. Our
approach in this article relates to the calculation of the homogeneity of a
group of learners during decision making in an educational project.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1694</identifier>
 <datestamp>2015-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1694</id><created>2014-09-05</created><updated>2015-03-16</updated><authors><author><keyname>Flouri</keyname><forenames>Tomas</forenames></author><author><keyname>Giaquinta</keyname><forenames>Emanuele</forenames></author><author><keyname>Kobert</keyname><forenames>Kassian</forenames></author><author><keyname>Ukkonen</keyname><forenames>Esko</forenames></author></authors><title>Longest common substrings with k mismatches</title><categories>cs.DS</categories><comments>Accepted version</comments><doi>10.1016/j.ipl.2015.03.006</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The longest common substring with $k$-mismatches problem is to find, given
two strings $S_1$ and $S_2$, a longest substring $A_1$ of $S_1$ and $A_2$ of
$S_2$ such that the Hamming distance between $A_1$ and $A_2$ is $\le k$. We
introduce a practical $O(nm)$ time and $O(1)$ space solution for this problem,
where $n$ and $m$ are the lengths of $S_1$ and $S_2$, respectively. This
algorithm can also be used to compute the matching statistics with
$k$-mismatches of $S_1$ and $S_2$ in $O(nm)$ time and $O(m)$ space. Moreover,
we also present a theoretical solution for the $k = 1$ case which runs in $O(n
\log m)$ time, assuming $m\le n$, and uses $O(m)$ space, improving over the
existing $O(nm)$ time and $O(m)$ space bound of Babenko and Starikovskaya.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1695</identifier>
 <datestamp>2014-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1695</id><created>2014-09-05</created><authors><author><keyname>Schatz</keyname><forenames>Simon P.</forenames></author><author><keyname>Yucelen</keyname><forenames>Tansel</forenames></author></authors><title>Scalability Concept for Predictable Closed-Loop Response of Adaptive
  Controllers</title><categories>cs.SY math.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new concept called scalability to adaptive control in this
paper. In particular, we analyze how to scale learning rates of adaptive weight
update laws of various adaptive control schemes with respect to given command
profiles to achieve a predictable closed-loop response. An illustrative
numerical example is provided to demonstrate the proposed concept, which
emphasize that it can be an effective tool for validation and verification of
adaptive controllers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1699</identifier>
 <datestamp>2014-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1699</id><created>2014-09-05</created><authors><author><keyname>Belciug</keyname><forenames>Cristian-Eduard</forenames></author><author><keyname>Schipor</keyname><forenames>Ovidiu-Andrei</forenames></author><author><keyname>Danubianu</keyname><forenames>Mirela</forenames></author></authors><title>Exercises for Children with Dyslalia-Software Infrastructure</title><categories>cs.OH</categories><comments>5 pages, 3 figures</comments><journal-ref>Distributed Systems, University Of Suceava, 2007</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to help children with dyslalia we created a set of software
exercises. This set has a unitary software block (data base, programming
language, programming philosophy). In this paper we present this software
infrastructure with its advantage and disadvantage. The exercises are part of a
software system named LOGOMON. Therefore, besides horizontal compatibilities
(between exercises) vertical compatibilities are also presented (with LOGOMON
system). Concerning database tables used for modulus of exercises, a part of
them is &quot;inherited&quot; from LOGOMON application and another are specific for
exercises application. We also need to specify that there were necessary minor
changes of database tables used by LOGOMON. As programming language we used C#,
implemented in Visual Studio 2005. We developed specific interfaces elements
and classes. We also used multimedia resources that were necessary for
exercises (images, correct pronouncing obtained from speech therapist
recording, video clips). Another section of this application is related to
loading of exercises on mobile devices (Pocket PC). A part of code has been
imported directly, but there were a lot of files that need to be rewritten.
Anyway, the multimedia resources were used without any processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1714</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1714</id><created>2014-09-05</created><updated>2016-01-22</updated><authors><author><keyname>Cristiani</keyname><forenames>Emiliano</forenames></author><author><keyname>Rocchi</keyname><forenames>Leonardo</forenames></author></authors><title>A level-set-based method for fixing overhangs in 3D printing</title><categories>math.NA cs.GR</categories><msc-class>65D17, 35F21</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  3D printers based on the Fused Decomposition Modeling create objects
layer-by-layer dropping fused material. As a consequence, strong overhangs
cannot be printed because the new-come material does not find a suitable
support over the last deposed layer. In these cases, one can add some support
structures (scaffolds) which make the object printable, to be removed at the
end. In this paper we propose a level-set method to create object-dependent
support structures, specifically conceived to reduce both the amount of
additional material and the printing time. We also review some open problems
about 3D printing which can be of interests for the mathematical community.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1715</identifier>
 <datestamp>2014-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1715</id><created>2014-09-05</created><authors><author><keyname>di Tollo</keyname><forenames>Giacomo</forenames></author><author><keyname>Lardeux</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames></author><author><keyname>Maturana</keyname><forenames>Jorge</forenames></author><author><keyname>Saubion</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames></author></authors><title>An Experimental Study of Adaptive Control for Evolutionary Algorithms</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The balance of exploration versus exploitation (EvE) is a key issue on
evolutionary computation. In this paper we will investigate how an adaptive
controller aimed to perform Operator Selection can be used to dynamically
manage the EvE balance required by the search, showing that the search
strategies determined by this control paradigm lead to an improvement of
solution quality found by the evolutionary algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1716</identifier>
 <datestamp>2014-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1716</id><created>2014-09-05</created><authors><author><keyname>Theodorakopoulos</keyname><forenames>George</forenames></author><author><keyname>Shokri</keyname><forenames>Reza</forenames></author><author><keyname>Troncoso</keyname><forenames>Carmela</forenames></author><author><keyname>Hubaux</keyname><forenames>Jean-Pierre</forenames></author><author><keyname>Boudec</keyname><forenames>Jean-Yves Le</forenames></author></authors><title>Prolonging the Hide-and-Seek Game: Optimal Trajectory Privacy for
  Location-Based Services</title><categories>cs.CR</categories><comments>Workshop on Privacy in the Electronic Society (WPES 2014)</comments><acm-class>C.2.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Human mobility is highly predictable. Individuals tend to only visit a few
locations with high frequency, and to move among them in a certain sequence
reflecting their habits and daily routine. This predictability has to be taken
into account in the design of location privacy preserving mechanisms (LPPMs) in
order to effectively protect users when they continuously expose their position
to location-based services (LBSs). In this paper, we describe a method for
creating LPPMs that are customized for a user's mobility profile taking into
account privacy and quality of service requirements. By construction, our LPPMs
take into account the sequential correlation across the user's exposed
locations, providing the maximum possible trajectory privacy, i.e., privacy for
the user's present location, as well as past and expected future locations.
Moreover, our LPPMs are optimal against a strategic adversary, i.e., an
attacker that implements the strongest inference attack knowing both the LPPM
operation and the user's mobility profile. The optimality of the LPPMs in the
context of trajectory privacy is a novel contribution, and it is achieved by
formulating the LPPM design problem as a Bayesian Stackelberg game between the
user and the adversary. An additional benefit of our formal approach is that
the design parameters of the LPPM are chosen by the optimization algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1722</identifier>
 <datestamp>2014-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1722</id><created>2014-09-05</created><authors><author><keyname>Christ</keyname><forenames>Marie G.</forenames></author><author><keyname>Favrholdt</keyname><forenames>Lene M.</forenames></author><author><keyname>Larsen</keyname><forenames>Kim S.</forenames></author></authors><title>Online Multi-Coloring with Advice</title><categories>cs.DS</categories><comments>IMADA-preprint-cs</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of online graph multi-coloring with advice.
Multi-coloring is often used to model frequency allocation in cellular
networks. We give several nearly tight upper and lower bounds for the most
standard topologies of cellular networks, paths and hexagonal graphs. For the
path, negative results trivially carry over to bipartite graphs, and our
positive results are also valid for bipartite graphs. The advice given
represents information that is likely to be available, studying for instance
the data from earlier similar periods of time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1726</identifier>
 <datestamp>2014-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1726</id><created>2014-09-05</created><authors><author><keyname>Cerin&#x161;ek</keyname><forenames>Monika</forenames></author><author><keyname>Batagelj</keyname><forenames>Vladimir</forenames></author></authors><title>Network analysis of Zentralblatt MATH data</title><categories>cs.SI cs.DL physics.soc-ph</categories><comments>25 pages, 10 figures, 13 tables</comments><msc-class>01A90, 00A15, 91D30, 68R10, 93A15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze the data about works (papers, books) from the time period
1990-2010 that are collected in Zentralblatt MATH database. The data were
converted into four 2-mode networks (works $\times$ authors, works $\times$
journals, works $\times$ keywords and works $\times$ MSCs) and into a partition
of works by publication year. The networks were analyzed using Pajek -- a
program for analysis and visualization of large networks. We explore the
distributions of some properties of works and the collaborations among
mathematicians. We also take a closer look at the characteristics of the field
of graph theory as were realized with the publications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1730</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1730</id><created>2014-09-05</created><updated>2015-02-02</updated><authors><author><keyname>Trajanovski</keyname><forenames>Stojan</forenames></author><author><keyname>Hayel</keyname><forenames>Yezekael</forenames></author><author><keyname>Altman</keyname><forenames>Eitan</forenames></author><author><keyname>Wang</keyname><forenames>Huijuan</forenames></author><author><keyname>Van Mieghem</keyname><forenames>Piet</forenames></author></authors><title>Decentralized Protection Strategies against SIS Epidemics in Networks</title><categories>cs.SY cs.GT cs.SI</categories><comments>accepted for publication in IEEE Transactions on Control of Network
  Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Defining an optimal protection strategy against viruses, spam propagation or
any other kind of contamination process is an important feature for designing
new networks and architectures. In this work, we consider decentralized optimal
protection strategies when a virus is propagating over a network through a SIS
epidemic process. We assume that each node in the network can fully protect
itself from infection at a constant cost, or the node can use recovery
software, once it is infected.
  We model our system using a game theoretic framework and find pure, mixed
equilibria, and the Price of Anarchy (PoA) in several network topologies.
Further, we propose both a decentralized algorithm and an iterative procedure
to compute a pure equilibrium in the general case of a multiple communities
network. Finally, we evaluate the algorithms and give numerical illustrations
of all our results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1739</identifier>
 <datestamp>2014-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1739</id><created>2014-09-05</created><authors><author><keyname>Paschos</keyname><forenames>Georgios</forenames></author><author><keyname>Modiano</keyname><forenames>Eytan</forenames></author></authors><title>Throughput Optimal Routing in Overlay Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Maximum throughput requires path diversity enabled by bifurcating traffic at
different network nodes. In this work, we consider a network where traffic
bifurcation is allowed only at a subset of nodes called \emph{routers}, while
the rest nodes (called \emph{forwarders}) cannot bifurcate traffic and hence
only forward packets on specified paths. This implements an overlay network of
routers where each overlay link corresponds to a path in the physical network.
We study dynamic routing implemented at the overlay. We develop a queue-based
policy, which is shown to be maximally stable (throughput optimal) for a
restricted class of network scenarios where overlay links do not correspond to
overlapping physical paths. Simulation results show that our policy yields
better delay over dynamic policies that allow bifurcation at all nodes, such as
the backpressure policy. Additionally, we provide a heuristic extension of our
proposed overlay routing scheme for the unrestricted class of networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1744</identifier>
 <datestamp>2014-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1744</id><created>2014-09-05</created><authors><author><keyname>Traag</keyname><forenames>V. A.</forenames></author><author><keyname>Reinanda</keyname><forenames>R.</forenames></author><author><keyname>van Klinken</keyname><forenames>G.</forenames></author></authors><title>Structure of an elite co-occurrence network</title><categories>physics.soc-ph cs.CL cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rise of social media allowed for rich analyses of their content and their
network structure. As traditional media (i.e. newspapers and magazines) are
being digitized, similar analyses can be undertaken. This provides a glimpse of
the elite, as the news mostly revolves around the more influential members of
society. We here focus on a network structure derived from co-occurrences of
people in the media. This network has a strong core with peripheral clusters
being connected to the core. Nonetheless, these characteristics seem to be
mainly a result from the bipartite structure of the data. We employ a simple
growing bipartite model that can qualitatively reproduce such a core-periphery
structure. Two self-reinforcing processes are vital: (1) more frequently
occurring persons are more likely to occur again; and (2) if two people
co-occur frequently, they are more likely to co-occur again. This suggests that
the core-periphery structure is not necessarily reflective of the elite network
in society, but might be an artefact of how they are portrayed in the media.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1749</identifier>
 <datestamp>2014-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1749</id><created>2014-09-05</created><authors><author><keyname>Piotr&#xf3;w</keyname><forenames>Marek</forenames></author></authors><title>Faster Small-Constant-Periodic Merging Networks</title><categories>cs.DS</categories><comments>arXiv admin note: substantial text overlap with arXiv:1401.0396</comments><msc-class>68Q05, 68Q25</msc-class><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of merging two sorted sequences on a comparator
network that is used repeatedly, that is, if the output is not sorted, the
network is applied again using the output as input. The challenging task is to
construct such networks of small depth (called a period in this context). In
our previous paper entitled Faster 3-Periodic Merging Network we reduced twice
the time of merging on $3$-periodic networks, i.e. from $12\log N$ to $6\log
N$, compared to the first construction given by Kuty{\l}owski, Lory\'s and
Oesterdikhoff. Note that merging on $2$-periodic networks require linear time.
In this paper we extend our construction, which is based on Canfield and
Williamson $(\log N)$-periodic sorter, and the analysis from that paper to any
period $p \ge 4$. For $p\ge 4$ our $p$-periodic network merges two sorted
sequences of length $N/2$ in at most $\frac{2p}{p-2}\log N + p\frac{p-8}{p-2}$
rounds. The previous bound given by Kuty{\l}owski at al. was
$\frac{2.25p}{p-2.42}\log N$. That means, for example, that our $4$-periodic
merging networks work in time upper-bounded by $4\log N$ and our $6$-periodic
ones in time upper-bounded by $3\log N$ compared to the corresponding $5.67\log
N$ and $3.8\log N$ previous bounds. Our construction is regular and follows the
same periodification schema, whereas some additional techniques were used
previously to tune the construction for $p \ge 4$. Moreover, our networks are
also periodic sorters and tests on random permutations show that average
sorting time is closed to $\log^2 N$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1752</identifier>
 <datestamp>2014-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1752</id><created>2014-09-03</created><authors><author><keyname>Fouche</keyname><forenames>Willem L.</forenames></author></authors><title>Diophantine properties of Brownian motion: recursive aspects</title><categories>cs.LO</categories><comments>Appeared in: Logic, Computation, Hierarchies, (Brattka, Diener,
  Spreen (Eds)), Ontos Verlag, 2014, pp 139-156. arXiv admin note: substantial
  text overlap with arXiv:1409.1060</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We use recent results on the Fourier analysis of the zero sets of Brownian
motion to explore the diophantine properties of an algorithmically random
Brownian motion (also known as a complex oscillation). We discuss the
construction and definability of perfect sets which are linearly independent
over the rationals directly from Martin-L\&quot;of random reals. Finally we explore
the recent work of Tsirelson on countable dense sets to study the diophantine
properties of local minimisers of Brownian motion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1781</identifier>
 <datestamp>2015-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1781</id><created>2014-09-05</created><authors><author><keyname>Wilson</keyname><forenames>David</forenames></author><author><keyname>England</keyname><forenames>Matthew</forenames></author><author><keyname>Bradford</keyname><forenames>Russell</forenames></author><author><keyname>Davenport</keyname><forenames>James H.</forenames></author></authors><title>Using the distribution of cells by dimension in a cylindrical algebraic
  decomposition</title><categories>cs.SC</categories><comments>8 pages</comments><msc-class>68W30</msc-class><acm-class>I.1.2</acm-class><journal-ref>Proceedings of the 16th International Symposium on Symbolic and
  Numeric Algorithms for Scientific Computing (SYNASC '14), pp. 53--60. IEEE,
  2014</journal-ref><doi>10.1109/SYNASC.2014.15</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the distribution of cells by dimension in cylindrical
algebraic decompositions (CADs). We find that they follow a standard
distribution which seems largely independent of the underlying problem or CAD
algorithm used. Rather, the distribution is inherent to the cylindrical
structure and determined mostly by the number of variables.
  This insight is then combined with an algorithm that produces only
full-dimensional cells to give an accurate method of predicting the number of
cells in a complete CAD. Since constructing only full-dimensional cells is
relatively inexpensive (involving no costly algebraic number calculations) this
leads to heuristics for helping with various questions of problem formulation
for CAD, such as choosing an optimal variable ordering. Our experiments
demonstrate that this approach can be highly effective.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1786</identifier>
 <datestamp>2014-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1786</id><created>2014-09-05</created><updated>2014-09-30</updated><authors><author><keyname>Guo</keyname><forenames>Jin-Li</forenames></author></authors><title>Zero-determinant strategies in iterated multi-strategy games</title><categories>cs.GT physics.soc-ph q-fin.EC</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Self-serving, rational agents sometimes cooperate to their mutual benefit.
The two-player iterated prisoner's dilemma game is a model for including the
emergence of cooperation. It is generally believed that there is no simple
ultimatum strategy which a player can control the return of the other
participants. The recent discovery of the powerful class of zero-determinant
strategies in the iterated prisoner's dilemma dramatically expands our
understanding of the classic game by uncovering strategies that provide a
unilateral advantage to sentient players pitted against unwitting opponents.
However, strategies in the prisoner's dilemma game are only two strategies. Are
there these results for general multi-strategy games? To address this question,
the paper develops a theory for zero-determinant strategies for multi-strategy
games, with any number of strategies. The analytical results exhibit a similar
yet different scenario to the case of two-strategy games. Zero-determinant
strategies in iterated prisoner's dilemma can be seen as degenerate case of our
results. The results are also applied to the snowdrift game, the hawk-dove game
and the chicken game.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1789</identifier>
 <datestamp>2014-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1789</id><created>2014-09-05</created><authors><author><keyname>Huang</keyname><forenames>Gary B.</forenames></author><author><keyname>Plaza</keyname><forenames>Stephen</forenames></author></authors><title>Identifying Synapses Using Deep and Wide Multiscale Recursive Networks</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we propose a learning framework for identifying synapses using
a deep and wide multi-scale recursive (DAWMR) network, previously considered in
image segmentation applications. We apply this approach on electron microscopy
data from invertebrate fly brain tissue. By learning features directly from the
data, we are able to achieve considerable improvements over existing techniques
that rely on a small set of hand-designed features. We show that this system
can reduce the amount of manual annotation required, in both acquisition of
training data as well as verification of inferred detections.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1793</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1793</id><created>2014-09-05</created><updated>2015-05-16</updated><authors><author><keyname>Chen</keyname><forenames>Tao</forenames></author><author><keyname>Faniyi</keyname><forenames>Funmilade</forenames></author><author><keyname>Bahsoon</keyname><forenames>Rami</forenames></author><author><keyname>Lewis</keyname><forenames>Peter R.</forenames></author><author><keyname>Yao</keyname><forenames>Xin</forenames></author><author><keyname>Minku</keyname><forenames>Leandro L.</forenames></author><author><keyname>Esterle</keyname><forenames>Lukas</forenames></author></authors><title>The Handbook of Engineering Self-Aware and Self-Expressive Systems</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When faced with the task of designing and implementing a new self-aware and
self-expressive computing system, researchers and practitioners need a set of
guidelines on how to use the concepts and foundations developed in the
Engineering Proprioception in Computing Systems (EPiCS) project. This report
provides such guidelines on how to design self-aware and self-expressive
computing systems in a principled way. We have documented different categories
of self-awareness and self-expression level using architectural patterns. We
have also documented common architectural primitives, their possible candidate
techniques and attributes for architecting self-aware and self-expressive
systems. Drawing on the knowledge obtained from the previous investigations, we
proposed a pattern driven methodology for engineering self-aware and
self-expressive systems to assist in utilising the patterns and primitives
during design. The methodology contains detailed guidance to make decisions
with respect to the possible design alternatives, providing a systematic way to
build self-aware and self-expressive systems. Then, we qualitatively and
quantitatively evaluated the methodology using two case studies. The results
reveal that our pattern driven methodology covers the main aspects of
engineering self-aware and self-expressive systems, and that the resulted
systems perform significantly better than the non-self-aware systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1801</identifier>
 <datestamp>2014-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1801</id><created>2014-09-05</created><updated>2014-12-04</updated><authors><author><keyname>Plaza</keyname><forenames>Stephen M.</forenames></author><author><keyname>Parag</keyname><forenames>Toufiq</forenames></author><author><keyname>Huang</keyname><forenames>Gary B.</forenames></author><author><keyname>Olbris</keyname><forenames>Donald J.</forenames></author><author><keyname>Saunders</keyname><forenames>Mathew A.</forenames></author><author><keyname>Rivlin</keyname><forenames>Patricia K.</forenames></author></authors><title>Annotating Synapses in Large EM Datasets</title><categories>q-bio.QM cs.CV q-bio.NC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reconstructing neuronal circuits at the level of synapses is a central
problem in neuroscience and becoming a focus of the emerging field of
connectomics. To date, electron microscopy (EM) is the most proven technique
for identifying and quantifying synaptic connections. As advances in EM make
acquiring larger datasets possible, subsequent manual synapse identification
({\em i.e.}, proofreading) for deciphering a connectome becomes a major time
bottleneck. Here we introduce a large-scale, high-throughput, and
semi-automated methodology to efficiently identify synapses. We successfully
applied our methodology to the Drosophila medulla optic lobe, annotating many
more synapses than previous connectome efforts. Our approaches are extensible
and will make the often complicated process of synapse identification
accessible to a wider-community of potential proofreaders.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1805</identifier>
 <datestamp>2014-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1805</id><created>2014-09-05</created><updated>2014-09-28</updated><authors><author><keyname>Vestergaard</keyname><forenames>Christian L.</forenames></author><author><keyname>G&#xe9;nois</keyname><forenames>Mathieu</forenames></author><author><keyname>Barrat</keyname><forenames>Alain</forenames></author></authors><title>How memory generates heterogeneous dynamics in temporal networks</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI q-bio.PE</categories><journal-ref>Phys. Rev. E 90, 042805 (2014)</journal-ref><doi>10.1103/PhysRevE.90.042805</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Empirical temporal networks display strong heterogeneities in their dynamics,
which profoundly affect processes taking place on these networks, such as rumor
and epidemic spreading. Despite the recent wealth of data on temporal networks,
little work has been devoted to the understanding of how such heterogeneities
can emerge from microscopic mechanisms at the level of nodes and links. Here we
show that long-term memory effects are present in the creation and
disappearance of links in empirical networks. We thus consider a simple
generative modeling framework for temporal networks able to incorporate these
memory mechanisms. This allows us to study separately the role of each of these
mechanisms in the emergence of heterogeneous network dynamics. In particular,
we show analytically and numerically how heterogeneous distributions of contact
durations, of inter-contact durations and of numbers of contacts per link
emerge. We also study the individual effect of heterogeneities on dynamical
processes, such as the paradigmatic Susceptible-Infected epidemic spreading
model. Our results confirm in particular the crucial role of the distributions
of inter-contact durations and of the numbers of contacts per link.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1831</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1831</id><created>2014-09-05</created><authors><author><keyname>Grindrod</keyname><forenames>Peter</forenames></author><author><keyname>Higham</keyname><forenames>Desmond J.</forenames></author><author><keyname>MacKay</keyname><forenames>Robert S.</forenames></author></authors><title>Opportunities at the Mathematics/Future Cities Interface</title><categories>cs.CY cs.SI physics.data-an</categories><comments>A revised version of this document will appear in SIAM News (Society
  of Industrial and Applied Mathematics)</comments><report-no>University of Strathclyde Mathematics and Statistics Research Report
  10 (2014)</report-no><msc-class>05C82, 94C15, 65F50</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We make the case for mathematicians and statisticians to stake their claim in
the fast-moving and high-impact research field that is becoming known as Future
Cities. After assessing the Future Cities arena, we provide some illustrative
challenges where mathematical scientists can make an impact.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1846</identifier>
 <datestamp>2014-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1846</id><created>2014-09-05</created><authors><author><keyname>Kokalj-Filipovic</keyname><forenames>Silvija</forenames></author><author><keyname>Greenstein</keyname><forenames>Larry</forenames></author><author><keyname>Cheng</keyname><forenames>Bin</forenames></author><author><keyname>Gruteser</keyname><forenames>Marco</forenames></author></authors><title>V2V Propagation Modeling with Imperfect RSSI Samples</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe three in-field data collection efforts yielding a large database
of RSSI values vs. time or distance from vehicles communicating with each other
via DSRC. We show several data processing schemes we have devised to develop
Vehicle-to-Vehicle (V2V) propagation models from such data. The database is
limited in several important ways, not least, the presence of a high noise
floor that limits the distance over which good modeling is feasible. Another is
the presence of interference from multiple active transmitters. Our methodology
makes it possible to obtain, despite these limitations, accurate models of
median path loss vs. distance, shadow fading, and fast fading caused by
multipath. We aim not to develop a new V2V model, but to show the methods
enabling such a model to be obtained from in-field RSSI data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1879</identifier>
 <datestamp>2014-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1879</id><created>2014-08-26</created><authors><author><keyname>Chen</keyname><forenames>Pengfei</forenames></author><author><keyname>Qi</keyname><forenames>Yong</forenames></author><author><keyname>Hou</keyname><forenames>Di</forenames></author><author><keyname>Liu</keyname><forenames>Jiankang</forenames></author></authors><title>Bio-inspired Mechanism and Model Exploration of Software Aging</title><categories>cs.SE</categories><comments>41 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software systems situated in network environment may experience performance
degradation, availability decrease and even crash during long time running,
which is called software aging. This phenomenon has been studied for more than
15 years, but most of the literatures studied software as a black box, none of
them uncovered the fundamental and widely accepted mechanism of software aging
as far as we know. Through analyzing the characteristics between biological
aging and software aging, we ?nd some interesting common points and bridge the
gap between these two seemingly unrelated phenomena. The free radical aging
theory in biological studies is also applicative to explore the mechanism and
model of software aging. This paper ?nds an equivalent concept named `software
free radical' in software aging to free radical in biological aging. In our
study, the accumulation of `software free radical' is a root cause of software
aging. Using the free radical modeling methodology in biological aging, we give
a model for describing the kinetic of software aging based on feedback loops.
Although this paper doesn't give enough theoretical proof of the modeling
method, the practical results show that the feedback loop model can describe
the kinetic of software aging precisely. To further validate the aging
mechanism, we propose several software rejuvenation strategies focusing on
cleaning the `software free radical'. The results show that software aging can
be mitigated e?ectively by strengthening negative feedback loop or weakening
positive feedback loop. This paper is the ?rst try to answer the question `How
software ages' through interdisciplinary studies. Leveraging the conclusions in
this paper, people can design better software systems or keep their systems at
a high performance level during long time running.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1889</identifier>
 <datestamp>2015-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1889</id><created>2014-09-05</created><updated>2015-02-10</updated><authors><author><keyname>Vu</keyname><forenames>Thanh Long</forenames></author><author><keyname>Turitsyn</keyname><forenames>Konstantin</forenames></author></authors><title>Lyapunov Functions Family Approach to Transient Stability Assessment</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Analysis of transient stability of strongly nonlinear post-fault dynamics is
one of the most computationally challenging parts of Dynamic Security
Assessment. This paper proposes a novel approach for assessment of transient
stability of the system. The approach generalizes the idea of energy methods,
and extends the concept of energy function to a more general Lyapunov Functions
Family (LFF) constructed via Semi-Definite-Programming techniques. Unlike the
traditional energy function and its variations, the constructed Lyapunov
functions are proven to be decreasing only in a finite neighborhood of the
equilibrium point. However, we show that they can still certify stability of a
broader set of initial conditions in comparison to the traditional energy
function in the closest-UEP method. Moreover, the certificates of stability can
be constructed via a sequence of convex optimization problems that are
tractable even for large scale systems. We also propose specific algorithms for
adaptation of the Lyapunov functions to specific initial conditions and
demonstrate the effectiveness of the approach on a number of IEEE test cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1892</identifier>
 <datestamp>2014-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1892</id><created>2014-09-05</created><authors><author><keyname>Zhao</keyname><forenames>Ting</forenames></author><author><keyname>Plaza</keyname><forenames>Stephen M</forenames></author></authors><title>Automatic Neuron Type Identification by Neurite Localization in the
  Drosophila Medulla</title><categories>q-bio.NC cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mapping the connectivity of neurons in the brain (i.e., connectomics) is a
challenging problem due to both the number of connections in even the smallest
organisms and the nanometer resolution required to resolve them. Because of
this, previous connectomes contain only hundreds of neurons, such as in the
C.elegans connectome. Recent technological advances will unlock the mysteries
of increasingly large connectomes (or partial connectomes). However, the value
of these maps is limited by our ability to reason with this data and understand
any underlying motifs. To aid connectome analysis, we introduce algorithms to
cluster similarly-shaped neurons, where 3D neuronal shapes are represented as
skeletons. In particular, we propose a novel location-sensitive clustering
algorithm. We show clustering results on neurons reconstructed from the
Drosophila medulla that show high-accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1911</identifier>
 <datestamp>2014-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1911</id><created>2014-09-05</created><authors><author><keyname>Guevara</keyname><forenames>Miguel</forenames></author><author><keyname>Mendoza</keyname><forenames>Marcelo</forenames></author></authors><title>Revealing Comparative Advantages in the Backbone of Science</title><categories>cs.DL</categories><comments>In 2013 Computational Scientometrics Workshop, October 28, 2013, San
  Francisco, CA, USA, co-located with CIKM 13</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mapping Science across countries is a challenging task in the field of
Scientometrics. A number of efforts trying to cope with this task has been
discussed in the state of the art, addressing this challenge by processing
collections of scientific digital libraries and visualizing author-based
measures (for instance, the h-index) or document-based measures (for instance,
the averaged number of citations per document). A major drawback of these
approaches is related to the presence of bias. The bigger the country, the
higher the measure value. We explore the use of an econometric index to tackle
this limitation, known as the Revealed Comparative Advantage measure (RCA).
Using RCA, the diversity and ubiquity of each field of knowledge is mapped
across countries. Then, a RCA-based proximity function is explored to visualize
citation and h-index ubiquity. Science maps relating 27 knowledge areas and 237
countries are introduced using data crawled from Scimago that ranges from 1996
to 2011. Our results shows that the proposal is feasible and can be extended to
ellaborate a global scientific production characterization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1914</identifier>
 <datestamp>2014-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1914</id><created>2014-09-05</created><authors><author><keyname>Vasilache</keyname><forenames>Nicolas</forenames></author><author><keyname>Baskaran</keyname><forenames>Muthu</forenames></author><author><keyname>Henretty</keyname><forenames>Tom</forenames></author><author><keyname>Meister</keyname><forenames>Benoit</forenames></author><author><keyname>Langston</keyname><forenames>M. Harper</forenames></author><author><keyname>Tavarageri</keyname><forenames>Sanket</forenames></author><author><keyname>Lethin</keyname><forenames>Richard</forenames></author></authors><title>A Tale of Three Runtimes</title><categories>cs.DC cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This contribution discusses the automatic generation of event-driven,
tuple-space based programs for task-oriented execution models from a sequential
C specification. We developed a hierarchical mapping solution using
auto-parallelizing compiler technology to target three different runtimes
relying on event-driven tasks (EDTs). Our solution benefits from the important
observation that loop types encode short, transitive relations among EDTs that
are compact and efficiently evaluated at runtime. In this context, permutable
loops are of particular importance as they translate immediately into
conservative point-to-point synchronizations of distance 1. Our solution
generates calls into a runtime-agnostic C++ layer, which we have retargeted to
Intel's Concurrent Collections (CnC), ETI's SWARM, and the Open Community
Runtime (OCR). Experience with other runtime systems motivates our introduction
of support for hierarchical async-finishes in CnC. Experimental data is
provided to show the benefit of automatically generated code for EDT-based
runtimes as well as comparisons across runtimes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1917</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1917</id><created>2014-09-05</created><authors><author><keyname>Rana</keyname><forenames>Rajib</forenames></author><author><keyname>Kusy</keyname><forenames>Brano</forenames></author><author><keyname>Wall</keyname><forenames>Josh</forenames></author><author><keyname>Hu</keyname><forenames>Wen</forenames></author></authors><title>Novel Methods for Activity Classification and Occupany Prediction
  Enabling Fine-grained HVAC Control</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Much of the energy consumption in buildings is due to HVAC systems, which has
motivated several recent studies on making these systems more energy-
efficient. Occupancy and activity are two important aspects, which need to be
correctly estimated for optimal HVAC control. However, state-of-the-art methods
to estimate occupancy and classify activity require infrastructure and/or
wearable sensors which suffers from lower acceptability due to higher cost.
Encouragingly, with the advancement of the smartphones, these are becoming more
achievable. Most of the existing occupancy estimation tech- niques have the
underlying assumption that the phone is always carried by its user. However,
phones are often left at desk while attending meeting or other events, which
generates estimation error for the existing phone based occupancy algorithms.
Similarly, in the recent days the emerging theory of Sparse Random Classifier
(SRC) has been applied for activity classification on smartphone, however,
there are rooms to improve the on-phone process- ing. We propose a novel sensor
fusion method which offers almost 100% accuracy for occupancy estimation. We
also propose an activity classifica- tion algorithm, which offers similar
accuracy as of the state-of-the-art SRC algorithms while offering 50% reduction
in processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1976</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1976</id><created>2014-09-05</created><authors><author><keyname>Zhou</keyname><forenames>Quan</forenames></author><author><keyname>Chen</keyname><forenames>Wenlin</forenames></author><author><keyname>Song</keyname><forenames>Shiji</forenames></author><author><keyname>Gardner</keyname><forenames>Jacob R.</forenames></author><author><keyname>Weinberger</keyname><forenames>Kilian Q.</forenames></author><author><keyname>Chen</keyname><forenames>Yixin</forenames></author></authors><title>A Reduction of the Elastic Net to Support Vector Machines with an
  Application to GPU Computing</title><categories>stat.ML cs.LG</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The past years have witnessed many dedicated open-source projects that built
and maintain implementations of Support Vector Machines (SVM), parallelized for
GPU, multi-core CPUs and distributed systems. Up to this point, no comparable
effort has been made to parallelize the Elastic Net, despite its popularity in
many high impact applications, including genetics, neuroscience and systems
biology. The first contribution in this paper is of theoretical nature. We
establish a tight link between two seemingly different algorithms and prove
that Elastic Net regression can be reduced to SVM with squared hinge loss
classification. Our second contribution is to derive a practical algorithm
based on this reduction. The reduction enables us to utilize prior efforts in
speeding up and parallelizing SVMs to obtain a highly optimized and parallel
solver for the Elastic Net and Lasso. With a simple wrapper, consisting of only
11 lines of MATLAB code, we obtain an Elastic Net implementation that naturally
utilizes GPU and multi-core CPUs. We demonstrate on twelve real world data
sets, that our algorithm yields identical results as the popular (and highly
optimized) glmnet implementation but is one or several orders of magnitude
faster.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1980</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1980</id><created>2014-09-05</created><authors><author><keyname>Bhatnagar</keyname><forenames>Manav R.</forenames></author></authors><title>On the Sum of Correlated Squared $\kappa-\mu$ Shadowed Random Variables
  and its Application to Performance Analysis of MRC</title><categories>cs.IT math.IT</categories><comments>IEEE Transactions on Vehicular Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the statistical characterization of the sum of the
squared $\kappa-\mu$ shadowed random variables with correlated shadowing
components. The probability density function (PDF) of this sum is obtained in
the form of a power series. The derived PDF is utilized for obtaining the
performance results of the maximal ratio combining (MRC) scheme over correlated
$\kappa-\mu$ shadowed fading channels. First, we derive the moment generating
function (MGF) of the received signal-to-noise ratio of the MRC receiver. By
using the derived MGF expression, the analytical diversity order is obtained;
it is deduced on the basis of this analysis that the diversity of the MRC
receiver over correlated $\kappa-\mu$ shadowed channels depends upon the number
of diversity branches and $\mu$ parameter. Further, the analytical average bit
error rate of the MRC scheme is also derived, which is applicable for $M$-PSK
and $M$-QAM constellations. The Shannon capacity of the correlated $\kappa-\mu$
shadowed channels is also derived in the form of the Meijer-G function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1981</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1981</id><created>2014-09-06</created><authors><author><keyname>GP</keyname><forenames>Ramesh.</forenames></author><author><keyname>CV</keyname><forenames>Aravind.</forenames></author><author><keyname>R</keyname><forenames>Rajparthiban.</forenames></author><author><keyname>Soysa</keyname><forenames>N.</forenames></author></authors><title>A Body Area Network through Wireless Technology</title><categories>cs.HC cs.CY</categories><comments>6 Pages, 6 Figures, 1 table</comments><journal-ref>International Journal of Computer Science and Engineering
  Communications, Volume 2,2014,pp 432-437</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  A physiological signal monitoring system and alerting system using wireless
technology is presented. The two types of physiological signal monitoring are
captured from the body through leads and using the radio-frequency transmitting
and receiving module the data are interfaced to computer systems. Furthering
using a developed user interface module the captured signals are analyzed for
checking abnormality. Any significant recordings are transmitted to the
physicians hand phone by using external serial SMS modem. ECG signal de-noising
is conducted by using low-pass and high-pass filters. EEG signals de-noising is
conducted by using band-pass filters set. A comparative evaluation of the
module with the manual recording shows encouraging results. The ECG and EEG
pattern are presented in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1987</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1987</id><created>2014-09-06</created><authors><author><keyname>Das</keyname><forenames>Kalyani</forenames></author></authors><title>Complexity to Find Wiener Index of Some Graphs</title><categories>cs.DM</categories><comments>6 pages</comments><msc-class>05C12, 05C85</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Wiener index is one of the oldest graph parameter which is used to study
molecular-graph-based structure. This parameter was first proposed by Harold
Wiener in 1947 to determining the boiling point of paraffin. The Wiener index
of a molecular graph measures the compactness of the underlying molecule. This
parameter is wide studied area for molecular chemistry. It is used to study the
physio-chemical properties of the underlying organic compounds. The Wiener
index of a connected graph is denoted by W(G) and is defined as, that is W(G)
is the sum of distances between all pairs (ordered) of vertices of G. In this
paper, we give the algorithmic idea to find the Wiener index of some graphs,
like cactus graphs and intersection graphs, viz. interval, circular-arc,
permutation, trapezoid graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.1989</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.1989</id><created>2014-09-06</created><authors><author><keyname>Jin</keyname><forenames>Wei</forenames></author><author><keyname>Orso</keyname><forenames>Alessandro</forenames></author></authors><title>Improving Efficiency and Scalability of Formula-based Debugging</title><categories>cs.SE</categories><comments>12 pages</comments><acm-class>D.2.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Formula-based debugging techniques are becoming increasingly popular, as they
provide a principled way to identify potentially faulty statements together
with information that can help fix such statements. Although effective, these
approaches are computationally expensive, which limits their practical
applicability. Moreover, they tend to focus on failing test cases alone, thus
ignoring the wealth of information provided by passing tests. To mitigate these
issues, we propose two techniques: on-demand formula computation (OFC) and
clause weighting (CW). OFC improves the overall efficiency of formula-based
debugging by exploring all and only the parts of a program that are relevant to
a failure. CW improves the accuracy of formula-based debugging by leveraging
statistical fault-localization information that accounts for passing tests. Our
empirical results show that both techniques are effective and can improve the
state of the art in formula-based debugging.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2002</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2002</id><created>2014-09-06</created><authors><author><keyname>Babakhani</keyname><forenames>Saba</forenames></author><author><keyname>Mozaffari</keyname><forenames>Niloofar</forenames></author><author><keyname>Hamzeh</keyname><forenames>Ali</forenames></author></authors><title>A Martingale Approach to Detect Peak of News in Social Network</title><categories>cs.SI physics.soc-ph</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Nowadays, social medias such as Twitter, Memetracker and Blogs have become
powerful tools to propagate information. They facilitate quick dissemination
sequence of information such as news article, blog posts, user's interests and
thoughts through large scale. Providing strong means to analyzing social
networks structure and how information diffuse through them is essential. Many
recent studies emphasize on modeling information diffusion and their patterns
to gain some useful knowledge. In this paper, we propose a statistical approach
to online detect peak points of news when spread over social networks, to the
best of our knowledge has never investigated before. The proposed model use
martingale approach to predict peak points when news reached the peak of its
popularity. Experimental results on real datasets show good performance of our
approach to online detect these peak points.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2003</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2003</id><created>2014-09-06</created><authors><author><keyname>Vorel</keyname><forenames>Vojt&#x11b;ch</forenames></author></authors><title>Complexity of a Problem Concerning Reset Words for Eulerian Binary
  Automata</title><categories>cs.FL</categories><comments>Extended version of a paper presented at LATA 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A word is called a reset word for a deterministic finite automaton if it maps
all the states of the automaton to a unique state. Deciding about the existence
of a reset word of a given maximum length for a given automaton is known to be
an NP-complete problem. We prove that it remains NP-complete even if restricted
to Eulerian automata with binary alphabets, as it has been conjectured by
Martyugin (2011).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2008</identifier>
 <datestamp>2015-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2008</id><created>2014-09-06</created><updated>2015-01-25</updated><authors><author><keyname>Rohe</keyname><forenames>Klaus</forenames></author></authors><title>Computing the coefficients for the power series solution of the
  Lane-Emden equation with the Python library SymPy</title><categories>cs.MS</categories><comments>14 pages, 4 figures, 2 source code listings</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  It is shown how the Python library Sympy can be used to compute symbolically
the coefficients of the power series solution of the Lane-Emden equation (LEE).
Sympy is an open source Python library for symbolic mathematics. The power
series solutions are compared to the numerically computed solutions using
matplotlib. The results of a run time measurement of the implemented algorithm
are discussed at the end.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2013</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2013</id><created>2014-09-06</created><updated>2015-08-18</updated><authors><author><keyname>Altarelli</keyname><forenames>F.</forenames></author><author><keyname>Braunstein</keyname><forenames>A.</forenames></author><author><keyname>Dall'Asta</keyname><forenames>L.</forenames></author></authors><title>Statics and dynamics of selfish interactions in distributed service
  systems</title><categories>cs.GT cond-mat.dis-nn cs.MA physics.soc-ph</categories><comments>30 pages, 10 figures</comments><journal-ref>PLoS ONE 10(7): e0119286 (2015)</journal-ref><doi>10.1371/journal.pone.0119286</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a class of games which model the competition among agents to access
some service provided by distributed service units and which exhibit congestion
and frustration phenomena when service units have limited capacity. We propose
a technique, based on the cavity method of statistical physics, to characterize
the full spectrum of Nash equilibria of the game. The analysis reveals a large
variety of equilibria, with very different statistical properties. Natural
selfish dynamics, such as best-response, usually tend to large-utility
equilibria, even though those of smaller utility are exponentially more
numerous. Interestingly, the latter actually can be reached by selecting the
initial conditions of the best-response dynamics close to the saturation limit
of the service unit capacities. We also study a more realistic stochastic
variant of the game by means of a simple and effective approximation of the
average over the random parameters, showing that the properties of the
average-case Nash equilibria are qualitatively similar to the deterministic
ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2017</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2017</id><created>2014-09-06</created><authors><author><keyname>Zareh</keyname><forenames>Mehran</forenames></author><author><keyname>Dimarogonas</keyname><forenames>Dimos V.</forenames></author><author><keyname>Franceschelli</keyname><forenames>Mauro</forenames></author><author><keyname>Johansson</keyname><forenames>Karl Henrik</forenames></author><author><keyname>Seatzu</keyname><forenames>Carla</forenames></author></authors><title>Consensus in multi-agent systems with non-periodic sampled-data exchange
  and uncertain network topology</title><categories>cs.SY</categories><comments>arXiv admin note: substantial text overlap with arXiv:1407.3006</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper consensus in second-order multi-agent systems with a
non-periodic sampled-data exchange among agents is investigated. The sampling
is random with bounded inter-sampling intervals. It is assumed that each agent
has exact knowledge of its own state at any time instant. The considered local
interaction rule is PD-type. Sufficient conditions for stability of the
consensus protocol to a time-invariant value are derived based on LMIs. Such
conditions only require the knowledge of the connectivity of the graph modeling
the network topology. Numerical simulations are presented to corroborate the
theoretical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2019</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2019</id><created>2014-09-06</created><authors><author><keyname>Chen</keyname><forenames>Chao</forenames></author></authors><title>An ontology-based approach to the optimization of non-binary
  (2,v)-regular LDPC codes</title><categories>cs.IT math.IT</categories><comments>Technical Report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A non-binary (2,v)-regular LDPC code is defined by a parity-check matrix with
column weight 2 and row weight v. In this report, we give an ontology-based
approach to the optimization for this class of codes. All possible
inter-connected cycle patterns that lead to low symbol-weight codewords are
identified to put together the ontology. The optimization goal is to improve
the distance property of equivalent binary images. Using the proposed method,
the estimation and optimization of bit-distance spectrum becomes easily
handleable. Three codes in the CCSDS recommendation are analyzed and several
codes with good minimum bit-distance are designed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2030</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2030</id><created>2014-09-06</created><authors><author><keyname>Andreev</keyname><forenames>Fedor</forenames></author><author><keyname>Kalantari</keyname><forenames>Bahman</forenames></author></authors><title>Algorithms and Polynomiography for Solving Quaternion Quadratic
  Equations</title><categories>cs.NA</categories><comments>17 pages, 6 figures, 15 images, 1 table</comments><msc-class>65Y20, 65D18, 65D99, 65D99, 97A30</msc-class><acm-class>I.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Solving a quadratic equation $P(x)=ax^2+bx+c=0$ with real coefficients is
known to middle school students. Solving the equation over the quaternions is
not straightforward. Huang and So \cite{Huang} give a complete set of formulas,
breaking it into several cases depending on the coefficients. From a result of
the second author in \cite{kalQ}, zeros of $P(x)$ can be expressed in terms of
the zeros of a real quartic equation. This drastically simplifies solving a
quadratic equation. Here we also consider solving $P(x)=0$ iteratively via
Newton and Halley methods developed in \cite{kalQ}. We prove a property of the
Jacobian of Newton and Halley methods and describe several 2D polynomiography
based on these methods. The images not only encode the outcome of the iterative
process, but by measuring the time taken to render them we find the relative
speed of convergence for the methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2042</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2042</id><created>2014-09-06</created><authors><author><keyname>Antikacioglu</keyname><forenames>Arda</forenames></author><author><keyname>Ravi</keyname><forenames>R.</forenames></author><author><keyname>Srihdar</keyname><forenames>Srinath</forenames></author></authors><title>Recommendation Subgraphs for Web Discovery</title><categories>cs.IR cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recommendations are central to the utility of many websites including
YouTube, Quora as well as popular e-commerce stores. Such sites typically
contain a set of recommendations on every product page that enables visitors to
easily navigate the website. Choosing an appropriate set of recommendations at
each page is one of the key features of backend engines that have been deployed
at several e-commerce sites.
  Specifically at BloomReach, an engine consisting of several independent
components analyzes and optimizes its clients' websites. This paper focuses on
the structure optimizer component which improves the website navigation
experience that enables the discovery of novel content.
  We begin by formalizing the concept of recommendations used for discovery. We
formulate this as a natural graph optimization problem which in its simplest
case, reduces to a bipartite matching problem. In practice, solving these
matching problems requires superlinear time and is not scalable. Also,
implementing simple algorithms is critical in practice because they are
significantly easier to maintain in production. This motivated us to analyze
three methods for solving the problem in increasing order of sophistication: a
sampling algorithm, a greedy algorithm and a more involved partitioning based
algorithm.
  We first theoretically analyze the performance of these three methods on
random graph models characterizing when each method will yield a solution of
sufficient quality and the parameter ranges when more sophistication is needed.
We complement this by providing an empirical analysis of these algorithms on
simulated and real-world production data. Our results confirm that it is not
always necessary to implement complicated algorithms in the real-world and that
very good practical results can be obtained by using heuristics that are backed
by the confidence of concrete theoretical guarantees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2045</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2045</id><created>2014-09-06</created><authors><author><keyname>Mokhtari</keyname><forenames>Aryan</forenames></author><author><keyname>Ribeiro</keyname><forenames>Alejandro</forenames></author></authors><title>Global Convergence of Online Limited Memory BFGS</title><categories>math.OC cs.LG stat.ML</categories><comments>37 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Global convergence of an online (stochastic) limited memory version of the
Broyden-Fletcher- Goldfarb-Shanno (BFGS) quasi-Newton method for solving
optimization problems with stochastic objectives that arise in large scale
machine learning is established. Lower and upper bounds on the Hessian
eigenvalues of the sample functions are shown to suffice to guarantee that the
curvature approximation matrices have bounded determinants and traces, which,
in turn, permits establishing convergence to optimal arguments with probability
1. Numerical experiments on support vector machines with synthetic data
showcase reductions in convergence time relative to stochastic gradient descent
algorithms as well as reductions in storage and computation relative to other
online quasi-Newton methods. Experimental evaluation on a search engine
advertising problem corroborates that these advantages also manifest in
practical applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2048</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2048</id><created>2014-09-06</created><authors><author><keyname>Jouneghani</keyname><forenames>Farzad Ghafari</forenames></author><author><keyname>Babazadeh</keyname><forenames>Mohammad</forenames></author><author><keyname>Salami</keyname><forenames>Davoud</forenames></author><author><keyname>Movla</keyname><forenames>Hossein</forenames></author></authors><title>Quantum Belief Propagation Algorithm versus Suzuki-Trotter approach in
  the one-dimensional Heisenberg chains</title><categories>quant-ph cs.IT math.IT</categories><comments>11 pages- 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantum systems are the future candidates for computers and information
processing devices. Information about quantum states and processes may be
incomplete and scattered in these systems. We use a quantum version of Belief
Propagation(BP) Algorithm to integrate the distributed information. In this
algorithm the distributed information, which is in the form of density matrix,
can be approximated to local structures. The validity of this algorithm is
measured in comparison with Suzuki-Trotter(ST) method, using simulated
information. ST in 3-body Heisenberg example gives a more accurate answer,
however Quantum Belief Propagation (QBP) runs faster based on complexity. In
order to develop it in the future, we should be looking for ways to increase
the accuracy of QBP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2050</identifier>
 <datestamp>2015-03-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2050</id><created>2014-09-06</created><authors><author><keyname>Czarnuch</keyname><forenames>Stephen</forenames></author><author><keyname>Mihailidis</keyname><forenames>Alex</forenames></author></authors><title>Depth image hand tracking from an overhead perspective using partially
  labeled, unbalanced data: Development and real-world testing</title><categories>cs.CV</categories><comments>9 pages, 1 figure</comments><doi>10.3109/17483107.2015.1027304</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the development and evaluation of a hand tracking algorithm based
on single depth images captured from an overhead perspective for use in the
COACH prompting system. We train a random decision forest body part classifier
using approximately 5,000 manually labeled, unbalanced, partially labeled
training images. The classifier represents a random subset of pixels in each
depth image with a learned probability density function across all trained body
parts. A local mode-find approach is used to search for clusters present in the
underlying feature space sampled by the classified pixels. In each frame, body
part positions are chosen as the mode with the highest confidence. User hand
positions are translated into hand washing task actions based on proximity to
environmental objects. We validate the performance of the classifier and task
action proposals on a large set of approximately 24,000 manually labeled
images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2056</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2056</id><created>2014-09-06</created><authors><author><keyname>Kalantari</keyname><forenames>Bahman</forenames></author></authors><title>A One-Line Proof of the Fundamental Theorem of Algebra with Newton's
  Method as a Consequence</title><categories>cs.NA cs.MA</categories><comments>3 pages</comments><msc-class>65Y20, 65D18, 65D99, 65D99, 97A30</msc-class><acm-class>I.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many proofs of the fundamental theorem of algebra rely on the fact that the
minimum of the modulus of a complex polynomial over the complex plane is
attained at some complex number. The proof then follows by arguing the minimum
value is zero. This can be done by proving that at any complex number that is
not a zero of the polynomial we can exhibit a direction of descent for the
modulus. In this note we present a very short and simple proof of the existence
of such descent direction. In particular, our descent direction gives rise to
Newton's method for solving a polynomial equation via modulus minimization and
also makes the iterates definable at any critical point.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2064</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2064</id><created>2014-09-06</created><authors><author><keyname>L&#xe9;vesque</keyname><forenames>Martin</forenames></author></authors><title>Rethinking the Contention Resolution Mechanism in WiMAX Networks using
  Lattice Correlators for Improved Smart Grid Communication Performance</title><categories>cs.NI</categories><comments>9 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  By using experimental measurements of smart grid applications compliant with
IEC 61850 in trace-driven WiMAX simulations, we show that the WiMAX MAC
protocol efficiency decreases as a function of the number of stations. To avoid
this shortcoming, we propose and analyze a novel WiMAX MAC protocol for smart
grid applications, which uses lattice correlators to improve the
throughput-delay performance significantly. For the considered configurations,
the obtained maximum throughput of the proposed MAC protocol outperforms the
current WiMAX MAC protocol by up to 41%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2073</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2073</id><created>2014-09-06</created><authors><author><keyname>Kortkamp</keyname><forenames>Tobias</forenames></author></authors><title>An NLP Assistant for Clide</title><categories>cs.CL</categories><comments>Bachelor Report</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This report describes an NLP assistant for the collaborative development
environment Clide, that supports the development of NLP applications by
providing easy access to some common NLP data structures. The assistant
visualizes text fragments and their dependencies by displaying the semantic
graph of a sentence, the coreference chain of a paragraph and mined triples
that are extracted from a paragraph's semantic graphs and linked using its
coreference chain. Using this information and a logic programming library, we
create an NLP database which is used by a series of queries to mine the
triples. The algorithm is tested by translating a natural language text
describing a graph to an actual graph that is shown as an annotation in the
text editor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2080</identifier>
 <datestamp>2015-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2080</id><created>2014-09-07</created><updated>2015-01-26</updated><authors><author><keyname>Bellec</keyname><forenames>P.</forenames></author><author><keyname>Benhajali</keyname><forenames>Y.</forenames></author><author><keyname>Carbonell</keyname><forenames>F.</forenames></author><author><keyname>Dansereau</keyname><forenames>C.</forenames></author><author><keyname>Albouy</keyname><forenames>G.</forenames></author><author><keyname>Pelland</keyname><forenames>M.</forenames></author><author><keyname>Craddock</keyname><forenames>C.</forenames></author><author><keyname>Collignon</keyname><forenames>O.</forenames></author><author><keyname>Doyon</keyname><forenames>J.</forenames></author><author><keyname>Stip</keyname><forenames>E.</forenames></author><author><keyname>Orban</keyname><forenames>P.</forenames></author></authors><title>Multiscale statistical testing for connectome-wide association studies
  in fMRI</title><categories>q-bio.QM cs.CV stat.AP</categories><comments>54 pages, 12 main figures, 1 main table, 10 supplementary figures, 1
  supplementary table</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Alterations in brain connectivity have been associated with a variety of
clinical disorders using functional magnetic resonance imaging (fMRI). We
investigated empirically how the number of brain parcels (or scale) impacted
the results of a mass univariate general linear model (GLM) on connectomes. The
brain parcels used as nodes in the connectome analysis were functionnally
defined by a group cluster analysis. We first validated that a classic
Benjamini-Hochberg procedure with parametric GLM tests did control
appropriately the false-discovery rate (FDR) at a given scale. We then observed
on realistic simulations that there was no substantial inflation of the FDR
across scales, as long as the FDR was controlled independently within each
scale, and the presence of true associations could be established using an
omnibus permutation test combining all scales. Second, we observed both on
simulations and on three real resting-state fMRI datasets (schizophrenia,
congenital blindness, motor practice) that the rate of discovery varied
markedly as a function of scales, and was relatively higher for low scales,
below 25. Despite the differences in discovery rate, the statistical maps
derived at different scales were generally very consistent in the three real
datasets. Some seeds still showed effects better observed around 50,
illustrating the potential benefits of multiscale analysis. On real data, the
statistical maps agreed well with the existing literature. Overall, our results
support that the multiscale GLM connectome analysis with FDR is statistically
valid and can capture biologically meaningful effects in a variety of
experimental conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2081</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2081</id><created>2014-09-07</created><authors><author><keyname>Ye</keyname><forenames>Juntao</forenames></author></authors><title>History-free Collision Response for Deformable Surfaces</title><categories>cs.GR</categories><comments>originally 4 pages, submitted as Technical Brief to Siggraph Asia
  2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Continuous collision detection (CCD) and response methods are widely adopted
in dynamics simulation of deformable models. They are history-based, as their
success is strictly based on an assumption of a collision-free state at the
start of each time interval. On the other hand, in many applications surfaces
have normals defined to designate their orientation (i.e. front- and
back-face), yet CCD methods are totally blind to such orientation
identification (thus are orientation-free). We notice that if such information
is utilized, many penetrations can be untangled. In this paper we present a
history-free method for separation of two penetrating meshes, where at least
one of them has clarified surface orientation. This method first computes all
edge-face (E-F) intersections with discrete collision detection (DCD), and then
builds a number of penetration stencils. On response, the stencil vertices are
relocated into a penetration-free state, via a global displacement minimizer.
Our method is very effective for handling penetration between two meshes, being
it an initial configuration or in the middle of physics simulation. The major
limitation is that it is not applicable to self-collision within one mesh at
the time being.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2088</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2088</id><created>2014-09-07</created><authors><author><keyname>Kruse</keyname><forenames>Michael</forenames><affiliation>LRI, INRIA Saclay - Ile de France</affiliation></author></authors><title>Introducing Molly: Distributed Memory Parallelization with LLVM</title><categories>cs.PL cs.DC</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Programming for distributed memory machines has always been a tedious task,
but necessary because compilers have not been sufficiently able to optimize for
such machines themselves. Molly is an extension to the LLVM compiler toolchain
that is able to distribute and reorganize workload and data if the program is
organized in statically determined loop control-flows. These are represented as
polyhedral integer-point sets that allow program transformations applied on
them. Memory distribution and layout can be declared by the programmer as
needed and the necessary asynchronous MPI communication is generated
automatically. The primary motivation is to run Lattice QCD simulations on IBM
Blue Gene/Q supercomputers, but since the implementation is not yet completed,
this paper shows the capabilities on Conway's Game of Life.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2089</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2089</id><created>2014-09-07</created><authors><author><keyname>Kruse</keyname><forenames>Michael</forenames><affiliation>LRI, INRIA Saclay - Ile de France</affiliation></author></authors><title>Perfrewrite -- Program Complexity Analysis via Source Code
  Instrumentation</title><categories>cs.PL</categories><comments>ACACES 2012 summer school (2012)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most program profiling methods output the execution time of one specific
program execution, but not its computational complexity class in terms of the
big-O notation. Perfrewrite is a tool based on LLVM's Clang compiler to rewrite
a program such that it tracks semantic information while the program executes
and uses it to guess memory usage, communication and computational complexity.
While source code instrumentation is a standard technique for profiling, using
it for deriving formulas is an uncommon approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2095</identifier>
 <datestamp>2016-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2095</id><created>2014-09-07</created><updated>2016-01-04</updated><authors><author><keyname>Jeong</keyname><forenames>Seongah</forenames></author><author><keyname>Simeone</keyname><forenames>Osvaldo</forenames></author><author><keyname>Haimovich</keyname><forenames>Alexander</forenames></author><author><keyname>Kang</keyname><forenames>Joonhyuk</forenames></author></authors><title>Optimal Fronthaul Quantization for Cloud Radio Positioning</title><categories>cs.IT math.IT</categories><comments>17 pages, 5 figures, 1 table, IEEE Transactions on Vehicular
  Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless positioning systems that are implemented by means of a Cloud Radio
Access Networks (C-RANs) may provide cost-effective solutions, particularly for
indoor localization. In a C-RAN, the baseband processing, including
localization, is carried out at a centralized control unit (CU) based on
quantized baseband signals received from the RUs over finite-capacity fronthaul
links. In this paper, the problem of maximizing the localization accuracy over
fronthaul quantization/compression is formulated by adopting the Cram\'{e}r-Rao
bound (CRB) on the localization accuracy as the performance metric of interest
and information-theoretic bounds on the compression rate. The analysis
explicitly accounts for the uncertainty of parameters at the CU via a robust,
or worst-case, optimization formulation. The proposed algorithm leverages the
Charnes-Cooper transformation and Difference-of-Convex (DC) programming, and is
validated via numerical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2097</identifier>
 <datestamp>2014-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2097</id><created>2014-09-07</created><updated>2014-10-04</updated><authors><author><keyname>De Cristofaro</keyname><forenames>Emiliano</forenames></author><author><keyname>Friedman</keyname><forenames>Arik</forenames></author><author><keyname>Jourjon</keyname><forenames>Guillaume</forenames></author><author><keyname>Kaafar</keyname><forenames>Mohamed Ali</forenames></author><author><keyname>Shafiq</keyname><forenames>M. Zubair</forenames></author></authors><title>Paying for Likes? Understanding Facebook Like Fraud Using Honeypots</title><categories>cs.SI cs.CR physics.soc-ph</categories><comments>To appear in IMC 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Facebook pages offer an easy way to reach out to a very large audience as
they can easily be promoted using Facebook's advertising platform. Recently,
the number of likes of a Facebook page has become a measure of its popularity
and profitability, and an underground market of services boosting page likes,
aka like farms, has emerged. Some reports have suggested that like farms use a
network of profiles that also like other pages to elude fraud protection
algorithms, however, to the best of our knowledge, there has been no systematic
analysis of Facebook pages' promotion methods.
  This paper presents a comparative measurement study of page likes garnered
via Facebook ads and by a few like farms. We deploy a set of honeypot pages,
promote them using both methods, and analyze garnered likes based on likers'
demographic, temporal, and social characteristics. We highlight a few
interesting findings, including that some farms seem to be operated by bots and
do not really try to hide the nature of their operations, while others follow a
stealthier approach, mimicking regular users' behavior.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2100</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2100</id><created>2014-09-07</created><authors><author><keyname>Emadi</keyname><forenames>Mohammad Javad</forenames></author><author><keyname>Khormuji</keyname><forenames>Majid Nasiri</forenames></author><author><keyname>Skoglund</keyname><forenames>Mikael</forenames></author><author><keyname>Aref</keyname><forenames>Mohammad Reza</forenames></author></authors><title>Multi layer Gelfand Pinsker Strategies for the Generalized Multiple
  Access Channel</title><categories>cs.IT math.IT</categories><comments>26 pages, 8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a two-user state-dependent generalized multiple-access channel
(GMAC) with correlated states. It is assumed that each encoder has
\emph{noncausal} access to channel state information (CSI). We develop an
achievable rate region by employing rate-splitting, block Markov encoding,
Gelfand--Pinsker multicoding, superposition coding and joint typicality
decoding. In the proposed scheme, the encoders use a partial decoding strategy
to collaborate in the next block, and the receiver uses a backward decoding
strategy with joint unique decoding at each stage. Our achievable rate region
includes several previously known regions proposed in the literature for
different scenarios of multiple-access and relay channels. Then, we consider
two Gaussian GMACs with additive interference. In the first model, we assume
that the interference is known noncausally at both of the encoders and
construct a multi-layer Costa precoding scheme that removes \emph{completely}
the effect of the interference. In the second model, we consider a doubly dirty
Gaussian GMAC in which each of interferences is known noncausally only at one
encoder. We derive an inner bound and analyze the achievable rate region for
the latter model and interestingly prove that if one of the encoders knows the
full CSI, there exists an achievable rate region which is \emph{independent} of
the power of interference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2104</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2104</id><created>2014-09-07</created><authors><author><keyname>Luo</keyname><forenames>Lei</forenames></author><author><keyname>Shen</keyname><forenames>Chunhua</forenames></author><author><keyname>Liu</keyname><forenames>Xinwang</forenames></author><author><keyname>Zhang</keyname><forenames>Chunyuan</forenames></author></authors><title>A Computational Model of the Short-Cut Rule for 2D Shape Decomposition</title><categories>cs.CV</categories><comments>11 pages</comments><doi>10.1109/TIP.2014.2376188</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new 2D shape decomposition method based on the short-cut rule.
The short-cut rule originates from cognition research, and states that the
human visual system prefers to partition an object into parts using the
shortest possible cuts. We propose and implement a computational model for the
short-cut rule and apply it to the problem of shape decomposition. The model we
proposed generates a set of cut hypotheses passing through the points on the
silhouette which represent the negative minima of curvature. We then show that
most part-cut hypotheses can be eliminated by analysis of local properties of
each. Finally, the remaining hypotheses are evaluated in ascending length
order, which guarantees that of any pair of conflicting cuts only the shortest
will be accepted. We demonstrate that, compared with state-of-the-art shape
decomposition methods, the proposed approach achieves decomposition results
which better correspond to human intuition as revealed in psychological
experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2112</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2112</id><created>2014-09-07</created><authors><author><keyname>Alfalayleh</keyname><forenames>Mousa</forenames></author><author><keyname>Brankovic</keyname><forenames>Ljiljana</forenames></author></authors><title>Quantifying Privacy: A Novel Entropy-Based Measure of Disclosure Risk</title><categories>cs.CR</categories><comments>20 pages, 4 figures</comments><msc-class>68P99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well recognised that data mining and statistical analysis pose a
serious treat to privacy. This is true for financial, medical, criminal and
marketing research. Numerous techniques have been proposed to protect privacy,
including restriction and data modification. Recently proposed privacy models
such as differential privacy and k-anonymity received a lot of attention and
for the latter there are now several improvements of the original scheme, each
removing some security shortcomings of the previous one. However, the challenge
lies in evaluating and comparing privacy provided by various techniques. In
this paper we propose a novel entropy based security measure that can be
applied to any generalisation, restriction or data modification technique. We
use our measure to empirically evaluate and compare a few popular methods,
namely query restriction, sampling and noise addition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2116</identifier>
 <datestamp>2015-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2116</id><created>2014-09-07</created><updated>2015-02-04</updated><authors><author><keyname>D'Argenio</keyname><forenames>Pedro</forenames></author><author><keyname>Legay</keyname><forenames>Axel</forenames></author><author><keyname>Sedwards</keyname><forenames>Sean</forenames></author><author><keyname>Traonouez</keyname><forenames>Louis-Marie</forenames></author></authors><title>Smart Sampling for Lightweight Verification of Markov Decision Processes</title><categories>cs.DS</categories><comments>IEEE conference style, 11 pages, 5 algorithms, 11 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Markov decision processes (MDP) are useful to model optimisation problems in
concurrent systems. To verify MDPs with efficient Monte Carlo techniques
requires that their nondeterminism be resolved by a scheduler. Recent work has
introduced the elements of lightweight techniques to sample directly from
scheduler space, but finding optimal schedulers by simple sampling may be
inefficient. Here we describe &quot;smart&quot; sampling algorithms that can make
substantial improvements in performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2119</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2119</id><created>2014-09-07</created><authors><author><keyname>Ugrinovskii</keyname><forenames>V.</forenames></author></authors><title>Detectability of distributed consensus-based observer networks: An
  elementary analysis and extensions</title><categories>cs.SY</categories><comments>Accepted for presentation at the 2014 Australian Control Conference,
  Canberra Australia, Nov 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper continues the study of local detectability and observability
requirements on components of distributed observers networks to ensure
detectability properties of the network. First, we present a sketch of an
elementary proof of the known result equating the multiplicity of the zero
eigenvalue of the Laplace matrix of a digraph to the number of its maximal
reachable subgraphs. Unlike the existing algebraic proof, we use a direct
analysis of the graph topology. This result is then used in the second part of
the paper to extend our previous results which connect the detectability of an
observer network with corresponding local detectability and observability
properties of its node observers. The proposed extension allows for
nonidentical matrices to be used in the interconnections.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2123</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2123</id><created>2014-09-07</created><authors><author><keyname>Benosman</keyname><forenames>Mouhacine</forenames></author><author><keyname>Di Cairano</keyname><forenames>Stefano</forenames></author><author><keyname>Weiss</keyname><forenames>Avishai</forenames></author></authors><title>Extremum Seeking-based Iterative Learning Linear MPC</title><categories>cs.SY</categories><comments>To appear at the IEEE MSC 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we study the problem of adaptive MPC for linear time-invariant
uncertain models. We assume linear models with parametric uncertainties, and
propose an iterative multi-variable extremum seeking (MES)-based learning MPC
algorithm to learn on-line the uncertain parameters and update the MPC model.
We show the effectiveness of this algorithm on a DC servo motor control
example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2124</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2124</id><created>2014-09-07</created><authors><author><keyname>Benosman</keyname><forenames>Mouhacine</forenames></author></authors><title>Multi-Parametric Extremum Seeking-based Auto-Tuning for Robust
  Input-Output Linearization Control</title><categories>cs.SY</categories><comments>To appear at the IEEE CDC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study in this paper the problem of iterative feedback gains tuning for a
class of nonlinear systems. We consider Input-Output linearizable nonlinear
systems with additive uncertainties. We first design a nominal Input-Output
linearization-based controller that ensures global uniform boundedness of the
output tracking error dynamics. Then, we complement the robust controller with
a model-free multi-parametric extremum seeking (MES) control to iteratively
auto-tune the feedback gains. We analyze the stability of the whole controller,
i.e. robust nonlinear controller plus model-free learning algorithm. We use
numerical tests to demonstrate the performance of this method on a mechatronics
example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2129</identifier>
 <datestamp>2015-08-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2129</id><created>2014-09-07</created><updated>2014-09-17</updated><authors><author><keyname>Dong</keyname><forenames>Xianlei</forenames></author><author><keyname>Bollen</keyname><forenames>Johan</forenames></author></authors><title>Computational models of consumer confidence from large-scale online
  attention data: crowd-sourcing econometrics</title><categories>cs.CY</categories><comments>21 pages, 6 figures, 13 tables</comments><doi>10.1371/journal.pone.0120039</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Economies are instances of complex socio-technical systems that are shaped by
the interactions of large numbers of individuals. The individual behavior and
decision-making of consumer agents is determined by complex psychological
dynamics that include their own assessment of present and future economic
conditions as well as those of others, potentially leading to feedback loops
that affect the macroscopic state of the economic system. We propose that the
large-scale interactions of a nation's citizens with its online resources can
reveal the complex dynamics of their collective psychology, including their
assessment of future system states. Here we introduce a behavioral index of
Chinese Consumer Confidence (C3I) that computationally relates large-scale
online search behavior recorded by Google Trends data to the macroscopic
variable of consumer confidence. Our results indicate that such computational
indices may reveal the components and complex dynamics of consumer psychology
as a collective socio-economic phenomenon, potentially leading to improved and
more refined economic forecasting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2138</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2138</id><created>2014-09-07</created><authors><author><keyname>Kapralov</keyname><forenames>Michael</forenames></author><author><keyname>Khanna</keyname><forenames>Sanjeev</forenames></author><author><keyname>Sudan</keyname><forenames>Madhu</forenames></author></authors><title>Streaming Lower Bounds for Approximating MAX-CUT</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of estimating the value of max cut in a graph in the
streaming model of computation. At one extreme, there is a trivial
$2$-approximation for this problem that uses only $O(\log n)$ space, namely,
count the number of edges and output half of this value as the estimate for max
cut value. On the other extreme, if one allows $\tilde{O}(n)$ space, then a
near-optimal solution to the max cut value can be obtained by storing an
$\tilde{O}(n)$-size sparsifier that essentially preserves the max cut. An
intriguing question is if poly-logarithmic space suffices to obtain a
non-trivial approximation to the max-cut value (that is, beating the factor
$2$). It was recently shown that the problem of estimating the size of a
maximum matching in a graph admits a non-trivial approximation in
poly-logarithmic space.
  Our main result is that any streaming algorithm that breaks the
$2$-approximation barrier requires $\tilde{\Omega}(\sqrt{n})$ space even if the
edges of the input graph are presented in random order. Our result is obtained
by exhibiting a distribution over graphs which are either bipartite or
$\frac{1}{2}$-far from being bipartite, and establishing that
$\tilde{\Omega}(\sqrt{n})$ space is necessary to differentiate between these
two cases. Thus as a direct corollary we obtain that $\tilde{\Omega}(\sqrt{n})$
space is also necessary to test if a graph is bipartite or $\frac{1}{2}$-far
from being bipartite.
  We also show that for any $\epsilon &gt; 0$, any streaming algorithm that
obtains a $(1 + \epsilon)$-approximation to the max cut value when edges arrive
in adversarial order requires $n^{1 - O(\epsilon)}$ space, implying that
$\Omega(n)$ space is necessary to obtain an arbitrarily good approximation to
the max cut value.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2139</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2139</id><created>2014-09-07</created><authors><author><keyname>Charikar</keyname><forenames>Moses</forenames></author><author><keyname>Henzinger</keyname><forenames>Monika</forenames></author><author><keyname>Nguyen</keyname><forenames>Huy L.</forenames></author></authors><title>Online Bipartite Matching with Decomposable Weights</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a weighted online bipartite matching problem: $G(V_1, V_2, E)$ is a
weighted bipartite graph where $V_1$ is known beforehand and the vertices of
$V_2$ arrive online. The goal is to match vertices of $V_2$ as they arrive to
vertices in $V_1$, so as to maximize the sum of weights of edges in the
matching. If assignments to $V_1$ cannot be changed, no bounded competitive
ratio is achievable. We study the weighted online matching problem with {\em
free disposal}, where vertices in $V_1$ can be assigned multiple times, but
only get credit for the maximum weight edge assigned to them over the course of
the algorithm. For this problem, the greedy algorithm is $0.5$-competitive and
determining whether a better competitive ratio is achievable is a well known
open problem.
  We identify an interesting special case where the edge weights are
decomposable as the product of two factors, one corresponding to each end point
of the edge. This is analogous to the well studied related machines model in
the scheduling literature, although the objective functions are different. For
this case of decomposable edge weights, we design a 0.5664 competitive
randomized algorithm in complete bipartite graphs. We show that such instances
with decomposable weights are non-trivial by establishing upper bounds of 0.618
for deterministic and $0.8$ for randomized algorithms.
  A tight competitive ratio of $1-1/e \approx 0.632$ was known previously for
both the 0-1 case as well as the case where edge weights depend on the offline
vertices only, but for these cases, reassignments cannot change the quality of
the solution. Beating 0.5 for weighted matching where reassignments are
necessary has been a significant challenge. We thus give the first online
algorithm with competitive ratio strictly better than 0.5 for a non-trivial
case of weighted matching with free disposal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2140</identifier>
 <datestamp>2014-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2140</id><created>2014-09-07</created><authors><author><keyname>Beattie</keyname><forenames>Christopher</forenames></author><author><keyname>Gugercin</keyname><forenames>Serkan</forenames></author></authors><title>Model Reduction by Rational Interpolation</title><categories>math.NA cs.NA cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The last two decades have seen major developments in interpolatory methods
for model reduction of large-scale linear dynamical systems. Advances of note
include the ability to produce (locally) optimal reduced models at modest cost;
refined methods for deriving interpolatory reduced models directly from
input/output measurements; and extensions for the reduction of parametrized
systems. This chapter offers a survey of interpolatory model reduction methods
starting from basic principles and ranging up through recent developments that
include weighted model reduction and structure-preserving methods based on
generalized coprime representations. Our discussion is supported by an
assortment of numerical examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2141</identifier>
 <datestamp>2014-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2141</id><created>2014-09-07</created><updated>2014-10-17</updated><authors><author><keyname>Rahimian</keyname><forenames>Ardavan</forenames></author><author><keyname>Pakdehi</keyname><forenames>Davood Momeni</forenames></author></authors><title>Design and Realization of an S-Band Microwave Low-Noise Amplifier for
  Wireless RF Subsystems</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study undertakes the theoretical design, CAD modeling, realization, and
performance analysis of a microwave low-noise amplifier (LNA) which has been
accurately developed for operation at 3.0 GHz (S-band). The objective of this
research is to thoroughly analyze and develop a reliable microstrip LNA
intended for a potential employment in wireless communication systems, and
satellite applications. The S-band microwave LNA demonstrates the
appropriateness to develop a high-performance and well-established device
realization for wireless RF systems. The microwave amplifier simulations have
been conducted using the latest version of the AWR Design Environment software.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2153</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2153</id><created>2014-09-07</created><authors><author><keyname>Raheja</keyname><forenames>J. L.</forenames></author><author><keyname>Dhiraj</keyname></author><author><keyname>Gopinath</keyname><forenames>D.</forenames></author><author><keyname>Chaudhary</keyname><forenames>Ankit</forenames></author></authors><title>GUI system for Elders/Patients in Intensive Care</title><categories>cs.HC cs.CY</categories><comments>In proceedings of the 4th IEEE International Conference on
  International Technology Management Conference, Chicago, IL USA, 12-15 June,
  2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the old age, few people need special care if they are suffering from
specific diseases as they can get stroke while they are in normal life routine.
Also patients of any age, who are not able to walk, need to be taken care of
personally but for this, either they have to be in hospital or someone like
nurse should be with them for better care. This is costly in terms of money and
man power. A person is needed for 24x7 care of these people. To help in this
aspect we purposes a vision based system which will take input from the patient
and will provide information to the specified person, who is currently may not
in the patient room. This will reduce the need of man power, also a continuous
monitoring would not be needed. The system is using MS Kinect for gesture
detection for better accuracy and this system can be installed at home or
hospital easily. The system provides GUI for simple usage and gives visual and
audio feedback to user. This system work on natural hand interaction and need
no training before using and also no need to wear any glove or color strip.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2156</identifier>
 <datestamp>2014-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2156</id><created>2014-09-07</created><authors><author><keyname>Shahin</keyname><forenames>Ashraf A.</forenames></author></authors><title>Variability Modeling for Customizable SaaS Applications</title><categories>cs.SE cs.DC</categories><comments>11 pages, 10 figures</comments><doi>10.5121/ijcsit.2014.6503</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most of current Software-as-a-Service (SaaS) applications are developed as
customizable service-oriented applications that serve a large number of tenants
(users) by one application instance. The current rapid evolution of SaaS
applications increases the demand to study the commonality and variability in
software product lines that produce customizable SaaS applications. During
runtime, Customizability is required to achieve different tenants'
requirements. During the development process, defining and realizing commonalty
and variability in SaaS applications' families is required to develop reusable,
flexible, and customizable SaaS applications at lower costs, in shorter time,
and with higher quality. In this paper, Orthogonal Variability Model (OVM) is
used to model variability in a separated model, which is used to generate
simple and understandable customization model. Additionally, Service oriented
architecture Modeling Language (SoaML) is extended to define and realize
commonalty and variability during the development of SaaS applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2170</identifier>
 <datestamp>2015-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2170</id><created>2014-09-07</created><updated>2015-02-26</updated><authors><author><keyname>Bodirsky</keyname><forenames>Manuel</forenames></author><author><keyname>Bradley-Williams</keyname><forenames>David</forenames></author><author><keyname>Pinsker</keyname><forenames>Michael</forenames></author><author><keyname>Pongr&#xe1;cz</keyname><forenames>Andr&#xe1;s</forenames></author></authors><title>The universal homogeneous binary tree</title><categories>math.LO cs.CC math.CO</categories><comments>19 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A partial order is called semilinear iff the upper bounds of each element are
linearly ordered and any two elements have a common upper bound. There exists,
up to isomorphism, a unique countable existentially closed semilinear order,
which we denote by S2. We study the reducts of S2, that is, the relational
structures with the same domain as S2 all of whose relations are first-order
definable in S2. Our main result is a classification of the model-complete
cores of the reducts of S2. From this, we also obtain a classification of
reducts up to first-order interdefinability, which is equivalent to a
classification of all closed permutation groups that contain the automorphism
group of S2.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2172</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2172</id><created>2014-09-07</created><authors><author><keyname>Ercal</keyname><forenames>Gunes</forenames></author></authors><title>On Vertex Attack Tolerance in Regular Graphs</title><categories>cs.DM cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We have previously introduced vertex attack tolerance (VAT), denoted
mathematically as $\tau(G) = \min_{S \subset V}
\frac{|S|}{|V-S-C_{max}(V-S)|+1}$ where $C_{max}(V-S)$ is the largest connected
component in $V-S$, as an appropriate mathematical measure of resilience in the
face of targeted node attacks for arbitrary degree networks. A part of the
motivation for VAT was the observation that, whereas conductance, $\Phi(G)$,
captures both edge based and node based resilience for regular graphs,
conductance fails to capture node based resilience for heterogeneous degree
distributions. We had previously demonstrated an upper bound on VAT via
conductance for the case of $d$-regular graphs $G$ as follows: $\tau(G) \le
d\Phi(G)$ if $\Phi(G) \le \frac{1}{d^2}$ and $\tau(G) \le d^2\Phi(G)$
otherwise. In this work, we provide a new matching lower bound: $\tau(G) \ge
\frac{1}{d}\Phi(G)$. The lower and upper bound combined show that $\tau(G) =
\Theta(\Phi(G))$ for regular constant degree $d$ and yield spectral bounds as
corollaries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2177</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2177</id><created>2014-09-07</created><authors><author><keyname>Chaudhuri</keyname><forenames>Kamalika</forenames></author><author><keyname>Hsu</keyname><forenames>Daniel</forenames></author><author><keyname>Song</keyname><forenames>Shuang</forenames></author></authors><title>The Large Margin Mechanism for Differentially Private Maximization</title><categories>cs.LG cs.DS cs.IT math.IT math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A basic problem in the design of privacy-preserving algorithms is the private
maximization problem: the goal is to pick an item from a universe that
(approximately) maximizes a data-dependent function, all under the constraint
of differential privacy. This problem has been used as a sub-routine in many
privacy-preserving algorithms for statistics and machine-learning.
  Previous algorithms for this problem are either range-dependent---i.e., their
utility diminishes with the size of the universe---or only apply to very
restricted function classes. This work provides the first general-purpose,
range-independent algorithm for private maximization that guarantees
approximate differential privacy. Its applicability is demonstrated on two
fundamental tasks in data mining and machine learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2186</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2186</id><created>2014-09-07</created><updated>2015-06-17</updated><authors><author><keyname>Chen</keyname><forenames>Pin-Yu</forenames></author><author><keyname>Hero</keyname><forenames>Alfred O.</forenames><suffix>III</suffix></author></authors><title>Universal Phase Transition in Community Detectability under a Stochastic
  Block Model</title><categories>cs.SI physics.soc-ph</categories><comments>9 pages, 7 figures, to appear in Physical Review E</comments><doi>10.1103/PhysRevE.91.032804</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove the existence of an asymptotic phase transition threshold on
community detectability for the spectral modularity method [M. E. J. Newman,
Phys. Rev. E 74, 036104 (2006) and Proc. National Academy of Sciences. 103,
8577 (2006)] under a stochastic block model. The phase transition on community
detectability occurs as the inter-community edge connection probability $p$
grows. This phase transition separates a sub-critical regime of small $p$,
where modularity-based community detection successfully identifies the
communities, from a super-critical regime of large $p$ where successful
community detection is impossible. We show that, as the community sizes become
large, the asymptotic phase transition threshold $p^*$ is equal to
$\sqrt{p_1\cdot p_2}$, where $p_i~(i=1,2)$ is the within-community edge
connection probability. Thus the phase transition threshold is universal in the
sense that it does not depend on the ratio of community sizes. The universal
phase transition phenomenon is validated by simulations for moderately sized
communities. Using the derived expression for the phase transition threshold we
propose an empirical method for estimating this threshold from real-world data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2187</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2187</id><created>2014-09-07</created><authors><author><keyname>Song</keyname><forenames>Fang</forenames></author></authors><title>A Note on Quantum Security for Post-Quantum Cryptography</title><categories>quant-ph cs.CR</categories><comments>To appear in PQCrypto2014. Same content with different formatting</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Shor's quantum factoring algorithm and a few other efficient quantum
algorithms break many classical crypto-systems. In response, people proposed
post-quantum cryptography based on computational problems that are believed
hard even for quantum computers. However, security of these schemes against
\emph{quantum} attacks is elusive. This is because existing security analysis
(almost) only deals with classical attackers and arguing security in the
presence of quantum adversaries is challenging due to unique quantum features
such as no-cloning.
  This work proposes a general framework to study which classical security
proofs can be restored in the quantum setting. Basically, we split a security
proof into (a sequence of) classical security reductions, and investigate what
security reductions are &quot;quantum-friendly&quot;. We characterize sufficient
conditions such that a classical reduction can be &quot;lifted&quot; to the quantum
setting. We then apply our lifting theorems to post-quantum signature schemes.
We are able to show that the classical generic construction of hash-tree based
signatures from one-way functions and and a more efficient variant proposed
in~\cite{BDH11} carry over to the quantum setting. Namely, assuming existence
of (classical) one-way functions that are resistant to efficient quantum
inversion algorithms, there exists a quantum-secure signature scheme. We note
that the scheme in~\cite{BDH11} is a promising (post-quantum) candidate to be
implemented in practice and our result further justifies it. Finally we
demonstrate the generality of our framework by showing that several existing
works (Full-Domain hash in the quantum random-oracle model~\cite{Zha12ibe} and
the simple hybrid arguments framework in~\cite{HSS11}) can be reformulated
under our unified framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2193</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2193</id><created>2014-09-07</created><authors><author><keyname>Huang</keyname><forenames>Xiaowei</forenames></author><author><keyname>van der Meyden</keyname><forenames>Ron</forenames></author></authors><title>An Epistemic Strategy Logic</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper presents an extension of temporal epistemic logic with operators
that quantify over agent strategies. Unlike previous work on alternating
temporal epistemic logic, the semantics works with systems whose states
explicitly encode the strategy being used by each of the agents. This provides
a natural way to express what agents would know were they to be aware of the
strategies being used by other agents. A number of examples that rely upon the
ability to express an agent's knowledge about the strategies being used by
other agents are presented to motivate the framework, including reasoning about
game theoretic equilibria, knowledge-based programs, and information theoretic
computer security policies. Relationships to several variants of alternating
temporal epistemic logic are discussed. The computational complexity of model
checking the logic and several of its fragments are also characterized.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2195</identifier>
 <datestamp>2014-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2195</id><created>2014-09-07</created><updated>2014-09-11</updated><authors><author><keyname>Fried</keyname><forenames>Daniel</forenames></author><author><keyname>Surdeanu</keyname><forenames>Mihai</forenames></author><author><keyname>Kobourov</keyname><forenames>Stephen</forenames></author><author><keyname>Hingle</keyname><forenames>Melanie</forenames></author><author><keyname>Bell</keyname><forenames>Dane</forenames></author></authors><title>Analyzing the Language of Food on Social Media</title><categories>cs.CL cs.CY cs.SI</categories><comments>An extended abstract of this paper will appear in IEEE Big Data 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the predictive power behind the language of food on social
media. We collect a corpus of over three million food-related posts from
Twitter and demonstrate that many latent population characteristics can be
directly predicted from this data: overweight rate, diabetes rate, political
leaning, and home geographical location of authors. For all tasks, our
language-based models significantly outperform the majority-class baselines.
Performance is further improved with more complex natural language processing,
such as topic modeling. We analyze which textual features have most predictive
power for these datasets, providing insight into the connections between the
language of food, geographic locale, and community characteristics. Lastly, we
design and implement an online system for real-time query and visualization of
the dataset. Visualization tools, such as geo-referenced heatmaps,
semantics-preserving wordclouds and temporal histograms, allow us to discover
more complex, global patterns mirrored in the language of food.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2201</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2201</id><created>2014-09-08</created><authors><author><keyname>Siami</keyname><forenames>Milad</forenames></author><author><keyname>Motee</keyname><forenames>Nader</forenames></author></authors><title>Systemic Measures for Performance and Robustness of Large-Scale
  Interconnected Dynamical Networks</title><categories>math.OC cs.SY</categories><comments>Submitted to the IEEE Conference on Decision and Control (IEEE CDC)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we develop a novel unified methodology for performance and
robustness analysis of linear dynamical networks. We introduce the notion of
systemic measures for the class of first--order linear consensus networks. We
classify two important types of performance and robustness measures according
to their functional properties: convex systemic measures and Schur--convex
systemic measures. It is shown that a viable systemic measure should satisfy
several fundamental properties such as homogeneity, monotonicity, convexity,
and orthogonal invariance. In order to support our proposed unified framework,
we verify functional properties of several existing performance and robustness
measures from the literature and show that they all belong to the class of
systemic measures. Moreover, we introduce new classes of systemic measures
based on (a version of) the well--known Riemann zeta function, input--output
system norms, and etc. Then, it is shown that for a given linear dynamical
network one can take several different strategies to optimize a given
performance and robustness systemic measure via convex optimization. Finally,
we characterized an interesting fundamental limit on the best achievable value
of a given systemic measure after adding some certain number of new weighted
edges to the underlying graph of the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2208</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2208</id><created>2014-09-08</created><authors><author><keyname>Tucker</keyname><forenames>Christopher A.</forenames></author></authors><title>A wireless hand-held platform for robotic behavior control</title><categories>cs.HC cs.RO</categories><comments>12 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The need for customizable properties in autonomous robotic platforms, such as
in-home nursing care for the elderly and parallel implementations of
human-to-machine control interfaces creates an opportunity to introduce methods
deploying commonly available mobile devices running robotic command
applications in managed code. This paper will discuss a human-to-machine
interface and demonstrate a prototype consisting of a mobile device running a
configurable application communicating with a mobile robot using a managed,
type-safe language, C#.NET, over Bluetooth.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2222</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2222</id><created>2014-09-08</created><authors><author><keyname>Hajizadeh</keyname><forenames>Nahid</forenames></author><author><keyname>Ahmadzadeh</keyname><forenames>Marzieh</forenames></author></authors><title>Analysis of factors that affect the students academic performance - Data
  Mining Approach</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Analysis of factors that affect students academic performance - Data Mining
Approach
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2223</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2223</id><created>2014-09-08</created><authors><author><keyname>Hajizadeh</keyname><forenames>Nahid</forenames></author><author><keyname>Keshtgari</keyname><forenames>Manijeh</forenames></author><author><keyname>Ahmadzadeh</keyname><forenames>Marzieh</forenames></author></authors><title>Assessment of classification techniques on predicting success or failure
  of Software reusability</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Assessment of classification techniques on predicting success or failure of
Software reusability
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2232</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2232</id><created>2014-09-08</created><authors><author><keyname>Wang</keyname><forenames>Jim Jing-Yan</forenames></author></authors><title>When coding meets ranking: A joint framework based on local learning</title><categories>cs.CV cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse coding, which represents a data point as a s- parse reconstruction
code with regard to a dictionary, has been a popular data representation
method. Meanwhile, in database retrieval problems, learn the ranking scores
from data points plays an important role. Up to new, these two methods have
always been used individually, assuming that data coding and ranking are two
independent and irrele- vant problems. However, is there any internal
relationship between sparse coding and ranking score learning? If yes, how to
explore this internal relationship? In this paper, we try to answer these
questions by developing the first join- t sparse coding and ranking score
learning algorithm. To explore the local distribution in the sparse code space,
and also to bridgecoding and rankingproblems, we assume that in the
neighborhood of each data points, the ranking scores can be approximated from
the corresponding sparse codes by a local linear function. By considering the
local approx- imation error of ranking scores, reconstruction error and
sparsity of sparse coding, and the query information pro- vided by the user, we
construct an unified objective func- tion for learning of sparse codes,
dictionary and rankings scores. An iterative algorithm is developed to optimize
the objective function to jointly learn the sparse codes, dictio- nary and
rankings scores.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2234</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2234</id><created>2014-09-08</created><authors><author><keyname>Bucher</keyname><forenames>Matthias</forenames></author><author><keyname>Chatzivasileiadis</keyname><forenames>Spyros</forenames></author><author><keyname>Andersson</keyname><forenames>G&#xf6;ran</forenames></author></authors><title>Managing Flexibility in Multi-Area Power Systems</title><categories>cs.SY</categories><comments>submitted to IEEE Transactions on Power Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a framework to efficiently characterize the
available operational flexibility in a multi-area power system. We focus on the
available reserves and the tie-line flows. The proposed approach is an
alternative to the current calculation of the Available Transfer Capacity
(ATC), as it considers location and availability of reserves, transmission
constraints, and interdependencies of tie-line flows between different areas,
while it takes into account the N-1 security criterion. The method is based on
computational geometry using polytopic projections. It requires only a limited
amount of information exchange and does not need central coordination. The
method has two versions: a passive and an active approach, where neighboring
areas can share reserves. In that respect we also introduce the term &quot;exported
flexibility&quot;, which could form the basis for a new trading product in
electricity markets. Case studies demonstrate the improved tie-line
utilization, especially if reserves are shared, and the visualization benefits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2235</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2235</id><created>2014-09-08</created><updated>2014-09-13</updated><authors><author><keyname>Mo</keyname><forenames>Qi</forenames></author><author><keyname>Yeh</keyname><forenames>Hengchin</forenames></author><author><keyname>Manocha</keyname><forenames>Dinesh</forenames></author></authors><title>Tracing Analytic Ray Curves for Light and Sound Propagation in
  Non-linear Media</title><categories>cs.GR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The physical world consists of spatially varying media, such as the
atmosphere and the ocean, in which light and sound propagates along non-linear
trajectories. This presents a challenge to existing ray-tracing based methods,
which are widely adopted to simulate propagation due to their efficiency and
flexibility, but assume linear rays. We present a novel algorithm that traces
analytic ray curves computed from local media gradients, and utilizes the
closed-form solutions of both the intersections of the ray curves with planar
surfaces, and the travel distance. By constructing an adaptive unstructured
mesh, our algorithm is able to model general media profiles that vary in three
dimensions with complex boundaries consisting of terrains and other scene
objects such as buildings. We trace the analytic ray curves using the adaptive
unstructured mesh, which considerably improves the efficiency over prior
methods. We highlight the algorithm's application on simulation of sound and
visual propagation in outdoor scenes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2246</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2246</id><created>2014-09-08</created><authors><author><keyname>Wette</keyname><forenames>Philip</forenames></author><author><keyname>Karl</keyname><forenames>Holger</forenames></author></authors><title>DCT${^2}$Gen: A Versatile TCP Traffic Generator for Data Centers</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Only little is publicly known about traffic in non-educational data centers.
Recent studies made some knowledge available, which gives us the opportunity to
create more realistic traffic models for data center research. We used this
knowledge to create the first publicly available traffic generator that
produces realistic traffic between hosts in data centers of arbitrary size. We
characterize traffic by using six probability distribution functions and
concentrate on the generation of traffic on flow-level. The distribution
functions are described as step functions, which makes our generator highly
configurable to generate traffic for different kinds of data centers. Moreover,
in data centers, traffic between hosts in the same rack and hosts in different
racks have different properties. We model this phenomenon, making our generated
traffic very realistic. We carefully evaluated our approach and conclude that
it reproduces these characteristics with high accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2248</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2248</id><created>2014-09-08</created><authors><author><keyname>Finko</keyname><forenames>Oleg</forenames></author><author><keyname>Dichenko</keyname><forenames>Sergey</forenames></author></authors><title>Secure pseudo-random linear binary sequences generators based on
  arithmetic polynoms</title><categories>cs.CR</categories><msc-class>94C10, 94A60, 11K45, 11A07</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new approach to constructing of pseudo-random binary sequences
(PRS) generators for the purpose of cryptographic data protection, secured from
the perpetrator's attacks, caused by generation of masses of hardware errors
and faults. The new method is based on use of linear polynomial arithmetic for
the realization of systems of boolean characteristic functions of PRS'
generators. &quot;Arithmetizatio&quot; of systems of logic formulas has allowed to apply
mathematical apparatus of residue systems for multisequencing of the process of
PRS generation and organizing control of computing errors, caused by hardware
faults. This has guaranteed high security of PRS generator's functioning and,
consequently, security of tools for cryptographic data protection based on
those PRSs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2255</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2255</id><created>2014-09-08</created><authors><author><keyname>Teixeira</keyname><forenames>Jose</forenames></author></authors><title>Open-coopetition in the PC and mobile industries: the WebKit case</title><categories>cs.CY</categories><comments>As submitted to the 6th workshop on coopetition strategy Coopetition
  Strategy and Practice UMEA, Sweden, May 22-23, 2014</comments><acm-class>D.9; K.6.3; H.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In an era of software crisis, the move of firms towards
geographically-distributed software development teams is being challenged by
collaboration issues. On this matter, the open-source phenomenon may shed some
light, as successful cases on distributed collaboration in the open-source
community have been recurrently reported. While practitioners move with
difficulties towards globally distributed software development, there is a lack
of research addressing the collaboration dynamics of large-scale distributed
software projects. More particularly, even if there are empirical
manifestations of collaboration among software-houses that market
rival-software products within the same market, there is a clear lack of
research addressing the development of information systems by coopetitive
manners. While addressing a previous call for the advancement of methods and
techniques to support the visualization of temporal aspects to represent change
and evolution in ecosystems, we combined and virtual-ethnography (VA) with a
Social Network Analysis (SNA) over publicly-available and naturally-occurring
data that allowed us to re-construct and visualize the evolution of the WebKit
collaboration in a sequence of networks. We started by screening, publicly
available data such as company announcements, financial reports and
specialized-press that allowed us to gain insights of the industrial context.
After attaining a better understanding of the the competitive dynamics of the
mobile-devices and PC industries, we started extracting and analyzing the
social network of the WebKit community leveraging SNA, which is an emergent
method widely established across disciplines of social sciences in general.Our
case confirmed much of the established literature on coopetition, but more
detailed observations revealed that not all coopetition theoretical
propositions can be generalized to the open-source arena.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2261</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2261</id><created>2014-09-08</created><authors><author><keyname>Ajah</keyname><forenames>Ifeyinwa Angela</forenames></author></authors><title>Evaluation of Enhanced Security Solutions in 802.11-Based Networks</title><categories>cs.CR cs.NI</categories><comments>The paper holistically surveyed and evaluated the outcome of past
  researches on security protocols; 802.1X, WPA, WPA2 ,in juxtaposition to VPN
  SSL, and VPN IPsec. It discovered that strength of each solution depends on
  how well the encryption, authentication and integrity techniques work. The
  work suggested using a Defence-in-Depth Strategy and integration of biometric
  solution in 802.11i</comments><journal-ref>IJNSA, Vol.6, No.4, July 2014 International Journal of Network
  Security &amp; Its Applications (IJNSA), Vol.6, No.4, July 2014 International
  Journal of Network Security &amp; Its Applications (IJNSA), Vol.6, No.4, July
  2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditionally, 802.11-based networks that relied on wired equivalent protocol
(WEP) were especially vulnerable to packet sniffing. Today, wireless networks
are more prolific, and the monitoring devices used to find them are mobile and
easy to access. Securing wireless networks can be difficult because these
networks consist of radio transmitters and receivers, and anybody can listen,
capture data and attempt to compromise it. In recent years, a range of
technologies and mechanisms have helped make networking more secure. This paper
holistically evaluated various enhanced protocols proposed to solve WEP related
authentication, confidentiality and integrity problems. It discovered that
strength of each solution depends on how well the encryption, authentication
and integrity techniques work. The work suggested using a Defence-in-Depth
Strategy and integration of biometric solution in 802.11i. Comprehensive
in-depth comparative analysis of each of the security mechanisms is driven by
review of related work in WLAN security solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2264</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2264</id><created>2014-09-08</created><authors><author><keyname>Nawaz</keyname><forenames>Sarfraz</forenames></author><author><keyname>Mascolo</keyname><forenames>Cecilia</forenames></author></authors><title>Mining Users' Significant Driving Routes with Low-power Sensors</title><categories>cs.CY cs.NI</categories><comments>ACM Conference on Embedded Networked Sensor Systems (SenSys 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While there is significant work on sensing and recognition of significant
places for users, little attention has been given to users' significant routes.
Recognizing these routine journeys, opens doors to the development of novel
applications, like personalized travel alerts, and enhancement of user's travel
experience. However, the high energy consumption of traditional location
sensing technologies, such as GPS or WiFi based localization, is a barrier to
passive and ubiquitous route sensing through smartphones.
  In this paper, we present a passive route sensing framework that continuously
monitors a vehicle user solely through a phone's gyroscope and accelerometer.
This approach can differentiate and recognize various routes taken by the user
by time warping angular speeds experienced by the phone while in transit and is
independent of phone orientation and location within the vehicle, small detours
and traffic conditions. We compare the route learning and recognition
capabilities of this approach with GPS trajectory analysis and show that it
achieves similar performance. Moreover, with an embedded co-processor, common
to most new generation phones, it achieves energy savings of an order of
magnitude over the GPS sensor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2287</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2287</id><created>2014-09-08</created><authors><author><keyname>Damianou</keyname><forenames>Andreas C.</forenames></author><author><keyname>Titsias</keyname><forenames>Michalis K.</forenames></author><author><keyname>Lawrence</keyname><forenames>Neil D.</forenames></author></authors><title>Variational Inference for Uncertainty on the Inputs of Gaussian Process
  Models</title><categories>stat.ML cs.AI cs.CV cs.LG</categories><comments>51 pages (of which 10 is Appendix), 19 figures</comments><msc-class>60G15 (Primary), 58E30</msc-class><acm-class>G.3; G.1.2; I.2.6; I.5.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Gaussian process latent variable model (GP-LVM) provides a flexible
approach for non-linear dimensionality reduction that has been widely applied.
However, the current approach for training GP-LVMs is based on maximum
likelihood, where the latent projection variables are maximized over rather
than integrated out. In this paper we present a Bayesian method for training
GP-LVMs by introducing a non-standard variational inference framework that
allows to approximately integrate out the latent variables and subsequently
train a GP-LVM by maximizing an analytic lower bound on the exact marginal
likelihood. We apply this method for learning a GP-LVM from iid observations
and for learning non-linear dynamical systems where the observations are
temporally correlated. We show that a benefit of the variational Bayesian
procedure is its robustness to overfitting and its ability to automatically
select the dimensionality of the nonlinear latent space. The resulting
framework is generic, flexible and easy to extend for other purposes, such as
Gaussian process regression with uncertain inputs and semi-supervised Gaussian
processes. We demonstrate our method on synthetic data and standard machine
learning benchmarks, as well as challenging real world datasets, including high
resolution video data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2290</identifier>
 <datestamp>2014-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2290</id><created>2014-09-08</created><authors><author><keyname>Decelle</keyname><forenames>Aur&#xe9;lien</forenames></author><author><keyname>H&#xfc;ttel</keyname><forenames>Janina</forenames></author><author><keyname>Saade</keyname><forenames>Alaa</forenames></author><author><keyname>Moore</keyname><forenames>Cristopher</forenames></author></authors><title>Computational Complexity, Phase Transitions, and Message-Passing for
  Community Detection</title><categories>cond-mat.dis-nn cond-mat.stat-mech cs.CC physics.soc-ph</categories><comments>Chapter of &quot;Statistical Physics, Optimization, Inference, and
  Message-Passing Algorithms&quot;, Eds.: F. Krzakala, F. Ricci-Tersenghi, L.
  Zdeborova, R. Zecchina, E. W. Tramel, L. F. Cugliandolo (Oxford University
  Press, to appear)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We take a whirlwind tour of problems and techniques at the boundary of
computer science and statistical physics. We start with a brief description of
P, NP, and NP-completeness. We then discuss random graphs, including the
emergence of the giant component and the k-core, using techniques from
branching processes and differential equations. Using these tools as well as
the second moment method, we give upper and lower bounds on the critical clause
density for random k-SAT. We end with community detection in networks,
variational methods, the Bethe free energy, belief propagation, the
detectability transition, and the non-backtracking matrix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2291</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2291</id><created>2014-09-08</created><updated>2014-09-14</updated><authors><author><keyname>Chatterjee</keyname><forenames>Krishnendu</forenames></author><author><keyname>Pavlogiannis</keyname><forenames>Andreas</forenames></author><author><keyname>K&#xf6;&#xdf;ler</keyname><forenames>Alexander</forenames></author><author><keyname>Schmid</keyname><forenames>Ulrich</forenames></author></authors><title>A Framework for Automated Competitive Analysis of On-line Scheduling of
  Firm-Deadline Tasks</title><categories>cs.DS cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a flexible framework for the automated competitive analysis of
on-line scheduling algorithms for firm-deadline real-time tasks based on
multi-objective graphs: Given a taskset and an on-line scheduling algorithm
specified as a labeled transition system, along with some optional safety,
liveness, and/or limit-average constraints for the adversary, we automatically
compute the competitive ratio of the algorithm w.r.t. a clairvoyant scheduler.
We demonstrate the flexibility and power of our approach by comparing the
competitive ratio of several on-line algorithms, including $D^{over}$, that
have been proposed in the past, for various tasksets. Our experimental results
reveal that none of these algorithms is universally optimal, in the sense that
there are tasksets where other schedulers provide better performance. Our
framework is hence a very useful design tool for selecting optimal algorithms
for a given application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2294</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2294</id><created>2014-09-08</created><authors><author><keyname>ter Beek</keyname><forenames>Maurice H.</forenames><affiliation>ISTI-CNR, Pisa, Italy</affiliation></author><author><keyname>Ravara</keyname><forenames>Ant&#xf3;nio</forenames><affiliation>New University of Lisbon, Portugal</affiliation></author></authors><title>Proceedings 10th International Workshop on Automated Specification and
  Verification of Web Systems</title><categories>cs.LO cs.PL cs.SE</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 163, 2014</journal-ref><doi>10.4204/EPTCS.163</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  These proceedings contain the papers presented at the 10th International
Workshop on Automated Specification and Verification of Web Systems (WWV 2014),
which was held on 18 July 2014 in Vienna, Austria, as a satellite workshop of
the Federated Logic Conference (FLoC 2014), associated to the 7th International
Joint Conference on Automated Reasoning (IJCAR 2014), as part of the Vienna
Summer of Logic (VSL 2014).
  WWV is a yearly workshop that aims at providing an interdisciplinary forum to
facilitate the cross-fertilization and the advancement of hybrid methods that
exploit concepts and tools drawn from rule-based programming, formal methods,
software engineering and Web-oriented research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2303</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2303</id><created>2014-09-08</created><authors><author><keyname>Tesfamicael</keyname><forenames>Solomon A.</forenames></author><author><keyname>Godana</keyname><forenames>Bruhtesfa E.</forenames></author><author><keyname>Barzideh</keyname><forenames>Faraz</forenames></author></authors><title>Compressed Sensing Performance Analysis via Replica Method using
  Bayesian framework</title><categories>cs.IT math.IT</categories><comments>The analytical work and results were presented at the 2012 IEEE
  European School of Information Theory in Antalya, Turkey between the 16th and
  the 20th of April</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compressive sensing (CS) is a new methodology to capture signals at lower
rate than the Nyquist sampling rate when the signals are sparse or sparse in
some domain. The performance of CS estimators is analyzed in this paper using
tools from statistical mechanics, especially called replica method. This method
has been used to analyze communication systems like Code Division Multiple
Access (CDMA) and multiple input multi- ple output (MIMO) systems with large
size. Replica analysis, now days rigorously proved, is an efficient tool to
analyze large systems in general. Specifically, we analyze the performance of
some of the estimators used in CS like LASSO (the Least Absolute Shrinkage and
Selection Operator) estimator and Zero-Norm regularizing estimator as a special
case of maximum a posteriori (MAP) estimator by using Bayesian framework to
connect the CS estimators and replica method. We use both replica symmetric
(RS) ansatz and one-step replica symmetry breaking (1RSB) ansatz, clamming the
latter is efficient when the problem is not convex. This work is more
analytical in its form. It is deferred for next step to focus on the numerical
results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2304</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2304</id><created>2014-09-08</created><authors><author><keyname>Zanin</keyname><forenames>Massimiliano</forenames></author><author><keyname>Perez</keyname><forenames>David</forenames></author><author><keyname>Chatterjee</keyname><forenames>Kumardev</forenames></author><author><keyname>Kolovos</keyname><forenames>Dimitrios S.</forenames></author><author><keyname>Paige</keyname><forenames>Richard F.</forenames></author><author><keyname>Horst</keyname><forenames>Andreas</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>On Demand Data Analysis and Filtering for Inaccurate Flight Trajectories</title><categories>cs.SE</categories><comments>5 pages, 5 figures. D. Schaefer (ed) Proceedings of the SESAR
  Innovation Days (2011) EUROCONTROL, November 29 - December 1, 2011, Toulouse,
  France</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper reports on work performed in the context of the COMPASS SESAR-JU
WP-E project, on developing an approach for identifying and filtering
inaccurate trajectories (ghost flights) in historical data originating from the
EUROCONTROL-operated Demand Data Repository (DDR).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2306</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2306</id><created>2014-09-08</created><authors><author><keyname>Fisch</keyname><forenames>M. Norbert</forenames></author><author><keyname>Look</keyname><forenames>Markus</forenames></author><author><keyname>Pinkernell</keyname><forenames>Claas</forenames></author><author><keyname>Plesser</keyname><forenames>Stefan</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>State-Based Modeling of Buildings and Facilities</title><categories>cs.SE</categories><comments>8 pages, 6 figures. Proceedings of the 11th International Conference
  for Enhanced Building Operations (ICEBO' 11), New York City, USA, October
  2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Research on energy efficiency of today's buildings focuses on the monitoring
of a building's behavior while in operation. But without a formalized
description of the data measured, including their correlations and in
particular the expected measurements, the full potential of the collected data
can not necessarily be exploited. Who knows if a measured value is good or bad?
This problem becomes more virulent as smart control systems sometimes exhibit
intelligent, but unexpected behavior (e.g. starting heating at unconventional
times). Therefore we defined a methodology starting already at the design of
the building leading to a formalized specification of the implementation of a
building's management system, which seamlessly integrates to an intelligent
monitoring. DIN EN ISO 16484 proposes a method to describe functional
requirements in an easy to understand way. We extended its use of state
machines to our proposed concept of state based modeling. This proved to be a
wholesome approach to easily model buildings and facilities according to the
DIN EN ISO 16484 while providing the possibility to apply sophisticated and
meaningful analysis methods during monitoring.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2307</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2307</id><created>2014-09-08</created><authors><author><keyname>Maoz</keyname><forenames>Shahar</forenames></author><author><keyname>Ringert</keyname><forenames>Jan Oliver</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>Summarizing Semantic Model Differences</title><categories>cs.SE</categories><comments>10 pages, 7 figures. ME 2011 - Models and Evolution, Wellington, New
  Zealand. Ed: B. Sch\&quot;atz, D. Deridder, A. Pierantonio, J. Sprinkle, D.
  Tamzalit, Wellington, New Zealand, Okt. 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fundamental building blocks for managing and understanding software evolution
in the context of model-driven engineering are differencing operators one can
use for model comparisons. Semantic model differencing deals with the
definition and computation of semantic diff operators for model comparison,
operators whose input consists of two models and whose output is a set of diff
witnesses, instances of one model that are not instances of the other. However,
in many cases the complete set of diff witnesses is too large to be efficiently
computed and effectively presented. Moreover, many of the witnesses are very
similar and hence not interesting. Thus, an important challenge of semantic
differencing relates to witness selection and presentation. In this paper we
propose to address this challenge using a summarization technique, based on a
notion of equivalence that partitions the set of diff witnesses. The result of
the computation is a summary set, consisting of a single representative witness
from each equivalence class. We demonstrate our ideas using two concrete diff
operators, for class diagrams and for activity diagrams, where the computation
of the summary set is efficient and does not require the enumeration of all
witnesses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2309</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2309</id><created>2014-09-08</created><authors><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author><author><keyname>Weisem&#xf6;ller</keyname><forenames>Ingo</forenames></author></authors><title>A Domain Specific Transformation Language</title><categories>cs.SE</categories><comments>10 pages, 5 figures. ME 2011 - Models and Evolution, Wellington, New
  Zealand. Ed: B. Sch\&quot;atz, D. Deridder, A. Pierantonio, J. Sprinkle, D.
  Tamzalit, Wellington, New Zealand, Okt. 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Domain specific languages (DSLs) allow domain experts to model parts of the
system under development in a problem-oriented notation that is well-known in
the respective domain. The introduction of a DSL is often accompanied the
desire to transform its instances. Although the modeling language is domain
specific, the transformation language used to describe modifications, such as
model evolution or refactoring operations, on the underlying model, usually is
a rather domain independent language nowadays. Most transformation languages
use a generic notation of model patterns that is closely related to typed and
attributed graphs or to object diagrams (the abstract syntax). A notation that
reflects the transformed elements of the original DSL in its own concrete
syntax would be strongly preferable, because it would be more comprehensible
and easier to learn for domain experts. In this paper we present a
transformation language that reuses the concrete syntax of a textual modeling
language for hierarchical automata, which allows domain experts to describe
models as well as modifications of models in a convenient, yet precise manner.
As an outlook, we illustrate a scenario where we generate transformation
languages from existing textual languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2310</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2310</id><created>2014-09-08</created><authors><author><keyname>Ringert</keyname><forenames>Jan Oliver</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author><author><keyname>Wortmann</keyname><forenames>Andreas</forenames></author></authors><title>MontiArcAutomaton: Modeling Architecture and Behavior of Robotic Systems</title><categories>cs.SE</categories><comments>3 pages, 4 figures. Workshops and Tutorials Proceedings of the 2013
  IEEE International Conference on Robotics and Automation (ICRA), May 6-10,
  2013, Karlsruhe, Germany</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Robotics poses a challenge for software engineering as the vast numbers of
different robot platforms impose different requirements on robot control
architectures. The platform dependent development of robotic applications
impedes reusability and portability. The lack of reusability hampers broad
propagation of robotics applications. The MontiArcAutomaton architecture and
behavior modeling framework provides an integrated, platform independent
structure and behavior modeling language with an extensible code generation
framework. MontiArcAutomaton's central concept is encapsulation and
decomposition known from Component &amp; Connector Architecture Description
Languages. This concept is extended from the modeling language to the code
generation and target runtime framework to bridge the gap of platform specific
and independent implementations along well designed interfaces. This
facilitates the reuse of robot applications and makes their development more
efficient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2311</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2311</id><created>2014-09-08</created><authors><author><keyname>Haber</keyname><forenames>Arne</forenames></author><author><keyname>Renel</keyname><forenames>Holger</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author><author><keyname>Schaefer</keyname><forenames>Ina</forenames></author></authors><title>Evolving Delta-oriented Software Product Line Architectures</title><categories>cs.SE</categories><comments>26 pages, 6 figures</comments><journal-ref>Large-Scale Complex IT Systems. Development, Operation and
  Management, 17th Monterey Workshop 2012, Oxford, UK, March 19-21, 2012. LNCS
  Vol. 7539, pp. 183-208</journal-ref><doi>10.1007/978-3-642-34059-8_10</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Diversity is prevalent in modern software systems. Several system variants
exist at the same time in order to adapt to changing user requirements.
Additionally, software systems evolve over time in order to adjust to
unanticipated changes in their application environment. In modern software
development, software architecture modeling is an important means to deal with
system complexity by architectural decomposition. This leads to the need of
architectural description languages that can represent spatial and temporal
variability. In this paper, we present delta modeling of software architectures
as a uniform modeling formalism for architectural variability in space and in
time. In order to avoid degeneration of the product line model under system
evolution, we present refactoring techniques to maintain and improve the
quality of the variability model. Using a running example from the automotive
domain, we evaluate our approach by carrying out a case study that compares
delta modeling with annotative variability modeling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2313</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2313</id><created>2014-09-08</created><authors><author><keyname>Maoz</keyname><forenames>Shahar</forenames></author><author><keyname>Ringert</keyname><forenames>Jan Oliver</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>Semantically Configurable Consistency Analysis for Class and Object
  Diagrams</title><categories>cs.SE</categories><comments>15 pages, 7 figures. Received Best Paper Award and ACM Distinguished
  Paper Award at the MODELS 2011 Conference</comments><journal-ref>Model Driven Engineering Languages and Systems (MODELS 2011),
  Wellington, New Zealand. pp. 153-167, LNCS 6981, 2011.</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Checking consistency between an object diagram (OD) and a class diagram (CD)
is an important analysis problem. However, several variations in the semantics
of CDs and ODs, as used in different contexts and for different purposes,
create a challenge for analysis tools. To address this challenge in this paper
we investigate semantically configurable model analysis. We formalize the
variability in the languages semantics using a feature model: each
configuration that the model permits induces a different semantics. Moreover,
we develop a parametrized analysis that can be instantiated to comply with
every legal configuration of the feature model. Thus, the analysis is
semantically congured and its results change according to the semantics induced
by the selected feature configuration. The ideas are implemented using a
parametrized transformation to Alloy. The work can be viewed as a case study
example for a formal and automated approach to handling semantic variability in
modeling languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2314</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2314</id><created>2014-09-08</created><authors><author><keyname>Maoz</keyname><forenames>Shahar</forenames></author><author><keyname>Ringert</keyname><forenames>Jan Oliver</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>CD2Alloy: Class Diagrams Analysis Using Alloy Revisited</title><categories>cs.SE</categories><comments>15 pages, 8 figures</comments><journal-ref>Model Driven Engineering Languages and Systems (MODELS 2011),
  Wellington, New Zealand. pp. 592-607, LNCS 6981, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present CD2Alloy, a novel, powerful translation of UML class diagrams
(CDs) to Alloy. Unlike existing translations, which are based on a shallow
embedding strategy, and are thus limited to checking consistency and generating
conforming object models of a single CD, and support a limited set of CD
language features, CD2Alloy uses a deeper embedding strategy. Rather than
mapping each CD construct to a semantically equivalent Alloy construct,
CD2Alloy defines (some) CD constructs as new concepts within Alloy. This
enables solving several analysis problems that involve more than one CD and
could not be solved by earlier works, and supporting an extended list of CD
language features. The ideas are implemented in a prototype Eclipse plug-in.
The work advances the state-of-the-art in CD analysis, and can also be viewed
as an interesting case study for the different possible translations of one
modeling language to another, their strengths and weaknesses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2315</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2315</id><created>2014-09-08</created><authors><author><keyname>Haber</keyname><forenames>Arne</forenames></author><author><keyname>Kutz</keyname><forenames>Thomas</forenames></author><author><keyname>Rendel</keyname><forenames>Holger</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author><author><keyname>Schaefer</keyname><forenames>Ina</forenames></author></authors><title>Towards a Family-based Analysis of Applicability Conditions in
  Architectural Delta Models</title><categories>cs.SE</categories><comments>10 pages, 7 figures. Variability for You Proceedings of VARY
  InternationalWorkshop affiliated with ACM/IEEE 14th International Conference
  on Model Driven Engineering Languages and Systems (MODELS'11), IT University
  Technical Report Series TR-2011-144</comments><report-no>TR-2011-144</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modeling variability in software architectures is a fundamental part of
software product line development. ?-MontiArc allows describing architectural
variability in a modular way by a designated core architecture and a set of
architectural delta models modifying the core architecture to realize other
architecture variants. Delta models have to satisfy a set of applicability
conditions for the definedness of the architectural variants. The applicability
conditions can in principle be checked by generating all possible architecture
variants, which requires considering the same intermediate architectures
repeatedly. In order to reuse previously computed architecture variants, we
propose a family-based analysis of the applicability conditions using the
concept of inverse deltas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2317</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2317</id><created>2014-09-08</created><authors><author><keyname>Haber</keyname><forenames>Arne</forenames></author><author><keyname>Kutz</keyname><forenames>Thomas</forenames></author><author><keyname>Rendel</keyname><forenames>Holger</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author><author><keyname>Schaefer</keyname><forenames>Ina</forenames></author></authors><title>Delta-oriented Architectural Variability Using MontiCore</title><categories>cs.SE</categories><comments>10 pages, 9 figures. ECSA '11 5th European Conference on Software
  Architecture: Companion Volume, ACM New York, NY, USA, Article No. 6,
  September 2011</comments><doi>10.1145/2031759.2031767</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modeling of software architectures is a fundamental part of software
development processes. Reuse of software components and early analysis of
software topologies allow the reduction of development costs and increases
software quality. Integrating variability modeling concepts into architecture
description languages (ADLs) is essential for the development of diverse
software systems with high demands on software quality. In this paper, we
present the integration of delta modeling into the existing ADL MontiArc. Delta
modeling is a language-independent variability modeling approach supporting
proactive, reactive and extractive product line development. We show how
?-MontiArc, a language for explicit modeling of architectural variability based
on delta modeling, is implemented as domain-specific language (DSL) using the
DSL development framework MontiCore. We also demonstrate how MontiCore's
language reuse mechanisms provide efficient means to derive an implementation
of ?-MontiArc tool implementation. We evaluate ?-Monti-Arc by comparing it with
annotative variability modeling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2318</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2318</id><created>2014-09-08</created><authors><author><keyname>Haber</keyname><forenames>Arne</forenames></author><author><keyname>Renel</keyname><forenames>Holger</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author><author><keyname>Schaefer</keyname><forenames>Ina</forenames></author><author><keyname>van der Linden</keyname><forenames>Frank</forenames></author></authors><title>Hierarchical Variability Modeling for Software Architectures</title><categories>cs.SE</categories><comments>10 pages, 9 figures. Proceedings of International Software Product
  Lines Conference (SPLC 2011), IEEE Computer Society, August 2011</comments><doi>10.1109/SPLC.2011.28</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hierarchically decomposed component-based system development reduces design
complexity by supporting distribution of work and component reuse. For product
line development, the variability of the components to be deployed in different
products has to be represented by appropriate means. In this paper, we propose
hierarchical variability modeling which allows specifying component variability
integrated with the component hierarchy and locally to the components.
Components can contain variation points determining where components may vary.
Associated variants define how this variability can be realized in different
component configurations. We present a meta model for hierarchical variability
modeling to formalize the conceptual ideas. In order to obtain an
implementation of the proposed approach together with tool support, we extend
the existing architectural description language MontiArc with hierarchical
variability modeling. We illustrate the presented approach using an example
from the automotive systems domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2329</identifier>
 <datestamp>2015-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2329</id><created>2014-09-08</created><updated>2015-02-19</updated><authors><author><keyname>Zaremba</keyname><forenames>Wojciech</forenames></author><author><keyname>Sutskever</keyname><forenames>Ilya</forenames></author><author><keyname>Vinyals</keyname><forenames>Oriol</forenames></author></authors><title>Recurrent Neural Network Regularization</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a simple regularization technique for Recurrent Neural Networks
(RNNs) with Long Short-Term Memory (LSTM) units. Dropout, the most successful
technique for regularizing neural networks, does not work well with RNNs and
LSTMs. In this paper, we show how to correctly apply dropout to LSTMs, and show
that it substantially reduces overfitting on a variety of tasks. These tasks
include language modeling, speech recognition, image caption generation, and
machine translation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2332</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2332</id><created>2014-09-08</created><updated>2016-01-03</updated><authors><author><keyname>Wan</keyname><forenames>Neng</forenames></author><author><keyname>Yao</keyname><forenames>Weiran</forenames></author></authors><title>Partially Independent Control Scheme for Spacecraft Rendezvous in
  Near-Circular Orbits</title><categories>cs.SY math.DS math.OC physics.space-ph</categories><comments>18 pages, 7 figures, Submitted to ASCE Journal of Aerospace
  Engineering</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to the complexity and inconstancy of the space environment, accurate
mathematical models for spacecraft rendezvous are difficult to obtain, which
consequently complicates the control tasks. In this paper, a linearized
time-variant plant model with external perturbations is adopted to approximate
the real circumstance. To realize the robust stability with optimal performance
cost, a partially independent control scheme is proposed, which consists of a
robust anti-windup controller for the in-plane motion and a ${{H}_{\infty}}$
controller for the out-of-plane motion. Finally, a rendezvous simulation is
given to corroborate the practicality and advantages of the partially
independent control scheme over a coupled control scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2352</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2352</id><created>2014-09-08</created><authors><author><keyname>Maoz</keyname><forenames>Shahar</forenames></author><author><keyname>Ringert</keyname><forenames>Jan Oliver</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>ADDiff: Semantic Differencing for Activity Diagrams</title><categories>cs.SE</categories><comments>11 pages, 9 figures</comments><journal-ref>Proc. Euro. Soft. Eng. Conf. and SIGSOFT Symp. on the Foundations
  of Soft. Eng. (ESEC/FSE'11), pp. 179-189, ACM, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Activity diagrams (ADs) have recently become widely used in the modeling of
workflows, business processes, and web-services, where they serve various
purposes, from documentation, requirement definitions, and test case
specifications, to simulation and code generation. As models, programs, and
systems evolve over time, understanding changes and their impact is an
important challenge, which has attracted much research efforts in recent years.
In this paper we present addiff, a semantic differencing operator for ADs.
Unlike most existing approaches to model comparison, which compare the concrete
or the abstract syntax of two given diagrams and output a list of syntactical
changes or edit operations, addiff considers the Semantics of the diagrams at
hand and outputs a set of diff witnesses, each of which is an execution trace
that is possible in the first AD and is not possible in the second. We motivate
the use of addiff, formally define it, and show two algorithms to compute it, a
concrete forward-search algorithm and a symbolic xpoint algorithm, implemented
using BDDs and integrated into the Eclipse IDE. Empirical results and examples
demonstrate the feasibility and unique contribution of addiff to the
state-of-the-art in version comparison and evolution analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2353</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2353</id><created>2014-09-08</created><authors><author><keyname>Maoz</keyname><forenames>Shahar</forenames></author><author><keyname>Ringert</keyname><forenames>Jan Oliver</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>Modal Object Diagrams</title><categories>cs.SE</categories><comments>25 pages, 9 figures</comments><journal-ref>Proc. 25th Euro. Conf. on Object Oriented Programming (ECOOP'11),
  LNCS 6813, pp. 281-305, Springer, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While object diagrams (ODs) are widely used as a means to document
object-oriented systems, they are expressively weak, as they are limited to
describe specific possible snapshots of the system at hand. In this paper we
introduce modal object diagrams (MODs), which extend the classical OD language
with positive/negative and example/invariant modalities. The extended language
allows the designer to specify not only positive example models but also
negative examples, ones that the system should not allow, positive invariants,
ones that all system's snapshots should include, and negative invariants, ones
that no system snapshot is allowed to include. Moreover, as a primary
application of the extended language we provide a formal verification technique
that decides whether a given class diagram satisfies (i.e., models) a
multi-modal object diagrams specification. In case of a negative answer, the
technique outputs relevant counterexample object models, as applicable. The
verification is based on a reduction to Alloy. The ideas are implemented in a
prototype Eclipse plug-in. Examples show the usefulness of the extended
language in specifying structural requirements of object-oriented systems in an
intuitive yet expressive way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2355</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2355</id><created>2014-09-08</created><authors><author><keyname>Maoz</keyname><forenames>Shahar</forenames></author><author><keyname>Ringert</keyname><forenames>Jan Oliver</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>CDDiff: Semantic Differencing for Class Diagrams</title><categories>cs.SE</categories><comments>25 pages, 9 figures</comments><journal-ref>Proc. 25th Euro. Conf. on Object Oriented Programming (ECOOP'11),
  LNCS 6813, pp. 230-254, Springer, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Class diagrams (CDs), which specify classes and the relationships between
them, are widely used for modeling the structure of object-oriented systems. As
models, programs, and systems evolve over time, during the development
lifecycle and beyond it, effective change management is a major challenge in
software development, which has attracted much research efforts in recent
years. In this paper we present cddiff, a semantic diff operator for CDs.
Unlike most existing approaches to model comparison, which compare the concrete
or the abstract syntax of two given diagrams and output a list of syntactical
changes or edit operations, cddiff considers the semantics of the diagrams at
hand and outputs a set of diff witnesses, each of which is an object model that
is possible in the first CD and is not possible in the second. We motivate the
use of cddiff, formally define it, and show how it is computed. The computation
is based on a reduction to Alloy. The work is implemented in a prototype
Eclipse plug-in. Examples show the unique contribution of our approach to the
state-of-the-art in version comparison and evolution analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2356</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2356</id><created>2014-09-08</created><authors><author><keyname>Maoz</keyname><forenames>Shahar</forenames></author><author><keyname>Ringert</keyname><forenames>Jan Oliver</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>An Operational Semantics for Activity Diagrams using SMV</title><categories>cs.SE</categories><comments>48 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This document defines an operational semantics for activity diagrams (ADs)
using a translation to SMV. The translation is inspired by the work of Eshuis
[Esh06] and extends it with support for data. Each execution step of the SMV
module obtained from an AD represents an executed action of this AD with
interleaved execution of concurrent branches. An implementation of the given
translation was used in the context of semantic differencing for ADs [MRR11].
We define the translation and give two examples, showing ADs and their complete
representation in SMV.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2358</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2358</id><created>2014-09-08</created><authors><author><keyname>Haber</keyname><forenames>Arne</forenames></author><author><keyname>Rendel</keyname><forenames>Holger</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author><author><keyname>Schaefer</keyname><forenames>Ina</forenames></author></authors><title>Delta Modeling for Software Architectures</title><categories>cs.SE</categories><comments>10 pages, 6 figures. &gt; Tagungsband des Dagstuhl-Workshop MBEES:
  Modellbasierte Entwicklung eingebetteter Systeme VII, fortiss GmbH M\&quot;unchen,
  February 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Architectural modeling is an integral part of modern software development. In
particular, diverse systems benefit from precise architectural models since
similar components can often be reused between different system variants.
However, during all phases of diverse system development, system variability
has to be considered and modeled by appropriate means. Delta modeling is a
language-independent approach for modeling system variability. A set of diverse
systems is represented by a core system and a set of deltas specifying
modifications to the core system. In this paper, we give a first sketch of how
to apply delta modeling in MontiArc, an existing architecture description
language, in order to obtain an integrated modeling language for architectural
variability. The developed language, MontiArc, allows the modular modeling of
variable software architectures and supports proactive as well as extractive
product line development.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2359</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2359</id><created>2014-09-08</created><authors><author><keyname>Sprinkle</keyname><forenames>Jonathan</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author><author><keyname>Vangheluwe</keyname><forenames>Hans</forenames></author><author><keyname>Karsai</keyname><forenames>Gabor</forenames></author></authors><title>Metamodelling: State of the Art and Research Challenges</title><categories>cs.SE</categories><comments>20 pages, 3 figures</comments><journal-ref>MBEERTS: Model-Based Engineering of Embedded Real-Time Systems, p.
  57 - 76. International Dagstuhl Workshop, Dagstuhl Castle, Germany, LNCS
  6100, Springer Berlin, October 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This chapter discusses the current state of the art, and emerging research
challenges, for metamodelling. In the state-of-the-art review on metamodelling,
we review approaches, abstractions, and tools for metamodelling, evaluate them
with respect to their expressivity, investigate what role(s) metamodels may
play at run-time and how semantics can be assigned to metamodels and the domain
specific modeling languages they could define. In the emerging challenges
section on metamodelling we highlight research issues regarding the management
of complexity, consistency, and evolution of metamodels, and how the semantics
of metamodels impacts each of these.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2361</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2361</id><created>2014-09-08</created><authors><author><keyname>Levendovszky</keyname><forenames>Tihamer</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author><author><keyname>Sch&#xe4;tz</keyname><forenames>Bernhard</forenames></author><author><keyname>Sprinkle</keyname><forenames>Jonathan</forenames></author></authors><title>Model Evolution and Management</title><categories>cs.SE</categories><comments>30 pages , 4 figures</comments><journal-ref>MBEERTS: Model-Based Engineering of Embedded Real-Time Systems, p.
  241 - 270. International Dagstuhl Workshop, Dagstuhl Castle, Germany, LNCS
  6100, Springer Berlin, October 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As complex software and systems development projects need models as an
important planning, structuring and development technique, models now face
issues resolved for software earlier: models need to be versioned, differences
captured, syntactic and semantic correctness checked as early as possible,
documented, presented in easily accessible forms, etc. Quality management needs
to be established for models as well as their relationship to other models, to
code and to requirement documents precisely clarified and tracked. Business and
product requirements, product technologies as well as development tools evolve.
This also means we need evolutionary technologies both for models within a
language and if the language evolves also for an upgrade of the models. This
chapter discusses the state of the art in model management and evolution and
sketches what is still necessary for models to become as usable and used as
software.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2364</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2364</id><created>2014-09-08</created><authors><author><keyname>Plesser</keyname><forenames>Stefan</forenames></author><author><keyname>Fisch</keyname><forenames>M. Norbert</forenames></author><author><keyname>Pinkernell</keyname><forenames>Claas</forenames></author><author><keyname>Kurpck</keyname><forenames>Thomas</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>The Energy Navigator - A Web based Platform for functional Quality
  Mangement in Buildings</title><categories>cs.SE</categories><comments>7 pages, 5 figures.Proceedings of the 10th International Conference
  for Enhanced Building Operations (ICEBO' 10), Kuwait City, Kuwait, October
  2010. arXiv admin note: substantial text overlap with arXiv:1409.0416</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy efficient buildings require high quality standards for all their
technical equipment to enable their efficient and successful operation and
management. Building simulations enable engineers to design integrated HVAC
systems with complex building automation systems to control all their technical
functions. Numerous studies show that especially these supposedly innovative
buildings often do not reach their energy efficiency targets when in operation.
Key reasons for the suboptimal performance are imprecise functional
descriptions and a lack of commissioning and monitoring of the technical
systems that leave suboptimal operation undetected. In the research project
&quot;Energy Navigator&quot; we create a web-based platform that enables engineers to
create a comprehensive and precise functional description for the buildings
services. The system reuses this functional description - written in an
appropriate domain specific language - to control the building operation, to
signal malfunctions or faults, and in particular to measure energy efficiency
over time. The innovative approach of the platform is the combination of design
and control within one artifact linking the phases of design and operation and
improving the cost effectiveness for both services. The paper will describe the
concept of the platform, the technical innovation and first application
examples of the research project.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2365</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2365</id><created>2014-09-08</created><authors><author><keyname>Rons</keyname><forenames>Nadine</forenames></author></authors><title>Investigation of Partition Cells as a Structural Basis Suitable for
  Assessments of Individual Scientists</title><categories>cs.DL</categories><comments>9 pages, 3 figures</comments><journal-ref>Proceedings of the science and technology indicators conference
  2014 Leiden 'Context Counts: Pathways to Master Big and Little Data', 3-5
  September 2014, Leiden, the Netherlands, Ed Noyons (Ed.), 463-472</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Individual, excellent scientists have become increasingly important in the
research funding landscape. Accurate bibliometric measures of an individual's
performance could help identify excellent scientists, but still present a
challenge. One crucial aspect in this respect is an adequate delineation of the
sets of publications that determine the reference values to which a scientist's
publication record and its citation impact should be compared. The structure of
partition cells formed by intersecting fixed subject categories in a database
has been proposed to approximate a scientist's specialty more closely than can
be done with the broader subject categories. This paper investigates this cell
structure's suitability as an underlying basis for methodologies to assess
individual scientists, from two perspectives: (1) Proximity to the actual
structure of publication records of individual scientists: The distribution and
concentration of publications over the highly fragmented structure of partition
cells are examined for a sample of ERC grantees; (2) Proximity to customary
levels of accuracy: Differences in commonly used reference values (mean
expected number of citations per publication, and threshold number of citations
for highly cited publications) between adjacent partition cells are compared to
differences in two other dimensions: successive publication years and
successive citation window lengths. Findings from both perspectives are in
support of partition cells rather than the larger subject categories as a
journal based structure on which to construct and apply methodologies for the
assessment of highly specialized publication records such as those of
individual scientists.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2366</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2366</id><created>2014-09-08</created><authors><author><keyname>Gr&#xf6;nninger</keyname><forenames>Hans</forenames></author><author><keyname>Rei&#xdf;</keyname><forenames>Dirk</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>Towards a Semantics of Activity Diagrams with Semantic Variation Points</title><categories>cs.SE</categories><comments>15 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  UML activity diagrams have become an established notation to model control
and data ow on various levels of abstraction, ranging from fine-grained
descriptions of algorithms to high-level workflow models in business
applications. A formal semantics has to capture the flexibility of the
interpretation of activity diagrams in real systems, which makes it
inappropriate to define a fixed formal semantics. In this paper, we define a
semantics with semantic variation points that allow for a customizable,
application-specific interpretation of activity diagrams. We examine concrete
variants of the activity diagram semantics which may also entail variants of
the syntax re ecting the intended use at hand.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2367</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2367</id><created>2014-09-08</created><authors><author><keyname>Krahn</keyname><forenames>Holger</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author><author><keyname>V&#xf6;lkel</keyname><forenames>Stefan</forenames></author></authors><title>MontiCore: a Framework for Compositional Development of Domain Specific
  Languages</title><categories>cs.SE</categories><comments>20 pages, 6 figures</comments><journal-ref>International Journal on Software Tools for Technology Transfer
  (STTT), Volume 12, Issue 5, pp. 353-372, September 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Domain specific languages (DSLs) are increasingly used today. Coping with
complex language definitions, evolving them in a structured way, and ensuring
their error freeness are the main challenges of DSL design and implementation.
The use of modular language definitions and composition operators are therefore
inevitable in the independent development of language components. In this
article, we discuss these arising issues by describing a framework for the
compositional development of textual DSLs and their supporting tools. We use a
redundance-free definition of a readable concrete syntax and a comprehensible
abstract syntax as both representations significantly overlap in their
structure. For enhancing the usability of the abstract syntax, we added
concepts like associations and inheritance to a grammar- based definition in
order to build up arbitrary graphs (as known from metamodeling). Two modularity
concepts, grammar inheritance and embedding, are discussed. They permit
compositional language definition and thus simplify the extension of languages
based on already existing ones. We demonstrate that compositional engineering
of new languages is a useful concept when project-individual DSLs with
appropriate tool support are defined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2368</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2368</id><created>2014-09-08</created><authors><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author><author><keyname>Schindler</keyname><forenames>Martin</forenames></author><author><keyname>V&#xf6;lkel</keyname><forenames>Steven</forenames></author><author><keyname>Weisem&#xf6;ller</keyname><forenames>Ingo</forenames></author></authors><title>Generative Software Development</title><categories>cs.SE</categories><comments>2 pages, 8 figures</comments><journal-ref>Proceedings of the 32nd International Conference on Software
  Engineering (ICSE 2010), Volume 2, pp. 473-474 (tutorial summary), Cape Town,
  South Africa, May 2010. ACM, 2010</journal-ref><doi>10.1145/1810295.1810436</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Generation of software from modeling languages such as UML and domain
specific languages (DSLs) has become an important paradigm in software
engineering. In this contribution, we present some positions on software
development in a model based, generative manner based on home grown DSLs as
well as the UML. This includes development of DSLs as well as development of
models in these languages in order to generate executable code, test cases or
models in different languages. Development of formal DSLs contains concepts of
meta-models or grammars (syntax), context conditions (static analysis and
quality assurance) as well as possibilities to define the semantics of a
language. The growing number and complexity of DSLs is addressed by concepts
for the modular and compositional development of languages and their tools.
Moreover, we introduce approaches to code generation and model transformation.
Finally, we give an overview of the relevance of DSLs for various steps of
software development processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2370</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2370</id><created>2014-09-08</created><authors><author><keyname>Dytso</keyname><forenames>Alex</forenames></author><author><keyname>Rini</keyname><forenames>Stefano</forenames></author><author><keyname>Devroye</keyname><forenames>Natasha</forenames></author><author><keyname>Tuninetti</keyname><forenames>Daniela</forenames></author></authors><title>On the Capacity Region of the Two-user Interference Channel with a
  Cognitive Relay</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers a variation of the classical two-user interference
channel where the communication of two interfering source-destination pairs is
aided by an additional node that has a priori knowledge of the messages to be
transmitted, which is referred to as the it cognitive relay. For this
Interference Channel with a Cognitive Relay (ICCR) In particular, for the class
of injective semi-deterministic ICCRs, a sum-rate upper bound is derived for
the general memoryless ICCR and further tightened for the Linear Deterministic
Approximation (LDA) of the Gaussian noise channel at high SNR, which disregards
the noise and focuses on the interaction among the users' signals. The capacity
region of the symmetric LDA is completely characterized except for the regime
of moderately weak interference and weak links from the CR to the destinations.
The insights gained from the analysis of the LDA are then translated back to
the symmetric Gaussian noise channel (GICCR). For the symmetric GICCR, an
approximate characterization (to within a constant gap) of the capacity region
is provided for a parameter regime where capacity was previously unknown. The
approximately optimal scheme suggests that message cognition at a relay is
beneficial for interference management as it enables simultaneous over the air
neutralization of the interference at both destinations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2373</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2373</id><created>2014-09-08</created><authors><author><keyname>Biermeyer</keyname><forenames>Jan O.</forenames></author><author><keyname>Templeton</keyname><forenames>Todd R.</forenames></author><author><keyname>Berger</keyname><forenames>Christian</forenames></author><author><keyname>Gonzalez</keyname><forenames>Humberto</forenames></author><author><keyname>Naikal</keyname><forenames>Nikhil</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author><author><keyname>Sastry</keyname><forenames>S. Shankar</forenames></author></authors><title>Rapid Integration and Calibration of New Sensors Using the Berkeley
  Aachen Robotics Toolkit (BART)</title><categories>cs.SE cs.RO</categories><comments>17 pages, 5 figures. Proceedings des 11. Braunschweiger Symposiums
  &quot;Automatisierungssysteme, Assistenzsysteme und eingebettete Systeme f\&quot;ur
  Transportmittel&quot;, ITS Niedersachsen, Braunschweig, 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  After the three DARPA Grand Challenge contests many groups around the world
have continued to actively research and work toward an autonomous vehicle
capable of accomplishing a mission in a given context (e.g. desert, city) while
following a set of prescribed rules, but none has been completely successful in
uncontrolled environments, a task that many people trivially fulfill every day.
We believe that, together with improving the sensors used in cars and the
artificial intelligence algorithms used to process the information, the
community should focus on the systems engineering aspects of the problem, i.e.
the limitations of the car (in terms of space, power, or heat dissipation) and
the limitations of the software development cycle. This paper explores these
issues and our experiences overcoming them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2375</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2375</id><created>2014-09-08</created><authors><author><keyname>Haber</keyname><forenames>Arne</forenames></author><author><keyname>Ringert</keyname><forenames>Jan Oliver</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>Towards Architectural Programming of Embedded Systems</title><categories>cs.SE</categories><comments>10 pages, 6 figures. Tagungsband des Dagstuhl-Workshop MBEES:
  Modellbasierte Entwicklung eingebetteter Systeme VI. Informatik-Bericht
  2010-01, Institut f\&quot;ur Software Systems Engineering, TU Braunschweig, 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Integrating architectural elements with a modern programming language is
essential to ensure a smooth combination of architectural design and
programming. In this position statement, we motivate a combination of
architectural description for distributed, asynchronously communicating systems
and Java as an example for such an integration. The result is an ordinary
programming language, that exhibits architecture, data structure and behavior
within one view. Mappings or tracing between different views is unnecessary. A
prototypical implementation of a compiler demonstrates the possibilities and
challenges of architectural programming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2376</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2376</id><created>2014-09-08</created><authors><author><keyname>Berger</keyname><forenames>Christian</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author><author><keyname>V&#xf6;lkel</keyname><forenames>Steven</forenames></author></authors><title>Extensible Validation Framework for DSLs using MontiCore on the Example
  of Coding Guidelines</title><categories>cs.SE</categories><comments>13 pages, 7 figures. Symposium on Automotive/Avionics Systems
  Engineering 2009 (SAASE 09). San Diego, California, USA, October 2009</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unit test environments are today's state of the art for many programming
languages to keep the software's quality above a certain level. However, the
software's syntactic quality necessary for the developers themselves is not
covered by the aforementioned frameworks. This paper presents a tool realized
using the DSL framework MontiCore for automatically validating easily
extensible coding guidelines for any domain specific language or even general
purpose languages like C++ and its application in an automotive R&amp;D project
where a German OEM and several suppliers were involved. Moreover, it was
exemplary applied on UML/P-based sequence charts as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2377</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2377</id><created>2014-09-08</created><authors><author><keyname>Berger</keyname><forenames>Christian</forenames></author><author><keyname>G&#xfc;lke</keyname><forenames>Tim</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>ProcDSL + ProcEd - a Web-based Editing Solution for Domain Specific
  Process-Engineering</title><categories>cs.SE</categories><comments>Proceedings of the 9th OOPSLA Workshop on Domain-Specific Modeling
  (DSM' 09). Helsinki School of Economics. TR no B-108. Orlando, Florida, USA,
  October 2009</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a high-tech country products are becoming rapidly more complex. To manage
the development process as well as to encounter unforeseen challenges, the
understanding and thus the explicit modeling of organizational workflows is
more important than ever. However, available tools to support this work, in
most cases force a new notation upon the company or cannot be adapted to a
given publication layout in a reasonable amount of time. Additionally,
collaboration among colleagues as well as different business units is
complicated and less supported. Since it is of vital importance for a company
to be able to change its processes fast and adapt itself to new market
situations, the need for tools supporting this evolution is equally crucial. In
this paper we present a domain specific language (DSL) developed for modeling a
company's workflows. Furthermore, the DSL is embedded in a web-based editor
providing transparent access using modern web 2.0 technologies. Results of the
DSL's as well as the editor's application to document, model, and improve
selected workflows of a German automotive manufacturer are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2378</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2378</id><created>2014-09-08</created><authors><author><keyname>Karsai</keyname><forenames>Gabor</forenames></author><author><keyname>Krahn</keyname><forenames>Holger</forenames></author><author><keyname>Pinkernell</keyname><forenames>Claas</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author><author><keyname>Schindler</keyname><forenames>Martin</forenames></author><author><keyname>V&#xf6;lkel</keyname><forenames>Steven</forenames></author></authors><title>Design Guidelines for Domain Specific Languages</title><categories>cs.SE</categories><comments>7 pages, 5 figures. Proceedings of the 9th OOPSLA Workshop on
  Domain-Specific Modeling (DSM' 09). Helsinki School of Economics. TR no
  B-108. Orlando, Florida, USA, October 2009</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Designing a new domain specific language is as any other complex task
sometimes error-prone and usually time consuming, especially if the language
shall be of high-quality and comfortably usable. Existing tool support focuses
on the simplification of technical aspects but lacks support for an enforcement
of principles for a good language design. In this paper we investigate
guidelines that are useful for designing domain specific languages, largely
based on our experience in developing languages as well as relying on existing
guidelines on general purpose (GPLs) and modeling languages. We defined
guidelines to support a DSL developer to achieve better quality of the language
design and a better acceptance among its users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2380</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2380</id><created>2014-09-08</created><authors><author><keyname>Dukaczewski</keyname><forenames>Michael</forenames></author><author><keyname>Reiss</keyname><forenames>Dirk</forenames></author><author><keyname>Stein</keyname><forenames>Mark</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>MontiWeb - Modular Development of Web Information Systems</title><categories>cs.SE</categories><comments>6 pages, 6 figures. Proceedings of the 9th OOPSLA Workshop on
  Domain-Specific Modeling (DSM' 09). Helsinki School of Economics. TR no
  B-108. Orlando, Florida, USA, October 2009</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The development process of web information systems is often tedious, error
prone and usually involves redundant steps of work. Therefore, it is rather
e_x000E_fficient to employ a modeldriven approach for the systematic aspects
that comprise such a system. This involves models for the data structure that
shall be handled by the system (here: class diagrams), various editable and
read-only presentations (views) on combinations and extractions of the
underlying data (here: a special view language) and ways to connect these views
and define data flow between them (here: activity diagrams). In this paper, we
present the MontiWeb approach to model and generate these aspects in a modular
manner by incorperating the MontiCore framework. Therefor we shortly introduce
the infrastructure that helps to develop modular systems. This involves the
whole development process from defining the modeling languages to final code
generation as well as all steps in between. We present the text-based class and
activity diagram languages as well as a view language that are used to model
our system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2383</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2383</id><created>2014-09-08</created><updated>2015-05-05</updated><authors><author><keyname>Liavas</keyname><forenames>Athanasios P.</forenames></author><author><keyname>Sidiropoulos</keyname><forenames>Nicholas D.</forenames></author></authors><title>Parallel Algorithms for Constrained Tensor Factorization via the
  Alternating Direction Method of Multipliers</title><categories>cs.NA cs.DC cs.IT math.IT</categories><comments>Submitted to the IEEE Transactions on Signal Processing</comments><doi>10.1109/TSP.2015.2454476</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tensor factorization has proven useful in a wide range of applications, from
sensor array processing to communications, speech and audio signal processing,
and machine learning. With few recent exceptions, all tensor factorization
algorithms were originally developed for centralized, in-memory computation on
a single machine; and the few that break away from this mold do not easily
incorporate practically important constraints, such as nonnegativity. A new
constrained tensor factorization framework is proposed in this paper, building
upon the Alternating Direction method of Multipliers (ADMoM). It is shown that
this simplifies computations, bypassing the need to solve constrained
optimization problems in each iteration; and it naturally leads to distributed
algorithms suitable for parallel implementation on regular high-performance
computing (e.g., mesh) architectures. This opens the door for many emerging big
data-enabled applications. The methodology is exemplified using nonnegativity
as a baseline constraint, but the proposed framework can more-or-less readily
incorporate many other types of constraints. Numerical experiments are very
encouraging, indicating that the ADMoM-based nonnegative tensor factorization
(NTF) has high potential as an alternative to state-of-the-art approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2388</identifier>
 <datestamp>2014-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2388</id><created>2014-09-08</created><authors><author><keyname>Look</keyname><forenames>Markus</forenames></author><author><keyname>Perez</keyname><forenames>Antonio Navarro</forenames></author><author><keyname>Ringert</keyname><forenames>Jan Oliver</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author><author><keyname>Wortmann</keyname><forenames>Andreas</forenames></author></authors><title>Black-box Integration of Heterogeneous Modeling Languages for
  Cyber-Physical Systems</title><categories>cs.SE cs.RO</categories><comments>6 pages, 4 figures. GEMOC Workshop 2013 - International Workshop on
  The Globalization of Modeling Languages, Miami, Florida (USA), Volume 1102 of
  CEUR Workshop Proceedings, CEUR-WS.org, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Robots belong to a class of Cyber-Physical Systems where complex software as
a mobile device has to full tasks in a complex environment. Modeling robotics
applications for analysis and code generation requires modeling languages for
the logical software architecture and the system behavior. The
MontiArcAutomaton modeling framework integrates six independently developed
modeling languages to model robotics applications: a component &amp; connector
architecture description language, automata, I/O tables, class diagrams, OCL,
and a Java DSL. We describe how we integrated these languages into
MontiArcAutomaton a-posteriori in a black-box integration fashion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2390</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2390</id><created>2014-09-08</created><authors><author><keyname>Menezes</keyname><forenames>Telmo</forenames></author><author><keyname>Roth</keyname><forenames>Camille</forenames></author></authors><title>Symbolic regression of generative network models</title><categories>cs.NE cs.SI physics.soc-ph</categories><doi>10.1038/srep06284</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Networks are a powerful abstraction with applicability to a variety of
scientific fields. Models explaining their morphology and growth processes
permit a wide range of phenomena to be more systematically analysed and
understood. At the same time, creating such models is often challenging and
requires insights that may be counter-intuitive. Yet there currently exists no
general method to arrive at better models. We have developed an approach to
automatically detect realistic decentralised network growth models from
empirical data, employing a machine learning technique inspired by natural
selection and defining a unified formalism to describe such models as computer
programs. As the proposed method is completely general and does not assume any
pre-existing models, it can be applied &quot;out of the box&quot; to any given network.
To validate our approach empirically, we systematically rediscover pre-defined
growth laws underlying several canonical network generation models and credible
laws for diverse real-world networks. We were able to find programs that are
simple enough to lead to an actual understanding of the mechanisms proposed,
namely for a simple brain and a social network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2391</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2391</id><created>2014-09-08</created><authors><author><keyname>Kogan</keyname><forenames>Dmitry</forenames></author><author><keyname>Krauthgamer</keyname><forenames>Robert</forenames></author></authors><title>Sketching Cuts in Graphs and Hypergraphs</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sketching and streaming algorithms are in the forefront of current research
directions for cut problems in graphs. In the streaming model, we show that
$(1-\epsilon)$-approximation for Max-Cut must use $n^{1-O(\epsilon)}$ space;
moreover, beating $4/5$-approximation requires polynomial space. For the
sketching model, we show that $r$-uniform hypergraphs admit a
$(1+\epsilon)$-cut-sparsifier (i.e., a weighted subhypergraph that
approximately preserves all the cuts) with $O(\epsilon^{-2} n (r+\log n))$
edges. We also make first steps towards sketching general CSPs (Constraint
Satisfaction Problems).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2398</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2398</id><created>2014-09-08</created><authors><author><keyname>Ordyniak</keyname><forenames>Sebastian</forenames></author><author><keyname>Popa</keyname><forenames>Alexandru</forenames></author></authors><title>A Parameterized Study of Maximum Generalized Pattern Matching Problems</title><categories>cs.CC cs.DS cs.FL</categories><comments>to appear in Proc. IPEC'14</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The generalized function matching (GFM) problem has been intensively studied
starting with [Ehrenfeucht and Rozenberg, 1979]. Given a pattern p and a text
t, the goal is to find a mapping from the letters of p to non-empty substrings
of t, such that applying the mapping to p results in t. Very recently, the
problem has been investigated within the framework of parameterized complexity
[Fernau, Schmid, and Villanger, 2013].
  In this paper we study the parameterized complexity of the optimization
variant of GFM (called Max-GFM), which has been introduced in [Amir and Nor,
2007]. Here, one is allowed to replace some of the pattern letters with some
special symbols &quot;?&quot;, termed wildcards or don't cares, which can be mapped to an
arbitrary substring of the text. The goal is to minimize the number of
wildcards used.
  We give a complete classification of the parameterized complexity of Max-GFM
and its variants under a wide range of parameterizations, such as, the number
of occurrences of a letter in the text, the size of the text alphabet, the
number of occurrences of a letter in the pattern, the size of the pattern
alphabet, the maximum length of a string matched to any pattern letter, the
number of wildcards and the maximum size of a string that a wildcard can be
mapped to.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2399</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2399</id><created>2014-09-08</created><authors><author><keyname>&#x10c;&#xe1;p</keyname><forenames>Michal</forenames></author><author><keyname>Nov&#xe1;k</keyname><forenames>Peter</forenames></author><author><keyname>Kleiner</keyname><forenames>Alexander</forenames></author><author><keyname>Seleck&#xfd;</keyname><forenames>Martin</forenames></author></authors><title>Prioritized Planning Algorithms for Trajectory Coordination of Multiple
  Mobile Robots</title><categories>cs.RO cs.AI cs.MA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An important capability of autonomous multi-robot systems is to prevent
collision among the individual robots. One approach to this problem is to plan
conflict-free trajectories and let each of the robots follow its pre-planned
trajectory. A widely used practical method for multi-robot trajectory planning
is prioritized planning, which has been shown to be effective in practice, but
is in general incomplete. Formal analysis of instances that are provably
solvable by prioritized planning is still missing. Moreover, prioritized
planning is a centralized algorithm, which may be in many situations
undesirable.
  In this paper we a) propose a revised version of prioritized planning and
characterize the class of instances that are provably solvable by the algorithm
and b) propose an asynchronous decentralized variant of prioritized planning,
which maintains the desirable properties of the centralized version and in the
same time exploits the distributed computational power of the individual
robots, which in most situations allows to find the joint trajectories faster.
  The experimental evaluation performed on real-world indoor maps shows that a)
the revised version of prioritized planning reliably solves a wide class of
instances on which both classical prioritized planning and popular reactive
technique ORCA fail and b) the asynchronous decentralized algorithm provides
solution faster than the previously proposed synchronized decentralized
algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2408</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2408</id><created>2014-09-08</created><authors><author><keyname>B&#xe9;rard</keyname><forenames>B&#xe9;atrice</forenames></author><author><keyname>Haddad</keyname><forenames>Serge</forenames></author><author><keyname>Jovanovi&#x107;</keyname><forenames>Aleksandra</forenames></author><author><keyname>Lime</keyname><forenames>Didier</forenames></author></authors><title>Interrupt Timed Automata with Auxiliary Clocks and Parameters</title><categories>cs.LO cs.FL</categories><comments>26 pages, 6 figures, extended version from Reachability Problems 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interrupt Timed Automata (ITA) is an expressive timed model, introduced to
take into account interruptions, according to levels. Due to this feature, this
formalism is incomparable with Timed Automata. However several decidability
results related to reachability and model checking have been obtained. We add
auxiliary clocks to ITA, thereby extending its expressive power while
preserving decidability of reachability. Moreover, we define a parametrized
version of ITA, with polynomials of parameters appearing in guards and updates.
While parametric reasoning is particularly relevant for timed models, it very
often leads to undecidability results. We prove that various reachability
problems, including &quot;robust&quot; reachability, are decidable for this model, and we
give complexity upper bounds for a fixed or variable number of clocks, levels
and parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2413</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2413</id><created>2014-08-24</created><authors><author><keyname>Rino</keyname><forenames>Franco</forenames></author></authors><title>Image processing</title><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Gabor filters can extract multi-orientation and multiscale features from face
images. Researchers have designed different ways to use the magnitude of the
filtered results for face recognition: Gabor Fisher classifier exploited only
the magnitude information of Gabor magnitude pictures (GMPs); Local Gabor
Binary Pattern uses only the gradient information. In this paper, we regard
GMPs as smooth surfaces. By completely describing the shape of GMPs, we get a
face representation method called Gabor Surface Feature (GSF). First, we
compute the magnitude, 1st and 2nd derivatives of GMPs, then binarize them and
transform them into decimal values. Finally we construct joint histograms and
use subspace methods for classification. Experiments on FERET, ORL and FRGC
1.0.4 database show the effectiveness of GSF.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2426</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2426</id><created>2014-09-08</created><authors><author><keyname>Yang</keyname><forenames>Jed</forenames></author></authors><title>Some NP-complete edge packing and partitioning problems in planar graphs</title><categories>math.CO cs.CC</categories><comments>6 pages, 2 figures</comments><msc-class>68Q17 (Primary) 05C70, 68R10 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph packing and partitioning problems have been studied in many contexts,
including from the algorithmic complexity perspective. Consider the packing
problem of determining whether a graph contains a spanning tree and a cycle
that do not share edges. Bern\'ath and Kir\'aly proved that this decision
problem is NP-complete and asked if the same result holds when restricting to
planar graphs. Similarly, they showed that the packing problem with a spanning
tree and a path between two distinguished vertices is NP-complete. They also
established the NP-completeness of the partitioning problem of determining
whether the edge set of a graph can be partitioned into a spanning tree and a
(not-necessarily spanning) tree. We prove that all three problems remain
NP-complete even when restricted to planar graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2432</identifier>
 <datestamp>2014-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2432</id><created>2014-09-08</created><authors><author><keyname>Gambs</keyname><forenames>Sebastien</forenames></author><author><keyname>Ranellucci</keyname><forenames>Samuel</forenames></author><author><keyname>Tapp</keyname><forenames>and Alain</forenames></author></authors><title>The Crypto-democracy and the Trustworthy</title><categories>cs.CR cs.CY</categories><comments>DPM 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the current architecture of the Internet, there is a strong asymmetry in
terms of power between the entities that gather and process personal data
(e.g., major Internet companies, telecom operators, cloud providers, ...) and
the individuals from which this personal data is issued. In particular,
individuals have no choice but to blindly trust that these entities will
respect their privacy and protect their personal data. In this position paper,
we address this issue by proposing an utopian crypto-democracy model based on
existing scientific achievements from the field of cryptography. More
precisely, our main objective is to show that cryptographic primitives,
including in particular secure multiparty computation, offer a practical
solution to protect privacy while minimizing the trust assumptions. In the
crypto-democracy envisioned, individuals do not have to trust a single physical
entity with their personal data but rather their data is distributed among
several institutions. Together these institutions form a virtual entity called
the Trustworthy that is responsible for the storage of this data but which can
also compute on it (provided first that all the institutions agree on this).
Finally, we also propose a realistic proof-of-concept of the Trustworthy, in
which the roles of institutions are played by universities. This
proof-of-concept would have an important impact in demonstrating the
possibilities offered by the crypto-democracy paradigm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2433</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2433</id><created>2014-09-08</created><authors><author><keyname>Kolokolova</keyname><forenames>Antonina</forenames></author><author><keyname>Nizamee</keyname><forenames>Renesa</forenames></author></authors><title>Approximating solution structure of the Weighted Sentence Alignment
  problem</title><categories>cs.CL cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the complexity of approximating solution structure of the bijective
weighted sentence alignment problem of DeNero and Klein (2008). In particular,
we consider the complexity of finding an alignment that has a significant
overlap with an optimal alignment. We discuss ways of representing the solution
for the general weighted sentence alignment as well as phrases-to-words
alignment problem, and show that computing a string which agrees with the
optimal sentence partition on more than half (plus an arbitrarily small
polynomial fraction) positions for the phrases-to-words alignment is NP-hard.
For the general weighted sentence alignment we obtain such bound from the
agreement on a little over 2/3 of the bits. Additionally, we generalize the
Hamming distance approximation of a solution structure to approximating it with
respect to the edit distance metric, obtaining similar lower bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2450</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2450</id><created>2014-09-08</created><authors><author><keyname>West</keyname><forenames>Robert</forenames></author><author><keyname>Paskov</keyname><forenames>Hristo S.</forenames></author><author><keyname>Leskovec</keyname><forenames>Jure</forenames></author><author><keyname>Potts</keyname><forenames>Christopher</forenames></author></authors><title>Exploiting Social Network Structure for Person-to-Person Sentiment
  Analysis</title><categories>cs.SI cs.CL physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Person-to-person evaluations are prevalent in all kinds of discourse and
important for establishing reputations, building social bonds, and shaping
public opinion. Such evaluations can be analyzed separately using signed social
networks and textual sentiment analysis, but this misses the rich interactions
between language and social context. To capture such interactions, we develop a
model that predicts individual A's opinion of individual B by synthesizing
information from the signed social network in which A and B are embedded with
sentiment analysis of the evaluative texts relating A to B. We prove that this
problem is NP-hard but can be relaxed to an efficiently solvable hinge-loss
Markov random field, and we show that this implementation outperforms text-only
and network-only versions in two very different datasets involving
community-level decision-making: the Wikipedia Requests for Adminship corpus
and the Convote U.S. Congressional speech corpus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2455</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2455</id><created>2014-09-08</created><authors><author><keyname>Shi</keyname><forenames>Mao</forenames></author></authors><title>Degree reduction of disk rational B\'ezier curves</title><categories>cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an algorithm for optimal multi-degree reduction of
rational disk B\'ezier curve in the $L^{2}$ norm. We start by introducing a
novel disk rational B\'ezier based on parallel projection, whose properties are
also discussed. Then we transform multi-degree reduction of the error radius
curve and the weight curve of the disk rational B\'ezier curve into solving a
constrained quadratic programming (QP) problem. Finally, applying weighted
least squares, we provide the optimal multi-degree reduced polynomial
approximation of the center curve of the original disk rational B\'ezier curve.
Also this paper gives error estimation for this algorithm, and shows some
numerical examples to illustrate the correctness and the validity of
theoretical reasoning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2456</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2456</id><created>2014-09-08</created><authors><author><keyname>Wylie</keyname><forenames>Tim</forenames></author><author><keyname>Zhu</keyname><forenames>Binhai</forenames></author></authors><title>Intermittent Map Matching with the Discrete Fr\'echet Distance</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we focus on the map matching problem where the goal is to find
a path through a planar graph such that the path through the vertices closely
matches a given polygonal curve. The map matching problem is usually approached
with the Fr\'echet distance matching the edges of the path as well. Here, we
formally define the discrete map matching problem based on the discrete
Fr\'echet distance. We then look at the complexities of some variations of the
problem which allow for vertices in the graph to be unique or reused, and
whether there is a bound on the length of the path or the number of vertices
from the graph used in the path. We prove several of these problems to be
NP-complete, and then conclude the paper with some open questions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2457</identifier>
 <datestamp>2014-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2457</id><created>2014-09-08</created><updated>2014-12-02</updated><authors><author><keyname>Fan</keyname><forenames>Chenglin</forenames></author><author><keyname>Filtser</keyname><forenames>Omrit</forenames></author><author><keyname>Katz</keyname><forenames>Matthew J.</forenames></author><author><keyname>Wylie</keyname><forenames>Tim</forenames></author><author><keyname>Zhu</keyname><forenames>Binhai</forenames></author></authors><title>On the Chain Pair Simplification Problem</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of efficiently computing and visualizing the structural
resemblance between a pair of protein backbones in 3D has led Bereg et al. to
pose the Chain Pair Simplification problem (CPS). In this problem, given two
polygonal chains $A$ and $B$ of lengths $m$ and $n$, respectively, one needs to
simplify them simultaneously, such that each of the resulting simplified
chains, $A'$ and $B'$, is of length at most $k$ and the discrete \frechet\
distance between $A'$ and $B'$ is at most $\delta$, where $k$ and $\delta$ are
given parameters.
  In this paper we study the complexity of CPS under the discrete \frechet\
distance (CPS-3F), i.e., where the quality of the simplifications is also
measured by the discrete \frechet\ distance. Since CPS-3F was posed in 2008,
its complexity has remained open. However, it was believed to be \npc, since
CPS under the Hausdorff distance (CPS-2H) was shown to be \npc. We first prove
that the weighted version of CPS-3F is indeed weakly \npc\, even on the line,
based on a reduction from the set partition problem. Then, we prove that CPS-3F
is actually polynomially solvable, by presenting an $O(m^2n^2\min\{m,n\})$ time
algorithm for the corresponding minimization problem. In fact, we prove a
stronger statement, implying, for example, that if weights are assigned to the
vertices of only one of the chains, then the problem remains polynomially
solvable. We also study a few less rigid variants of CPS and present efficient
solutions for them.
  Finally, we present some experimental results that suggest that (the
minimization version of) CPS-3F is significantly better than previous
algorithms for the motivating biological application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2459</identifier>
 <datestamp>2015-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2459</id><created>2014-09-08</created><updated>2015-03-17</updated><authors><author><keyname>Katz</keyname><forenames>Daniel J.</forenames></author><author><keyname>Langevin</keyname><forenames>Philippe</forenames></author></authors><title>Proof of a Conjectured Three-Valued Family of Weil Sums of Binomials</title><categories>math.NT cs.IT math.CO math.IT</categories><comments>19 pages</comments><msc-class>11T23, 11T24, 11L05, 11G20, 05C90</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider Weil sums of binomials of the form $W_{F,d}(a)=\sum_{x \in F}
\psi(x^d-a x)$, where $F$ is a finite field, $\psi\colon F\to {\mathbb C}$ is
the canonical additive character, $\gcd(d,|F^\times|)=1$, and $a \in F^\times$.
If we fix $F$ and $d$ and examine the values of $W_{F,d}(a)$ as $a$ runs
through $F^\times$, we always obtain at least three distinct values unless $d$
is degenerate (a power of the characteristic of $F$ modulo $|F^\times|$).
Choices of $F$ and $d$ for which we obtain only three values are quite rare and
desirable in a wide variety of applications. We show that if $F$ is a field of
order $3^n$ with $n$ odd, and $d=3^r+2$ with $4 r \equiv 1 \pmod{n}$, then
$W_{F,d}(a)$ assumes only the three values $0$ and $\pm 3^{(n+1)/2}$. This
proves the 2001 conjecture of Dobbertin, Helleseth, Kumar, and Martinsen. The
proof employs diverse methods involving trilinear forms, counting points on
curves via multiplicative character sums, divisibility properties of Gauss
sums, and graph theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2465</identifier>
 <datestamp>2014-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2465</id><created>2014-09-08</created><updated>2014-09-09</updated><authors><author><keyname>Rey-Otero</keyname><forenames>Ives</forenames></author><author><keyname>Delbracio</keyname><forenames>Mauricio</forenames></author><author><keyname>Morel</keyname><forenames>Jean-Michel</forenames></author></authors><title>Comparing Feature Detectors: A bias in the repeatability criteria, and
  how to correct it</title><categories>cs.CV</categories><comments>Fixed typo in affiliations</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most computer vision application rely on algorithms finding local
correspondences between different images. These algorithms detect and compare
stable local invariant descriptors centered at scale-invariant keypoints.
Because of the importance of the problem, new keypoint detectors and
descriptors are constantly being proposed, each one claiming to perform better
(or to be complementary) to the preceding ones. This raises the question of a
fair comparison between very diverse methods. This evaluation has been mainly
based on a repeatability criterion of the keypoints under a series of image
perturbations (blur, illumination, noise, rotations, homotheties, homographies,
etc). In this paper, we argue that the classic repeatability criterion is
biased towards algorithms producing redundant overlapped detections. To
compensate this bias, we propose a variant of the repeatability rate taking
into account the descriptors overlap. We apply this variant to revisit the
popular benchmark by Mikolajczyk et al., on classic and new feature detectors.
Experimental evidence shows that the hierarchy of these feature detectors is
severely disrupted by the amended comparator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2475</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2475</id><created>2014-09-08</created><authors><author><keyname>Hasan</keyname><forenames>Monowar</forenames></author><author><keyname>Hossain</keyname><forenames>Ekram</forenames></author></authors><title>Distributed Resource Allocation in 5G Cellular Networks</title><categories>cs.NI cs.IT math.IT</categories><comments>Book chapter in &quot;Towards 5G: Applications, Requirements and Candidate
  Technologies'', Wiley, 2015, Eds. Rath Vannithamby and Shilpa Telwar</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The 5G cellular wireless systems will have a multi-tier architecture
consisting of macrocells, different types of licensed small cells and D2D
networks to serve users with different quality-of-service (QoS) requirements in
a spectrum efficient manner. Distributed resource allocation and interference
management is one of the fundamental research challenges for such multi-tier
heterogeneous networks. In this chapter, we consider the radio resource
allocation problem in a multi-tier orthogonal frequency division multiple
access (OFDMA)-based cellular (e.g., 5G LTE-A) network. In particular, we
present three novel approaches for distributed resource allocation in such
networks utilizing the concepts of stable matching, factor-graph based message
passing, and distributed auction. We illustrate each of the modeling schemes
with respect to a practical radio resource allocation problem. In particular,
we consider a multi-tier network consisting a macro base station (MBS), a set
of small cell base stations (SBSs) and corresponding small cell user equipments
(SUEs), as well as D2D user equipments (DUEs). There is a common set of radio
resources (e.g., resource blocks [RBs]) available to the network tiers (e.g.,
MBS, SBSs and DUEs). The SUEs and DUEs use the available resources (e.g., RB
and power level) in an underlay manner as long as the interference caused to
the macro tier (e.g., macro user equipments [MUEs]) remains below a given
threshold. Followed by a brief theoretical overview of the modeling tools
(e.g., stable matching, message passing and auction algorithm), we present the
distributed solution approaches for the resource allocation problem in the
aforementioned network setup. We also provide a brief qualitative comparison in
terms of various performance metrics such as complexity, convergence, algorithm
overhead etc.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2484</identifier>
 <datestamp>2014-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2484</id><created>2014-09-08</created><authors><author><keyname>Ismaeel</keyname><forenames>Ayad Ghany</forenames></author><author><keyname>Yousif</keyname><forenames>Raghad Z.</forenames></author></authors><title>Simulate High Traffic and Effective Cost an Academic Kurdistan Network
  Based on DWDM using OPNET</title><categories>cs.NI</categories><comments>14 pages, 9 figures, 5 Tables</comments><journal-ref>European Academic Research Vol. II, Issue 6/ September 2014. Pages
  7608-7621</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Till now the academic institutes in Kurdistan region not connected via
network with each other and with research centers worldwide, this paper
suggests design an optical IP-Network for academic Kurdistan region based on
DWDM (Dense wavelength Division Multiplexer) backbone. The design has been
developed using minimum spanning tree between the main realistic locations for
campuses in each province were calculated using Prim's algorithm in province
level. This proposed network simulated using OPENT IT GURU, and then
information was collected using GPS and GIS about the campuses (to obtain
realistic locations) of each university in university level, finally unique
design model was assumed for each campus in campus level. Simulation the
proposed design shows results of delay, traffic sends, traffic receives;
utility, performance (packet/second) and throughput are measured in the case of
heavy load. Addition to that the hub based network has longer response time
than the switch based network. Key words: IP-Network, OPNET, DWDM, Prim's
algorithm, Intranet, GPS and GIS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2485</identifier>
 <datestamp>2014-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2485</id><created>2014-09-08</created><authors><author><keyname>Maoz</keyname><forenames>Shahar</forenames></author><author><keyname>Ringert</keyname><forenames>Jan Oliver</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>A Manifesto for Semantic Model Differencing</title><categories>cs.SE</categories><comments>10 pages, 7 figures. arXiv admin note: text overlap with
  arXiv:1409.2355, arXiv:1409.2352</comments><journal-ref>Proceedings Int. Workshop on Models and Evolution (ME'10),
  co-located with MoDELS'10. J. Dingel and A. Solberg (Eds.): MoDELS Workshops,
  LNCS 6627, pp. 194 - 203, 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Models are heavily used in software engineering and together with their
systems they evolve over time. Thus, managing their changes is an important
challenge for system maintainability. Existing approaches to model differencing
concentrate on heuristics matching between model elements and on finding and
presenting differences at a concrete or abstract syntactic level. While showing
some success, these approaches are inherently limited to comparing syntactic
structures. This paper is a manifesto for research on semantic model
differencing. We present our vision to develop semantic diff operators for
model comparisons: operators whose input consists of two models and whose
output is a set of diff witnesses, instances of one model that are not
instances of the other. In particular, if the models are syntactically
different but there are no diff witnesses, the models are semantically
equivalent. We demonstrate our vision using two concrete diff operators, for
class diagrams and for activity diagrams. We motivate the use of semantic diff
operators, brie y discuss the algorithms to compute them, list related
challenges, and show their application and potential use as new fundamental
building blocks for change management in model-driven engineering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2486</identifier>
 <datestamp>2014-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2486</id><created>2014-09-08</created><authors><author><keyname>Rehman</keyname><forenames>Saeed ur</forenames></author><author><keyname>Raja</keyname><forenames>Gulistan</forenames></author></authors><title>Performance Evaluation of HEVC over Broadband Networks</title><categories>cs.NI</categories><comments>IJCSI International Journal of Computer Science Issues, Vol. 11,
  Issue 4, No 1, July 2014 ISSN (Print): 1694-0814 | ISSN (Online): 1694-0784</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  High efficiency video coding is the current trend setting standard for coding
and transmission of video content. This standard has brought in many
improvements over its predecessor H264 standard. In this paper, performance
evaluation of transmitting HEVC content over the simulated network environment
for disaster and calamity stricken area has been under taken. In the simulation
the transmitted video sequences are subjected to various error models in NS3
simulator. The effects of speed and number of hosts on the jitter and delay
characteristics of the underlying network while transmitting flows of HEVC
based content has been evaluated. The extent and effect of network errors on
the quality of HEVC bit stream in terms PSNR has also been tested. The results
show that HEVC performs better for up to 0.001 percent network error, for up to
30 simultaneously transmitting nodes and for nodes travelling at speeds up to
100m/s.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2530</identifier>
 <datestamp>2014-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2530</id><created>2014-09-08</created><authors><author><keyname>AlJadda</keyname><forenames>Khalifeh</forenames></author><author><keyname>Korayem</keyname><forenames>Mohammed</forenames></author><author><keyname>Ortiz</keyname><forenames>Camilo</forenames></author><author><keyname>Russell</keyname><forenames>Chris</forenames></author><author><keyname>Bernal</keyname><forenames>David</forenames></author><author><keyname>Payson</keyname><forenames>Lamar</forenames></author><author><keyname>Brown</keyname><forenames>Scott</forenames></author><author><keyname>Grainger</keyname><forenames>Trey</forenames></author></authors><title>Augmenting recommendation systems using a model of semantically-related
  terms extracted from user behavior</title><categories>cs.IR</categories><comments>RecSys2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Common difficulties like the cold-start problem and a lack of sufficient
information about users due to their limited interactions have been major
challenges for most recommender systems (RS). To overcome these challenges and
many similar ones that result in low accuracy (precision and recall)
recommendations, we propose a novel system that extracts semantically-related
search keywords based on the aggregate behavioral data of many users. These
semantically-related search keywords can be used to substantially increase the
amount of knowledge about a specific user's interests based upon even a few
searches and thus improve the accuracy of the RS. The proposed system is
capable of mining aggregate user search logs to discover semantic relationships
between key phrases in a manner that is language agnostic, human
understandable, and virtually noise-free. These semantically related keywords
are obtained by looking at the links between queries of similar users which, we
believe, represent a largely untapped source for discovering latent semantic
relationships between search terms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2536</identifier>
 <datestamp>2014-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2536</id><created>2014-09-08</created><authors><author><keyname>Winter</keyname><forenames>Andreas</forenames></author></authors><title>Coding Theorem and Strong Converse for Quantum Channels</title><categories>quant-ph cs.IT math.IT</categories><comments>5+1 pp, IEEEtran. Posted now, after 15 years, in view of the recent
  interest in strong converses. Related work by Ogawa &amp; Nagaoka
  (quant-ph/9808063), and significant subsequent progress in quant-ph/0012127,
  quant-ph/0206186, arXiv:0903.2838, arXiv:0912.5537, arXiv:1106.3089,
  arXiv:1108.5327, arXiv:1205.1712, arXiv:1208.1478, arXiv:1301.4927,
  arXiv:1306.1586, arXiv:1310.7028, arXiv:1401.4161, arXiv:1402.5940,
  arXiv:1404.5940, arXiv:1405.1797, arXiv:1406.2946, arXiv:1408.3373,
  arXiv:1408.5328</comments><journal-ref>IEEE Trans. Inf. Theory 45(7):2481-2485 (1999)</journal-ref><doi>10.1109/18.796385</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this correspondence we present a new proof of Holevo's coding theorem for
transmitting classical information through quantum channels, and its strong
converse. The technique is largely inspired by Wolfowitz's combinatorial
approach using types of sequences. As a by-product of our approach which is
independent of previous ones, both in the coding theorem and the converse, we
can give a new proof of Holevo's information bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2549</identifier>
 <datestamp>2014-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2549</id><created>2014-09-08</created><authors><author><keyname>Parsaeefard</keyname><forenames>Saeedeh</forenames></author><author><keyname>Le-Ngoc</keyname><forenames>Tho</forenames></author></authors><title>Full-Duplex Relay with Jamming Protocol for Improving Physical-Layer
  Security</title><categories>cs.IT math.IT</categories><comments>PIMRC 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a jointly cooperative relay and jamming protocol based on
full-duplex (FD) capable relay to increase the source-destination secrecy rate
in the presence of different types of eavesdroppers. In this so called
\textit{FD-Relay with jamming (FDJ)} protocol, the FD-Relay, first,
simultaneously receives data and sends jamming to the eavesdropper, and, then,
forwards the data, while the source jams the eavesdropper. Achievable secrecy
rates of the proposed FDJ in presence of different eavesdropper types and
self-interference (SI) are derived and compared with those of the traditional
half-duplex (HD) relay. The adaptive power allocation for secrecy rate
maximization in a multi-carrier scenario for both proposed FDJ and HD-Relay is
formulated as a non-convex optimization problem and corresponding iterative
solution algorithm is developed using the difference-of-two-concave-functions
(DC) programming technique. The simulation results confirm that FDJ offers
significant improvements in the secrecy rate over the HD-Relay.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2552</identifier>
 <datestamp>2014-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2552</id><created>2014-09-08</created><updated>2014-10-02</updated><authors><author><keyname>Li</keyname><forenames>Yan</forenames></author></authors><title>Sparse Additive Model using Symmetric Nonnegative Definite Smoothers</title><categories>stat.ML cs.LG</categories><comments>This is a term project report and has been withdrawn by the authors;
  arXiv admin note: author list has been modified due to misrepresentation of
  authorship</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new algorithm, called adaptive sparse backfitting algorithm,
for solving high dimensional Sparse Additive Model (SpAM) utilizing symmetric,
non-negative definite smoothers. Unlike the previous sparse backfitting
algorithm, our method is essentially a block coordinate descent algorithm that
guarantees to converge to the optimal solution. It bridges the gap between the
population backfitting algorithm and that of the data version. We also prove
variable selection consistency under suitable conditions. Numerical studies on
both synthesis and real data are conducted to show that adaptive sparse
backfitting algorithm outperforms previous sparse backfitting algorithm in
fitting and predicting high dimensional nonparametric models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2553</identifier>
 <datestamp>2014-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2553</id><created>2014-09-08</created><authors><author><keyname>Chodpathumwan</keyname><forenames>Yodsawalai</forenames><affiliation>University of Illinois at Urbana-Champaign</affiliation></author><author><keyname>Picado</keyname><forenames>Jose</forenames><affiliation>Oregon State University</affiliation></author><author><keyname>Termehchy</keyname><forenames>Arash</forenames><affiliation>Oregon State University</affiliation></author><author><keyname>Fern</keyname><forenames>Alan</forenames><affiliation>Oregon State University</affiliation></author><author><keyname>Sun</keyname><forenames>Yizhou</forenames><affiliation>Northeaster University</affiliation></author></authors><title>Representation Independent Analytics Over Structured Data</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Database analytics algorithms leverage quantifiable structural properties of
the data to predict interesting concepts and relationships. The same
information, however, can be represented using many different structures and
the structural properties observed over particular representations do not
necessarily hold for alternative structures. Thus, there is no guarantee that
current database analytics algorithms will still provide the correct insights,
no matter what structures are chosen to organize the database. Because these
algorithms tend to be highly effective over some choices of structure, such as
that of the databases used to validate them, but not so effective with others,
database analytics has largely remained the province of experts who can find
the desired forms for these algorithms. We argue that in order to make database
analytics usable, we should use or develop algorithms that are effective over a
wide range of choices of structural organizations. We introduce the notion of
representation independence, study its fundamental properties for a wide range
of data analytics algorithms, and empirically analyze the amount of
representation independence of some popular database analytics algorithms. Our
results indicate that most algorithms are not generally representation
independent and find the characteristics of more representation independent
heuristics under certain representational shifts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2559</identifier>
 <datestamp>2014-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2559</id><created>2014-09-08</created><updated>2014-12-01</updated><authors><author><keyname>Fujiwara</keyname><forenames>Yuichiro</forenames></author></authors><title>Ability of stabilizer quantum error correction to protect itself from
  its own imperfection</title><categories>quant-ph cs.IT math.IT</categories><comments>9 pages, 3 tables, final accepted version for publication in Physical
  Review A (v2: improved main theorem, slightly expanded each section,
  reformatted for readability, v3: corrected an error and typos in the proof of
  Theorem 2, v4: edited language)</comments><journal-ref>Physical Review A, 90 (2014) 062304</journal-ref><doi>10.1103/PhysRevA.90.062304</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The theory of stabilizer quantum error correction allows us to actively
stabilize quantum states and simulate ideal quantum operations in a noisy
environment. It is critical is to correctly diagnose noise from its syndrome
and nullify it accordingly. However, hardware that performs quantum error
correction itself is inevitably imperfect in practice. Here, we show that
stabilizer codes possess a built-in capability of correcting errors not only on
quantum information but also on faulty syndromes extracted by themselves.
Shor's syndrome extraction for fault-tolerant quantum computation is naturally
improved. This opens a path to realizing the potential of stabilizer quantum
error correction hidden within an innocent looking choice of generators and
stabilizer operators that have been deemed redundant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2574</identifier>
 <datestamp>2014-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2574</id><created>2014-09-08</created><updated>2014-11-19</updated><authors><author><keyname>Hershey</keyname><forenames>John R.</forenames></author><author><keyname>Roux</keyname><forenames>Jonathan Le</forenames></author><author><keyname>Weninger</keyname><forenames>Felix</forenames></author></authors><title>Deep Unfolding: Model-Based Inspiration of Novel Deep Architectures</title><categories>cs.LG cs.NE stat.ML</categories><comments>Added sections on reducing belief propagation to network activation
  functions, and on conversion between conventional network parameters and BP
  potentials for binary MRFs. Some bugs and typos were also fixed, and notation
  made a bit clearer</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Model-based methods and deep neural networks have both been tremendously
successful paradigms in machine learning. In model-based methods, problem
domain knowledge can be built into the constraints of the model, typically at
the expense of difficulties during inference. In contrast, deterministic deep
neural networks are constructed in such a way that inference is
straightforward, but their architectures are generic and it is unclear how to
incorporate knowledge. This work aims to obtain the advantages of both
approaches. To do so, we start with a model-based approach and an associated
inference algorithm, and \emph{unfold} the inference iterations as layers in a
deep network. Rather than optimizing the original model, we \emph{untie} the
model parameters across layers, in order to create a more powerful network. The
resulting architecture can be trained discriminatively to perform accurate
inference within a fixed network size. We show how this framework allows us to
interpret conventional networks as mean-field inference in Markov random
fields, and to obtain new architectures by instead using belief propagation as
the inference algorithm. We then show its application to a non-negative matrix
factorization model that incorporates the problem-domain knowledge that sound
sources are additive. Deep unfolding of this model yields a new kind of
non-negative deep neural network, that can be trained using a multiplicative
backpropagation-style update algorithm. We present speech enhancement
experiments showing that our approach is competitive with conventional neural
networks despite using far fewer parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2578</identifier>
 <datestamp>2014-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2578</id><created>2014-09-08</created><authors><author><keyname>Cetinkaya</keyname><forenames>Ahmet</forenames></author><author><keyname>Hayakawa</keyname><forenames>Tomohisa</forenames></author></authors><title>Feedback Control of Switched Stochastic Systems Using Randomly Available
  Active Mode Information</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Almost sure asymptotic stabilization of a discrete-time switched stochastic
system is investigated. Information on the active operation mode of the
switched system is assumed to be available for control purposes only at random
time instants. We propose a stabilizing feedback control framework that
utilizes the information obtained through mode observations. We first consider
the case where stochastic properties of mode observation instants are fully
known. We obtain sufficient asymptotic stabilization conditions for the
closed-loop switched stochastic system under our proposed control law. We then
explore the case where exact knowledge of the stochastic properties of mode
observation instants is not available. We present a set of alternative
stabilization conditions for this case. The results for both cases are
predicated on the analysis of a sequence-valued process that encapsulates the
stochastic nature of the evolution of active operation mode between mode
observation instants. Finally, we demonstrate the efficacy of our results with
numerical examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2579</identifier>
 <datestamp>2014-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2579</id><created>2014-09-09</created><authors><author><keyname>Feng</keyname><forenames>Ting-ting</forenames></author><author><keyname>Wu</keyname><forenames>Gang</forenames></author></authors><title>A theoretical contribution to the fast implementation of null linear
  discriminant analysis method using random matrix multiplication with scatter
  matrices</title><categories>cs.NA cs.CV cs.LG</categories><comments>7 pages</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The null linear discriminant analysis method is a competitive approach for
dimensionality reduction. The implementation of this method, however, is
computationally expensive. Recently, a fast implementation of null linear
discriminant analysis method using random matrix multiplication with scatter
matrices was proposed. However, if the random matrix is chosen arbitrarily, the
orientation matrix may be rank deficient, and some useful discriminant
information will be lost. In this paper, we investigate how to choose the
random matrix properly, such that the two criteria of the null LDA method are
satisfied theoretically. We give a necessary and sufficient condition to
guarantee full column rank of the orientation matrix. Moreover, the geometric
characterization of the condition is also described.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2585</identifier>
 <datestamp>2014-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2585</id><created>2014-09-09</created><authors><author><keyname>Skoumas</keyname><forenames>Georgios</forenames></author><author><keyname>Schmid</keyname><forenames>Klaus Arthur</forenames></author><author><keyname>Joss&#xe9;</keyname><forenames>Gregor</forenames></author><author><keyname>Z&#xfc;fle</keyname><forenames>Andreas</forenames></author><author><keyname>Nascimento</keyname><forenames>Mario A.</forenames></author><author><keyname>Renz</keyname><forenames>Matthias</forenames></author><author><keyname>Pfoser</keyname><forenames>Dieter</forenames></author></authors><title>Towards Knowledge-Enriched Path Computation</title><categories>cs.DB</categories><comments>Accepted as a short paper at ACM SIGSPATIAL GIS 2014</comments><acm-class>H.2.8</acm-class><doi>10.1145/2666310.2666485</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Directions and paths, as commonly provided by navigation systems, are usually
derived considering absolute metrics, e.g., finding the shortest path within an
underlying road network. With the aid of crowdsourced geospatial data we aim at
obtaining paths that do not only minimize distance but also lead through more
popular areas using knowledge generated by users. We extract spatial relations
such as &quot;nearby&quot; or &quot;next to&quot; from travel blogs, that define closeness between
pairs of points of interest (PoIs) and quantify each of these relations using a
probabilistic model. Subsequently, we create a relationship graph where each
node corresponds to a PoI and each edge describes the spatial connection
between the respective PoIs. Using Bayesian inference we obtain a probabilistic
measure of spatial closeness according to the crowd. Applying this measure to
the corresponding road network, we obtain an altered cost function which does
not exclusively rely on distance, and enriches an actual road networks taking
crowdsourced spatial relations into account. Finally, we propose two routing
algorithms on the enriched road networks. To evaluate our approach, we use
Flickr photo data as a ground truth for popularity. Our experimental results --
based on real world datasets -- show that the paths computed w.r.t.\ our
alternative cost function yield competitive solutions in terms of path length
while also providing more &quot;popular&quot; paths, making routing easier and more
informative for the user.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2590</identifier>
 <datestamp>2014-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2590</id><created>2014-09-09</created><authors><author><keyname>Alarte</keyname><forenames>Juli&#xe1;n</forenames><affiliation>Universitat Polit&#xe8;cnica de Val&#xe8;ncia, Valencia, Spain</affiliation></author><author><keyname>Insa</keyname><forenames>David</forenames><affiliation>Universitat Polit&#xe8;cnica de Val&#xe8;ncia, Valencia, Spain</affiliation></author><author><keyname>Silva</keyname><forenames>Josep</forenames><affiliation>Universitat Polit&#xe8;cnica de Val&#xe8;ncia, Valencia, Spain</affiliation></author><author><keyname>Tamarit</keyname><forenames>Salvador</forenames><affiliation>Universidad Polit&#xe9;cnica de Madrid, Madrid, Spain</affiliation></author></authors><title>Automatic Detection of Webpages that Share the Same Web Template</title><categories>cs.IR</categories><comments>In Proceedings WWV 2014, arXiv:1409.2294</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 163, 2014, pp. 2-15</journal-ref><doi>10.4204/EPTCS.163.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Template extraction is the process of isolating the template of a given
webpage. It is widely used in several disciplines, including webpages
development, content extraction, block detection, and webpages indexing. One of
the main goals of template extraction is identifying a set of webpages with the
same template without having to load and analyze too many webpages prior to
identifying the template. This work introduces a new technique to automatically
discover a reduced set of webpages in a website that implement the template.
This set is computed with an hyperlink analysis that computes a very small set
with a high level of confidence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2591</identifier>
 <datestamp>2014-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2591</id><created>2014-09-09</created><authors><author><keyname>Ramanujam</keyname><forenames>R.</forenames><affiliation>IMSc, Chennai</affiliation></author><author><keyname>Sheerazuddin</keyname><forenames>S.</forenames><affiliation>SSNCE, Chennai</affiliation></author></authors><title>A Local Logic for Realizability in Web Service Choreographies</title><categories>cs.LO cs.DC</categories><comments>In Proceedings WWV 2014, arXiv:1409.2294</comments><proxy>EPTCS</proxy><acm-class>F.4.1,H.3.5</acm-class><journal-ref>EPTCS 163, 2014, pp. 16-35</journal-ref><doi>10.4204/EPTCS.163.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Web service choreographies specify conditions on observable interactions
among the services. An important question in this regard is realizability:
given a choreography C, does there exist a set of service implementations I
that conform to C ? Further, if C is realizable, is there an algorithm to
construct implementations in I ? We propose a local temporal logic in which
choreographies can be specified, and for specifications in the logic, we solve
the realizability problem by constructing service implementations (when they
exist) as communicating automata. These are nondeterministic finite state
automata with a coupling relation. We also report on an implementation of the
realizability algorithm and discuss experimental results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2592</identifier>
 <datestamp>2014-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2592</id><created>2014-09-09</created><authors><author><keyname>Che</keyname><forenames>Yue Ling</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author><author><keyname>Gong</keyname><forenames>Yi</forenames></author><author><keyname>Duan</keyname><forenames>Lingjie</forenames></author></authors><title>On Spatial Capacity of Wireless Ad Hoc Networks with Threshold Based
  Scheduling</title><categories>cs.IT math.IT</categories><comments>28 pages,7 figures, submitted for possible journal publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies spatial capacity in a stochastic wireless ad hoc network,
where multi-stage probing and data transmission are sequentially performed. We
propose a novel signal-to-interference-ratio (SIR) threshold based scheduling
scheme, where by starting with the first probing, each transmitter iteratively
decides to further probe or stay idle, depending on whether the estimated SIR
in the proceeding probing is larger or smaller than a predefined threshold.
Although one can assume that the transmitters are initially deployed according
to a homogeneous Poisson point process (PPP), the SIR based scheduling makes
the PPP no longer applicable to model the locations of retained transmitters in
the subsequent probing and data transmission phases, due to the interference
induced coupling in their decisions. We first focus on single-stage probing and
find that when the SIR threshold is set sufficiently small to assure an
acceptable interference level in the network, the proposed scheme can greatly
outperform the non-scheduling reference scheme in terms of spatial capacity. We
clearly characterize the spatial capacity and obtain exact/approximate
closed-form expressions, by proposing a new approximate approach to deal with
the correlated SIR distributions over non-Poisson point processes. Then we
successfully extend to multi-stage probing by properly designing the multiple
SIR thresholds to assure gradual improvement of the spatial capacity.
Furthermore, we analyze the impact of multi-stage probing overhead and present
a probing-capacity tradeoff in scheduling design. Finally, extensive numerical
results are presented to demonstrate the performance of the proposed scheduling
as compared to existing schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2593</identifier>
 <datestamp>2014-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2593</id><created>2014-09-09</created><authors><author><keyname>Oliva</keyname><forenames>Paulo</forenames><affiliation>Queen Mary University of London</affiliation></author></authors><title>Proceedings Fifth International Workshop on Classical Logic and
  Computation</title><categories>cs.LO</categories><proxy>EPTCS</proxy><acm-class>F.4.1</acm-class><journal-ref>EPTCS 164, 2014</journal-ref><doi>10.4204/EPTCS.164</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Classical Logic and Computation (CL&amp;C) 2014 is the fifth edition of this
workshop series. The workshop series intends to cover all work aiming to
explore computational aspects of classical logic and mathematics. Its focus is
on the exploration of the computational content of mathematical and logical
principles, aiming to bring together researchers from both fields and exchange
ideas. In this fifth edition we received 18 submissions of both short and full
papers. Fourteen (14) of these were selected to present at the meeting in
Vienna, and six (6) full papers were accepted to appear at this ETPCS special
volume. Topics covered by this years submissions included: translations of
classical to intuitionistic proofs, witness extraction from classical proofs,
confluence properties for classical systems, linear logic, constructive
semantics for classical logic (game semantics, realizability), and the study of
calculi based on classical logic (lambda-mu-calculus, continuation calculus).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2594</identifier>
 <datestamp>2014-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2594</id><created>2014-09-09</created><authors><author><keyname>Petersen</keyname><forenames>Ian R.</forenames></author></authors><title>A Direct Coupling Coherent Quantum Observer for a Single Qubit Finite
  Level Quantum System</title><categories>quant-ph cs.SY math.OC</categories><comments>A preliminary version of this paper has been accepted to appear in
  the 2014 Australian Control Conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the problem of constructing a direct coupling quantum
observer for a single qubit finite level quantum system plant. The proposed
observer is a single mode linear quantum system which is shown to be able to
estimate one of the plant variables in a time averaged sense. A numerical
example and simulations are included to illustrate the properties of the
observer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2597</identifier>
 <datestamp>2014-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2597</id><created>2014-09-09</created><authors><author><keyname>Niazadeh</keyname><forenames>Rad</forenames></author><author><keyname>Yuan</keyname><forenames>Yang</forenames></author><author><keyname>Kleinberg</keyname><forenames>Robert D.</forenames></author></authors><title>Simple and Near-Optimal Mechanisms For Market Intermediation</title><categories>cs.GT</categories><comments>To appear in WINE'14, the 10th conference on Web and Internet
  Economics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A prevalent market structure in the Internet economy consists of buyers and
sellers connected by a platform (such as Amazon or eBay) that acts as an
intermediary and keeps a share of the revenue of each transaction. While the
optimal mechanism that maximizes the intermediary's profit in such a setting
may be quite complicated, the mechanisms observed in reality are generally much
simpler, e.g., applying an affine function to the price of the transaction as
the intermediary's fee. Loertscher and Niedermayer [2007] initiated the study
of such fee-setting mechanisms in two-sided markets, and we continue this
investigation by addressing the question of when an affine fee schedule is
approximately optimal for worst-case seller distribution. On one hand our work
supplies non-trivial sufficient conditions on the buyer side (i.e. linearity of
marginal revenue function, or MHR property of value and value minus cost
distributions) under which an affine fee schedule can obtain a constant
fraction of the intermediary's optimal profit for all seller distributions. On
the other hand we complement our result by showing that proper affine
fee-setting mechanisms (e.g. those used in eBay and Amazon selling plans) are
unable to extract a constant fraction of optimal profit in the worst-case
seller distribution. As subsidiary results we also show there exists a constant
gap between maximum surplus and maximum revenue under the aforementioned
conditions. Most of the mechanisms that we propose are also prior-independent
with respect to the seller, which signifies the practical implications of our
result.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2603</identifier>
 <datestamp>2014-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2603</id><created>2014-09-09</created><authors><author><keyname>Cheng</keyname><forenames>Zhanpeng</forenames></author><author><keyname>Eppstein</keyname><forenames>David</forenames></author></authors><title>Linear-time Algorithms for Proportional Apportionment</title><categories>cs.DS</categories><comments>13 pages, to appear in ISAAC 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The apportionment problem deals with the fair distribution of a discrete set
of $k$ indivisible resources (such as legislative seats) to $n$ entities (such
as parties or geographic subdivisions). Highest averages methods are a
frequently used class of methods for solving this problem. We present an
$O(n)$-time algorithm for performing apportionment under a large class of
highest averages methods. Our algorithm works for all highest averages methods
used in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2612</identifier>
 <datestamp>2014-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2612</id><created>2014-09-09</created><updated>2014-09-10</updated><authors><author><keyname>Balbiani</keyname><forenames>Philippe</forenames></author><author><keyname>van Ditmarsch</keyname><forenames>Hans</forenames></author></authors><title>A simple proof of the completeness of APAL</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a simple proof of the completeness of arbitrary public
announcement logic APAL. The proof is an improvement over the proof found in
the publication Knowable as Known after an Announcement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2620</identifier>
 <datestamp>2014-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2620</id><created>2014-09-09</created><authors><author><keyname>Gupta</keyname><forenames>Suyog</forenames></author><author><keyname>Sindhwani</keyname><forenames>Vikas</forenames></author><author><keyname>Gopalakrishnan</keyname><forenames>Kailash</forenames></author></authors><title>Learning Machines Implemented on Non-Deterministic Hardware</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper highlights new opportunities for designing large-scale machine
learning systems as a consequence of blurring traditional boundaries that have
allowed algorithm designers and application-level practitioners to stay -- for
the most part -- oblivious to the details of the underlying hardware-level
implementations. The hardware/software co-design methodology advocated here
hinges on the deployment of compute-intensive machine learning kernels onto
compute platforms that trade-off determinism in the computation for improvement
in speed and/or energy efficiency. To achieve this, we revisit digital
stochastic circuits for approximating matrix computations that are ubiquitous
in machine learning algorithms. Theoretical and empirical evaluation is
undertaken to assess the impact of the hardware-induced computational noise on
algorithm performance. As a proof-of-concept, a stochastic hardware simulator
is employed for training deep neural networks for image recognition problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2634</identifier>
 <datestamp>2014-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2634</id><created>2014-09-09</created><authors><author><keyname>G&#xfc;lke</keyname><forenames>Tim</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author><author><keyname>Jansen</keyname><forenames>Martin</forenames></author><author><keyname>Axmann</keyname><forenames>Joachim</forenames></author></authors><title>High-Level Requirements Management and Complexity Costs in Automotive
  Development Projects: A Problem Statement</title><categories>cs.SE</categories><comments>7 pages. Requirements Engineering: Foundation for Software Quality
  18th International Working Conference, Proceedings, REFSQ 2012, Essen,
  Germany, March 19-22, 2012</comments><doi>10.1007/978-3-642-28714-5_8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Effective requirements management plays an important role when it comes to
the support of product development teams in the automotive industry. A precise
positioning of new cars in the market is based on features and characteristics
described as requirements as well as on costs and profits. [Question/problem]
However, introducing or changing requirements does not only impact the product
and its parts, but may lead to overhead costs in the OEM due to increased
complexity. The raised overhead costs may well exceed expected gains or costs
from the changed requirements. [Principal ideas/results] By connecting
requirements with direct and overhead costs, decision making based on
requirements could become more valuable. [Contribution] This problem statement
results from a detailed examination of the effects of requirements management
practices on process complexity and vice versa as well as on how today's
requirements management tools assist in this respect. We present findings from
a joined research project of RWTH Aachen University and Volkswagen
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2650</identifier>
 <datestamp>2014-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2650</id><created>2014-09-09</created><authors><author><keyname>Sbeity</keyname><forenames>Ihab</forenames></author><author><keyname>Dbouk</keyname><forenames>Mohamed</forenames></author><author><keyname>Kobeissi</keyname><forenames>Habib</forenames></author></authors><title>Combining the analytical hierarchy process and the genetic algorithm to
  solve the timetable problem</title><categories>cs.AI cs.NE</categories><comments>International Journal of Software Engineering &amp; Applications (IJSEA),
  Vol.5, No.4, July 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main problems of school course timetabling are time, curriculum, and
classrooms. In addition there are other problems that vary from one institution
to another. This paper is intended to solve the problem of satisfying the
teachers preferred schedule in a way that regards the importance of the teacher
to the supervising institute, i.e. his score according to some criteria.
Genetic algorithm (GA) has been presented as an elegant method in solving
timetable problem (TTP) in order to produce solutions with no conflict. In this
paper, we consider the analytic hierarchy process (AHP) to efficiently obtain a
score for each teacher, and consequently produce a GA-based TTP solution that
satisfies most of the teachers preferences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2651</identifier>
 <datestamp>2014-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2651</id><created>2014-09-09</created><authors><author><keyname>Bessi</keyname><forenames>Alessandro</forenames></author><author><keyname>Caldarelli</keyname><forenames>Guido</forenames></author><author><keyname>Del Vicario</keyname><forenames>Michela</forenames></author><author><keyname>Scala</keyname><forenames>Antonio</forenames></author><author><keyname>Quattrociocchi</keyname><forenames>Walter</forenames></author></authors><title>Social determinants of content selection in the age of (mis)information</title><categories>cs.SI cs.CY physics.data-an physics.soc-ph</categories><comments>misinformation, collective narratives, crowd dynamics, information
  spreading</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite the enthusiastic rhetoric about the so called \emph{collective
intelligence}, conspiracy theories -- e.g. global warming induced by chemtrails
or the link between vaccines and autism -- find on the Web a natural medium for
their dissemination. Users preferentially consume information according to
their system of beliefs and the strife within users of opposite narratives may
result in heated debates. In this work we provide a genuine example of
information consumption from a sample of 1.2 million of Facebook Italian users.
We show by means of a thorough quantitative analysis that information
supporting different worldviews -- i.e. scientific and conspiracist news -- are
consumed in a comparable way by their respective users. Moreover, we measure
the effect of the exposure to 4709 evidently false information (satirical
version of conspiracy theses) and to 4502 debunking memes (information aiming
at contrasting unsubstantiated rumors) of the most polarized users of
conspiracy claims. We find that either contrasting or teasing consumers of
conspiracy narratives increases their probability to interact again with
unsubstantiated rumors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2655</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2655</id><created>2014-09-09</created><updated>2015-09-10</updated><authors><author><keyname>Mackey</keyname><forenames>Lester</forenames></author><author><keyname>Bryan</keyname><forenames>Jordan</forenames></author><author><keyname>Mo</keyname><forenames>Man Yue</forenames></author></authors><title>Weighted Classification Cascades for Optimizing Discovery Significance
  in the HiggsML Challenge</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a minorization-maximization approach to optimizing common
measures of discovery significance in high energy physics. The approach
alternates between solving a weighted binary classification problem and
updating class weights in a simple, closed-form manner. Moreover, an argument
based on convex duality shows that an improvement in weighted classification
error on any round yields a commensurate improvement in discovery significance.
We complement our derivation with experimental results from the 2014 Higgs
boson machine learning challenge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2666</identifier>
 <datestamp>2014-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2666</id><created>2014-09-09</created><authors><author><keyname>Nurdin</keyname><forenames>Hendra I.</forenames></author></authors><title>Quantum filtering for multiple input multiple output systems driven by
  arbitrary zero-mean jointly Gaussian input fields</title><categories>quant-ph cs.SY math-ph math.MP</categories><comments>19 pages, no figures. Published in a special issue of the Russian
  Journal of Mathematical Physics dedicated to the memory of Slava Belavkin</comments><journal-ref>Russian Journal of Mathematical Physics 21(3), pp. 386-398 (2014)</journal-ref><doi>10.1134/S106192081403011X</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we treat the quantum filtering problem for multiple input
multiple output (MIMO) Markovian open quantum systems coupled to multiple boson
fields in an arbitrary zero-mean jointly Gaussian state, using the reference
probability approach formulated by Bouten and van Handel as a quantum version
of a well-known method of the same name from classical nonlinear filtering
theory, and exploiting the generalized Araki-Woods representation of Gough.
This includes Gaussian field states such as vacuum, squeezed vacuum, thermal,
and squeezed thermal states as special cases. The contribution is a derivation
of the general quantum filtering equation (or stochastic master equation as
they are known in the quantum optics community) in the full MIMO setup for any
zero-mean jointy Gaussian input field states, up to some mild rank assumptions
on certain matrices relating to the measurement vector.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2668</identifier>
 <datestamp>2014-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2668</id><created>2014-09-06</created><authors><author><keyname>Melenhorst</keyname><forenames>Mark</forenames><affiliation>Delft University of Technology</affiliation></author><author><keyname>Blanco</keyname><forenames>Mar&#xed;a Men&#xe9;ndez</forenames><affiliation>University of Trento</affiliation></author><author><keyname>Larson</keyname><forenames>Martha</forenames><affiliation>Delft University of Technology</affiliation></author></authors><title>A Crowdsourcing Procedure for the Discovery of Non-Obvious Attributes of
  Social Image</title><categories>cs.IR cs.MM</categories><comments>6 pages, 3 figures, Extended version of paper to appear in CrowdMM
  2014: International ACM Workshop on Crowdsourcing for Multimedia</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Research on mid-level image representations has conventionally concentrated
relatively obvious attributes and overlooked non-obvious attributes, i.e.,
characteristics that are not readily observable when images are viewed
independently of their context or function. Non-obvious attributes are not
necessarily easily nameable, but nonetheless they play a systematic role in
people`s interpretation of images. Clusters of related non-obvious attributes,
called interpretation dimensions, emerge when people are asked to compare
images, and provide important insight on aspects of social images that are
considered relevant. In contrast to aesthetic or affective approaches to image
analysis, non-obvious attributes are not related to the personal perspective of
the viewer. Instead, they encode a conventional understanding of the world,
which is tacit, rather than explicitly expressed. This paper introduces a
procedure for discovering non-obvious attributes using crowdsourcing. We
discuss this procedure using a concrete example of a crowdsourcing task on
Amazon Mechanical Turk carried out in the domain of fashion. An analysis
comparing discovered non-obvious attributes with user tags demonstrated the
added value delivered by our procedure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2674</identifier>
 <datestamp>2014-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2674</id><created>2014-09-09</created><authors><author><keyname>Chaaban</keyname><forenames>Anas</forenames></author><author><keyname>Sezgin</keyname><forenames>Aydin</forenames></author></authors><title>When Can a Relay Reduce End-to-End Communication Delay?</title><categories>cs.IT math.IT</categories><comments>6 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The impact of relaying on the latency of communication in a relay channel is
studied. Both decode-forward (DF) and amplify-forward (AF) are considered, and
are compared with the point-to-point (P2P) scheme which does not use the relay.
The question as to whether DF and AF can decrease the latency of communicating
a number of bits with a given reliability requirement is addressed. Latency
expressions for the three schemes are derived. Although both DF and AF use a
block-transmission structure which sends the information over multiple
transmission blocks, they can both achieve latencies lower that P2P. Conditions
under which this occurs are obtained. Interestingly, these conditions are more
strict when compared to the conditions under which DF and AF achieve higher
information-theoretic rates than P2P.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2697</identifier>
 <datestamp>2014-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2697</id><created>2014-09-05</created><authors><author><keyname>Sahu</keyname><forenames>Sanjaya Kumar</forenames></author><author><keyname>Dixit</keyname><forenames>T. V.</forenames></author><author><keyname>Neema</keyname><forenames>D. D.</forenames></author></authors><title>Particle Swarm Optimized Fuzzy Controller for Indirect Vector Control of
  Multilevel Inverter Fed Induction Motor</title><categories>cs.NE</categories><comments>9 pages, published in Volume 11, issue 4, july 2014, IJCSI</comments><journal-ref>Volume 11, issue 4, july 2014, IJCSI</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Particle Swarm Optimized (PSO) fuzzy controller has been proposed for
indirect vector control of induction motor. In this proposed scheme a Neutral
Point Clamped (NPC) multilevel inverter is used and hysteresis current control
technique has been adopted for switching the IGBTs. A Mamdani type fuzzy
controller is used in place of conventional PI controller. To ensure better
performance of fuzzy controller all parameters such as membership functions,
normalizing and de-normalizing parameters are optimized using PSO. The
performance of proposed controller is investigated under various load and speed
conditions. The simulation results show its stability and robustness for high
performance derives applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2702</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2702</id><created>2014-09-09</created><authors><author><keyname>Setti</keyname><forenames>Francesco</forenames></author><author><keyname>Russell</keyname><forenames>Chris</forenames></author><author><keyname>Bassetti</keyname><forenames>Chiara</forenames></author><author><keyname>Cristani</keyname><forenames>Marco</forenames></author></authors><title>F-formation Detection: Individuating Free-standing Conversational Groups
  in Images</title><categories>cs.CV</categories><comments>32 pages, submitted to PLOS One</comments><doi>10.1371/journal.pone.0139160</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Detection of groups of interacting people is a very interesting and useful
task in many modern technologies, with application fields spanning from
video-surveillance to social robotics. In this paper we first furnish a
rigorous definition of group considering the background of the social sciences:
this allows us to specify many kinds of group, so far neglected in the Computer
Vision literature. On top of this taxonomy, we present a detailed state of the
art on the group detection algorithms. Then, as a main contribution, we present
a brand new method for the automatic detection of groups in still images, which
is based on a graph-cuts framework for clustering individuals; in particular we
are able to codify in a computational sense the sociological definition of
F-formation, that is very useful to encode a group having only proxemic
information: position and orientation of people. We call the proposed method
Graph-Cuts for F-formation (GCFF). We show how GCFF definitely outperforms all
the state of the art methods in terms of different accuracy measures (some of
them are brand new), demonstrating also a strong robustness to noise and
versatility in recognizing groups of various cardinality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2710</identifier>
 <datestamp>2014-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2710</id><created>2014-09-09</created><authors><author><keyname>Chennupati</keyname><forenames>Gopinath</forenames></author></authors><title>eAnt-Miner : An Ensemble Ant-Miner to Improve the ACO Classification</title><categories>cs.NE</categories><comments>13 pages, 2 figures, 6 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ant Colony Optimization (ACO) has been applied in supervised learning in
order to induce classification rules as well as decision trees, named
Ant-Miners. Although these are competitive classifiers, the stability of these
classifiers is an important concern that owes to their stochastic nature. In
this paper, to address this issue, an acclaimed machine learning technique
named, ensemble of classifiers is applied, where an ACO classifier is used as a
base classifier to prepare the ensemble. The main trade-off is, the predictions
in the new approach are determined by discovering a group of models as opposed
to the single model classification. In essence, we prepare multiple models from
the randomly replaced samples of training data from which, a unique model is
prepared by aggregating the models to test the unseen data points. The main
objective of this new approach is to increase the stability of the Ant-Miner
results there by improving the performance of ACO classification. We found that
the ensemble Ant-Miners significantly improved the stability by reducing the
classification error on unseen data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2711</identifier>
 <datestamp>2014-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2711</id><created>2014-09-09</created><authors><author><keyname>Bozzelli</keyname><forenames>Laura</forenames></author><author><keyname>Maubert</keyname><forenames>Bastien</forenames></author><author><keyname>Pinchinat</keyname><forenames>Sophie</forenames></author></authors><title>Unifying Hyper and Epistemic Temporal Logic</title><categories>cs.LO</categories><comments>12 pages, plus 35 pages of appendix. Submitted to FSTTCS 2014</comments><msc-class>03B70</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the literature, two powerful temporal logic formalisms have been proposed
for expressing information flow security requirements, that in general, go
beyond regular properties. One is classic, based on the knowledge modalities of
epistemic logic. The other one, the so called hyper logic, is more recent and
subsumes many proposals from the literature; it is based on explicit and
simultaneous quantification over multiple paths. In an attempt to better
understand how these logics compare with each other, we consider the logic
KCTL* (the extension of CTL* with knowledge modalities and synchronous perfect
recall semantics) and HyperCTL*. We first establish that KCTL* and HyperCTL*
are expressively incomparable. Second, we introduce and study a natural linear
past extension of HyperCTL* to unify KCTL* and HyperCTL*; indeed, we show that
KCTL* can be easily translated in linear time into the proposed logic.
Moreover, we show that the model-checking problem for this novel logic is
decidable, and we provide its exact computational complexity in terms of a new
measure of path quantifiers' alternation. For this, we settle open complexity
issues for unrestricted quantified propositional temporal logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2731</identifier>
 <datestamp>2014-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2731</id><created>2014-09-09</created><authors><author><keyname>Atserias</keyname><forenames>Albert</forenames></author><author><keyname>Lauria</keyname><forenames>Massimo</forenames></author><author><keyname>Nordstr&#xf6;m</keyname><forenames>Jakob</forenames></author></authors><title>Narrow Proofs May Be Maximally Long</title><categories>cs.CC cs.DM cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that there are 3-CNF formulas over n variables that can be refuted
in resolution in width w but require resolution proofs of size n^Omega(w). This
shows that the simple counting argument that any formula refutable in width w
must have a proof in size n^O(w) is essentially tight. Moreover, our lower
bound generalizes to polynomial calculus resolution (PCR) and Sherali-Adams,
implying that the corresponding size upper bounds in terms of degree and rank
are tight as well. Our results do not extend all the way to Lasserre, however,
where the formulas we study have proofs of constant rank and size polynomial in
both n and w.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2733</identifier>
 <datestamp>2015-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2733</id><created>2014-09-09</created><updated>2015-06-25</updated><authors><author><keyname>Atminas</keyname><forenames>Aistis</forenames></author><author><keyname>Kami&#x144;ski</keyname><forenames>Marcin</forenames></author><author><keyname>Raymond</keyname><forenames>Jean-Florent</forenames></author></authors><title>Scattered packings of cycles</title><categories>cs.DM cs.DS math.CO</categories><msc-class>05C38</msc-class><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem Scattered Cycles which, given a graph $G$ and two
positive integers $r$ and $\ell$, asks whether $G$ contains a collection of $r$
cycles that are pairwise at distance at least $\ell$. This problem generalizes
the problem Disjoint Cycles which corresponds to the case $\ell = 1$. We prove
that when parameterized by $r$, $\ell$, and the maximum degree $\Delta$, the
problem Scattered Cycles admits a kernel on $24 \ell^2 \Delta^\ell r \log(8
\ell^2 \Delta^\ell r)$ vertices. We also provide a $(16 \ell^2
\Delta^\ell)$-kernel for the case $r=2$ and a $(148 \Delta r \log r)$-kernel
for the case $\ell = 1$. Our proofs rely on two simple reduction rules and a
careful analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2752</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2752</id><created>2014-09-09</created><updated>2015-06-07</updated><authors><author><keyname>Makhzani</keyname><forenames>Alireza</forenames></author><author><keyname>Frey</keyname><forenames>Brendan</forenames></author></authors><title>Winner-Take-All Autoencoders</title><categories>cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a winner-take-all method for learning hierarchical
sparse representations in an unsupervised fashion. We first introduce
fully-connected winner-take-all autoencoders which use mini-batch statistics to
directly enforce a lifetime sparsity in the activations of the hidden units. We
then propose the convolutional winner-take-all autoencoder which combines the
benefits of convolutional architectures and autoencoders for learning
shift-invariant sparse representations. We describe a way to train
convolutional autoencoders layer by layer, where in addition to lifetime
sparsity, a spatial sparsity within each feature map is achieved using
winner-take-all activation functions. We will show that winner-take-all
autoencoders can be used to to learn deep sparse representations from the
MNIST, CIFAR-10, ImageNet, Street View House Numbers and Toronto Face datasets,
and achieve competitive classification performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2760</identifier>
 <datestamp>2014-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2760</id><created>2014-09-08</created><authors><author><keyname>Ivanova</keyname><forenames>Inga</forenames></author><author><keyname>Strand</keyname><forenames>Oivind</forenames></author><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author></authors><title>Synergy cycles in the Norwegian innovation system: The relation between
  synergy and cycle values</title><categories>cs.CY q-fin.EC</categories><comments>31 pages, 16 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The knowledge base of an economy measured in terms of Triple Helix relations
can be analyzed in terms of mutual information among geographical, sectorial,
and size distributions of firms as dimensions of the probabilistic entropy. The
resulting synergy values of a TH system provide static snapshots. In this
study, we add the time dimension and analyze the synergy dynamics using the
Norwegian innovation system as an example. The synergy among the three
dimensions can be mapped as a set of partial time series and spectrally
analyzed. The results suggest that the synergy at the level of both the country
and its 19 counties shoe non-chaotic oscillatory behavior and resonates in a
set of natural frequencies. That is, synergy surges and drops are non-random
and can be analyzed and predicted. There is a proportional dependence between
the amplitudes of oscillations and synergy values and an inverse proportional
dependence between the oscillation frequencies' relative inputs and synergy
values. This analysis of the data informs us that one can expect
frequency-related synergy-volatility growth in relation to the synergy value
and a shift in the synergy volatility towards the long-term fluctuations with
the synergy growth.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2762</identifier>
 <datestamp>2014-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2762</id><created>2014-09-09</created><authors><author><keyname>Karydi</keyname><forenames>Efthalia</forenames></author><author><keyname>Margaritis</keyname><forenames>Konstantinos G.</forenames></author></authors><title>Parallel and Distributed Collaborative Filtering: A Survey</title><categories>cs.IR cs.DC</categories><comments>46 pages</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Collaborative filtering is amongst the most preferred techniques when
implementing recommender systems. Recently, great interest has turned towards
parallel and distributed implementations of collaborative filtering algorithms.
This work is a survey of the parallel and distributed collaborative filtering
implementations, aiming not only to provide a comprehensive presentation of the
field's development, but also to offer future research orientation by
highlighting the issues that need to be further developed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2764</identifier>
 <datestamp>2014-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2764</id><created>2014-09-09</created><authors><author><keyname>Tsukada</keyname><forenames>Takeshi</forenames></author><author><keyname>Ong</keyname><forenames>C. -H. Luke</forenames></author></authors><title>Innocent Strategies are Sheaves over Plays---Deterministic,
  Non-deterministic and Probabilistic Innocence</title><categories>cs.PL</categories><acm-class>F.3.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although the HO/N games are fully abstract for PCF, the traditional notion of
innocence (which underpins these games) is not satisfactory for such language
features as non-determinism and probabilistic branching, in that there are
stateless terms that are not innocent. Based on a category of P-visible plays
with a notion of embedding as morphisms, we propose a natural generalisation by
viewing innocent strategies as sheaves over (a site of) plays, echoing a slogan
of Hirschowitz and Pous. Our approach gives rise to fully complete game models
in each of the three cases of deterministic, nondeterministic and probabilistic
branching. To our knowledge, in the second and third cases, ours are the first
such factorisation-free constructions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2778</identifier>
 <datestamp>2014-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2778</id><created>2014-09-09</created><authors><author><keyname>Camilli</keyname><forenames>Matteo</forenames></author></authors><title>Verification of Reachability Problems for Time Basic Petri Nets</title><categories>cs.LO cs.SE</categories><comments>21 pages. arXiv admin note: substantial text overlap with
  arXiv:1107.1166 by other authors</comments><acm-class>D.2.2; D.2.4; I.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Time-Basic Petri nets, is a powerful formalism for model- ing real-time
systems where time constraints are expressed through time functions of
marking's time description associated with transition, representing possible
firing times. We introduce a technique for reachability analysis based on the
building of finite contraction of the infinite state space associated with such
a models. The technique constructs a finite symbolic reachability graph relying
on a sort of time coverage, and over- comes the limitations of the existing
available analyzers for Time-Basic nets, based in turn on a time-bounded
inspection of a (possibly infinite) reachability-tree. A key feature of the
technique is the introduction of the Time Anonymous concept, which allows the
identification of components not influencing the evolution of a model. A
running example is used throughout the paper to sketch the symbolic graph
construction. The graph construction algorithm has been automated by a Java
tool-set, described in the paper together with its main functionality and
analysis capability. A use case describing a real-world example has been
employed to benchmark the technique and the tool-set. The main outcome of this
test are also presented in the paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2792</identifier>
 <datestamp>2015-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2792</id><created>2014-09-09</created><updated>2015-01-28</updated><authors><author><keyname>Lin</keyname><forenames>Xingqin</forenames></author><author><keyname>Heath</keyname><forenames>Robert W.</forenames><suffix>Jr.</suffix></author><author><keyname>Andrews</keyname><forenames>Jeffrey G.</forenames></author></authors><title>The Interplay between Massive MIMO and Underlaid D2D Networking</title><categories>cs.IT math.IT</categories><comments>35 pages; 7 figures; submitted to IEEE Transactions on Wireless
  Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a device-to-device (D2D) underlaid cellular network, the uplink spectrum
is reused by the D2D transmissions, causing mutual interference with the
ongoing cellular transmissions. Massive MIMO is appealing in such a context as
the base station's (BS's) large antenna array can nearly null the D2D-to-BS
interference. The multi-user transmission in massive MIMO, however, may lead to
increased cellular-to-D2D interference. This paper studies the interplay
between massive MIMO and underlaid D2D networking in a multi-cell setting. We
investigate cellular and D2D spectral efficiency under both perfect and
imperfect channel state information (CSI) at the receivers that employ partial
zero-forcing. Compared to the case without D2D, there is a loss in cellular
spectral efficiency due to D2D underlay. With perfect CSI, the loss can be
completely overcome if the number of canceled D2D interfering signals is scaled
with the number of BS antennas at an arbitrarily slow rate. With imperfect CSI,
in addition to pilot contamination, a new asymptotic effect termed underlay
contamination arises. In the non-asymptotic regime, simple analytical lower
bounds are derived for both the cellular and D2D spectral efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2800</identifier>
 <datestamp>2014-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2800</id><created>2014-09-09</created><authors><author><keyname>Parag</keyname><forenames>Toufiq</forenames></author></authors><title>Enforcing Label and Intensity Consistency for IR Target Detection</title><categories>cs.CV</categories><comments>First appeared in OTCBVS 2011 \cite{parag11otcbvs}. This manuscript
  presents updated results and an extension</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study formulates the IR target detection as a binary classification
problem of each pixel. Each pixel is associated with a label which indicates
whether it is a target or background pixel. The optimal label set for all the
pixels of an image maximizes aposteriori distribution of label configuration
given the pixel intensities. The posterior probability is factored into (or
proportional to) a conditional likelihood of the intensity values and a prior
probability of label configuration. Each of these two probabilities are
computed assuming a Markov Random Field (MRF) on both pixel intensities and
their labels. In particular, this study enforces neighborhood dependency on
both intensity values, by a Simultaneous Auto Regressive (SAR) model, and on
labels, by an Auto-Logistic model. The parameters of these MRF models are
learned from labeled examples. During testing, an MRF inference technique,
namely Iterated Conditional Mode (ICM), produces the optimal label for each
pixel. The detection performance is further improved by incorporating temporal
information through background subtraction. High performances on benchmark
datasets demonstrate effectiveness of this method for IR target detection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2802</identifier>
 <datestamp>2015-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2802</id><created>2014-09-09</created><updated>2015-02-12</updated><authors><author><keyname>March</keyname><forenames>William B.</forenames></author><author><keyname>Biros</keyname><forenames>George</forenames></author></authors><title>Far-Field Compression for Fast Kernel Summation Methods in High
  Dimensions</title><categories>cs.LG stat.ML</categories><comments>43 pages, 21 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider fast kernel summations in high dimensions: given a large set of
points in $d$ dimensions (with $d \gg 3$) and a pair-potential function (the
{\em kernel} function), we compute a weighted sum of all pairwise kernel
interactions for each point in the set. Direct summation is equivalent to a
(dense) matrix-vector multiplication and scales quadratically with the number
of points. Fast kernel summation algorithms reduce this cost to log-linear or
linear complexity.
  Treecodes and Fast Multipole Methods (FMMs) deliver tremendous speedups by
constructing approximate representations of interactions of points that are far
from each other. In algebraic terms, these representations correspond to
low-rank approximations of blocks of the overall interaction matrix. Existing
approaches require an excessive number of kernel evaluations with increasing
$d$ and number of points in the dataset.
  To address this issue, we use a randomized algebraic approach in which we
first sample the rows of a block and then construct its approximate, low-rank
interpolative decomposition. We examine the feasibility of this approach
theoretically and experimentally. We provide a new theoretical result showing a
tighter bound on the reconstruction error from uniformly sampling rows than the
existing state-of-the-art. We demonstrate that our sampling approach is
competitive with existing (but prohibitively expensive) methods from the
literature. We also construct kernel matrices for the Laplacian, Gaussian, and
polynomial kernels -- all commonly used in physics and data analysis. We
explore the numerical properties of blocks of these matrices, and show that
they are amenable to our approach. Depending on the data set, our randomized
algorithm can successfully compute low rank approximations in high dimensions.
We report results for data sets with ambient dimensions from four to 1,000.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2819</identifier>
 <datestamp>2014-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2819</id><created>2014-09-09</created><authors><author><keyname>Saberi</keyname><forenames>Bagher</forenames></author><author><keyname>Ghadiri</keyname><forenames>Nasser</forenames></author></authors><title>A Sample-Based Approach to Data Quality Assessment in Spatial Databases
  with Application to Mobile Trajectory Nearest-Neighbor Search</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spatial data is playing an emerging role in new technologies such as web and
mobile mapping and Geographic Information Systems (GIS). Important decisions in
political, social and many other aspects of modern human life are being made
using location data. Decision makers in many countries are exploiting spatial
databases for collecting information, analyzing them and planning for the
future. In fact, not every spatial database is suitable for this type of
application. Inaccuracy, imprecision and other deficiencies are present in
location data just as any other type of data and may have a negative impact on
credibility of any action taken based on unrefined information. So we need a
method for evaluating the quality of spatial data and separating usable data
from misleading data which leads to weak decisions. On the other hand, spatial
databases are usually huge in size and therefore working with this type of data
has a negative impact on efficiency. To improve the efficiency of working with
spatial big data, we need a method for shrinking the volume of data. Sampling
is one of these methods, but its negative effects on the quality of data are
inevitable. In this paper we are trying to show and assess this change in
quality of spatial data that is a consequence of sampling. We used this
approach for evaluating the quality of sampled spatial data related to mobile
user trajectories in China which are available in a well-known spatial
database. The results show that sample-based control of data quality will
increase the query performance significantly, without losing too much accuracy.
Based on this results some future improvements are pointed out which will help
to process location-based queries faster than before and to make more accurate
location-based decisions in limited times.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2821</identifier>
 <datestamp>2014-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2821</id><created>2014-09-09</created><authors><author><keyname>Ghaffari</keyname><forenames>Meysam</forenames></author><author><keyname>Ghadiri</keyname><forenames>Nasser</forenames></author></authors><title>Ambiguity-Driven Fuzzy C-Means Clustering: How to Detect Uncertain
  Clustered Records</title><categories>cs.AI cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As a well-known clustering algorithm, Fuzzy C-Means (FCM) allows each input
sample to belong to more than one cluster, providing more flexibility than
non-fuzzy clustering methods. However, the accuracy of FCM is subject to false
detections caused by noisy records, weak feature selection and low certainty of
the algorithm in some cases. The false detections are very important in some
decision-making application domains like network security and medical
diagnosis, where weak decisions based on such false detections may lead to
catastrophic outcomes. They are mainly emerged from making decisions about a
subset of records that do not provide enough evidence to make a good decision.
In this paper, we propose a method for detecting such ambiguous records in FCM
by introducing a certainty factor to decrease invalid detections. This approach
enables us to send the detected ambiguous records to another discrimination
method for a deeper investigation, thus increasing the accuracy by lowering the
error rate. Most of the records are still processed quickly and with low error
rate which prevents performance loss compared to similar hybrid methods.
Experimental results of applying the proposed method on several datasets from
different domains show a significant decrease in error rate as well as improved
sensitivity of the algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2826</identifier>
 <datestamp>2014-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2826</id><created>2014-09-07</created><authors><author><keyname>Cao</keyname><forenames>Guofeng</forenames></author><author><keyname>Wang</keyname><forenames>Shaowen</forenames></author><author><keyname>Hwang</keyname><forenames>Myunghwa</forenames></author><author><keyname>Padmanabhan</keyname><forenames>Anand</forenames></author><author><keyname>Zhang</keyname><forenames>Zhenhua</forenames></author><author><keyname>Soltani</keyname><forenames>Kiumars</forenames></author></authors><title>A Scalable Framework for Spatiotemporal Analysis of Location-based
  Social Media Data</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the past several years, social media (e.g., Twitter and Facebook) has been
experiencing a spectacular rise and popularity, and becoming a ubiquitous
discourse for content sharing and social networking. With the widespread of
mobile devices and location-based services, social media typically allows users
to share whereabouts of daily activities (e.g., check-ins and taking photos),
and thus strengthens the roles of social media as a proxy to understand human
behaviors and complex social dynamics in geographic spaces. Unlike conventional
spatiotemporal data, this new modality of data is dynamic, massive, and
typically represented in stream of unstructured media (e.g., texts and photos),
which pose fundamental representation, modeling and computational challenges to
conventional spatiotemporal analysis and geographic information science. In
this paper, we describe a scalable computational framework to harness massive
location-based social media data for efficient and systematic spatiotemporal
data analysis. Within this framework, the concept of space-time trajectories
(or paths) is applied to represent activity profiles of social media users. A
hierarchical spatiotemporal data model, namely a spatiotemporal data cube
model, is developed based on collections of space-time trajectories to
represent the collective dynamics of social media users across aggregation
boundaries at multiple spatiotemporal scales. The framework is implemented
based upon a public data stream of Twitter feeds posted on the continent of
North America. To demonstrate the advantages and performance of this framework,
an interactive flow mapping interface (including both single-source and
multiple-source flow mapping) is developed to allow real-time, and interactive
visual exploration of movement dynamics in massive location-based social media
at multiple scales.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2835</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2835</id><created>2014-09-09</created><updated>2015-06-02</updated><authors><author><keyname>Cavalcante</keyname><forenames>Renato L. G.</forenames></author><author><keyname>Pollakis</keyname><forenames>Emmanuel</forenames></author><author><keyname>Stanczak</keyname><forenames>Slawomir</forenames></author></authors><title>Power Estimation in LTE systems with the General Framework of Standard
  Interference Mappings</title><categories>cs.IT cs.NI math.IT</categories><comments>IEEE Global SIP 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We devise novel techniques to obtain the downlink power inducing a given load
in long-term evolution (LTE) systems, where we define load as the fraction of
resource blocks in the time-frequency grid being requested by users from a
given base station. These techniques are particularly important because
previous studies have proved that the data rate requirement of users can be
satisfied with lower transmit energy if we allow the load to increase. Those
studies have also shown that obtaining the power assignment from a desired load
profile can be posed as a fixed point problem involving standard interference
mappings, but so far the mappings have not been obtained explicitly. One of our
main contributions in this study is to close this gap. We derive an
interference mapping having as its fixed point the power assignment inducing a
desired load, assuming that such an assignment exists. Having this mapping in
closed form, we simplify the proof of the aforementioned known results, and we
also devise novel iterative algorithms for power computation that have many
numerical advantages over previous methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2848</identifier>
 <datestamp>2015-08-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2848</id><created>2014-09-09</created><updated>2015-07-31</updated><authors><author><keyname>Shamir</keyname><forenames>Ohad</forenames></author></authors><title>A Stochastic PCA and SVD Algorithm with an Exponential Convergence Rate</title><categories>cs.LG cs.NA math.OC stat.ML</categories><comments>Fixed a minor bug in the proof of lemma 1 (which does not affect the
  result)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe and analyze a simple algorithm for principal component analysis
and singular value decomposition, VR-PCA, which uses computationally cheap
stochastic iterations, yet converges exponentially fast to the optimal
solution. In contrast, existing algorithms suffer either from slow convergence,
or computationally intensive iterations whose runtime scales with the data
size. The algorithm builds on a recent variance-reduced stochastic gradient
technique, which was previously analyzed for strongly convex optimization,
whereas here we apply it to an inherently non-convex problem, using a very
different analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2863</identifier>
 <datestamp>2015-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2863</id><created>2014-09-09</created><updated>2015-02-16</updated><authors><author><keyname>Bornmann</keyname><forenames>Lutz</forenames></author></authors><title>Usefulness of altmetrics for measuring the broader impact of research: A
  case study using data from PLOS (altmetrics) and F1000Prime (paper tags)</title><categories>cs.DL</categories><comments>arXiv admin note: text overlap with arXiv:1406.7611</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Purpose: Whereas citation counts allow the measurement of the impact of
research on research itself, an important role in the measurement of the impact
of research on other parts of society is ascribed to altmetrics. The present
case study investigates the usefulness of altmetrics for measuring the broader
impact of research. Methods: This case study is essentially based on a dataset
with papers obtained from F1000. The dataset was augmented with altmetrics
(such as Twitter counts) which were provided by PLOS (the Public Library of
Science). In total, the case study covers a total of 1,082 papers. Findings:
The F1000 dataset contains tags on papers which were assigned intellectually by
experts and which can characterise a paper. The most interesting tag for
altmetric research is &quot;good for teaching&quot;. This tag is assigned to papers which
could be of interest to a wider circle of readers than the peers in a
specialist area. Particularly on Facebook and Twitter, one could expect papers
with this tag to be mentioned more often than those without this tag. With
respect to the &quot;good for teaching&quot; tag, the results from regression models were
able to confirm these expectations: Papers with this tag show significantly
higher Facebook and Twitter counts than papers without this tag. This
association could not be seen with Mendeley or Figshare counts (that is with
counts from platforms which are chiefly of interest in a scientific context).
Conclusions: The results of the current study indicate that Facebook and
Twitter, but not Figshare or Mendeley, can provide indications of papers which
are of interest to a broader circle of readers (and not only for the peers in a
specialist area), and seem therefore be useful for societal impact measurement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2864</identifier>
 <datestamp>2014-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2864</id><created>2014-09-09</created><authors><author><keyname>Lawrence</keyname><forenames>Michael</forenames></author><author><keyname>Morgan</keyname><forenames>Martin</forenames></author></authors><title>Scalable Genomics with R and Bioconductor</title><categories>q-bio.GN cs.DC</categories><comments>Published in at http://dx.doi.org/10.1214/14-STS476 the Statistical
  Science (http://www.imstat.org/sts/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-STS-STS476</report-no><journal-ref>Statistical Science 2014, Vol. 29, No. 2, 214-226</journal-ref><doi>10.1214/14-STS476</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper reviews strategies for solving problems encountered when analyzing
large genomic data sets and describes the implementation of those strategies in
R by packages from the Bioconductor project. We treat the scalable processing,
summarization and visualization of big genomic data. The general ideas are well
established and include restrictive queries, compression, iteration and
parallel computing. We demonstrate the strategies by applying Bioconductor
packages to the detection and analysis of genetic variants from a whole genome
sequencing experiment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2897</identifier>
 <datestamp>2014-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2897</id><created>2014-09-09</created><authors><author><keyname>Cheamanunkul</keyname><forenames>Sunsern</forenames></author><author><keyname>Freund</keyname><forenames>Yoav</forenames></author></authors><title>Co-adaptation in a Handwriting Recognition System</title><categories>cs.HC cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Handwriting is a natural and versatile method for human-computer interaction,
especially on small mobile devices such as smart phones. However, as
handwriting varies significantly from person to person, it is difficult to
design handwriting recognizers that perform well for all users. A natural
solution is to use machine learning to adapt the recognizer to the user. One
complicating factor is that, as the computer adapts to the user, the user also
adapts to the computer and probably changes their handwriting. This paper
investigates the dynamics of co-adaptation, a process in which both the
computer and the user are adapting their behaviors in order to improve the
speed and accuracy of the communication through handwriting. We devised an
information-theoretic framework for quantifying the efficiency of a handwriting
system where the system includes both the user and the computer. Using this
framework, we analyzed data collected from an adaptive handwriting recognition
system and characterized the impact of machine adaptation and of human
adaptation. We found that both machine adaptation and human adaptation have
significant impact on the input rate and must be considered together in order
to improve the efficiency of the system as a whole.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2902</identifier>
 <datestamp>2014-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2902</id><created>2014-09-09</created><authors><author><keyname>Jamil</keyname><forenames>Noreen</forenames></author><author><keyname>Chen</keyname><forenames>Xuemei</forenames></author><author><keyname>Cloninger</keyname><forenames>Alex</forenames></author></authors><title>The Hildreth's Algorithm with Applications to Soft Constraints for User
  Interface Layout</title><categories>cs.NA math.NA</categories><comments>16 pages, 3 figures. arXiv admin note: substantial text overlap with
  arXiv:1309.7001</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Hildreth's algorithm is a row action method for solving large systems of
inequalities. This algorithm is efficient for problems with sparse matrices, as
opposed to direct methods such as Gaussian elimination or QR-factorization. We
apply the Hildreth's algorithm, as well as a randomized version, along with
prioritized selection of the inequalities, to efficiently detect the highest
priority feasible subsystem of equations. We prove convergence results and
feasibility criteria for both cyclic and randomized Hildreth's algorithm, as
well as a mixed algorithm which uses Hildreth's algorithm for inequalities and
Kaczmarz algorithm for equalities. These prioritized, sparse systems of
inequalities commonly appear in constraint-based user interface (UI) layout
specifications. The performance and convergence of these proposed algorithms
are evaluated empirically using randomly generated UI layout specifications of
various sizes. The results show that these methods offer improvements in
performance over standard methods like Matlab's LINPROG, a well-known efficient
linear programming solver, and the recent developed Kaczmarz algorithm with
prioritized IIS detection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2905</identifier>
 <datestamp>2014-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2905</id><created>2014-09-09</created><authors><author><keyname>Cheamanunkul</keyname><forenames>Sunsern</forenames></author><author><keyname>Ettinger</keyname><forenames>Evan</forenames></author><author><keyname>Freund</keyname><forenames>Yoav</forenames></author></authors><title>Non-Convex Boosting Overcomes Random Label Noise</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The sensitivity of Adaboost to random label noise is a well-studied problem.
LogitBoost, BrownBoost and RobustBoost are boosting algorithms claimed to be
less sensitive to noise than AdaBoost. We present the results of experiments
evaluating these algorithms on both synthetic and real datasets. We compare the
performance on each of datasets when the labels are corrupted by different
levels of independent label noise. In presence of random label noise, we found
that BrownBoost and RobustBoost perform significantly better than AdaBoost and
LogitBoost, while the difference between each pair of algorithms is
insignificant. We provide an explanation for the difference based on the margin
distributions of the algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2908</identifier>
 <datestamp>2014-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2908</id><created>2014-09-09</created><authors><author><keyname>Benson</keyname><forenames>Austin R.</forenames></author><author><keyname>Ballard</keyname><forenames>Grey</forenames></author></authors><title>A Framework for Practical Parallel Fast Matrix Multiplication</title><categories>cs.DC cs.MS cs.NA</categories><acm-class>G.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Matrix multiplication is a fundamental computation in many scientific
disciplines. In this paper, we show that novel fast matrix multiplication
algorithms can significantly outperform vendor implementations of the classical
algorithm and Strassen's fast algorithm on modest problem sizes and shapes.
Furthermore, we show that the best choice of fast algorithm depends not only on
the size of the matrices but also the shape. We develop a code generation tool
to automatically implement multiple sequential and shared-memory parallel
variants of each fast algorithm, including our novel parallelization scheme.
This allows us to rapidly benchmark over 20 fast algorithms on several problem
sizes. Furthermore, we discuss a number of practical implementation issues for
these algorithms on shared-memory machines that can direct further research on
making fast algorithms practical.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2913</identifier>
 <datestamp>2014-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2913</id><created>2014-09-09</created><authors><author><keyname>Eldan</keyname><forenames>Ronen</forenames></author><author><keyname>Singh</keyname><forenames>Mohit</forenames></author></authors><title>Efficient Algorithms for Discrepancy Minimization in Convex Sets</title><categories>cs.DS cs.CG math.PR</categories><comments>Preliminary version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A result of Spencer states that every collection of $n$ sets over a universe
of size $n$ has a coloring of the ground set with $\{-1,+1\}$ of discrepancy
$O(\sqrt{n})$. A geometric generalization of this result was given by Gluskin
(see also Giannopoulos) who showed that every symmetric convex body $K\subseteq
R^n$ with Gaussian measure at least $e^{-\epsilon n}$, for a small
$\epsilon&gt;0$, contains a point $y\in K$ where a constant fraction of
coordinates of $y$ are in $\{-1,1\}$. This is often called a partial coloring
result. While both these results were inherently non-algorithmic, recently
Bansal (see also Lovett-Meka) gave a polynomial time algorithm for Spencer's
setting and Rothvo\ss gave a randomized polynomial time algorithm obtaining the
same guarantee as the result of Gluskin and Giannopoulos.
  This paper has several related results. First we prove another constructive
version of the result of Gluskin and Giannopoulos via an optimization of a
linear function. This implies a linear programming based algorithm for
combinatorial discrepancy obtaining the same result as Spencer. Our second
result gives a new approach to obtains partial colorings and shows that every
convex body $K\subseteq R^n$, possibly non-symmetric, with Gaussian measure at
least $e^{-\epsilon n}$, for a small $\epsilon&gt;0$, contains a point $y\in K$
where a constant fraction of coordinates of $y$ are in $\{-1,1\}$. Finally, we
give a simple proof that shows that for any $\delta &gt;0$ there exists a constant
$c&gt;0$ such that given a body $K$ with $\gamma_n(K)\geq \delta$, a uniformly
random $x$ from $\{-1,1\}^n$ is in $cK$ with constant probability. This gives
an algorithmic version of a special case of the result of Banaszczyk.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2918</identifier>
 <datestamp>2014-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2918</id><created>2014-09-09</created><authors><author><keyname>Mastriani</keyname><forenames>Mario</forenames></author></authors><title>Quantum Edge Detection for Image Segmentation in Optical Environments</title><categories>cs.CV</categories><comments>31 pages, 20 figures, 5 tables. arXiv admin note: substantial text
  overlap with arXiv:1406.5121, arXiv:1408.2427; and text overlap with
  arXiv:quant-ph/0402085 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A quantum edge detector for image segmentation in optical environments is
presented in this work. A Boolean version of the same detector is presented
too. The quantum version of the new edge detector works with computational
basis states, exclusively. This way, we can easily avoid the problem of quantum
measurement retrieving the result of applying the new detector on the image.
Besides, a new criterion and logic based on projections onto vertical axis of
Bloch's Sphere exclusively are presented too. This approach will allow us: 1) a
simpler development of logic quantum operations, where they will closer to
those used in the classical logic operations, 2) building simple and robust
classical-to-quantum and quantum-to-classical interfaces. Said so far is
extended to quantum algorithms outside image processing too. In a special
section on metric and simulations, a new metric based on the comparison between
the classical and quantum versions algorithms for edge detection of images is
presented. Notable differences between the results of classical and quantum
versions of such algorithms (outside and inside of quantum computer,
respectively) show the existence of implementation problems involved in the
experiment, and that they have not been properly modeled for optical
environments. However, although they are different, the quantum results are
equally valid. The latter is clearly seen in the computer simulations
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2919</identifier>
 <datestamp>2014-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2919</id><created>2014-09-09</created><authors><author><keyname>Manita</keyname><forenames>Anatoly</forenames></author></authors><title>Intrinsic scales for high-dimensional LEVY-driven models with
  non-Markovian synchronizing updates</title><categories>math.PR cs.SY math-ph math.MP</categories><comments>50 pages</comments><msc-class>60K35, 60K05, 60G51, 60G55, 60J25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose stochastic $N$-component synchronization models
$(x_{1}(t),...,x_{N}(t))$, $x_{j}\in\mathbb{R}^{d}$, $t\in\mathbb{R}_{+}$,
whose dynamics is described by Levy processes and synchronizing jumps. We prove
that symmetric models reach synchronization in a stochastic sense: differences
between components $d_{kj}^{(N)}(t)=x_{k}(t)-x_{j}(t)$ have limits in
distribution as $t\rightarrow\infty$. We give conditions of existence of
natural (intrinsic) space scales for large synchronized systems, i.e., we are
looking for such sequences $\{b_{N}\}$ that distribution of
$d_{kj}^{(N)}(\infty)/b_{N}$ converges to some limit as $N\rightarrow\infty$.
It appears that such sequence exists if the Levy process enters a domain of
attraction of some stable law. For Markovian synchronization models based on
$\alpha$-stable Levy processes this results holds for any finite $N$ in the
precise form with $b_{N}=(N-1)^{1/\alpha}$. For non-Markovian models similar
results hold only in the asymptotic sense. The class of limiting laws includes
the Linnik distributions. We also discuss generalizations of these theorems to
the case of non-uniform matrix-based intrinsic scales. The central point of our
proofs is a representation of characteristic functions of $d_{kj}^{(N)}(t)$ via
probability distribution of a superposition of $N$ independent renewal
processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2928</identifier>
 <datestamp>2014-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2928</id><created>2014-09-09</created><authors><author><keyname>Rodionov</keyname><forenames>Anatoly</forenames></author></authors><title>Path algebra algorithm for finding longest increasing subsequence</title><categories>cs.DS</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  New algorithm for finding longest increasing subsequence is discussed. This
algorithm is based on the ideas of idempotent mathematics and uses Max-Plus
idempotent semiring. Problem of finding longest increasing sub- sequence is
reformulated in a matrix form and solved with linear algebra.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2944</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2944</id><created>2014-09-09</created><updated>2015-06-18</updated><authors><author><keyname>Wang</keyname><forenames>Hao</forenames></author><author><keyname>Wang</keyname><forenames>Naiyan</forenames></author><author><keyname>Yeung</keyname><forenames>Dit-Yan</forenames></author></authors><title>Collaborative Deep Learning for Recommender Systems</title><categories>cs.LG cs.CL cs.IR cs.NE stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Collaborative filtering (CF) is a successful approach commonly used by many
recommender systems. Conventional CF-based methods use the ratings given to
items by users as the sole source of information for learning to make
recommendation. However, the ratings are often very sparse in many
applications, causing CF-based methods to degrade significantly in their
recommendation performance. To address this sparsity problem, auxiliary
information such as item content information may be utilized. Collaborative
topic regression (CTR) is an appealing recent method taking this approach which
tightly couples the two components that learn from two different sources of
information. Nevertheless, the latent representation learned by CTR may not be
very effective when the auxiliary information is very sparse. To address this
problem, we generalize recent advances in deep learning from i.i.d. input to
non-i.i.d. (CF-based) input and propose in this paper a hierarchical Bayesian
model called collaborative deep learning (CDL), which jointly performs deep
representation learning for the content information and collaborative filtering
for the ratings (feedback) matrix. Extensive experiments on three real-world
datasets from different domains show that CDL can significantly advance the
state of the art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2973</identifier>
 <datestamp>2014-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2973</id><created>2014-09-10</created><authors><author><keyname>Traag</keyname><forenames>V. A.</forenames></author><author><keyname>Reinanda</keyname><forenames>R.</forenames></author><author><keyname>Hicks</keyname><forenames>J.</forenames></author><author><keyname>van Klinken</keyname><forenames>G.</forenames></author></authors><title>Dynamics of Media Attention</title><categories>physics.soc-ph cs.SI</categories><journal-ref>Proceedings of ICCSA 2014, Le Havre, France, pp.301--304</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Studies of human attention dynamics analyses how attention is focused on
specific topics, issues or people. In online social media, there are clear
signs of exogenous shocks, bursty dynamics, and an exponential or powerlaw
lifetime distribution. We here analyse the attention dynamics of traditional
media, focussing on co-occurrence of people in newspaper articles. The results
are quite different from online social networks and attention. Different
regimes seem to be operating at two different time scales. At short time scales
we see evidence of bursty dynamics and fast decaying edge lifetimes and
attention. This behaviour disappears for longer time scales, and in that regime
we find Poissonian dynamics and slower decaying lifetimes. We propose that a
cascading Poisson process may take place, with issues arising at a constant
rate over a long time scale, and faster dynamics at a shorter time scale.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2978</identifier>
 <datestamp>2014-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2978</id><created>2014-09-10</created><authors><author><keyname>Filmus</keyname><forenames>Yuval</forenames></author><author><keyname>Lauria</keyname><forenames>Massimo</forenames></author><author><keyname>Mik&#x161;a</keyname><forenames>Mladen</forenames></author><author><keyname>Nordstr&#xf6;m</keyname><forenames>Jakob</forenames></author><author><keyname>Vinyals</keyname><forenames>Marc</forenames></author></authors><title>From Small Space to Small Width in Resolution</title><categories>cs.CC cs.DM cs.LO</categories><acm-class>F.1.3; F.4.1; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In 2003, Atserias and Dalmau resolved a major open question about the
resolution proof system by establishing that the space complexity of CNF
formulas is always an upper bound on the width needed to refute them. Their
proof is beautiful but somewhat mysterious in that it relies heavily on tools
from finite model theory. We give an alternative, completely elementary proof
that works by simple syntactic manipulations of resolution refutations. As a
by-product, we develop a &quot;black-box&quot; technique for proving space lower bounds
via a &quot;static&quot; complexity measure that works against any resolution
refutation---previous techniques have been inherently adaptive. We conclude by
showing that the related question for polynomial calculus (i.e., whether space
is an upper bound on degree) seems unlikely to be resolvable by similar
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2983</identifier>
 <datestamp>2014-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2983</id><created>2014-09-10</created><authors><author><keyname>Bogomolov</keyname><forenames>Andrey</forenames></author><author><keyname>Lepri</keyname><forenames>Bruno</forenames></author><author><keyname>Staiano</keyname><forenames>Jacopo</forenames></author><author><keyname>Oliver</keyname><forenames>Nuria</forenames></author><author><keyname>Pianesi</keyname><forenames>Fabio</forenames></author><author><keyname>Pentland</keyname><forenames>Alex</forenames></author></authors><title>Once Upon a Crime: Towards Crime Prediction from Demographics and Mobile
  Data</title><categories>cs.CY cs.SI physics.soc-ph</categories><comments>10 pages, 3 figures. To appear in ACM International Conference on
  Multimodal Interaction (ICMI 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a novel approach to predict crime in a geographic
space from multiple data sources, in particular mobile phone and demographic
data. The main contribution of the proposed approach lies in using aggregated
and anonymized human behavioral data derived from mobile network activity to
tackle the crime prediction problem. While previous research efforts have used
either background historical knowledge or offenders' profiling, our findings
support the hypothesis that aggregated human behavioral data captured from the
mobile network infrastructure, in combination with basic demographic
information, can be used to predict crime. In our experimental results with
real crime data from London we obtain an accuracy of almost 70% when predicting
whether a specific area in the city will be a crime hotspot or not. Moreover,
we provide a discussion of the implications of our findings for data-driven
crime analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.2993</identifier>
 <datestamp>2014-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.2993</id><created>2014-09-10</created><authors><author><keyname>Tang</keyname><forenames>Jian</forenames></author><author><keyname>Zhang</keyname><forenames>Ming</forenames></author><author><keyname>Mei</keyname><forenames>Qiaozhu</forenames></author></authors><title>&quot;Look Ma, No Hands!&quot; A Parameter-Free Topic Model</title><categories>cs.LG cs.CL cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It has always been a burden to the users of statistical topic models to
predetermine the right number of topics, which is a key parameter of most topic
models. Conventionally, automatic selection of this parameter is done through
either statistical model selection (e.g., cross-validation, AIC, or BIC) or
Bayesian nonparametric models (e.g., hierarchical Dirichlet process). These
methods either rely on repeated runs of the inference algorithm to search
through a large range of parameter values which does not suit the mining of big
data, or replace this parameter with alternative parameters that are less
intuitive and still hard to be determined. In this paper, we explore to
&quot;eliminate&quot; this parameter from a new perspective. We first present a
nonparametric treatment of the PLSA model named nonparametric probabilistic
latent semantic analysis (nPLSA). The inference procedure of nPLSA allows for
the exploration and comparison of different numbers of topics within a single
execution, yet remains as simple as that of PLSA. This is achieved by
substituting the parameter of the number of topics with an alternative
parameter that is the minimal goodness of fit of a document. We show that the
new parameter can be further eliminated by two parameter-free treatments:
either by monitoring the diversity among the discovered topics or by a weak
supervision from users in the form of an exemplar topic. The parameter-free
topic model finds the appropriate number of topics when the diversity among the
discovered topics is maximized, or when the granularity of the discovered
topics matches the exemplar topic. Experiments on both synthetic and real data
prove that the parameter-free topic model extracts topics with a comparable
quality comparing to classical topic models with &quot;manual transmission&quot;. The
quality of the topics outperforms those extracted through classical Bayesian
nonparametric models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3005</identifier>
 <datestamp>2014-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3005</id><created>2014-09-10</created><authors><author><keyname>Mahdaouy</keyname><forenames>Abdelkader El</forenames></author><author><keyname>Ouatik</keyname><forenames>Sa&#xef;d EL Alaoui</forenames></author><author><keyname>Gaussier</keyname><forenames>Eric</forenames></author></authors><title>A Study of Association Measures and their Combination for Arabic MWT
  Extraction</title><categories>cs.CL</categories><comments>This paper have been presented and published in 10th International
  Conference on Terminology and Artificial Intelligence Proceedings</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatic Multi-Word Term (MWT) extraction is a very important issue to many
applications, such as information retrieval, question answering, and text
categorization. Although many methods have been used for MWT extraction in
English and other European languages, few studies have been applied to Arabic.
In this paper, we propose a novel, hybrid method which combines linguistic and
statistical approaches for Arabic Multi-Word Term extraction. The main
contribution of our method is to consider contextual information and both
termhood and unithood for association measures at the statistical filtering
step. In addition, our technique takes into account the problem of MWT
variation in the linguistic filtering step. The performance of the proposed
statistical measure (NLC-value) is evaluated using an Arabic environment corpus
by comparing it with some existing competitors. Experimental results show that
our NLC-value measure outperforms the other ones in term of precision for both
bi-grams and tri-grams.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3021</identifier>
 <datestamp>2014-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3021</id><created>2014-09-10</created><authors><author><keyname>Bitar</keyname><forenames>Ibrahim El</forenames></author><author><keyname>Belouadha</keyname><forenames>Fatima-Zahra</forenames></author><author><keyname>Roudies</keyname><forenames>Ounsa</forenames></author></authors><title>Semantic web service discovery approaches: overview and limitations</title><categories>cs.IR cs.DB</categories><comments>16 pages, 1 figure, 2 tables,
  http://airccse.org/journal/ijcses/papers/5414ijcses02.pdf volume 5, Number 4,
  2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The semantic Web service discovery has been given massive attention within
the last few years. With the increasing number of Web services available on the
web, looking for a particular service has become very difficult, especially
with the evolution of the clients needs. In this context, various approaches to
discover semantic Web services have been proposed. In this paper, we compare
these approaches in order to assess their maturity and their adaptation to the
current domain requirements. The outcome of this comparison will help us to
identify the mechanisms that constitute the strengths of the existing
approaches, and thereafter will serve as guideline to determine the basis for a
discovery approach more adapted to the current context of Web services.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3024</identifier>
 <datestamp>2014-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3024</id><created>2014-09-10</created><authors><author><keyname>Fouda</keyname><forenames>Y. M.</forenames></author></authors><title>One-Dimensional Vector based Pattern Matching</title><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Template matching is a basic method in image analysis to extract useful
information from images. In this paper, we suggest a new method for pattern
matching. Our method transform the template image from two dimensional image
into one dimensional vector. Also all sub-windows (same size of template) in
the reference image will transform into one dimensional vectors. The three
similarity measures SAD, SSD, and Euclidean are used to compute the likeness
between template and all sub-windows in the reference image to find the best
match. The experimental results show the superior performance of the proposed
method over the conventional methods on various template of different sizes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3037</identifier>
 <datestamp>2014-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3037</id><created>2014-09-10</created><authors><author><keyname>Chockalingam</keyname><forenames>Sabarathinam</forenames></author><author><keyname>Lallie</keyname><forenames>Harjinder Singh</forenames></author></authors><title>The Conceptual Idea of Online Social Media Site (SMS) User Account
  Penetration Testing System</title><categories>cs.SI cs.CY</categories><journal-ref>S Chockalingam, HS Lallie, The Conceptual Idea of Online Social
  Media Site (SMS) User Account Penetration Testing System, International
  Journal of Security, Privacy, and Trust Management, vol 3, no 4, pp 1 10, Aug
  2014</journal-ref><doi>10.5121/ijsptm.2014.3401</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social Media Site (SMS) usage has grown rapidly in the last few years. This
sudden increase in SMS usage creates an opportunity for data leakage which
could compromise personal and/or professional life. In this work, we have
reviewed traditional penetration testing process and discussed the failures of
traditional penetration testing process to test the 'People' layer of Simple
Enterprise Security Architecture (SESA) model. In order to overcome these
failures, we have developed the conceptual idea of online SMS user account
penetration testing system that could be applied to online SMS user account and
the user account could be categorised based on the rating points. This could
help us to avoid leaking information that is sensitive and/or damage their
reputation. Finally, we have also listed the best practice guidelines of online
SMS usage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3040</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3040</id><created>2014-09-10</created><updated>2014-11-10</updated><authors><author><keyname>Gravin</keyname><forenames>Nick</forenames></author><author><keyname>Peres</keyname><forenames>Yuval</forenames></author><author><keyname>Sivan</keyname><forenames>Balasubramanian</forenames></author></authors><title>Towards Optimal Algorithms for Prediction with Expert Advice</title><categories>cs.LG cs.GT math.PR</categories><comments>Fixed a typo in Theorem 3</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the classical problem of prediction with expert advice in the
adversarial setting with a geometric stopping time. In 1965, Cover gave the
optimal algorithm for the case of $2$ experts. In this paper, we design the
optimal algorithm, adversary and regret for the case of $3$ experts. Further,
we show that the optimal algorithm for $2$ and $3$ experts is a probability
matching algorithm (analogous to Thompson sampling) against a particular
randomized adversary. Remarkably, it turns out that this algorithm is not only
optimal against this adversary, but also minimax optimal against all possible
adversaries.
  We establish a constant factor separation between the regrets achieved by the
optimal algorithm and the widely used multiplicative weights algorithm. Along
the way, we improve the regret lower bounds for the multiplicative weights
algorithm for an arbitrary number of experts and show that this is tight for
$2$ experts.
  A novel aspect of our analysis is that we develop upper and lower bounds
simultaneously, analogous to the primal-dual method. The analysis of the
optimal adversary relies on delicate random walk estimates. We further use this
connection to develop an improved regret bound for the case of $4$ experts, and
provide a general framework for designing the optimal algorithm for an
arbitrary number of experts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3050</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3050</id><created>2014-09-10</created><updated>2014-11-10</updated><authors><author><keyname>Timo</keyname><forenames>Roy</forenames></author><author><keyname>Wigger</keyname><forenames>Mich&#xe9;le</forenames></author></authors><title>Slepian-Wolf Coding for Broadcasting with Cooperative Base-Stations</title><categories>cs.IT math.IT</categories><comments>16 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a base-station (BS) cooperation model for broadcasting a discrete
memoryless source in a cellular or heterogeneous network. The model allows the
receivers to use helper BSs to improve network performance, and it permits the
receivers to have prior side information about the source. We establish the
model's information-theoretic limits in two operational modes: In Mode 1, the
helper BSs are given information about the channel codeword transmitted by the
main BS, and in Mode 2 they are provided correlated side information about the
source. Optimal codes for Mode 1 use \emph{hash-and-forward coding} at the
helper BSs; while, in Mode 2, optimal codes use source codes from Wyner's
\emph{helper source-coding problem} at the helper BSs. We prove the optimality
of both approaches by way of a new list-decoding generalisation of [8, Thm. 6],
and, in doing so, show an operational duality between Modes 1 and 2.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3059</identifier>
 <datestamp>2015-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3059</id><created>2014-09-10</created><updated>2015-03-26</updated><authors><author><keyname>Peixoto</keyname><forenames>Tiago P.</forenames></author></authors><title>Model selection and hypothesis testing for large-scale network models
  with overlapping groups</title><categories>physics.data-an cond-mat.dis-nn cs.SI physics.comp-ph physics.soc-ph</categories><comments>20 pages,7 figures, 1 table</comments><journal-ref>Phys. Rev. X 5, 011033 (2015)</journal-ref><doi>10.1103/PhysRevX.5.011033</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The effort to understand network systems in increasing detail has resulted in
a diversity of methods designed to extract their large-scale structure from
data. Unfortunately, many of these methods yield diverging descriptions of the
same network, making both the comparison and understanding of their results a
difficult challenge. A possible solution to this outstanding issue is to shift
the focus away from ad hoc methods and move towards more principled approaches
based on statistical inference of generative models. As a result, we face
instead the more well-defined task of selecting between competing generative
processes, which can be done under a unified probabilistic framework. Here, we
consider the comparison between a variety of generative models including
features such as degree correction, where nodes with arbitrary degrees can
belong to the same group, and community overlap, where nodes are allowed to
belong to more than one group. Because such model variants possess an
increasing number of parameters, they become prone to overfitting. In this
work, we present a method of model selection based on the minimum description
length criterion and posterior odds ratios that is capable of fully accounting
for the increased degrees of freedom of the larger models, and selects the best
one according to the statistical evidence available in the data. In applying
this method to many empirical unweighted networks from different fields, we
observe that community overlap is very often not supported by statistical
evidence and is selected as a better model only for a minority of them. On the
other hand, we find that degree correction tends to be almost universally
favored by the available data, implying that intrinsic node proprieties (as
opposed to group properties) are often an essential ingredient of network
formation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3062</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3062</id><created>2014-09-10</created><updated>2014-10-27</updated><authors><author><keyname>Devanur</keyname><forenames>Nikhil R.</forenames></author><author><keyname>Peres</keyname><forenames>Yuval</forenames></author><author><keyname>Sivan</keyname><forenames>Balasubramanian</forenames></author></authors><title>Perfect Bayesian Equilibria in Repeated Sales</title><categories>cs.GT cs.DM</categories><comments>To appear in SODA 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A special case of Myerson's classic result describes the revenue-optimal
equilibrium when a seller offers a single item to a buyer. We study a repeated
sales extension of this model: a seller offers to sell a single fresh copy of
an item to the same buyer every day via a posted price. The buyer's value for
the item is unknown to the seller but is drawn initially from a publicly known
distribution F and remains the same throughout.
  We study this setting where the seller is unable to commit to future prices
and find several surprises. First, if the horizon is fixed, previous work
showed that an equilibrium exists, and all equilibria yield tiny or constant
revenue. This is a far cry from the linearly growing benchmark of getting
Myerson optimal revenue each day. Our first result shows that this is because
the buyer strategies in these equilibria are necessarily unnatural. We restrict
to a natural class of buyer strategies called threshold strategies, and show
that threshold equilibria rarely exist. Second, if the seller can commit not to
raise prices upon purchase, while still retaining the possibility of lowering
prices in future, we show that threshold equilibria are guaranteed to exist for
the power law family of distributions. As an example, if F is uniform in [0,1],
the seller can extract revenue of order $\sqrt{n}$ in $n$ rounds as opposed to
the constant revenue obtainable when he is unable to make commitments. Finally,
we consider the infinite horizon game with partial commitment, where both the
seller and the buyer discount the future utility by a factor of $1-\delta \in
[0,1)$. When the value distribution is uniform in [0,1], there exists a
threshold equilibrium with expected revenue at least $\frac{4}{3+2\sqrt{2}}
\sim 69$% of the Myerson optimal revenue benchmark.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3078</identifier>
 <datestamp>2014-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3078</id><created>2014-09-10</created><authors><author><keyname>Borna</keyname><forenames>Keivan</forenames></author><author><keyname>Hashemi</keyname><forenames>Vahid Haji</forenames></author></authors><title>An improved genetic algorithm with a local optimization strategy and an
  extra mutation level for solving traveling salesman problem</title><categories>cs.NE</categories><comments>7 pages, 1 Figure</comments><journal-ref>International Journal of Computer Science, Engineering and
  Information Technology (IJCSEIT), Vol. 4, No.4, August 2014, 47-53</journal-ref><doi>10.5121/ijcseit.2014.4405</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Traveling salesman problem (TSP) is proved to be NP-complete in most
cases. The genetic algorithm (GA) is one of the most useful algorithms for
solving this problem. In this paper a conventional GA is compared with an
improved hybrid GA in solving TSP. The improved or hybrid GA consist of
conventional GA and two local optimization strategies. The first strategy is
extracting all sequential groups including four cities of samples and changing
the two central cities with each other. The second local optimization strategy
is similar to an extra mutation process. In this step with a low probability a
sample is selected. In this sample two random cities are defined and the path
between these cities is reversed. The computation results show that the
proposed method also finds better paths than the conventional GA within an
acceptable computation time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3081</identifier>
 <datestamp>2014-10-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3081</id><created>2014-09-10</created><updated>2014-10-02</updated><authors><author><keyname>Arulselvan</keyname><forenames>Ashwin</forenames></author><author><keyname>Gro&#xdf;</keyname><forenames>Martin</forenames></author><author><keyname>Skutella</keyname><forenames>Martin</forenames></author></authors><title>Graph Orientation and Flows Over Time</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Flows over time are used to model many real-world logistic and routing
problems. The networks underlying such problems -- streets, tracks, etc. -- are
inherently undirected and directions are only imposed on them to reduce the
danger of colliding vehicles and similar problems. Thus the question arises,
what influence the orientation of the network has on the network flow over time
problem that is being solved on the oriented network. In the literature, this
is also referred to as the contraflow or lane reversal problem.
  We introduce and analyze the price of orientation: How much flow is lost in
any orientation of the network if the time horizon remains fixed? We prove that
there is always an orientation where we can still send $\frac{1}{3}$ of the
flow and this bound is tight. For the special case of networks with a single
source or sink, this fraction is $\frac12$ which is again tight. We present
more results of similar flavor and also show non-approximability results for
finding the best orientation for single and multicommodity maximum flows over
time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3083</identifier>
 <datestamp>2015-04-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3083</id><created>2014-09-10</created><updated>2015-02-02</updated><authors><author><keyname>Erhard</keyname><forenames>Michael</forenames></author><author><keyname>Strauch</keyname><forenames>Hans</forenames></author></authors><title>Flight control of tethered kites in autonomous pumping cycles for
  airborne wind energy</title><categories>cs.SY</categories><comments>16 pages, 22 figures, minor revision submitted to Control Engineering
  Practice</comments><journal-ref>Control Engineering Practice, Vol. 40, July 2015, 13-26</journal-ref><doi>10.1016/j.conengprac.2015.03.001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy harvesting based on tethered kites benefits from exploiting higher
wind speeds at higher altitudes. The setup considered in this paper is based on
a pumping cycle. It generates energy by winching out at high tether forces,
driving an electrical generator while flying crosswind. Then it winches in at a
stationary neutral position, thus leaving a net amount of generated energy.
  The focus of this paper is put on the flight control design, which implements
an accurate direction control towards target points and allows for a flight
with an eight-down pattern. An extended overview on the control system
approach, as well as details of each element of the flight controller, are
presented. The control architecture is motivated by a simple, yet comprehensive
model for the kite dynamics.
  In addition, winch strategies based on an optimization scheme are presented.
In order to demonstrate the real world functionality of the presented
algorithms, flight data from a fully automated pumping-cycle operation of a
small-scale prototype are given. The setup is based on a 30m$^2$ kite linked to
a ground-based 50kW electrical motor/generator by a single line.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3092</identifier>
 <datestamp>2014-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3092</id><created>2014-09-10</created><authors><author><keyname>Xu</keyname><forenames>Weitao</forenames></author><author><keyname>Yuan</keyname><forenames>Dongfeng</forenames></author><author><keyname>Xue</keyname><forenames>Liangfei</forenames></author></authors><title>Design and Implementation of Intelligent Community System Based on Thin
  Client and Cloud Computing</title><categories>cs.CY cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the continuous development of science and technology, the intelligent
development of community system becomes a trend. Meanwhile, smart mobile
devices and cloud computing technology are increasingly used in intelligent
information systems; however, smart mobile devices such as smartphone and smart
pad, also known as thin clients, limited by either their capacities (CPU,
memory or battery) or their network resources, do not always meet users'
satisfaction in using mobile services. Mobile cloud computing, in which
resource-rich virtual machines of smart mobile device are provided to a
customer as a service, can be terrific solution for expanding the limitation of
real smart mobile device, but the resources utilization rate is low and the
information cannot be shared easily. To address the problems above, this paper
proposes an information system for intelligent community, which is composed of
thin clients, wide band network and cloud computing servers. On one hand, the
thin clients with the characteristics of energy efficiency, high robustness and
high computing capacity can efficiently avoid the problems encountered in the
PC architecture and mobile devices. On the other hand, the cloud computing
servers in the proposed information system solve the problems of resource
sharing barriers. Finally, the system is built in real environments to evaluate
the performance. We deploy the proposed system in a community with more than
2000 residents, and it is demonstrated that the proposed system is robust and
efficient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3093</identifier>
 <datestamp>2014-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3093</id><created>2014-09-10</created><updated>2014-11-08</updated><authors><author><keyname>Kalai</keyname><forenames>Gil</forenames></author><author><keyname>Kindler</keyname><forenames>Guy</forenames></author></authors><title>Gaussian Noise Sensitivity and BosonSampling</title><categories>quant-ph cs.CC</categories><comments>Version 2: A few corrections and additions</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the sensitivity to noise of |permanent(X)|^2 for random real and
complex n x n Gaussian matrices X, and show that asymptotically the correlation
between the noisy and noiseless outcomes tends to zero when the noise level is
{\omega}(1)/n. This suggests that, under certain reasonable noise models, the
probability distributions produced by noisy BosonSampling are very sensitive to
noise. We also show that when the amount of noise is constant the noisy value
of |permanent(X)|^2 can be approximated efficiently on a classical computer.
These results seem to weaken the possibility of demonstrating quantum-speedup
via BosonSampling without quantum fault-tolerance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3107</identifier>
 <datestamp>2015-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3107</id><created>2014-09-10</created><updated>2015-01-07</updated><authors><author><keyname>Che</keyname><forenames>Yue Ling</forenames></author><author><keyname>Duan</keyname><forenames>Lingjie</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author></authors><title>Spatial Throughput Maximization of Wireless Powered Communication
  Networks</title><categories>cs.IT math.IT</categories><comments>15 double-column pages, 8 figures, to appear in IEEE JSAC in February
  2015, special issue on wireless communications powered by energy harvesting
  and wireless energy transfer</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless charging is a promising way to power wireless nodes' transmissions.
This paper considers new dual-function access points (APs) which are able to
support the energy/information transmission to/from wireless nodes. We focus on
a large-scale wireless powered communication network (WPCN), and use stochastic
geometry to analyze the wireless nodes' performance tradeoff between energy
harvesting and information transmission. We study two cases with battery-free
and battery-deployed wireless nodes. For both cases, we consider a
harvest-then-transmit protocol by partitioning each time frame into a downlink
(DL) phase for energy transfer, and an uplink (UL) phase for information
transfer. By jointly optimizing frame partition between the two phases and the
wireless nodes' transmit power, we maximize the wireless nodes' spatial
throughput subject to a successful information transmission probability
constraint. For the battery-free case, we show that the wireless nodes prefer
to choose small transmit power to obtain large transmission opportunity. For
the battery-deployed case, we first study an ideal infinite-capacity battery
scenario for wireless nodes, and show that the optimal charging design is not
unique, due to the sufficient energy stored in the battery. We then extend to
the practical finite-capacity battery scenario. Although the exact performance
is difficult to be obtained analytically, it is shown to be upper and lower
bounded by those in the infinite-capacity battery scenario and the battery-free
case, respectively. Finally, we provide numerical results to corroborate our
study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3108</identifier>
 <datestamp>2014-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3108</id><created>2014-09-10</created><authors><author><keyname>Liang</keyname><forenames>Shuying</forenames></author><author><keyname>Sun</keyname><forenames>Weibin</forenames></author><author><keyname>Might</keyname><forenames>Matthew</forenames></author><author><keyname>Keep</keyname><forenames>Andy</forenames></author><author><keyname>Van Horn</keyname><forenames>David</forenames></author></authors><title>Pruning, Pushdown Exception-Flow Analysis</title><categories>cs.PL</categories><comments>14th IEEE International Working Conference on Source Code Analysis
  and Manipulation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Statically reasoning in the presence of exceptions and about the effects of
exceptions is challenging: exception-flows are mutually determined by
traditional control-flow and points-to analyses. We tackle the challenge of
analyzing exception-flows from two angles. First, from the angle of pruning
control-flows (both normal and exceptional), we derive a pushdown framework for
an object-oriented language with full-featured exceptions. Unlike traditional
analyses, it allows precise matching of throwers to catchers. Second, from the
angle of pruning points-to information, we generalize abstract garbage
collection to object-oriented programs and enhance it with liveness analysis.
We then seamlessly weave the techniques into enhanced reachability computation,
yielding highly precise exception-flow analysis, without becoming intractable,
even for large applications. We evaluate our pruned, pushdown exception-flow
analysis, comparing it with an established analysis on large scale standard
Java benchmarks. The results show that our analysis significantly improves
analysis precision over traditional analysis within a reasonable analysis time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3126</identifier>
 <datestamp>2014-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3126</id><created>2014-09-10</created><authors><author><keyname>Akin</keyname><forenames>Sami</forenames></author><author><keyname>Gursoy</keyname><forenames>Mustafa Cenk</forenames></author></authors><title>Performance Analysis of Cognitive Radio Systems with Imperfect Channel
  Sensing and Estimation</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In cognitive radio systems, employing sensing-based spectrum access
strategies, secondary users are required to perform channel sensing in order to
detect the activities of primary users. In realistic scenarios, channel sensing
occurs with possible errors due to miss-detections and false alarms. As another
challenge, time-varying fading conditions in the channel between the secondary
transmitter and the secondary receiver have to be learned via channel
estimation. In this paper, performance of causal channel estimation methods in
correlated cognitive radio channels under imperfect channel sensing results is
analyzed, and achievable rates under both channel and sensing uncertainty are
investigated. Initially, cognitive radio channel model with channel sensing
error and channel estimation is described. Then, using pilot symbols, minimum
mean square error (MMSE) and linear-MMSE (L-MMSE) estimation methods are
employed at the secondary receiver to learn the channel fading coefficients.
Expressions for the channel estimates and mean-squared errors (MSE) are
determined, and their dependencies on channel sensing results, and pilot symbol
period and energy are investigated. Since sensing uncertainty leads to
uncertainty in the variance of the additive disturbance, channel estimation
strategies and performance are interestingly shown to depend on the sensing
reliability. It is further shown that the L-MMSE estimation method, which is in
general suboptimal, performs very close to MMSE estimation. Furthermore,
assuming the channel estimation errors and the interference introduced by the
primary users as zero-mean and Gaussian distributed, achievable rate
expressions of linear modulation schemes and Gaussian signaling are determined.
Subsequently, the training period, and data and pilot symbol energy allocations
are jointly optimized to maximize the achievable rates for both signaling
schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3136</identifier>
 <datestamp>2014-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3136</id><created>2014-09-10</created><authors><author><keyname>Garreau</keyname><forenames>Damien</forenames><affiliation>INRIA Paris - Rocquencourt, DI-ENS</affiliation></author><author><keyname>Lajugie</keyname><forenames>R&#xe9;mi</forenames><affiliation>INRIA Paris - Rocquencourt, DI-ENS</affiliation></author><author><keyname>Arlot</keyname><forenames>Sylvain</forenames><affiliation>INRIA Paris - Rocquencourt, DI-ENS</affiliation></author><author><keyname>Bach</keyname><forenames>Francis</forenames><affiliation>INRIA Paris - Rocquencourt, DI-ENS</affiliation></author></authors><title>Metric Learning for Temporal Sequence Alignment</title><categories>cs.LG</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose to learn a Mahalanobis distance to perform
alignment of multivariate time series. The learning examples for this task are
time series for which the true alignment is known. We cast the alignment
problem as a structured prediction task, and propose realistic losses between
alignments for which the optimization is tractable. We provide experiments on
real data in the audio to audio context, where we show that the learning of a
similarity measure leads to improvements in the performance of the alignment
task. We also propose to use this metric learning framework to perform feature
selection and, from basic audio features, build a combination of these with
better performance for the alignment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3143</identifier>
 <datestamp>2014-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3143</id><created>2014-09-09</created><updated>2014-09-11</updated><authors><author><keyname>Gomez-Diaz</keyname><forenames>Teresa</forenames><affiliation>LIGM</affiliation></author></authors><title>Free software, Open source software, licenses. A short presentation
  including a procedure for research software and data dissemination</title><categories>cs.CY</categories><comments>4 pages</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main goal of this document is to help the research community to
understand the basic concepts of software distribution: Free software, Open
source software, licenses. This document also includes a procedure for research
software and data dissemination.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3144</identifier>
 <datestamp>2014-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3144</id><created>2014-09-09</created><authors><author><keyname>Lang</keyname><forenames>Duncan Temple</forenames></author></authors><title>Enhancing R with Advanced Compilation Tools and Methods</title><categories>stat.CO cs.MS cs.PL</categories><comments>Published in at http://dx.doi.org/10.1214/13-STS462 the Statistical
  Science (http://www.imstat.org/sts/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-STS-STS462</report-no><journal-ref>Statistical Science 2014, Vol. 29, No. 2, 181-200</journal-ref><doi>10.1214/13-STS462</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  I describe an approach to compiling common idioms in R code directly to
native machine code and illustrate it with several examples. Not only can this
yield significant performance gains, but it allows us to use new approaches to
computing in R. Importantly, the compilation requires no changes to R itself,
but is done entirely via R packages. This allows others to experiment with
different compilation strategies and even to define new domain-specific
languages within R. We use the Low-Level Virtual Machine (LLVM) compiler
toolkit to create the native code and perform sophisticated optimizations on
the code. By adopting this widely used software within R, we leverage its
ability to generate code for different platforms such as CPUs and GPUs, and
will continue to benefit from its ongoing development. This approach
potentially allows us to develop high-level R code that is also fast, that can
be compiled to work with different data representations and sources, and that
could even be run outside of R. The approach aims to both provide a compiler
for a limited subset of the R language and also to enable R programmers to
write other compilers. This is another approach to help us write high-level
descriptions of what we want to compute, not how.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3160</identifier>
 <datestamp>2014-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3160</id><created>2014-09-10</created><authors><author><keyname>Dallal</keyname><forenames>Jehad Al</forenames></author></authors><title>Extended distributed UML-based protocol synthesis method</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Synthesizing specifications for real time applications that involve
distributed communication protocol entities from a service specification, which
is modeled in the UML state machine with composite states, is a time-consuming
and labor-intensive task. Existing synthesis techniques for UML-based service
specifications do not account for timing constrains and, therefore, cannot be
used in real time applications for which the timing constraints are crucial and
must be considered. In this paper, we address the problem of time assignment to
the events defined in the service specification modeled in UML state machine.
In addition, we show how to extend a technique that automatically synthesizes
UML-based protocol specifications from a service specification to consider the
timing constraints given in the service specification. The resulting
synthesized protocol is guaranteed to conform to the timing constraints given
in the service specification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3165</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3165</id><created>2014-09-10</created><updated>2015-10-15</updated><authors><author><keyname>Hilmarsson</keyname><forenames>&#xcd;sak</forenames></author><author><keyname>J&#xf3;nsd&#xf3;ttir</keyname><forenames>Ingibj&#xf6;rg</forenames></author><author><keyname>Sigur&#xf0;ard&#xf3;ttir</keyname><forenames>Steinunn</forenames></author><author><keyname>Vi&#xf0;arsd&#xf3;ttir</keyname><forenames>L&#xed;na</forenames></author><author><keyname>Ulfarsson</keyname><forenames>Henning</forenames></author></authors><title>Wilf-classification of mesh patterns of short length</title><categories>math.CO cs.DM</categories><comments>27 pages, 3 figures</comments><msc-class>05A05, 05A15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper starts the Wilf-classification of mesh patterns of length 2.
Although there are initially 1024 patterns to consider we introduce automatic
methods to reduce the number of potentially different Wilf-classes to at most
65. By enumerating some of the remaining classes we bring that upper-bound
further down to 56. Finally, we conjecture that the actual number of
Wilf-classes of mesh patterns of length 2 is 46.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3174</identifier>
 <datestamp>2014-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3174</id><created>2014-09-10</created><authors><author><keyname>Bakshy</keyname><forenames>Eytan</forenames></author><author><keyname>Eckles</keyname><forenames>Dean</forenames></author><author><keyname>Bernstein</keyname><forenames>Michael S.</forenames></author></authors><title>Designing and Deploying Online Field Experiments</title><categories>cs.HC cs.PL cs.SI stat.AP</categories><comments>Proceedings of the 23rd international conference on World wide web,
  283-292</comments><acm-class>H.5.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online experiments are widely used to compare specific design alternatives,
but they can also be used to produce generalizable knowledge and inform
strategic decision making. Doing so often requires sophisticated experimental
designs, iterative refinement, and careful logging and analysis. Few tools
exist that support these needs. We thus introduce a language for online field
experiments called PlanOut. PlanOut separates experimental design from
application code, allowing the experimenter to concisely describe experimental
designs, whether common &quot;A/B tests&quot; and factorial designs, or more complex
designs involving conditional logic or multiple experimental units. These
latter designs are often useful for understanding causal mechanisms involved in
user behaviors. We demonstrate how experiments from the literature can be
implemented in PlanOut, and describe two large field experiments conducted on
Facebook with PlanOut. For common scenarios in which experiments are run
iteratively and in parallel, we introduce a namespaced management system that
encourages sound experimental practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3176</identifier>
 <datestamp>2014-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3176</id><created>2014-09-10</created><authors><author><keyname>Xuan</keyname><forenames>Jifeng</forenames><affiliation>INRIA Lille - Nord Europe</affiliation></author><author><keyname>Monperrus</keyname><forenames>Martin</forenames><affiliation>INRIA Lille - Nord Europe</affiliation></author></authors><title>Test Case Purification for Improving Fault Localization</title><categories>cs.SE</categories><proxy>ccsd</proxy><journal-ref>FSE - 22nd ACM SIGSOFT International Symposium on the Foundations
  of Software Engineering (2014)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Finding and fixing bugs are time-consuming activities in software
development. Spectrum-based fault localization aims to identify the faulty
position in source code based on the execution trace of test cases. Failing
test cases and their assertions form test oracles for the failing behavior of
the system under analysis. In this paper, we propose a novel concept of
spectrum driven test case purification for improving fault localization. The
goal of test case purification is to separate existing test cases into small
fractions (called purified test cases) and to enhance the test oracles to
further localize faults. Combining with an original fault localization
technique (e.g., Tarantula), test case purification results in better ranking
the program statements. Our experiments on 1800 faults in six open-source Java
programs show that test case purification can effectively improve existing
fault localization techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3182</identifier>
 <datestamp>2015-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3182</id><created>2014-09-10</created><updated>2015-02-19</updated><authors><author><keyname>Gharibian</keyname><forenames>Sevag</forenames></author><author><keyname>Sikora</keyname><forenames>Jamie</forenames></author></authors><title>Ground state connectivity of local Hamiltonians</title><categories>quant-ph cond-mat.str-el cs.CC</categories><comments>29 pages; v2 includes a new result concerning the tightness of the
  Traversal Lemma, as well as minor changes throughout to address anonymous
  referee comments (we thank the referees for their feedback)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The study of ground state energies of local Hamiltonians has played a
fundamental role in quantum complexity theory. In this paper, we take a new
direction by introducing the physically motivated notion of &quot;ground state
connectivity&quot; of local Hamiltonians, which captures problems in areas ranging
from quantum stabilizer codes to quantum memories. We show that determining how
&quot;connected&quot; the ground space of a local Hamiltonian is can range from
QCMA-complete to NEXP-complete. As a result, we obtain a natural QCMA-complete
problem, a goal which has generally proven difficult since the conception of
QCMA over a decade ago. Our proofs rely on a new technical tool, the Traversal
Lemma, which analyzes the Hilbert space a local unitary evolution must traverse
under certain conditions. We show that this lemma is tight up to a polynomial
factor with respect to the length of the unitary evolution in question.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3184</identifier>
 <datestamp>2014-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3184</id><created>2014-09-10</created><authors><author><keyname>Rebiha</keyname><forenames>Rachid</forenames></author><author><keyname>Moura</keyname><forenames>Arnaldo Vieira</forenames></author><author><keyname>Matringe</keyname><forenames>Nadir</forenames></author></authors><title>Characterization of Termination for Linear Loop Programs</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present necessary and sufficient conditions for the termination of linear
homogeneous programs. We also develop a complete method to check termination
for this class of programs. Our complete characterization of termination for
such programs is based on linear algebraic methods. We reduce the verification
of the termination problem to checking the orthogonality of a well determined
vector space and a certain vector, both related to loops in the program.
Moreover, we provide theoretical results and symbolic computational methods
guaranteeing the soundness, completeness and numerical stability of the
approach. Finally, we show that it is enough to interpret variable values over
a specific countable number field, or even over its ring of integers, when one
wants to check termination over the reals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3188</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3188</id><created>2014-09-10</created><updated>2014-11-23</updated><authors><author><keyname>Ernvall-Hyt&#xf6;nen</keyname><forenames>Anne-Maria</forenames></author><author><keyname>Sethuraman</keyname><forenames>B. A.</forenames></author></authors><title>Counterexample to the $l$-modular Belfiore-Sol\'e Conjecture</title><categories>cs.IT math.IT math.NT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the secrecy function conjecture that states that the maximum of
the secrecy function of an $l$-modular lattice occurs at $1/\sqrt{l}$ is false,
by proving that the 4-modular lattice $C^(4) = \mathbb{Z} \oplus
\sqrt{2}\mathbb{Z} \oplus 2\mathbb{Z}$ fails to satisfy this conjecture. We
also indicate how the secrecy function must be modified in the $l$-modular case
to have a more reasonable chance for it to have a maximum at $1/\sqrt{l}$, and
show that the conjecture, modified with this new secrecy function, is true for
various odd 2-modular lattices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3192</identifier>
 <datestamp>2014-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3192</id><created>2014-09-10</created><authors><author><keyname>Goodrich</keyname><forenames>Michael T.</forenames></author><author><keyname>Pszona</keyname><forenames>Pawe&#x142;</forenames></author></authors><title>Two-Phase Bicriterion Search for Finding Fast and Efficient Electric
  Vehicle Routes</title><categories>cs.DS</categories><comments>11 pages, 5 tables, 10 figures. To appear at ACM SIGSPATIAL 2014</comments><acm-class>F.2.2; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of finding an electric vehicle route that optimizes both driving
time and energy consumption can be modeled as a bicriterion path problem.
Unfortunately, the problem of finding optimal bicriterion paths is NP-complete.
This paper studies such problems restricted to two-phase paths, which
correspond to a common way people drive electric vehicles, where a driver uses
one driving style (say, minimizing driving time) at the beginning of a route
and another driving style (say, minimizing energy consumption) at the end. We
provide efficient polynomial-time algorithms for finding optimal two-phase
paths in bicriterion networks, and we empirically verify the effectiveness of
these algorithms for finding good electric vehicle driving routes in the road
networks of various U.S. states. In addition, we show how to incorporate
charging stations into these algorithms, in spite of the computational
challenges introduced by the negative energy consumption of such network
vertices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3206</identifier>
 <datestamp>2014-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3206</id><created>2014-09-10</created><authors><author><keyname>Georgiev</keyname><forenames>Petko</forenames></author><author><keyname>Lane</keyname><forenames>Nicholas D.</forenames></author><author><keyname>Rachuri</keyname><forenames>Kiran K.</forenames></author><author><keyname>Mascolo</keyname><forenames>Cecilia</forenames></author></authors><title>DSP.Ear: Leveraging Co-Processor Support for Continuous Audio Sensing on
  Smartphones</title><categories>cs.SD</categories><comments>15 pages, 12th ACM Conference on Embedded Network Sensor Systems
  (SenSys '14)</comments><acm-class>H.1.2; C.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rapidly growing adoption of sensor-enabled smartphones has greatly fueled
the proliferation of applications that use phone sensors to monitor user
behavior. A central sensor among these is the microphone which enables, for
instance, the detection of valence in speech, or the identification of
speakers. Deploying multiple of these applications on a mobile device to
continuously monitor the audio environment allows for the acquisition of a
diverse range of sound-related contextual inferences. However, the cumulative
processing burden critically impacts the phone battery.
  To address this problem, we propose DSP.Ear - an integrated sensing system
that takes advantage of the latest low-power DSP co-processor technology in
commodity mobile devices to enable the continuous and simultaneous operation of
multiple established algorithms that perform complex audio inferences. The
system extracts emotions from voice, estimates the number of people in a room,
identifies the speakers, and detects commonly found ambient sounds, while
critically incurring little overhead to the device battery. This is achieved
through a series of pipeline optimizations that allow the computation to remain
largely on the DSP. Through detailed evaluation of our prototype implementation
we show that, by exploiting a smartphone's co-processor, DSP.Ear achieves a 3
to 7 times increase in the battery lifetime compared to a solution that uses
only the phone's main processor. In addition, DSP.Ear is 2 to 3 times more
power efficient than a naive DSP solution without optimizations. We further
analyze a large-scale dataset from 1320 Android users to show that in about
80-90% of the daily usage instances DSP.Ear is able to sustain a full day of
operation (even in the presence of other smartphone workloads) with a single
battery charge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3207</identifier>
 <datestamp>2015-05-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3207</id><created>2014-09-10</created><updated>2015-05-01</updated><authors><author><keyname>Chen</keyname><forenames>Pin-Yu</forenames></author><author><keyname>Hero</keyname><forenames>Alfred O.</forenames><suffix>III</suffix></author></authors><title>Phase Transitions in Spectral Community Detection</title><categories>cs.SI physics.soc-ph</categories><comments>9 pages, 4 figures, submitted to IEEE Trans. on Signal Processing.
  arXiv admin note: text overlap with arXiv:1504.02412</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a network consisting of two subnetworks (communities) connected by
some external edges. Given the network topology, the community detection
problem can be cast as a graph partitioning problem that aims to identify the
external edges as the graph cut that separates these two subnetworks. In this
paper, we consider a general model where two arbitrarily connected subnetworks
are connected by random external edges. Using random matrix theory and
concentration inequalities, we show that when one performs community detection
via spectral clustering there exists an abrupt phase transition as a function
of the random external edge connection probability. Specifically, the community
detection performance transitions from almost perfect detectability to low
detectability near some critical value of the random external edge connection
probability. We derive upper and lower bounds on the critical value and show
that the bounds are equal to each other when two subnetwork sizes are
identical. Using simulated and experimental data we show how these bounds can
be empirically estimated to validate the detection reliability of any
discovered communities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3208</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3208</id><created>2014-09-10</created><updated>2015-01-20</updated><authors><author><keyname>Bermejo-Vega</keyname><forenames>Juan</forenames></author><author><keyname>Lin</keyname><forenames>Cedric Yen-Yu</forenames></author><author><keyname>Nest</keyname><forenames>Maarten Van den</forenames></author></authors><title>Normalizer circuits and a Gottesman-Knill theorem for
  infinite-dimensional systems</title><categories>quant-ph cs.CC math-ph math.MP</categories><comments>54 pages + appendices. The discussion has been extended in this
  version</comments><report-no>MIT-CTP/4583</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  $\textit{Normalizer circuits}$ [1,2] are generalized Clifford circuits that
act on arbitrary finite-dimensional systems $\mathcal{H}_{d_1}\otimes ...
\otimes \mathcal{H}_{d_n}$ with a standard basis labeled by the elements of a
finite Abelian group $G=\mathbb{Z}_{d_1}\times... \times \mathbb{Z}_{d_n}$.
Normalizer gates implement operations associated with the group $G$ and can be
of three types: quantum Fourier transforms, group automorphism gates and
quadratic phase gates. In this work, we extend the normalizer formalism [1,2]
to infinite dimensions, by allowing normalizer gates to act on systems of the
form $\mathcal{H}_\mathbb{Z}^{\otimes a}$: each factor $\mathcal{H}_\mathbb{Z}$
has a standard basis labeled by $\textit{integers}$ $\mathbb{Z}$, and a Fourier
basis labeled by $\textit{angles}$, elements of the circle group $\mathbb{T}$.
Normalizer circuits become hybrid quantum circuits acting both on continuous-
and discrete-variable systems. We show that infinite-dimensional normalizer
circuits can be efficiently simulated classically with a generalized
$\textit{stabilizer formalism}$ for Hilbert spaces associated with groups of
the form $\mathbb{Z}^a\times \mathbb{T}^b \times
\mathbb{Z}_{d_1}\times...\times \mathbb{Z}_{d_n}$. We develop new techniques to
track stabilizer-groups based on normal forms for group automorphisms and
quadratic functions. We use our normal forms to reduce the problem of
simulating normalizer circuits to that of finding general solutions of systems
of mixed real-integer linear equations [3] and exploit this fact to devise a
robust simulation algorithm: the latter remains efficient even in pathological
cases where stabilizer groups become infinite, uncountable and non-compact. The
techniques developed in this paper might find applications in the study of
fault-tolerant quantum computation with superconducting qubits [4,5].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3211</identifier>
 <datestamp>2014-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3211</id><created>2014-09-10</created><authors><author><keyname>Tschantz</keyname><forenames>Michael Carl</forenames></author><author><keyname>Afroz</keyname><forenames>Sadia</forenames></author><author><keyname>Paxson</keyname><forenames>Vern</forenames></author><author><keyname>Tygar</keyname><forenames>J. D.</forenames></author></authors><title>On Modeling the Costs of Censorship</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We argue that the evaluation of censorship evasion tools should depend upon
economic models of censorship. We illustrate our position with a simple model
of the costs of censorship. We show how this model makes suggestions for how to
evade censorship. In particular, from it, we develop evaluation criteria. We
examine how our criteria compare to the traditional methods of evaluation
employed in prior works.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3215</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3215</id><created>2014-09-10</created><updated>2014-12-14</updated><authors><author><keyname>Sutskever</keyname><forenames>Ilya</forenames></author><author><keyname>Vinyals</keyname><forenames>Oriol</forenames></author><author><keyname>Le</keyname><forenames>Quoc V.</forenames></author></authors><title>Sequence to Sequence Learning with Neural Networks</title><categories>cs.CL cs.LG</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep Neural Networks (DNNs) are powerful models that have achieved excellent
performance on difficult learning tasks. Although DNNs work well whenever large
labeled training sets are available, they cannot be used to map sequences to
sequences. In this paper, we present a general end-to-end approach to sequence
learning that makes minimal assumptions on the sequence structure. Our method
uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to
a vector of a fixed dimensionality, and then another deep LSTM to decode the
target sequence from the vector. Our main result is that on an English to
French translation task from the WMT'14 dataset, the translations produced by
the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's
BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did
not have difficulty on long sentences. For comparison, a phrase-based SMT
system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM
to rerank the 1000 hypotheses produced by the aforementioned SMT system, its
BLEU score increases to 36.5, which is close to the previous best result on
this task. The LSTM also learned sensible phrase and sentence representations
that are sensitive to word order and are relatively invariant to the active and
the passive voice. Finally, we found that reversing the order of the words in
all source sentences (but not target sentences) improved the LSTM's performance
markedly, because doing so introduced many short term dependencies between the
source and the target sentence which made the optimization problem easier.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3225</identifier>
 <datestamp>2014-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3225</id><created>2014-09-10</created><authors><author><keyname>Aggarwal</keyname><forenames>Saurabh</forenames></author><author><keyname>Kuri</keyname><forenames>Joy</forenames></author></authors><title>Strategies for Utility Maximization in Social Groups with Preferential
  Exploration</title><categories>cs.GT cs.DS cs.MA cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a \emph{Social Group} of networked nodes, seeking a &quot;universe&quot; of
segments for maximization of their utility. Each node has a subset of the
universe, and access to an expensive link for downloading data. Nodes can also
acquire the universe by exchanging copies of segments among themselves, at low
cost, using inter-node links. While exchanges over inter-node links ensure
minimum or negligible cost, some nodes in the group try to exploit the system.
We term such nodes as `non-reciprocating nodes' and prohibit such behavior by
proposing the &quot;Give-and-Take&quot; criterion, where exchange is allowed iff each
participating node has segments unavailable with the other. Following this
criterion for inter-node links, each node wants to maximize its utility, which
depends on the node's segment set available with the node. Link activation
among nodes requires mutual consent of participating nodes. Each node tries to
find a pairing partner by preferentially exploring nodes for link formation and
unpaired nodes choose to download a segment using the expensive link with
segment aggressive probability. We present various linear complexity
decentralized algorithms based on \emph{Stable Roommates Problem} that can be
used by nodes (as per their behavioral nature) for choosing the best strategy
based on available information. Then, we present decentralized randomized
algorithm that performs close to optimal for large number of nodes. We define
\emph{Price of Choices} for benchmarking performance for social groups
(consisting of non-aggressive nodes only). We evaluate performances of various
algorithms and characterize the behavioral regime that will yield best results
for node and social group, spending the minimal on expensive link. We consider
social group consisting of non-aggressive nodes and benchmark performances of
proposed algorithms with the optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3246</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3246</id><created>2014-09-10</created><authors><author><keyname>Bogale</keyname><forenames>Tadilo Endeshaw</forenames></author><author><keyname>Vandendorpe</keyname><forenames>Luc</forenames></author><author><keyname>Le</keyname><forenames>Long Bao</forenames></author></authors><title>Wideband Sensing and Optimization for Cognitive Radio Networks with
  Noise Variance Uncertainty</title><categories>stat.AP cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Communications (Revised version)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers wide-band spectrum sensing and optimization for
cognitive radio (CR) networks with noise variance uncertainty. It is assumed
that the considered wide-band contains one or more white sub-bands. Under this
assumption, we consider throughput maximization of the CR network while
appropriately protecting the primary network. We address this problem as
follows. First, we propose novel ratio based test statistics for detecting the
edges of each sub-band. Second, we employ simple energy comparison approach to
choose one reference white sub-band. Third, we propose novel generalized energy
detector (GED) for examining each of the remaining sub-bands by exploiting the
noise information of the reference white sub-band. Finally, we optimize the
sensing time ($T_o$) to maximize the CR network throughput using the detection
and false alarm probabilities of the GED. The proposed GED does not suffer from
signal to noise ratio (SNR) wall and outperforms the existing signal detectors.
Moreover, the relationship between the proposed GED and conventional energy
detector (CED) is quantified analytically. We show that the optimal $T_o$
depends on the noise variance information. In particular, with $10$TV bands,
SNR=$-20$dB and $2$s frame duration, we found that the optimal $T_o$ is
$28.5$ms ($50.6$ms) with perfect (imperfect) noise variance scenario.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3275</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3275</id><created>2014-09-10</created><authors><author><keyname>Gomaa</keyname><forenames>Ahmad</forenames></author><author><keyname>Jalloul</keyname><forenames>Louay</forenames></author></authors><title>Efficient Soft-Input Soft-Output MIMO Chase Detectors for arbitrary
  number of streams</title><categories>cs.IT math.IT</categories><comments>5 pages, 3 figures</comments><doi>10.1109/LSP.2014.2385776</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present novel soft-input soft-output (SISO) multiple-input multiple-output
(MIMO) detectors based on the Chase detection principle [1] in the context of
iterative and decoding (IDD). The proposed detector complexity is linear in the
signal modulation constellation size and the number of spatial streams. Two
variants of the SISO detector are developed, referred to as SISO B-Chase and
SISO L-Chase. An efficient method is presented that uses the decoder output to
modulate the signal constellation decision boundaries inside the detector
leading to the SISO detector architecture. The performance of these detectors
significantly improves with just a few number of IDD iterations. The effect of
transmit and receive antenna correlation is simulated. For the high-correlation
case, the superiority of SISO B-Chase over the SISO L-Chase is demonstrated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3276</identifier>
 <datestamp>2014-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3276</id><created>2014-09-10</created><authors><author><keyname>Tomas</keyname><forenames>Bill Jason</forenames></author><author><keyname>Jiang</keyname><forenames>Yingtao</forenames></author><author><keyname>Yang</keyname><forenames>Mei</forenames></author></authors><title>Co-Emulation of Scan-Chain Based Designs Utilizing SCE-MI Infrastructure</title><categories>cs.OH</categories><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  As the complexity of the scan algorithm is dependent on the number of design
registers, large SoC scan designs can no longer be verified in RTL simulation
unless partitioned into smaller sub-blocks. This paper proposes a methodology
to decrease scan-chain verification time utilizing SCE-MI, a widely used
communication protocol for emulation, and an FPGA-based emulation platform. A
high-level (SystemC) testbench and FPGA synthesizable hardware transactor
models are developed for the scan-chain ISCAS89 S400 benchmark circuit for
high-speed communication between the host CPU workstation and the FPGA
emulator. The emulation results are compared to other verification
methodologies (RTL Simulation, Simulation Acceleration, and Transaction-based
emulation), and found to be 82% faster than regular RTL simulation. In
addition, the emulation runs in the MHz speed range, allowing the incorporation
of software applications, drivers, and operating systems, as opposed to the Hz
range in RTL simulation or sub-megahertz range as accomplished in
transaction-based emulation. In addition, the integration of scan testing and
acceleration/emulation platforms allows more complex DFT methods to be
developed and tested on a large scale system, decreasing the time to market for
products.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3288</identifier>
 <datestamp>2014-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3288</id><created>2014-09-10</created><updated>2014-11-13</updated><authors><author><keyname>Hartig</keyname><forenames>Olaf</forenames></author></authors><title>Reconciliation of RDF* and Property Graphs</title><categories>cs.DB</categories><comments>slightly changed the definition of PGs and added the notion of
  property uniqueness</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Both the notion of Property Graphs (PG) and the Resource Description
Framework (RDF) are commonly used models for representing graph-shaped data.
While there exist some system-specific solutions to convert data from one model
to the other, these solutions are not entirely compatible with one another and
none of them appears to be based on a formal foundation. In fact, for the PG
model, there does not even exist a commonly agreed-upon formal definition.
  The aim of this document is to reconcile both models formally. To this end,
the document proposes a formalization of the PG model and introduces
well-defined transformations between PGs and RDF. As a result, the document
provides a basis for the following two innovations: On one hand, by
implementing the RDF-to-PG transformations defined in this document, PG-based
systems can enable their users to load RDF data and make it accessible in a
compatible, system-independent manner using, e.g., the graph traversal language
Gremlin or the declarative graph query language Cypher. On the other hand, the
PG-to-RDF transformation in this document enables RDF data management systems
to support compatible, system-independent queries over the content of Property
Graphs by using the standard RDF query language SPARQL. Additionally, this
document represents a foundation for systematic research on relationships
between the two models and between their query languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3289</identifier>
 <datestamp>2015-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3289</id><created>2014-09-10</created><updated>2015-04-03</updated><authors><author><keyname>Tzoumas</keyname><forenames>V.</forenames></author><author><keyname>Rahimian</keyname><forenames>M. A.</forenames></author><author><keyname>Pappas</keyname><forenames>G. J.</forenames></author><author><keyname>Jadbabaie</keyname><forenames>A.</forenames></author></authors><title>Minimal Actuator Placement with Bounds on Control Effort</title><categories>math.OC cs.MA cs.SY math.DS</categories><comments>Some content as v3; this version corrects a latex compilation bug of
  that version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of minimal actuator placement in a linear system
subject to an average control energy bound. First, following the recent work of
Olshevsky, we prove that this is NP-hard. Then, we provide an efficient
algorithm which, for a given range of problem parameters, approximates up to a
multiplicative factor of O(logn), n being the network size, any optimal
actuator set that meets the same energy criteria; this is the best
approximation factor one can achieve in polynomial time, in the worst case.
Moreover, the algorithm uses a perturbed version of the involved control energy
metric, which we prove to be supermodular. Next, we focus on the related
problem of cardinality-constrained actuator placement for minimum control
effort, where the optimal actuator set is selected so that an average input
energy metric is minimized. While this is also an NP-hard problem, we use our
proposed algorithm to efficiently approximate its solutions as well. Finally,
we run our algorithms over large random networks to illustrate their
efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3290</identifier>
 <datestamp>2014-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3290</id><created>2014-09-10</created><authors><author><keyname>Xu</keyname><forenames>Wenyan</forenames></author></authors><title>A cirquent calculus system with clustering and ranking</title><categories>cs.LO</categories><comments>arXiv admin note: substantial text overlap with arXiv:1402.4172</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cirquent calculus is a new proof-theoretic and semantic approach introduced
by G.Japaridze for the needs of his theory of computability logic. The earlier
article &quot;From formulas to cirquents in computability logic&quot; by Japaridze
generalized the concept of cirquents to the version with what are termed
clusterng and ranking, and showed that, through cirquents with clustering and
ranking, one can capture, refine and generalize the so called extended IF
logic. Japaridze's treatment of extended IF logic, however, was purely
semantical, and no deductive system was proposed. The present paper
syntactically constructs a cirquent calculus system with clustering and
ranking, sound and complete w.r.t. the propositional fragment of cirquent-based
semantics. Such a system can be considered not only a conservative extension of
classical propositional logic but also, when limited to cirquents with no more
than 2 ranks, an axiomatization of purely propositional extended IF logic in
its full generality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3302</identifier>
 <datestamp>2014-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3302</id><created>2014-09-10</created><updated>2014-09-12</updated><authors><author><keyname>Kroer</keyname><forenames>Christian</forenames></author><author><keyname>Sandholm</keyname><forenames>Tuomas</forenames></author></authors><title>Extensive-Form Game Imperfect-Recall Abstractions With Bounds</title><categories>cs.GT cs.MA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Imperfect-recall abstraction has emerged as the leading paradigm for
practical large-scale equilibrium computation in incomplete-information games.
However, imperfect-recall abstractions are poorly understood, and only weak
algorithm-specific guarantees on solution quality are known. In this paper, we
show the first general, algorithm-agnostic, solution quality guarantees for
Nash equilibria and approximate self-trembling equilibria computed in
imperfect-recall abstractions, when implemented in the original
(perfect-recall) game. Our results are for a class of games that generalizes
the only previously known class of imperfect-recall abstractions where any
results had been obtained. Further, our analysis is tighter in two ways, each
of which can lead to an exponential reduction in the solution quality error
bound.
  We then show that for extensive-form games that satisfy certain properties,
the problem of computing a bound-minimizing abstraction for a single level of
the game reduces to a clustering problem, where the increase in our bound is
the distance function. This reduction leads to the first imperfect-recall
abstraction algorithm with solution quality bounds. We proceed to show a divide
in the class of abstraction problems. If payoffs are at the same scale at all
information sets considered for abstraction, the input forms a metric space.
Conversely, if this condition is not satisfied, we show that the input does not
form a metric space. Finally, we use these results to experimentally
investigate the quality of our bound for single-level abstraction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3307</identifier>
 <datestamp>2014-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3307</id><created>2014-09-10</created><authors><author><keyname>Chang</keyname><forenames>Tsung-Hui</forenames></author></authors><title>A Proximal Dual Consensus ADMM Method for Multi-Agent Constrained
  Optimization</title><categories>cs.SY math.OC</categories><comments>submitted to IEEE Transactions on Signal Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies efficient distributed optimization methods for multi-agent
networks. Specifically, we consider a convex optimization problem with a
globally coupled linear equality constraint and local polyhedra constraints,
and develop distributed optimization methods based on the alternating direction
method of multipliers (ADMM). The considered problem has many applications in
machine learning and smart grid control problems. Due to the presence of the
polyhedra constraints, agents in the existing methods have to deal with
polyhedra constrained subproblems at each iteration. One of the key issues is
that projection onto a polyhedra constraint is not trivial, which prohibits
from closed-form solutions or the use of simple algorithms for solving these
subproblems. In this paper, by judiciously integrating the proximal
minimization method with ADMM, we propose a new distributed optimization method
where the polyhedra constraints are handled softly as penalty terms in the
subproblems. This makes the subproblems efficiently solvable and consequently
reduces the overall computation time. Furthermore, we propose a randomized
counterpart that is robust against randomly ON/OFF agents and imperfect
communication links. We analytically show that both the proposed methods have a
worst-case $\mathcal{O}(1/k)$ convergence rate, where $k$ is the iteration
number. Numerical results show that the proposed methods offer considerably
lower computation time than the existing distributed ADMM method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3313</identifier>
 <datestamp>2014-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3313</id><created>2014-09-10</created><authors><author><keyname>Geuvers</keyname><forenames>Herman</forenames><affiliation>Radboud Universiteit Nijmegen, Technical University Eindhoven</affiliation></author><author><keyname>Geraedts</keyname><forenames>Wouter</forenames><affiliation>Radboud Universiteit Nijmegen</affiliation></author><author><keyname>Geron</keyname><forenames>Bram</forenames><affiliation>University of Birmingham</affiliation></author><author><keyname>van Stegeren</keyname><forenames>Judith</forenames><affiliation>Radboud Universiteit Nijmegen</affiliation></author></authors><title>A type system for Continuation Calculus</title><categories>cs.LO cs.PL</categories><comments>In Proceedings CL&amp;C 2014, arXiv:1409.2593</comments><proxy>EPTCS</proxy><acm-class>F.3.3</acm-class><journal-ref>EPTCS 164, 2014, pp. 1-17</journal-ref><doi>10.4204/EPTCS.164.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Continuation Calculus (CC), introduced by Geron and Geuvers, is a simple
foundational model for functional computation. It is closely related to lambda
calculus and term rewriting, but it has no variable binding and no pattern
matching. It is Turing complete and evaluation is deterministic. Notions like
&quot;call-by-value&quot; and &quot;call-by-name&quot; computation are available by choosing
appropriate function definitions: e.g. there is a call-by-value and a
call-by-name addition function. In the present paper we extend CC with types,
to be able to define data types in a canonical way, and functions over these
data types, defined by iteration. Data type definitions follow the so-called
&quot;Scott encoding&quot; of data, as opposed to the more familiar &quot;Church encoding&quot;.
The iteration scheme comes in two flavors: a call-by-value and a call-by-name
iteration scheme. The call-by-value variant is a double negation variant of
call-by-name iteration. The double negation translation allows to move between
call-by-name and call-by-value.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3314</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3314</id><created>2014-09-10</created><authors><author><keyname>van Bakel</keyname><forenames>Steffen</forenames><affiliation>Imperial College London</affiliation></author><author><keyname>Vigliotti</keyname><forenames>Maria Grazia</forenames><affiliation>Adelard PLC</affiliation></author></authors><title>A fully-abstract semantics of lambda-mu in the pi-calculus</title><categories>cs.LO</categories><comments>In Proceedings CL&amp;C 2014, arXiv:1409.2593</comments><proxy>EPTCS</proxy><acm-class>F.3.2</acm-class><journal-ref>EPTCS 164, 2014, pp. 33-47</journal-ref><doi>10.4204/EPTCS.164.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the lambda-mu-calculus, extended with explicit substitution, and
define a compositional output-based interpretation into a variant of the
pi-calculus with pairing that preserves single-step explicit head reduction
with respect to weak bisimilarity. We define four notions of weak equivalence
for lambda-mu -- one based on weak reduction, two modelling weak head-reduction
and weak explicit head reduction (all considering terms without weak
head-normal form equivalent as well), and one based on weak approximation --
and show they all coincide. We will then show full abstraction results for our
interpretation for the weak equivalences with respect to weak bisimilarity on
processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3315</identifier>
 <datestamp>2014-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3315</id><created>2014-09-10</created><authors><author><keyname>Basaldella</keyname><forenames>Michele</forenames><affiliation>Universit&#xe9; d'Aix-Marseille, CNRS, I2M, Marseille, France</affiliation></author></authors><title>Infinitary Classical Logic: Recursive Equations and Interactive
  Semantics</title><categories>cs.LO</categories><comments>In Proceedings CL&amp;C 2014, arXiv:1409.2593</comments><proxy>EPTCS</proxy><acm-class>F.4.1</acm-class><journal-ref>EPTCS 164, 2014, pp. 48-62</journal-ref><doi>10.4204/EPTCS.164.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present an interactive semantics for derivations in an
infinitary extension of classical logic. The formulas of our language are
possibly infinitary trees labeled by propositional variables and logical
connectives. We show that in our setting every recursive formula equation has a
unique solution. As for derivations, we use an infinitary variant of
Tait-calculus to derive sequents. The interactive semantics for derivations
that we introduce in this article is presented as a debate (interaction tree)
between a test &lt;&lt; T &gt;&gt; (derivation candidate, Proponent) and an environment &lt;&lt;
not S &gt;&gt; (negation of a sequent, Opponent). We show a completeness theorem for
derivations that we call interactive completeness theorem: the interaction
between &lt;&lt; T &gt;&gt; (test) and &lt;&lt; not S &gt;&gt; (environment) does not produce errors
(i.e., Proponent wins) just in case &lt;&lt; T &gt;&gt; comes from a syntactical derivation
of &lt;&lt; S &gt;&gt;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3316</identifier>
 <datestamp>2014-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3316</id><created>2014-09-10</created><authors><author><keyname>Santo</keyname><forenames>Jos&#xe9; Esp&#xed;rito</forenames></author><author><keyname>Matthes</keyname><forenames>Ralph</forenames></author><author><keyname>Nakazawa</keyname><forenames>Koji</forenames></author><author><keyname>Pinto</keyname><forenames>Lu&#xed;s</forenames></author></authors><title>Confluence for classical logic through the distinction between values
  and computations</title><categories>cs.LO</categories><comments>In Proceedings CL&amp;C 2014, arXiv:1409.2593</comments><proxy>EPTCS</proxy><acm-class>F.4.1</acm-class><journal-ref>EPTCS 164, 2014, pp. 63-77</journal-ref><doi>10.4204/EPTCS.164.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We apply an idea originated in the theory of programming languages - monadic
meta-language with a distinction between values and computations - in the
design of a calculus of cut-elimination for classical logic. The
cut-elimination calculus we obtain comprehends the call-by-name and
call-by-value fragments of Curien-Herbelin's lambda-bar-mu-mu-tilde-calculus
without losing confluence, and is based on a distinction of &quot;modes&quot; in the
proof expressions and &quot;mode&quot; annotations in types. Modes resemble colors and
polarities, but are quite different: we give meaning to them in terms of a
monadic meta-language where the distinction between values and computations is
fully explored. This meta-language is a refinement of the classical monadic
language previously introduced by the authors, and is also developed in the
paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3319</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3319</id><created>2014-09-10</created><authors><author><keyname>Longani</keyname><forenames>Pattama</forenames></author></authors><title>Square Grid Points Coveraged by Connected Sources with Coverage Radius
  of One on a Two-Dimensional Grid</title><categories>cs.CC cs.NI</categories><comments>20 pages, 10 figures, International Journal of Computer Science &amp;
  Information Technology (IJCSIT)</comments><msc-class>68M10, 68Q17, 68Q25</msc-class><acm-class>F.4.1; C.2.3</acm-class><journal-ref>International Journal of Computer Science &amp; Information Technology
  (IJCSIT) Vol 6, No 4, August 2014</journal-ref><doi>10.5121/ijcsit</doi><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  We take some parts of a theoretical mobility model in a two-dimension grid
proposed by Greenlaw and Kantabutra to be our model. The model has eight
necessary factors that we commonly use in a mobile wireless network: sources or
wireless signal providers, the directions that a source can move, users or
mobile devices, the given directions which define a user's movement, the given
directions which define a source's movement, source's velocity, source's
coverage, and obstacles. However, we include only the sources, source's
coverage, and the obstacles in our model. We define Square Grid Points Coverage
(SGPC) problem to minimize number of sources with coverage radius of one to
cover a square grid point size of p with the restriction that all the sources
must be communicable and proof that SGPC is in NP-complete class. We also give
an Approx-Square-Grid-Coverage (ASGC) algorithm to compute the approximate
solution of SGPC. ASGC uses the rule that any number can be obtained from the
addition of 3, 4 and 5 and then combines 3-gadgets, 4-gadgets and 5-gadgets to
specify the position of sources to cover a square grid point size of p. We find
that the algorithm achieves an approximation ratio of . Moreover, we state
about the extension usage of our algorithm and show some examples. We show that
if we use ASGC on a square grid size of p and if sources can be moved, the area
under the square grid can be covered in eight-time-steps movement. We also
prove that if we extend our source coverage radius to 1.59, without any
movement the area under the square gird will also be covered. Further studies
are also discussed and a list of some tentative problems is given in the
conclusion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3323</identifier>
 <datestamp>2014-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3323</id><created>2014-09-11</created><authors><author><keyname>Ben-David</keyname><forenames>Shalev</forenames></author></authors><title>The Structure of Promises in Quantum Speedups</title><categories>quant-ph cs.CC</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It has long been known that in the usual black-box model, one cannot get
super-polynomial quantum speedups without some promise on the inputs. In this
paper, we examine certain types of symmetric promises, and show that they also
cannot give rise to super-polynomial quantum speedups. We conclude that
exponential quantum speedups only occur given &quot;structured&quot; promises on the
input.
  Specifically, we show that there is a polynomial relationship of degree $12$
between $D(f)$ and $Q(f)$ for any function $f$ defined on permutations
(elements of $\{0,1,\dots, M-1\}^n$ in which each alphabet element occurs
exactly once). We generalize this result to all functions $f$ defined on orbits
of the symmetric group action $S_n$ (which acts on an element of $\{0,1,\dots,
M-1\}^n$ by permuting its entries). We also show that when $M$ is constant, any
function $f$ defined on a &quot;symmetric set&quot; - one invariant under $S_n$ -
satisfies $R(f)=O(Q(f)^{12(M-1)})$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3330</identifier>
 <datestamp>2014-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3330</id><created>2014-09-11</created><updated>2014-09-17</updated><authors><author><keyname>Makki</keyname><forenames>Behrooz</forenames></author><author><keyname>Svensson</keyname><forenames>Tommy</forenames></author><author><keyname>Zorzi</keyname><forenames>Michele</forenames></author></authors><title>Finite Block-length Analysis of the Incremental Redundancy HARQ</title><categories>cs.IT math.IT</categories><comments>appears in IEEE Wireless Communication Letters 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This letter studies the power-limited throughput of a communication system
utilizing incremental redundancy (INR) hybrid automatic repeat request (HARQ).
We use some recent results on the achievable rates of finite-length codes to
analyze the system performance. With codewords of finite length, we derive
closed-form expressions for the outage probabilities of INR HARQ and study the
throughput in the cases with variable-length coding. Moreover, we evaluate the
effect of feedback delay on the throughput and derive sufficient conditions for
the usefulness of the HARQ protocols, in terms of power-limited throughput. The
results show that, for a large range of HARQ feedback delays, the throughput is
increased by finite-length coding INR HARQ, if the sub-codeword lengths are
properly adapted.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3331</identifier>
 <datestamp>2014-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3331</id><created>2014-09-11</created><authors><author><keyname>Makki</keyname><forenames>Behrooz</forenames></author><author><keyname>Svensson</keyname><forenames>Tommy</forenames></author><author><keyname>Debbah</keyname><forenames>Merouane</forenames></author></authors><title>Reinforcement-based data transmission in temporally-correlated fading
  channels: Partial CSIT scenario</title><categories>cs.IT math.IT</categories><comments>Accepted for publication in ISWCS 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reinforcement algorithms refer to the schemes where the results of the
previous trials and a reward-punishment rule are used for parameter setting in
the next steps. In this paper, we use the concept of reinforcement algorithms
to develop different data transmission models in wireless networks. Considering
temporally-correlated fading channels, the results are presented for the cases
with partial channel state information at the transmitter (CSIT). As
demonstrated, the implementation of reinforcement algorithms improves the
performance of communication setups remarkably, with the same feedback
load/complexity as in the state-of-the-art schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3336</identifier>
 <datestamp>2014-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3336</id><created>2014-09-11</created><updated>2014-09-17</updated><authors><author><keyname>Makki</keyname><forenames>Behrooz</forenames></author><author><keyname>Svensson</keyname><forenames>Tommy</forenames></author><author><keyname>Zorzi</keyname><forenames>Michele</forenames></author></authors><title>Green communication via Type-I ARQ: Finite block-length analysis</title><categories>cs.IT math.IT</categories><comments>Accepted for publication in GLOBECOM 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the effect of optimal power allocation on the performance
of communication systems utilizing automatic repeat request (ARQ). Considering
Type-I ARQ, the problem is cast as the minimization of the outage probability
subject to an average power constraint. The analysis is based on some recent
results on the achievable rates of finite-length codes and we investigate the
effect of codewords length on the performance of ARQ-based systems. We show
that the performance of ARQ protocols is (almost) insensitive to the length of
the codewords, for codewords of length $\ge 50$ channel uses. Also, optimal
power allocation improves the power efficiency of the ARQ-based systems
substantially. For instance, consider a Rayleigh fading channel, codewords of
rate 1 nats-per-channel-use and outage probability $10^{-3}.$ Then, with a
maximum of 2 and 3 transmissions, the implementation of power-adaptive ARQ
reduces the average power, compared to the open-loop communication setup, by 17
and 23 dB, respectively, a result which is (almost) independent of the
codewords length. Also, optimal power allocation increases the diversity gain
of the ARQ protocols considerably.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3358</identifier>
 <datestamp>2014-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3358</id><created>2014-09-11</created><authors><author><keyname>Mou</keyname><forenames>Lili</forenames></author><author><keyname>Li</keyname><forenames>Ge</forenames></author><author><keyname>Liu</keyname><forenames>Yuxuan</forenames></author><author><keyname>Peng</keyname><forenames>Hao</forenames></author><author><keyname>Jin</keyname><forenames>Zhi</forenames></author><author><keyname>Xu</keyname><forenames>Yan</forenames></author><author><keyname>Zhang</keyname><forenames>Lu</forenames></author></authors><title>Building Program Vector Representations for Deep Learning</title><categories>cs.SE cs.LG cs.NE</categories><comments>This paper was submitted to ICSE'14</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep learning has made significant breakthroughs in various fields of
artificial intelligence. Advantages of deep learning include the ability to
capture highly complicated features, weak involvement of human engineering,
etc. However, it is still virtually impossible to use deep learning to analyze
programs since deep architectures cannot be trained effectively with pure back
propagation. In this pioneering paper, we propose the &quot;coding criterion&quot; to
build program vector representations, which are the premise of deep learning
for program analysis. Our representation learning approach directly makes deep
learning a reality in this new field. We evaluate the learned vector
representations both qualitatively and quantitatively. We conclude, based on
the experiments, the coding criterion is successful in building program
representations. To evaluate whether deep learning is beneficial for program
analysis, we feed the representations to deep neural networks, and achieve
higher accuracy in the program classification task than &quot;shallow&quot; methods, such
as logistic regression and the support vector machine. This result confirms the
feasibility of deep learning to analyze programs. It also gives primary
evidence of its success in this new field. We believe deep learning will become
an outstanding technique for program analysis in the near future.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3360</identifier>
 <datestamp>2015-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3360</id><created>2014-09-11</created><updated>2015-02-18</updated><authors><author><keyname>Chatterjee</keyname><forenames>Krishnendu</forenames></author><author><keyname>Chmel&#xed;k</keyname><forenames>Martin</forenames></author><author><keyname>Gupta</keyname><forenames>Raghav</forenames></author><author><keyname>Kanodia</keyname><forenames>Ayush</forenames></author></authors><title>Qualitative Analysis of POMDPs with Temporal Logic Specifications for
  Robotics Applications</title><categories>cs.LO cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider partially observable Markov decision processes (POMDPs), that are
a standard framework for robotics applications to model uncertainties present
in the real world, with temporal logic specifications. All temporal logic
specifications in linear-time temporal logic (LTL) can be expressed as parity
objectives. We study the qualitative analysis problem for POMDPs with parity
objectives that asks whether there is a controller (policy) to ensure that the
objective holds with probability 1 (almost-surely). While the qualitative
analysis of POMDPs with parity objectives is undecidable, recent results show
that when restricted to finite-memory policies the problem is EXPTIME-complete.
While the problem is intractable in theory, we present a practical approach to
solve the qualitative analysis problem. We designed several heuristics to deal
with the exponential complexity, and have used our implementation on a number
of well-known POMDP examples for robotics applications. Our results provide the
first practical approach to solve the qualitative analysis of robot motion
planning with LTL properties in the presence of uncertainty.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3367</identifier>
 <datestamp>2014-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3367</id><created>2014-09-11</created><authors><author><keyname>Muller</keyname><forenames>Gabriel L.</forenames></author></authors><title>HTML5 WebSocket protocol and its application to distributed computing</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  HTML5 WebSocket protocol brings real time communication in web browsers to a
new level. Daily, new products are designed to stay permanently connected to
the web. WebSocket is the technology enabling this revolution. WebSockets are
supported by all current browsers, but it is still a new technology in constant
evolution.
  WebSockets are slowly replacing older client-server communication
technologies. As opposed to comet-like technologies WebSockets' remarkable
performances is a result of the protocol's fully duplex nature and because it
doesn't rely on HTTP communications.
  To begin with this paper studies the WebSocket protocol and different
WebSocket servers implementations. This first theoretic part focuses more
deeply on heterogeneous implementations and OpenCL. The second part is a
benchmark of a new promising library.
  The real-time engine used for testing purposes is SocketCluster.
SocketCluster provides a highly scalable WebSocket server that makes use of all
available cpu cores on an instance. The scope of this work is reduced to
vertical scaling of SocketCluster.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3379</identifier>
 <datestamp>2015-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3379</id><created>2014-09-11</created><updated>2014-10-07</updated><authors><author><keyname>Schreiber</keyname><forenames>Michael</forenames></author></authors><title>A variant of the h-index to measure recent performance</title><categories>cs.DL physics.soc-ph</categories><comments>9 pages, 6 figures</comments><journal-ref>JASIST 66(11), 2373-2380 (2015)</journal-ref><doi>10.1002/asi.23438</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The predictive power of the h-index has been shown to depend for a long time
on citations to rather old publications. This has raised doubts about its
usefulness for predicting future scientific achievements. Here I investigate a
variant which considers only the recent publications and is therefore more
useful in academic hiring processes and for the allocation of research
resources. It is simply defined in analogy to the usual h-index, but taking
into account only the publications from recent years, and it can easily be
determined from the ISI Web of Knowledge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3404</identifier>
 <datestamp>2014-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3404</id><created>2014-09-11</created><authors><author><keyname>Klemenjak</keyname><forenames>Christoph</forenames></author><author><keyname>Egarter</keyname><forenames>Dominik</forenames></author><author><keyname>Elmenreich</keyname><forenames>Wilfried</forenames></author></authors><title>YoMo - The Arduino based Smart Metering Board</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Smart meters are an enabling technology for many smart grid applications.
This paper introduces a design for a low-cost smart meter system as well as the
fundamentals of smart metering. The smart meter platform, provided as open
hardware, is designed with a connector interface compatible to the Arduino
platform, thus opening the possibilities for smart meters with flexible
hardware and computation features, starting from low-cost 8 bit micro
controllers up to powerful single board computers that can run Linux. The
metering platform features a current transformer which allows a non-intrusive
installation of the current measurement unit. The suggested design can switch
loads, offers a variable sampling frequency, and provides measurement data such
as active power, reactive and apparent power. Results indicate that measurement
accuracy and resolution of the proposed metering platform are sufficient for a
range of different applications and loads from a few watts up to five
kilowatts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3413</identifier>
 <datestamp>2014-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3413</id><created>2014-09-11</created><authors><author><keyname>ElBamby</keyname><forenames>Mohammed S.</forenames></author><author><keyname>Bennis</keyname><forenames>Mehdi</forenames></author><author><keyname>Saad</keyname><forenames>Walid</forenames></author><author><keyname>Latva-aho</keyname><forenames>Matti</forenames></author></authors><title>Content-Aware User Clustering and Caching in Wireless Small Cell
  Networks</title><categories>cs.NI</categories><comments>In the IEEE 11th International Symposium on Wireless Communication
  Systems (ISWCS) 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the problem of content-aware user clustering and content
caching in wireless small cell networks is studied. In particular, a service
delay minimization problem is formulated, aiming at optimally caching contents
at the small cell base stations (SCBSs). To solve the optimization problem, we
decouple it into two interrelated subproblems. First, a clustering algorithm is
proposed grouping users with similar content popularity to associate similar
users to the same SCBS, when possible. Second, a reinforcement learning
algorithm is proposed to enable each SCBS to learn the popularity distribution
of contents requested by its group of users and optimize its caching strategy
accordingly. Simulation results show that by correlating the different
popularity patterns of different users, the proposed scheme is able to minimize
the service delay by 42% and 27%, while achieving a higher offloading gain of
up to 280% and 90%, respectively, compared to random caching and unclustered
learning schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3426</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3426</id><created>2014-09-11</created><updated>2015-12-09</updated><authors><author><keyname>Duan</keyname><forenames>Runyao</forenames></author><author><keyname>Winter</keyname><forenames>Andreas</forenames></author></authors><title>No-Signalling Assisted Zero-Error Capacity of Quantum Channels and an
  Information Theoretic Interpretation of the Lovasz Number</title><categories>quant-ph cs.IT math.CO math.IT</categories><comments>44 pages, 4 figures (eps); v2 has a new title, several small errors
  corrected and new results on channel simulation; v3 is the final (journal)
  version</comments><journal-ref>IEEE Trans. Inf. Theory 62(2):891-914, 2016</journal-ref><doi>10.1109/TIT.2015.2507979</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the one-shot zero-error classical capacity of a quantum channel
assisted by quantum no-signalling correlations, and the reverse problem of
exact simulation of a prescribed channel by a noiseless classical one. Quantum
no-signalling correlations are viewed as two-input and two-output completely
positive and trace preserving maps with linear constraints enforcing that the
device cannot signal. Both problems lead to simple semidefinite programmes
(SDPs) that depend only on the Kraus operator space of the channel. In
particular, we show that the zero-error classical simulation cost is precisely
the conditional min-entropy of the Choi-Jamiolkowski matrix of the given
channel. The zero-error classical capacity is given by a similar-looking but
different SDP; the asymptotic zero-error classical capacity is the
regularization of this SDP, and in general we do not know of any simple form.
  Interestingly however, for the class of classical-quantum channels, we show
that the asymptotic capacity is given by a much simpler SDP, which coincides
with a semidefinite generalization of the fractional packing number suggested
earlier by Aram Harrow. This finally results in an operational interpretation
of the celebrated Lovasz $\vartheta$ function of a graph as the zero-error
classical capacity of the graph assisted by quantum no-signalling correlations,
the first information theoretic interpretation of the Lovasz number.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3428</identifier>
 <datestamp>2014-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3428</id><created>2014-09-11</created><authors><author><keyname>Pauly</keyname><forenames>Arno</forenames></author><author><keyname>Fouch&#xe9;</keyname><forenames>Willem L.</forenames></author></authors><title>How constructive is constructing measures?</title><categories>cs.LO math.PR</categories><msc-class>03D78, 03B30, 28A33</msc-class><acm-class>F.1.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given some set, how hard is it to construct a measure supported by it? We
classify some variations of this task in the Weihrauch lattice. Particular
attention is paid to Frostman measures on sets with positive Hausdorff
dimension. As a side result, the Weihrauch degree of Hausdorff dimension itself
is determined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3436</identifier>
 <datestamp>2014-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3436</id><created>2014-09-11</created><updated>2014-09-12</updated><authors><author><keyname>Meunier</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames></author><author><keyname>Sarrabezolles</keyname><forenames>Pauline</forenames></author></authors><title>Colorful linear programming, Nash equilibrium, and pivots</title><categories>cs.DM cs.CG</categories><msc-class>68R05, 68U05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The colorful Carath\'eodory theorem, proved by B\'ar\'any in 1982, states
that given $d+1$ sets of points $S_1,\ldots,S_{d+1}$ in $\mathbb R^d$, such
that each $S_i$ contains $0$ in its convex hull, there exists a set
$T\subseteq\bigcup_{i=1}^{d+1}S_i$ containing $0$ in its convex hull and such
that $|T\cap S_i|\leq 1$ for all $i\in\{1,\ldots,d+1\}$. An intriguing question
-- still open -- is whether such a set $T$, whose existence is ensured, can be
found in polynomial time. In 1997, B\'ar\'any and Onn defined colorful linear
programming as algorithmic questions related to the colorful Carath\'eodory
theorem. The question we just mentioned comes under colorful linear
programming.
  We present new complexity results for colorful linear programming problems
and propose a variant of the &quot;B\'ar\'any-Onn&quot; algorithm, which is an algorithm
computing a set $T$ whose existence is ensured by the colorful Carath\'eodory
theorem. Our algorithm makes a clear connection with the simplex algorithm.
Some combinatorial versions of the colorful Carath\'eodory theorem are also
discussed from an algorithmic point of view. Finally, we show that computing a
Nash equilibrium in a bimatrix game is polynomially reducible to a colorful
linear programming problem. On our track, we found a new way to prove that a
complementarity problem belongs to the PPAD class with the help of Sperner's
lemma.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3440</identifier>
 <datestamp>2014-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3440</id><created>2014-09-11</created><authors><author><keyname>Ballet</keyname><forenames>St&#xe9;phane</forenames></author><author><keyname>Pieltant</keyname><forenames>Julia</forenames></author></authors><title>Tower of algebraic function fields with maximal Hasse-Witt invariant and
  tensor rank of multiplication in any extension of $\mathbb{F}_2$ and
  $\mathbb{F}_3$</title><categories>math.AG cs.IT math.IT</categories><msc-class>14H05, 11Y16, 12E20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Up until now, it was recognized that a large number of 2-torsion points was a
technical barrier to improve the bounds for the symmetric tensor rank of
multiplication in every extension of any finite field. In this paper, we show
that there are two exceptional cases, namely the extensions of $\mathbb{F}_2$
and $\mathbb{F}_3$. In particular, using the definition field descent on the
field with 2 or 3 elements of a Garcia-Stichtenoth tower of algebraic function
fields which is asymptotically optimal in the sense of Drinfel'd-Vladut and has
maximal Hasse-Witt invariant, we obtain a significant improvement of the
uniform bounds for the symmetric tensor rank of multiplication in any extension
of $\mathbb{F}_2$ and $\mathbb{F}_3$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3446</identifier>
 <datestamp>2014-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3446</id><created>2014-09-11</created><authors><author><keyname>Dutta</keyname><forenames>Haimonti</forenames></author><author><keyname>Srinivasan</keyname><forenames>Ashwin</forenames></author></authors><title>Consensus-Based Modelling using Distributed Feature Construction</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A particularly successful role for Inductive Logic Programming (ILP) is as a
tool for discovering useful relational features for subsequent use in a
predictive model. Conceptually, the case for using ILP to construct relational
features rests on treating these features as functions, the automated discovery
of which necessarily requires some form of first-order learning. Practically,
there are now several reports in the literature that suggest that augmenting
any existing features with ILP-discovered relational features can substantially
improve the predictive power of a model. While the approach is straightforward
enough, much still needs to be done to scale it up to explore more fully the
space of possible features that can be constructed by an ILP system. This is in
principle, infinite and in practice, extremely large. Applications have been
confined to heuristic or random selections from this space. In this paper, we
address this computational difficulty by allowing features to be constructed in
a distributed manner. That is, there is a network of computational units, each
of which employs an ILP engine to construct some small number of features and
then builds a (local) model. We then employ a consensus-based algorithm, in
which neighboring nodes share information to update local models. For a
category of models (those with convex loss functions), it can be shown that the
algorithm will result in all nodes converging to a consensus model. In
practice, it may be slow to achieve this convergence. Nevertheless, our results
on synthetic and real datasets that suggests that in relatively short time the
&quot;best&quot; node in the network reaches a model whose predictive accuracy is
comparable to that obtained using more computational effort in a
non-distributed setting (the best node is identified as the one whose weights
converge first).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3463</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3463</id><created>2014-09-11</created><authors><author><keyname>Zheng</keyname><forenames>Yousi</forenames></author><author><keyname>Shroff</keyname><forenames>Ness</forenames></author><author><keyname>Sinha</keyname><forenames>Prasun</forenames></author></authors><title>Heavy Traffic Limits for GI/H/n Queues: Theory and Application</title><categories>cs.PF cs.DC cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a GI/H/n queueing system. In this system, there are multiple
servers in the queue. The inter-arrival time is general and independent, and
the service time follows hyper-exponential distribution. Instead of stochastic
differential equations, we propose two heavy traffic limits for this system,
which can be easily applied in practical systems. In applications, we show how
to use these heavy traffic limits to design a power efficient cloud computing
environment based on different QoS requirements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3495</identifier>
 <datestamp>2014-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3495</id><created>2014-09-11</created><authors><author><keyname>Robinson-Garcia</keyname><forenames>Nicolas</forenames></author><author><keyname>Jim&#xe9;nez-Contreras</keyname><forenames>Evaristo</forenames></author><author><keyname>Calero-Medina</keyname><forenames>Clara</forenames></author></authors><title>The impact of a few: The effect of alternative formulas for recruiting
  talent in a non-competitive system</title><categories>cs.DL</categories><comments>This paper was presented as poster at the STI Conference held in
  Leiden, 3-5 september, 2014</comments><journal-ref>Proceedings of the Science and Technology Indicators Conference
  2014 Leiden, pp. 457-462</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper analyzes the effect of the Catalan programme ICREA for the
selection of talented researchers on the percentage of highly cited papers of
Catalan universities
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3505</identifier>
 <datestamp>2014-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3505</id><created>2014-09-11</created><authors><author><keyname>Ouyang</keyname><forenames>Wanli</forenames></author><author><keyname>Luo</keyname><forenames>Ping</forenames></author><author><keyname>Zeng</keyname><forenames>Xingyu</forenames></author><author><keyname>Qiu</keyname><forenames>Shi</forenames></author><author><keyname>Tian</keyname><forenames>Yonglong</forenames></author><author><keyname>Li</keyname><forenames>Hongsheng</forenames></author><author><keyname>Yang</keyname><forenames>Shuo</forenames></author><author><keyname>Wang</keyname><forenames>Zhe</forenames></author><author><keyname>Xiong</keyname><forenames>Yuanjun</forenames></author><author><keyname>Qian</keyname><forenames>Chen</forenames></author><author><keyname>Zhu</keyname><forenames>Zhenyao</forenames></author><author><keyname>Wang</keyname><forenames>Ruohui</forenames></author><author><keyname>Loy</keyname><forenames>Chen-Change</forenames></author><author><keyname>Wang</keyname><forenames>Xiaogang</forenames></author><author><keyname>Tang</keyname><forenames>Xiaoou</forenames></author></authors><title>DeepID-Net: multi-stage and deformable deep convolutional neural
  networks for object detection</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose multi-stage and deformable deep convolutional
neural networks for object detection. This new deep learning object detection
diagram has innovations in multiple aspects. In the proposed new deep
architecture, a new deformation constrained pooling (def-pooling) layer models
the deformation of object parts with geometric constraint and penalty. With the
proposed multi-stage training strategy, multiple classifiers are jointly
optimized to process samples at different difficulty levels. A new pre-training
strategy is proposed to learn feature representations more suitable for the
object detection task and with good generalization capability. By changing the
net structures, training strategies, adding and removing some key components in
the detection pipeline, a set of models with large diversity are obtained,
which significantly improves the effectiveness of modeling averaging. The
proposed approach ranked \#2 in ILSVRC 2014. It improves the mean averaged
precision obtained by RCNN, which is the state-of-the-art of object detection,
from $31\%$ to $45\%$. Detailed component-wise analysis is also provided
through extensive experimental evaluation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3512</identifier>
 <datestamp>2014-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3512</id><created>2014-09-10</created><authors><author><keyname>Dhungana</keyname><forenames>Udaya Raj</forenames></author><author><keyname>Shakya</keyname><forenames>Subarna</forenames></author><author><keyname>Baral</keyname><forenames>Kabita</forenames></author><author><keyname>Sharma</keyname><forenames>Bharat</forenames></author></authors><title>Word Sense Disambiguation using WSD specific Wordnet of Polysemy Words</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new model of WordNet that is used to disambiguate the
correct sense of polysemy word based on the clue words. The related words for
each sense of a polysemy word as well as single sense word are referred to as
the clue words. The conventional WordNet organizes nouns, verbs, adjectives and
adverbs together into sets of synonyms called synsets each expressing a
different concept. In contrast to the structure of WordNet, we developed a new
model of WordNet that organizes the different senses of polysemy words as well
as the single sense words based on the clue words. These clue words for each
sense of a polysemy word as well as for single sense word are used to
disambiguate the correct meaning of the polysemy word in the given context
using knowledge based Word Sense Disambiguation (WSD) algorithms. The clue word
can be a noun, verb, adjective or adverb.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3518</identifier>
 <datestamp>2015-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3518</id><created>2014-09-11</created><updated>2015-04-13</updated><authors><author><keyname>Kim</keyname><forenames>Do-kyum</forenames></author><author><keyname>Voelker</keyname><forenames>Geoffrey M.</forenames></author><author><keyname>Saul</keyname><forenames>Lawrence K.</forenames></author></authors><title>Topic Modeling of Hierarchical Corpora</title><categories>stat.ML cs.IR cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of topic modeling in corpora whose documents are
organized in a multi-level hierarchy. We explore a parametric approach to this
problem, assuming that the number of topics is known or can be estimated by
cross-validation. The models we consider can be viewed as special
(finite-dimensional) instances of hierarchical Dirichlet processes (HDPs). For
these models we show that there exists a simple variational approximation for
probabilistic inference. The approximation relies on a previously unexploited
inequality that handles the conditional dependence between Dirichlet latent
variables in adjacent levels of the model's hierarchy. We compare our approach
to existing implementations of nonparametric HDPs. On several benchmarks we
find that our approach is faster than Gibbs sampling and able to learn more
predictive models than existing variational methods. Finally, we demonstrate
the large-scale viability of our approach on two newly available corpora from
researchers in computer security---one with 350,000 documents and over 6,000
internal subcategories, the other with a five-level deep hierarchy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3525</identifier>
 <datestamp>2014-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3525</id><created>2014-09-11</created><authors><author><keyname>Portmann</keyname><forenames>Christopher</forenames></author><author><keyname>Renner</keyname><forenames>Renato</forenames></author></authors><title>Cryptographic security of quantum key distribution</title><categories>quant-ph cs.CR cs.IT math.IT</categories><comments>31+23 pages. 28 figures. Comments and questions welcome</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work is intended as an introduction to cryptographic security and a
motivation for the widely used Quantum Key Distribution (QKD) security
definition. We review the notion of security necessary for a protocol to be
usable in a larger cryptographic context, i.e., for it to remain secure when
composed with other secure protocols. We then derive the corresponding security
criterion for QKD. We provide several examples of QKD composed in sequence and
parallel with different cryptographic schemes to illustrate how the error of a
composed protocol is the sum of the errors of the individual protocols. We also
discuss the operational interpretations of the distance metric used to quantify
these errors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3530</identifier>
 <datestamp>2014-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3530</id><created>2014-09-11</created><authors><author><keyname>Savinov</keyname><forenames>Alexandr</forenames></author></authors><title>Concept-oriented model: inference in hierarchical multidimensional space</title><categories>cs.DB</categories><comments>19 pages, 14 figures, Full version of the paper published in DATA
  2012 conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In spite of its fundamental importance, inference has not been an inherent
function of multidimensional models and analytical applications. These models
are mainly aimed at numeric (quantitative) analysis where the notions of
inference and semantics are not well defined. In this paper we argue that
inference can be and should be integral part of multidimensional data models
and analytical applications. It is demonstrated how inference can be defined
using only multidimensional terms like axes and coordinates as opposed to using
logic-based approaches. We propose a novel approach to inference in
multidimensional space based on the concept-oriented model of data and
introduce elementary operations which are then used to define constraint
propagation and inference procedures. We describe a query language with
inference operator and demonstrate its usefulness in solving complex analytical
tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3531</identifier>
 <datestamp>2014-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3531</id><created>2014-09-09</created><authors><author><keyname>Chambers</keyname><forenames>John M.</forenames></author></authors><title>Object-Oriented Programming, Functional Programming and R</title><categories>stat.ME cs.PL cs.SE stat.CO</categories><comments>Published in at http://dx.doi.org/10.1214/13-STS452 the Statistical
  Science (http://www.imstat.org/sts/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-STS-STS452</report-no><journal-ref>Statistical Science 2014, Vol. 29, No. 2, 167-180</journal-ref><doi>10.1214/13-STS452</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper reviews some programming techniques in R that have proved useful,
particularly for substantial projects. These include several versions of
object-oriented programming, used in a large number of R packages. The review
tries to clarify the origins and ideas behind the various versions, each of
which is valuable in the appropriate context. R has also been strongly
influenced by the ideas of functional programming and, in particular, by the
desire to combine functional with object oriented programming. To clarify how
this particular mix of ideas has turned out in the current R language and
supporting software, the paper will first review the basic ideas behind
object-oriented and functional programming, and then examine the evolution of R
with these ideas providing context. Functional programming supports
well-defined, defensible software giving reproducible results. Object-oriented
programming is the mechanism par excellence for managing complexity while
keeping things simple for the user. The two paradigms have been valuable in
supporting major software for fitting models to data and numerous other
statistical applications. The paradigms have been adopted, and adapted,
distinctively in R. Functional programming motivates much of R but R does not
enforce the paradigm. Object-oriented programming from a functional perspective
differs from that used in non-functional languages, a distinction that needs to
be emphasized to avoid confusion. R initially replicated the S language from
Bell Labs, which in turn was strongly influenced by earlier program libraries.
At each stage, new ideas have been added, but the previous software continues
to show its influence in the design as well. Outlining the evolution will
further clarify why we currently have this somewhat unusual combination of
ideas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3533</identifier>
 <datestamp>2014-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3533</id><created>2014-09-09</created><authors><author><keyname>Ali</keyname><forenames>Asad</forenames></author><author><keyname>Fern&#xe1;ndez</keyname><forenames>Maribel</forenames></author></authors><title>Static Enforcement of Role-Based Access Control</title><categories>cs.SE cs.PL</categories><comments>In Proceedings WWV 2014, arXiv:1409.2294</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 163, 2014, pp. 36-50</journal-ref><doi>10.4204/EPTCS.163.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new static approach to Role-Based Access Control (RBAC) policy
enforcement. The static approach we advocate includes a new design methodology,
for applications involving RBAC, which integrates the security requirements
into the system's architecture. We apply this new approach to policies
restricting calls to methods in Java applications. We present a language to
express RBAC policies on calls to methods in Java, a set of design patterns
which Java programs must adhere to for the policy to be enforced statically,
and a description of the checks made by our static verifier for static
enforcement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3536</identifier>
 <datestamp>2014-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3536</id><created>2014-09-11</created><updated>2014-11-18</updated><authors><author><keyname>Lakshminarayanan</keyname><forenames>Chandrashekar</forenames></author><author><keyname>Bhatnagar</keyname><forenames>Shalabh</forenames></author></authors><title>A Generalized Reduced Linear Program for Markov Decision Processes</title><categories>cs.SY</categories><comments>24 pages, submitted to AAAI on November 19 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Markov decision processes (MDPs) with large number of states are of high
practical interest. However, conventional algorithms to solve MDP are
computationally infeasible in this scenario. Approximate dynamic programming
(ADP) methods tackle this issue by computing approximate solutions. A widely
applied ADP method is approximate linear program (ALP) which makes use of
linear function approximation and offers theoretical performance guarantees.
Nevertheless, the ALP is difficult to solve due to the presence of a large
number of constraints and in practice, a reduced linear program (RLP) is solved
instead. The RLP has a tractable number of constraints sampled from the
original constraints of the ALP. Though the RLP is known to perform well in
experiments the theoretical guarantees are available only for a specific RLP
obtained under idealized assumptions.
  In this paper, we generalize the RLP to define a generalized reduced linear
program (GRLP) which has a tractable number of constraints that are obtained as
positive linear combinations of the original constraints of the ALP. The main
contribution of this paper is the novel theoretical framework developed to
obtain error bounds for any given GRLP. Central to our framework are two
$\max$-norm contraction operators. Our result solves theoretically justifies
linear approximation of constraints. We discuss the implication of our results
in the contexts of ADP and reinforcement learning. We also demonstrate via an
example in the domain of controlled queues that the experiments conform to the
theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3552</identifier>
 <datestamp>2015-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3552</id><created>2014-09-11</created><updated>2014-09-19</updated><authors><author><keyname>Bocharov</keyname><forenames>Alex</forenames></author><author><keyname>Roetteler</keyname><forenames>Martin</forenames></author><author><keyname>Svore</keyname><forenames>Krysta M.</forenames></author></authors><title>Efficient synthesis of probabilistic quantum circuits with fallback</title><categories>quant-ph cs.ET</categories><comments>17 pages, 7 figures; added Appendix F on the runtime performance of
  the synthesis algorithm</comments><journal-ref>Phys. Rev. A 91, 052317 (2015)</journal-ref><doi>10.1103/PhysRevA.91.052317</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently it has been shown that Repeat-Until-Success (RUS) circuits can
approximate a given single-qubit unitary with an expected number of $T$ gates
of about $1/3$ of what is required by optimal, deterministic, ancilla-free
decompositions over the Clifford+$T$ gate set. In this work, we introduce a
more general and conceptually simpler circuit decomposition method that allows
for synthesis into protocols that probabilistically implement quantum circuits
over several universal gate sets including, but not restricted to, the
Clifford+$T$ gate set. The protocol, which we call Probabilistic Quantum
Circuits with Fallback (PQF), implements a walk on a discrete Markov chain in
which the target unitary is an absorbing state and in which transitions are
induced by multi-qubit unitaries followed by measurements. In contrast to RUS
protocols, the presented PQF protocols terminate after a finite number of
steps. Specifically, we apply our method to the Clifford+$T$, Clifford+$V$, and
Clifford+$\pi/12$ gate sets to achieve decompositions with expected gate counts
of $\log_b(1/\varepsilon)+O(\log(\log(1/\varepsilon)))$, where $b$ is a
quantity related to the expansion property of the underlying universal gate
set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3554</identifier>
 <datestamp>2014-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3554</id><created>2014-09-07</created><updated>2014-09-27</updated><authors><author><keyname>Chaudhary</keyname><forenames>Ankit</forenames></author></authors><title>Finger-Stylus for Non Touch-Enable Systems</title><categories>cs.HC</categories><comments>JKSU Engineering Sciences, Elsevier, 2015</comments><doi>10.1016/j.jksues.2014.02.002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since computer was invented, people are using many devices to interact with
computer. Initially there were keyboard, mouse etc. but with the advancement of
technology, new ways are being discovered that are quite usual and natural to
the humans like stylus, touch-enable systems. In the current age of technology,
user is expected to touch the machine interface to give input. Hand gesture is
such a way to interact with machines where natural bare hand is used to
communicate without touching machine interface. It gives a feeling to user that
he is interacting in natural way to some human, not with traditional machines.
This paper presents a technique where user needs not to touch the machine
interface to draw on screen. Here hand finger draws shapes on monitor like
stylus, without touching the monitor. This method can be used in many
applications including games. The finger was used as an input device that acts
like paint-brush or finger-stylus and is used to make shapes in front of the
camera. Fingertip extraction and motion tracking were done in Matlab with real
time constraints. This work is an early attempt to replace stylus with the
natural finger without touching screen.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3558</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3558</id><created>2014-09-11</created><updated>2015-06-29</updated><authors><author><keyname>Brandeho</keyname><forenames>Mathieu</forenames></author><author><keyname>Roland</keyname><forenames>J&#xe9;r&#xe9;mie</forenames></author></authors><title>A universal adiabatic quantum query algorithm</title><categories>quant-ph cs.CC</categories><comments>22 pages, compared to v1, includes a rigorous proof of the
  correctness of the algorithm based on a version of the adiabatic theorem that
  does not require a spectral gap</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantum query complexity is known to be characterized by the so-called
quantum adversary bound. While this result has been proved in the standard
discrete-time model of quantum computation, it also holds for continuous-time
(or Hamiltonian-based) quantum computation, due to a known equivalence between
these two query complexity models. In this work, we revisit this result by
providing a direct proof in the continuous-time model. One originality of our
proof is that it draws new connections between the adversary bound, a modern
technique of theoretical computer science, and early theorems of quantum
mechanics. Indeed, the proof of the lower bound is based on Ehrenfest's
theorem, while the upper bound relies on the adiabatic theorem, as it goes by
constructing a universal adiabatic quantum query algorithm. Another originality
is that we use for the first time in the context of quantum computation a
version of the adiabatic theorem that does not require a spectral gap.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3560</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3560</id><created>2014-09-11</created><updated>2014-09-14</updated><authors><author><keyname>Gao</keyname><forenames>Sicun</forenames></author></authors><title>Descriptive Control Theory: A Proposal</title><categories>cs.LO cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Logic is playing an increasingly important role in the engineering of
real-time, hybrid, and cyber-physical systems, but mostly in the form of
posterior verification and high-level analysis. The core methodology in the
design of real-world systems consists mainly of control theory and numerical
analysis, and has remained mostly free of logic and formal approaches. As a
result, besides facing extreme difficulty in guaranteeing the reliability of
these systems, engineers are also missing out the computational power of
logic-based methods that has greatly advanced in the past decades. To change
this situation, we need a logical and computational foundation for control
theory. The name &quot;descriptive control theory&quot; emphasizes the overarching theme
of using logic to express, analyze, and solve problems in control theory. If
the program is successfully carried out, logical approaches will significantly
extend existing engineering methods towards a unified methodology for handling
nonlinear and hybrid systems, and bring design automation and reliability to an
unprecedented level in the broad field of engineering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3562</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3562</id><created>2014-09-11</created><updated>2016-02-20</updated><authors><author><keyname>Mosonyi</keyname><forenames>Milan</forenames></author><author><keyname>Ogawa</keyname><forenames>Tomohiro</forenames></author></authors><title>Strong converse exponent for classical-quantum channel coding</title><categories>quant-ph cs.IT math-ph math.IT math.MP</categories><comments>v6: New section on entanglement breaking, and covariant channels.
  Submitted version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We determine the exact strong converse exponent of classical-quantum channel
coding, for every rate above the Holevo capacity. Our form of the exponent is
an exact analogue of Arimoto's, given as a transform of the Renyi capacities
with parameters alpha&gt;1. It is important to note that, unlike in the classical
case, there are many inequivalent ways to define the Renyi divergence of
states, and hence the R\'enyi capacities of channels. Our exponent is in terms
of the Renyi capacities corresponding to a version of the Renyi divergences
that has been introduced recently in [M\&quot;uller-Lennert, Dupuis, Szehr, Fehr and
Tomamichel, J. Math. Phys. 54, 122203, (2013)], and [Wilde, Winter, Yang,
Commun. Math. Phys. 331, (2014)]. Our result adds to the growing body of
evidence that this new version is the natural definition for the purposes of
strong converse problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3600</identifier>
 <datestamp>2014-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3600</id><created>2014-09-11</created><authors><author><keyname>Chen</keyname><forenames>Ke</forenames></author><author><keyname>Dumitrescu</keyname><forenames>Adrian</forenames></author></authors><title>Select with Groups of $3$ or $4$ Takes Linear Time</title><categories>cs.DS math.CO</categories><comments>9 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We revisit the selection problem, namely that of computing the $i$th order
statistic of $n$ given elements, in particular the classical deterministic
algorithm by grouping and partition due to Blum, Floyd, Pratt, Rivest, and
Tarjan~(1973). While the original algorithm uses groups of odd size at least
$5$ and runs in linear time, it has been perpetuated in the literature that
using groups of $3$ or $4$ will force the worst-case running time to become
superlinear, namely $\Omega(n \log{n})$. We first point out that the arguments
existent in the literature justifying the superlinear worst-case running time
fall short of proving the claim. We further prove that it is possible to use
group size $3$ or $4$ while maintaining the worst case linear running time. To
this end we introduce two simple variants of the classical algorithm, the
repeated step algorithm and the shifting target algorithm, both running in
linear time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3620</identifier>
 <datestamp>2015-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3620</id><created>2014-09-11</created><updated>2015-04-22</updated><authors><author><keyname>Mouzali</keyname><forenames>Aziz</forenames></author><author><keyname>Merazka</keyname><forenames>Fatiha</forenames></author><author><keyname>Markham</keyname><forenames>Damian</forenames></author></authors><title>Quantum Secret Sharing with error correction</title><categories>quant-ph cs.CR cs.IT math.IT</categories><comments>We just add the four figures</comments><journal-ref>Communication in Theoretical Physics vol 58 No 5 page 661,671 year
  2012</journal-ref><doi>10.1088/0253-6102/58/5/09</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate in this work a quantum error correction on a five-qubits graph
state used for secret sharing through five noisy channels. We describe the
procedure for the five, seven and nine qubits codes. It is known that the three
codes always allow error recovery if only one among the sents qubits is
disturbed in the transmitting channel. However, if two qubits and more are
disturbed, then the correction will depend on the used code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3635</identifier>
 <datestamp>2014-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3635</id><created>2014-09-11</created><authors><author><keyname>Zhang</keyname><forenames>Meng</forenames></author><author><keyname>Liu</keyname><forenames>Yuan</forenames></author><author><keyname>Feng</keyname><forenames>Suili</forenames></author></authors><title>Energy Harvesting for Secure OFDMA Systems</title><categories>cs.IT math.IT</categories><comments>Accepted by WCSP 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy harvesting and physical-layer security in wireless networks are of
great significance. In this paper, we study the simultaneous wireless
information and power transfer (SWIPT) in downlink orthogonal
frequency-division multiple access (OFDMA) systems, where each user applies
power splitting to coordinate the energy harvesting and information decoding
processes while secrecy information requirement is guaranteed. The problem is
formulated to maximize the aggregate harvested power at the users while
satisfying secrecy rate requirements of all users by subcarrier allocation and
the optimal power splitting ratio selection. Due to the NP-hardness of the
problem, we propose an efficient iterative algorithm. The numerical results
show that the proposed method outperforms conventional methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3638</identifier>
 <datestamp>2014-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3638</id><created>2014-09-11</created><authors><author><keyname>Tang</keyname><forenames>Weijun</forenames></author><author><keyname>Zhang</keyname><forenames>Rongbin</forenames></author><author><keyname>Liu</keyname><forenames>Yuan</forenames></author><author><keyname>Feng</keyname><forenames>Suili</forenames></author></authors><title>Joint Resource Allocation for eICIC in Heterogeneous Networks</title><categories>cs.IT cs.NI math.IT</categories><comments>Accepted by Globecom 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interference coordination between high-power macros and low-power picos
deeply impacts the performance of heterogeneous networks (HetNets). It should
deal with three challenges: user association with macros and picos, the amount
of almost blank subframe (ABS) that macros should reserve for picos, and
resource block (RB) allocation strategy in each eNB. We formulate the three
issues jointly for sum weighted logarithmic utility maximization while
maintaining proportional fairness of users. A class of distributed algorithms
are developed to solve the joint optimization problem. Our framework can be
deployed for enhanced inter-cell interference coordination (eICIC) in existing
LTE-A protocols. Extensive evaluation are performed to verify the effectiveness
of our algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3651</identifier>
 <datestamp>2014-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3651</id><created>2014-09-12</created><authors><author><keyname>Vaghela</keyname><forenames>Dushyant</forenames></author></authors><title>An Advanced Approach On Load Balancing in Grid Computing</title><categories>cs.DC</categories><comments>We have applied our Research work on various servers, NGIX performs
  better, VPS Hosting Godadday servers Representative for
  http://explorequotes.com/ working fine, finally we have concluded that all
  the experiments were satisfactory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the rapid development in wide area networks and low cost, powerful
computational resources, grid computing has gained its popularity. With the
advent of grid computing, space limitations of conventional distributed systems
can be overcome and underutilized computing resources at different locations
around the world can be put to distributed jobs. Workload and resource
management is the main key grid services at the service level of grid
infrastructures, out of which load balancing in the main concern for grid
developers. It has been found that load is the major problem which server
faces, especially when the number of users increases. A lot of research is
being done in the area of load management. This paper presents the various
mechanisms of load balancing in grid computing so that the readers will get an
idea of which algorithm would be suitable in different situations. Keywords:
wide area network, distributed computing, load balancing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3653</identifier>
 <datestamp>2014-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3653</id><created>2014-09-12</created><authors><author><keyname>Li</keyname><forenames>Lihong</forenames></author><author><keyname>Munos</keyname><forenames>Remi</forenames></author><author><keyname>Szepesvari</keyname><forenames>Csaba</forenames></author></authors><title>On Minimax Optimal Offline Policy Evaluation</title><categories>cs.AI</categories><acm-class>I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the off-policy evaluation problem, where one aims to
estimate the value of a target policy based on a sample of observations
collected by another policy. We first consider the multi-armed bandit case,
establish a minimax risk lower bound, and analyze the risk of two standard
estimators. It is shown, and verified in simulation, that one is minimax
optimal up to a constant, while another can be arbitrarily worse, despite its
empirical success and popularity. The results are applied to related problems
in contextual bandits and fixed-horizon Markov decision processes, and are also
related to semi-supervised learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3660</identifier>
 <datestamp>2014-11-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3660</id><created>2014-09-12</created><updated>2014-11-17</updated><authors><author><keyname>Zhu</keyname><forenames>Feiyun</forenames></author><author><keyname>Fan</keyname><forenames>Bin</forenames></author><author><keyname>Zhu</keyname><forenames>Xinliang</forenames></author><author><keyname>Wang</keyname><forenames>Ying</forenames></author><author><keyname>Xiang</keyname><forenames>Shiming</forenames></author><author><keyname>Pan</keyname><forenames>Chunhong</forenames></author></authors><title>10,000+ Times Accelerated Robust Subset Selection (ARSS)</title><categories>cs.LG cs.CV stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Subset selection from massive data with noised information is increasingly
popular for various applications. This problem is still highly challenging as
current methods are generally slow in speed and sensitive to outliers. To
address the above two issues, we propose an accelerated robust subset selection
(ARSS) method. Specifically in the subset selection area, this is the first
attempt to employ the $\ell_{p}(0&lt;p\leq1)$-norm based measure for the
representation loss, preventing large errors from dominating our objective. As
a result, the robustness against outlier elements is greatly enhanced.
Actually, data size is generally much larger than feature length, i.e. $N\gg
L$. Based on this observation, we propose a speedup solver (via ALM and
equivalent derivations) to highly reduce the computational cost, theoretically
from $O(N^{4})$ to $O(N{}^{2}L)$. Extensive experiments on ten benchmark
datasets verify that our method not only outperforms state of the art methods,
but also runs 10,000+ times faster than the most related method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3665</identifier>
 <datestamp>2014-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3665</id><created>2014-09-12</created><updated>2014-11-07</updated><authors><author><keyname>Beigi</keyname><forenames>Salman</forenames></author><author><keyname>Gohari</keyname><forenames>Amin</forenames></author></authors><title>A Monotone Measure for Non-Local Correlations</title><categories>quant-ph cs.IT math.IT</categories><comments>43 pages, 1 table, 3 figures, V2: added new results answering some of
  the open questions raised in the first version, V3: changed title, improved
  readability</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  No-signaling boxes are the abstract objects for studying non-locality, and
wirings are local operations on the space of no-signaling boxes. This means
that, no matter how non-local the nature is, the set of physical non-local
correlations must be closed under wirings. Then, one approach to identify the
non-locality of nature is to characterize closed sets of non-local
correlations. Although non-trivial examples of wirings of no-signaling boxes
are known, there is no systematic way to study wirings. In particular, given a
set of no-signaling boxes, we do not know a general method to prove that it is
closed under wirings. In this paper, we propose the first general method to
construct such closed sets of non-local correlations. We show that a well-known
measure of correlation, called maximal correlation, when appropriately defined
for non-local correlations, is monotonically decreasing under wirings. This
establishes a conjecture about the impossibility of simulating isotropic boxes
from each other, implying the existence of a continuum of closed sets of
non-local boxes under wirings. To prove our main result, we introduce some
mathematical tools that may be of independent interest: we define a notion of
maximal correlation ribbon as a generalization of maximal correlation, and
provide a connection between it and a known object called hypercontractivity
ribbon; we show that these two ribbons are monotone under wirings too.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3682</identifier>
 <datestamp>2014-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3682</id><created>2014-09-12</created><authors><author><keyname>Sauer</keyname><forenames>Caetano</forenames></author><author><keyname>H&#xe4;rder</keyname><forenames>Theo</forenames></author></authors><title>A novel recovery mechanism enabling fine-granularity locking and fast,
  REDO-only recovery</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a series of novel techniques and algorithms for transaction
commit, logging, recovery, and propagation control. In combination, they
provide a recovery component that maintains the persistent state of the
database (both log and data pages) always in a committed state. Recovery from
system and media failures only requires only REDO operations, which can happen
concurrently with the processing of new transactions. The mechanism supports
fine-granularity locking, partial rollbacks, and snapshot isolation for reader
transactions. Our design does not assume a specific hardware configuration such
as non-volatile RAM or flash---it is designed for traditional disk
environments. Nevertheless, it can exploit modern I/O devices for higher
transaction throughput and reduced recovery time with a high degree of
flexibility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3686</identifier>
 <datestamp>2015-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3686</id><created>2014-09-12</created><updated>2015-11-19</updated><authors><author><keyname>Cao</keyname><forenames>Pan</forenames></author><author><keyname>Zappone</keyname><forenames>Alessio</forenames></author><author><keyname>Jorswieck</keyname><forenames>Eduard A.</forenames></author></authors><title>Grouping-based Interference Alignment with IA-Cell Assignment in
  Multi-Cell MIMO MAC under Limited Feedback</title><categories>cs.IT math.IT</categories><comments>IEEE Trans. Signal Processing, accepted, Oct. 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interference alignment (IA) is a promising technique to efficiently mitigate
interference and to enhance the capacity of a wireless communication network.
This paper proposes a grouping-based interference alignment (GIA) with
optimized IA-Cell assignment for the multiple cells interfering multiple-input
and multiple-output (MIMO) multiple access channel (MAC) network under limited
feedback. This work consists of three main parts: 1) a complete study
(including some new improvements) of the GIA with respect to the degrees of
freedom (DoF) and optimal linear transceiver design is performed, which allows
for low-complexity and distributed implementation; 2) based on the GIA, the
concept of IA-Cell assignment is introduced. Three IA-Cell assignment
algorithms are proposed for the setup with different backhaul overhead and
their DoF and rate performance is investigated; 3) the performance of the
proposed GIA algorithms is studied under limited feedback of IA precoders. To
enable efficient feedback, a dynamic feedback bit allocation (DBA) problem is
formulated and solved in closed-form. The practical implementation, the
required backhaul overhead, and the complexity of the proposed algorithms are
analyzed. Numerical results show that our proposed algorithms greatly
outperform the traditional GIA under both unlimited and limited feedback.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3696</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3696</id><created>2014-09-12</created><updated>2016-03-04</updated><authors><author><keyname>Bezd&#x11b;k</keyname><forenames>Peter</forenames></author><author><keyname>Bene&#x161;</keyname><forenames>Nikola</forenames></author><author><keyname>Barnat</keyname><forenames>Ji&#x159;&#xed;</forenames></author><author><keyname>&#x10c;ern&#xe1;</keyname><forenames>Ivana</forenames></author></authors><title>LTL Parameter Synthesis of Parametric Timed Automata</title><categories>cs.FL cs.LO</categories><comments>23 pages, extended version</comments><msc-class>68Q60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The parameter synthesis problem for parametric timed automata is undecidable
in general even for very simple reachability properties. In this paper we
introduce restrictions on parameter valuations under which the parameter
synthesis problem is decidable for LTL properties. The investigated bounded
integer parameter synthesis problem could be solved using an explicit
enumeration of all possible parameter valuations. We propose an alternative
symbolic zone-based method for this problem which results in a faster
computation. Our technique extends the ideas of the automata-based approach to
LTL model checking of timed automata. To justify the usefulness of our
approach, we provide experimental evaluation and compare our method with
explicit enumeration technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3700</identifier>
 <datestamp>2014-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3700</id><created>2014-09-12</created><authors><author><keyname>Li</keyname><forenames>Xingfu</forenames></author><author><keyname>Zhu</keyname><forenames>Daming</forenames></author></authors><title>A 4/3-approximation algorithm for finding a spanning tree to maximize
  its internal vertices</title><categories>cs.DS</categories><comments>15 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper focuses on finding a spanning tree of a graph to maximize the
number of its internal vertices. We present an approximation algorithm for this
problem which can achieve a performance ratio $\frac{4}{3}$ on undirected
simple graphs. This improves upon the best known approximation algorithm with
performance ratio $\frac{5}{3}$ before. Our algorithm benefits from a new
observation for bounding the number of internal vertices of a spanning tree,
which reveals that a spanning tree of an undirected simple graph has less
internal vertices than the edges a maximum path-cycle cover of that graph has.
We can also give an example to show that the performance ratio $\frac{4}{3}$ is
actually tight for this algorithm. To decide how difficult it is for this
problem to be approximated, we show that finding a spanning tree of an
undirected simple graph to maximize its internal vertices is Max-SNP-Hard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3714</identifier>
 <datestamp>2014-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3714</id><created>2014-09-12</created><authors><author><keyname>Ammari</keyname><forenames>Habib</forenames></author><author><keyname>Wang</keyname><forenames>Han</forenames></author></authors><title>Time-domain multiscale shape identification in electro-sensing</title><categories>math.NA cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents premier and innovative time-domain multi-scale method for
shape identification in electro-sensing using pulse-type signals. The method is
based on transform-invariant shape descriptors computed from filtered
polarization tensors at multi-scales. The proposed algorithm enjoys a
remarkable noise robustness even with far-field measurements at very limited
angle of view. It opens a door for pulsed imaging using echolocation and
induction data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3717</identifier>
 <datestamp>2014-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3717</id><created>2014-09-12</created><authors><author><keyname>Coelho</keyname><forenames>Francisco</forenames></author><author><keyname>Nogueira</keyname><forenames>Vitor</forenames></author></authors><title>Probabilistic Selection in AgentSpeak(L)</title><categories>cs.MA cs.AI</categories><comments>8 pages, 3 figures</comments><msc-class>68T42, 68T27, 68T37, 67T45</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Agent programming is mostly a symbolic discipline and, as such, draws little
benefits from probabilistic areas as machine learning and graphical models.
However, the greatest objective of agent research is the achievement of
autonomy in dynamical and complex environments --- a goal that implies
embracing uncertainty and therefore the entailed representations, algorithms
and techniques. This paper proposes an innovative and conflict free two layer
approach to agent programming that uses already established methods and tools
from both symbolic and probabilistic artificial intelligence. Moreover, this
framework is illustrated by means of a widely used agent programming example,
GoldMiners.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3725</identifier>
 <datestamp>2014-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3725</id><created>2014-09-12</created><authors><author><keyname>Verlaine</keyname><forenames>Bertrand</forenames></author><author><keyname>Jureta</keyname><forenames>Ivan J.</forenames></author><author><keyname>Faulkner</keyname><forenames>St&#xe9;phane</forenames></author></authors><title>Aligning a Service Provisioning Model of a Service-Oriented System with
  the ITIL v.3 Life Cycle</title><categories>cs.CY cs.SE</categories><comments>This document is the technical work of a conference paper submitted
  to the International Conference on Exploring Service Science 1.5 (IESS 2015)</comments><msc-class>68M01</msc-class><acm-class>D.2.1; D.2.9; D.2.11; K.6.1</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Bringing together the ICT and the business layer of a service-oriented system
(SoS) remains a great challenge. Few papers tackle the management of SoS from
the business and organizational point of view. One solution is to use the
well-known ITIL v.3 framework. The latter enables to transform the organization
into a service-oriented organizational which focuses on the value provided to
the service customers. In this paper, we align the steps of the service
provisioning model with the ITIL v.3 processes. The alignment proposed should
help organizations and IT teams to integrate their ICT layer, represented by
the SoS, and their business layer, represented by ITIL v.3. One main advantage
of this combined use of ITIL and a SoS is the full service orientation of the
company.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3741</identifier>
 <datestamp>2014-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3741</id><created>2014-09-12</created><updated>2014-10-01</updated><authors><author><keyname>Deligkas</keyname><forenames>Argyrios</forenames></author><author><keyname>Fearnley</keyname><forenames>John</forenames></author><author><keyname>Savani</keyname><forenames>Rahul</forenames></author><author><keyname>Spirakis</keyname><forenames>Paul</forenames></author></authors><title>Computing Approximate Nash Equilibria in Polymatrix Games</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In an $\epsilon$-Nash equilibrium, a player can gain at most $\epsilon$ by
unilaterally changing his behaviour. For two-player (bimatrix) games with
payoffs in $[0,1]$, the best-known$\epsilon$ achievable in polynomial time is
0.3393. In general, for $n$-player games an $\epsilon$-Nash equilibrium can be
computed in polynomial time for an $\epsilon$ that is an increasing function of
$n$ but does not depend on the number of strategies of the players. For
three-player and four-player games the corresponding values of $\epsilon$ are
0.6022 and 0.7153, respectively. Polymatrix games are a restriction of general
$n$-player games where a player's payoff is the sum of payoffs from a number of
bimatrix games. There exists a very small but constant $\epsilon$ such that
computing an $\epsilon$-Nash equilibrium of a polymatrix game is \PPAD-hard.
Our main result is that a $(0.5+\delta)$-Nash equilibrium of an $n$-player
polymatrix game can be computed in time polynomial in the input size and
$\frac{1}{\delta}$. Inspired by the algorithm of Tsaknakis and Spirakis, our
algorithm uses gradient descent on the maximum regret of the players. We also
show that this algorithm can be applied to efficiently find a
$(0.5+\delta)$-Nash equilibrium in a two-player Bayesian game.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3742</identifier>
 <datestamp>2014-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3742</id><created>2014-09-12</created><authors><author><keyname>Abu-Khzam</keyname><forenames>Faisal N.</forenames></author><author><keyname>Bazgan</keyname><forenames>Cristina</forenames></author><author><keyname>Chopin</keyname><forenames>Morgan</forenames></author><author><keyname>Fernau</keyname><forenames>Henning</forenames></author></authors><title>Data Reductions and Combinatorial Bounds for Improved Approximation
  Algorithms</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Kernelization algorithms in the context of Parameterized Complexity are often
based on a combination of reduction rules and combinatorial insights. We will
expose in this paper a similar strategy for obtaining polynomial-time
approximation algorithms. Our method features the use of
approximation-preserving reductions, akin to the notion of parameterized
reductions. We exemplify this method to obtain the currently best approximation
algorithms for \textsc{Harmless Set}, \textsc{Differential} and
\textsc{Multiple Nonblocker}, all of them can be considered in the context of
securing networks or information propagation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3762</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3762</id><created>2014-09-12</created><updated>2014-09-15</updated><authors><author><keyname>Costa</keyname><forenames>Jo&#xe3;o Pita</forenames></author><author><keyname>&#x160;kraba</keyname><forenames>Primo&#x17e;</forenames></author><author><keyname>Vejdemo-Johansson</keyname><forenames>Mikael</forenames></author></authors><title>Aspects of an internal logic for persistence</title><categories>math.RA cs.CG math.LO</categories><msc-class>03G10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The foundational character of certain algebraic structures as Boolean
algebras and Heyting algebras is rooted in their potential to model classical
and constructive logic, respectively. In this paper we discuss the
contributions of algebraic logic to the study of persistence based on a new
operation on the ordered structure of the input diagram of vector spaces and
linear maps given by a filtration. Within the context of persistence theory, we
give an analysis of the underlying algebra, derive universal properties and
discuss new applications. We highlight the definition of the implication
operation within this construction, as well as interpret its meaning within
persistent homology, multidimensional persistence and zig-zag persistence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3767</identifier>
 <datestamp>2014-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3767</id><created>2014-09-11</created><authors><author><keyname>Hanumanthappa</keyname><forenames>J.</forenames></author><author><keyname>Annaiah</keyname><forenames>H.</forenames></author></authors><title>DW&amp;C:Dollops Wise Curtail IPv4/IPv6 Transition Mechanism using NS2</title><categories>cs.NI cs.PF</categories><comments>9 pages, International Journal of Engineering Trends and
  Technology(IJETT),2014</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  BD-SIIT and DSTM are widely deployed IPv4/IPv6 Transition mechanism to
improve the performance of the computer network in terms of Throughput,End to
End Delay(EED) and Packet Drop Rate(PDR).In this journal paper we have
Implemented and Compared the Performance Issues of our newly proposed Dollops
Wise Curtail(DW&amp;C)IPv4/IPv6 Migration Mechanism with BD-SIIT and DSTM in NS2.
Implementation and Comparison Performance Analysis between Dollops Wise
Curtail,BD-SIIT and DSTM shows that Dollops Wise Curtail IPv4/IPv6 migration
algorithm performance outperforms than BD-SIIT and DSTM.Based on extensive
simulations,we show that DW&amp;C algorithm reduces the Packet Drop Rate(PDR),End
to End Delay(EED) and achieves better Throughput than BD-SIIT and DSTM.In our
research work observation,the performance metrics such as Throughput,EED and
PLR for DW&amp;C,BD-SIIT and DSTM are measured using TCP,UDP,FTP and CBR Traffics
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3768</identifier>
 <datestamp>2014-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3768</id><created>2014-09-12</created><authors><author><keyname>Oh</keyname><forenames>Sang-Yun</forenames></author><author><keyname>Dalal</keyname><forenames>Onkar</forenames></author><author><keyname>Khare</keyname><forenames>Kshitij</forenames></author><author><keyname>Rajaratnam</keyname><forenames>Bala</forenames></author></authors><title>Optimization Methods for Sparse Pseudo-Likelihood Graphical Model
  Selection</title><categories>stat.CO cs.LG stat.ML</categories><comments>NIPS accepted version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse high dimensional graphical model selection is a popular topic in
contemporary machine learning. To this end, various useful approaches have been
proposed in the context of $\ell_1$-penalized estimation in the Gaussian
framework. Though many of these inverse covariance estimation approaches are
demonstrably scalable and have leveraged recent advances in convex
optimization, they still depend on the Gaussian functional form. To address
this gap, a convex pseudo-likelihood based partial correlation graph estimation
method (CONCORD) has been recently proposed. This method uses coordinate-wise
minimization of a regression based pseudo-likelihood, and has been shown to
have robust model selection properties in comparison with the Gaussian
approach. In direct contrast to the parallel work in the Gaussian setting
however, this new convex pseudo-likelihood framework has not leveraged the
extensive array of methods that have been proposed in the machine learning
literature for convex optimization. In this paper, we address this crucial gap
by proposing two proximal gradient methods (CONCORD-ISTA and CONCORD-FISTA) for
performing $\ell_1$-regularized inverse covariance matrix estimation in the
pseudo-likelihood framework. We present timing comparisons with coordinate-wise
minimization and demonstrate that our approach yields tremendous payoffs for
$\ell_1$-penalized partial correlation graph estimation outside the Gaussian
setting, thus yielding the fastest and most scalable approach for such
problems. We undertake a theoretical analysis of our approach and rigorously
demonstrate convergence, and also derive rates thereof.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3771</identifier>
 <datestamp>2014-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3771</id><created>2014-09-12</created><authors><author><keyname>Razis</keyname><forenames>Gerasimos</forenames></author><author><keyname>Anagnostopoulos</keyname><forenames>Ioannis</forenames></author></authors><title>Semantifying Twitter: the influenceTracker ontology</title><categories>cs.SI cs.IR</categories><comments>arXiv admin note: text overlap with arXiv:1404.5239</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose an ontology schema towards semantification
provision of Twitter social analytics. The ontology is deployed over a publicly
available service that measures how influential a Twitter account is, by
combining its social activity and interaction over Twittersphere. Apart from
influential quantity and quality measures, the service provides a SPARQL
endpoint where users can perform advance semantic queries through the RDFized
Twitter entities (mentions, replies, hashtags, photos, URLs) over the semantic
graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3798</identifier>
 <datestamp>2014-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3798</id><created>2014-09-12</created><authors><author><keyname>Mansuripur</keyname><forenames>Masud</forenames></author></authors><title>Advances in Macromolecular Data Storage</title><categories>physics.bio-ph cs.ET</categories><comments>6 pages, 6 figures, 21 references</comments><journal-ref>Published in Proceedings of SPIE, Optical Data Storage 2014,
  edited by R. Katayama and T.D. Milster, Vol. 9201, 92010A (2014)</journal-ref><doi>10.1117/12.2060549</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose to develop a new method of information storage to replace magnetic
hard disk drives and other instruments of secondary/backup data storage. The
proposed method stores petabytes of user-data in a sugar cube (1 cm3), and can
read/write that information at hundreds of megabits/sec. Digital information is
recorded and stored in the form of a long macromolecule consisting of at least
two bases, A and B. (This would be similar to DNA strands constructed from the
four nucleic acids G,C,A,T.) The macromolecules initially enter the system as
blank slates. A macromolecule with, say, 10,000 identical bases in the form of
AAAAA....AAA may be used to record a kilobyte block of user-data (including
modulation and error-correction coding), although, in this blank state, it can
only represent the null sequence 00000....000. Suppose this blank string of A's
is dragged before an atomically-sharp needle of a scanning tunneling microscope
(STM). When electric pulses are applied to the needle in accordance with the
sequence of 0s and 1s of a 1 kB block of user-data, selected A molecules will
be transformed into B molecules (e.g., a fraction of A will be broken off and
discarded). The resulting string now encodes the user-data in the form of
AABABBA...BAB. The same STM needle can subsequently read the recorded
information, as A and B would produce different electric signals when the
strand passes under the needle. The macromolecule now represents a data block
to be stored in a &quot;parking lot&quot; within the sugar cube, and later brought to a
read station on demand. Millions of parking spots and thousands of Read/Write
stations may be integrated within the micro-fabricated sugar cube, thus
providing access to petabytes of user-data in a scheme that benefits from the
massive parallelism of thousands of Read/Write stations within the same
three-dimensionally micro-structured device.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3799</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3799</id><created>2014-09-12</created><authors><author><keyname>Sgrignoli</keyname><forenames>Paolo</forenames></author></authors><title>The World Trade Web: A Multiple-Network Perspective</title><categories>physics.soc-ph cs.SI q-fin.EC</categories><comments>PhD thesis at IMT Institute for Advanced Studies Lucca. 114 pages, 12
  figures, 18 tables</comments><doi>10.6092/imtlucca/e-theses/137</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  International Trade (IT) plays a fundamental role in today's economy: by
connecting world countries production and consumption processes, it radically
contributes in shaping their economy and development path. Although its
evolving structure and determinants have been widely analyzed in the
literature, much less has been done to understand its interplay with other
complex phenomena. The aim of this work is, precisely in this direction, to
study the relations of IT with International Migration (IM) and Foreign Direct
Investments (FDI). In both cases the procedure used is to first approach the
problem in a multiple-networks perspective and than deepen the analysis by
using ad hoc econometrics techniques. With respect to IM, a general positive
correlation with IT is highlighted and product categories for which this effect
is stronger are identified and cross-checked with previous classifications.
Next, employing spatial econometric techniques and proposing a new way to
define country neighbors based on the most intense IM flows, direct/indirect
network effects are studied and a stronger competitive effect of third country
migrants is identified for a specific product class. In the case of FDI, first
correlations between the two networks are identified, highlighting how they can
be mostly explained by countries economic/demographic size and geographical
distance. Then, using the Heckman selection model with a gravity equation,
(non-linear) components arising from distance, position in the Global Supply
Chain and presence of Regional Trade Agreements are studied. Finally, it is
shown how IT and FDI correlation changes with sectors: they are complements in
manufacturing, but substitutes in services.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3804</identifier>
 <datestamp>2014-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3804</id><created>2014-09-11</created><authors><author><keyname>Ad&#xe1;mek</keyname><forenames>Ji&#x159;&#xed;</forenames></author><author><keyname>Bowler</keyname><forenames>Nathan</forenames></author><author><keyname>Levy</keyname><forenames>Paul B.</forenames></author><author><keyname>Milius</keyname><forenames>Stefan</forenames></author></authors><title>Coproducts of Monads on Set</title><categories>cs.LO</categories><comments>Presented at the conference &quot;27th Annual Symposium on Logic in
  Computer Science (LICS 2012)&quot;. The current version contains proofs of some of
  the results in the appendix</comments><proxy>Jiri Adamek</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Coproducts of monads on Set have arisen in both the study of computational
effects and universal algebra.
  We describe coproducts of consistent monads on Set by an initial algebra
formula, and prove also the converse: if the coproduct exists, so do the
required initial algebras. That formula was, in the case of ideal monads, also
used by Ghani and Uustalu. We deduce that coproduct embeddings of consistent
monads are injective; and that a coproduct of injective monad morphisms is
injective.
  Two consistent monads have a coproduct iff either they have arbitrarily large
common fixpoints, or one is an exception monad, possibly modified to preserve
the empty set. Hence a consistent monad has a coproduct with every monad iff it
is an exception monad, possibly modified to preserve the empty set. We also
show other fixpoint results, including that a functor (not constant on nonempty
sets) is finitary iff every sufficiently large cardinal is a fixpoint.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3805</identifier>
 <datestamp>2014-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3805</id><created>2014-09-11</created><authors><author><keyname>Ad&#xe1;mek</keyname><forenames>Ji&#x159;&#xed;</forenames></author></authors><title>Colimits of Monads</title><categories>cs.LO</categories><proxy>Jiri Adamek</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The category of all monads over many-sorted sets (and over other &quot;set-like&quot;
categories) is proved to have coequalizers and strong cointersections. And a
general diagram has a colimit whenever all the monads involved preserve
monomorphisms and have arbitrarily large joint pre-fixpoints. In contrast,
coequalizers fail to exist e.g. for monads over the (presheaf) category of
graphs.
  For more general categories we extend the results on coproducts of monads
from [2]. We call a monad separated if, when restricted to monomorphisms, its
unit has a complement. We prove that every collection of separated monads with
arbitrarily large joint pre-fixpoints has a coproduct. And a concrete formula
for these coproducts is presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3809</identifier>
 <datestamp>2014-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3809</id><created>2014-09-12</created><updated>2014-12-01</updated><authors><author><keyname>Crankshaw</keyname><forenames>Daniel</forenames></author><author><keyname>Bailis</keyname><forenames>Peter</forenames></author><author><keyname>Gonzalez</keyname><forenames>Joseph E.</forenames></author><author><keyname>Li</keyname><forenames>Haoyuan</forenames></author><author><keyname>Zhang</keyname><forenames>Zhao</forenames></author><author><keyname>Franklin</keyname><forenames>Michael J.</forenames></author><author><keyname>Ghodsi</keyname><forenames>Ali</forenames></author><author><keyname>Jordan</keyname><forenames>Michael I.</forenames></author></authors><title>The Missing Piece in Complex Analytics: Low Latency, Scalable Model
  Management and Serving with Velox</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To support complex data-intensive applications such as personalized
recommendations, targeted advertising, and intelligent services, the data
management community has focused heavily on the design of systems to support
training complex models on large datasets. Unfortunately, the design of these
systems largely ignores a critical component of the overall analytics process:
the deployment and serving of models at scale. In this work, we present Velox,
a new component of the Berkeley Data Analytics Stack. Velox is a data
management system for facilitating the next steps in real-world, large-scale
analytics pipelines: online model management, maintenance, and serving. Velox
provides end-user applications and services with a low-latency, intuitive
interface to models, transforming the raw statistical models currently trained
using existing offline large-scale compute frameworks into full-blown,
end-to-end data products capable of recommending products, targeting
advertisements, and personalizing web content. To provide up-to-date results
for these complex models, Velox also facilitates lightweight online model
maintenance and selection (i.e., dynamic weighting). In this paper, we describe
the challenges and architectural considerations required to achieve this
functionality, including the abilities to span online and offline systems, to
adaptively adjust model materialization strategies, and to exploit inherent
statistical properties such as model error tolerance, all while operating at
&quot;Big Data&quot; scale.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3813</identifier>
 <datestamp>2014-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3813</id><created>2014-09-12</created><authors><author><keyname>Versley</keyname><forenames>Yannick</forenames></author></authors><title>Incorporating Semi-supervised Features into Discontinuous Easy-First
  Constituent Parsing</title><categories>cs.CL</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper describes adaptations for EaFi, a parser for easy-first parsing of
discontinuous constituents, to adapt it to multiple languages as well as make
use of the unlabeled data that was provided as part of the SPMRL shared task
2014.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3819</identifier>
 <datestamp>2014-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3819</id><created>2014-09-12</created><authors><author><keyname>Doligez</keyname><forenames>Damien</forenames><affiliation>INRIA Paris-Rocquencourt</affiliation></author><author><keyname>Kriener</keyname><forenames>Jael</forenames><affiliation>MSR - INRIA</affiliation></author><author><keyname>Lamport</keyname><forenames>Leslie</forenames><affiliation>MSR - INRIA</affiliation></author><author><keyname>Libal</keyname><forenames>Tomer</forenames><affiliation>MSR - INRIA</affiliation></author><author><keyname>Merz</keyname><forenames>Stephan</forenames><affiliation>INRIA Nancy - Grand Est / LORIA</affiliation></author></authors><title>Coalescing: Syntactic Abstraction for Reasoning in First-Order Modal
  Logics</title><categories>cs.LO</categories><comments>appears in Automated Reasoning in Quantified Non-Classical Logics
  (2014)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a syntactic abstraction method to reason about first-order modal
logics by using theorem provers for standard first-order logic and for
propositional modal logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3821</identifier>
 <datestamp>2015-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3821</id><created>2014-09-12</created><updated>2015-07-30</updated><authors><author><keyname>Montanari</keyname><forenames>Andrea</forenames></author></authors><title>Computational Implications of Reducing Data to Sufficient Statistics</title><categories>stat.CO cs.IT cs.LG math.IT</categories><comments>20 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a large dataset and an estimation task, it is common to pre-process the
data by reducing them to a set of sufficient statistics. This step is often
regarded as straightforward and advantageous (in that it simplifies statistical
analysis). I show that -on the contrary- reducing data to sufficient statistics
can change a computationally tractable estimation problem into an intractable
one. I discuss connections with recent work in theoretical computer science,
and implications for some techniques to estimate graphical models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3836</identifier>
 <datestamp>2014-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3836</id><created>2014-09-12</created><updated>2014-09-17</updated><authors><author><keyname>Bresler</keyname><forenames>Guy</forenames></author><author><keyname>Gamarnik</keyname><forenames>David</forenames></author><author><keyname>Shah</keyname><forenames>Devavrat</forenames></author></authors><title>Hardness of parameter estimation in graphical models</title><categories>cs.CC cs.AI cs.IT math.IT stat.CO</categories><comments>15 pages. To appear in NIPS 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of learning the canonical parameters specifying an
undirected graphical model (Markov random field) from the mean parameters. For
graphical models representing a minimal exponential family, the canonical
parameters are uniquely determined by the mean parameters, so the problem is
feasible in principle. The goal of this paper is to investigate the
computational feasibility of this statistical task. Our main result shows that
parameter estimation is in general intractable: no algorithm can learn the
canonical parameters of a generic pair-wise binary graphical model from the
mean parameters in time bounded by a polynomial in the number of variables
(unless RP = NP). Indeed, such a result has been believed to be true (see the
monograph by Wainwright and Jordan (2008)) but no proof was known.
  Our proof gives a polynomial time reduction from approximating the partition
function of the hard-core model, known to be hard, to learning approximate
parameters. Our reduction entails showing that the marginal polytope boundary
has an inherent repulsive property, which validates an optimization procedure
over the polytope that does not use any knowledge of its structure (as required
by the ellipsoid method and others).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3837</identifier>
 <datestamp>2015-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3837</id><created>2014-09-12</created><authors><author><keyname>Sgrignoli</keyname><forenames>Paolo</forenames></author><author><keyname>Agliari</keyname><forenames>Elena</forenames></author><author><keyname>Burioni</keyname><forenames>Raffaella</forenames></author><author><keyname>Schianchi</keyname><forenames>Augusto</forenames></author></authors><title>Instability and network effects in innovative markets</title><categories>physics.soc-ph cs.SI q-fin.GN</categories><comments>20 pages, 6 figures. 7th workshop on &quot;Dynamic Models in Economics and
  Finance&quot; - MDEF2012 (COST Action IS1104), Urbino (2012)</comments><journal-ref>Mathematics and Computers in Simulation, Volume 108, February
  2015, Pages 260-271</journal-ref><doi>10.1016/j.matcom.2014.05.013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a network of interacting agents and we model the process of
choice on the adoption of a given innovative product by means of
statistical-mechanics tools. The modelization allows us to focus on the effects
of direct interactions among agents in establishing the success or failure of
the product itself. Mimicking real systems, the whole population is divided
into two sub-communities called, respectively, Innovators and Followers, where
the former are assumed to display more influence power. We study in detail and
via numerical simulations on a random graph two different scenarios:
no-feedback interaction, where innovators are cohesive and not sensitively
affected by the remaining population, and feedback interaction, where the
influence of followers on innovators is non negligible. The outcomes are
markedly different: in the former case, which corresponds to the creation of a
niche in the market, Innovators are able to drive and polarize the whole
market. In the latter case the behavior of the market cannot be definitely
predicted and become unstable. In both cases we highlight the emergence of
collective phenomena and we show how the final outcome, in terms of the number
of buyers, is affected by the concentration of innovators and by the
interaction strengths among agents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3838</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3838</id><created>2014-09-12</created><authors><author><keyname>Alizadeh</keyname><forenames>Ardalan</forenames></author><author><keyname>Bahrami</keyname><forenames>Hamid Reza</forenames></author><author><keyname>Maleki</keyname><forenames>Mehdi</forenames></author><author><keyname>Sastry</keyname><forenames>Shivakumar</forenames></author></authors><title>Spatial Sensing and Cognitive Radio Communication in the Presence of A
  $K$-User Interference Primary Network</title><categories>cs.NI cs.IT math.IT</categories><comments>IEEE Journal on Selected Areas in Communications, April 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the feasibility of cognitive radio (CR) communication in the
presence of a $K$-user multi-input multi-output (MIMO) interference channel as
the primary network. Assuming that the primary interference network has unused
spatial degrees of freedom (DoFs), we first investigate the sufficient
condition on the number of antennas at the secondary transmitter under which
the secondary system can communicate while causing no interference to the
primary receivers. We show that, to maximize the benefit, the secondary
transmitter should have at least the same number of antennas as the spatial
DoFs of the primary system. We then derive the secondary precoding and decoding
matrices to have zero interference leakage into the primary network while the
signal-to-interference plus noise ratio (SINR) at the secondary receiver is
maximized. As the success of the secondary communication depends on the
availability of unused DoFs, we then propose a fast sensing method based on the
eigenvalue analysis of the received signal covariance matrix to determine the
availability of unused DoFs or equivalently spatial holes. Since the proposed
fast sensing method cannot identify the indices of inactive primary streams, we
also provide a fine sensing method based on the generalized likelihood ratio
test (GLRT) to decide the absence of individual primary streams. Simulation
results show that the proposed CR sensing and transmission scheme can, in
practice, provide a significant throughput while causing no interference to the
primary receivers, and that the sensing detects the spatial holes of the
primary network with high detection probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3854</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3854</id><created>2014-09-12</created><authors><author><keyname>Celebi</keyname><forenames>M. Emre</forenames></author><author><keyname>Kingravi</keyname><forenames>Hassan A.</forenames></author></authors><title>Linear, Deterministic, and Order-Invariant Initialization Methods for
  the K-Means Clustering Algorithm</title><categories>cs.LG cs.CV</categories><comments>21 pages, 2 figures, 5 tables, Partitional Clustering Algorithms
  (Springer, 2014). arXiv admin note: substantial text overlap with
  arXiv:1304.7465, arXiv:1209.1960</comments><acm-class>I.5.3; H.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the past five decades, k-means has become the clustering algorithm of
choice in many application domains primarily due to its simplicity, time/space
efficiency, and invariance to the ordering of the data points. Unfortunately,
the algorithm's sensitivity to the initial selection of the cluster centers
remains to be its most serious drawback. Numerous initialization methods have
been proposed to address this drawback. Many of these methods, however, have
time complexity superlinear in the number of data points, which makes them
impractical for large data sets. On the other hand, linear methods are often
random and/or sensitive to the ordering of the data points. These methods are
generally unreliable in that the quality of their results is unpredictable.
Therefore, it is common practice to perform multiple runs of such methods and
take the output of the run that produces the best results. Such a practice,
however, greatly increases the computational requirements of the otherwise
highly efficient k-means algorithm. In this chapter, we investigate the
empirical performance of six linear, deterministic (non-random), and
order-invariant k-means initialization methods on a large and diverse
collection of data sets from the UCI Machine Learning Repository. The results
demonstrate that two relatively unknown hierarchical initialization methods due
to Su and Dy outperform the remaining four methods with respect to two
objective effectiveness criteria. In addition, a recent method due to Erisoglu
et al. performs surprisingly poorly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3865</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3865</id><created>2014-09-12</created><authors><author><keyname>V'yugin</keyname><forenames>Vladimir V.</forenames></author></authors><title>On Stability Property of Probability Laws with Respect to Small
  Violations of Algorithmic Randomness</title><categories>cs.CC</categories><comments>25 pages. arXiv admin note: substantial text overlap with
  arXiv:1105.4274, arXiv:0806.4572</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a stability property of probability laws with respect to small
violations of algorithmic randomness. A sufficient condition of stability is
presented in terms of Schnorr tests of algorithmic randomness. Most probability
laws, like the strong law of large numbers, the law of iterated logarithm, and
even Birkhoff's pointwise ergodic theorem for ergodic transformations, are
stable in this sense. Nevertheless, the phenomenon of instability occurs in
ergodic theory. Firstly, the stability property of the Birkhoff's ergodic
theorem is non-uniform. Moreover, a computable non-ergodic measure preserving
transformation can be constructed such that ergodic theorem is non-stable. We
also show that any universal data compression scheme is also non-stable with
respect to the class of all computable ergodic measures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3867</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3867</id><created>2014-09-12</created><authors><author><keyname>Singh</keyname><forenames>Vishwakarma</forenames></author><author><keyname>Singh</keyname><forenames>Ambuj K.</forenames></author></authors><title>Nearest Keyword Set Search in Multi-dimensional Datasets</title><categories>cs.DB cs.IR</categories><comments>Accepted as Full Research Paper to ICDE 2014, Chicago, IL, USA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Keyword-based search in text-rich multi-dimensional datasets facilitates many
novel applications and tools. In this paper, we consider objects that are
tagged with keywords and are embedded in a vector space. For these datasets, we
study queries that ask for the tightest groups of points satisfying a given set
of keywords. We propose a novel method called ProMiSH (Projection and Multi
Scale Hashing) that uses random projection and hash-based index structures, and
achieves high scalability and speedup. We present an exact and an approximate
version of the algorithm. Our empirical studies, both on real and synthetic
datasets, show that ProMiSH has a speedup of more than four orders over
state-of-the-art tree-based techniques. Our scalability tests on datasets of
sizes up to 10 million and dimensions up to 100 for queries having up to 9
keywords show that ProMiSH scales linearly with the dataset size, the dataset
dimension, the query size, and the result size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3870</identifier>
 <datestamp>2015-05-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3870</id><created>2014-09-12</created><updated>2015-01-30</updated><authors><author><keyname>Williams</keyname><forenames>Jake Ryland</forenames></author><author><keyname>Bagrow</keyname><forenames>James P.</forenames></author><author><keyname>Danforth</keyname><forenames>Christopher M.</forenames></author><author><keyname>Dodds</keyname><forenames>Peter Sheridan</forenames></author></authors><title>Text mixing shapes the anatomy of rank-frequency distributions: A modern
  Zipfian mechanics for natural language</title><categories>cs.CL physics.soc-ph</categories><comments>9 pages, 6 figures, and 1 table</comments><journal-ref>Phys. Rev. E 91, 052811 (2015)</journal-ref><doi>10.1103/PhysRevE.91.052811</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Natural languages are full of rules and exceptions. One of the most famous
quantitative rules is Zipf's law which states that the frequency of occurrence
of a word is approximately inversely proportional to its rank. Though this
`law' of ranks has been found to hold across disparate texts and forms of data,
analyses of increasingly large corpora over the last 15 years have revealed the
existence of two scaling regimes. These regimes have thus far been explained by
a hypothesis suggesting a separability of languages into core and non-core
lexica. Here, we present and defend an alternative hypothesis, that the two
scaling regimes result from the act of aggregating texts. We observe that text
mixing leads to an effective decay of word introduction, which we show provides
accurate predictions of the location and severity of breaks in scaling. Upon
examining large corpora from 10 languages in the Project Gutenberg eBooks
collection (eBooks), we find emphatic empirical support for the universality of
our claim.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3879</identifier>
 <datestamp>2015-04-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3879</id><created>2014-09-12</created><updated>2015-04-23</updated><authors><author><keyname>Liao</keyname><forenames>Qianli</forenames></author><author><keyname>Leibo</keyname><forenames>Joel Z.</forenames></author><author><keyname>Poggio</keyname><forenames>Tomaso</forenames></author></authors><title>Unsupervised learning of clutter-resistant visual representations from
  natural videos</title><categories>cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Populations of neurons in inferotemporal cortex (IT) maintain an explicit
code for object identity that also tolerates transformations of object
appearance e.g., position, scale, viewing angle [1, 2, 3]. Though the learning
rules are not known, recent results [4, 5, 6] suggest the operation of an
unsupervised temporal-association-based method e.g., Foldiak's trace rule [7].
Such methods exploit the temporal continuity of the visual world by assuming
that visual experience over short timescales will tend to have invariant
identity content. Thus, by associating representations of frames from nearby
times, a representation that tolerates whatever transformations occurred in the
video may be achieved. Many previous studies verified that such rules can work
in simple situations without background clutter, but the presence of visual
clutter has remained problematic for this approach. Here we show that temporal
association based on large class-specific filters (templates) avoids the
problem of clutter. Our system learns in an unsupervised way from natural
videos gathered from the internet, and is able to perform a difficult
unconstrained face recognition task on natural images: Labeled Faces in the
Wild [8].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3881</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3881</id><created>2014-09-12</created><authors><author><keyname>Bloodgood</keyname><forenames>Michael</forenames></author><author><keyname>Vijay-Shanker</keyname><forenames>K.</forenames></author></authors><title>An Approach to Reducing Annotation Costs for BioNLP</title><categories>cs.CL cs.LG stat.ML</categories><comments>2 pages, 1 figure, 5 tables; appeared in Proceedings of the Workshop
  on Current Trends in Biomedical Natural Language Processing at ACL
  (Association for Computational Linguistics) 2008</comments><acm-class>I.2.7; I.2.6; I.5.1; I.5.4</acm-class><journal-ref>In Proceedings of the Workshop on Current Trends in Biomedical
  Natural Language Processing, pages 104-105, Columbus, Ohio, June 2008.
  Association for Computational Linguistics</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is a broad range of BioNLP tasks for which active learning (AL) can
significantly reduce annotation costs and a specific AL algorithm we have
developed is particularly effective in reducing annotation costs for these
tasks. We have previously developed an AL algorithm called ClosestInitPA that
works best with tasks that have the following characteristics: redundancy in
training material, burdensome annotation costs, Support Vector Machines (SVMs)
work well for the task, and imbalanced datasets (i.e. when set up as a binary
classification problem, one class is substantially rarer than the other). Many
BioNLP tasks have these characteristics and thus our AL algorithm is a natural
approach to apply to BioNLP tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3893</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3893</id><created>2014-09-12</created><authors><author><keyname>Bassily</keyname><forenames>Raef</forenames></author><author><keyname>Smith</keyname><forenames>Adam</forenames></author></authors><title>Causal Erasure Channels</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the communication problem over binary causal adversarial erasure
channels. Such a channel maps $n$ input bits to $n$ output symbols in
$\{0,1,\wedge\}$, where $\wedge$ denotes erasure. The channel is causal if, for
every $i$, the channel adversarially decides whether to erase the $i$th bit of
its input based on inputs $1,...,i$, before it observes bits $i+1$ to $n$. Such
a channel is $p$-bounded if it can erase at most a $p$ fraction of the input
bits over the whole transmission duration. Causal channels provide a natural
model for channels that obey basic physical restrictions but are otherwise
unpredictable or highly variable. For a given erasure rate $p$, our goal is to
understand the optimal rate (the capacity) at which a randomized (stochastic)
encoder/decoder can transmit reliably across all causal $p$-bounded erasure
channels. In this paper, we introduce the causal erasure model and provide new
upper bounds and lower bounds on the achievable rate. Our bounds separate the
achievable rate in the causal erasures setting from the rates achievable in two
related models: random erasure channels (strictly weaker) and fully adversarial
erasure channels (strictly stronger). Specifically, we show:
  - A strict separation between random and causal erasures for all constant
erasure rates $p\in(0,1)$.
  - A strict separation between causal and fully adversarial erasures for
$p\in(0,\phi)$ where $\phi \approx 0.348$.
  - For $p\in[\phi,1/2)$, we show codes for causal erasures that have higher
rate than the best known constructions for fully adversarial channels.
  Our results contrast with existing results on correcting causal bit-flip
errors (as opposed to erasures) [Dey et. al 2008, 2009], [Haviv-Langberg 2011].
For the separations we provide, the analogous separations for bit-flip models
are either not known at all or much weaker.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3900</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3900</id><created>2014-09-12</created><updated>2016-01-26</updated><authors><author><keyname>Rawat</keyname><forenames>Ankit Singh</forenames></author><author><keyname>Mazumdar</keyname><forenames>Arya</forenames></author><author><keyname>Vishwanath</keyname><forenames>Sriram</forenames></author></authors><title>Cooperative Local Repair in Distributed Storage</title><categories>cs.IT math.IT</categories><comments>Fixed some minor issues in Theorem 1, EURASIP Journal on Advances in
  Signal Processing, December 2015</comments><doi>10.1186/s13634-015-0292-0</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Erasure-correcting codes, that support local repair of codeword symbols, have
attracted substantial attention recently for their application in distributed
storage systems. This paper investigates a generalization of the usual locally
repairable codes. In particular, this paper studies a class of codes with the
following property: any small set of codeword symbols can be reconstructed
(repaired) from a small number of other symbols. This is referred to as
cooperative local repair. The main contribution of this paper is bounds on the
trade-off of the minimum distance and the dimension of such codes, as well as
explicit constructions of families of codes that enable cooperative local
repair. Some other results regarding cooperative local repair are also
presented, including an analysis for the well-known Hadamard/Simplex codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3902</identifier>
 <datestamp>2015-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3902</id><created>2014-09-12</created><updated>2015-01-07</updated><authors><author><keyname>Ngo</keyname><forenames>Hien Quoc</forenames></author><author><keyname>Matthaiou</keyname><forenames>Michail</forenames></author><author><keyname>Larsson</keyname><forenames>Erik G.</forenames></author></authors><title>Massive MIMO with Optimal Power and Training Duration Allocation</title><categories>cs.IT math.IT</categories><comments>corrected version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the uplink of massive multicell multiple-input multiple-output
systems, where the base stations (BSs), equipped with massive arrays, serve
simultaneously several terminals in the same frequency band. We assume that the
BS estimates the channel from uplink training, and then uses the maximum ratio
combining technique to detect the signals transmitted from all terminals in its
own cell. We propose an optimal resource allocation scheme which jointly
selects the training duration, training signal power, and data signal power in
order to maximize the sum spectral efficiency, for a given total energy budget
spent in a coherence interval. Numerical results verify the benefits of the
optimal resource allocation scheme. Furthermore, we show that more training
signal power should be used at low signal-to-noise ratio (SNRs), and vice versa
at high SNRs. Interestingly, for the entire SNR regime, the optimal training
duration is equal to the number of terminals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3903</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3903</id><created>2014-09-12</created><authors><author><keyname>Mustafidah</keyname><forenames>Hindayati</forenames></author><author><keyname>Suwarsito</keyname></author></authors><title>Analysis of Competence Level and the Attendance of the Lecturer in Its
  Effects on Students Grade Using Fuzzy Quantification Theory</title><categories>cs.CY</categories><comments>5 pages, 1 figure, 5 tables, 4 equations, published with
  International Journal of Computer Science Issues (IJCSI)</comments><journal-ref>IJCSI, 11(4):75-79, July 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It known that teachers as educators should have competence that shows its
quality so that the lecture material provided can be absorbed by the student.
The competencies in this term include the pedagogic, professional, personality,
and social. The competence that owned by lecturer can be obtained from the
results of the assessment conducted by the student through the filling of the
questionnaire. This study conducted an analysis of the level of the lecturer
competence of the relationship between the present of lecturer in classroom
with a percentage of the value of passing students in courses using fuzzy
quantification theory. Based on the results of the four competencies acquired
professional competence that contributes most of 79.45% in contributed the
attendance of lecturer will it affect the percentage of passing students in
courses that are shown with a percentage of the graduation minimum B.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3904</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3904</id><created>2014-09-12</created><authors><author><keyname>Zhong</keyname><forenames>Caijun</forenames></author><author><keyname>Suraweera</keyname><forenames>Himal A.</forenames></author><author><keyname>Zheng</keyname><forenames>Gan</forenames></author><author><keyname>Krikidis</keyname><forenames>Ioannis</forenames></author><author><keyname>Zhang</keyname><forenames>Zhaoyang</forenames></author></authors><title>Wireless Information and Power Transfer with Full Duplex Relaying</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a dual-hop full-duplex relaying system, where the energy
constrained relay node is powered by radio frequency signals from the source
using the time-switching architecture, both the amplify-and-forward and
decode-and-forward relaying protocols are studied. Specifically, we provide an
analytical characterization of the achievable throughput of three different
communication modes, namely, instantaneous transmission, delay-constrained
transmission, and delay tolerant transmission. In addition, the optimal time
split is studied for different transmission modes. Our results reveal that,
when the time split is optimized, the full-duplex relaying could substantially
boost the system throughput compared to the conventional half-duplex relaying
architecture for all three transmission modes. In addition, it is shown that
the instantaneous transmission mode attains the highest throughput. However,
compared to the delay-constrained transmission mode, the throughput gap is
rather small. Unlike the instantaneous time split optimization which requires
instantaneous channel state information, the optimal time split in the
delay-constrained transmission mode depends only on the statistics of the
channel, hence, is suitable for practical implementations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3905</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3905</id><created>2014-09-12</created><authors><author><keyname>Rudelson</keyname><forenames>Mark</forenames></author><author><keyname>Samorodnitsky</keyname><forenames>Alex</forenames></author><author><keyname>Zeitouni</keyname><forenames>Ofer</forenames></author></authors><title>Random Gaussian matrices and Hafnian estimators</title><categories>math.PR cs.DS math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze the behavior of the Barvinok estimator of the hafnian of even
dimension, symmetric matrices with non negative entries. We introduce a
condition under which the Barvinok estimator achieves sub-exponential errors,
and show that this condition is almost optimal. Using that hafnians count the
number of perfect matchings in graphs, we conclude that Barvinok's estimator
gives a polynomial-time algorithm for the approximate (up to subexponential
errors) evaluation of the number of perfect matchings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3906</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3906</id><created>2014-09-12</created><authors><author><keyname>Shen</keyname><forenames>Ju</forenames></author><author><keyname>Yang</keyname><forenames>Jianjun</forenames></author><author><keyname>Taha-abusneineh</keyname><forenames>Sami</forenames></author><author><keyname>Payne</keyname><forenames>Bryson</forenames></author><author><keyname>Hitz</keyname><forenames>Markus</forenames></author></authors><title>Structure Preserving Large Imagery Reconstruction</title><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  With the explosive growth of web-based cameras and mobile devices, billions
of photographs are uploaded to the internet. We can trivially collect a huge
number of photo streams for various goals, such as image clustering, 3D scene
reconstruction, and other big data applications. However, such tasks are not
easy due to the fact the retrieved photos can have large variations in their
view perspectives, resolutions, lighting, noises, and distortions.
Fur-thermore, with the occlusion of unexpected objects like people, vehicles,
it is even more challenging to find feature correspondences and reconstruct
re-alistic scenes. In this paper, we propose a structure-based image completion
algorithm for object removal that produces visually plausible content with
consistent structure and scene texture. We use an edge matching technique to
infer the potential structure of the unknown region. Driven by the estimated
structure, texture synthesis is performed automatically along the estimated
curves. We evaluate the proposed method on different types of images: from
highly structured indoor environment to natural scenes. Our experimental
results demonstrate satisfactory performance that can be potentially used for
subsequent big data processing, such as image localization, object retrieval,
and scene reconstruction. Our experiments show that this approach achieves
favorable results that outperform existing state-of-the-art techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3909</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3909</id><created>2014-09-13</created><authors><author><keyname>Morin</keyname><forenames>Sophie</forenames></author><author><keyname>Robert</keyname><forenames>Jean-Marc</forenames></author><author><keyname>Gabora</keyname><forenames>Liane</forenames></author></authors><title>A New Course on Creativity in an Engineering Program: Foundations and
  Issues</title><categories>cs.CY q-bio.NC</categories><comments>10 pages, Intl Conf on Innovative Design and Manufacturing (pp.
  270-275). Aug 13-15, Montreal. IEEE Conference Proceedings</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The importance of innovation in the world's economy, now undeniable, draws
great attention to the need to improve organizations' creative potential. In
the last 60 years, hundreds of books have been written on the subject and
hundreds of webpages display information on how to be more creative and achieve
innovation. Several North American and European universities offer graduated
programs in creativity. However, building an effective and validated creativity
training program is not without challenges. Because of the nature of their
work, engineers are often asked to be innovative. Without aiming for a degree
in creativity, could future engineers benefit from training programs in
creativity? This article presents the conceptual framework and pedagogical
elements of a new course in creativity for engineering students.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3912</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3912</id><created>2014-09-13</created><authors><author><keyname>Matsui</keyname><forenames>Kota</forenames></author><author><keyname>Kumagai</keyname><forenames>Wataru</forenames></author><author><keyname>Kanamori</keyname><forenames>Takafumi</forenames></author></authors><title>Parallel Distributed Block Coordinate Descent Methods based on Pairwise
  Comparison Oracle</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper provides a block coordinate descent algorithm to solve
unconstrained optimization problems. In our algorithm, computation of function
values or gradients is not required. Instead, pairwise comparison of function
values is used. Our algorithm consists of two steps; one is the direction
estimate step and the other is the search step. Both steps require only
pairwise comparison of function values, which tells us only the order of
function values over two points. In the direction estimate step, a Newton type
search direction is estimated. A computation method like block coordinate
descent methods is used with the pairwise comparison. In the search step, a
numerical solution is updated along the estimated direction. The computation in
the direction estimate step can be easily parallelized, and thus, the algorithm
works efficiently to find the minimizer of the objective function. Also, we
show an upper bound of the convergence rate. In numerical experiments, we show
that our method efficiently finds the optimal solution compared to some
existing methods based on the pairwise comparison.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3913</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3913</id><created>2014-09-13</created><authors><author><keyname>Lee</keyname><forenames>Jae-Yeong</forenames></author><author><keyname>Yu</keyname><forenames>Wonpil</forenames></author></authors><title>Concurrent Tracking of Inliers and Outliers</title><categories>cs.CV</categories><comments>draft</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In object tracking, outlier is one of primary factors which degrade
performance of image-based tracking algorithms. In this respect, therefore,
most of the existing methods simply discard detected outliers and pay little or
no attention to employing them as an important source of information for motion
estimation. We consider outliers as important as inliers for object tracking
and propose a motion estimation algorithm based on concurrent tracking of
inliers and outliers. Our tracker makes use of pyramidal implementation of the
Lucas-Kanade tracker to estimate motion flows of inliers and outliers and final
target motion is estimated robustly based on both of these information.
Experimental results from challenging benchmark video sequences confirm
enhanced tracking performance, showing highly stable target tracking under
severe occlusion compared with state-of-the-art algorithms. The proposed
algorithm runs at more than 100 frames per second even without using a hardware
accelerator, which makes the proposed method more practical and portable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3914</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3914</id><created>2014-09-13</created><authors><author><keyname>Palasek</keyname><forenames>Stan</forenames></author></authors><title>A Reputation-Based Model for Decision-Making in Online Social Networks</title><categories>cs.SI physics.soc-ph</categories><comments>11 pages, 5 figures</comments><msc-class>91D30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The online exchange of social recognition including, for instance, the
Facebook &quot;like&quot; appears to produce a scarce allocation without a clear utility
function defined for anyone involved. Given the importance attached to such
digital commodities by both users and advertisers, it is of interest to study
the forces governing their economics. Here we propose a centrality measure akin
to eigenvector centrality to describe an individual's perceived importance in
an online social network. It is shown in silico that strategically maximizing
this prestige metric results in finite nontrivial rates of &quot;like&quot; endowment.
Furthermore, it is found that systems of reputation-seeking agents are
supported most robustly by networks with the features of real human societies
including preferential attachment and the small-world property. We conclude
that the incentive system studied here can produce realistic behavior and may
therefore provide a framework for a more general model of decision-making in
online communities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3917</identifier>
 <datestamp>2014-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3917</id><created>2014-09-13</created><updated>2014-09-15</updated><authors><author><keyname>He</keyname><forenames>Zhe</forenames></author><author><keyname>Xu</keyname><forenames>Rui-Jie</forenames></author><author><keyname>Liu</keyname><forenames>Si-Wu</forenames></author><author><keyname>Wang</keyname><forenames>Bing-Hong</forenames></author></authors><title>The upper bound of packet transmission capacity in local static routings</title><categories>cs.NI cond-mat.stat-mech</categories><comments>7pages,4figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a universal analysis for static routings on networks and describe
the congestion characteristics by the theory. The relation between average
transmission time and transmission capacity is described by inequality
T0Rc0&lt;=1. For large scale sparse networks, the non-trivial upper bond of
transmission capacity Rc0 is limited by Rc0&lt;=1/&lt; 1/k &gt; in some approximate
conditions. the theoretical results agree with simulations on BA Networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3920</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3920</id><created>2014-09-13</created><authors><author><keyname>Das</keyname><forenames>Anup Kumar</forenames></author></authors><title>The 7 Habits of Highly Effective Research Communicators</title><categories>cs.DL</categories><comments>Book Chapter. In Gautam Maity et. al. (Eds.), Charaibeti: Golden
  Jubilee Commemorative Volume (pp. 356-365). Kolkata, India: Department of
  Library and Information Science, Jadavpur University, 2014. ISBN:
  978-81-929886-0-3</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The emergence of Web 2.0 and simultaneously Library 2.0 platforms has helped
the library and information professionals to outreach to new audiences beyond
their physical boundaries. In a globalized society, information becomes very
useful resource for socio-economic empowerment of marginalized communities,
economic prosperity of common citizens, and knowledge enrichment of liberated
minds. Scholarly information becomes both developmental and functional for
researchers working towards advancement of knowledge. We must recognize a relay
of information flow and information ecology while pursuing scholarly research.
Published scholarly literatures we consult that help us in creation of new
knowledge. Similarly, our published scholarly works should be outreached to
future researchers for regeneration of next dimension of knowledge.
Fortunately, present day research communicators have many freely available
personalized digital tools to outreach to globalized research audiences having
similar research interests. These tools and techniques, already adopted by many
researchers in different subject areas across the world, should be
enthusiastically utilized by LIS researchers in South Asia for global
dissemination of their scholarly research works. This newly found enthusiasm
will soon become integral part of the positive habits and cultural practices of
research communicators in LIS domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3924</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3924</id><created>2014-09-13</created><authors><author><keyname>Wang</keyname><forenames>Yuguang</forenames></author><author><keyname>Cao</keyname><forenames>Feilong</forenames></author><author><keyname>Yuan</keyname><forenames>Yubo</forenames></author></authors><title>A study on effectiveness of extreme learning machine</title><categories>cs.NE cs.LG</categories><journal-ref>Neurocomputing, 74(16):2483--2490, 2011</journal-ref><doi>10.1016/j.neucom.2010.11.030</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Extreme learning machine (ELM), proposed by Huang et al., has been shown a
promising learning algorithm for single-hidden layer feedforward neural
networks (SLFNs). Nevertheless, because of the random choice of input weights
and biases, the ELM algorithm sometimes makes the hidden layer output matrix H
of SLFN not full column rank, which lowers the effectiveness of ELM. This paper
discusses the effectiveness of ELM and proposes an improved algorithm called
EELM that makes a proper selection of the input weights and bias before
calculating the output weights, which ensures the full column rank of H in
theory. This improves to some extend the learning rate (testing accuracy,
prediction accuracy, learning time) and the robustness property of the
networks. The experimental results based on both the benchmark function
approximation and real-world problems including classification and regression
applications show the good performances of EELM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3940</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3940</id><created>2014-09-13</created><authors><author><keyname>Chattopadhyay</keyname><forenames>Arpan</forenames></author><author><keyname>Ghosh</keyname><forenames>Avishek</forenames></author><author><keyname>Rao</keyname><forenames>Akhila S.</forenames></author><author><keyname>Dwivedi</keyname><forenames>Bharat</forenames></author><author><keyname>Anand</keyname><forenames>S. V. R.</forenames></author><author><keyname>Coupechoux</keyname><forenames>Marceau</forenames></author><author><keyname>Kumar</keyname><forenames>Anurag</forenames></author></authors><title>Impromptu Deployment of Wireless Relay Networks: Experiences Along a
  Forest Trail</title><categories>cs.NI</categories><comments>7 pages, accepted in IEEE MASS 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We are motivated by the problem of impromptu or as- you-go deployment of
wireless sensor networks. As an application example, a person, starting from a
sink node, walks along a forest trail, makes link quality measurements (with
the previously placed nodes) at equally spaced locations, and deploys relays at
some of these locations, so as to connect a sensor placed at some a priori
unknown point on the trail with the sink node. In this paper, we report our
experimental experiences with some as-you-go deployment algorithms. Two
algorithms are based on Markov decision process (MDP) formulations; these
require a radio propagation model. We also study purely measurement based
strategies: one heuristic that is motivated by our MDP formulations, one
asymptotically optimal learning algorithm, and one inspired by a popular
heuristic. We extract a statistical model of the propagation along a forest
trail from raw measurement data, implement the algorithms experimentally in the
forest, and compare them. The results provide useful insights regarding the
choice of the deployment algorithm and its parameters, and also demonstrate the
necessity of a proper theoretical formulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3941</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3941</id><created>2014-09-13</created><updated>2014-10-18</updated><authors><author><keyname>Adj&#xe9;</keyname><forenames>Assal&#xe9;</forenames></author><author><keyname>Magron</keyname><forenames>Victor</forenames></author></authors><title>Polynomial Template Generation using Sum-of-Squares Programming</title><categories>cs.LO math.OC</categories><comments>23 pages, 3 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Template abstract domains allow to express more interesting properties than
classical abstract domains. However, template generation is a challenging
problem when one uses template abstract domains for program analysis. In this
paper, we relate template generation with the program properties that we want
to prove. We focus on one-loop programs with nested conditional branches. We
formally define the notion of well-representative template basis with respect
to such programs and a given property. The definition relies on the fact that
template abstract domains produce inductive invariants. We show that these
invariants can be obtained by solving certain systems of functional
inequalities. Then, such systems can be strengthened using a hierarchy of
sum-of-squares (SOS) problems when we consider programs written in polynomial
arithmetic. Each step of the SOS hierarchy can possibly provide a solution
which in turn yields an invariant together with a certificate that the desired
property holds. The interest of this approach is illustrated on nontrivial
program examples in polynomial arithmetic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3942</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3942</id><created>2014-09-13</created><authors><author><keyname>Sharma</keyname><forenames>Richa</forenames></author><author><keyname>Nigam</keyname><forenames>Shweta</forenames></author><author><keyname>Jain</keyname><forenames>Rekha</forenames></author></authors><title>Polarity detection movie reviews in hindi language</title><categories>cs.CL cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays peoples are actively involved in giving comments and reviews on
social networking websites and other websites like shopping websites, news
websites etc. large number of people everyday share their opinion on the web,
results is a large number of user data is collected .users also find it trivial
task to read all the reviews and then reached into the decision. It would be
better if these reviews are classified into some category so that the user
finds it easier to read. Opinion Mining or Sentiment Analysis is a natural
language processing task that mines information from various text forms such as
reviews, news, and blogs and classify them on the basis of their polarity as
positive, negative or neutral. But, from the last few years, user content in
Hindi language is also increasing at a rapid rate on the Web. So it is very
important to perform opinion mining in Hindi language as well. In this paper a
Hindi language opinion mining system is proposed. The system classifies the
reviews as positive, negative and neutral for Hindi language. Negation is also
handled in the proposed system. Experimental results using reviews of movies
show the effectiveness of the system
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3943</identifier>
 <datestamp>2015-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3943</id><created>2014-09-13</created><updated>2015-11-12</updated><authors><author><keyname>Holub</keyname><forenames>&#x160;t&#x11b;p&#xe1;n</forenames></author><author><keyname>Masopust</keyname><forenames>Tom&#xe1;&#x161;</forenames></author><author><keyname>Thomazo</keyname><forenames>Micha&#xeb;l</forenames></author></authors><title>Alternating Towers and Piecewise Testable Separators</title><categories>cs.FL</categories><comments>Update on the state complexity of AFAs under the operation of
  downward closure</comments><msc-class>68Q45, 68Q17, 68Q25, 03D05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two languages are separable by a piecewise testable language if and only if
there exists no infinite tower between them. An infinite tower is an infinite
sequence of strings alternating between the two languages such that every
string is a subsequence (scattered substring) of all the strings that follow.
For regular languages represented by nondeterministic finite automata, the
existence of an infinite tower is decidable in polynomial time. In this paper,
we investigate the complexity of a particular method to compute a piecewise
testable separator. We show that it is closely related to the height of maximal
finite towers, and provide the upper and lower bounds with respect to the size
of the given nondeterministic automata. Specifically, we show that the upper
bound is polynomial with respect to the number of states with the cardinality
of the alphabet in the exponent. Concerning the lower bound, we show that
towers of exponential height with respect to the cardinality of the alphabet
exist. Since these towers mostly turn out to be sequences of prefixes, we also
provide a comparison with towers of prefixes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3947</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3947</id><created>2014-09-13</created><authors><author><keyname>Savinov</keyname><forenames>Alexandr</forenames></author></authors><title>Concept-Oriented Programming: References, Classes and Inheritance
  Revisited</title><categories>cs.SE cs.PL</categories><comments>13 pages, 6 figures, Full version of the paper published in ICSOFT
  2012 conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main goal of concept-oriented programming (COP) is describing how objects
are represented and accessed. It makes references (object locations)
first-class elements of the program responsible for many important functions
which are difficult to model via objects. COP rethinks and generalizes such
primary notions of object-orientation as class and inheritance by introducing a
novel construct, concept, and a new relation, inclusion. An advantage is that
using only a few basic notions we are able to describe many general patterns of
thoughts currently belonging to different programming paradigms: modeling
object hierarchies (prototype-based program-ming), precedence of parent methods
over child methods (inner methods in Beta), modularizing cross-cutting
con-cerns (aspect-oriented programming), value-orientation (functional
programming). Since COP remains backward compatible with object-oriented
programming, it can be viewed as a perspective direction for developing a
simple and natural unified programming model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3954</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3954</id><created>2014-09-13</created><authors><author><keyname>Sun</keyname><forenames>Shunqiao</forenames></author><author><keyname>Bajwa</keyname><forenames>Waheed U.</forenames></author><author><keyname>Petropulu</keyname><forenames>Athina P.</forenames></author></authors><title>MIMO-MC Radar: A MIMO Radar Approach Based on Matrix Completion</title><categories>cs.IT math.IT stat.AP</categories><comments>29 pages, 13 figures, IEEE Trans. on Aerospace and Electronic Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a typical MIMO radar scenario, transmit nodes transmit orthogonal
waveforms, while each receive node performs matched filtering with the known
set of transmit waveforms, and forwards the results to the fusion center. Based
on the data it receives from multiple antennas, the fusion center formulates a
matrix, which, in conjunction with standard array processing schemes, such as
MUSIC, leads to target detection and parameter estimation. In MIMO radars with
compressive sensing (MIMO-CS), the data matrix is formulated by each receive
node forwarding a small number of compressively obtained samples. In this
paper, it is shown that under certain conditions, in both sampling cases, the
data matrix at the fusion center is low-rank, and thus can be recovered based
on knowledge of a small subset of its entries via matrix completion (MC)
techniques. Leveraging the low-rank property of that matrix, we propose a new
MIMO radar approach, termed, MIMO-MC radar, in which each receive node either
performs matched filtering with a small number of randomly selected dictionary
waveforms or obtains sub-Nyquist samples of the received signal at random
sampling instants, and forwards the results to a fusion center. Based on the
received samples, and with knowledge of the sampling scheme, the fusion center
partially fills the data matrix and subsequently applies MC techniques to
estimate the full matrix. MIMO-MC radars share the advantages of the recently
proposed MIMO-CS radars, i.e., high resolution with reduced amounts of data,
but unlike MIMO-CS radars do not require grid discretization. The MIMO-MC radar
concept is illustrated through a linear uniform array configuration, and its
target estimation performance is demonstrated via simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3959</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3959</id><created>2014-09-13</created><updated>2015-05-11</updated><authors><author><keyname>Lois</keyname><forenames>Brian</forenames></author><author><keyname>Vaswani</keyname><forenames>Namrata</forenames></author></authors><title>A Correctness Result for Online Robust PCA</title><categories>cs.IT math.IT</categories><comments>A shorter version of this appears in the Proceedings of ICASSP 2015.
  Please read arXiv:1409.3959 (Online Matrix Completion and Online Robust PCA,
  Proc. of ISIT 2015 and submitted to IEEE Trans. Info. Th.) for a strictly
  improved correctness result (it provides a performance guarantee for a more
  practical ReProCS algorithm under almost the same conditions as this work)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work studies the problem of sequentially recovering a sparse vector
$x_t$ and a vector from a low-dimensional subspace $l_t$ from knowledge of
their sum $m_t = x_t + l_t$. If the primary goal is to recover the
low-dimensional subspace where the $l_t$'s lie, then the problem is one of
online or recursive robust principal components analysis (PCA). To the best of
our knowledge, this is the first correctness result for online robust PCA. We
prove that if the $l_t$'s obey certain denseness and slow subspace change
assumptions, and the support of $x_t$ changes by at least a certain amount at
least every so often, and some other mild assumptions hold, then with high
probability, the support of $x_t$ will be recovered exactly, and the error made
in estimating $x_t$ and $l_t$ will be small. An example of where such a problem
might arise is in separating a sparse foreground and slowly changing dense
background in a surveillance video.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3960</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3960</id><created>2014-09-13</created><authors><author><keyname>Cao</keyname><forenames>Xianghui</forenames></author><author><keyname>Liu</keyname><forenames>Lu</forenames></author><author><keyname>Shen</keyname><forenames>Wenlong</forenames></author><author><keyname>Tang</keyname><forenames>Jin</forenames></author><author><keyname>Cheng</keyname><forenames>Yu</forenames></author></authors><title>Real-Time Misbehavior Detection in IEEE 802.11e Based WLANs</title><categories>cs.NI</categories><comments>Accepted to IEEE Globecom 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Enhanced Distributed Channel Access (EDCA) specification in the IEEE
802.11e standard supports heterogeneous backoff parameters and arbitration
inter-frame space (AIFS), which makes a selfish node easy to manipulate these
parameters and misbehave. In this case, the network-wide fairness cannot be
achieved any longer. Many existing misbehavior detectors, primarily designed
for legacy IEEE 802.11 networks, become inapplicable in such a heterogeneous
network configuration. In this paper, we propose a novel real-time hybrid-share
(HS) misbehavior detector for IEEE 802.11e based wireless local area networks
(WLANs). The detector keeps updating its state based on every successful
transmission and makes detection decisions by comparing its state with a
threshold. We develop mathematical analysis of the detector performance in
terms of both false positive rate and average detection rate. Numerical results
show that the proposed detector can effectively detect both contention window
based and AIFS based misbehavior with only a short detection window.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3964</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3964</id><created>2014-09-13</created><updated>2016-02-02</updated><authors><author><keyname>Bazzani</keyname><forenames>Loris</forenames></author><author><keyname>Bergamo</keyname><forenames>Alessandro</forenames></author><author><keyname>Anguelov</keyname><forenames>Dragomir</forenames></author><author><keyname>Torresani</keyname><forenames>Lorenzo</forenames></author></authors><title>Self-taught Object Localization with Deep Networks</title><categories>cs.CV</categories><comments>WACV 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces self-taught object localization, a novel approach that
leverages deep convolutional networks trained for whole-image recognition to
localize objects in images without additional human supervision, i.e., without
using any ground-truth bounding boxes for training. The key idea is to analyze
the change in the recognition scores when artificially masking out different
regions of the image. The masking out of a region that includes the object
typically causes a significant drop in recognition score. This idea is embedded
into an agglomerative clustering technique that generates self-taught
localization hypotheses. Our object localization scheme outperforms existing
proposal methods in both precision and recall for small number of subwindow
proposals (e.g., on ILSVRC-2012 it produces a relative gain of 23.4% over the
state-of-the-art for top-1 hypothesis). Furthermore, our experiments show that
the annotations automatically-generated by our method can be used to train
object detectors yielding recognition results remarkably close to those
obtained by training on manually-annotated bounding boxes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3970</identifier>
 <datestamp>2016-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3970</id><created>2014-09-13</created><updated>2015-12-31</updated><authors><author><keyname>Zheng</keyname><forenames>Yin</forenames></author><author><keyname>Zhang</keyname><forenames>Yu-Jin</forenames></author><author><keyname>Larochelle</keyname><forenames>Hugo</forenames></author></authors><title>A Deep and Autoregressive Approach for Topic Modeling of Multimodal Data</title><categories>cs.CV cs.IR cs.LG cs.NE</categories><comments>24 pages, 10 figures. A version has been accepted by TPAMI on Aug
  4th, 2015. Add footnote about how to train the model in practice in Section
  5.1. arXiv admin note: substantial text overlap with arXiv:1305.5306</comments><doi>10.1109/TPAMI.2015.2476802</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Topic modeling based on latent Dirichlet allocation (LDA) has been a
framework of choice to deal with multimodal data, such as in image annotation
tasks. Another popular approach to model the multimodal data is through deep
neural networks, such as the deep Boltzmann machine (DBM). Recently, a new type
of topic model called the Document Neural Autoregressive Distribution Estimator
(DocNADE) was proposed and demonstrated state-of-the-art performance for text
document modeling. In this work, we show how to successfully apply and extend
this model to multimodal data, such as simultaneous image classification and
annotation. First, we propose SupDocNADE, a supervised extension of DocNADE,
that increases the discriminative power of the learned hidden topic features
and show how to employ it to learn a joint representation from image visual
words, annotation words and class label information. We test our model on the
LabelMe and UIUC-Sports data sets and show that it compares favorably to other
topic models. Second, we propose a deep extension of our model and provide an
efficient way of training the deep model. Experimental results show that our
deep model outperforms its shallow version and reaches state-of-the-art
performance on the Multimedia Information Retrieval (MIR) Flickr data set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.3993</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.3993</id><created>2014-09-13</created><authors><author><keyname>Jain</keyname><forenames>Rishabh</forenames></author><author><keyname>Dutta</keyname><forenames>Rupanta Rwiteej</forenames></author><author><keyname>Tandon</keyname><forenames>Rajat</forenames></author></authors><title>Clear, Concise and Effective UI: Opinion and Suggestions</title><categories>cs.HC</categories><msc-class>00-01</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The most important aspect of any Software is the operability for the intended
audience. This factor of operability is encompassed in the user interface,
which serves as the only window to the features of the system. It is thus
essential that the User Interface provided is robust, concise and lucid.
Presently there are no properly defined rules or guidelines for user interface
design enabling a perfect design, since such a system cannot be perceived. This
article aims at providing suggestions in the design of the User Interface,
which would make it easier for the user to navigate through the system features
and also the developers to guide the users towards better utilization of the
features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4010</identifier>
 <datestamp>2015-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4010</id><created>2014-09-14</created><updated>2015-05-14</updated><authors><author><keyname>Sun</keyname><forenames>Jiajun</forenames></author></authors><title>An Incentive Mechanism for Periodical Mobile Crowdsensing from a
  Frugality Perspective</title><categories>cs.GT</categories><comments>This paper has been withdrawn by the author due to a crucial sign
  error in Section 3</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile crowdsensing (MCS) has been intensively explored recently due to its
flexible and pervasive sensing ability. Although many incentive mechanisms have
been built to attract extensive user participation, Most of these mechanisms
focus only on independent task scenarios, where the sensing tasks are
independent of each other. On the contrary, we focus on a periodical task
scenario, where each user participates in the same type of sensing tasks
periodically. In this paper, we consider the long-term user participation
incentive in a general periodical MCS system from a frugality payment
perspective. We explore the issue under both semi-online (the intra-period
interactive process is synchronous while the inter-period interactive process
is sequential and asynchronous during each period) and online user arrival
models (the previous two interactive processes are sequential and
asynchronous). In particular, we first propose a semi-online frugal incentive
mechanism by introducing a Lyapunov method. Moreover, we also extend it to an
online frugal incentive mechanism, which satisfies the constant frugality.
Besides, the two mechanisms can also satisfy computational efficiency,
asymptotical optimality, individual rationality and truthfulness. Through
extensive simulations, we evaluate the performance and validate the theoretical
properties of our online mechanisms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4014</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4014</id><created>2014-09-14</created><authors><author><keyname>Wang</keyname><forenames>Pichao</forenames></author><author><keyname>Li</keyname><forenames>Wanqing</forenames></author><author><keyname>Ogunbona</keyname><forenames>Philip</forenames></author><author><keyname>Gao</keyname><forenames>Zhimin</forenames></author><author><keyname>Zhang</keyname><forenames>Hanling</forenames></author></authors><title>Mining Mid-level Features for Action Recognition Based on Effective
  Skeleton Representation</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, mid-level features have shown promising performance in computer
vision. Mid-level features learned by incorporating class-level information are
potentially more discriminative than traditional low-level local features. In
this paper, an effective method is proposed to extract mid-level features from
Kinect skeletons for 3D human action recognition. Firstly, the orientations of
limbs connected by two skeleton joints are computed and each orientation is
encoded into one of the 27 states indicating the spatial relationship of the
joints. Secondly, limbs are combined into parts and the limb's states are
mapped into part states. Finally, frequent pattern mining is employed to mine
the most frequent and relevant (discriminative, representative and
non-redundant) states of parts in continuous several frames. These parts are
referred to as Frequent Local Parts or FLPs. The FLPs allow us to build
powerful bag-of-FLP-based action representation. This new representation yields
state-of-the-art results on MSR DailyActivity3D and MSR ActionPairs3D.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4016</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4016</id><created>2014-09-14</created><updated>2014-10-25</updated><authors><author><keyname>Abdulla</keyname><forenames>Mouhamed</forenames></author></authors><title>Simple Subroutine for Inhomogeneous Deployment</title><categories>cs.IT cs.NI math.IT</categories><comments>Proc. of the 6th IEEE Global Information Infrastructure and
  Networking Symposium (GIIS'14)</comments><doi>10.1109/GIIS.2014.6934284</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spatial modeling of wireless networks via analytical means has been
considered as a widely practiced mechanism for inference. As a result, some
geometrical deployment models have been proposed in literature. Although
practical in certain simulation instances, these models do not always produce
inhomogeneous nodal geometries in an effective and simple manner for practical
deployment situations. Therefore, we conceptualized a flexible approach for
realizing random inhomogeneity by proposing the area-specific deployment (ASD)
algorithm, which takes into account the clustering tendency of users. Overall,
the developed spatial-level network tool has the distinct advantage of
automatically producing infinitely many random realizations of users' geometry
by simply entering three parameters to the simulator: the size of the cellular
network, the number of deployment layers, and the overall quantity of nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4018</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4018</id><created>2014-09-14</created><authors><author><keyname>Hidru</keyname><forenames>Daniel</forenames></author><author><keyname>Goldenberg</keyname><forenames>Anna</forenames></author></authors><title>EquiNMF: Graph Regularized Multiview Nonnegative Matrix Factorization</title><categories>cs.LG cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nonnegative matrix factorization (NMF) methods have proved to be powerful
across a wide range of real-world clustering applications. Integrating multiple
types of measurements for the same objects/subjects allows us to gain a deeper
understanding of the data and refine the clustering. We have developed a novel
Graph-reguarized multiview NMF-based method for data integration called
EquiNMF. The parameters for our method are set in a completely automated
data-specific unsupervised fashion, a highly desirable property in real-world
applications. We performed extensive and comprehensive experiments on multiview
imaging data. We show that EquiNMF consistently outperforms other single-view
NMF methods used on concatenated data and multi-view NMF methods with different
types of regularizations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4033</identifier>
 <datestamp>2015-05-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4033</id><created>2014-09-14</created><updated>2015-05-25</updated><authors><author><keyname>Egan</keyname><forenames>Malcolm</forenames></author><author><keyname>Peters</keyname><forenames>Gareth W.</forenames></author><author><keyname>Nevat</keyname><forenames>Ido</forenames></author><author><keyname>Collings</keyname><forenames>Iain B.</forenames></author></authors><title>A Ruin Theoretic Design Approach for Wireless Cellular Network Sharing
  with Facilities</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the rise of cheap small-cells in wireless cellular networks, there are
new opportunities for third party providers to service local regions via
sharing arrangements with traditional operators. In fact, such arrangements are
highly desirable for large facilities---such as stadiums, universities, and
mines---as they already need to cover property costs, and often have fibre
backhaul and efficient power infrastructure. In this paper, we propose a new
network sharing arrangement between large facilities and traditional operators.
Our facility network sharing arrangement consists of two aspects: leasing of
core network access and spectrum from traditional operators; and service
agreements with users. Importantly, our incorporation of a user service
agreement into the arrangement means that resource allocation must account for
financial as well as physical resource constraints. This introduces a new
non-trivial dimension into wireless network resource allocation, which requires
a new evaluation framework---the data rate is no longer the only main
performance metric. Moreover, despite clear economic incentives to adopt
network sharing for facilities, a business case is lacking. As such, we develop
a general socio-technical evaluation framework based on ruin-theory, where the
key metric for the sharing arrangement is the probability that the facility has
less than zero revenue surplus. We then use our framework to evaluate our
facility network sharing arrangement, which offers guidance for leasing and
service agreement negotiations, as well as design of the wireless network
architecture, taking into account network revenue streams.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4035</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4035</id><created>2014-09-14</created><authors><author><keyname>Shraibman</keyname><forenames>Adi</forenames></author></authors><title>The Corruption Bound, Log Rank, and Communication Complexity</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that for every sign matrix $A$ there is a deterministic
communication protocol that uses $O(corr_{1/4}(A)\log^2 rk(A))$ bits of
communication, where $corr_{1/4}(A)$ is the corruption/rectangle bound with
error $1/4$. This bound generalizes several of the known upper bounds on
deterministic communication complexity, involving nondeterministic complexity,
randomized complexity, information complexity notions, and rank.
  It also implies that the corruption bound is a lower bound on exact quantum
communication complexity, if and only if quantum communication is polynomially
equivalent to deterministic communication complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4043</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4043</id><created>2014-09-14</created><authors><author><keyname>Hanumantharaju</keyname><forenames>M. C.</forenames></author><author><keyname>Ravishankar</keyname><forenames>M.</forenames></author><author><keyname>Rameshbabu</keyname><forenames>D. R.</forenames></author></authors><title>Design of Novel Algorithm and Architecture for Gaussian Based Color
  Image Enhancement System for Real Time Applications</title><categories>cs.AR cs.CV</categories><comments>15 pages, 15 figures</comments><doi>10.1007/978-3-642-36321-4_56</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the development of a new algorithm for Gaussian based
color image enhancement system. The algorithm has been designed into
architecture suitable for FPGA/ASIC implementation. The color image enhancement
is achieved by first convolving an original image with a Gaussian kernel since
Gaussian distribution is a point spread function which smoothen the image.
Further, logarithm-domain processing and gain/offset corrections are employed
in order to enhance and translate pixels into the display range of 0 to 255.
The proposed algorithm not only provides better dynamic range compression and
color rendition effect but also achieves color constancy in an image. The
design exploits high degrees of pipelining and parallel processing to achieve
real time performance. The design has been realized by RTL compliant Verilog
coding and fits into a single FPGA with a gate count utilization of 321,804.
The proposed method is implemented using Xilinx Virtex-II Pro XC2VP40-7FF1148
FPGA device and is capable of processing high resolution color motion pictures
of sizes of up to 1600x1200 pixels at the real time video rate of 116 frames
per second. This shows that the proposed design would work for not only still
images but also for high resolution video sequences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4044</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4044</id><created>2014-09-14</created><authors><author><keyname>Tapp</keyname><forenames>Alain</forenames></author></authors><title>A new approach in machine learning</title><categories>stat.ML cs.LG</categories><comments>Preliminary report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this technical report we presented a novel approach to machine learning.
Once the new framework is presented, we will provide a simple and yet very
powerful learning algorithm which will be benchmark on various dataset.
  The framework we proposed is based on booleen circuits; more specifically the
classifier produced by our algorithm have that form. Using bits and boolean
gates instead of real numbers and multiplication enable the the learning
algorithm and classifier to use very efficient boolean vector operations. This
enable both the learning algorithm and classifier to be extremely efficient.
The accuracy of the classifier we obtain with our framework compares very
favorably those produced by conventional techniques, both in terms of
efficiency and accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4046</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4046</id><created>2014-09-14</created><authors><author><keyname>Hanumantharaju</keyname><forenames>M. C</forenames></author><author><keyname>Ravishankar</keyname><forenames>M.</forenames></author><author><keyname>Rameshbabu</keyname><forenames>D. R</forenames></author><author><keyname>Aradhya</keyname><forenames>V. N Manjunath</forenames></author></authors><title>A New Framework for Retinex based Color Image Enhancement using Particle
  Swarm Optimization</title><categories>cs.CV</categories><comments>24 pages, 12 figures</comments><msc-class>68T45</msc-class><acm-class>H.2.0</acm-class><doi>10.1504/IJSI.2014.060241</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new approach for tuning the parameters of MultiScale Retinex (MSR) based
color image enhancement algorithm using a popular optimization method, namely,
Particle Swarm Optimization (PSO) is presented in this paper. The image
enhancement using MSR scheme heavily depends on parameters such as Gaussian
surround space constant, number of scales, gain and offset etc. Selection of
these parameters, empirically and its application to MSR scheme to produce
inevitable results are the major blemishes. The method presented here results
in huge savings of computation time as well as improvement in the visual
quality of an image, since the PSO exploited maximizes the MSR parameters. The
objective of PSO is to validate the visual quality of the enhanced image
iteratively using an effective objective criterion based on entropy and edge
information of an image. The PSO method of parameter optimization of MSR scheme
achieves a very good quality of reconstructed images, far better than that
possible with the other existing methods. Finally, the quality of the enhanced
color images obtained by the proposed method are evaluated using novel metric,
namely, Wavelet Energy (WE). The experimental results presented show that color
images enhanced using the proposed scheme are clearer, more vivid and
efficient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4063</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4063</id><created>2014-09-14</created><updated>2014-11-22</updated><authors><author><keyname>Costa</keyname><forenames>Alberto</forenames></author></authors><title>Some remarks on modularity density</title><categories>cs.SI physics.soc-ph</categories><comments>4 pages, 4 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A &quot;quantitative function&quot; for community detection called modularity density
has been proposed by Li, Zhang, Wang, Zhang, and Chen in $[$Phys. Rev. E 77,
036109 (2008)$]$. We study the modularity density maximization problem and we
discuss some features of the optimal solution. More precisely, we show that in
the optimal solution there can be communities having negative modularity
density, and we propose a modification of the original formulation to overcome
this issue. Moreover, we show that a clique can be divided into two or more
parts when maximizing the modularity density. We also compare the solution
found by maximizing the modularity density with that obtained by maximizing the
modularity on the Zachary karate club network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4064</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4064</id><created>2014-09-14</created><authors><author><keyname>Tu</keyname><forenames>Wenwen</forenames></author><author><keyname>Lai</keyname><forenames>Lifeng</forenames></author></authors><title>On the Simulatability Condition in Key Generation Over a
  Non-authenticated Public Channel</title><categories>cs.CR cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Simulatability condition is a fundamental concept in studying key generation
over a non-authenticated public channel, in which Eve is active and can
intercept, modify and falsify messages exchanged over the non-authenticated
public channel. Using this condition, Maurer and Wolf showed a remarkable &quot;all
or nothing&quot; result: if the simulatability condition does not hold, the key
capacity over the non-authenticated public channel will be the same as that of
the case with a passive Eve, while the key capacity over the non-authenticated
channel will be zero if the simulatability condition holds. However, two
questions remain open so far: 1) For a given joint probability mass function
(PMF), are there efficient algorithms (polynomial complexity algorithms) for
checking whether the simulatability condition holds or not?; and 2) If the
simulatability condition holds, are there efficient algorithms for finding the
corresponding attack strategy? In this paper, we answer these two open
questions affirmatively. In particular, for a given joint PMF, we construct a
linear programming (LP) problem and show that the simulatability condition
holds \textit{if and only if} the optimal value obtained from the constructed
LP is zero. Furthermore, we construct another LP and show that the minimizer of
the newly constructed LP is a valid attack strategy. Both LPs can be solved
with a polynomial complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4078</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4078</id><created>2014-09-14</created><authors><author><keyname>Burshteyn</keyname><forenames>Boris</forenames></author></authors><title>The distributed Language Hello White Paper</title><categories>cs.PL cs.DC</categories><comments>10 pages</comments><acm-class>D.3.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hello is a general-purpose, object-oriented, protocol-agnostic distributed
programming language. This paper explains the ideas that guided design of
Hello. It shows the spirit of Hello using two brief expressive programs and
provides a summary of language features. In addition, it explores historical
parallels between the binary programming of early computers and the distributed
programming of modern networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4080</identifier>
 <datestamp>2015-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4080</id><created>2014-09-14</created><updated>2015-02-19</updated><authors><author><keyname>Gauvrit</keyname><forenames>Nicolas</forenames></author><author><keyname>Singmann</keyname><forenames>Henrik</forenames></author><author><keyname>Soler-Toscano</keyname><forenames>Fernando</forenames></author><author><keyname>Zenil</keyname><forenames>Hector</forenames></author></authors><title>Algorithmic complexity for psychology: A user-friendly implementation of
  the coding theorem method</title><categories>cs.CC stat.AP</categories><comments>to appear in &quot;Behavioral Research Methods&quot;, 14 pages in journal
  format, R package at http://cran.r-project.org/web/packages/acss/index.html</comments><msc-class>91E45</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Kolmogorov-Chaitin complexity has long been believed to be impossible to
approximate when it comes to short sequences (e.g. of length 5-50). However,
with the newly developed \emph{coding theorem method} the complexity of strings
of length 2-11 can now be numerically estimated. We present the theoretical
basis of algorithmic complexity for short strings (ACSS) and describe an
R-package providing functions based on ACSS that will cover psychologists'
needs and improve upon previous methods in three ways: (1) ACSS is now
available not only for binary strings, but for strings based on up to 9
different symbols, (2) ACSS no longer requires time-consuming computing, and
(3) a new approach based on ACSS gives access to an estimation of the
complexity of strings of any length. Finally, three illustrative examples show
how these tools can be applied to psychology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4081</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4081</id><created>2014-09-14</created><authors><author><keyname>Knecht</keyname><forenames>Markus</forenames></author><author><keyname>Meier</keyname><forenames>Willi</forenames></author><author><keyname>Nicola</keyname><forenames>Carlo U.</forenames></author></authors><title>A space- and time-efficient Implementation of the Merkle Tree Traversal
  Algorithm</title><categories>cs.CR cs.DS</categories><comments>19 pages, 6 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an algorithm for the Merkle tree traversal problem which combines
the efficient space-time trade-off from the fractal Merkle tree [3] and the
space efficiency from the improved log space-time Merkle trees traversal [8].
We give an exhaustive analysis of the space and time efficiency of our
algorithm in function of the parameters H (the height of the Merkle tree) and h
(h = H L where L is the number of levels in the Merkle tree). We also analyze
the space impact when a continuous deterministic pseudo-random number generator
(PRNG) is used to generate the leaves. We further program a low storage-space
and a low time-overhead version of the algorithm in Java and measure its
performance with respect to the two different implementations cited above. Our
implementation uses the least space when a continuous PRNG is used for the leaf
calculation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4082</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4082</id><created>2014-09-14</created><authors><author><keyname>Pluzhnik</keyname><forenames>Evgeniy</forenames></author><author><keyname>Nikulchev</keyname><forenames>Evgeny</forenames></author></authors><title>Virtual Laboratories in Cloud Infrastructure of Educational Institutions</title><categories>cs.DC</categories><comments>3 pages, Published in: 2014 2nd International Conference on Emission
  Electronics (ICEE), Saint-Petersburg, Russia</comments><doi>10.1109/Emission.2014.6893974</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern educational institutions widely used virtual laboratories and cloud
technologies. In practice must deal with security, processing speed and other
tasks. The paper describes the experience of the construction of an
experimental stand cloud computing and network management. Models and control
principles set forth herein.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4083</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4083</id><created>2014-09-14</created><authors><author><keyname>Nikulchev</keyname><forenames>Evgeny</forenames></author></authors><title>Robust Chaos Generation on the Basis of Symmetry Violations in
  Attractors</title><categories>cs.SY</categories><comments>3 pages, 2014 2nd International Conference on Emission Electronics
  (ICEE), Saint-Petersburg, Russia</comments><doi>10.1109/Emission.2014.6893972</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a method for generating robust chaos. It is based on the search
algorithm weak symmetry violation in the reconstructed attractor. On its basis
the smooth functions in the form of a system of finite-difference equations. To
ensure robust chaos generator introduced piecewise continuous member. The
simulation results are given in the report.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4092</identifier>
 <datestamp>2014-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4092</id><created>2014-09-14</created><authors><author><keyname>Bhattacharya</keyname><forenames>Binay K.</forenames></author><author><keyname>De</keyname><forenames>Minati</forenames></author><author><keyname>Nandy</keyname><forenames>Subhas C.</forenames></author><author><keyname>Roy</keyname><forenames>Sasanka</forenames></author></authors><title>Facility location problems in the constant work-space read-only memory
  model</title><categories>cs.DS cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Facility location problems are captivating both from theoretical and
practical point of view. In this paper, we study some fundamental facility
location problems from the space-efficient perspective. Here the input is
considered to be given in a read-only memory and only constant amount of
work-space is available during the computation. This {\em constant-work-space
model} is well-motivated for handling big-data as well as for computing in
smart portable devices with small amount of extra-space.
  First, we propose a strategy to implement prune-and-search in this model. As
a warm up, we illustrate this technique for finding the Euclidean 1-center
constrained on a line for a set of points in $\IR^2$. This method works even if
the input is given in a sequential access read-only memory. Using this we show
how to compute (i) the Euclidean 1-center of a set of points in $\IR^2$, and
(ii) the weighted 1-center and weighted 2-center of a tree network. The running
time of all these algorithms are $O(n~poly(\log n))$. While the result of (i)
gives a positive answer to an open question asked by Asano, Mulzer, Rote and
Wang in 2011, the technique used can be applied to other problems which admit
solutions by prune-and-search paradigm. For example, we can apply the technique
to solve two and three dimensional linear programming in $O(n~poly(\log n))$
time in this model. To the best of our knowledge, these are the first
sub-quadratic time algorithms for all the above mentioned problems in the
constant-work-space model. We also present optimal linear time algorithms for
finding the centroid and weighted median of a tree in this model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4095</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4095</id><created>2014-09-14</created><authors><author><keyname>Balzer</keyname><forenames>Jonathan</forenames></author><author><keyname>Acevedo-Feliz</keyname><forenames>Daniel</forenames></author><author><keyname>Soatto</keyname><forenames>Stefano</forenames></author><author><keyname>H&#xf6;fer</keyname><forenames>Sebastian</forenames></author><author><keyname>Hadwiger</keyname><forenames>Markus</forenames></author><author><keyname>Beyerer</keyname><forenames>J&#xfc;rgen</forenames></author></authors><title>Cavlectometry: Towards Holistic Reconstruction of Large Mirror Objects</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a method based on the deflectometry principle for the
reconstruction of specular objects exhibiting significant size and geometric
complexity. A key feature of our approach is the deployment of an Automatic
Virtual Environment (CAVE) as pattern generator. To unfold the full power of
this extraordinary experimental setup, an optical encoding scheme is developed
which accounts for the distinctive topology of the CAVE. Furthermore, we devise
an algorithm for detecting the object of interest in raw deflectometric images.
The segmented foreground is used for single-view reconstruction, the background
for estimation of the camera pose, necessary for calibrating the sensor system.
Experiments suggest a significant gain of coverage in single measurements
compared to previous methods. To facilitate research on specular surface
reconstruction, we will make our data set publicly available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4097</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4097</id><created>2014-09-14</created><authors><author><keyname>Ning</keyname><forenames>Lipeng</forenames></author><author><keyname>Georgiou</keyname><forenames>Tryphon T.</forenames></author></authors><title>Metrics for matrix-valued measures via test functions</title><categories>cs.SY</categories><msc-class>46F05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is perhaps not widely recognized that certain common notions of distance
between probability measures have an alternative dual interpretation which
compares corresponding functionals against suitable families of test functions.
This dual viewpoint extends in a straightforward manner to suggest metrics
between matrix-valued measures. Our main interest has been in developing
weakly-continuous metrics that are suitable for comparing matrix-valued power
spectral density functions. To this end, and following the suggested recipe of
utilizing suitable families of test functions, we develop a weakly-continuous
metric that is analogous to the Wasserstein metric and applies to matrix-valued
densities. We use a numerical example to compare this metric to certain
standard alternatives including a different version of a matricial Wasserstein
metric developed earlier.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4102</identifier>
 <datestamp>2015-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4102</id><created>2014-09-14</created><updated>2015-10-09</updated><authors><author><keyname>Suhov</keyname><forenames>Yuri</forenames></author><author><keyname>Sekeh</keyname><forenames>Salimeh Yasaei</forenames></author></authors><title>Simple inequalities for weighted entropies</title><categories>cs.IT math.IT math.PR</categories><comments>19 pages The paper is wihdrawn as it needs improvement</comments><msc-class>60A10, 60B05, 60C05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A number of inequalities for the weighted entropies is proposed, mirroring
properties of a standard (Shannon) entropy and related quantities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4127</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4127</id><created>2014-09-14</created><updated>2015-06-15</updated><authors><author><keyname>Su</keyname><forenames>Yu-Chuan</forenames></author><author><keyname>Chiu</keyname><forenames>Tzu-Hsuan</forenames></author><author><keyname>Yeh</keyname><forenames>Chun-Yen</forenames></author><author><keyname>Huang</keyname><forenames>Hsin-Fu</forenames></author><author><keyname>Hsu</keyname><forenames>Winston H.</forenames></author></authors><title>Transfer Learning for Video Recognition with Scarce Training Data for
  Deep Convolutional Neural Network</title><categories>cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unconstrained video recognition and Deep Convolution Network (DCN) are two
active topics in computer vision recently. In this work, we apply DCNs as
frame-based recognizers for video recognition. Our preliminary studies,
however, show that video corpora with complete ground truth are usually not
large and diverse enough to learn a robust model. The networks trained directly
on the video data set suffer from significant overfitting and have poor
recognition rate on the test set. The same lack-of-training-sample problem
limits the usage of deep models on a wide range of computer vision problems
where obtaining training data are difficult. To overcome the problem, we
perform transfer learning from images to videos to utilize the knowledge in the
weakly labeled image corpus for video recognition. The image corpus help to
learn important visual patterns for natural images, while these patterns are
ignored by models trained only on the video corpus. Therefore, the resultant
networks have better generalizability and better recognition rate. We show that
by means of transfer learning from image to video, we can learn a frame-based
recognizer with only 4k videos. Because the image corpus is weakly labeled, the
entire learning process requires only 4k annotated instances, which is far less
than the million scale image data sets required by previous works. The same
approach may be applied to other visual recognition tasks where only scarce
training data is available, and it improves the applicability of DCNs in
various computer vision problems. Our experiments also reveal the correlation
between meta-parameters and the performance of DCNs, given the properties of
the target problem and data. These results lead to a heuristic for
meta-parameter selection for future researches, which does not rely on the time
consuming meta-parameter search.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4132</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4132</id><created>2014-09-14</created><authors><author><keyname>Elkind</keyname><forenames>Edith</forenames></author><author><keyname>Markakis</keyname><forenames>Evangelos</forenames></author><author><keyname>Obraztsova</keyname><forenames>Svetlana</forenames></author><author><keyname>Skowron</keyname><forenames>Piotr</forenames></author></authors><title>Equilibria of Plurality Voting: Lazy and Truth-biased Voters</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a systematic study of Plurality elections with strategic voters
who, in addition to having preferences over election winners, have secondary
preferences, which govern their behavior when their vote cannot affect the
election outcome. Specifically, we study two models that have been recently
considered in the literature: lazy voters, who prefer to abstain when they are
not pivotal, and truth-biased voters, who prefer to vote truthfully when they
are not pivotal. We extend prior work by investigating the behavior of both
lazy and truth-biased voters under different tie-breaking rules (lexicographic
rule, random voter rule, random candidate rule). Two of these six combinations
of secondary preferences and a tie-breaking rule have been studied in prior
work. In order to understand the impact of different secondary preferences and
tie-breaking rules on the election outcomes, we study the remaining four
combinations. We characterize pure Nash equilibria (PNE) of the resulting
strategic games and study the complexity of related computational problems. Our
results extend to settings where some of the voters may be non-strategic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4139</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4139</id><created>2014-09-14</created><authors><author><keyname>Zhao</keyname><forenames>Liang</forenames></author><author><keyname>Xuan</keyname><forenames>Jianhua</forenames></author><author><keyname>Wang</keyname><forenames>Yue</forenames></author></authors><title>A feasible roadmap for developing volumetric probability atlas of
  localized prostate cancer</title><categories>q-bio.QM cs.CV</categories><comments>13 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A statistical volumetric model, showing the probability map of localized
prostate cancer within the host anatomical structure, has been developed from
90 optically-imaged surgical specimens. This master model permits an accurate
characterization of prostate cancer distribution patterns and an atlas-informed
biopsy sampling strategy. The model is constructed by mapping individual
prostate models onto a site model, together with localized tumors. An accurate
multi-object non-rigid warping scheme is developed based on a mixture of
principal-axis registrations. We report our evaluation and pilot studies on the
effectiveness of the method and its application to optimizing needle biopsy
strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4149</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4149</id><created>2014-09-14</created><authors><author><keyname>Hawilo</keyname><forenames>Hassan</forenames></author><author><keyname>Shami</keyname><forenames>Abdallah</forenames></author><author><keyname>Mirahmadi</keyname><forenames>Maysam</forenames></author><author><keyname>Asal</keyname><forenames>Rasool</forenames></author></authors><title>NFV: State of the Art, Challenges and Implementation in Next Generation
  Mobile Networks (vEPC)</title><categories>cs.NI</categories><comments>To appear in IEEE Network Mag., November 2014 Issue</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As mobile network users look forward to the connectivity speeds of 5G
networks, service providers are facing challenges in complying with
connectivity demands without substantial financial investments. Network
Function Virtualization (NFV) is introduced as a new methodology that offers a
way out of this bottleneck. NFV is poised to change the core structure of
telecommunications infrastructure to be more cost-efficient. In this paper, we
introduce a Network Function Virtualization framework, and discuss the
challenges and requirements of its use in mobile networks. In particular, an
NFV framework in the virtual environment is proposed. Moreover, in order to
reduce signaling traffic and achieve better performance, this paper proposes a
criterion to bundle multiple functions of virtualized evolved packet-core in a
single physical device or a group of adjacent devices. The analysis shows that
the proposed grouping can reduce the network control traffic by 70 percent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4150</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4150</id><created>2014-09-14</created><authors><author><keyname>Daskalakis</keyname><forenames>Constantinos</forenames></author><author><keyname>Deckelbaum</keyname><forenames>Alan</forenames></author><author><keyname>Tzamos</keyname><forenames>Christos</forenames></author></authors><title>Strong Duality for a Multiple-Good Monopolist</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a duality-based framework for revenue maximization in a
multiple-good monopoly. Our framework shows that every optimal mechanism has a
certificate of optimality that takes the form of an optimal transportation map
between measures. Our framework improves previous partial results, by
establishing a strong duality theorem between optimal mechanism design and
optimal transportation, and is enabled by an extension of the Monge-Kantorovich
duality that accommodates convexity constraints. Using our framework, we prove
that grand-bundling mechanisms are optimal if and only if two stochastic
dominance conditions hold between specific measures induced by the type
distribution. This result strengthens several results in the literature, where
only sufficient conditions have been provided. As a corollary of our tight
characterization of bundling optimality, we show that the optimal mechanism for
$n$ independent uniform items each supported on $[c,c+1]$ is a grand bundling
mechanism, as long as $c$ is sufficiently large, extending Pavlov's result for
2 items [Pavlov11]. Finally, we provide a partial characterization of general
two-item mechanisms, and use our characterization to obtain optimal mechanisms
in several settings, including an example with two independently distributed
items where a continuum of lotteries is necessary for revenue maximization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4155</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4155</id><created>2014-09-15</created><authors><author><keyname>Xiong</keyname><forenames>Sicheng</forenames></author><author><keyname>Rosales</keyname><forenames>R&#xf3;mer</forenames></author><author><keyname>Pei</keyname><forenames>Yuanli</forenames></author><author><keyname>Fern</keyname><forenames>Xiaoli Z.</forenames></author></authors><title>Active Metric Learning from Relative Comparisons</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work focuses on active learning of distance metrics from relative
comparison information. A relative comparison specifies, for a data point
triplet $(x_i,x_j,x_k)$, that instance $x_i$ is more similar to $x_j$ than to
$x_k$. Such constraints, when available, have been shown to be useful toward
defining appropriate distance metrics. In real-world applications, acquiring
constraints often require considerable human effort. This motivates us to study
how to select and query the most useful relative comparisons to achieve
effective metric learning with minimum user effort. Given an underlying class
concept that is employed by the user to provide such constraints, we present an
information-theoretic criterion that selects the triplet whose answer leads to
the highest expected gain in information about the classes of a set of
examples. Directly applying the proposed criterion requires examining $O(n^3)$
triplets with $n$ instances, which is prohibitive even for datasets of moderate
size. We show that a randomized selection strategy can be used to reduce the
selection pool from $O(n^3)$ to $O(n)$, allowing us to scale up to larger-size
problems. Experiments show that the proposed method consistently outperforms
two baseline policies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4159</identifier>
 <datestamp>2014-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4159</id><created>2014-09-15</created><updated>2014-09-23</updated><authors><author><keyname>Chiriyath</keyname><forenames>Alex R.</forenames></author><author><keyname>Paul</keyname><forenames>Bryan</forenames></author><author><keyname>Bliss</keyname><forenames>Daniel W.</forenames></author></authors><title>Joint Radar-Communications Performance Inner Bounds: Data versus
  Estimation Information Rates</title><categories>cs.IT math.IT</categories><comments>This paper has been withdrawn by the author due to some changes that
  have to be made</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate cooperative radar and communications signaling. Each system
typically considers the other system a source of interference. Consequently,
the traditional solution is to isolate the two systems spectrally or spatially.
By considering the radar and communications operations to be a single joint
system, we derive inner or achievable performance bounds on a receiver that
observes communications and radar return in the same frequency allocation.
Bounds on performance of the joint system are measured in terms of data
information rate for communications and a novel radar estimation information
rate parameterization for the radar.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4161</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4161</id><created>2014-09-15</created><authors><author><keyname>Asudeh</keyname><forenames>Abolfazl</forenames></author><author><keyname>Zhang</keyname><forenames>Gensheng</forenames></author><author><keyname>Hassan</keyname><forenames>Naeemul</forenames></author><author><keyname>Li</keyname><forenames>Chengkai</forenames></author><author><keyname>Zaruba</keyname><forenames>Gergely V.</forenames></author></authors><title>Crowdsourcing Pareto-Optimal Object Finding by Pairwise Comparisons</title><categories>cs.AI cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is the first study on crowdsourcing Pareto-optimal object finding, which
has applications in public opinion collection, group decision making, and
information exploration. Departing from prior studies on crowdsourcing skyline
and ranking queries, it considers the case where objects do not have explicit
attributes and preference relations on objects are strict partial orders. The
partial orders are derived by aggregating crowdsourcers' responses to pairwise
comparison questions. The goal is to find all Pareto-optimal objects by the
fewest possible questions. It employs an iterative question-selection
framework. Guided by the principle of eagerly identifying non-Pareto optimal
objects, the framework only chooses candidate questions which must satisfy
three conditions. This design is both sufficient and efficient, as it is proven
to find a short terminal question sequence. The framework is further steered by
two ideas---macro-ordering and micro-ordering. By different micro-ordering
heuristics, the framework is instantiated into several algorithms with varying
power in pruning questions. Experiment results using both real crowdsourcing
marketplace and simulations exhibited not only orders of magnitude reductions
in questions when compared with a brute-force approach, but also
close-to-optimal performance from the most efficient instantiation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4164</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4164</id><created>2014-09-15</created><authors><author><keyname>Trescak</keyname><forenames>Tomas</forenames></author><author><keyname>Sierra</keyname><forenames>Carles</forenames></author><author><keyname>Simoff</keyname><forenames>Simeon</forenames></author><author><keyname>de Mantaras</keyname><forenames>Ramon Lopez</forenames></author></authors><title>Dispute Resolution Using Argumentation-Based Mediation</title><categories>cs.AI</categories><comments>6 pages</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Mediation is a process, in which both parties agree to resolve their dispute
by negotiating over alternative solutions presented by a mediator. In order to
construct such solutions, mediation brings more information and knowledge, and,
if possible, resources to the negotiation table. The contribution of this paper
is the automated mediation machinery which does that. It presents an
argumentation-based mediation approach that extends the logic-based approach to
argumentation-based negotiation involving BDI agents. The paper describes the
mediation algorithm. For comparison it illustrates the method with a case study
used in an earlier work. It demonstrates how the computational mediator can
deal with realistic situations in which the negotiating agents would otherwise
fail due to lack of knowledge and/or resources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4169</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4169</id><created>2014-09-15</created><authors><author><keyname>N.</keyname><forenames>Rama</forenames></author><author><keyname>Lakshmanan</keyname><forenames>Meenakshi</forenames></author></authors><title>An Algorithm Based on Empirical Methods, for Text-to-Tuneful-Speech
  Synthesis of Sanskrit Verse</title><categories>cs.CL</categories><comments>International Journal of Computer Science and Network Security,
  Vol.10, No. 1, January 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rendering of Sanskrit poetry from text to speech is a problem that has
not been solved before. One reason may be the complications in the language
itself. We present unique algorithms based on extensive empirical analysis, to
synthesize speech from a given text input of Sanskrit verses. Using a
pre-recorded audio units database which is itself tremendously reduced in size
compared to the colossal size that would otherwise be required, the algorithms
work on producing the best possible, tunefully rendered chanting of the given
verse. His would enable the visually impaired and those with reading
disabilities to easily access the contents of Sanskrit verses otherwise
available only in writing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4194</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4194</id><created>2014-09-15</created><authors><author><keyname>Kashfi</keyname><forenames>Pariya</forenames></author><author><keyname>Feldt</keyname><forenames>Robert</forenames></author><author><keyname>Nilsson</keyname><forenames>Agneta</forenames></author><author><keyname>Svensson</keyname><forenames>Richard Berntsson</forenames></author></authors><title>Models for Integrating UX into Software Engineering Practice: an
  Industrial Validation</title><categories>cs.SE cs.HC</categories><comments>8 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The user's overall experience and perception of functionalities and qualities
of a product, User eXperience (UX), is becoming increasingly important for
success of software products. Yet, many software companies face challenges with
their UX practices, hence fail to achieve a good UX in their products. Part of
these challenges are rooted in inadequate knowledge and awareness about UX but
also in that UX models are commonly not well integrated with existing software
engineering (SE) models and concepts. Therefore, we present two SE-specific
models of UX for practitioners: (i) a layered model that shows the relation
between functional, quality, and UX requirements, and (ii) a general, UX-aware
software process overview model that shows the additional concepts and
activities that can help achieve a good UX. Validation of the models in
interviews with 12 practitioners and researchers generally found the models
useful for practice; for instance to raise knowledge and awareness about UX,
improve communications regarding UX and facilitating making UX-aware decisions
in the development process. In total, we identified six different areas of use
for the models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4205</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4205</id><created>2014-09-15</created><authors><author><keyname>Conejo</keyname><forenames>B.</forenames></author><author><keyname>Komodakis</keyname><forenames>N.</forenames></author><author><keyname>Leprince</keyname><forenames>S.</forenames></author><author><keyname>Avouac</keyname><forenames>J. P.</forenames></author></authors><title>Speeding-up Graphical Model Optimization via a Coarse-to-fine Cascade of
  Pruning Classifiers</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a general and versatile framework that significantly speeds-up
graphical model optimization while maintaining an excellent solution accuracy.
The proposed approach relies on a multi-scale pruning scheme that is able to
progressively reduce the solution space by use of a novel strategy based on a
coarse-to-fine cascade of learnt classifiers. We thoroughly experiment with
classic computer vision related MRF problems, where our framework constantly
yields a significant time speed-up (with respect to the most efficient
inference methods) and obtains a more accurate solution than directly
optimizing the MRF.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4230</identifier>
 <datestamp>2014-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4230</id><created>2014-09-15</created><updated>2014-09-18</updated><authors><author><keyname>Rebiha</keyname><forenames>Rachid</forenames></author><author><keyname>Moura</keyname><forenames>Arnaldo Vieira</forenames></author><author><keyname>Matringe</keyname><forenames>Nadir</forenames></author></authors><title>On the Termination of Linear and Affine Programs over the Integers</title><categories>cs.DM cs.LO</categories><comments>arXiv admin note: substantial text overlap with arXiv:1407.4556</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The termination problem for affine programs over the integers was left open
in\cite{Braverman}. For more that a decade, it has been considered and cited as
a challenging open problem. To the best of our knowledge, we present here the
most complete response to this issue: we show that termination for affine
programs over Z is decidable under an assumption holding for almost all affine
programs, except for an extremely small class of zero Lesbegue measure. We use
the notion of asymptotically non-terminating initial variable values} (ANT, for
short) for linear loop programs over Z. Those values are directly associated to
initial variable values for which the corresponding program does not terminate.
We reduce the termination problem of linear affine programs over the integers
to the emptiness check of a specific ANT set of initial variable values. For
this class of linear or affine programs, we prove that the corresponding ANT
set is a semi-linear space and we provide a powerful computational methods
allowing the automatic generation of these $ANT$ sets. Moreover, we are able to
address the conditional termination problem too. In other words, by taking ANT
set complements, we obtain a precise under-approximation of the set of inputs
for which the program does terminate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4237</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4237</id><created>2014-09-12</created><authors><author><keyname>Ray</keyname><forenames>Camellia</forenames></author><author><keyname>Das</keyname><forenames>Jayanta Kumar</forenames></author><author><keyname>Choudhury</keyname><forenames>Pabitra Pal</forenames></author></authors><title>On Analysis And Generation Of Biologically Important Boolean Functions</title><categories>cs.SY</categories><comments>11 pages, 7 figures. arXiv admin note: text overlap with
  arXiv:1405.2271</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Boolean networks are used to model biological networks such as gene
regulatory networks. Often Boolean networks show very chaotic behavior which is
sensitive to any small perturbations.In order to reduce the chaotic behavior
and to attain stability in the gene regulatory network,nested canalizing
functions(NCF)are best suited NCF and its variants have a wide range of
applications in system biology. Previously many work were done on the
application of canalizing functions but there were fewer methods to check if
any arbitrary Boolean function is canalizing or not. In this paper, by using
Karnaugh Map this problem gas been solved and also it has been shown that when
the canalizing functions of n variable is given, all the canalizing functions
of n+1 variable could be generated by the method of concatenation. In this
paper we have uniquely identified the number of NCFs having a particular
hamming distance (H.D) generated by each variable x as starting canalizing
input. Partially nested canalizing functions of 4 variables have also been
studied in this paper. Keywords: Karnaugh Map, Canalizing function, Nested
canalizing function, Partially nested canalizing function,concatenation
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4244</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4244</id><created>2014-09-12</created><authors><author><keyname>Baquela</keyname><forenames>Enrique Gabriel</forenames></author><author><keyname>Olivera</keyname><forenames>Ana Carolina</forenames></author></authors><title>An OvS-MultiObjective Algorithm Approach for Lane Reversal Problem</title><categories>cs.NE</categories><comments>Submitted to ALIO/EURO 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The lane reversal has proven to be a useful method to mitigate traffic
congestion during rush hour or in case of specific events that affect high
traffic volumes. In this work we propose a methodology that is placed within
optimization via Simulation, by means of which a multi-objective genetic
algorithm and simulations of traffic are used to determine the configuration of
ideal lane reversal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4253</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4253</id><created>2014-09-15</created><authors><author><keyname>Cosentino</keyname><forenames>Valerio</forenames></author><author><keyname>Izquierdo</keyname><forenames>Javier Luis Canovas</forenames></author><author><keyname>Cabot</keyname><forenames>Jordi</forenames></author></authors><title>Three Metrics to Explore the Openness of GitHub projects</title><categories>cs.SE</categories><comments>4 pages, 4 figures, sent to Mining Challenge track at MSR'14,
  rejected</comments><acm-class>H.4.0; D.2.8; J.4</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Open source software projects evolve thanks to a group of volunteers that
help in their development. Thus, the success of these projects depends on their
ability to attract (and keep) developers. We believe the openness of a project,
i.e., how easy is for a new user to actively contribute to it, can help to make
a project more attractive. To explore the openness of a software project, we
propose three metrics focused on: (1) the distribution of the project
community, (2) the rate of acceptance of external contributions and (3) the
time it takes to become an official collaborator of the project. We have
adapted and applied these metrics to a subset of GitHub projects, thus giving
some practical findings on their openness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4256</identifier>
 <datestamp>2015-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4256</id><created>2014-09-11</created><updated>2014-12-16</updated><authors><author><keyname>Ekeberg</keyname><forenames>Tomas</forenames></author><author><keyname>Engblom</keyname><forenames>Stefan</forenames></author><author><keyname>Liu</keyname><forenames>Jing</forenames></author></authors><title>Machine learning for ultrafast X-ray diffraction patterns on large-scale
  GPU clusters</title><categories>q-bio.BM cs.DC cs.LG physics.bio-ph q-bio.QM</categories><msc-class>68W10, 68W15, 68U10</msc-class><journal-ref>Int. J. High Perf. Comput. Appl. 29(2):233--243 (2015)</journal-ref><doi>10.1177/1094342015572030</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The classical method of determining the atomic structure of complex molecules
by analyzing diffraction patterns is currently undergoing drastic developments.
Modern techniques for producing extremely bright and coherent X-ray lasers
allow a beam of streaming particles to be intercepted and hit by an ultrashort
high energy X-ray beam. Through machine learning methods the data thus
collected can be transformed into a three-dimensional volumetric intensity map
of the particle itself. The computational complexity associated with this
problem is very high such that clusters of data parallel accelerators are
required.
  We have implemented a distributed and highly efficient algorithm for
inversion of large collections of diffraction patterns targeting clusters of
hundreds of GPUs. With the expected enormous amount of diffraction data to be
produced in the foreseeable future, this is the required scale to approach real
time processing of data at the beam site. Using both real and synthetic data we
look at the scaling properties of the application and discuss the overall
computational viability of this exciting and novel imaging technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4263</identifier>
 <datestamp>2015-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4263</id><created>2014-09-12</created><updated>2015-03-04</updated><authors><author><keyname>Cattaneo</keyname><forenames>G.</forenames></author><author><keyname>Conte</keyname><forenames>G.</forenames></author><author><keyname>Leporini</keyname><forenames>R.</forenames></author></authors><title>Unitary and anti-unitary quantum description of the classical not gate</title><categories>quant-ph cs.IT math.IT</categories><comments>This paper has been withdrawn by the author due to a crucial sign
  error in equation 1</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the unitary and the anti--unitary operator realizations of two
important genuine quantum gates that transform elements of the computational
basis of into superpositions: the square root of the identity and the square
root of the negation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4266</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4266</id><created>2014-09-15</created><authors><author><keyname>Broutin</keyname><forenames>Nicolas</forenames></author><author><keyname>Marckert</keyname><forenames>Jean-Fran&#xe7;ois</forenames></author></authors><title>A new encoding of coalescent processes. Applications to the additive and
  multiplicative cases</title><categories>math.PR cs.DM math.CO</categories><comments>29 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We revisit the discrete additive and multiplicative coalescents, starting
with $n$ particles with unit mass. These cases are known to be related to some
&quot;combinatorial coalescent processes&quot;: a time reversal of a fragmentation of
Cayley trees or a parking scheme in the additive case, and the random graph
process $(G(n,p))_p$ in the multiplicative case. Time being fixed, encoding
these combinatorial objects in real-valued processes indexed by the line is the
key to describing the asymptotic behaviour of the masses as $n\to +\infty$.
  We propose to use the Prim order on the vertices instead of the classical
breadth-first (or depth-first) traversal to encode the combinatorial coalescent
processes. In the additive case, this yields interesting connections between
the different representations of the process. In the multiplicative case, it
allows one to answer to a stronger version of an open question of Aldous [Ann.
Probab., vol. 25, pp. 812--854, 1997]: we prove that not only the sequence of
(rescaled) masses, seen as a process indexed by the time $\lambda$, converges
in distribution to the reordered sequence of lengths of the excursions above
the current minimum of a Brownian motion with parabolic drift $(B_t+\lambda t -
t^2/2, t\geq 0)$, but we also construct a version of the standard augmented
multiplicative coalescent of Bhamidi, Budhiraja and Wang [Probab. Theory Rel.,
to appear] using an additional Poisson point process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4269</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4269</id><created>2014-09-15</created><authors><author><keyname>Wang</keyname><forenames>Xianwen</forenames></author><author><keyname>Liu</keyname><forenames>Chen</forenames></author><author><keyname>Fang</keyname><forenames>Zhichao</forenames></author><author><keyname>Mao</keyname><forenames>Wenli</forenames></author></authors><title>From Attention to Citation, What and How Does Altmetrics Work?</title><categories>cs.DL physics.data-an physics.soc-ph</categories><comments>10 pages, 7 tables and 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scholarly and social impacts of scientific publications could be measured by
various metrics. In this study, the relationship between various metrics of
63,805 PLOS research articles are studied. Generally, article views correlate
well with citation, however, different types of article view have different
levels of correlation with citation, when pdf download correlates the citation
most significantly. It's necessary for publishers and journals to provide
detailed and comprehensive article metrics. Although the low correlation
between social attention and citation is confirmed by this study and previous
studies, more than ever, we find that social attention is highly correlated
with article view, especially the browser html view. Social attention is the
important source that bringing network traffic to browser html view and may
lead to citation subsequently. High altmetric score has the potential role in
promoting the long-term academic impact of articles, when a conceptual model is
proposed to interpret the conversion from social attention to article view, and
to citation finally.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4271</identifier>
 <datestamp>2015-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4271</id><created>2014-09-15</created><updated>2015-04-10</updated><authors><author><keyname>Zeng</keyname><forenames>Xiangrong</forenames></author><author><keyname>Figueiredo</keyname><forenames>M&#xe1;rio A. T.</forenames></author></authors><title>The Ordered Weighted $\ell_1$ Norm: Atomic Formulation, Projections, and
  Algorithms</title><categories>cs.DS cs.CV cs.IT cs.LG math.IT</categories><comments>13 pages, 17 figures. The latest version of this paper was submitted
  to a journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ordered weighted $\ell_1$ norm (OWL) was recently proposed, with two
different motivations: its good statistical properties as a sparsity promoting
regularizer; the fact that it generalizes the so-called {\it octagonal
shrinkage and clustering algorithm for regression} (OSCAR), which has the
ability to cluster/group regression variables that are highly correlated. This
paper contains several contributions to the study and application of OWL
regularization: the derivation of the atomic formulation of the OWL norm; the
derivation of the dual of the OWL norm, based on its atomic formulation; a new
and simpler derivation of the proximity operator of the OWL norm; an efficient
scheme to compute the Euclidean projection onto an OWL ball; the instantiation
of the conditional gradient (CG, also known as Frank-Wolfe) algorithm for
linear regression problems under OWL regularization; the instantiation of
accelerated projected gradient algorithms for the same class of problems.
Finally, a set of experiments give evidence that accelerated projected gradient
algorithms are considerably faster than CG, for the class of problems
considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4276</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4276</id><created>2014-09-12</created><authors><author><keyname>Cilibrasi</keyname><forenames>Rudi L.</forenames><affiliation>CWI, Amsterdam</affiliation></author><author><keyname>Vitanyi</keyname><forenames>Paul M. B.</forenames><affiliation>CWI and University of Amsterdam</affiliation></author></authors><title>A Fast Quartet Tree Heuristic for Hierarchical Clustering</title><categories>cs.LG cs.CE cs.DS</categories><comments>LaTeX, 40 pages, 11 figures; this paper has substantial overlap with
  arXiv:cs/0606048 in cs.DS</comments><journal-ref>Pattern Recognition, 44 (2011) 662-677</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Minimum Quartet Tree Cost problem is to construct an optimal weight tree
from the $3{n \choose 4}$ weighted quartet topologies on $n$ objects, where
optimality means that the summed weight of the embedded quartet topologies is
optimal (so it can be the case that the optimal tree embeds all quartets as
nonoptimal topologies). We present a Monte Carlo heuristic, based on randomized
hill climbing, for approximating the optimal weight tree, given the quartet
topology weights. The method repeatedly transforms a dendrogram, with all
objects involved as leaves, achieving a monotonic approximation to the exact
single globally optimal tree. The problem and the solution heuristic has been
extensively used for general hierarchical clustering of nontree-like
(non-phylogeny) data in various domains and across domains with heterogeneous
data. We also present a greatly improved heuristic, reducing the running time
by a factor of order a thousand to ten thousand. All this is implemented and
available, as part of the CompLearn package. We compare performance and running
time of the original and improved versions with those of UPGMA, BioNJ, and NJ,
as implemented in the SplitsTree package on genomic data for which the latter
are optimized.
  Keywords: Data and knowledge visualization, Pattern
matching--Clustering--Algorithms/Similarity measures, Hierarchical clustering,
Global optimization, Quartet tree, Randomized hill-climbing,
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4279</identifier>
 <datestamp>2015-05-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4279</id><created>2014-09-15</created><updated>2015-05-08</updated><authors><author><keyname>Papageorgiou</keyname><forenames>George</forenames></author><author><keyname>Bouboulis</keyname><forenames>Pantelis</forenames></author><author><keyname>Theodoridis</keyname><forenames>Sergios</forenames></author><author><keyname>Themelis</keyname><forenames>Kostantinos</forenames></author></authors><title>Robust Linear Regression Analysis - A Greedy Approach</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The task of robust linear estimation in the presence of outliers is of
particular importance in signal processing, statistics and machine learning.
Although the problem has been stated a few decades ago and solved using
classical (considered nowadays) methods, recently it has attracted more
attention in the context of sparse modeling, where several notable
contributions have been made. In the present manuscript, a new approach is
considered in the framework of greedy algorithms. The noise is split into two
components: a) the inlier bounded noise and b) the outliers, which are
explicitly modeled by employing sparsity arguments. Based on this scheme, a
novel efficient algorithm (Greedy Algorithm for Robust Denoising - GARD), is
derived. GARD alternates between a least square optimization criterion and an
Orthogonal Matching Pursuit (OMP) selection step that identifies the outliers.
The case where only outliers are present has been studied separately, where
bounds on the \textit{Restricted Isometry Property} guarantee that the recovery
of the signal via GARD is exact. Moreover, theoretical results concerning
convergence as well as the derivation of error bounds in the case of additional
bounded noise are discussed. Finally, we provide extensive simulations, which
demonstrate the comparative advantages of the new technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4290</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4290</id><created>2014-09-15</created><authors><author><keyname>Braverman</keyname><forenames>Mark</forenames></author><author><keyname>Mao</keyname><forenames>Jieming</forenames></author></authors><title>Simulating Noisy Channel Interaction</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that $T$ rounds of interaction over the binary symmetric channel
$BSC_{1/2-\epsilon}$ with feedback can be simulated with $O(\epsilon^2 T)$
rounds of interaction over a noiseless channel. We also introduce a more
general &quot;energy cost&quot; model of interaction over a noisy channel. We show energy
cost to be equivalent to external information complexity, which implies that
our simulation results are unlikely to carry over to energy complexity. Our
main technical innovation is a self-reduction from simulating a noisy channel
to simulating a slightly-less-noisy channel, which may have other applications
in the area of interactive compression.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4297</identifier>
 <datestamp>2014-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4297</id><created>2014-09-15</created><updated>2014-11-06</updated><authors><author><keyname>Mirsoleimani</keyname><forenames>S. Ali</forenames></author><author><keyname>Plaat</keyname><forenames>Aske</forenames></author><author><keyname>Vermaseren</keyname><forenames>Jos</forenames></author><author><keyname>Herik</keyname><forenames>Jaap van den</forenames></author></authors><title>Performance analysis of a 240 thread tournament level MCTS Go program on
  the Intel Xeon Phi</title><categories>cs.PF</categories><comments>7 pages, 12 figues</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In 2013 Intel introduced the Xeon Phi, a new parallel co-processor board. The
Xeon Phi is a cache-coherent many-core shared memory architecture claiming
CPU-like versatility, programmability, high performance, and power efficiency.
The first published micro-benchmark studies indicate that many of Intel's
claims appear to be true. The current paper is the first study on the Phi of a
complex artificial intelligence application. It contains an open source MCTS
application for playing tournament quality Go (an oriental board game). We
report the first speedup figures for up to 240 parallel threads on a real
machine, allowing a direct comparison to previous simulation studies. After a
substantial amount of work, we observed that performance scales well up to 32
threads, largely confirming previous simulation results of this Go program,
although the performance surprisingly deteriorates between 32 and 240 threads.
Furthermore, we report (1) unexpected performance anomalies between the Xeon
Phi and Xeon CPU for small problem sizes and small numbers of threads, and (2)
that performance is sensitive to scheduling choices. Achieving good performance
on the Xeon Phi for complex programs is not straightforward; it requires a deep
understanding of (1) search patterns, (2) of scheduling, and (3) of the
architecture and its many cores and caches. In practice, the Xeon Phi is less
straightforward to program for than originally envisioned by Intel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4299</identifier>
 <datestamp>2014-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4299</id><created>2014-09-15</created><authors><author><keyname>Da Lozzo</keyname><forenames>Giordano</forenames></author><author><keyname>Jel&#xed;nek</keyname><forenames>V&#xed;t</forenames></author><author><keyname>Kratochv&#xed;l</keyname><forenames>Jan</forenames></author><author><keyname>Rutter</keyname><forenames>Ignaz</forenames></author></authors><title>Planar Embeddings with Small and Uniform Faces</title><categories>cs.CG cs.CC cs.DM cs.DS</categories><comments>23 pages, 5 figures, extended version of 'Planar Embeddings with
  Small and Uniform Faces' (The 25th International Symposium on Algorithms and
  Computation, 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by finding planar embeddings that lead to drawings with favorable
aesthetics, we study the problems MINMAXFACE and UNIFORMFACES of embedding a
given biconnected multi-graph such that the largest face is as small as
possible and such that all faces have the same size, respectively.
  We prove a complexity dichotomy for MINMAXFACE and show that deciding whether
the maximum is at most $k$ is polynomial-time solvable for $k \leq 4$ and
NP-complete for $k \geq 5$. Further, we give a 6-approximation for minimizing
the maximum face in a planar embedding. For UNIFORMFACES, we show that the
problem is NP-complete for odd $k \geq 7$ and even $k \geq 10$. Moreover, we
characterize the biconnected planar multi-graphs admitting 3- and 4-uniform
embeddings (in a $k$-uniform embedding all faces have size $k$) and give an
efficient algorithm for testing the existence of a 6-uniform embedding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4304</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4304</id><created>2014-09-15</created><authors><author><keyname>Hoefer</keyname><forenames>Martin</forenames></author><author><keyname>Wagner</keyname><forenames>Lisa</forenames></author></authors><title>Matching Dynamics with Constraints</title><categories>cs.GT</categories><comments>Conference Version in WINE 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study uncoordinated matching markets with additional local constraints
that capture, e.g., restricted information, visibility, or externalities in
markets. Each agent is a node in a fixed matching network and strives to be
matched to another agent. Each agent has a complete preference list over all
other agents it can be matched with. However, depending on the constraints and
the current state of the game, not all possible partners are available for
matching at all times. For correlated preferences, we propose and study a
general class of hedonic coalition formation games that we call coalition
formation games with constraints. This class includes and extends many recently
studied variants of stable matching, such as locally stable matching, socially
stable matching, or friendship matching. Perhaps surprisingly, we show that all
these variants are encompassed in a class of &quot;consistent&quot; instances that always
allow a polynomial improvement sequence to a stable state. In addition, we show
that for consistent instances there always exists a polynomial sequence to
every reachable state. Our characterization is tight in the sense that we
provide exponential lower bounds when each of the requirements for consistency
is violated. We also analyze matching with uncorrelated preferences, where we
obtain a larger variety of results. While socially stable matching always
allows a polynomial sequence to a stable state, for other classes different
additional assumptions are sufficient to guarantee the same results. For the
problem of reaching a given stable state, we show NP-hardness in almost all
considered classes of matching games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4320</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4320</id><created>2014-09-15</created><updated>2015-03-03</updated><authors><author><keyname>Fu</keyname><forenames>Xiao</forenames></author><author><keyname>Ma</keyname><forenames>Wing-Kin</forenames></author><author><keyname>Chan</keyname><forenames>Tsung-Han</forenames></author><author><keyname>Bioucas-Dias</keyname><forenames>Jos&#xe9; M.</forenames></author></authors><title>Self-Dictionary Sparse Regression for Hyperspectral Unmixing: Greedy
  Pursuit and Pure Pixel Search are Related</title><categories>stat.ML cs.IT math.IT math.OC</categories><doi>10.1109/JSTSP.2015.2410763</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers a recently emerged hyperspectral unmixing formulation
based on sparse regression of a self-dictionary multiple measurement vector
(SD-MMV) model, wherein the measured hyperspectral pixels are used as the
dictionary. Operating under the pure pixel assumption, this SD-MMV formalism is
special in that it allows simultaneous identification of the endmember spectral
signatures and the number of endmembers. Previous SD-MMV studies mainly focus
on convex relaxations. In this study, we explore the alternative of greedy
pursuit, which generally provides efficient and simple algorithms. In
particular, we design a greedy SD-MMV algorithm using simultaneous orthogonal
matching pursuit. Intriguingly, the proposed greedy algorithm is shown to be
closely related to some existing pure pixel search algorithms, especially, the
successive projection algorithm (SPA). Thus, a link between SD-MMV and pure
pixel search is revealed. We then perform exact recovery analyses, and prove
that the proposed greedy algorithm is robust to noise---including its
identification of the (unknown) number of endmembers---under a sufficiently low
noise level. The identification performance of the proposed greedy algorithm is
demonstrated through both synthetic and real-data experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4321</identifier>
 <datestamp>2015-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4321</id><created>2014-09-15</created><updated>2015-06-02</updated><authors><author><keyname>Bachelier</keyname><forenames>Olivier</forenames><affiliation>LIAS</affiliation></author><author><keyname>Henrion</keyname><forenames>Didier</forenames><affiliation>LAAS-MAC</affiliation></author><author><keyname>Yeganefar</keyname><forenames>Nima</forenames><affiliation>LIAS</affiliation></author><author><keyname>Mehdi</keyname><forenames>Driss</forenames><affiliation>LAII</affiliation></author></authors><title>On the solutions to complex parameter-dependent LMIs involved in the
  stability analysis of 2D discrete models</title><categories>math.OC cs.SY</categories><comments>Updated version</comments><proxy>ccsd</proxy><report-no>Rapport LAAS n{\textdegree} 14419</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this short communique is to adapt a result established by Bliman,
related to the possible approximation of the solutions to
real-parameter-dependent linear matrix inequalities (LMIs), to the special
context of stability analysis of 2D discrete Roesser models. While Bliman
considered the case of LMIs involving several real parameters, which is
especially crucial for the analysis of linear systems against parametric
deflections, the stability of Roesser models leads to consider LMIs with only
one single complex parameter. Extending the results from real parameters to
complex ones is not straightforward in our opinion. This is why the present
note discusses precautions to be taken concerning this case before applying the
results in a 2D context. Actually, it is shown that a well-known condition for
structural stability of a 2D discrete Roesser can be relaxed into an LMI system
whose solution polynomially depends on a single complex parameter over the unit
circle.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4326</identifier>
 <datestamp>2015-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4326</id><created>2014-09-15</created><updated>2015-10-20</updated><authors><author><keyname>&#x17d;bontar</keyname><forenames>Jure</forenames></author><author><keyname>LeCun</keyname><forenames>Yann</forenames></author></authors><title>Computing the Stereo Matching Cost with a Convolutional Neural Network</title><categories>cs.CV cs.LG cs.NE</categories><comments>Conference on Computer Vision and Pattern Recognition (CVPR), June
  2015</comments><doi>10.1109/CVPR.2015.7298767</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a method for extracting depth information from a rectified image
pair. We train a convolutional neural network to predict how well two image
patches match and use it to compute the stereo matching cost. The cost is
refined by cross-based cost aggregation and semiglobal matching, followed by a
left-right consistency check to eliminate errors in the occluded regions. Our
stereo method achieves an error rate of 2.61 % on the KITTI stereo dataset and
is currently (August 2014) the top performing method on this dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4327</identifier>
 <datestamp>2014-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4327</id><created>2014-09-15</created><authors><author><keyname>Jayaraman</keyname><forenames>Dinesh</forenames></author><author><keyname>Grauman</keyname><forenames>Kristen</forenames></author></authors><title>Zero Shot Recognition with Unreliable Attributes</title><categories>cs.CV stat.ML</categories><comments>To appear at NIPS 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In principle, zero-shot learning makes it possible to train a recognition
model simply by specifying the category's attributes. For example, with
classifiers for generic attributes like \emph{striped} and \emph{four-legged},
one can construct a classifier for the zebra category by enumerating which
properties it possesses---even without providing zebra training images. In
practice, however, the standard zero-shot paradigm suffers because attribute
predictions in novel images are hard to get right. We propose a novel random
forest approach to train zero-shot models that explicitly accounts for the
unreliability of attribute predictions. By leveraging statistics about each
attribute's error tendencies, our method obtains more robust discriminative
models for the unseen classes. We further devise extensions to handle the
few-shot scenario and unreliable attribute descriptions. On three datasets, we
demonstrate the benefit for visual category learning with zero or few training
examples, a critical domain for rare categories or categories defined on the
fly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4331</identifier>
 <datestamp>2014-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4331</id><created>2014-09-15</created><authors><author><keyname>Abuzainab</keyname><forenames>Nof</forenames></author><author><keyname>Vinnakota</keyname><forenames>Sai Rakshit</forenames></author><author><keyname>Touati</keyname><forenames>Corinne</forenames></author></authors><title>Coalition Formation Game for Cooperative Cognitive Radio Using Gibbs
  Sampling</title><categories>cs.GT cs.IT math.IT</categories><comments>7 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers a cognitive radio network in which each secondary user
selects a primary user to assist in order to get a chance of accessing the
primary user channel. Thus, each group of secondary users assisting the same
primary user forms a coaltion. Within each coalition, sequential relaying is
employed, and a relay ordering algorithm is used to make use of the relays in
an efficient manner. It is required then to find the optimal sets of secondary
users assisting each primary user such that the sum of their rates is
maximized. The problem is formulated as a coalition formation game, and a Gibbs
Sampling based algorithm is used to find the optimal coalition structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1409.4338</identifier>
 <datestamp>2015-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1409.4338</id><created>2014-09-15</created><updated>2015-09-16</updated><authors><author><keyname>Berta</keyname><forenames>Mario</forenames></author><author><keyname>Christandl</keyname><forenames>Matthias</forenames></author><author><keyname>Touchette</keyname><forenames>Dave</forenames></author></authors><title>Smooth Entropy Bounds on One-Shot Quantum State Redistribution</title><categories>quant-ph cs.IT math.IT</categories><comments>v3: 29 pages, 1 figure, extended strong converse discussion</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In quantum state redistribution as introduced in [Luo and Devetak (2009)] and
[Devetak and Yard (2008)], there are four systems of interest: the $A$ system
held by Alice, the $B$ system held by Bob, the $C$ system that is to be
transmitted from Alice to Bob, and the $R$ system that holds a purification of
the state in the $ABC$ registers. We give upper and lower bounds on the amount
of quantum communication and entanglement required to perform the task of
quantum state redistribution in a one-shot setting. Our bounds are in terms of
the smooth conditional min- and max-entropy, and the smooth max-information.
The protocol for the upper bound has a clear structure, building on the work
[Oppenheim (2008)]: it decomposes the quantum state redistribution task into
two simpler quantum state merging tasks by introducing a coherent relay. In the
independent and identical (iid) asymptotic limit our bounds for the quantum
communication cost converge to the quantum conditional mutual information
$I(C:R|B)$, and our bounds for the total cost converge to the conditional
entropy $H(C|B)$. This yields an alternative proof of optimality of these rates
for quantum state redistribution in the iid asymptotic limit. In particular, we
obtain a strong converse for quantum state redistribution, which even holds
when allowing for feedback.
</abstract></arXiv>
</metadata>
</record>
<resumptionToken cursor="65000" completeListSize="102538">1122234|66001</resumptionToken>
</ListRecords>
</OAI-PMH>
