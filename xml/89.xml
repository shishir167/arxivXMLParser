<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
<responseDate>2016-03-09T04:00:10Z</responseDate>
<request verb="ListRecords" resumptionToken="1122234|88001">http://export.arxiv.org/oai2</request>
<ListRecords>
<record>
<header>
 <identifier>oai:arXiv.org:1511.08844</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.08844</id><created>2015-11-27</created><authors><author><keyname>Raza</keyname><forenames>Arif</forenames></author><author><keyname>Capretz</keyname><forenames>Luiz Fernando</forenames></author><author><keyname>Ahmed</keyname><forenames>Faheem</forenames></author></authors><title>An Empirical Study of Open Source Software Usability: The Industrial
  Perspective</title><categories>cs.HC</categories><comments>arXiv admin note: text overlap with arXiv:1507.06882</comments><journal-ref>International Journal of Open Source Software and Processes,
  3(1):1-16, 2011</journal-ref><doi>10.4018/jossp.2011010101</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent years have seen a sharp increase in the use of open source projects by
common novice users; Open Source Software (OSS) is thus no longer a reserved
arena for software developers and computer gurus. Although user-centered
designs are gaining popularity in OSS, usability is still not considered as one
of the prime objectives in many design scenarios. In this paper, we analyze
industry users perception of usability factors, including understandability,
learnability, operability and attractiveness, on OSS usability. The research
model of this empirical study establishes the relationship between the key
usability factors and OSS usability from industrial perspective. In order to
conduct the study, a data set of 105 industry users is included. The results of
the empirical investigation indicate the significance of the key factors for
OSS usability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.08845</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.08845</id><created>2015-11-27</created><authors><author><keyname>Capretz</keyname><forenames>Luiz Fernando</forenames></author></authors><title>Psychological Types of Brazilian Software Engineering Students</title><categories>cs.SE cs.CY</categories><comments>arXiv admin note: text overlap with arXiv:1507.06873</comments><journal-ref>Journal of Psychological Type, 68(5):37-42, 2008</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this investigation was to establish the personality profile of
Brazilian software engineering students according to the MBTI. This study also
shows that the software engineering field attracts students of some types more
than other types, for instance: Is, Ps, IPs, TPs, and INs are significantly
represented in that group as opposed to E, Js, EJs, TJs, ENs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.08851</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.08851</id><created>2015-11-27</created><authors><author><keyname>Hamana</keyname><forenames>Makoto</forenames></author><author><keyname>Matsuda</keyname><forenames>Kazutaka</forenames></author><author><keyname>Asada</keyname><forenames>Kazuyuki</forenames></author></authors><title>The Algebra of Recursive Graph Transformation Language UnCAL: Complete
  Axiomatisation and Iteration Categorical Semantics</title><categories>cs.LO cs.DB</categories><comments>40 pages</comments><acm-class>F.3.2, F.3.3, H.2.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this paper is to provide mathematical foundations of a graph
transformation language, called UnCAL, using categorical semantics of type
theory and fixed points. About twenty years ago, Buneman et al. developed a
graph database query language UnQL on the top of a functional meta-language
UnCAL for describing and manipulating graphs. Recently, the functional
programming community has shown renewed interest in UnCAL, because it provides
an efficient graph transformation language which is useful for various
applications, such as bidirectional computation.
  In order to make UnCAL more flexible and fruitful for further extensions and
applications, in this paper, we give a more conceptual understanding of UnCAL
using categorical semantics. Our general interest of this paper is to clarify
what is the algebra of UnCAL. Thus, we give an equational axiomatisation and
categorical semantics of UnCAL, both of which are new. We show that the
axiomatisation is complete for the original bisimulation semantics of UnCAL.
Moreover, we provide a clean characterisation of the computation mechanism of
UnCAL called &quot;structural recursion on graphs&quot; using our categorical semantics.
We show a concrete model of UnCAL given by the lambdaG-calclus, which shows an
interesting connection to lazy functional programming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.08854</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.08854</id><created>2015-11-27</created><updated>2015-12-01</updated><authors><author><keyname>Kozachinskiy</keyname><forenames>Alexander</forenames></author></authors><title>Some Bounds on Communication Complexity of Gap Hamming Distance</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we obtain some bounds on communication complexity of Gap
Hamming Distance problem ($\mathsf{GHD}^n_{L, U}$): Alice and Bob are given
binary string of length $n$ and they are guaranteed that Hamming distance
between their inputs is either $\le L$ or $\ge U$ for some $L &lt; U$. They have
to output 0, if the first inequality holds, and 1, if the second inequality
holds.
  In this paper we study the communication complexity of $\mathsf{GHD}^n_{L,
U}$ for probabilistic protocols with one-sided error and for deterministic
protocols. Our first result is a protocol which communicates
$O\left(\left(\frac{s}{U}\right)^\frac{1}{3} \cdot n\log n\right)$ bits and has
one-sided error probability $e^{-s}$ provided $s \ge \frac{(L +
\frac{10}{n})^3}{U^2}$.
  Our second result is about deterministic communication complexity of
$\mathsf{GHD}^n_{0,\, t}$. Surprisingly, it can be computed with logarithmic
precision: $$\mathrm{D}(\mathsf{GHD}^n_{0,\, t}) = n - \log_2 V_2\left(n,
\left\lfloor\frac{t}{2}\right\rfloor\right) + O(\log n),$$ where $V_2(n, r)$
denotes the size of Hamming ball of radius $r$.
  As an application of this result for every $c &lt; 2$ we prove a
$\Omega\left(\frac{n(2 - c)^2}{p}\right)$ lower bound on the space complexity
of any $c$-approximate deterministic $p$-pass streaming algorithm for computing
the number of distinct elements in a data stream of length $n$ with tokens
drawn from the universe $U = \{1, 2, \ldots, n\}$. Previously that lower bound
was known for $c &lt; \frac{3}{2}$ and for $c &lt; 2$ but with larger $|U|$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.08855</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.08855</id><created>2015-11-27</created><authors><author><keyname>Webber</keyname><forenames>Francisco De Sousa</forenames></author></authors><title>Semantic Folding Theory And its Application in Semantic Fingerprinting</title><categories>cs.AI cs.CL q-bio.NC</categories><comments>57 pages, white paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Human language is recognized as a very complex domain since decades. No
computer system has been able to reach human levels of performance so far. The
only known computational system capable of proper language processing is the
human brain. While we gather more and more data about the brain, its
fundamental computational processes still remain obscure. The lack of a sound
computational brain theory also prevents the fundamental understanding of
Natural Language Processing. As always when science lacks a theoretical
foundation, statistical modeling is applied to accommodate as many sampled
real-world data as possible. An unsolved fundamental issue is the actual
representation of language (data) within the brain, denoted as the
Representational Problem. Starting with Jeff Hawkins' Hierarchical Temporal
Memory (HTM) theory, a consistent computational theory of the human cortex, we
have developed a corresponding theory of language data representation: The
Semantic Folding Theory. The process of encoding words, by using a topographic
semantic space as distributional reference frame into a sparse binary
representational vector is called Semantic Folding and is the central topic of
this document. Semantic Folding describes a method of converting language from
its symbolic representation (text) into an explicit, semantically grounded
representation that can be generically processed by Hawkins' HTM networks. As
it turned out, this change in representation, by itself, can solve many complex
NLP problems by applying Boolean operators and a generic similarity function
like the Euclidian Distance. Many practical problems of statistical NLP
systems, like the high cost of computation, the fundamental incongruity of
precision and recall , the complex tuning procedures etc., can be elegantly
overcome by applying Semantic Folding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.08857</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.08857</id><created>2015-11-27</created><authors><author><keyname>Buyya</keyname><forenames>Rajkumar</forenames></author><author><keyname>Barreto</keyname><forenames>Diana</forenames></author></authors><title>Multi-Cloud Resource Provisioning with Aneka: A Unified and Integrated
  Utilisation of Microsoft Azure and Amazon EC2 Instances</title><categories>cs.DC</categories><comments>14 pages, 12 figures. Conference paper, Proceedings of the 2015
  International Conference on Computing and Network Communications (CoCoNet
  2015, IEEE Press, USA), Trivandrum, India, December 16-19, 2015</comments><acm-class>C.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many vendors are offering computing services on subscription basis via
Infrastructure-as-a-Service (IaaS) model. Users can acquire resources from
different providers and get the best of each of them to run their applications.
However, deploying applications in multi-cloud environments is a complex task.
Therefore, application platforms are needed to help developers to succeed.
Aneka is one such platform that supports developers to program and deploy
distributed applications in multi-cloud environments. It can be used to
provision resources from different cloud providers and can be configured to
request resources dynamically according to the needs of specific applications.
This paper presents extensions incorporated in Aneka to support the deployment
of applications in multi-cloud environments. The first extension shows the
flexibility of Aneka architecture to add cloud providers. Specifically, we
describe the addition of Microsoft Azure IaaS cloud provider. We also discuss
the inclusion of public IPs to communicate resources located in different
networks and the functionality of using PowerShell to automatize installation
of Aneka on remote resources. We demonstrate how an application composed of
independent tasks improves its total execution time when it is deployed in the
multi-cloud environment created by Aneka using resources provisioned from Azure
and EC2.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.08861</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.08861</id><created>2015-11-27</created><authors><author><keyname>Zhao</keyname><forenames>Hang</forenames></author><author><keyname>Gallo</keyname><forenames>Orazio</forenames></author><author><keyname>Frosio</keyname><forenames>Iuri</forenames></author><author><keyname>Kautz</keyname><forenames>Jan</forenames></author></authors><title>Is L2 a Good Loss Function for Neural Networks for Image Processing?</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neural networks are becoming central in several areas of computer vision and
image processing. Different architectures have been proposed to solve specific
problems. The impact of the loss layer of neural networks, however, has not
received much attention by the research community: the default and most common
choice is L2. This can be particularly limiting in the context of image
processing, since L2 correlates poorly with perceived image quality.
  In this paper we bring attention to alternative choices. We study the
performance of several losses, including perceptually-motivated losses, and
propose a novel, differentiable error function. We show that the quality of the
results improves significantly with better loss functions, even for the same
network architecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.08862</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.08862</id><created>2015-11-27</created><authors><author><keyname>Zahedinejad</keyname><forenames>Ehsan</forenames></author><author><keyname>Ghosh</keyname><forenames>Joydip</forenames></author><author><keyname>Sanders</keyname><forenames>Barry C.</forenames></author></authors><title>Designing high-fidelity single-shot three-qubit gates: A machine
  learning approach</title><categories>quant-ph cs.LG</categories><comments>18 pages, 19 figures. Comments welcome</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Three-qubit quantum gates are crucial for quantum error correction and
quantum information processing. We generate policies for quantum control
procedures to design three types of three-qubit gates, namely Toffoli,
Controlled-Not-Not and Fredkin gates. The design procedures are applicable to
an architecture of nearest-neighbor-coupled superconducting artificial atoms.
The resultant fidelity for each gate is above 99.9%, which is an accepted
threshold fidelity for fault-tolerant quantum computing. We test our policy in
the presence of decoherence-induced noise as well as show its robustness
against random external noise generated by the control electronics. The
three-qubit gates are designed via our machine learning algorithm called
Subspace-Selective Self-Adaptive Differential Evolution (SuSSADE).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.08865</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.08865</id><created>2015-11-27</created><authors><author><keyname>Muhammad</keyname><forenames>Khan</forenames></author></authors><title>Steganography: A Secure way for Transmission in Wireless Sensor Networks</title><categories>cs.MM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Addressing the security concerns in wireless sensor networks (WSN) is a
challenging task, which has attracted the attention of many researchers from
the last few decades. Researchers have presented various schemes in WSN,
addressing the problems of processing, bandwidth, load balancing, and efficient
routing. However, little work has been done on security aspects of WSN. In a
typical WSN network, the tiny nodes installed on different locations sense the
surrounding environment, send the collected data to their neighbors, which in
turn is forwarded to a sink node. The sink node aggregate the data received
from different sensors and send it to the base station for further processing
and necessary actions. In highly critical sensor networks such as military and
law enforcement agencies networks, the transmission of such aggregated data via
the public network Internet is very sensitive and vulnerable to various attacks
and risks. Therefore, this paper provides a solution for addressing these
security issues based on steganography, where the aggregated data can be
embedded as a secret message inside an innocent-looking cover image. The stego
image containing the embedded data can be then sent to fusion center using
Internet. At the fusion center, the hidden data is extracted from the image,
the required processing is performed and decision is taken accordingly.
Experimentally, the proposed method is evaluated by objective analysis using
peak signal-to-noise ratio (PSNR), mean square error (MSE), normalized cross
correlation (NCC), and structural similarity index metric (SSIM), providing
promising results in terms of security and image quality, thus validating its
superiority.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.08876</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.08876</id><created>2015-11-28</created><authors><author><keyname>Shahrasbi</keyname><forenames>Behzad</forenames></author></authors><title>Stability Analysis of Network of Similar-Plants via Feedback Network</title><categories>cs.SY</categories><comments>5 pages; 4 figures; submitted to IEEE Systems Conference 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Here, we study a networked control system with similar linear time-invariant
plants. Using master stability function method we propose a network
optimization method to minimize the feedback network order in the sense
Frobenius norm. Then we verify our results with numerical example. We show that
this method outperforms the known feedback network optimization methods namely
matching condition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.08886</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.08886</id><created>2015-11-28</created><authors><author><keyname>El</keyname><forenames>Roy Or -</forenames></author><author><keyname>Hershkovitz</keyname><forenames>Rom</forenames></author><author><keyname>Wetzler</keyname><forenames>Aaron</forenames></author><author><keyname>Rosman</keyname><forenames>Guy</forenames></author><author><keyname>Bruckstein</keyname><forenames>Alfred M.</forenames></author><author><keyname>Kimmel</keyname><forenames>Ron</forenames></author></authors><title>Real-Time Depth Refinement for Specular Objects</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The introduction of consumer RGB-D scanners set off a major boost in 3D
computer vision research. Yet, the precision of existing depth scanners is not
accurate enough to recover fine details of a scanned object. While modern
shading based depth refinement methods have been proven to work well with
Lambertian objects, they break down in the presence of specularities. We
present a novel shape from shading framework that addresses this issue and
enhances both diffuse and specular objects' depth profiles. We take advantage
of the built-in monochromatic IR projector and IR images of the RGB-D scanners
and present a lighting model that accounts for the specular regions in the
input image. Using this model, we reconstruct the depth map in real-time. Both
quantitative tests and visual evaluations prove that the proposed method
produces state of the art depth reconstruction results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.08887</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.08887</id><created>2015-11-28</created><authors><author><keyname>Ding</keyname><forenames>Tian</forenames></author><author><keyname>Yuan</keyname><forenames>Xiaojun</forenames></author><author><keyname>Liew</keyname><forenames>Soung Chang</forenames></author></authors><title>Degrees of Freedom of the Symmetric Multi-Relay MIMO Y Channel</title><categories>cs.IT math.IT</categories><comments>31 pages, 5 figures, submitted to IEEE Trans. Inf. Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the degrees of freedom (DoF) of a new network
information flow model called symmetric multi-relay MIMO Y channel, where three
user nodes, each with M antennas, communicate via K geographically separated
relay nodes, each with N antennas. For this model, we establish a general DoF
achievability framework based on linear precoding and post-processing methods.
The framework poses a nonlinear problem with respect to user precoders and
post-processors, as well as relay precoders. To solve this problem, we adopt an
uplink-dowlink asymmetric strategy, where the user precoders are designed for
signal alignment and the user post-processors are used for interference
neutralization. With the user precoder and post-processor designs fixed as
such, the original problem then reduces to a problem of relay precoder design.
To address the solvability of the system, we propose a general method for
solving matrix equations. This method is also useful to the DoF analysis of
many other multiway relay networks. Together with the techniques of antenna
disablement and symbol extension, an achievable DoF of the symmetric
multi-relay MIMO Y channel is derived for an arbitrary setup of (K, M, N). We
show that, for K &gt;= 2, the optimal DoF is achieved for M/N in [0,
max{sqrt(3K)/3,1}) and [(3K+sqrt(9K^2-12K))/6,infinity). We also show that for
symmetric multi-relay MIMO Y channel, our design outperforms the conventional
uplink-downlink symmetric design considerably.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.08889</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.08889</id><created>2015-11-28</created><authors><author><keyname>Alahmadi</keyname><forenames>Adel</forenames></author><author><keyname>Deza</keyname><forenames>Michel</forenames></author><author><keyname>Sikiri&#x107;</keyname><forenames>Mathieu Dutour</forenames></author><author><keyname>Sol&#xe9;</keyname><forenames>Patrick</forenames></author></authors><title>The joint weight enumerator of an LCD code and its dual</title><categories>math.MG cs.IT math.IT</categories><comments>5 pages, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A binary linear code is called {\em LCD} if it intersects its dual trivially.
We show that the coefficients of the joint weight enumerator of such a code
with its dual satisfy linear constraints, leading to a new linear programming
bound on the size of an LCD code of given length and minimum distance. In
addition, we show that this polynomial is, in general, an invariant of a matrix
group of dimension $4$ and order $12$. Also, we sketch a Gleason formula for
this weight enumerator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.08893</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.08893</id><created>2015-11-28</created><authors><author><keyname>Buscemi</keyname><forenames>Francesco</forenames></author></authors><title>Degradable channels, less noisy channels, and quantum statistical
  morphisms: an equivalence relation</title><categories>quant-ph cs.IT math.IT math.ST stat.TH</categories><comments>14 pages, no figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two partial orderings among communication channels, namely, `being degradable
into' and `being less noisy than,' are reconsidered in the light of recent
results about statistical comparisons of quantum channels. Though our analysis
covers at once both classical and quantum channels, we also provide a separate
treatment of classical noisy channels, and show how, in this case, an
alternative self-contained proof can be constructed, with its own particular
merits with respect to the general result.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.08899</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.08899</id><created>2015-11-28</created><authors><author><keyname>Moustafa</keyname><forenames>Mohamed</forenames></author></authors><title>Applying deep learning to classify pornographic images and videos</title><categories>cs.CV cs.MM cs.NE</categories><comments>PSIVT 2015, the final publication is available at link.springer.com</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is no secret that pornographic material is now a one-click-away from
everyone, including children and minors. General social media networks are
striving to isolate adult images and videos from normal ones. Intelligent image
analysis methods can help to automatically detect and isolate questionable
images in media. Unfortunately, these methods require vast experience to design
the classifier including one or more of the popular computer vision feature
descriptors. We propose to build a classifier based on one of the recently
flourishing deep learning techniques. Convolutional neural networks contain
many layers for both automatic features extraction and classification. The
benefit is an easier system to build (no need for hand-crafting features and
classifiers). Additionally, our experiments show that it is even more accurate
than the state of the art methods on the most recent benchmark dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.08904</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.08904</id><created>2015-11-28</created><authors><author><keyname>Marbach</keyname><forenames>Peter</forenames></author></authors><title>Modeling and Analysis of Information Communities</title><categories>cs.SI cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Communities are an important feature of social networks. In fact, it seems
that communities are necessary for a social network to be efficient. However,
there exist very few formal studies of the actual role of communities in social
networks, how they emerge, and how they are structured. The goal of this paper
is to propose a mathematical model to study communities in social networks. For
this, we consider a particular case of a social network, namely information
networks. We assume that there is a population of agents who are interested in
obtaining content. Agents differ in the type of content they are interested in.
The goal of agents is to form communities in order to maximize their utility
for obtaining and producing content. We use this model to characterize the
structure of communities that emerge in this setting. While the proposed model
is very simple, the obtained results suggest that it indeed is able to capture
key properties of information communities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.08905</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.08905</id><created>2015-11-28</created><authors><author><keyname>Hong</keyname><forenames>Mingyi</forenames></author><author><keyname>Chang</keyname><forenames>Tsung-Hui</forenames></author></authors><title>Stochastic Proximal Gradient Consensus Over Random Networks</title><categories>math.OC cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider solving a convex, possibly stochastic optimization problem over a
randomly time-varying multi-agent network. Each agent has access to some local
objective function, and it only has unbiased estimates of the gradients of the
smooth component. We develop a dynamic stochastic proximal-gradient consensus
(DySPGC) algorithm, with the following key features: i) it works for both the
static and certain randomly time-varying networks, ii) it allows the agents to
utilize either the exact or stochastic gradient information, iii) it is
convergent with provable rate. In particular, we show that the proposed
algorithm converges to a global optimal solution, with a rate of
$\mathcal{O}(1/r)$ [resp. $\mathcal{O}(1/\sqrt{r})$] when the exact (resp.
stochastic) gradient is available, where r is the iteration counter.
  Interestingly, the developed algorithm bridges a number of (seemingly
unrelated) distributed optimization algorithms, such as the EXTRA (Shi et al.
2014), the PG-EXTRA (Shi et al. 2015), the IC/IDC-ADMM (Chang et al. 2014), and
the DLM (Ling et al. 2015) and the classical distributed subgradient method.
Identifying such relationship allows for significant generalization of these
methods. We also discuss one such generalization which accelerates the DySPGC
(hence accelerating EXTRA, PG-EXTRA, IC-ADMM).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.08913</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.08913</id><created>2015-11-28</created><authors><author><keyname>Guo</keyname><forenames>Qi</forenames></author><author><keyname>Dan</keyname><forenames>Le</forenames></author><author><keyname>Yin</keyname><forenames>Dong</forenames></author><author><keyname>Ji</keyname><forenames>Xiangyang</forenames></author></authors><title>Sliding-Window Optimization on an Ambiguity-Clearness Graph for
  Multi-object Tracking</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-object tracking remains challenging due to frequent occurrence of
occlusions and outliers. In order to handle this problem, we propose an
Approximation-Shrink Scheme for sequential optimization. This scheme is
realized by introducing an Ambiguity-Clearness Graph to avoid conflicts and
maintain sequence independent, as well as a sliding window optimization
framework to constrain the size of state space and guarantee convergence. Based
on this window-wise framework, the states of targets are clustered in a
self-organizing manner. Moreover, we show that the traditional online and batch
tracking methods can be embraced by the window-wise framework. Experiments
indicate that with only a small window, the optimization performance can be
much better than online methods and approach to batch methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.08915</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.08915</id><created>2015-11-28</created><updated>2016-02-11</updated><authors><author><keyname>Urbani</keyname><forenames>Jacopo</forenames></author><author><keyname>Jacobs</keyname><forenames>Ceriel</forenames></author><author><keyname>Kr&#xf6;tzsch</keyname><forenames>Markus</forenames></author></authors><title>Column-Oriented Datalog Materialization for Large Knowledge Graphs
  (Extended Technical Report)</title><categories>cs.DB cs.AI</categories><acm-class>I.2.3; H.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The evaluation of Datalog rules over large Knowledge Graphs (KGs) is
essential for many applications. In this paper, we present a new method of
materializing Datalog inferences, which combines a column-based memory layout
with novel optimization methods that avoid redundant inferences at runtime. The
pro-active caching of certain subqueries further increases efficiency. Our
empirical evaluation shows that this approach can often match or even surpass
the performance of state-of-the-art systems, especially under restricted
resources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.08920</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.08920</id><created>2015-11-28</created><authors><author><keyname>Kola&#x159;&#xed;k</keyname><forenames>Filip</forenames></author><author><keyname>Patz&#xe1;k</keyname><forenames>Bo&#x159;ek</forenames></author><author><keyname>Zeman</keyname><forenames>Jan</forenames></author></authors><title>Computational Homogenization of Fresh Concrete Flow Around Reinforcing
  Bars</title><categories>cs.CE</categories><comments>23 pages, 15 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by casting of fresh concrete in reinforced concrete structures, we
introduce a numerical model of a steady-state non-Newtonian fluid flow through
a porous domain. Our approach combines homogenization techniques to represent
the reinforced domain by the Darcy law with an interface coupling of the Stokes
and Darcy flows through the Beavers-Joseph-Saffman conditions. The ensuing
two-scale problem is solved by the Finite Element Method with consistent
linearization and the results obtained from the homogenization approach are
verified against fully resolved direct numerical simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.08936</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.08936</id><created>2015-11-28</created><updated>2016-02-08</updated><authors><author><keyname>Abdelhadi</keyname><forenames>Ahmed</forenames></author><author><keyname>Rechia</keyname><forenames>Felipe</forenames></author><author><keyname>Narayanan</keyname><forenames>Arvind</forenames></author><author><keyname>Teixeira</keyname><forenames>Thiago</forenames></author><author><keyname>Lent</keyname><forenames>Ricardo</forenames></author><author><keyname>Benhaddou</keyname><forenames>Driss</forenames></author><author><keyname>Lee</keyname><forenames>Hyunwoo</forenames></author><author><keyname>Clancy</keyname><forenames>T. Charles</forenames></author></authors><title>Position Estimation of Robotic Mobile Nodes in Wireless Testbed using
  GENI</title><categories>cs.NI</categories><comments>(c) 2016 IEEE. Personal use of this material is permitted. Permission
  from IEEE must be obtained for all other uses, in any current or future
  media, including reprinting/republishing this material for advertising or
  promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a low complexity experimental RF-based indoor localization system
based on the collection and processing of WiFi RSSI signals and processing
using a RSS-based multi-lateration algorithm to determine a robotic mobile
node's location. We use a real indoor wireless testbed called w-iLab.t that is
deployed in Zwijnaarde, Ghent, Belgium. One of the unique attributes of this
testbed is that it provides tools and interfaces using Global Environment for
Network Innovations (GENI) project to easily create reproducible wireless
network experiments in a controlled environment. We provide a low complexity
algorithm to estimate the location of the mobile robots in the indoor
environment. In addition, we provide a comparison between some of our collected
measurements with their corresponding location estimation and the actual robot
location. The comparison shows an accuracy between 0.65 and 5 meters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.08941</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.08941</id><created>2015-10-20</created><updated>2015-12-01</updated><authors><author><keyname>Eswaran</keyname><forenames>K.</forenames></author></authors><title>On the storage and retrieval of primes and other random numbers using
  n-dimensional geometry</title><categories>cs.CG</categories><comments>15 pages, 2 figures. arXiv admin note: substantial text overlap with
  arXiv:1509.08742</comments><acm-class>F.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that if you represent all primes with less than n-digits as points in
n-dimensional space, then they can be stored and retrieved conveniently using
n-dimensional geometry. Also once you have calculated all the prime numbers
less than n digits, it is very easy to find out if a given number having less
than n-digits is or is not a prime. We do this by separating all the primes
which are represented by points in n-dimension space by planes. It so turns out
that the number of planes q, required to separate all the points represented by
primes less than n-digit, are very few in number. Thus we obtain a very
efficient storage and retrieval system in n-dimensional space. In addition the
storage and retieval repository has the property that when new primes are added
there is no need to start all over, we can begin where we last left off and add
the new primes in the repository and add new planes that separate them as and
when necessary. Also we can arrange matters such that the repository can begin
to accept larger primes which has more digits say n' where n' &gt; n. The
algorithm does not make use of any property of prime numbers or of integers in
general,except for the fact that any n-digit integer can be represented as a
point in n-dimension space. Therefore the method can serve to be a storage and
retrieval repository of any set of given integers, in practical cases they can
represent information. Thus the algorithm can be used to devise a very
efficient storage and retrieval system for large amounts of digital data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.08951</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.08951</id><created>2015-11-28</created><authors><author><keyname>Fernando</keyname><forenames>Basura</forenames></author><author><keyname>Gavves</keyname><forenames>Efstratios</forenames></author><author><keyname>Muselet</keyname><forenames>Damien</forenames></author><author><keyname>Tuytelaars</keyname><forenames>Tinne</forenames></author></authors><title>MidRank: Learning to rank based on subsequences</title><categories>cs.CV cs.LG</categories><comments>To appear in ICCV 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a supervised learning to rank algorithm that effectively orders
images by exploiting the structure in image sequences. Most often in the
supervised learning to rank literature, ranking is approached either by
analyzing pairs of images or by optimizing a list-wise surrogate loss function
on full sequences. In this work we propose MidRank, which learns from
moderately sized sub-sequences instead. These sub-sequences contain useful
structural ranking information that leads to better learnability during
training and better generalization during testing. By exploiting sub-sequences,
the proposed MidRank improves ranking accuracy considerably on an extensive
array of image ranking applications and datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.08952</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.08952</id><created>2015-11-28</created><authors><author><keyname>Nakashole</keyname><forenames>Ndapandula</forenames></author></authors><title>Bootstrapping Ternary Relation Extractors</title><categories>cs.CL cs.AI</categories><comments>6 pages</comments><msc-class>68T50</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Binary relation extraction methods have been widely studied in recent years.
However, few methods have been developed for higher n-ary relation extraction.
One limiting factor is the effort required to generate training data. For
binary relations, one only has to provide a few dozen pairs of entities per
relation, as training data. For ternary relations (n=3), each training instance
is a triplet of entities, placing a greater cognitive load on people. For
example, many people know that Google acquired Youtube but not the dollar
amount or the date of the acquisition and many people know that Hillary Clinton
is married to Bill Clinton by not the location or date of their wedding. This
makes higher n-nary training data generation a time consuming exercise in
searching the Web. We present a resource for training ternary relation
extractors. This was generated using a minimally supervised yet effective
approach. We present statistics on the size and the quality of the dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.08956</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.08956</id><created>2015-11-28</created><authors><author><keyname>Akhtar</keyname><forenames>Naveed</forenames></author><author><keyname>Shafait</keyname><forenames>Faisal</forenames></author><author><keyname>Mian</keyname><forenames>Ajmal</forenames></author></authors><title>Sparseness helps: Sparsity Augmented Collaborative Representation for
  Classification</title><categories>cs.CV</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many classification approaches first represent a test sample using the
training samples of all the classes. This collaborative representation is then
used to label the test sample. It was a common belief that sparseness of the
representation is the key to success for this classification scheme. However,
more recently, it has been claimed that it is the collaboration and not the
sparseness that makes the scheme effective. This claim is attractive as it
allows to relinquish the computationally expensive sparsity constraint over the
representation. In this paper, we first extend the analysis supporting this
claim and then show that sparseness explicitly contributes to improved
classification, hence it should not be completely ignored for computational
gains. Inspired by this result, we augment a dense collaborative representation
with a sparse representation and propose an efficient classification method
that capitalizes on the resulting representation. The augmented representation
and the classification method work together meticulously to achieve higher
accuracy and lower computational time compared to state-of-the-art
collaborative representation based classification approaches. Experiments on
benchmark face, object and action databases show the efficacy of our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.08957</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.08957</id><created>2015-11-28</created><authors><author><keyname>Farsad</keyname><forenames>Nariman</forenames></author><author><keyname>Goldsmith</keyname><forenames>Andrea</forenames></author></authors><title>A Novel Molecular Communication System Using Acids, Bases and Hydrogen
  Ions</title><categories>cs.ET cs.IT cs.SY math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Concentration modulation, whereby information is encoded in the concentration
level of chemicals, is considered. One of the main challenges with such systems
is the limited control the transmitter has on the concentration level at the
receiver. For example, concentration cannot be directly decreased by the
transmitter, and the decrease in concentration over time occurs solely due to
transport mechanisms such as diffusion. This can result in inter-symbol
interference (ISI), which can have degrading effects on performance. In this
work, a new and novel scheme is proposed that uses the transmission of acids,
bases, and the concentration of hydrogen ions for carrying information. By
employing this technique, the concentration of hydrogen ions at the receiver
can be both increased and decreased through the sender's transmissions. This
enables novel ISI mitigation schemes as well as the possibility to form a wider
array of signal patterns at the receiver.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.08963</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.08963</id><created>2015-11-28</created><updated>2015-12-14</updated><authors><author><keyname>Aragam</keyname><forenames>Bryon</forenames></author><author><keyname>Amini</keyname><forenames>Arash A.</forenames></author><author><keyname>Zhou</keyname><forenames>Qing</forenames></author></authors><title>Learning Directed Acyclic Graphs with Penalized Neighbourhood Regression</title><categories>math.ST cs.LG stat.ML stat.TH</categories><comments>46 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of estimating a directed acyclic graph (DAG) for a
multivariate normal distribution from high-dimensional data with $p\gg n$. Our
main results establish nonasymptotic deviation bounds on the estimation error,
sparsity bounds, and model selection consistency for a penalized least squares
estimator under concave regularization. The proofs rely on interpreting the
graphical model as a recursive linear structural equation model, which reduces
the estimation problem to a series of tractable neighbourhood regressions and
allows us to avoid making any assumptions regarding faithfulness. In doing so,
we provide some novel techniques for handling general nonidentifiable and
nonconvex problems. These techniques are used to guarantee uniform control over
a superexponential number of neighbourhood regression problems by exploiting
various notions of monotonicity among them. Our results apply to a wide variety
of practical situations that allow for arbitrary nondegenerate covariance
structures as well as many popular regularizers including the MCP, SCAD,
$\ell_{0}$ and $\ell_{1}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.08967</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.08967</id><created>2015-11-28</created><authors><author><keyname>Lee</keyname><forenames>Lisa</forenames></author></authors><title>Robotic Search &amp; Rescue via Online Multi-task Reinforcement Learning</title><categories>cs.AI cs.LG cs.RO</categories><comments>7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reinforcement learning (RL) is a general and well-known method that a robot
can use to learn an optimal control policy to solve a particular task. We would
like to build a versatile robot that can learn multiple tasks, but using RL for
each of them would be prohibitively expensive in terms of both time and
wear-and-tear on the robot. To remedy this problem, we use the Policy Gradient
Efficient Lifelong Learning Algorithm (PG-ELLA), an online multi-task RL
algorithm that enables the robot to efficiently learn multiple consecutive
tasks by sharing knowledge between these tasks to accelerate learning and
improve performance. We implemented and evaluated three RL methods--Q-learning,
policy gradient RL, and PG-ELLA--on a ground robot whose task is to find a
target object in an environment under different surface conditions. In this
paper, we discuss our implementations as well as present an empirical analysis
of their learning performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.08971</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.08971</id><created>2015-11-29</created><authors><author><keyname>Saxena</keyname><forenames>Akrati</forenames></author><author><keyname>Iyengar</keyname><forenames>S. R. S.</forenames></author></authors><title>Evolving Models for Meso-Scale Structures</title><categories>cs.SI physics.soc-ph</categories><comments>Published in COMSNETS 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Real world complex networks are scale free and possess meso-scale properties
like core-periphery and community structure. We study evolution of the core
over time in real world networks. This paper proposes evolving models for both
unweighted and weighted scale free networks having local and global
core-periphery as well as community structure. Network evolves using
topological growth, self growth, and weight distribution function. To validate
the correctness of proposed models, we use K-shell and S-shell decomposition
methods. Simulation results show that the generated unweighted networks follow
power law degree distribution with droop head and heavy tail. Similarly,
generated weighted networks follow degree, strength, and edge-weight power law
distributions. We further study other properties of complex networks, such as
clustering coefficient, nearest neighbor degree, and strength degree
correlation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.08972</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.08972</id><created>2015-11-29</created><updated>2015-12-14</updated><authors><author><keyname>Lu</keyname><forenames>Guanping</forenames></author><author><keyname>Qiu</keyname><forenames>Robert C.</forenames></author><author><keyname>Yu</keyname><forenames>Wenxian</forenames></author></authors><title>Uplink One-tone Filtered Multitone Modulation Transmission for Machine
  Type Communications</title><categories>cs.IT math.IT</categories><comments>This paper has been withdrawn by the author due to the paper has
  something wrong</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To accommodate current machine type communications (MTC), an uplink waveform
is proposed where MTC nodes use one carrier to transmit signal, and central
nodes demodulate different nodes' signal jointly. Furthermore, the carrier
bandwidth is variable to fit for the channels of nodes. This waveform may
reduce the hardware complexity of low cost MTC nodes, and loose the time and
frequency domain synchronization requirements of the entire system. This paper
also provides the interference analysis and complexity comparisons of proposed
scheme and orthogonal frequency division multiplexing (OFDM).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.08975</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.08975</id><created>2015-11-29</created><updated>2016-01-25</updated><authors><author><keyname>Ye</keyname><forenames>Jong Chul</forenames></author><author><keyname>Kim</keyname><forenames>Jong Min</forenames></author><author><keyname>Jin</keyname><forenames>Kyong Hwan</forenames></author><author><keyname>Lee</keyname><forenames>Kiryung</forenames></author></authors><title>Compressive Sampling using Annihilating Filter-based Low-Rank
  Interpolation</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While the recent theory of compressed sensing or compressive sampling (CS)
provides an opportunity to overcome the Nyquist limit in recovering sparse
signals, a recovery algorithm usually takes the form of penalized least squares
or constraint optimization framework that is crucially dependent on the signal
representation. In this paper, we propose a drastically different two-step
Fourier CS framework that can be implemented as a measurement domain data
interpolation, after which the image reconstruction can be done using classical
analytic reconstruction methods. The main idea is originated from the
fundamental duality between the sparsity in the primary space and the
low-rankness of a structured matrix in the reciprocal spaces, which shows that
the low-rank interpolator as a digital correction filter can enjoy all the
benefit of sparse recovery with performance guarantees. Most notably, the
proposed low-rank interpolation approach can be regarded as a generation of
recent spectral compressed sensing to recover large class of finite rate of
innovations (FRI) signals at near optimal sampling rate. Moreover, for the case
of cardinal representation, we can show that the proposed low-rank
interpolation will benefit from inherent regularization. Using the powerful
dual certificates and golfing scheme, we show that the new framework still
achieves the near-optimal sampling rate for general class of FRI signal
recovery, and the sampling rate can be further reduced for the class of
cardinal splines. Numerical results using various type of signals confirmed
that the proposed low-rank interpolation approach has significant better phase
transition than the conventional CS approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.08977</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.08977</id><created>2015-11-29</created><updated>2016-02-24</updated><authors><author><keyname>Yuan</keyname><forenames>Xiaojun</forenames></author><author><keyname>Fan</keyname><forenames>Congmin</forenames></author><author><keyname>Zhang</keyname><forenames>Ying Jun</forenames></author></authors><title>Fundamental Limits of Training-Based Multiuser MIMO Systems</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we endeavour to seek a fundamental understanding of the
potentials and limitations of training-based multiuser multiple-input
multiple-output (MIMO) systems. In a multiuser MIMO system, users are
geographically separated. So, the near-far effect plays an indispensable role
in channel fading. The existing optimal training design for conventional MIMO
does not take the near-far effect into account, and thus is not applicable to a
multiuser MIMO system. In this work, we use the majorization theory as a basic
tool to study the tradeoff between the channel estimation quality and the
information throughput. We establish tight upper and lower bounds of the
throughput, and prove that the derived lower bound is asymptotically optimal
for throughput maximization at high signal-to-noise ratio. Our analysis shows
that the optimal training sequences for throughput maximization in a multiuser
MIMO system are in general not orthogonal to each other. Furthermore, due to
the near-far effect, the optimal training design for throughput maximization is
to deactivate a portion of users with the weakest channels in transmission.
These observations shed light on the practical design of training-based
multiuser MIMO systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.08986</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.08986</id><created>2015-11-29</created><authors><author><keyname>Singh</keyname><forenames>Sukhpal</forenames></author><author><keyname>Chana</keyname><forenames>Inderveer</forenames></author><author><keyname>Buyya</keyname><forenames>Rajkumar</forenames></author></authors><title>Agri-Info: Cloud Based Autonomic System for Delivering Agriculture as a
  Service</title><categories>cs.DC</categories><comments>31 pages, 28 figures</comments><report-no>Technical Report CLOUDS-TR-2015-2, Cloud Computing and Distributed
  Systems Laboratory, The University of Melbourne, Nov. 27, 2015</report-no><acm-class>C.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud computing has emerged as an important paradigm for managing and
delivering services efficiently over the Internet. Convergence of cloud
computing with technologies such as wireless sensor networking and mobile
computing offers new applications of cloud services but this requires
management of Quality of Service (QoS) parameters to efficiently monitor and
measure the delivered services. This paper presents a QoS-aware Cloud Based
Autonomic Information System for delivering agriculture related information as
a service through the use of latest Cloud technologies which manage various
types of agriculture related data based on different domains. Proposed system
gathers information from various users through preconfigured devices and
manages and provides required information to users automatically. Further,
Cuckoo Optimization Algorithm has been used for efficient resource allocation
at infrastructure level for effective utilization of resources. We have
evaluated the performance of the proposed approach in Cloud environment and
experimental results show that the proposed system performs better in terms of
resource utilization, execution time, cost and computing capacity along with
other QoS parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.08987</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.08987</id><created>2015-11-29</created><authors><author><keyname>Udomsak</keyname><forenames>Napas</forenames></author></authors><title>How do the naive Bayes classifier and the Support Vector Machine compare
  in their ability to forecast the Stock Exchange of Thailand?</title><categories>cs.LG</categories><comments>16 pages</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This essay investigates the question of how the naive Bayes classifier and
the support vector machine compare in their ability to forecast the Stock
Exchange of Thailand. The theory behind the SVM and the naive Bayes classifier
is explored. The algorithms are trained using data from the month of January
2010, extracted from the MarketWatch.com website. Input features are selected
based on previous studies of the SET100 Index. The Weka 3 software is used to
create models from the labeled training data. Mean squared error and proportion
of correctly classified instances, and a number of other error measurements are
the used to compare the two algorithms. This essay shows that these two
algorithms are currently not advanced enough to accurately model the stock
exchange. Nevertheless, the naive Bayes is better than the support vector
machine at predicting the Stock Exchange of Thailand.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.08990</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.08990</id><created>2015-11-29</created><updated>2016-02-07</updated><authors><author><keyname>Barger</keyname><forenames>Artem</forenames></author><author><keyname>Feldman</keyname><forenames>Dan</forenames></author></authors><title>k-Means for Streaming and Distributed Big Sparse Data</title><categories>cs.DS</categories><comments>16 pages, 44 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide the first streaming algorithm for computing a provable
approximation to the $k$-means of sparse Big data. Here, sparse Big Data is a
set of $n$ vectors in $\mathbb{R}^d$, where each vector has $O(1)$ non-zeroes
entries, and $d\geq n$. E.g., adjacency matrix of a graph, web-links, social
network, document-terms, or image-features matrices.
  Our streaming algorithm stores at most $\log n\cdot k^{O(1)}$ input points in
memory. If the stream is distributed among $M$ machines, the running time
reduces by a factor of $M$, while communicating a total of $M\cdot k^{O(1)}$
(sparse) input points between the machines.
  % Our main technical result is a deterministic algorithm for computing a
sparse $(k,\epsilon)$-coreset, which is a weighted subset of $k^{O(1)}$ input
points that approximates the sum of squared distances from the $n$ input points
to every $k$ centers, up to $(1\pm\epsilon)$ factor, for any given constant
$\epsilon&gt;0$. This is the first such coreset of size independent of both $d$
and $n$.
  Existing algorithms use coresets of size at least polynomial in $d$, or
project the input points on a subspace which diminishes their sparsity, thus
require memory and communication $\Omega(d)=\Omega(n)$ even for $k=2$.
  Experimental results real public datasets shows that our algorithm boost the
performance of such given heuristics even in the off-line setting. Open code is
provided for reproducibility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.08996</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.08996</id><created>2015-11-29</created><authors><author><keyname>Zhang</keyname><forenames>Yi</forenames></author><author><keyname>Xiao</keyname><forenames>Yanghua</forenames></author><author><keyname>Hwang</keyname><forenames>Seung-won</forenames></author><author><keyname>Wang</keyname><forenames>Haixun</forenames></author><author><keyname>Wang</keyname><forenames>X. Sean</forenames></author><author><keyname>Wang</keyname><forenames>Wei</forenames></author></authors><title>Entity Suggestion by Example using a Conceptual Taxonomy</title><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Entity suggestion by example (ESbE) refers to a type of entity acquisition
query in which a user provides a set of example entities as the query and
obtains in return some entities that best complete the concept underlying the
given query. Such entity acquisition queries can be useful in many applications
such as related-entity recommendation and query expansion. A number of ESbE
query processing solutions exist in the literature. However, they mostly build
only on the idea of entity co-occurrences either in text or web lists, without
taking advantage of the existence of many web-scale conceptual taxonomies that
consist of hierarchical isA relationships between entity-concept pairs. This
paper provides a query processing method based on the relevance models between
entity sets and concepts. These relevance models can be used to obtain the
fine-grained concepts implied by the query entity set, and the entities that
belong to a given concept, thereby providing the entity suggestions. Extensive
evaluations with real data sets show that the accuracy of the queries processed
with this new method is significantly higher than that of existing solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.08999</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.08999</id><created>2015-11-29</created><authors><author><keyname>Sturm</keyname><forenames>Thomas</forenames></author><author><keyname>Voigt</keyname><forenames>Marco</forenames></author><author><keyname>Weidenbach</keyname><forenames>Christoph</forenames></author></authors><title>Separation of Quantified First-Order Variables entails Decidability</title><categories>cs.LO</categories><comments>Preliminary draft, 8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a fragment of first-order logic with equality which strictly
generalizes two already well-known ones -- the Bernays-Sch\&quot;onfinkel-Ramsey
fragment (BSR) and the Monadic fragment. Satisfiability remains decidable in
the new fragment. The defining principle is the separation of universally
quantified variables from existentially quantified ones on the level of atoms.
Thus, our classification neither rests on restrictions of quantifier prefixes
(as in the BSR case) nor on restrictions with respect to the arity of the
occurring predicate symbols (as in the monadic case).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09009</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09009</id><created>2015-11-29</created><authors><author><keyname>Zhang</keyname><forenames>Yi</forenames></author><author><keyname>Xiao</keyname><forenames>Yanghua</forenames></author><author><keyname>Hwang</keyname><forenames>Seung-won</forenames></author><author><keyname>Wang</keyname><forenames>Wei</forenames></author></authors><title>Long Concept Query on Conceptual Taxonomies</title><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the problem of finding typical entities when the concept
is given as a query. For a short concept such as university, this is a
well-studied problem of retrieving knowledge base such as Microsoft's Probase
and Google's isA database pre-materializing entities found for the concept in
Hearst patterns of the web corpus. However, we find most real-life queries are
long concept queries (LCQs), such as top American private university, which
cannot and should not be pre-materialized. Our goal is an online construction
of entity retrieval for LCQs. We argue a naive baseline of rewriting LCQs into
an intersection of an expanded set of composing short concepts leads to highly
precise results with extremely low recall. Instead, we propose to augment the
concept list, by identifying related concepts of the query concept. However, as
such increase of recall often invites false positives and decreases precision
in return, we propose the following two techniques: First, we identify concepts
with different relatedness to generate linear orderings and pairwise ordering
constraints. Second, we rank entities trying to avoid conflicts with these
constraints, to prune out lowly ranked one (likely false positives). With these
novel techniques, our approach significantly outperforms state-of-the-arts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09011</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09011</id><created>2015-11-29</created><authors><author><keyname>Pierron</keyname><forenames>Th&#xe9;o</forenames></author><author><keyname>Place</keyname><forenames>Thomas</forenames></author><author><keyname>Zeitoun</keyname><forenames>Marc</forenames></author></authors><title>Quantifier Alternation for Infinite Words</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the expressive power of quantifier alternation hierarchy of
first-order logic over words. This hierarchy includes the classes ${\Sigma}_i$
(sentences having at most $i$ blocks of quantifiers starting with an $\exists$)
and $\mathcal{B}{\Sigma}_i$ (Boolean combinations of ${\Sigma}_i$ sentences).
So far, this expressive power has been effectively characterized for the lower
levels only. Recently, a breakthrough was made over finite words, and decidable
characterizations were obtained for $\mathcal{B}{\Sigma}_2$ and ${\Sigma}_3$,
by relying on a decision problem called separation, and solving it for
${\Sigma}_2$. The contribution of this paper is a generalization of these
results to the setting of infinite words: we solve separation for ${\Sigma}_2$
and ${\Sigma}_3$, and obtain decidable characterizations of
$\mathcal{B}{\Sigma}_2$ and ${\Sigma}_3$ as consequences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09013</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09013</id><created>2015-11-29</created><authors><author><keyname>Bao</keyname><forenames>Hengyao</forenames></author><author><keyname>Fang</keyname><forenames>Jun</forenames></author><author><keyname>Chen</keyname><forenames>Zhi</forenames></author><author><keyname>Li</keyname><forenames>Hongbin</forenames></author><author><keyname>Li</keyname><forenames>Shaoqian</forenames></author></authors><title>An Efficient Bayesian PAPR Reduction Method for OFDM-Based Massive MIMO
  Systems</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of peak-to-average power ratio (PAPR) reduction in
orthogonal frequency-division multiplexing (OFDM) based massive multiple-input
multiple-output (MIMO) downlink systems. Specifically, given a set of symbol
vectors to be transmitted to K users, the problem is to find an OFDM-modulated
signal that has a low PAPR and meanwhile enables multiuser interference (MUI)
cancellation. Unlike previous works that tackled the problem using convex
optimization, we take a Bayesian approach and develop an efficient PAPR
reduction method by exploiting the redundant degrees-of-freedom of the transmit
array. The sought-after signal is treated as a random vector with a
hierarchical truncated Gaussian mixture prior, which has the potential to
encourage a low PAPR signal with most of its samples concentrated on the
boundaries. A variational expectation-maximization (EM) strategy is developed
to obtain estimates of the hyperparameters associated with the prior model,
along with the signal. In addition, the generalized approximate message passing
(GAMP) is embedded into the variational EM framework, which results in a
significant reduction in computational complexity of the proposed algorithm.
Simulation results show our proposed algorithm achieves a substantial
performance improvement over existing methods in terms of both the PAPR
reduction and computational complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09021</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09021</id><created>2015-11-29</created><updated>2016-02-04</updated><authors><author><keyname>Lages</keyname><forenames>Jos&#xe9;</forenames></author><author><keyname>Patt</keyname><forenames>Antoine</forenames></author><author><keyname>Shepelyansky</keyname><forenames>Dima L.</forenames></author></authors><title>Wikipedia Ranking of World Universities</title><categories>cs.SI cs.CY physics.soc-ph</categories><comments>13 pages, 17 figures, 7 tables. Accepted for publication in Eur.
  Phys. J. B. Supporting information is available at
  http://perso.utinam.cnrs.fr/~lages/datasets/WRWU/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We use the directed networks between articles of 24 Wikipedia language
editions for producing the Wikipedia Ranking of World Universities (WRWU) using
PageRank, 2DRank and CheiRank algorithms. This approach allows to incorporate
various cultural views on world universities using the mathematical statistical
analysis independent of cultural preferences. The Wikipedia ranking of top 100
universities provides about 60 percent overlap with the Shanghai university
ranking demonstrating the reliable features of this approach. At the same time
WRWU incorporates all knowledge accumulated at 24 Wikipedia editions giving
stronger highlights for historically important universities leading to a
different estimation of efficiency of world countries in university education.
The historical development of university ranking is analyzed during ten
centuries of their history.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09024</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09024</id><created>2015-11-29</created><authors><author><keyname>Fan</keyname><forenames>Congmin</forenames></author><author><keyname>Yuan</keyname><forenames>Xiaojun</forenames></author><author><keyname>Zhang</keyname><forenames>Ying Jun</forenames></author></authors><title>Scalable Uplink Signal Detection in C-RANs via Randomized Gaussian
  Message Passing with Channel Sparsification</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Cloud Radio Access Network (C-RAN), the high computational complexity of
signal processing becomes unaffordable due to the large number of RRHs and
users. This paper endeavours to use message passing to design a scalable uplink
signal detection algorithm, in the sense that the complexity grows linear with
the network size. However, message passing cannot be directly applied in C-RAN
as its computational complexity is very high and the convergence cannot be
guaranteed. In this paper, we propose a randomized Gaussian message passing
(RGMP) algorithm with channel sparsification to reduce the complexity and to
improve the convergence. We first sparsify the channel matrix based on a
distance threshold. In this way, messages only need to be exchanged among
nearby users and remote radio heads (RRHs). This leads to a linear
computational complexity with the number of RRHs and users. Then, we introduce
the RGMP algorithm to improve the convergence of message passing. Instead of
exchanging messages simultaneously or in a fixed order, we propose to exchange
messages in a random order. Numerical results show that the proposed RGMP
algorithm has better convergence performance than conventional message passing.
The randomness of the message update schedule also simplifies the analysis,
which allows us to derive some convergence conditions for the RGMP algorithm.
Besides analysis, we also compare the convergence rate of RGMP with existing
low-complexity algorithms through extensive simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09030</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09030</id><created>2015-11-29</created><authors><author><keyname>Thoma</keyname><forenames>Martin</forenames></author></authors><title>On-line Recognition of Handwritten Mathematical Symbols</title><categories>cs.CV</categories><doi>10.5445/IR/1000048047</doi><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Finding the name of an unknown symbol is often hard, but writing the symbol
is easy. This bachelor's thesis presents multiple systems that use the pen
trajectory to classify handwritten symbols. Five preprocessing steps, one data
augmentation algorithm, five features and five variants for multilayer
Perceptron training were evaluated using 166898 recordings which were collected
with two crowdsourcing projects. The evaluation results of these 21 experiments
were used to create an optimized recognizer which has a TOP1 error of less than
17.5% and a TOP3 error of 4.0%. This is an improvement of 18.5% for the TOP1
error and 29.7% for the TOP3 error.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09033</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09033</id><created>2015-11-29</created><updated>2015-12-22</updated><authors><author><keyname>Littwin</keyname><forenames>Etai</forenames></author><author><keyname>Wolf</keyname><forenames>Lior</forenames></author></authors><title>The Multiverse Loss for Robust Transfer Learning</title><categories>cs.CV</categories><comments>In the second version, whitening was applied in the CIFAR-100
  experiments in order to improve results. Figure 2 in [v1] had a duplicate
  subfigure which is now fixed</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep learning techniques are renowned for supporting effective transfer
learning. However, as we demonstrate, the transferred representations support
only a few modes of separation and much of its dimensionality is unutilized. In
this work, we suggest to learn, in the source domain, multiple orthogonal
classifiers. We prove that this leads to a reduced rank representation, which,
however, supports more discriminative directions. Interestingly, the softmax
probabilities produced by the multiple classifiers are likely to be identical.
Experimental results, on CIFAR-100 and LFW, further demonstrate the
effectiveness of our method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09044</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09044</id><created>2015-11-29</created><authors><author><keyname>Vadidpour</keyname><forenames>Vahid</forenames></author><author><keyname>Rastegarnia</keyname><forenames>Amir</forenames></author><author><keyname>Khalili</keyname><forenames>Azam</forenames></author><author><keyname>Sanei</keyname><forenames>Saeid</forenames></author></authors><title>Partial-Diffusion Least Mean-Square Estimation Over Networks Under Noisy
  Information Exchange</title><categories>cs.SY</categories><comments>6pages, 4 Figures, Submitted to ICEE 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Partial diffusion scheme is an effective method for reducing computational
load and power consumption in adaptive network implementation. The Information
is exchanged among the nodes, usually over noisy links. In this paper, we
consider a general version of partial-diffusion least-mean-square (PDLMS)
algorithm in the presence of various sources of imperfect information
exchanges. Like the established PDLMS, we consider two different schemes to
select the entries, sequential and stochastic, for transmission at each
iteration. Our objective is to analyze the aggregate effect of these
perturbations on general PDLMS strategies. Simulation results demonstrate that
considering noisy link assumption adds a new complexity to the related
optimization problem and the trade-off between communication cost and
estimation performance in comparison to ideal case becomes unbalanced.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09047</identifier>
 <datestamp>2016-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09047</id><created>2015-11-29</created><updated>2016-02-11</updated><authors><author><keyname>Scharpff</keyname><forenames>Joris</forenames></author><author><keyname>Roijers</keyname><forenames>Diederik M.</forenames></author><author><keyname>Oliehoek</keyname><forenames>Frans A.</forenames></author><author><keyname>Spaan</keyname><forenames>Matthijs T. J.</forenames></author><author><keyname>de Weerdt</keyname><forenames>Mathijs M.</forenames></author></authors><title>Solving Transition-Independent Multi-agent MDPs with Sparse Interactions
  (Extended version)</title><categories>cs.AI cs.MA</categories><comments>This article is an extended version of the paper that was published
  under the same title in the Proceedings of the Thirtieth AAAI Conference on
  Artificial Intelligence (AAAI16), held in Phoenix, Arizona USA on February
  12-17, 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In cooperative multi-agent sequential decision making under uncertainty,
agents must coordinate to find an optimal joint policy that maximises joint
value. Typical algorithms exploit additive structure in the value function, but
in the fully-observable multi-agent MDP setting (MMDP) such structure is not
present. We propose a new optimal solver for transition-independent MMDPs, in
which agents can only affect their own state but their reward depends on joint
transitions. We represent these dependencies compactly in conditional return
graphs (CRGs). Using CRGs the value of a joint policy and the bounds on
partially specified joint policies can be efficiently computed. We propose
CoRe, a novel branch-and-bound policy search algorithm building on CRGs. CoRe
typically requires less runtime than the available alternatives and finds
solutions to problems previously unsolvable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09050</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09050</id><created>2015-11-29</created><authors><author><keyname>Saxena</keyname><forenames>Akrati</forenames></author><author><keyname>Malik</keyname><forenames>Vaibhav</forenames></author><author><keyname>Iyengar</keyname><forenames>S. R. S.</forenames></author></authors><title>Rank me thou shalln't Compare me</title><categories>cs.SI</categories><comments>arXiv admin note: text overlap with arXiv:1511.05732</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Centrality measures are used to quantify the importance of a node in a given
complex network. In the present work, we calculate the degree centrality
ranking of a node without having entire network. The proposed method uses
degree centrality value of the node and few network parameters to calculate
ranking of a node. These network parameters are total number of nodes, minimum,
maximum and average degree of the network. These parameters are calculated
using sampling method. The proposed method is validated on Barabasi-Albert
model. Simulation results show that the method gives good performance for
higher degree nodes and it is decreased for lower degrees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09058</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09058</id><created>2015-11-29</created><updated>2015-12-02</updated><authors><author><keyname>Malyshkin</keyname><forenames>Vladislav Gennadievich</forenames></author></authors><title>Multiple-Instance Learning: Radon-Nikodym Approach to Distribution
  Regression Problem</title><categories>cs.LG</categories><comments>Gramar fixes. Off by one error in eigenvalues problem fixed</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For distribution regression problem, where a bag of $x$--observations is
mapped to a single $y$ value, a one--step solution is proposed. The problem of
random distribution to random value is transformed to random vector to random
value by taking distribution moments of $x$ observations in a bag as random
vector. Then Radon--Nikodym or least squares theory can be applied, what give
$y(x)$ estimator. The probability distribution of $y$ is also obtained, what
requires solving generalized eigenvalues problem, matrix spectrum (not
depending on $x$) give possible $y$ outcomes and depending on $x$ probabilities
of outcomes can be obtained by projecting the distribution with fixed $x$ value
(delta--function) to corresponding eigenvector. A library providing numerically
stable polynomial basis for these calculations is available, what make the
proposed approach practical.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09059</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09059</id><created>2015-11-29</created><authors><author><keyname>Shamdasani</keyname><forenames>Jetendr</forenames></author><author><keyname>McClatchey</keyname><forenames>Richard</forenames></author><author><keyname>Branson</keyname><forenames>Andrew</forenames></author><author><keyname>Kovacs</keyname><forenames>Zsolt</forenames></author></authors><title>Analysis Traceability and Provenance for HEP</title><categories>cs.DB</categories><comments>8 pagesd, 4 figures. Presented at 21st Int Conf on Computing in High
  Energy and Nuclear Physics (CHEP15). Okinawa, Japan. April 2015</comments><doi>10.1088/1742-6596/664/3/032028</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the use of the CRISTAL software in the N4U project.
CRISTAL was used to create a set of provenance aware analysis tools for the
Neuroscience domain. This paper advocates that the approach taken in N4U to
build the analysis suite is sufficiently generic to be able to be applied to
the HEP domain. A mapping to the PROV model for provenance interoperability is
also presented and how this can be applied to the HEP domain for the
interoperability of HEP analyses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09061</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09061</id><created>2015-11-29</created><authors><author><keyname>Hasham</keyname><forenames>Khawar</forenames></author><author><keyname>Munir</keyname><forenames>Kamran</forenames></author><author><keyname>McClatchey</keyname><forenames>Richard</forenames></author></authors><title>Using Cloud-Aware Provenance to Reproduce Scientific Workflow Execution
  on Cloud</title><categories>cs.DB</categories><comments>10 pages, 5 figures, 1 table. Proc of the 5th International
  Conference on Cloud Computing and Services Science (CLOSER) Lisbon MAy 2015.
  arXiv admin note: substantial text overlap with arXiv:1502.01539</comments><doi>10.5220/0005452800490059</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Provenance has been thought of a mechanism to verify a workflow and to
provide workflow reproducibility. This provenance of scientific workflows has
been effectively carried out in Grid based scientific workflow systems.
However, recent adoption of Cloud-based scientific workflows present an
opportunity to investigate the suitability of existing approaches or propose
new approaches to collect provenance information from the Cloud and to utilize
it for workflow repeatability in the Cloud infrastructure. This paper presents
a novel approach that can assist in mitigating this challenge. This approach
can collect Cloud infrastructure information from an outside Cloud client along
with workflow provenance and can establish a mapping between them. This mapping
is later used to re-provision resources on the Cloud for workflow execution.
The reproducibility of the workflow execution is performed by: (a) capturing
the Cloud infrastructure information (virtual machine configuration) along with
the workflow provenance, (b) re-provisioning the similar resources on the Cloud
and re-executing the workflow on them and (c) by comparing the outputs of
workflows. The evaluation of the prototype suggests that the proposed approach
is feasible and can be investigated further. Moreover, there is no reference
reproducibility model exists in literature that can provide guidelines to
achieve this goal in Cloud. This paper also attempts to present a model that is
used in the proposed design to achieve workflow reproducibility in the Cloud
environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09065</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09065</id><created>2015-11-29</created><authors><author><keyname>McClatchey</keyname><forenames>Richard</forenames></author><author><keyname>Shamdasani</keyname><forenames>Jetendr</forenames></author><author><keyname>Branson</keyname><forenames>Andrew</forenames></author><author><keyname>Munir</keyname><forenames>Kamran</forenames></author><author><keyname>Kovacs</keyname><forenames>Zsolt</forenames></author></authors><title>Traceability and Provenance in Big Data Medical Systems</title><categories>cs.DB</categories><comments>6 pages, 3 diagrams. Proc of the 28th Int Symposium on Computer-Based
  Medical Systems (CBMS 2015) Sao Carlos, Brazil. June 2015. arXiv admin note:
  text overlap with arXiv:1502.01545</comments><doi>10.1109/CBMS.2015.10</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Providing an appropriate level of accessibility to and tracking of data or
process elements in large volumes of medical data, is an essential requirement
in the Big Data era. Researchers require systems that provide traceability of
information through provenance data capture and management to support their
clinical analyses. We present an approach that has been adopted in the neuGRID
and N4U projects, which aimed to provide detailed traceability to support
research analysis processes in the study of biomarkers for Alzheimers disease,
but is generically applicable across medical systems. To facilitate the
orchestration of complex, large-scale analyses in these projects we have
adapted CRISTAL, a workflow and provenance tracking solution. The use of
CRISTAL has provided a rich environment for neuroscientists to track and manage
the evolution of data and workflow usage over time in neuGRID and N4U.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09066</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09066</id><created>2015-11-29</created><authors><author><keyname>Munir</keyname><forenames>Kamran</forenames></author><author><keyname>Ahmad</keyname><forenames>Khawar Hasham</forenames></author><author><keyname>McClatchey</keyname><forenames>Richard</forenames></author></authors><title>Development of a Large-scale Neuroimages and Clinical Variables Data
  Atlas in the neuGRID4You (N4U) project</title><categories>cs.DB</categories><comments>35 pages, 15 figures, Journal of Biomedical Informatics, 2015</comments><doi>10.1016/j.jbi.2015.08.004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Exceptional growth in the availability of large-scale clinical imaging
datasets has led to the development of computational infrastructures offering
scientists access to image repositories and associated clinical variables data.
The EU FP7 neuGRID and its follow on neuGRID4You (N4U) project is a leading
e-Infrastructure where neuroscientists can find core services and resources for
brain image analysis. The core component of this e-Infrastructure is the N4U
Virtual Laboratory, which offers an easy access for neuroscientists to a wide
range of datasets and algorithms, pipelines, computational resources, services,
and associated support services. The foundation of this virtual laboratory is a
massive data store plus information services called the Data Atlas that stores
datasets, clinical study data, data dictionaries, algorithm/pipeline
definitions, and provides interfaces for parameterised querying so that
neuroscientists can perform analyses on required datasets. This paper presents
the overall design and development of the Data Atlas, its associated datasets
and indexing and a set of retrieval services that originated from the
development of the N4U Virtual Laboratory in the EU FP7 N4U project in the
light of user requirements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09067</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09067</id><created>2015-11-29</created><authors><author><keyname>Elawady</keyname><forenames>Mohamed</forenames></author></authors><title>Sparse Coral Classification Using Deep Convolutional Neural Networks</title><categories>cs.CV</categories><comments>Thesis Submitted for the Degree of MSc Erasmus Mundus in Vision and
  Robotics (VIBOT 2014)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Autonomous repair of deep-sea coral reefs is a recent proposed idea to
support the oceans ecosystem in which is vital for commercial fishing, tourism
and other species. This idea can be operated through using many small
autonomous underwater vehicles (AUVs) and swarm intelligence techniques to
locate and replace chunks of coral which have been broken off, thus enabling
re-growth and maintaining the habitat. The aim of this project is developing
machine vision algorithms to enable an underwater robot to locate a coral reef
and a chunk of coral on the seabed and prompt the robot to pick it up. Although
there is no literature on this particular problem, related work on fish
counting may give some insight into the problem. The technical challenges are
principally due to the potential lack of clarity of the water and platform
stabilization as well as spurious artifacts (rocks, fish, and crabs). We
present an efficient sparse classification for coral species using supervised
deep learning method called Convolutional Neural Networks (CNNs). We compute
Weber Local Descriptor (WLD), Phase Congruency (PC), and Zero Component
Analysis (ZCA) Whitening to extract shape and texture feature descriptors,
which are employed to be supplementary channels (feature-based maps) besides
basic spatial color channels (spatial-based maps) of coral input image, we also
experiment state-of-art preprocessing underwater algorithms for image
enhancement and color normalization and color conversion adjustment. Our
proposed coral classification method is developed under MATLAB platform, and
evaluated by two different coral datasets (University of California San Diego's
Moorea Labeled Corals, and Heriot-Watt University's Atlantic Deep Sea).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09070</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09070</id><created>2015-11-29</created><authors><author><keyname>Chen</keyname><forenames>Yanling</forenames></author><author><keyname>Koyluoglu</keyname><forenames>O. Ozan</forenames></author><author><keyname>Sezgin</keyname><forenames>Aydin</forenames></author></authors><title>Individual Secrecy for the Broadcast Channel</title><categories>cs.IT math.IT</categories><comments>49 pages, 13 figures, this paper was presented in part at IEEE
  International Symposium on Information Theory, Hong Kong, Jun. 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the problem of secure communication over broadcast
channels under the individual secrecy constraints. That is, the transmitter
wants to send two independent messages to two legitimate receivers in the
presence of an eavesdropper, while keeping the eavesdropper ignorant of each
message (i.e., the information leakage from each message to the eavesdropper is
made vanishing). Building upon Carleial-Hellman's secrecy coding, Wyner's
secrecy coding, the frameworks of superposition coding and Marton's coding
together with techniques such as rate splitting and indirect decoding,
achievable rate regions are developed. The proposed regions are compared with
those satisfying joint secrecy and without secrecy constraints, and the
individual secrecy capacity regions for special cases are characterized. In
particular, capacity region for the deterministic case is established, and for
the Gaussian model, a constant gap (i.e., 0.5 bits within the individual
secrecy capacity region) result is obtained. Overall, when compared with the
joint secrecy constraint, the results allow for trading-off secrecy level and
throughput in the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09072</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09072</id><created>2015-11-29</created><authors><author><keyname>Srimani</keyname><forenames>Tathagata</forenames></author><author><keyname>Manna</keyname><forenames>Bibhas</forenames></author><author><keyname>Mukhopadhyay</keyname><forenames>Anand Kumar</forenames></author><author><keyname>Roy</keyname><forenames>Kaushik</forenames></author><author><keyname>Sharad</keyname><forenames>Mrigank</forenames></author></authors><title>High Sensitivity Biosensor using Injection Locked Spin Torque
  Nano-Oscillators</title><categories>cs.ET</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With ever increasing research on magnetic nano systems it is shown to have
great potential in the areas of magnetic storage, biosensing, magnetoresistive
insulation etc. In the field of biosensing specifically Spin Valve sensors
coupled with Magnetic Nanolabels is showing great promise due to noise immunity
and energy efficiency [1]. In this paper we present the application of
injection locked based Spin Torque Nano Oscillator (STNO) suitable for high
resolution energy efficient labeled DNA Detection. The proposed STNO microarray
consists of 20 such devices oscillating at different frequencies making it
possible to multiplex all the signals using capacitive coupling. Frequency
Division Multiplexing can be aided with Time division multiplexing to increase
the device integration and decrease the readout time while maintaining the same
efficiency in presence of constant input referred noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09074</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09074</id><created>2015-11-29</created><authors><author><keyname>Ahasan</keyname><forenames>Sohail</forenames></author><author><keyname>Maji</keyname><forenames>Saurav</forenames></author><author><keyname>Roy</keyname><forenames>Kauhsik</forenames></author><author><keyname>Sharad</keyname><forenames>Mrigank</forenames></author></authors><title>Digital LDO with Time-Interleaved Comparators for Fast Response and Low
  Ripple</title><categories>cs.AR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  On-chip voltage regulation using distributed Digital Low Drop Out (LDO)
voltage regulators has been identified as a promising technique for efficient
power-management for emerging multi-core processors. Digital LDOs (DLDO) can
offer low voltage operation, faster transient response, and higher current
efficiency. Response time as well as output voltage ripple can be reduced by
increasing the speed of the dynamic comparators. However, the comparator offset
steeply increases for high clock frequencies, thereby leading to enhanced
variations in output voltage. In this work we explore the design of digital
LDOs with multiple dynamic comparators that can overcome this bottleneck. In
the proposed topology, we apply time-interleaved comparators with the same
voltage threshold and uniform current step in order to accomplish the
aforementioned features. Simulation based analysis shows that the DLDO with
time-interleaved comparators can achieve better overall performance in terms of
current efficiency, ripple and settling time. For a load step of 50mA, a DLDO
with 8 time-interleaved comparators could achieve an output ripple of less than
5mV, while achieving a settling time of less than 0.5us. Load current dependant
dynamic adjustment of clock frequency is proposed to maintain high current
efficiency of ~97%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09079</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09079</id><created>2015-11-29</created><authors><author><keyname>Gusev</keyname><forenames>Vladimir V.</forenames></author><author><keyname>Pribavkina</keyname><forenames>Elena V.</forenames></author></authors><title>Synchronizing automata and principal eigenvectors of the underlying
  digraphs</title><categories>cs.FL</categories><comments>11 pages, preliminary version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A coloring of a digraph with a fixed out-degree k is a distribution of k
labels over the edges resulting in a deterministic finite automaton. An
automaton is called synchronizing if there exists a word which sends all states
of the automaton to a single state. In the present paper we study connections
between spectral and synchronizing properties of digraphs. We show that if a
coloring of a digraph is not synchronizing, then the stationary distribution of
an associated Markov chain has a partition of coordinates into blocks of equal
sum. Moreover, if there exists such a partition, then there exists a
non-synchronizing automaton with such stationary distribution. We extend these
results to bound the number of non-synchronizing colorings for digraphs with
particular eigenvectors. We also demonstrate that the length of the shortest
synchronizing word of any coloring is at most $w^2 - 3w + 3$, where $w$ is the
sum of the coordinates of the integer principal eigenvector of the digraph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09080</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09080</id><created>2015-11-29</created><updated>2016-02-20</updated><authors><author><keyname>Robbel</keyname><forenames>Philipp</forenames></author><author><keyname>Oliehoek</keyname><forenames>Frans A.</forenames></author><author><keyname>Kochenderfer</keyname><forenames>Mykel J.</forenames></author></authors><title>Exploiting Anonymity in Approximate Linear Programming: Scaling to Large
  Multiagent MDPs (Extended Version)</title><categories>cs.AI cs.MA</categories><comments>Extended version of AAAI 2016 paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many exact and approximate solution methods for Markov Decision Processes
(MDPs) attempt to exploit structure in the problem and are based on
factorization of the value function. Especially multiagent settings, however,
are known to suffer from an exponential increase in value component sizes as
interactions become denser, meaning that approximation architectures are
restricted in the problem sizes and types they can handle. We present an
approach to mitigate this limitation for certain types of multiagent systems,
exploiting a property that can be thought of as &quot;anonymous influence&quot; in the
factored MDP. Anonymous influence summarizes joint variable effects efficiently
whenever the explicit representation of variable identity in the problem can be
avoided. We show how representational benefits from anonymity translate into
computational efficiencies, both for general variable elimination in a factor
graph but in particular also for the approximate linear programming solution to
factored MDPs. The latter allows to scale linear programming to factored MDPs
that were previously unsolvable. Our results are shown for the control of a
stochastic disease process over a densely connected graph with 50 nodes and 25
agents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09085</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09085</id><created>2015-11-29</created><updated>2015-12-01</updated><authors><author><keyname>Goswamy</keyname><forenames>Aranya</forenames></author><author><keyname>Kumashi</keyname><forenames>Sagar</forenames></author><author><keyname>Sehwag</keyname><forenames>Vikash</forenames></author><author><keyname>Singh</keyname><forenames>Siddharth Kumar</forenames></author><author><keyname>Jain</keyname><forenames>Manny</forenames></author><author><keyname>Roy</keyname><forenames>Kaushik</forenames></author><author><keyname>Sharad</keyname><forenames>Mrigank</forenames></author></authors><title>Energy Efficient and High Performance Current-Mode Neural Network
  Circuit using Memristors and Digitally Assisted Analog CMOS Neurons</title><categories>cs.ET cs.AR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Emerging nano-scale programmable Resistive-RAM (RRAM) has been identified as
a promising technology for implementing brain-inspired computing hardware.
Several neural network architectures, that essentially involve computation of
scalar products between input data vectors and stored network weights can be
efficiently implemented using high density cross-bar arrays of RRAM integrated
with CMOS. In such a design, the CMOS interface may be responsible for
providing input excitations and for processing the RRAM output. In order to
achieve high energy efficiency along with high integration density in RRAM
based neuromorphic hardware, the design of RRAM-CMOS interface can therefore
play a major role. In this work we propose design of high performance, current
mode CMOS interface for RRAM based neural network design. The use of current
mode excitation for input interface and design of digitally assisted
current-mode CMOS neuron circuit for the output interface is presented. The
proposed technique achieve 10x energy as well as performance improvement over
conventional approaches employed in literature. Network level simulations show
that the proposed scheme can achieve 2 orders of magnitude lower energy
dissipation as compared to a digital ASIC implementation of a feed-forward
neural network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09099</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09099</id><created>2015-11-29</created><authors><author><keyname>Bianchi</keyname><forenames>Filippo Maria</forenames></author><author><keyname>De Santis</keyname><forenames>Enrico</forenames></author><author><keyname>Montazeri</keyname><forenames>Hedieh</forenames></author><author><keyname>Naraei</keyname><forenames>Parisa</forenames></author><author><keyname>Sadeghian</keyname><forenames>Alireza</forenames></author></authors><title>Position paper: a general framework for applying machine learning
  techniques in operating room</title><categories>cs.CY cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this position paper we describe a general framework for applying machine
learning and pattern recognition techniques in healthcare. In particular, we
are interested in providing an automated tool for monitoring and incrementing
the level of awareness in the operating room and for identifying human errors
which occur during the laparoscopy surgical operation. The framework that we
present is divided in three different layers: each layer implements algorithms
which have an increasing level of complexity and which perform functionality
with an higher degree of abstraction. In the first layer, raw data collected
from sensors in the operating room during surgical operation, they are
pre-processed and aggregated. The results of this initial phase are transferred
to a second layer, which implements pattern recognition techniques and extract
relevant features from the data. Finally, in the last layer, expert systems are
employed to take high level decisions, which represent the final output of the
system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09101</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09101</id><created>2015-11-29</created><authors><author><keyname>Saleiro</keyname><forenames>Pedro</forenames></author><author><keyname>Amir</keyname><forenames>S&#xed;lvio</forenames></author><author><keyname>Silva</keyname><forenames>M&#xe1;rio J.</forenames></author><author><keyname>Soares</keyname><forenames>Carlos</forenames></author></authors><title>POPmine: Tracking Political Opinion on the Web</title><categories>cs.SI</categories><comments>2015 IEEE International Conference on Computer and Information
  Technology, Ubiquitous Computing and Communications</comments><doi>10.1109/CIT/IUCC/DASC/PICOM.2015.228</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The automatic content analysis of mass media in the social sciences has
become necessary and possible with the raise of social media and computational
power. One particularly promising avenue of research concerns the use of
opinion mining. We design and implement the POPmine system which is able to
collect texts from web-based conventional media (news items in mainstream media
sites) and social media (blogs and Twitter) and to process those texts,
recognizing topics and political actors, analyzing relevant linguistic units,
and generating indicators of both frequency of mention and polarity
(positivity/negativity) of mentions to political actors across sources, types
of sources, and across time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09107</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09107</id><created>2015-11-29</created><authors><author><keyname>Stalidis</keyname><forenames>Panagiotis</forenames></author><author><keyname>Giatsoglou</keyname><forenames>Maria</forenames></author><author><keyname>Diamantaras</keyname><forenames>Konstantinos</forenames></author><author><keyname>Sarigiannidis</keyname><forenames>George</forenames></author><author><keyname>Chatzisavvas</keyname><forenames>Konstantinos Ch.</forenames></author></authors><title>Machine Learning Sentiment Prediction based on Hybrid Document
  Representation</title><categories>cs.CL cs.AI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automated sentiment analysis and opinion mining is a complex process
concerning the extraction of useful subjective information from text. The
explosion of user generated content on the Web, especially the fact that
millions of users, on a daily basis, express their opinions on products and
services to blogs, wikis, social networks, message boards, etc., render the
reliable, automated export of sentiments and opinions from unstructured text
crucial for several commercial applications. In this paper, we present a novel
hybrid vectorization approach for textual resources that combines a weighted
variant of the popular Word2Vec representation (based on Term Frequency-Inverse
Document Frequency) representation and with a Bag- of-Words representation and
a vector of lexicon-based sentiment values. The proposed text representation
approach is assessed through the application of several machine learning
classification algorithms on a dataset that is used extensively in literature
for sentiment detection. The classification accuracy derived through the
proposed hybrid vectorization approach is higher than when its individual
components are used for text represenation, and comparable with
state-of-the-art sentiment detection methodologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09113</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09113</id><created>2015-11-05</created><authors><author><keyname>Yang</keyname><forenames>Jianxiao</forenames><affiliation>ENSTA ParisTech U2IS/IS</affiliation></author><author><keyname>Geller</keyname><forenames>Benoit</forenames><affiliation>ENSTA ParisTech U2IS/IS</affiliation></author><author><keyname>Wei</keyname><forenames>A</forenames><affiliation>CEDRIC</affiliation></author></authors><title>Bayesian and hybrid Cramer-Rao bounds for QAM dynamical phase estimation</title><categories>cs.IT cs.SY math.IT</categories><proxy>ccsd</proxy><journal-ref>Acoustics, Speech and Signal Processing, Apr 2009, Taipei, Taiwan.
  2009</journal-ref><doi>10.1109/ICASSP.2009.4960329</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  -In this paper, we study Bayesian and hybrid Cramer-Rao bounds for the
dynamical phase estimation of QAM modulated signals. We present the analytical
expressions for the various CRBs. This avoids the calculation of any matrix
inversion and thus greatly reduces the computation complexity. Through
simulations, we also illustrate the behaviors of the BCRB and of the HCRB with
the signal-to-noise ratio. Index Terms-Bayesian Cramer-Rao Bound (BCRB), Hybrid
Cramer-Rao Bound (HCRB), Synchronization Performance
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09116</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09116</id><created>2015-11-29</created><authors><author><keyname>Donoghue</keyname><forenames>Nolan</forenames></author><author><keyname>Hahn</keyname><forenames>Bridger</forenames></author><author><keyname>Xu</keyname><forenames>Helen</forenames></author><author><keyname>Kroeger</keyname><forenames>Thomas</forenames></author><author><keyname>Zage</keyname><forenames>David</forenames></author><author><keyname>Johnson</keyname><forenames>Rob</forenames></author></authors><title>Tracking Network Events with Write Optimized Data Structures: The Design
  and Implementation of TWIAD: The Write-Optimized IP Address Database</title><categories>cs.CR cs.DB</categories><comments>7 pages, 2 figures, 6 tables. Submitted and accepted to BADGERS 2015
  at RAID 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Access to network traffic records is an integral part of recognizing and
addressing network security breaches. Even with the increasing sophistication
of network attacks, basic network events such as connections between two IP
addresses play an important role in any network defense. Given the duration of
current attacks, long-term data archival is critical but typically very little
of the data is ever accessed. Previous work has provided tools and identified
the need to trace connections. However, traditional databases raise performance
concerns as they are optimized for querying rather than ingestion.
  The study of write-optimized data structures (WODS) is a new and growing
field that provides a novel approach to traditional storage structures (e.g.,
B-trees). WODS trade minor degradations in query performance for significant
gains in the ability to quickly insert more data elements, typically on the
order of 10 to 100 times more inserts per second. These efficient,
out-of-memory data structures can play a critical role in enabling robust,
long-term tracking of network events.
  In this paper, we present TWIAD, the Write-optimized IP Address Database.
TWIAD uses a write-optimized B-tree known as a B {\epsilon} tree to track all
IP address connections in a network traffic stream. Our initial implementation
focuses on utilizing lower cost hardware, demonstrating that basic long-term
tracking can be done without advanced equipment. We tested TWIAD on a modest
desktop system and showed a sustained ingestion rate of about 20,000 inserts
per second.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09120</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09120</id><created>2015-11-29</created><updated>2016-02-05</updated><authors><author><keyname>Nasser</keyname><forenames>Soliman</forenames></author><author><keyname>Jubran</keyname><forenames>Ibrahim</forenames></author><author><keyname>Feldman</keyname><forenames>Dan</forenames></author></authors><title>Low-cost and Faster Tracking Systems Using Core-sets for Pose-Estimation</title><categories>cs.RO cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How can a \$20 toy quadcopter hover using a weak &quot;Internet of Things&quot;
mini-board and a web-cam? In the pose-estimation problem we need to rotate a
set of $n$ marker (points) and choose one of their n! permutations, so that the
sum of squared corresponding distances to another ordered set of $n$ markers is
minimized. A popular heuristic for this problem is ICP.
  We prove that \emph{every} set has a weighted subset (core-set) of constant
size (independent of $n$), such that computing the optimal orientation of the
small core-set would yield \emph{exactly} the same result as using the full set
of $n$ markers. A deterministic algorithm for computing this core-set in $O(n)$
time is provided, using the Caratheodory Theorem from computational geometry.
  We developed a system that enables low-cost and real-time tracking by
computing this core-set on the cloud in one thread, and uses the last computed
core-set locally in a parallel thread. Our experimental results show how these
core-sets can boost the tracking time and quality for large and even small sets
of both IR and RGB markers on a toy quadcopter. Open source code for the system
and algorithm is provided
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09123</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09123</id><created>2015-11-25</created><authors><author><keyname>Wong</keyname><forenames>Ka-Chun</forenames></author></authors><title>A Short Survey on Data Clustering Algorithms</title><categories>cs.DS cs.CV cs.LG stat.CO stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With rapidly increasing data, clustering algorithms are important tools for
data analytics in modern research. They have been successfully applied to a
wide range of domains; for instance, bioinformatics, speech recognition, and
financial analysis. Formally speaking, given a set of data instances, a
clustering algorithm is expected to divide the set of data instances into the
subsets which maximize the intra-subset similarity and inter-subset
dissimilarity, where a similarity measure is defined beforehand. In this work,
the state-of-the-arts clustering algorithms are reviewed from design concept to
methodology; Different clustering paradigms are discussed. Advanced clustering
algorithms are also discussed. After that, the existing clustering evaluation
metrics are reviewed. A summary with future insights is provided at the end.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09128</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09128</id><created>2015-11-29</created><authors><author><keyname>Wu</keyname><forenames>Haibing</forenames></author><author><keyname>Gu</keyname><forenames>Yiwei</forenames></author><author><keyname>Sun</keyname><forenames>Shangdi</forenames></author><author><keyname>Gu</keyname><forenames>Xiaodong</forenames></author></authors><title>Aspect-based Opinion Summarization with Convolutional Neural Networks</title><categories>cs.CL cs.IR cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers Aspect-based Opinion Summarization (AOS) of reviews on
particular products. To enable real applications, an AOS system needs to
address two core subtasks, aspect extraction and sentiment classification. Most
existing approaches to aspect extraction, which use linguistic analysis or
topic modeling, are general across different products but not precise enough or
suitable for particular products. Instead we take a less general but more
precise scheme, directly mapping each review sentence into pre-defined aspects.
To tackle aspect mapping and sentiment classification, we propose two
Convolutional Neural Network (CNN) based methods, cascaded CNN and multitask
CNN. Cascaded CNN contains two levels of convolutional networks. Multiple CNNs
at level 1 deal with aspect mapping task, and a single CNN at level 2 deals
with sentiment classification. Multitask CNN also contains multiple aspect CNNs
and a sentiment CNN, but different networks share the same word embeddings.
Experimental results indicate that both cascaded and multitask CNNs outperform
SVM-based methods by large margins. Multitask CNN generally performs better
than cascaded CNN.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09134</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09134</id><created>2015-11-29</created><authors><author><keyname>Weiyi</keyname><forenames>Liu</forenames></author><author><keyname>Lingli</keyname><forenames>Chen</forenames></author><author><keyname>Guangmin</keyname><forenames>Hu</forenames></author></authors><title>Mining Essential Relationships under Multiplex Networks</title><categories>cs.SI physics.soc-ph</categories><comments>5 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In big data times, massive datasets often carry different relationships among
the same group of nodes, analyzing on these heterogeneous relationships may
give us a window to peek the essential relationships among nodes. In this
paper, first of all we propose a new metric &quot;similarity rate&quot; in order to
capture the changing rate of similarities between node-pairs though all
networks; secondly, we try to use this new metric to uncover essential
relationships between node-pairs which essential relationships are often hidden
and hard to get. From experiments study of Indonesian Terrorists dataset, this
new metric similarity rate function well for giving us a way to uncover
essential relationships from lots of appearances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09139</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09139</id><created>2015-11-29</created><authors><author><keyname>Moreno</keyname><forenames>Jaime A.</forenames></author></authors><title>Discontinuous integral control for mechanical systems</title><categories>cs.SY</categories><comments>8 pages, 4 figures.Previous version was submitted to CDC2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For mechanical systems we present a controller able to track an unknown
smooth signal, converging in finite time and by means of a continuous control
signal. The control scheme is insensitive against unknown perturbations with
bounded derivative. The controller consists of a non locally Lipschitz state
feedback control law, and a discontinuous integral controller, that is able to
estimate the unknown perturbation and to compensate for it. To complete an
output feedback control a continuous observer for the velocity is added. It is
shown that the closed loop consisting of state feedback, state observer and
discontinuous integral controller has an equilibrium point that is globally,
finite time stable, despite of perturbations with bounded derivative. The proof
is based on a new smooth Lyapunov function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09142</identifier>
 <datestamp>2015-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09142</id><created>2015-11-29</created><authors><author><keyname>Asghar</keyname><forenames>Muhammad Zubair</forenames></author><author><keyname>Ahmad</keyname><forenames>Shakeel</forenames></author><author><keyname>Marwat</keyname><forenames>Afsana</forenames></author><author><keyname>Kundi</keyname><forenames>Fazal Masud</forenames></author></authors><title>Sentiment Analysis on YouTube: A Brief Survey</title><categories>cs.SI cs.IR</categories><journal-ref>MAGNT Research Report (ISSN. 1444-8939), Vol.3 (1). PP: 1250-1257,
  2015</journal-ref><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Sentiment analysis or opinion mining is the field of study related to analyze
opinions, sentiments, evaluations, attitudes, and emotions of users which they
express on social media and other online resources. The revolution of social
media sites has also attracted the users towards video sharing sites, such as
YouTube. The online users express their opinions or sentiments on the videos
that they watch on such sites. This paper presents a brief survey of techniques
to analyze opinions posted by users about a particular video.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09147</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09147</id><created>2015-11-29</created><updated>2015-12-09</updated><authors><author><keyname>Irissappane</keyname><forenames>Athirai A.</forenames></author><author><keyname>Oliehoek</keyname><forenames>Frans A.</forenames></author><author><keyname>Zhang</keyname><forenames>Jie</forenames></author></authors><title>Scaling POMDPs For Selecting Sellers in E-markets-Extended Version</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In multiagent e-marketplaces, buying agents need to select good sellers by
querying other buyers (called advisors). Partially Observable Markov Decision
Processes (POMDPs) have shown to be an effective framework for optimally
selecting sellers by selectively querying advisors. However, current solution
methods do not scale to hundreds or even tens of agents operating in the
e-market. In this paper, we propose the Mixture of POMDP Experts (MOPE)
technique, which exploits the inherent structure of trust-based domains, such
as the seller selection problem in e-markets, by aggregating the solutions of
smaller sub-POMDPs. We propose a number of variants of the MOPE approach that
we analyze theoretically and empirically. Experiments show that MOPE can scale
up to a hundred agents thereby leveraging the presence of more advisors to
significantly improve buyer satisfaction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09149</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09149</id><created>2015-11-29</created><authors><author><keyname>Thompson</keyname><forenames>Steven K.</forenames></author></authors><title>Fast Moving Sampling Designs in Temporal Networks</title><categories>stat.ME cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a study related to this one I set up a temporal network simulation
environment for evaluating network intervention strategies. A network
intervention strategy consists of a sampling design to select nodes in the
network. An intervention is applied to nodes in the sample for the purpose of
changing the wider network in some desired way. The network intervention
strategies can represent natural agents such as viruses that spread in the
network, programs to prevent or reduce the virus spread, and the agency of
individual nodes, such as people, in forming and dissolving the links that
create, maintain or change the network. The present paper examines idealized
versions of the sampling designs used to that study. The purpose is to better
understand the natural and human network designs in real situations and to
provide a simple inference of design-based properties that in turn measure
properties of the time-changing network. The designs use link tracing and
sometimes other probabilistic procedures to add units to the sample and have an
ongoing attrition process by which units are removed from the sample.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09150</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09150</id><created>2015-11-29</created><authors><author><keyname>Varior</keyname><forenames>Rahul Rama</forenames></author><author><keyname>Wang</keyname><forenames>Gang</forenames></author></authors><title>Hierarchical Invariant Feature Learning with Marginalization for Person
  Re-Identification</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of matching pedestrians across multiple
camera views, known as person re-identification. Variations in lighting
conditions, environment and pose changes across camera views make
re-identification a challenging problem. Previous methods address these
challenges by designing specific features or by learning a distance function.
We propose a hierarchical feature learning framework that learns invariant
representations from labeled image pairs. A mapping is learned such that the
extracted features are invariant for images belonging to same individual across
views. To learn robust representations and to achieve better generalization to
unseen data, the system has to be trained with a large amount of data.
Critically, most of the person re-identification datasets are small. Manually
augmenting the dataset by partial corruption of input data introduces
additional computational burden as it requires several training epochs to
converge. We propose a hierarchical network which incorporates a
marginalization technique that can reap the benefits of training on large
datasets without explicit augmentation. We compare our approach with several
baseline algorithms as well as popular linear and non-linear metric learning
algorithms and demonstrate improved performance on challenging publicly
available datasets, VIPeR, CUHK01, CAVIAR4REID and iLIDS. Our approach also
achieves the stateof-the-art results on these datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09156</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09156</id><created>2015-11-29</created><authors><author><keyname>Fukunaga</keyname><forenames>Takuro</forenames></author></authors><title>Constant-approximation algorithms for highly connected multi-dominating
  sets in unit disk graphs</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given an undirected graph on a node set $V$ and positive integers $k$ and
$m$, a $k$-connected $m$-dominating set ($(k,m)$-CDS) is defined as a subset
$S$ of $V$ such that each node in $V \setminus S$ has at least $m$ neighbors in
$S$, and a $k$-connected subgraph is induced by $S$. The weighted $(k,m)$-CDS
problem is to find a minimum weight $(k,m)$-CDS in a given node-weighted graph.
The problem is called the unweighted $(k,m)$-CDS problem if the objective is to
minimize the cardinality of a $(k,m)$-CDS. These problems have been actively
studied for unit disk graphs, motivated by the application of constructing a
virtual backbone in a wireless ad hoc network. However, constant-approximation
algorithms are known only for $k \leq 3$ in the unweighted $(k,m)$-CDS problem,
and for $(k,m)=(1,1)$ in the weighted $(k,m)$-CDS problem. In this paper, we
consider the case in which $m \geq k$, and we present a simple $O(5^k
k!)$-approximation algorithm for the unweighted $(k,m)$-CDS problem, and a
primal-dual $O(k^2 \log k)$-approximation algorithm for the weighted
$(k,m)$-CDS problem. Both algorithms achieve constant approximation factors
when $k$ is a fixed constant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09159</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09159</id><created>2015-11-30</created><authors><author><keyname>Xu</keyname><forenames>Yangyang</forenames></author><author><keyname>Akrotirianakis</keyname><forenames>Ioannis</forenames></author><author><keyname>Chakraborty</keyname><forenames>Amit</forenames></author></authors><title>Proximal gradient method for huberized support vector machine</title><categories>stat.ML cs.LG cs.NA math.NA</categories><comments>in Pattern analysis and application, 2015</comments><doi>10.1007/s10044-015-0485-z</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Support Vector Machine (SVM) has been used in a wide variety of
classification problems. The original SVM uses the hinge loss function, which
is non-differentiable and makes the problem difficult to solve in particular
for regularized SVMs, such as with $\ell_1$-regularization. This paper
considers the Huberized SVM (HSVM), which uses a differentiable approximation
of the hinge loss function. We first explore the use of the Proximal Gradient
(PG) method to solving binary-class HSVM (B-HSVM) and then generalize it to
multi-class HSVM (M-HSVM). Under strong convexity assumptions, we show that our
algorithm converges linearly. In addition, we give a finite convergence result
about the support of the solution, based on which we further accelerate the
algorithm by a two-stage method. We present extensive numerical experiments on
both synthetic and real datasets which demonstrate the superiority of our
methods over some state-of-the-art methods for both binary- and multi-class
SVMs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09173</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09173</id><created>2015-11-30</created><authors><author><keyname>Zhang</keyname><forenames>Aiqi</forenames></author><author><keyname>Li</keyname><forenames>Ang</forenames></author><author><keyname>Zhu</keyname><forenames>Tingshao</forenames></author></authors><title>Recognizing Temporal Linguistic Expression Pattern of Individual with
  Suicide Risk on Social Media</title><categories>cs.SI cs.CL</categories><comments>16 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Suicide is a global public health problem. Early detection of individual
suicide risk plays a key role in suicide prevention. In this paper, we propose
to look into individual suicide risk through time series analysis of personal
linguistic expression on social media (Weibo). We examined temporal patterns of
the linguistic expression of individuals on Chinese social media (Weibo). Then,
we used such temporal patterns as predictor variables to build classification
models for estimating levels of individual suicide risk. Characteristics of
time sequence curves to linguistic features including parentheses, auxiliary
verbs, personal pronouns and body words are reported to affect performance of
suicide most, and the predicting model has a accuracy higher than 0.60, shown
by the results. This paper confirms the efficiency of the social media data in
detecting individual suicide risk. Results of this study may be insightful for
improving the performance of suicide prevention programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09174</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09174</id><created>2015-11-30</created><authors><author><keyname>Heng</keyname><forenames>Ziling</forenames></author><author><keyname>Yue</keyname><forenames>Qin</forenames></author></authors><title>A class of $q$-ary linear codes derived from irreducible cyclic codes</title><categories>cs.IT math.IT</categories><comments>5 pages. Submitted to IEEE</comments><msc-class>11T71, 11T55</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, linear codes with a few weights were widely investigated due to
their applications in secret sharing schemes and authentication schemes. In
this letter, we present a class of $q$-ary linear codes derived from
irreducible cyclic codes with $q$ a prime power. We use Gauss sums to represent
its Hamming weights and obtain the bounds of its minimum Hamming distance. In
some cases, we explicitly determine its weight distributions which generalize
some known results. It is quite interesting that many new codes obtained in
this letter are optimal according to some bounds on linear codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09180</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09180</id><created>2015-11-30</created><authors><author><keyname>Sayed</keyname><forenames>Ali H.</forenames></author><author><keyname>Zhao</keyname><forenames>Xiaochuan</forenames></author></authors><title>Asynchronous adaptive networks</title><categories>math.OC cs.LG cs.MA</categories><comments>30 pages, 7 figures. Manuscript to appear as a book chapter
  contribution in the edited volume Cognitive Dynamic Systems, S. Haykin, Ed.,
  Wiley, NY, 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a recent article [1] we surveyed advances related to adaptation, learning,
and optimization over synchronous networks. Various distributed strategies were
discussed that enable a collection of networked agents to interact locally in
response to streaming data and to continually learn and adapt to track drifts
in the data and models. Under reasonable technical conditions on the data, the
adaptive networks were shown to be mean-square stable in the slow adaptation
regime, and their mean-square-error performance and convergence rate were
characterized in terms of the network topology and data statistical moments
[2]. Classical results for single-agent adaptation and learning were recovered
as special cases. Following the works [3]-[5], this chapter complements the
exposition from [1] and extends the results to asynchronous networks. The
operation of this class of networks can be subject to various sources of
uncertainties that influence their dynamic behavior, including randomly
changing topologies, random link failures, random data arrival times, and
agents turning on and off randomly. In an asynchronous environment, agents may
stop updating their solutions or may stop sending or receiving information in a
random manner and without coordination with other agents. The presentation will
reveal that the mean-square-error performance of asynchronous networks remains
largely unaltered compared to synchronous networks. The results justify the
remarkable resilience of cooperative networks in the face of random events.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09186</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09186</id><created>2015-11-30</created><authors><author><keyname>Wang</keyname><forenames>Yong</forenames></author></authors><title>An Algebraic Approach for Approximity</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Comparison to traditionally accurate computing, approximate computing focuses
on the rapidity of the satisfactory solution, but not the unnecessary accuracy
of the solution. Approximate bisimularity is the approximate one corresponding
to traditionally accurate bisimilarity. Based on the work of distances between
basic processes, we propose an algebraic approach for distances between
processes to support a whole process calculus CCS, which contains prefix, sum,
composition, restriction, relabeling and recursion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09196</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09196</id><created>2015-11-30</created><updated>2016-01-25</updated><authors><author><keyname>Kameli</keyname><forenames>Hamid</forenames></author></authors><title>Non-Adaptive Group Testing on Graphs</title><categories>cs.DS cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Grebinski and Kucherov (1998) and Alon et al. (2004-2005) study the problem
of learning a hidden graph for some especial cases, such as hamiltonian cycle,
cliques, stars, and matchings. This problem is motivated by problems in
chemical reactions, molecular biology and genome sequencing.
  In this paper, we present a generalization of this problem. Precisely, we
consider a graph G and a subgraph H of G and we assume that G contains exactly
one defective subgraph isomorphic to H. The goal is to find the defective
subgraph by testing whether an induced subgraph contains an edge of the
defective subgraph, with the minimum number of tests. We present an upper bound
for the number of tests to find the defective subgraph by using the symmetric
and high probability variation of Lov\'asz Local Lemma.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09199</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09199</id><created>2015-11-30</created><authors><author><keyname>Brands</keyname><forenames>G.</forenames></author><author><keyname>Roellgen</keyname><forenames>C. B.</forenames></author><author><keyname>Vogel</keyname><forenames>K. U.</forenames></author></authors><title>QRKE: Extensions</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Permutable Chebyshev polynomials (T polynomials) defined over the field of
real numbers are suitable for creating a Diffie-Hellman-like key exchange
algorithm that is able to withstand attacks using quantum computers. The
algorithm takes advantage of the commutative properties of Chebyshev
polynomials of the first kind. We show how T polynomial values can be computed
faster and how the underlying principle can further be used to create public
key encryption methods, as well as certificate-like authentication-, and
signature schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09207</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09207</id><created>2015-11-30</created><updated>2016-02-03</updated><authors><author><keyname>Yao</keyname><forenames>Cong</forenames></author><author><keyname>Wu</keyname><forenames>Jianan</forenames></author><author><keyname>Zhou</keyname><forenames>Xinyu</forenames></author><author><keyname>Zhang</keyname><forenames>Chi</forenames></author><author><keyname>Zhou</keyname><forenames>Shuchang</forenames></author><author><keyname>Cao</keyname><forenames>Zhimin</forenames></author><author><keyname>Yin</keyname><forenames>Qi</forenames></author></authors><title>Incidental Scene Text Understanding: Recent Progresses on ICDAR 2015
  Robust Reading Competition Challenge 4</title><categories>cs.CV</categories><comments>3 pages, 2 figures, 5 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Different from focused texts present in natural images, which are captured
with user's intention and intervention, incidental texts usually exhibit much
more diversity, variability and complexity, thus posing significant
difficulties and challenges for scene text detection and recognition
algorithms. The ICDAR 2015 Robust Reading Competition Challenge 4 was launched
to assess the performance of existing scene text detection and recognition
methods on incidental texts as well as to stimulate novel ideas and solutions.
This report is dedicated to briefly introduce our strategies for this
challenging problem and compare them with prior arts in this field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09208</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09208</id><created>2015-11-30</created><authors><author><keyname>D&#xfc;tting</keyname><forenames>Paul</forenames></author><author><keyname>Kesselheim</keyname><forenames>Thomas</forenames></author><author><keyname>Tardos</keyname><forenames>&#xc9;va</forenames></author></authors><title>Algorithms as Mechanisms: The Price of Anarchy of Relax-and-Round</title><categories>cs.GT cs.DS</categories><comments>Extended abstract appeared in Proc. of 16th ACM Conference on
  Economics and Computation (EC'15)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many algorithms that are originally designed without explicitly considering
incentive properties are later combined with simple pricing rules and used as
mechanisms. The resulting mechanisms are often natural and simple to
understand. But how good are these algorithms as mechanisms? Truthful reporting
of valuations is typically not a dominant strategy (certainly not with a
pay-your-bid, first-price rule, but it is likely not a good strategy even with
a critical value, or second-price style rule either). Our goal is to show that
a wide class of approximation algorithms yields this way mechanisms with low
Price of Anarchy.
  The seminal result of Lucier and Borodin [SODA 2010] shows that combining a
greedy algorithm that is an $\alpha$-approximation algorithm with a
pay-your-bid payment rule yields a mechanism whose Price of Anarchy is
$O(\alpha)$. In this paper we significantly extend the class of algorithms for
which such a result is available by showing that this close connection between
approximation ratio on the one hand and Price of Anarchy on the other also
holds for the design principle of relaxation and rounding provided that the
relaxation is smooth and the rounding is oblivious.
  We demonstrate the far-reaching consequences of our result by showing its
implications for sparse packing integer programs, such as multi-unit auctions
and generalized matching, for the maximum traveling salesman problem, for
combinatorial auctions, and for single source unsplittable flow problems. In
all these problems our approach leads to novel simple, near-optimal mechanisms
whose Price of Anarchy either matches or beats the performance guarantees of
known mechanisms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09209</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09209</id><created>2015-11-30</created><authors><author><keyname>Ge</keyname><forenames>ZongYuan</forenames></author><author><keyname>Bewley</keyname><forenames>Alex</forenames></author><author><keyname>McCool</keyname><forenames>Christopher</forenames></author><author><keyname>Upcroft</keyname><forenames>Ben</forenames></author><author><keyname>Corke</keyname><forenames>Peter</forenames></author><author><keyname>Sanderson</keyname><forenames>Conrad</forenames></author></authors><title>Fine-Grained Classification via Mixture of Deep Convolutional Neural
  Networks</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel deep convolutional neural network (DCNN) system for
fine-grained image classification, called a mixture of DCNNs (MixDCNN). The
fine-grained image classification problem is characterised by large intra-class
variations and small inter-class variations. To overcome these problems our
proposed MixDCNN system partitions images into K subsets of similar images and
learns an expert DCNN for each subset. The output from each of the K DCNNs is
combined to form a single classification decision. In contrast to previous
techniques, we provide a formulation to perform joint end-to-end training of
the K DCNNs simultaneously. Extensive experiments, on three datasets using two
network structures (AlexNet and GoogLeNet), show that the proposed MixDCNN
system consistently outperforms other methods. It provides a relative
improvement of 12.7% and achieves state-of-the-art results on two datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09229</identifier>
 <datestamp>2015-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09229</id><created>2015-11-30</created><updated>2015-12-03</updated><authors><author><keyname>Belazzougui</keyname><forenames>Djamal</forenames></author></authors><title>Efficient Deterministic Single Round Document Exchange for Edit Distance</title><categories>cs.DS cs.CC</categories><comments>12 pages, under submission. This version has some minor corrections,
  clarifications and a simplification of the message size bound</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Suppose that we have two parties that possess each a binary string. Suppose
that the length of the first string (document) is $n$ and that the two strings
(documents) have edit distance (minimal number of deletes, inserts and
substitutions needed to transform one string into the other) at most $k$. The
problem we want to solve is to devise an efficient protocol in which the first
party sends a single message that allows the second party to guess the first
party's string. In this paper we show an efficient deterministic protocol for
this problem. The protocol runs in time $O(n\cdot \mathtt{polylog}(n))$ and has
message size $O(k^2+k\log^2n)$ bits. To the best of our knowledge, ours is the
first efficient deterministic protocol for this problem, if efficiency is
measured in both the message size and the running time. As an immediate
application of our new protocol, we show a new error correcting code that is
efficient even for large numbers of (adversarial) edit errors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09230</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09230</id><created>2015-11-30</created><authors><author><keyname>Adams</keyname><forenames>Robin</forenames></author><author><keyname>Jacobs</keyname><forenames>Bart</forenames></author></authors><title>A Type Theory for Probabilistic and Bayesian Reasoning</title><categories>cs.LO math.LO math.PR</categories><comments>38 pages</comments><acm-class>F.4.1; G.3; F.3.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a novel type theory and logic for probabilistic
reasoning. Its logic is quantitative, with fuzzy predicates. It includes
normalisation and conditioning of states. This conditioning uses a key aspect
that distinguishes our probabilistic type theory from quantum type theory,
namely the bijective correspondence between predicates and side-effect free
actions (called instrument, or assert, maps). The paper shows how suitable
computation rules can be derived from this predicate-action correspondence, and
uses these rules for calculating conditional probabilities in two well-known
examples of Bayesian reasoning in (graphical) models. Our type theory may thus
form the basis for a mechanisation of Bayesian inference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09231</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09231</id><created>2015-11-30</created><authors><author><keyname>Sun</keyname><forenames>Zhun</forenames></author><author><keyname>Ozay</keyname><forenames>Mete</forenames></author><author><keyname>Okatani</keyname><forenames>Takayuki</forenames></author></authors><title>Design of Kernels in Convolutional Neural Networks for Image
  Classification</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite the effectiveness of Convolutional Neural Networks (CNNs) for image
classification, our understanding of the relationship between shape of
convolution kernels and learned representations is limited. In this work, we
explore and employ the relationship between shape of kernels which define
Receptive Fields (RFs) in CNNs for learning of feature representations and
image classification. For this purpose, we first propose a feature
visualization method for visualization of pixel-wise classification score maps
of learned features. Motivated by our experimental results, and observations
reported in the literature for modeling of visual systems, we propose a novel
design of shape of kernels for learning of representations in CNNs. In the
experimental results, we achieved a state-of-the-art classification performance
compared to a base CNN model [28] by reducing the number of parameters and
computational time of the model using the ILSVRC-2012 dataset [24]. The
proposed models also outperform the state-of-the-art models employed on the
CIFAR-10/100 datasets [12] for image classification. Additionally, we analyzed
the robustness of the proposed method to occlusion for classification of
partially occluded images compared with the state-of-the-art methods. Our
results indicate the effectiveness of the proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09236</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09236</id><created>2015-11-30</created><authors><author><keyname>Janssen</keyname><forenames>A. J. E. M.</forenames></author><author><keyname>van Leeuwaarden</keyname><forenames>Johan S. H.</forenames></author></authors><title>Giant component sizes in scale-free networks with power-law degrees and
  cutoffs</title><categories>physics.soc-ph cs.SI physics.data-an</categories><doi>10.1209/0295-5075/112/68001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scale-free networks arise from power-law degree distributions. Due to the
finite size of real-world networks, the power law inevitably has a cutoff at
some maximum degree $\Delta$. We investigate the relative size of the giant
component $S$ in the large-network limit. We show that $S$ as a function of
$\Delta$ increases fast when $\Delta$ is just large enough for the giant
component to exist, but increases ever more slowly when $\Delta$ increases
further. This makes that while the degree distribution converges to a pure
power law when $\Delta\to\infty$, $S$ approaches its limiting value at a slow
pace. The convergence rate also depends on the power-law exponent $\tau$ of the
degree distribution. The worst rate of convergence is found to be for the case
$\tau\approx2$, which concerns many of the real-world networks reported in the
literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09249</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09249</id><created>2015-11-30</created><authors><author><keyname>Schmidhuber</keyname><forenames>Juergen</forenames></author></authors><title>On Learning to Think: Algorithmic Information Theory for Novel
  Combinations of Reinforcement Learning Controllers and Recurrent Neural World
  Models</title><categories>cs.AI cs.LG cs.NE</categories><comments>36 pages, 1 figure. arXiv admin note: substantial text overlap with
  arXiv:1404.7828</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the general problem of reinforcement learning (RL) in
partially observable environments. In 2013, our large RL recurrent neural
networks (RNNs) learned from scratch to drive simulated cars from
high-dimensional video input. However, real brains are more powerful in many
ways. In particular, they learn a predictive model of their initially unknown
environment, and somehow use it for abstract (e.g., hierarchical) planning and
reasoning. Guided by algorithmic information theory, we describe RNN-based AIs
(RNNAIs) designed to do the same. Such an RNNAI can be trained on never-ending
sequences of tasks, some of them provided by the user, others invented by the
RNNAI itself in a curious, playful fashion, to improve its RNN-based world
model. Unlike our previous model-building RNN-based RL machines dating back to
1990, the RNNAI learns to actively query its model for abstract reasoning and
planning and decision making, essentially &quot;learning to think.&quot; The basic ideas
of this report can be applied to many other cases where one RNN-like system
exploits the algorithmic information content of another. They are taken from a
grant proposal submitted in Fall 2014, and also explain concepts such as
&quot;mirror neurons.&quot; Experimental results will be described in separate papers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09250</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09250</id><created>2015-11-30</created><updated>2015-12-01</updated><authors><author><keyname>Ritter</keyname><forenames>Daniel</forenames></author><author><keyname>Rinderle-Ma</keyname><forenames>Stefanie</forenames></author></authors><title>Toward A Collection of Cloud Integration Patterns</title><categories>cs.SE</categories><comments>30 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud computing is one of the most exciting IT trends nowadays. It poses
several challenges on application integration with respect to, for example,
security. In this work we collect and categorize several new integration
patterns and pattern solutions with a focus on cloud integration requirements.
Their evidence and examples are based on an extensive literature review and a
system study of &quot;well-established&quot; open source systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09259</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09259</id><created>2015-11-30</created><authors><author><keyname>Newman</keyname><forenames>Alantha</forenames></author><author><keyname>R&#xf6;glin</keyname><forenames>Heiko</forenames></author><author><keyname>Seif</keyname><forenames>Johanna</forenames></author></authors><title>The Alternating Stock Size Problem and the Gasoline Puzzle</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a set S of integers whose sum is zero, consider the problem of finding
a permutation of these integers such that: (i) all prefixes of the ordering are
non-negative, and (ii) the maximum value of a prefix sum is minimized. Kellerer
et al. referred to this problem as the &quot;Stock Size Problem&quot; and showed that it
can be approximated to within 3/2. They also showed that an approximation ratio
of 2 can be achieved via several simple algorithms.
  We consider a related problem, which we call the &quot;Alternating Stock Size
Problem&quot;, where the number of positive and negative integers in the input set S
are equal. The problem is the same as above, but we are additionally required
to alternate the positive and negative numbers in the output ordering. This
problem also has several simple 2-approximations. We show that it can be
approximated to within 1.79.
  Then we show that this problem is closely related to an optimization version
of the Gasoline Puzzle due to Lovasz, in which we want to minimize the size of
the gas tank necessary to go around the track. We give a 2-approximation for
this problem, based on rounding an LP relaxation whose feasible solutions are
convex combinations of permutation matrices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09263</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09263</id><created>2015-11-30</created><updated>2016-01-06</updated><authors><author><keyname>Yu</keyname><forenames>Kui</forenames></author><author><keyname>Wu</keyname><forenames>Xindong</forenames></author><author><keyname>Ding</keyname><forenames>Wei</forenames></author><author><keyname>Pei</keyname><forenames>Jian</forenames></author></authors><title>Scalable and Accurate Online Feature Selection for Big Data</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Feature selection is important in many big data applications. There are at
least two critical challenges. Firstly, in many applications, the
dimensionality is extremely high, in millions, and keeps growing. Secondly,
feature selection has to be highly scalable, preferably in an online manner
such that each feature can be processed in a sequential scan. In this paper, we
develop SAOLA, a Scalable and Accurate OnLine Approach for feature selection.
With a theoretical analysis on bounds of the pairwise correlations between
features, SAOLA employs novel online pairwise comparison techniques to address
the two challenges and maintain a parsimonious model over time in an online
manner. Furthermore, to tackle the dimensionality that arrives by groups, we
extend our SAOLA algorithm, and then propose a novel group-SAOLA algorithm for
online group feature selection. The group-SAOLA algorithm can online maintain a
set of feature groups that is sparse at the level of both groups and individual
features simultaneously. An empirical study using a series of benchmark real
data sets shows that our two algorithms, SAOLA and group-SAOLA, are scalable on
data sets of extremely high dimensionality, and have superior performance over
the state-of-the-art feature selection methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09278</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09278</id><created>2015-11-30</created><authors><author><keyname>Bockelmann</keyname><forenames>Carsten</forenames></author><author><keyname>Monsees</keyname><forenames>Fabian</forenames></author><author><keyname>Woltering</keyname><forenames>Matthias</forenames></author><author><keyname>Dekorsy</keyname><forenames>Armin</forenames></author></authors><title>Hardware-In-the-Loop Measurements of the Multi-Carrier Compressed
  Sensing Multi-User Detection (MCSM) System</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  MCSM is a recently proposed novel system concept to solve the massive access
problem envisioned in future communication systems like 5G and industry 4.0
systems. This work focuses on the practical verification of the theoretical
gains that MCSM provides using a Hardware-In-the-Loop (HIL) measurement setup.
We present results in two different scenarios: (i) a LoS lab setup and (ii) a
non-LoS machine hall. In both scenarios MCSM shows promising performance in
terms of the number of supported users and the achieved reliability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09289</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09289</id><created>2015-11-30</created><authors><author><keyname>Chen</keyname><forenames>Jingyuan</forenames></author><author><keyname>Li</keyname><forenames>Yun</forenames></author><author><keyname>Ji</keyname><forenames>Lijun</forenames></author></authors><title>Combinatorial Constructions of Optimal $(m, n,4,2)$ Optical Orthogonal
  Signature Pattern Codes</title><categories>cs.DM</categories><comments>24 pages. arXiv admin note: text overlap with arXiv:1312.7589 by
  other authors</comments><msc-class>94C30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optical orthogonal signature pattern codes (OOSPCs) play an important role in
a novel type of optical code-division multiple-access (CDMA) network for
2-dimensional image transmission. There is a one-to-one correspondence between
an $(m, n, w, \lambda)$-OOSPC and a $(\lambda+1)$-$(mn,w,1)$ packing design
admitting an automorphism group isomorphic to $\mathbb{Z}_m\times
\mathbb{Z}_n$. In 2010, Sawa gave the first infinite class of $(m, n, 4,
2)$-OOSPCs by using $S$-cyclic Steiner quadruple systems. In this paper, we use
various combinatorial designs such as strictly $\mathbb{Z}_m\times
\mathbb{Z}_n$-invariant $s$-fan designs, strictly $\mathbb{Z}_m\times
\mathbb{Z}_n$-invariant $G$-designs and rotational Steiner quadruple systems to
present some constructions for $(m, n, 4, 2)$-OOSPCs. As a consequence, our new
constructions yield more infinite families of optimal $(m, n, 4, 2)$-OOSPCs.
Especially, we shall see that in some cases an optimal $(m, n, 4, 2)$-OOSPC can
not achieve the Johnson bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09290</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09290</id><created>2015-11-30</created><authors><author><keyname>Saleiro</keyname><forenames>Pedro</forenames></author><author><keyname>Sarmento</keyname><forenames>Lu&#xed;s</forenames></author></authors><title>&quot;Piaf&quot; vs &quot;Adele&quot;: classifying encyclopedic queries using automatically
  labeled training data</title><categories>cs.IR</categories><comments>in Proceedings of the 10th Conference on Open Research Areas in
  Information Retrieval, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Encyclopedic queries express the intent of obtaining information typically
available in encyclopedias, such as biographical, geographical or historical
facts. In this paper, we train a classifier for detecting the encyclopedic
intent of web queries. For training such a classifier, we automatically label
training data from raw query logs. We use click-through data to select positive
examples of encyclopedic queries as those queries that mostly lead to Wikipedia
articles. We investigated a large set of features that can be generated to
describe the input query. These features include both term-specific patterns as
well as query projections on knowledge bases items (e.g. Freebase). Results
show that using these feature sets it is possible to achieve an F1 score above
87%, competing with a Google-based baseline, which uses a much wider set of
signals to boost the ranking of Wikipedia for potential encyclopedic queries.
The results also show that both query projections on Wikipedia article titles
and Freebase entity match represent the most relevant groups of features. When
the training set contains frequent positive examples (i.e rare queries are
excluded) results tend to improve.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09293</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09293</id><created>2015-11-30</created><authors><author><keyname>Kalaitzis</keyname><forenames>Christos</forenames></author></authors><title>An Improved Approximation Guarantee for the Maximum Budgeted Allocation
  Problem</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the Maximum Budgeted Allocation problem, which is the problem of
assigning indivisible items to players with budget constraints. In its most
general form, an instance of the MBA problem might include many different
prices for the same item among different players, and different budget
constraints for every player. So far, the best approximation algorithms we know
for the MBA problem achieve a $3/4$-approximation ratio, and employ a natural
LP relaxation, called the Assignment-LP. In this paper, we give an algorithm
for MBA, and prove that it achieves a $3/4+c$-approximation ratio, for some
constant $c&gt;0$. This algorithm works by rounding solutions to an LP called the
Configuration-LP, therefore also showing that the Configuration-LP is strictly
stronger than the Assignment-LP (for which we know that the integrality gap is
$3/4$) for the MBA problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09295</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09295</id><created>2015-11-30</created><authors><author><keyname>Zannettou</keyname><forenames>Savvas</forenames></author><author><keyname>Sirivianos</keyname><forenames>Michael</forenames></author><author><keyname>Papadopoulos</keyname><forenames>Fragkiskos</forenames></author></authors><title>Exploiting Path Diversity in Datacenters using MPTCP-aware SDN</title><categories>cs.NI</categories><comments>6 pages, 4 figures</comments><acm-class>C.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, Multipath TCP (MPTCP) has been proposed as an alternative transport
approach for datacenter networks. MPTCP provides the ability to split a flow
into multiple paths thus providing better performance and resilience to
failures. Usually, MPTCP is combined with flow-based Equal-Cost Multi-Path
Routing (ECMP), which uses random hashing to split the MPTCP subflows over
different paths. However, random hashing can be suboptimal as distinct subflows
may end up using the same paths, while other available paths remain unutilized.
In this paper, we explore an MPTCP-aware SDN controller that facilitates an
alternative routing mechanism for the MPTCP subflows. The controller uses
packet inspection to provide deterministic subflow assignment to paths. Using
the controller, we show that MPTCP can deliver significantly improved
performance when connections are not limited by the access links of hosts. To
lessen the effect of throughput limitation due to access links, we also
investigate the usage of multiple interfaces at the hosts. We demonstrate,
using our modification of the MPTCP Linux Kernel, that using multiple subflows
per pair of IP addresses can yield improved performance in multi-interface
settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09300</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09300</id><created>2015-11-30</created><authors><author><keyname>Kratochv&#xed;l</keyname><forenames>V&#xe1;clav</forenames></author><author><keyname>Vomlel</keyname><forenames>Ji&#x159;&#xed;</forenames></author></authors><title>Influence diagrams for the optimization of a vehicle speed profile</title><categories>cs.AI</categories><comments>Presented at the Twelfth Annual Bayesian Modeling Applications
  Workshop, Amtsterdam, The Netherlands, 16th July 2015</comments><msc-class>68T37</msc-class><acm-class>I.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Influence diagrams are decision theoretic extensions of Bayesian networks.
They are applied to diverse decision problems. In this paper we apply influence
diagrams to the optimization of a vehicle speed profile. We present results of
computational experiments in which an influence diagram was used to optimize
the speed profile of a Formula 1 race car at the Silverstone F1 circuit. The
computed lap time and speed profiles correspond well to those achieved by test
pilots. An extended version of our model that considers a more complex
optimization function and diverse traffic constraints is currently being tested
onboard a testing car by a major car manufacturer. This paper opens doors for
new applications of influence diagrams.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09319</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09319</id><created>2015-11-30</created><authors><author><keyname>Del Pero</keyname><forenames>Luca</forenames></author><author><keyname>Ricco</keyname><forenames>Susanna</forenames></author><author><keyname>Sukthankar</keyname><forenames>Rahul</forenames></author><author><keyname>Ferrari</keyname><forenames>Vittorio</forenames></author></authors><title>Behavior Discovery and Alignment of Articulated Object Classes from
  Unstructured Video</title><categories>cs.CV</categories><comments>19 pages, 18 figure, 2 tables. arXiv admin note: substantial text
  overlap with arXiv:1411.7883</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Internet videos provide a wealth of data that could be used to learn the
appearance or expected behaviors of many object classes. However, most
supervised methods cannot exploit this data directly, as they require a large
amount of time-consuming manual annotations. As a step towards solving this
problem, we propose an automatic system for organizing the content of a
collection of videos of an articulated object class (e.g. tiger, horse). By
exploiting the recurring motion patterns of the class across videos, our
system: 1) identifies its characteristic behaviors; and 2) recovers
pixel-to-pixel alignments across different instances.
  The behavior discovery stage generates temporal video intervals, each
automatically trimmed to one instance of the discovered behavior, clustered by
type. It relies on our novel motion representation for articulated motion based
on the displacement of ordered pairs of trajectories (PoTs). The alignment
stage aligns hundreds of instances of the class to a great accuracy despite
considerable appearance variations (e.g. an adult tiger and a cub). It uses a
flexible Thin Plate Spline deformation model that can vary through time. We
carefully evaluate each step of our system on a new, fully annotated dataset.
On behavior discovery, we outperform the state-of-the-art Improved DTF
descriptor. On spatial alignment, we outperform the popular SIFT Flow
algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09324</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09324</id><created>2015-11-30</created><updated>2016-02-11</updated><authors><author><keyname>D&#xed;az-Caro</keyname><forenames>Alejandro</forenames></author><author><keyname>L&#xf3;pez</keyname><forenames>Pablo E. Mart&#xed;nez</forenames></author></authors><title>Isomorphisms considered as equalities: Projecting functions and
  enhancing partial application through and implementation of lambda+</title><categories>cs.LO</categories><comments>A prototype writen in Haskell can be found at
  http://diaz-caro.web.unq.edu.ar/IsoAsEq-v1.0.tar.gz</comments><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an implementation of lambda+, a recently introduced simply typed
lambda-calculus with pairs where isomorphic types are made equal. The rewrite
system of lambda+ is a rewrite system modulo an equivalence relation, which
makes its implementation non-trivial. We also extend lambda+ with natural
numbers and general recursion and use Beki\'c's theorem to split mutual
recursions. This splitting, together with the features of lambda+, allows for a
novel way of program transformation by reduction, by projecting a function
before it is applied in order to simplify it. Also, currying together with the
associativity and commutativity of pairs gives an enhanced form of partial
application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09325</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09325</id><created>2015-11-30</created><authors><author><keyname>Pastorelli</keyname><forenames>Elena</forenames></author><author><keyname>Paolucci</keyname><forenames>Pier Stanislao</forenames></author><author><keyname>Ammendola</keyname><forenames>Roberto</forenames></author><author><keyname>Biagioni</keyname><forenames>Andrea</forenames></author><author><keyname>Frezza</keyname><forenames>Ottorino</forenames></author><author><keyname>Cicero</keyname><forenames>Francesca Lo</forenames></author><author><keyname>Lonardo</keyname><forenames>Alessandro</forenames></author><author><keyname>Martinelli</keyname><forenames>Michele</forenames></author><author><keyname>Simula</keyname><forenames>Francesco</forenames></author><author><keyname>Vicini</keyname><forenames>Piero</forenames></author></authors><title>Scaling to 1024 software processes and hardware cores of the distributed
  simulation of a spiking neural network including up to 20G synapses</title><categories>cs.DC q-bio.NC</categories><comments>6 pages, 4 figures, 1 table</comments><acm-class>C.2.4; C.1.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This short report describes the scaling, up to 1024 software processes and
hardware cores, of a distributed simulator of plastic spiking neural networks.
A previous report demonstrated good scalability of the simulator up to 128
processes. Herein we extend the speed-up measurements and strong and weak
scaling analysis of the simulator to the range between 1 and 1024 software
processes and hardware cores. We simulated two-dimensional grids of cortical
columns including up to ~20G synapses connecting ~11M neurons. The neural
network was distributed over a set of MPI processes and the simulations were
run on a server platform composed of up to 64 dual-socket nodes, each socket
equipped with Intel Haswell E5-2630 v3 processors (8 cores @ 2.4 GHz clock).
All nodes are interconned through an InfiniBand network. The DPSNN simulator
has been developed by INFN in the framework of EURETILE and CORTICONIC European
FET Project and will be used by the WaveScalEW tem in the framework of the
Human Brain Project (HBP), SubProject 2 - Cognitive and Systems Neuroscience.
This report lays the groundwork for a more thorough comparison with the neural
simulation tool NEST.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09327</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09327</id><created>2015-11-30</created><updated>2016-01-04</updated><authors><author><keyname>Despr&#xe9;</keyname><forenames>Vincent</forenames></author><author><keyname>Lazarus</keyname><forenames>Francis</forenames></author></authors><title>Computing the Geometric Intersection Number of Curves</title><categories>cs.CG math.GT</categories><comments>Corrected a couple typos</comments><msc-class>57M15, 05C10, 55P99</msc-class><acm-class>F.2.2; G.2.1; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The geometric intersection number of a curve on a surface is the minimal
number of self-intersections of any homotopic curve, i.e. of any curve obtained
by continuous deformation. Likewise, the geometric intersection number of a
pair of curves is the minimal number of intersections of any homotopic pair.
Given two curves represented by closed walks of length at most $\ell$ on a
combinatorial surface of complexity $n$ we describe simple algorithms to
compute the geometric intersection number of each curve or of the two curves in
$O(n+ \ell^2)$ time. We also propose an algorithm of complexity
$O(n+\ell\log^2\ell)$ to decide if the geometric intersection number of a curve
is zero, i.e. if the curve is homotopic to a simple curve.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09329</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09329</id><created>2015-11-30</created><authors><author><keyname>Mart&#xed;nez-Pe&#xf1;as</keyname><forenames>Umberto</forenames></author></authors><title>On the roots and minimum rank distance of skew cyclic codes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Skew cyclic codes play the same role as cyclic codes in the theory of
error-correcting codes for the rank metric. In this paper, we give descriptions
of these codes by idempotent generators, root spaces and cyclotomic spaces. We
prove that the lattice of skew cyclic codes is anti-isomorphic to the lattice
of root spaces and extend the rank-BCH bound on their minimum rank distance to
rank-metric versions of the van Lint-Wilson's shift and Hartmann-Tzeng bounds.
Finally, we study skew cyclic codes which are linear over the base field,
proving that these codes include all classical cyclic codes equipped with the
Hamming metric.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09337</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09337</id><created>2015-11-30</created><updated>2016-01-23</updated><authors><author><keyname>Chung</keyname><forenames>Yu-An</forenames></author><author><keyname>Lin</keyname><forenames>Hsuan-Tien</forenames></author><author><keyname>Yang</keyname><forenames>Shao-Wen</forenames></author></authors><title>Cost-aware Pre-training for Multiclass Cost-sensitive Deep Learning</title><categories>cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep learning has been one of the most prominent machine learning techniques
nowadays, being the state-of-the-art on a broad range of applications where
automatic feature extraction is needed. Many such applications also demand
varying costs for different types of mis-classification errors, but it is not
clear whether or how such cost information can be incorporated into deep
learning to improve performance. In this work, we propose a novel cost-aware
algorithm that takes into account the cost information into not only the
training stage but also the pre-training stage of deep learning. The approach
allows deep learning to conduct automatic feature extraction with the cost
information effectively. Extensive experimental results demonstrate that the
proposed approach outperforms other deep learning models that do not digest the
cost information in the pre-training stage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09360</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09360</id><created>2015-11-30</created><authors><author><keyname>Abu-Khzam</keyname><forenames>Faisal N.</forenames></author></authors><title>On the Complexity of Multi-Parameterized Cluster Editing</title><categories>cs.DS cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Cluster Editing problem seeks a transformation of a given undirected
graph into a disjoint union of cliques via a minimum number of edge additions
or deletions. A multi-parameterized version of the problem is studied,
featuring a number of input parameters that bound the amount of both
edge-additions and deletions per single vertex, as well as the size of a
clique-cluster. We show that the problem remains NP-hard even when only one
edge can be deleted and at most two edges can be added per vertex. However, the
new formulation allows us to solve Cluster Editing (exactly) in polynomial time
when the number of edge-edit operations per vertex is smaller than half the
minimum cluster size. In other words, Correlation Clustering can be solved
efficiently when the number of false positives/negatives per single data
element is expected to be small compared to the minimum cluster size. As a
byproduct, we obtain a kernelization algorithm that delivers linear-size
kernels when the two edge-edit bounds are small constants.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09368</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09368</id><created>2015-11-30</created><authors><author><keyname>Zhang</keyname><forenames>Shihua</forenames></author><author><keyname>Hu</keyname><forenames>Guanghua</forenames></author><author><keyname>Min</keyname><forenames>Wenwen</forenames></author></authors><title>A neurodynamic framework for local community extraction in networks</title><categories>cs.SI physics.soc-ph</categories><comments>4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To understand the structure and organization of a large-scale social,
biological or technological network, it can be helpful to describe and extract
local communities or modules of the network. In this article, we develop a
neurodynamic framework to describe the local communities which correspond to
the stable states of a neuro-system built based on the network. The
quantitative criteria to describe the neurodynamic system can cover a large
range of objective functions. The resolution limit of these functions enable us
to propose a generic criterion to explore multi-resolution local communities.
We explain the advantages of this framework and illustrate them by testing on a
number of model and real-world networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09376</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09376</id><created>2015-11-30</created><authors><author><keyname>Chaturvedi</keyname><forenames>Snigdha</forenames></author><author><keyname>Srivastava</keyname><forenames>Shashank</forenames></author><author><keyname>Daume</keyname><forenames>Hal</forenames><suffix>III</suffix></author><author><keyname>Dyer</keyname><forenames>Chris</forenames></author></authors><title>Modeling Dynamic Relationships Between Characters in Literary Novels</title><categories>cs.CL cs.AI</categories><comments>9 pages, 1 figure. Accepted at AAAI 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Studying characters plays a vital role in computationally representing and
interpreting narratives. Unlike previous work, which has focused on inferring
character roles, we focus on the problem of modeling their relationships.
Rather than assuming a fixed relationship for a character pair, we hypothesize
that relationships are dynamic and temporally evolve with the progress of the
narrative, and formulate the problem of relationship modeling as a structured
prediction problem. We propose a semi-supervised framework to learn
relationship sequences from fully as well as partially labeled data. We present
a Markovian model capable of accumulating historical beliefs about the
relationship and status changes. We use a set of rich linguistic and
semantically motivated features that incorporate world knowledge to investigate
the textual content of narrative. We empirically demonstrate that such a
framework outperforms competitive baselines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09389</identifier>
 <datestamp>2015-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09389</id><created>2015-11-15</created><updated>2015-12-04</updated><authors><author><keyname>van Bevern</keyname><forenames>Ren&#xe9;</forenames></author><author><keyname>Kanj</keyname><forenames>Iyad</forenames></author><author><keyname>Komusiewicz</keyname><forenames>Christian</forenames></author><author><keyname>Niedermeier</keyname><forenames>Rolf</forenames></author><author><keyname>Sorge</keyname><forenames>Manuel</forenames></author></authors><title>Separating an r-outerplanar graph into gluable pieces</title><categories>cs.DM math.CO</categories><comments>15 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let G be an r-outerplanar graph with n vertices. We provide a sequence of
log(n)/(r+1)^(32r^2 +8r) separators in G, each containing a fixed number (at
most 2r) of integer- labeled vertices and each separating the graph in a
well-defined left and right side such that the following two conditions are
fulfilled. (1) The separators are nested, meaning that the left side of every
separator S is contained in all the left sides of separators followin S. (2)
For each pair of separators, glueing the left side of the first and the right
side of the second separator results in an r-outerplanar graph. Herein, gluing
means to take the disjoint union and identify the vertices in the separators
with the same labels.
  We apply the sequences as above to the problem of finding an r-outerplanar
hypergraph support. That is, the problem is for a given hypergraph to find an
r-outerplanar graph on the same vertex set such that each hyperedge induces a
connected subgraph. We give an alternative proof that this problem is (strongly
uniformly) fixed-parameter tractable with respect to r + m where m is the
number of hyperedges in the hypergraph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09392</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09392</id><created>2015-11-18</created><authors><author><keyname>Wo&#x142;k</keyname><forenames>Agnieszka</forenames></author><author><keyname>Wo&#x142;k</keyname><forenames>Krzysztof</forenames></author><author><keyname>Marasek</keyname><forenames>Krzysztof</forenames></author></authors><title>Enhancements in statistical spoken language translation by
  de-normalization of ASR results</title><categories>cs.CL stat.ML</categories><comments>International Academy Publishing. arXiv admin note: text overlap with
  arXiv:1510.04500</comments><journal-ref>Journal of Computers, 2016 VOL 11, ISSN: 1796-203X, p. 33-40, 2016</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spoken language translation (SLT) has become very important in an
increasingly globalized world. Machine translation (MT) for automatic speech
recognition (ASR) systems is a major challenge of great interest. This research
investigates that automatic sentence segmentation of speech that is important
for enriching speech recognition output and for aiding downstream language
processing. This article focuses on the automatic sentence segmentation of
speech and improving MT results. We explore the problem of identifying sentence
boundaries in the transcriptions produced by automatic speech recognition
systems in the Polish language. We also experiment with reverse normalization
of the recognized speech samples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09394</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09394</id><created>2015-11-30</created><authors><author><keyname>Fu</keyname><forenames>Peng</forenames></author><author><keyname>Komendantskaya</keyname><forenames>Ekaterina</forenames></author><author><keyname>Schrijvers</keyname><forenames>Tom</forenames></author><author><keyname>Pond</keyname><forenames>Andrew</forenames></author></authors><title>Proof Relevant Corecursive Resolution</title><categories>cs.LO</categories><comments>23 pages, with appendices in FLOPS 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Resolution lies at the foundation of both logic programming and type class
context reduction in functional languages. Terminating derivations by
resolution have well-defined inductive meaning, whereas some non-terminating
derivations can be understood coinductively. Cycle detection is a popular
method to capture a small subset of such derivations. We show that in fact
cycle detection is a restricted form of coinductive proof, in which the atomic
formula forming the cycle plays the role of coinductive hypothesis.
  This paper introduces a heuristic method for obtaining richer coinductive
hypotheses in the form of Horn formulas. Our approach subsumes cycle detection
and gives coinductive meaning to a larger class of derivations. For this
purpose we extend resolution with Horn formula resolvents and corecursive
evidence generation. We illustrate our method on non-terminating type class
resolution problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09402</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09402</id><created>2015-11-30</created><authors><author><keyname>Galbally</keyname><forenames>Elena</forenames></author><author><keyname>Small</keyname><forenames>Frank</forenames></author><author><keyname>Zanco</keyname><forenames>Ivan</forenames></author></authors><title>Retractable Prosthesis for Transfemoral Amputees Using Series Elastic
  Actuators and Force Control</title><categories>cs.RO</categories><comments>7 pages, 10 figures. arXiv admin note: text overlap with
  arXiv:0912.3956 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a highly functional and cost-effective prosthesis for transfemoral
amputees that uses series elastic actuators. These actuators allow for accurate
force control, low impedance and large dynamic range. The design involves one
active joint at the knee and a passive joint at the ankle. Additionally, the
socket was designed using mirroring of compliances to ensure maximum comfort.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09405</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09405</id><created>2015-11-30</created><authors><author><keyname>Bonfante</keyname><forenames>Guillaume</forenames></author><author><keyname>Deloup</keyname><forenames>Florian</forenames></author></authors><title>Decidability of regular language genus computation</title><categories>cs.FL</categories><comments>13 pages, 6 figures</comments><msc-class>68R10, 68R15</msc-class><acm-class>F.1.1; F.4.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The article continues the study of the genus of regular languages that the
authors introduced in a 2012 paper. We show that the minimal finite
deterministic automaton of a regular language can be arbitrary far away from a
finite deterministic automaton realizing the minimal genus and computing the
same language both in terms of the difference of genera and in terms of the
difference in size. However we conjecture the genus of every regular language
to be computable. We prove the conjecture for a class of regular languages on
$4$ or more letters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09413</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09413</id><created>2015-11-30</created><authors><author><keyname>Deng</keyname><forenames>Yansha</forenames></author><author><keyname>Noel</keyname><forenames>Adam</forenames></author><author><keyname>Elkashlan</keyname><forenames>Maged</forenames></author><author><keyname>Nallanathan</keyname><forenames>Arumugam</forenames></author><author><keyname>Cheung</keyname><forenames>Karen C.</forenames></author></authors><title>Molecular Communication with a Reversible Adsorption Receiver</title><categories>cs.IT math.IT</categories><comments>Submitted to ICC 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present an analytical model for a diffusive molecular
communication (MC) system with a reversible adsorption receiver in a fluid
environment. The time-varying spatial distribution of the information molecules
under the reversible adsorption and desorption reaction at the surface of a
bio-receiver is analytically characterized. Based on the spatial distribution,
we derive the number of newly-adsorbed information molecules expected in any
time duration. Importantly, we present a simulation framework for the proposed
model that accounts for the diffusion and reversible reaction. Simulation
results show the accuracy of our derived expressions, and demonstrate the
positive effect of the adsorption rate and the negative effect of the
desorption rate on the net number of newly-adsorbed information molecules
expected. Moreover, our analytical results simplify to the special case of an
absorbing receiver.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09423</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09423</id><created>2015-11-30</created><updated>2015-12-01</updated><authors><author><keyname>Carayol</keyname><forenames>Arnaud</forenames></author><author><keyname>Esik</keyname><forenames>Zoltan</forenames></author></authors><title>An analysis of the equational properties of the well-founded fixed point</title><categories>cs.DM cs.LO</categories><msc-class>06B23, 68T30</msc-class><acm-class>D.1.6; F.3.2; I.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Well-founded fixed points have been used in several areas of knowledge
representation and reasoning and to give semantics to logic programs involving
negation. They are an important ingredient of approximation fixed point theory.
We study the logical properties of the (parametric) well-founded fixed point
operation. We show that the operation satisfies several, but not all of the
equational properties of fixed point operations described by the axioms of
iteration theories.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09426</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09426</id><created>2015-11-30</created><updated>2016-01-26</updated><authors><author><keyname>Pehlevan</keyname><forenames>Cengiz</forenames></author><author><keyname>Chklovskii</keyname><forenames>Dmitri B.</forenames></author></authors><title>A Normative Theory of Adaptive Dimensionality Reduction in Neural
  Networks</title><categories>q-bio.NC cs.NE</categories><comments>Advances in Neural Information Processing Systems (NIPS), 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To make sense of the world our brains must analyze high-dimensional datasets
streamed by our sensory organs. Because such analysis begins with
dimensionality reduction, modelling early sensory processing requires
biologically plausible online dimensionality reduction algorithms. Recently, we
derived such an algorithm, termed similarity matching, from a Multidimensional
Scaling (MDS) objective function. However, in the existing algorithm, the
number of output dimensions is set a priori by the number of output neurons and
cannot be changed. Because the number of informative dimensions in sensory
inputs is variable there is a need for adaptive dimensionality reduction. Here,
we derive biologically plausible dimensionality reduction algorithms which
adapt the number of output dimensions to the eigenspectrum of the input
covariance matrix. We formulate three objective functions which, in the offline
setting, are optimized by the projections of the input dataset onto its
principal subspace scaled by the eigenvalues of the output covariance matrix.
In turn, the output eigenvalues are computed as i) soft-thresholded, ii)
hard-thresholded, iii) equalized thresholded eigenvalues of the input
covariance matrix. In the online setting, we derive the three corresponding
adaptive algorithms and map them onto the dynamics of neuronal activity in
networks with biologically plausible local learning rules. Remarkably, in the
last two networks, neurons are divided into two classes which we identify with
principal neurons and interneurons in biological circuits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09433</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09433</id><created>2015-11-30</created><authors><author><keyname>Oymak</keyname><forenames>Samet</forenames></author><author><keyname>Tropp</keyname><forenames>Joel A.</forenames></author></authors><title>Universality laws for randomized dimension reduction, with applications</title><categories>math.PR cs.DS cs.IT math.IT math.ST stat.ML stat.TH</categories><comments>Code for reproducing figures available at
  http://users.cms.caltech.edu/~jtropp/</comments><msc-class>Primary: 60D05, 60F17, Secondary: 60B20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dimension reduction is the process of embedding high-dimensional data into a
lower dimensional space to facilitate its analysis. In the Euclidean setting,
one fundamental technique for dimension reduction is to apply a random linear
map to the data. This dimension reduction procedure succeeds when it preserves
certain geometric features of the set. The question is how large the embedding
dimension must be to ensure that randomized dimension reduction succeeds with
high probability.
  This paper studies a natural family of randomized dimension reduction maps
and a large class of data sets. It proves that there is a phase transition in
the success probability of the dimension reduction map as the embedding
dimension increases. For a given data set, the location of the phase transition
is the same for all maps in this family. Furthermore, each map has the same
stability properties, as quantified through the restricted minimum singular
value. These results can be viewed as new universality laws in high-dimensional
stochastic geometry.
  Universality laws for randomized dimension reduction have many applications
in applied mathematics, signal processing, and statistics. They yield design
principles for numerical linear algebra algorithms, for compressed sensing
measurement ensembles, and for random linear codes. Furthermore, these results
have implications for the performance of statistical estimation methods under a
large class of random experimental designs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09439</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09439</id><created>2015-11-30</created><authors><author><keyname>Zhou</keyname><forenames>Xiaowei</forenames></author><author><keyname>Zhu</keyname><forenames>Menglong</forenames></author><author><keyname>Leonardos</keyname><forenames>Spyridon</forenames></author><author><keyname>Derpanis</keyname><forenames>Kosta</forenames></author><author><keyname>Daniilidis</keyname><forenames>Kostas</forenames></author></authors><title>Sparseness Meets Deepness: 3D Human Pose Estimation from Monocular Video</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the challenge of 3D full-body human pose estimation from
a monocular image sequence. Here, two cases are considered: (i) the image
locations of the human joints are provided and (ii) the image locations of
joints are unknown. In the former case, a novel approach is introduced that
integrates a sparsity-driven 3D geometric prior and temporal smoothness. In the
latter case, the former case is extended by treating the image locations of the
joints as latent variables. A deep fully convolutional network is trained to
predict the uncertainty maps of the 2D joint locations. The 3D pose estimates
are realized via an Expectation-Maximization algorithm over the entire
sequence, where it is shown that the 2D joint location uncertainties can be
conveniently marginalized out during inference. Empirical evaluation on the
Human3.6M dataset shows that the proposed approaches achieve greater 3D pose
estimation accuracy over state-of-the-art baselines. Further, the proposed
approach outperforms a publicly available 2D pose estimation baseline on the
challenging PennAction dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09440</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09440</id><created>2015-11-30</created><authors><author><keyname>Li</keyname><forenames>Chong</forenames></author><author><keyname>Elia</keyname><forenames>Nicola</forenames></author></authors><title>Control approach to computing the feedback capacity for stationary
  finite dimensional Gaussian channels</title><categories>cs.IT math.IT</categories><comments>to appear in 2015 Allerton</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We firstly extend the interpretation of feedback communication over
stationary finite dimensional Gaussian channels as feedback control systems by
showing that, the problem of finding stabilizing feedback controllers with
maximal reliable transmission rate over Youla parameters coincides with the
problem of finding strictly causal filters to achieve feedback capacity
recently derived in [1]. The aforementioned interpretation provides an approach
to construct deterministic feedback coding schemes (with double exponential
decaying error probability). We next propose an asymptotic capacity-achieving
upper bounds, which can be numerically evaluated by solving finite dimensional
dual optimizations. From the filters that achieve upper bounds, we derive
feasible filters which lead to a sequence of lower bounds. Thus, from the lower
bound filters we obtain communication systems that achieve the lower bound
rate. Extensive examples show the sequence of lower bounds is asymptotic
capacity-achieving as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09450</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09450</id><created>2015-11-30</created><authors><author><keyname>Tentrup</keyname><forenames>Leander</forenames></author><author><keyname>Weinert</keyname><forenames>Alexander</forenames></author><author><keyname>Zimmermann</keyname><forenames>Martin</forenames></author></authors><title>Approximating Optimal Bounds in Prompt-LTL Realizability in
  Doubly-exponential Time</title><categories>cs.LO cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this short note, we consider the optimization variant of the realizability
problem for specifications in Prompt Linear Temporal Logic (Prompt-LTL), which
extends Linear Temporal Logic (LTL) by the prompt eventually operator whose
scope is bounded by a parametric bound. In the realizability optimization
problem, one is interested in computing the optimal bound that allows to
realize a given specification. It is known that this problem is solvable in
triply-exponential time, but not whether it can be done in doubly-exponential
time, i.e., whether it is just as hard as solving LTL realizability. We take a
step towards resolving this problem by showing that the optimum can be
approximated within a factor of 2 in doubly-exponential time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09460</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09460</id><created>2015-11-30</created><authors><author><keyname>Chaturvedi</keyname><forenames>Snigdha</forenames></author><author><keyname>Goldwasser</keyname><forenames>Dan</forenames></author><author><keyname>Daume</keyname><forenames>Hal</forenames><suffix>III</suffix></author></authors><title>Ask, and shall you receive?: Understanding Desire Fulfillment in Natural
  Language Text</title><categories>cs.AI cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ability to comprehend wishes or desires and their fulfillment is
important to Natural Language Understanding. This paper introduces the task of
identifying if a desire expressed by a subject in a given short piece of text
was fulfilled. We propose various unstructured and structured models that
capture fulfillment cues such as the subject's emotional state and actions. Our
experiments with two different datasets demonstrate the importance of
understanding the narrative and discourse structure to address this task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1511.09468</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1511.09468</id><created>2015-11-30</created><authors><author><keyname>Pehlevan</keyname><forenames>Cengiz</forenames></author><author><keyname>Chklovskii</keyname><forenames>Dmitri B.</forenames></author></authors><title>Optimization theory of Hebbian/anti-Hebbian networks for PCA and
  whitening</title><categories>q-bio.NC cs.NE</categories><comments>Annual Allerton Conference on Communication, Control, and Computing
  (Allerton) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In analyzing information streamed by sensory organs, our brains face
challenges similar to those solved in statistical signal processing. This
suggests that biologically plausible implementations of online signal
processing algorithms may model neural computation. Here, we focus on such
workhorses of signal processing as Principal Component Analysis (PCA) and
whitening which maximize information transmission in the presence of noise. We
adopt the similarity matching framework, recently developed for principal
subspace extraction, but modify the existing objective functions by adding a
decorrelating term. From the modified objective functions, we derive online PCA
and whitening algorithms which are implementable by neural networks with local
learning rules, i.e. synaptic weight updates that depend on the activity of
only pre- and postsynaptic neurons. Our theory offers a principled model of
neural computations and makes testable predictions such as the dropout of
underutilized neurons.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00001</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00001</id><created>2015-11-28</created><authors><author><keyname>Hatko</keyname><forenames>Stan</forenames></author></authors><title>k-Nearest Neighbour Classification of Datasets with a Family of
  Distances</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The $k$-nearest neighbour ($k$-NN) classifier is one of the oldest and most
important supervised learning algorithms for classifying datasets.
Traditionally the Euclidean norm is used as the distance for the $k$-NN
classifier. In this thesis we investigate the use of alternative distances for
the $k$-NN classifier.
  We start by introducing some background notions in statistical machine
learning. We define the $k$-NN classifier and discuss Stone's theorem and the
proof that $k$-NN is universally consistent on the normed space $R^d$. We then
prove that $k$-NN is universally consistent if we take a sequence of random
norms (that are independent of the sample and the query) from a family of norms
that satisfies a particular boundedness condition. We extend this result by
replacing norms with distances based on uniformly locally Lipschitz functions
that satisfy certain conditions. We discuss the limitations of Stone's lemma
and Stone's theorem, particularly with respect to quasinorms and adaptively
choosing a distance for $k$-NN based on the labelled sample. We show the
universal consistency of a two stage $k$-NN type classifier where we select the
distance adaptively based on a split labelled sample and the query. We conclude
by giving some examples of improvements of the accuracy of classifying various
datasets using the above techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00035</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00035</id><created>2015-08-27</created><authors><author><keyname>Li&#xe9;nard</keyname><forenames>Jean</forenames><affiliation>ISIR</affiliation></author><author><keyname>Girard</keyname><forenames>Beno&#xee;t</forenames><affiliation>ISIR</affiliation></author></authors><title>A biologically constrained model of the whole basal ganglia addressing
  the paradoxes of connections and selection</title><categories>q-bio.NC cs.NE cs.RO</categories><comments>\&amp;lt;http://link.springer.com/article/10.1007%2Fs10827-013-0476-2\&amp;gt;.
  \&amp;lt;10.1007/s10827-013-0476-2\&amp;gt</comments><proxy>ccsd</proxy><journal-ref>Journal of Computational Neuroscience, Springer Verlag (Germany),
  2014, 36 (3), pp.445-468</journal-ref><doi>10.1007/s10827-013-0476-2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The basal ganglia nuclei form a complex network of nuclei often assumed to
perform selection, yet their individual roles and how they influence each other
is still largely unclear. In particular, the ties between the external and
internal parts of the globus pallidus are paradoxical, as anatomical data
suggest a potent inhibitory projection between them while electrophys-iological
recordings indicate that they have similar activities. Here we introduce a
theoretical study that reconciles both views on the intra-pallidal projection,
by providing a plausible characterization of the relationship between the
external and internal globus pallidus. Specifically, we developed a mean-field
model of the whole basal ganglia, whose parameterization is optimized to
respect best a collection of numerous anatomical and electrophysiological data.
We first obtained models respecting all our constraints, hence anatomical and
electrophysiological data on the intrapallidal projection are globally
consistent. This model furthermore predicts that both aforementioned views
about the intra-pallidal projection may be reconciled when this projection is
weakly inhibitory, thus making it possible to support similar neural activity
in both nuclei and for the entire basal ganglia to select between actions.
Second, we predicts that afferent projections are substantially unbalanced
towards the external segment, as it receives the strongest excitation from STN
and the weakest inhibition from the striatum. Finally, our study strongly
suggest that the intrapallidal connection pattern is not focused but diffuse,
as this latter pattern is more efficient for the overall selection performed in
the basal ganglia.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00037</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00037</id><created>2015-11-02</created><authors><author><keyname>Huang</keyname><forenames>Weiyu</forenames></author><author><keyname>Goldsberry</keyname><forenames>Leah</forenames></author><author><keyname>Wymbs</keyname><forenames>Nicholas F.</forenames></author><author><keyname>Grafton</keyname><forenames>Scott T.</forenames></author><author><keyname>Bassett</keyname><forenames>Danielle S.</forenames></author><author><keyname>Ribeiro</keyname><forenames>Alejandro</forenames></author></authors><title>Graph Frequency Analysis of Brain Signals</title><categories>q-bio.NC cs.CE cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents methods to analyze functional brain networks and signals
from graph spectral perspectives. The notion of frequency and filters
traditionally defined for signals supported on regular domains such as discrete
time and image grids has been recently generalized to irregular graph domains,
and defines brain graph frequency associated with different levels of spatial
smoothness across the brain regions. Brain network frequency also enables the
decomposition of brain signals into pieces corresponding to smooth or vibrant
variations. We relate graph frequency with principal component analysis when
the networks of interest denote functional connectivity. The methods are
utilized to analyze brain networks and signals as subjects master a simple
motor skill. We observe that brain signals corresponding to different graph
frequencies exhibit different levels of adaptability throughout learning.
Further, we notice the strong association between graph spectral property of
brain networks with the level of exposure to tasks performed, and recognize the
most contributing and important frequency signatures at different task
familiarity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00047</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00047</id><created>2015-10-17</created><authors><author><keyname>Smarandache</keyname><forenames>Florentin</forenames></author></authors><title>Symbolic Neutrosophic Theory</title><categories>cs.AI</categories><comments>195 pages, several graphs, Published as book in Bruxelles, 2015</comments><acm-class>I.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Symbolic (or Literal) Neutrosophic Theory is referring to the use of abstract
symbols (i.e. the letters T, I, F, or their refined indexed letters Tj, Ik, Fl)
in neutrosophics. We extend the dialectical triad thesis-antithesis-synthesis
to the neutrosophic tetrad thesis-antithesis-neutrothesis-neutrosynthesis. The
we introduce the neutrosophic system that is a quasi or (t,i,f) classical
system, in the sense that the neutrosophic system deals with quasi-terms
(concepts, attributes, etc.). Then the notions of Neutrosophic Axiom,
Neutrosophic Deducibility, Degree of Contradiction (Dissimilarity) of Two
Neutrosophic Axioms, etc. Afterwards a new type of structures, called (t, i, f)
Neutrosophic Structures, and we show particular cases of such structures in
geometry and in algebra. Also, a short history of the neutrosophic set,
neutrosophic numerical components and neutrosophic literal components,
neutrosophic numbers, etc. We construct examples of splitting the literal
indeterminacy (I) into literal subindeterminacies (I1, I2, and so on, Ir), and
to define a multiplication law of these literal subindeterminacies in order to
be able to build refined I neutrosophic algebraic structures. We define three
neutrosophic actions and their properties. We then introduce the prevalence
order on T,I,F with respect to a given neutrosophic operator. And the
refinement of neutrosophic entities A, neutA, and antiA. Then we extend the
classical logical operators to neutrosophic literal (symbolic) logical
operators and to refined literal (symbolic) logical operators, and we define
the refinement neutrosophic literal (symbolic) space. We introduce the
neutrosophic quadruple numbers (a+bT+cI+dF) and the refined neutrosophic
quadruple numbers. Then we define an absorbance law, based on a prevalence
order, in order to multiply the neutrosophic quadruple numbers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00054</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00054</id><created>2015-11-30</created><authors><author><keyname>DeVoe</keyname><forenames>Charles</forenames></author><author><keyname>Rahman</keyname><forenames>Shawon</forenames></author></authors><title>Incident Response Plan for a Small to Medium Sized Hospital</title><categories>cs.CR cs.CY</categories><doi>10.5121/ijnsa.2013.5201</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most small to medium health care organizations do not have the capability to
address cyber incidents within the organization. Those that do are poorly
trained and ill equipped. These health care organizations are subject to
various laws that address privacy concerns, proper handling of financial
information, and Personally Identifiable Information. Currently an IT staff
handles responses to these incidents in an Ad Hoc manner. A properly trained,
staffed, and equipped Cyber Incident Response Team is needed to quickly respond
to these incidents to minimize data loss, and provide forensic data for the
purpose of notification, disciplinary action, legal action, and to remove the
risk vector. This paper will use the proven Incident Command System model used
in emergency services to show any sized agency can have an adequate CIRT
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00061</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00061</id><created>2015-11-30</created><authors><author><keyname>Jungck</keyname><forenames>Kathleen</forenames></author><author><keyname>Rahman</keyname><forenames>Shawon</forenames></author></authors><title>Cloud Computing Avoids Downfall of Application Service Providers</title><categories>cs.DC cs.CR</categories><doi>10.5121/ijitcs.2011.1301</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Businesses have become dependent on ever increasing amounts of electronic
information and rapid transaction speeds. Experts such as Diffie speculate that
the end of isolated computing is at hand, and that within the next decade most
businesses will have made the shift to utility computing. In order to cut costs
while still implementing increasingly complex Information Technology services,
many companies turned to Application Service Providers (ASPs). Due to poor
business models, over competition, and poor internet availability and
bandwidth, many ASPs failed with the dot com crash. Other ASPs, however, who
embraced web services architecture and true internet delivery were well placed
as early cloud adopters. With the expanded penetration and bandwidth of
internet services today, better business plans, and a wide divergence of
offering, cloud computing is avoiding the ASP downfall, and is positioned to
emerge as an enduring paradigm in computing
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00064</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00064</id><created>2015-11-30</created><authors><author><keyname>Dees</keyname><forenames>Kyle</forenames></author><author><keyname>Rahman</keyname><forenames>Shawon</forenames></author></authors><title>Enhancing Infrastructure Security in Real Estate</title><categories>cs.CY</categories><doi>10.5121/ijnsa.2011.3604</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As a result of the increased dependency on obtaining information and
connecting each computer together for ease of access or communication,
organizations risk being attacked and losing private information through
breaches or insecure business activities. To help protect organizations and
their assets, companies need to develop a strong understanding of the risks
imposed on their company and the security solutions designed to prevent or
minimize vulnerabilities. To reduce the impact threats have on a network,
organizations need to: design a defense layer system that provides multiple
instances of protection to prevent unauthorized access to core information,
implement a strong network hardware/intrusion prevention system, and create
all-inclusive network or security policies that detail user rules and company
rights. In order to enhance the overall security of a basic infrastructure,
this paper will provide a detailed look into gathering the organizational
requirements, designing and implementing a secure physical network layout, and
selecting the standards needed to prevent unauthorized access
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00066</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00066</id><created>2015-11-30</created><authors><author><keyname>Solomonik</keyname><forenames>Edgar</forenames></author><author><keyname>Hoefler</keyname><forenames>Torsten</forenames></author></authors><title>Sparse Tensor Algebra as a Parallel Programming Model</title><categories>cs.MS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dense and sparse tensors allow the representation of most bulk data
structures in computational science applications. We show that sparse tensor
algebra can also be used to express many of the transformations on these
datasets, especially those which are parallelizable. Tensor computations are a
natural generalization of matrix and graph computations. We extend the usual
basic operations of tensor summation and contraction to arbitrary functions,
and further operations such as reductions and mapping. The expression of these
transformations in a high-level sparse linear algebra domain specific language
allows our framework to understand their properties at runtime to select the
preferred communication-avoiding algorithm. To demonstrate the efficacy of our
approach, we show how key graph algorithms as well as common numerical kernels
can be succinctly expressed using our interface and provide performance results
of a general library implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00067</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00067</id><created>2015-11-30</created><authors><author><keyname>Rice</keyname><forenames>Lee</forenames></author><author><keyname>Rahman</keyname><forenames>Shawon</forenames></author></authors><title>Non-profit Organizations' Need to Address Security for Effective
  Government Contacting</title><categories>cs.CR cs.CY</categories><doi>10.5121/ijnsa.2012.4404</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The need for information security within small to mid-size companies is
increasing. The risks of information security breach, data loss, and disaster
are growing. The impact of IT outages and issues on the company are
unacceptable to any size business and their clients. There are many ways to
address the security for IT departments. The need to address risks of attacks
as well as disasters is important to the IT security policies and procedures.
The IT departments of small to medium companies have to address these security
concerns within their budgets and other limited resources.Security planning,
design, and employee training that is needed requires input and agreement from
all levels of the company and management. This paper will discuss security
needs and methods to implement them into a corporate infrastructure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00070</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00070</id><created>2015-11-30</created><authors><author><keyname>Neal</keyname><forenames>DJ</forenames></author><author><keyname>Rahman</keyname><forenames>Shawon</forenames></author></authors><title>Video Surveillance in the Cloud?</title><categories>cs.CR cs.CY</categories><doi>10.5121/ijcis.2012.2301</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A high-resolution video surveillance management system incurs huge amounts of
storage and network bandwidth. The current infrastructure required to support a
high resolution video surveillance management system (VMS) is expensive and
time consuming to plan, implement and maintain. With the recent advances in
cloud technologies, opportunity for the utilization of virtualization and the
opportunity for distributed computing techniques of cloud storage have been
pursued on the basis to find out if the various cloud computing services that
are available can support the current requirements to a high resolution video
surveillance management system. The research concludes, after investigating and
comparing various Software as a Service (SaaS), Platform as a Service (PaaS),
and Infrastructure as a Service (IaaS) cloud computing provides what is
possible to architect a VMS using cloud technologies; however, it is more
expensive and it will require additional reviews for legal implications, as
well as emerging threats and countermeasures associated with using cloud
technologies for a video surveillance management system
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00071</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00071</id><created>2015-11-30</created><authors><author><keyname>Brussee</keyname><forenames>P. W. G.</forenames></author><author><keyname>Pouwelse</keyname><forenames>J. A.</forenames></author></authors><title>Survey of robust and resilient social media tools on Android</title><categories>cs.CR cs.CY</categories><comments>11 pages, 9 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an overview of robust and resilient social media tools to overcome
natural disasters, censorship and Internet kill switches. These social media
tools use Android devices to communicate during disasters and aim to overcome
attacks on freedom of expression. There is an abundance of projects that aim to
provide resilient communication, enhance privacy, and provide anonymity. We
focus specifically on the limited set of mature tools with a healthy
development community and Internet-deployment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00077</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00077</id><created>2015-11-30</created><updated>2015-12-11</updated><authors><author><keyname>Cairo</keyname><forenames>Massimo</forenames></author><author><keyname>Farina</keyname><forenames>Gabriele</forenames></author><author><keyname>Rizzi</keyname><forenames>Romeo</forenames></author></authors><title>Decoding Hidden Markov Models Faster Than Viterbi Via Online
  Matrix-Vector (max, +)-Multiplication</title><categories>cs.LG cs.DS cs.IT math.IT</categories><comments>AAAI 2016, to appear</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a novel algorithm for the maximum a posteriori
decoding (MAPD) of time-homogeneous Hidden Markov Models (HMM), improving the
worst-case running time of the classical Viterbi algorithm by a logarithmic
factor. In our approach, we interpret the Viterbi algorithm as a repeated
computation of matrix-vector $(\max, +)$-multiplications. On time-homogeneous
HMMs, this computation is online: a matrix, known in advance, has to be
multiplied with several vectors revealed one at a time. Our main contribution
is an algorithm solving this version of matrix-vector $(\max,+)$-multiplication
in subquadratic time, by performing a polynomial preprocessing of the matrix.
Employing this fast multiplication algorithm, we solve the MAPD problem in
$O(mn^2/ \log n)$ time for any time-homogeneous HMM of size $n$ and observation
sequence of length $m$, with an extra polynomial preprocessing cost negligible
for $m &gt; n$. To the best of our knowledge, this is the first algorithm for the
MAPD problem requiring subquadratic time per observation, under the only
assumption -- usually verified in practice -- that the transition probability
matrix does not change with time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00082</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00082</id><created>2015-11-30</created><authors><author><keyname>Rader</keyname><forenames>Marc</forenames></author><author><keyname>Rahman</keyname><forenames>Shawon</forenames></author></authors><title>Exploring Historical and Emerging Phishing Techniques and Mitigating the
  Associated Security Risks</title><categories>cs.CR cs.CY</categories><doi>10.5121/ijnsa.2013.5402</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Organizations invest heavily in technical controls for their Information
Assurance (IA) infrastructure. These technical controls mitigate and reduce the
risk of damage caused by outsider attacks. Most organizations rely on training
to mitigate and reduce risk of non-technical attacks such as social
engineering. Organizations lump IA training into small modules that personnel
typically rush through because the training programs lack enough depth and
creativity to keep a trainee engaged. The key to retaining knowledge is making
the information memorable. This paper describes common and emerging attack
vectors and how to lower and mitigate the associated risks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00085</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00085</id><created>2015-11-30</created><authors><author><keyname>Todd</keyname><forenames>Margie</forenames></author><author><keyname>Rahman</keyname><forenames>Shawon</forenames></author></authors><title>Complete Network Security Protection for SME's within Limited Resources</title><categories>cs.CR cs.NI</categories><doi>10.5121/ijnsa.2013.5601</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The purpose of this paper is to present a comprehensive budget conscious
security plan for smaller enterprises that lack security guidelines.The authors
believe this paper will assist users to write an individualized security plan.
In addition to providing the top ten free or affordable tools get some sort of
semblance of security implemented, the paper also provides best practices on
the topics of Authentication, Authorization, Auditing, Firewall, Intrusion
Detection &amp; Monitoring, and Prevention. The methods employed have been
implemented at Company XYZ referenced throughout
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00101</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00101</id><created>2015-11-30</created><authors><author><keyname>Yu</keyname><forenames>Miao</forenames></author><author><keyname>Shen</keyname><forenames>Shuhan</forenames></author><author><keyname>Hu</keyname><forenames>Zhanyi</forenames></author></authors><title>Dynamic Parallel and Distributed Graph Cuts</title><categories>cs.DS cs.CV</categories><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Graph-cuts are widely used in computer vision. In order to speed up the
optimization process and improve the scalability for large graphs, Strandmark
and Kahl introduced a splitting method to split a graph into multiple subgraphs
for parallel computation in both shared and distributed memory models. However,
this parallel algorithm (parallel BK-algorithm) does not have a polynomial
bound on the number of iterations and is found non-convergent in some cases due
to the possible multiple optimal solutions of its sub-problems.
  To remedy this non-convergence problem, in this work we first introduce a
merging method capable of merging any number of those adjacent sub-graphs which
could hardly reach an agreement on their overlapped region in the parallel
BKalgorithm. Based on the pseudo-boolean representations of graphcuts,our
merging method is shown able to effectively reuse all the computed flows in
these sub-graphs. Through both the splitting and merging, we further propose a
dynamic parallel and distributed graph-cuts algorithm with guaranteed
convergence to the globally optimal solutions within a predefined number of
iterations. In essence, this work provides a general framework to allow more
sophisticated splitting and merging strategies to be employed to further boost
performance. Our dynamic parallel algorithm is validated with extensive
experimental results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00102</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00102</id><created>2015-11-30</created><authors><author><keyname>Zage</keyname><forenames>David</forenames></author><author><keyname>Xu</keyname><forenames>Helen</forenames></author><author><keyname>Kroeger</keyname><forenames>Thomas</forenames></author><author><keyname>Hahn</keyname><forenames>Bridger</forenames></author><author><keyname>Donoghue</keyname><forenames>Nolan</forenames></author><author><keyname>Benson</keyname><forenames>Thomas</forenames></author></authors><title>Secure Distributed Membership Tests via Secret Sharing: How to Hide Your
  Hostile Hosts Harnessing Shamir Secret Sharing</title><categories>cs.CR</categories><comments>6 pages and 3 figures. Submitted and accepted at CNC at ICNC 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data security and availability for operational use are frequently seen as
conflicting goals. Research on searchable encryption and homomorphic encryption
are a start, but they typically build from encryption methods that, at best,
provide protections based on problems assumed to be computationally hard. By
contrast, data encoding methods such as secret sharing provide
information-theoretic data protections. Archives that distribute data using
secret sharing can provide data protections that are resilient to malicious
insiders, compromised systems, and untrusted components.
  In this paper, we create the Serial Interpolation Filter, a method for
storing and interacting with sets of data that are secured and distributed
using secret sharing. We provide the ability to operate over set-oriented data
distributed across multiple repositories without exposing the original data.
Furthermore, we demonstrate the security of our method under various attacker
models and provide protocol extensions to handle colluding attackers. The
Serial Interpolation Filter provides information-theoretic protections from a
single attacker and computationally hard protections from colluding attackers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00103</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00103</id><created>2015-11-30</created><authors><author><keyname>Gillick</keyname><forenames>Dan</forenames></author><author><keyname>Brunk</keyname><forenames>Cliff</forenames></author><author><keyname>Vinyals</keyname><forenames>Oriol</forenames></author><author><keyname>Subramanya</keyname><forenames>Amarnag</forenames></author></authors><title>Multilingual Language Processing From Bytes</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe an LSTM-based model which we call Byte-to-Span (BTS) that reads
text as bytes and outputs span annotations of the form [start, length, label]
where start positions, lengths, and labels are separate entries in our
vocabulary. Because we operate on unicode bytes rather than language-specific
words or characters, we can analyze text in many languages with a single model.
Due to the small vocabulary size, these multilingual models are very compact,
but produce results similar to or better than the state-of-the-art in
Part-of-Speech tagging and Named Entity Recognition that use only the provided
training datasets (no external data sources). Our models are learning &quot;from
scratch&quot; in that they do not rely on any elements of the standard pipeline in
Natural Language Processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00112</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00112</id><created>2015-11-30</created><authors><author><keyname>Srivastava</keyname><forenames>Shashank</forenames></author><author><keyname>Chaturvedi</keyname><forenames>Snigdha</forenames></author><author><keyname>Mitchell</keyname><forenames>Tom</forenames></author></authors><title>Inferring Interpersonal Relations in Narrative Summaries</title><categories>cs.CL cs.AI cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Characterizing relationships between people is fundamental for the
understanding of narratives. In this work, we address the problem of inferring
the polarity of relationships between people in narrative summaries. We
formulate the problem as a joint structured prediction for each narrative, and
present a model that combines evidence from linguistic and semantic features,
as well as features based on the structure of the social community in the text.
We also provide a clustering-based approach that can exploit regularities in
narrative types. e.g., learn an affinity for love-triangles in romantic
stories. On a dataset of movie summaries from Wikipedia, our structured models
provide more than a 30% error-reduction over a competitive baseline that
considers pairs of characters in isolation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00115</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00115</id><created>2015-11-30</created><authors><author><keyname>Unnikrishnan</keyname><forenames>Jayakrishnan</forenames></author><author><keyname>Haghighatshoar</keyname><forenames>Saeid</forenames></author><author><keyname>Vetterli</keyname><forenames>Martin</forenames></author></authors><title>Unlabeled Sensing with Random Linear Measurements</title><categories>cs.IT math.IT math.PR</categories><comments>Submitted to IEEE Transactions on Information Theory, Nov 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of solving a linear sensing system when the observations
are unlabeled. Specifically we seek a solution to a linear system of equations
y = Ax when the order of the observations in the vector y is unknown. Focusing
on the setting in which A is a random matrix with i.i.d. entries, we show that
if the sensing matrix A admits an oversampling ratio of 2 or higher, then with
probability 1 it is possible to recover x exactly without the knowledge of the
order of the observations in y. Furthermore, if x is of dimension K, then any
2K entries of y are sufficient to recover x. This result implies the existence
of deterministic unlabeled sensing matrices with an oversampling factor of 2
that admit perfect reconstruction. The result is universal in that recovery is
guaranteed for all possible choices of x. While the proof is constructive, it
uses a combinatorial algorithm which is not practical, leaving the question of
complexity open. We also analyze a noisy version of the problem and show that
local stability is guaranteed by the solution. In particular, for every x, the
recovery error tends to zero as the signal-to-noise-ratio tends to infinity.
The question of universal stability is unclear. We also obtain a converse of
the result in the noiseless case: If the number of observations in y is less
than 2K, then with probability 1, universal recovery fails, i.e., with
probability 1, there exists distinct choices of x which lead to the same
unordered list of observations in y. In terms of applications, the unlabeled
sensing problem is related to data association problems encountered in
different domains including robotics where it is appears in a method called
&quot;simultaneous localization and mapping&quot; (SLAM), multi-target tracking
applications, and in sampling signals in the presence of jitter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00126</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00126</id><created>2015-11-30</created><authors><author><keyname>Nikolsky</keyname><forenames>Yuri</forenames></author><author><keyname>Gurinovich</keyname><forenames>Roman</forenames></author><author><keyname>Kuryan</keyname><forenames>Oleg</forenames></author><author><keyname>Pashuk</keyname><forenames>Aleksandr</forenames></author><author><keyname>Scherbakov</keyname><forenames>Alexej</forenames></author><author><keyname>Romantsov</keyname><forenames>Konstantin</forenames></author><author><keyname>Jellen</keyname><forenames>Leslie C.</forenames></author><author><keyname>Zhavoronkov</keyname><forenames>Alex</forenames></author></authors><title>GrantMed: a new, international system for tracking grants and funding
  trends in the life sciences</title><categories>cs.DL cs.DB cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite the success of PubMed and other search engines in managing the
massive volume of biomedical literature and the retrieval of individual
publications, grant-related data remains scattered and relatively inaccessible.
This is problematic, as project and funding data has significant analytical
value and could be integral to publication retrieval. Here, we introduce
GrantMed, a searchable international database of biomedical grants that
integrates some 20 million publications with the nearly 1.4 million research
projects and 650 billion dollars of funding that made them possible. For any
given topic in the life sciences, Grantmed provides instantaneous visualization
of the past 30 years of dollars spent and projects awarded, along with detailed
individual project descriptions, funding amounts, and links to investigators,
research organizations, and resulting publications. It summarizes trends in
funding and publication rates for areas of interest and merges data from
various national grant databases to create one international grant tracking
system. This information will benefit the research community and funding
entities alike. Users can view trends over time or current projects underway
and use this information to navigate the decision-making process in moving
forward. They can view projects prior to publication and records of previous
projects. Convenient access to this data for analytical purposes will be
beneficial in many ways, helping to prevent project overlap, reduce funding
redundancy, identify areas of success, accelerate dissemination of ideas, and
expose knowledge gaps in moving forward. It is our hope that this will be a
central resource for international life sciences research communities and the
funding organizations that support them, ultimately streamlining progress.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00127</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00127</id><created>2015-11-30</created><authors><author><keyname>Cohen</keyname><forenames>Joseph Paul</forenames></author><author><keyname>Aravena</keyname><forenames>Carla</forenames></author><author><keyname>Ding</keyname><forenames>Wei</forenames></author></authors><title>The cost of reading research. A study of Computer Science publication
  venues</title><categories>cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  What does the cost of academic publishing look like to the common researcher
today? Our goal is to convey the current state of academic publishing,
specifically in regards to the field of computer science and provide analysis
and data to be used as a basis for future studies. We will focus on author and
reader costs as they are the primary points of interaction within the
publishing world. In this work, we restrict our focus to only computer science
in order to make the data collection more feasible (the authors are computer
scientists) and hope future work can analyze and collect data across all
academic fields.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00130</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00130</id><created>2015-11-30</created><authors><author><keyname>Lin</keyname><forenames>Tsung-Yu</forenames></author><author><keyname>Ke</keyname><forenames>Tsung-Wei</forenames></author><author><keyname>Liu</keyname><forenames>Tyng-Luh</forenames></author></authors><title>Implicit Sparse Code Hashing</title><categories>cs.CV</categories><comments>9 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of converting large-scale high-dimensional image data
into binary codes so that approximate nearest-neighbor search over them can be
efficiently performed. Different from most of the existing unsupervised
approaches for yielding binary codes, our method is based on a
dimensionality-reduction criterion that its resulting mapping is designed to
preserve the image relationships entailed by the inner products of sparse
codes, rather than those implied by the Euclidean distances in the ambient
space. While the proposed formulation does not require computing any sparse
codes, the underlying computation model still inevitably involves solving an
unmanageable eigenproblem when extremely high-dimensional descriptors are used.
To overcome the difficulty, we consider the column-sampling technique and
presume a special form of rotation matrix to facilitate subproblem
decomposition. We test our method on several challenging image datasets and
demonstrate its effectiveness by comparing with state-of-the-art binary coding
techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00135</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00135</id><created>2015-11-30</created><updated>2015-12-02</updated><authors><author><keyname>Abbe</keyname><forenames>Emmanuel</forenames></author><author><keyname>Li</keyname><forenames>Jiange</forenames></author><author><keyname>Madiman</keyname><forenames>Mokshay</forenames></author></authors><title>Entropies of weighted sums in cyclic groups and applications to polar
  codes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note, we give some specific examples of i.i.d. random variables $X,
Y$ taking values in $\mZ/n\mZ$ such that the entropy of their sum is larger
than the entropy of their difference; however, we also show that this cannot
happen for $n=3$. Furthermore, we show that there exist i.i.d. $\mZ$-valued
random variables $X, Y$ with finite entropy such that $H(X+Y)$ can exceed
$H(X-Y)$ by an arbitrarily large amount, complementing a result of Lapidoth and
Pete (2008); this statement is also true for differential entropies of
real-valued random variables. These results are closely related to the study of
MSTD (more sums than differences) sets in additive number theory. Finally we
investigate polar codes for $q$-ary input channels using non-canonical kernels
to construct the generator matrix, and present applications of our results to
constructing polar codes with significantly improved error probability compared
to the canonical construction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00137</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00137</id><created>2015-11-30</created><authors><author><keyname>Hosseini</keyname><forenames>S. Amir</forenames></author><author><keyname>Lu</keyname><forenames>Zheng</forenames></author><author><keyname>de Veciana</keyname><forenames>Gustavo</forenames></author><author><keyname>Panwar</keyname><forenames>Shivendra S.</forenames></author></authors><title>SVC-based Multi-user Streamloading for Wireless Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present an approach for joint rate allocation and quality
selection for a novel video streaming scheme called streamloading.
Streamloading is a recently developed method for delivering high quality video
without violating copyright enforced restrictions on content access for video
streaming. In regular streaming services, content providers restrict the amount
of viewable video that users can download prior to playback. This approach can
cause inferior user experience due to bandwidth variations, especially in
mobile networks with varying capacity. In streamloading, the video is encoded
using Scalable Video Coding, and users are allowed to pre-fetch enhancement
layers and store them on the device, while base layers are streamed in a near
real-time fashion ensuring that buffering constraints on viewable content are
met.
  We begin by formulating the offline problem of jointly optimizing rate
allocation and quality selection for streamloading in a wireless network. This
motivates our proposed online algorithms for joint scheduling at the base
station and segment quality selection at receivers. The results indicate that
streamloading outperforms state-of-the-art streaming schemes in terms of the
number of additional streams we can admit for a given video quality.
Furthermore, the quality adaptation mechanism of our proposed algorithm
achieves a higher performance than baseline algorithms with no (or limited)
video-centric optimization of the base station's allocation of resources, e.g.,
proportional fairness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00142</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00142</id><created>2015-12-01</created><updated>2016-02-25</updated><authors><author><keyname>Zhang</keyname><forenames>Jiayi</forenames></author><author><keyname>Dai</keyname><forenames>Linglong</forenames></author><author><keyname>Sun</keyname><forenames>Shengyang</forenames></author><author><keyname>Wang</keyname><forenames>Zhaocheng</forenames></author></authors><title>On the Spectral Efficiency of Massive MIMO Systems with Low-Resolution
  ADCs</title><categories>cs.IT math.IT</categories><comments>4 pages, 2 figures, to appear in IEEE Communications Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The low-resolution analog-to-digital convertor (ADC) is a promising solution
to significantly reduce the power consumption of radio frequency circuits in
massive multiple-input multiple-output (MIMO) systems. In this letter, we
investigate the uplink spectral efficiency (SE) of massive MIMO systems with
low-resolution ADCs over Rician fading channels, where both perfect and
imperfect channel state information are considered. By modeling the
quantization noise of low-resolution ADCs as an additive quantization noise, we
derive tractable and exact approximation expressions of the uplink SE of
massive MIMO with the typical maximal-ratio combining (MRC) receivers. We also
analyze the impact of the ADC resolution, the Rician $K$-factor, and the number
of antennas on the uplink SE. Our derived results reveal that the use of
low-cost and low-resolution ADCs can still achieve satisfying SE in massive
MIMO systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00153</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00153</id><created>2015-12-01</created><authors><author><keyname>Jalal</keyname><forenames>Ajil</forenames></author><author><keyname>Vaze</keyname><forenames>Rahul</forenames></author><author><keyname>Bhaskar</keyname><forenames>Umang</forenames></author></authors><title>Online Budgeted Repeated Matching</title><categories>cs.DS</categories><comments>Constant competitive ratio for adwords problem without the 'small'
  edge weight restriction</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A basic combinatorial online resource allocation problem is considered, where
multiple servers have individual capacity constraints, and at each time slot, a
set of jobs arrives, that have potentially different weights to different
servers. At each time slot, a one-to-one matching has to be found between jobs
and servers, subject to individual capacity constraints, in an online manner.
The objective is to maximize the aggregate weight of jobs allotted to servers,
summed across time slots and servers, subject to individual capacity
constraints. This problem generalizes the well known adwords problem, and is
also relevant for various other modern applications. A simple greedy algorithm
is shown to be 3-competitive, whenever the weight of any edge is at most half
of the corresponding server capacity. Moreover, a randomized version of the
greedy algorithm is shown to be 6-competitive for the unrestricted edge weights
case. For parallel servers with small-weight jobs, we show that a
load-balancing algorithm is near-optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00156</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00156</id><created>2015-12-01</created><authors><author><keyname>Balkan</keyname><forenames>Ozgur</forenames></author><author><keyname>Kreutz-Delgado</keyname><forenames>Kenneth</forenames></author><author><keyname>Makeig</keyname><forenames>Scott</forenames></author></authors><title>Covariance-domain Dictionary Learning for Overcomplete EEG Source
  Identification</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an algorithm targeting the identification of more sources than
channels for electroencephalography (EEG). Our overcomplete source
identification algorithm, Cov-DL, leverages dictionary learning methods applied
in the covariance-domain. Assuming that EEG sources are uncorrelated within
moving time-windows and the scalp mixing is linear, the forward problem can be
transferred to the covariance domain which has higher dimensionality than the
original EEG channel domain. This allows for learning the overcomplete mixing
matrix that generates the scalp EEG even when there may be more sources than
sensors active at any time segment, i.e. when there are non-sparse sources.
This is contrary to straight-forward dictionary learning methods that are based
on the assumption of sparsity, which is not a satisfied condition in the case
of low-density EEG systems. We present two different learning strategies for
Cov-DL, determined by the size of the target mixing matrix. We demonstrate that
Cov-DL outperforms existing overcomplete ICA algorithms under various scenarios
of EEG simulations and real EEG experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00165</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00165</id><created>2015-12-01</created><authors><author><keyname>Bary</keyname><forenames>Galit</forenames></author></authors><title>Learning Using 1-Local Membership Queries</title><categories>cs.LG cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Classic machine learning algorithms learn from labelled examples. For
example, to design a machine translation system, a typical training set will
consist of English sentences and their translation. There is a stronger model,
in which the algorithm can also query for labels of new examples it creates.
E.g, in the translation task, the algorithm can create a new English sentence,
and request its translation from the user during training. This combination of
examples and queries has been widely studied. Yet, despite many theoretical
results, query algorithms are almost never used. One of the main causes for
this is a report (Baum and Lang, 1992) on very disappointing empirical
performance of a query algorithm. These poor results were mainly attributed to
the fact that the algorithm queried for labels of examples that are artificial,
and impossible to interpret by humans.
  In this work we study a new model of local membership queries (Awasthi et
al., 2012), which tries to resolve the problem of artificial queries. In this
model, the algorithm is only allowed to query the labels of examples which are
close to examples from the training set. E.g., in translation, the algorithm
can change individual words in a sentence it has already seen, and then ask for
the translation. In this model, the examples queried by the algorithm will be
close to natural examples and hence, hopefully, will not appear as artificial
or random. We focus on 1-local queries (i.e., queries of distance 1 from an
example in the training sample). We show that 1-local membership queries are
already stronger than the standard learning model. We also present an
experiment on a well known NLP task of sentiment analysis. In this experiment,
the users were asked to provide more information than merely indicating the
label. We present results that illustrate that this extra information is
beneficial in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00168</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00168</id><created>2015-12-01</created><updated>2015-12-11</updated><authors><author><keyname>Viotti</keyname><forenames>Paolo</forenames></author><author><keyname>Vukoli&#x107;</keyname><forenames>Marko</forenames></author></authors><title>Consistency in Non-Transactional Distributed Storage Systems</title><categories>cs.DC</categories><acm-class>H.3.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the years different meanings have been associated to the word
consistency in the distributed systems community. While in the '80s
&quot;consistency&quot; typically meant &quot;strong consistency&quot;, later defined also as
&quot;linearizability&quot;, in recent years, with the advent of highly available and
scalable systems, the notion of consistency was at the same time both weakened
and blurred.
  In this paper we aim to fill the void in literature, by providing a
structured and comprehensive overview of different consistency notions that
appeared in distributed systems, and in particular storage systems research, in
the last four decades. We overview more than 50 different consistency notions,
ranging from linearizability to eventual and weak consistency, defining
precisely many of these, in particular where the previous definitions were
ambiguous. We further provide a partial order among different consistency
predicates, ordering them by their semantic &quot;strength&quot;, which we believe will
reveal useful in further research. Finally, we map the consistency semantics to
different practical systems and research prototypes.
  The scope of this paper is restricted to non-transactional semantics, i.e.,
those that apply to single storage object operations. As such, our paper
complements the existing surveys done in the context of transactional, database
consistency semantics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00170</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00170</id><created>2015-12-01</created><authors><author><keyname>Cui</keyname><forenames>Yiming</forenames></author><author><keyname>Zhu</keyname><forenames>Conghui</forenames></author><author><keyname>Zhu</keyname><forenames>Xiaoning</forenames></author><author><keyname>Zhao</keyname><forenames>Tiejun</forenames></author></authors><title>Augmenting Phrase Table by Employing Lexicons for Pivot-based SMT</title><categories>cs.CL</categories><comments>7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pivot language is employed as a way to solve the data sparseness problem in
machine translation, especially when the data for a particular language pair
does not exist. The combination of source-to-pivot and pivot-to-target
translation models can induce a new translation model through the pivot
language. However, the errors in two models may compound as noise, and still,
the combined model may suffer from a serious phrase sparsity problem. In this
paper, we directly employ the word lexical model in IBM models as an additional
resource to augment pivot phrase table. In addition, we also propose a phrase
table pruning method which takes into account both of the source and target
phrasal coverage. Experimental result shows that our pruning method
significantly outperforms the conventional one, which only considers source
side phrasal coverage. Furthermore, by including the entries in the lexicon
model, the phrase coverage increased, and we achieved improved results in
Chinese-to-Japanese translation using English as pivot language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00172</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00172</id><created>2015-12-01</created><authors><author><keyname>Bach</keyname><forenames>Sebastian</forenames></author><author><keyname>Binder</keyname><forenames>Alexander</forenames></author><author><keyname>Montavon</keyname><forenames>Gr&#xe9;goire</forenames></author><author><keyname>M&#xfc;ller</keyname><forenames>Klaus-Robert</forenames></author><author><keyname>Samek</keyname><forenames>Wojciech</forenames></author></authors><title>Analyzing Classifiers: Fisher Vectors and Deep Neural Networks</title><categories>cs.CV</categories><comments>17 pages (10 main document + references , 7 appendix) 1 Table 7
  Figures 1 Algorithm submitted to CVPR on 06/11/2025</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fisher Vector classifiers and Deep Neural Networks (DNNs) are popular and
successful algorithms for solving image classification problems. However, both
are generally considered `black box' predictors as the non-linear
transformations involved have so far prevented transparent and interpretable
reasoning. Recently, a principled technique, Layer-wise Relevance Propagation
(LRP), has been developed in order to better comprehend the inherent structured
reasoning of complex nonlinear classification models such as Bag of Feature
models or DNNs. In this paper we (1) extend the LRP framework also for Fisher
Vector classifiers and then use it as analysis tool to (2) quantify the
importance of context for classification, (3) qualitatively compare DNNs
against FV classifiers in terms of important image regions and (4) detect
potential flaws and biases in data. All experiments are performed on the PASCAL
VOC 2007 data set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00174</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00174</id><created>2015-12-01</created><authors><author><keyname>Chaplick</keyname><forenames>Steven</forenames></author><author><keyname>Gu&#x15b;piel</keyname><forenames>Grzegorz</forenames></author><author><keyname>Gutowski</keyname><forenames>Grzegorz</forenames></author><author><keyname>Krawczyk</keyname><forenames>Tomasz</forenames></author><author><keyname>Liotta</keyname><forenames>Giuseppe</forenames></author></authors><title>The Partial Visibility Representation Extension Problem</title><categories>cs.CG</categories><msc-class>68U05</msc-class><acm-class>I.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a graph $G$, a function $\psi$ is called a \emph{bar visibility
representation} of $G$ when for each vertex $v \in V(G)$, $\psi(v)$ is a
horizontal line segment (\emph{bar}) and $uv \in E(G)$ iff there is an
unobstructed, vertical, $\varepsilon$-wide line of sight between $\psi(u)$ and
$\psi(v)$. Graphs admitting such representations are well understood (via
simple characterizations) and recognizable in linear time. For a directed graph
$G$, a bar visibility representation $\psi$ of $G$, additionally, puts the bar
$\psi(u)$ strictly below the bar $\psi(v)$ for each directed edge $(u,v)$ of
$G$. We study a generalization of the recognition problem where a function
$\psi'$ defined on a subset $V'$ of $V(G)$ is given and the question is whether
there is a bar visibility representation $\psi$ of $G$ with $\psi(v) =
\psi'(v)$ for every $v \in V'$. We show that for undirected graphs this problem
together with closely related problems are \NP-complete, but for certain cases
involving directed graphs it is solvable in polynomial time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00177</identifier>
 <datestamp>2016-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00177</id><created>2015-12-01</created><updated>2016-03-07</updated><authors><author><keyname>Cui</keyname><forenames>Yiming</forenames></author><author><keyname>Wang</keyname><forenames>Shijin</forenames></author><author><keyname>Li</keyname><forenames>Jianfeng</forenames></author><author><keyname>Wang</keyname><forenames>Yuguang</forenames></author></authors><title>LSTM Neural Reordering Feature for Statistical Machine Translation</title><categories>cs.CL cs.AI cs.NE</categories><comments>6 pages, withdrawn by the author due to a error in formula</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Artificial neural networks are powerful models, which have been widely
applied into many aspects of machine translation, such as language modeling and
translation modeling. Though notable improvements have been made in these
areas, the reordering problem still remains a challenge in statistical machine
translations. In this paper, we present a novel neural reordering model that
directly models word pairs and alignment. By utilizing LSTM recurrent neural
networks, much longer context could be learned for reordering prediction.
Experimental results on NIST OpenMT12 Arabic-English and Chinese-English
1000-best rescoring task show that our LSTM neural reordering feature is robust
and achieves significant improvements over various baseline systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00180</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00180</id><created>2015-12-01</created><authors><author><keyname>Lesnick</keyname><forenames>Michael</forenames></author><author><keyname>Wright</keyname><forenames>Matthew</forenames></author></authors><title>Interactive Visualization of 2-D Persistence Modules</title><categories>math.AT cs.CG math.AC</categories><comments>75 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of this work is to extend the standard persistent homology pipeline
for exploratory data analysis to the 2-D persistence setting, in a practical,
computationally efficient way. To this end, we introduce RIVET, a software tool
for the visualization of 2-D persistence modules, and present mathematical
foundations for this tool. RIVET provides an interactive visualization of the
barcodes of 1-D affine slices of a 2-D persistence module $M$. It also computes
and visualizes the dimension of each vector space in $M$ and the bigraded Betti
numbers of $M$. At the heart of our computational approach is a novel data
structure based on planar line arrangements, on which we can perform fast
queries to find the barcode of any slice of $M$. We present an efficient
algorithm for constructing this data structure and establish bounds on its
complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00184</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00184</id><created>2015-12-01</created><authors><author><keyname>Friedrich</keyname><forenames>Tobias</forenames></author><author><keyname>Krohmer</keyname><forenames>Anton</forenames></author></authors><title>On the diameter of hyperbolic random graphs</title><categories>cs.DM</categories><journal-ref>42nd International Colloquium on Automata, Languages, and
  Programming, ICALP 2015, pp. 614--625</journal-ref><doi>10.1007/978-3-662-47666-6_49</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large real-world networks are typically scale-free. Recent research has shown
that such graphs are described best in a geometric space. More precisely, the
internet can be mapped to a hyperbolic space such that geometric greedy routing
performs close to optimal (Bogun\'a, Papadopoulos, and Krioukov. Nature
Communications, 1:62, 2010). This observation pushed the interest in hyperbolic
networks as a natural model for scale-free networks. Hyperbolic random graphs
follow a power-law degree distribution with controllable exponent $\beta$ and
show high clustering (Gugelmann, Panagiotou, and Peter. ICALP, pp. 573-585,
2012).
  For understanding the structure of the resulting graphs and for analyzing the
behavior of network algorithms, the next question is bounding the size of the
diameter. The only known explicit bound is $\mathcal O((\log
n)^{32/((3-\beta)(5-\beta)) + 1})$ (Kiwi and Mitsche. ANALCO, pp. 26-39, 2015).
We present two much simpler proofs for an improved upper bound of $\mathcal
O((\log n)^{2/(3-\beta)})$ and a lower bound of $\Omega(\log n)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00196</identifier>
 <datestamp>2015-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00196</id><created>2015-12-01</created><updated>2015-12-03</updated><authors><author><keyname>Sch&#xf6;nig</keyname><forenames>Stefan</forenames></author></authors><title>SQL Queries for Declarative Process Mining on Event Logs of Relational
  Databases</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Flexible business processes can often be modelled more easily using a
declarative rather than a procedural modelling approach. Process mining aims at
automating the discovery of business process models. Existing declarative
process mining approaches either su?er performance issues with real-life event
logs or limit their expressiveness to a specifi?c set of constaint types.
Lately, with RelationalXES a relational database architecture for storing event
log data has been introduced. In this technical report, we introduce a mining
approach that directly works on relational event data by querying the log with
conventional SQL. We provide a list of SQL queries for discovering a set of
commonly used and mined process constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00198</identifier>
 <datestamp>2015-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00198</id><created>2015-12-01</created><updated>2015-12-04</updated><authors><author><keyname>Largillier</keyname><forenames>Thomas</forenames></author><author><keyname>Peyronnet</keyname><forenames>Guillaume</forenames></author><author><keyname>Peyronnet</keyname><forenames>Sylvain</forenames></author></authors><title>Efficient filtering of adult content using textual information</title><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays adult content represents a non negligible proportion of the Web
content. It is of the utmost importance to protect children from this content.
Search engines, as an entry point for Web navigation are ideally placed to deal
with this issue.
  In this paper, we propose a method that builds a safe index i.e.
adult-content free for search engines. This method is based on a filter that
uses only textual information from the web page and the associated URL.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00201</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00201</id><created>2015-12-01</created><authors><author><keyname>Kasem-Madani</keyname><forenames>Saffija</forenames></author><author><keyname>Meier</keyname><forenames>Michael</forenames></author></authors><title>Security and Privacy Policy Languages: A Survey, Categorization and Gap
  Identification</title><categories>cs.CR</categories><comments>13 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For security and privacy management and enforcement purposes, various policy
languages have been presented. We give an overview on 27 security and privacy
policy languages and present a categorization framework for policy languages.
We show how the current policy languages are represented in the framework and
summarize our interpretation. We show up identified gaps and motivate for the
adoption of policy languages for the specification of privacy-utility trade-off
policies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00210</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00210</id><created>2015-12-01</created><authors><author><keyname>Meidlinger</keyname><forenames>Michael</forenames></author><author><keyname>Balatsoukas-Stimming</keyname><forenames>Alexios</forenames></author><author><keyname>Burg</keyname><forenames>Andreas</forenames></author><author><keyname>Matz</keyname><forenames>Gerald</forenames></author></authors><title>Quantized Message Passing for LDPC Codes</title><categories>cs.IT math.IT</categories><comments>2015 Asilomar Conference on Signals, Systems, and Computer</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a quantized decoding algorithm for low- density parity-check codes
where the variable node update rule of the standard min-sum algorithm is
replaced with a look-up table (LUT) that is designed using an
information-theoretic criterion. We show that even with message resolutions as
low as 3 bits, the proposed algorithm can achieve better error rates than a
floating-point min-sum decoder. Moreover, we study in detail the effect of
different decoder design parameters, like the design SNR and the LUT tree
structure on the performance of our decoder, and we propose some complexity
reduction techniques, such as LUT re-use and message alphabet downsizing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00213</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00213</id><created>2015-12-01</created><updated>2016-03-03</updated><authors><author><keyname>Ksairi</keyname><forenames>Nassar</forenames></author><author><keyname>Tomasin</keyname><forenames>Stefano</forenames></author><author><keyname>Debbah</keyname><forenames>M&#xe9;rouane</forenames></author></authors><title>A Multi-Service Oriented Multiple-Access Scheme for Next-Generation
  Mobile Networks</title><categories>cs.IT math.IT</categories><comments>5 pages, 3 figures, submitted to the European Conference on Networks
  and Communications (EuCNC 2016)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the key requirements for fifth-generation (5G) cellular networks is
their ability to handle densely connected devices with different quality of
service (QoS) requirements. In this article, we present multi-service oriented
multiple access (MOMA), an integrated access scheme for massive connections
with diverse QoS profiles and/or traffic patterns originating from both
handheld devices and machine-to-machine (M2M) transmissions. MOMA is based on
a) stablishing separate classes of users based on relevant criteria that go
beyond the simple handheld/M2M split, b) class dependent hierarchical spreading
of the data signal and c) a mix of multiuser and single-user detection schemes
at the receiver. Practical implementations of the MOMA principle are provided
for base stations (BSs) that are equipped with a large number of antenna
elements. Finally, it is shown that such a
massive-multiple-input-multiple-output (MIMO) scenario enables the achievement
of all the benefits of MOMA even with a simple receiver structure that allows
to concentrate the receiver complexity where effectively needed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00215</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00215</id><created>2015-12-01</created><authors><author><keyname>Mijumbi</keyname><forenames>Rashid</forenames></author></authors><title>On the Energy Efficiency Prospects of Network Function Virtualization</title><categories>cs.NI</categories><comments>arXiv admin note: text overlap with arXiv:1509.07675</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network Function Virtualization (NFV) has recently received significant
attention as an innovative way of deploying network services. By decoupling
network functions from the physical equipment on which they run, NFV has been
proposed as passage towards service agility, better time-to-market, and reduced
Capital Expenses (CAPEX) and Operating Expenses (OPEX). One of the main selling
points of NFV is its promise for better energy efficiency resulting from
consolidation of resources as well as their more dynamic utilization. However,
there are currently no studies or implementations which attach values to energy
savings that can be expected, which could make it hard for Telecommunication
Service Providers (TSPs) to make investment decisions. In this paper, we
utilize Bell Labs' GWATT tool to estimate the energy savings that could result
from the three main NFV use cases Virtualized Evolved Packet Core (VEPC),
Virtualized Customer Premises Equipment (VCPE) and Virtualized Radio Access
Network (VRAN). We determine that the part of the mobile network with the
highest energy utilization prospects is the Evolved Packet Core (EPC) where
virtualization of functions leads to a 22% reduction in energy consumption and
a 32% enhancement in energy efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00217</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00217</id><created>2015-12-01</created><authors><author><keyname>Mijumbi</keyname><forenames>Rashid</forenames></author></authors><title>Placement and Scheduling of Functions in Network Function Virtualization</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The virtualization of Radio Access Networks (RANs) has been proposed as one
of the important use cases of Network Function Virtualization (NFV). In VRAN!s
(VRAN!s), some functions from a Base Station (BS), such as those which make up
the Base Band Unit (BBU), may be implemented in a shared infrastructure located
at either a data center or distributed in network nodes. For the latter option,
one challenge is in deciding which subset of the available network nodes can be
used to host the physical BBU servers (the placement problem), and then to
which of the available physical BBUs each Remote Radio Head (RRH) should be
assigned (the assignment problem). These two problems constitute what we refer
to as the VRANPAP! (VRAN-PAP!). In this paper, we start by formally defining
the VRAN-PAP! before formulating it as a Binary Integer Linear Program (BILP)
whose objective is to minimize the server and front haul link setup costs as
well as the latency between each RRH and its assigned BBU. Since the BILP could
become computationally intractable, we also propose a greedy approximation for
larger instances of the VRAN-PAP!. We perform simulations to compare both
algorithms in terms of solution quality as well as computation time under
varying network sizes and setup budgets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00219</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00219</id><created>2015-12-01</created><authors><author><keyname>Mijumbi</keyname><forenames>Rashid</forenames></author></authors><title>Placement and Assignment of Servers in Virtualized Radio Access Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The virtualization of Radio Access Networks (RANs) has been proposed as one
of the important use cases of Network Function Virtualization (NFV). In
Virtualized Radio Access Networks (VRANs), some functions from a Base Station
(BS), such as those which make up the Base Band Unit (BBU), may be implemented
in a shared infrastructure located at either a data center or distributed in
network nodes. For the latter option, one challenge is in deciding which subset
of the available network nodes can be used to host the physical BBU servers
(the placement problem), and then to which of the available physical BBUs each
Remote Radio Head (RRH) should be assigned (the assignment problem). These two
problems constitute what we refer to as the VRAN Placement and Assignment
Problem (VRAN-PAP). In this paper, we start by formally defining the VRAN-PAP
before formulating it as a Binary Integer Linear Program (BILP) whose objective
is to minimize the server and front haul link setup costs as well as the
latency between each RRH and its assigned BBU. Since the BILP could become
computationally intractable, we also propose a greedy approximation for larger
instances of the VRAN-PAP. We perform simulations to compare both algorithms in
terms of solution quality as well as computation time under varying network
sizes and setup budgets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00226</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00226</id><created>2015-12-01</created><authors><author><keyname>Sharma</keyname><forenames>Naresh</forenames></author></authors><title>More on a trace inequality in quantum information theory</title><categories>quant-ph cs.IT math.IT</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is known that for a completely positive and trace preserving (cptp) map
${\cal N}$, $\text{Tr}$ $\exp$$\{ \log \sigma$ $+$ ${\cal N}^\dagger [\log
{\cal N}(\rho)$ $-\log {\cal N}(\sigma)] \}$ $\leqslant$ $\text{Tr}$ $\rho$
when $\rho$, $\sigma$, ${\cal N}(\rho)$, and ${\cal N}(\sigma)$ are strictly
positive. We state and prove a relevant version of this inequality for the
hitherto unaddressed case of these matrices being nonnegative. Our treatment
also provides an alternate proof for the strictly positive case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00228</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00228</id><created>2015-12-01</created><updated>2016-01-21</updated><authors><author><keyname>Rosales-M&#xe9;ndez</keyname><forenames>Henry</forenames></author><author><keyname>Ram&#xed;rez-Cruz</keyname><forenames>Yunior</forenames></author></authors><title>MOCICE-BCubed F$_1$: A New Evaluation Measure for Biclustering
  Algorithms</title><categories>cs.LG cs.IR</categories><acm-class>I.5.3; H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The validation of biclustering algorithms remains a challenging task, even
though a number of measures have been proposed for evaluating the quality of
these algorithms. Although no criterion is universally accepted as the overall
best, a number of meta-evaluation conditions to be satisfied by biclustering
algorithms have been enunciated. In this work, we present MOCICE-BCubed F$_1$,
a new external measure for evaluating biclusterings, in the scenario where gold
standard annotations are available for both the object clusters and the
associated feature subspaces. Our proposal relies on the so-called
micro-objects transformation and satisfies the most comprehensive set of
meta-evaluation conditions so far enunciated for biclusterings. Additionally,
the proposed measure adequately handles the occurrence of overlapping in both
the object and feature spaces. Moreover, when used for evaluating traditional
clusterings, which are viewed as a particular case of biclustering, the
proposed measure also satisfies the most comprehensive set of meta-evaluation
conditions so far enunciated for this task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00237</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00237</id><created>2015-12-01</created><authors><author><keyname>An</keyname><forenames>Dongsheng</forenames></author><author><keyname>Suo</keyname><forenames>Jinli</forenames></author><author><keyname>Ji</keyname><forenames>Xiangyang</forenames></author><author><keyname>Wang</keyname><forenames>Haoqian</forenames></author><author><keyname>Dai</keyname><forenames>Qionghai</forenames></author></authors><title>Fast and High Quality Highlight Removal from A Single Image</title><categories>cs.CV</categories><comments>11 pages, 10 figures, submitted to IEEE TIP</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Specular reflection exists widely in photography and causes the recorded
color deviating from its true value, so fast and high quality highlight removal
from a single nature image is of great importance. In spite of the progress in
the past decades in highlight removal, achieving wide applicability to the
large diversity of nature scenes is quite challenging. To handle this problem,
we propose an analytic solution to highlight removal based on an L2
chromaticity definition and corresponding dichromatic model. Specifically, this
paper derives a normalized dichromatic model for the pixels with identical
diffuse color: a unit circle equation of projection coefficients in two
subspaces that are orthogonal to and parallel with the illumination,
respectively. In the former illumination orthogonal subspace, which is
specular-free, we can conduct robust clustering with an explicit criterion to
determine the cluster number adaptively. In the latter illumination parallel
subspace, a property called pure diffuse pixels distribution rule (PDDR) helps
map each specular-influenced pixel to its diffuse component. In terms of
efficiency, the proposed approach involves few complex calculation, and thus
can remove highlight from high resolution images fast. Experiments show that
this method is of superior performance in various challenging cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00242</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00242</id><created>2015-12-01</created><authors><author><keyname>Wu</keyname><forenames>Haibing</forenames></author><author><keyname>Gu</keyname><forenames>Xiaodong</forenames></author></authors><title>Towards Dropout Training for Convolutional Neural Networks</title><categories>cs.LG cs.CV cs.NE</categories><comments>This paper has been published in Neural Networks,
  http://www.sciencedirect.com/science/article/pii/S0893608015001446</comments><journal-ref>Neural Networks 71: 1-10 (2015)</journal-ref><doi>10.1016/j.neunet.2015.07.007</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, dropout has seen increasing use in deep learning. For deep
convolutional neural networks, dropout is known to work well in fully-connected
layers. However, its effect in convolutional and pooling layers is still not
clear. This paper demonstrates that max-pooling dropout is equivalent to
randomly picking activation based on a multinomial distribution at training
time. In light of this insight, we advocate employing our proposed
probabilistic weighted pooling, instead of commonly used max-pooling, to act as
model averaging at test time. Empirical evidence validates the superiority of
probabilistic weighted pooling. We also empirically show that the effect of
convolutional dropout is not trivial, despite the dramatically reduced
possibility of over-fitting due to the convolutional architecture. Elaborately
designing dropout training simultaneously in max-pooling and fully-connected
layers, we achieve state-of-the-art performance on MNIST, and very competitive
results on CIFAR-10 and CIFAR-100, relative to other approaches without data
augmentation. Finally, we compare max-pooling dropout and stochastic pooling,
both of which introduce stochasticity based on multinomial distributions at
pooling stage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00250</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00250</id><created>2015-12-01</created><updated>2015-12-11</updated><authors><author><keyname>Ghazi-Zahedi</keyname><forenames>Keyan</forenames></author><author><keyname>Haeufle</keyname><forenames>Daniel F. B.</forenames></author><author><keyname>Montufar</keyname><forenames>Guido</forenames></author><author><keyname>Schmitt</keyname><forenames>Syn</forenames></author><author><keyname>Ay</keyname><forenames>Nihat</forenames></author></authors><title>Evaluating Morphological Computation in Muscle and DC-motor Driven
  Models of Human Hopping</title><categories>cs.AI cs.IT cs.RO math.IT</categories><comments>10 pages, 4 figures, 1 table, 5 algorithms</comments><msc-class>68T40, 97R40, 68Q30, 92C10</msc-class><acm-class>I.2; I.2.m</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the context of embodied artificial intelligence, morphological computation
refers to processes which are conducted by the body (and environment) that
otherwise would have to be performed by the brain. Exploiting environmental and
morphological properties is an important feature of embodied systems. The main
reason is that it allows to significantly reduce the controller complexity. An
important aspect of morphological computation is that it cannot be assigned to
an embodied system per se, but that it is, as we show, behavior- and
state-dependent. In this work, we evaluate two different measures of
morphological computation that can be applied in robotic systems and in
computer simulations of biological movement. As an example, these measures were
evaluated on muscle and DC-motor driven hopping models. We show that a
state-dependent analysis of the hopping behaviors provides additional insights
that cannot be gained from the averaged measures alone. This work includes
algorithms and computer code for the measures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00259</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00259</id><created>2015-12-01</created><authors><author><keyname>Saltarin</keyname><forenames>Jonnahtan</forenames></author><author><keyname>Bourtsoulatze</keyname><forenames>Eirina</forenames></author><author><keyname>Thomos</keyname><forenames>Nikolaos</forenames></author><author><keyname>Braun</keyname><forenames>Torsten</forenames></author></authors><title>NetCodCCN: a Network Coding approach for Content-Centric Networks</title><categories>cs.NI</categories><comments>Accepted for inclusion in the IEEE INFOCOM 2016 technical program</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Content-Centric Networking (CCN) naturally supports multi-path communication,
as it allows the simultaneous use of multiple interfaces (e.g. LTE and WiFi).
When multiple sources and multiple clients are considered, the optimal set of
distribution trees should be determined in order to optimally use all the
available interfaces. This is not a trivial task, as it is a computationally
intense procedure that should be done centrally. The need for central
coordination can be removed by employing network coding, which also offers
improved resiliency to errors and large throughput gains. In this paper, we
propose NetCodCCN, a protocol for integrating network coding in CCN. In
comparison to previous works proposing to enable network coding in CCN,
NetCodCCN permit Interest aggregation and Interest pipelining, which reduce the
data retrieval times. The experimental evaluation shows that the proposed
protocol leads to significant improvements in terms of content retrieval delay
compared to the original CCN. Our results demonstrate that the use of network
coding adds robustness to losses and permits to exploit more efficiently the
available network resources. The performance gains are verified for content
retrieval in various network scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00272</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00272</id><created>2015-10-30</created><authors><author><keyname>Skipsey</keyname><forenames>Samuel Cadellin</forenames></author><author><keyname>De Witt</keyname><forenames>Shaun</forenames></author><author><keyname>Dewhurst</keyname><forenames>Alastair</forenames></author><author><keyname>Britton</keyname><forenames>David</forenames></author><author><keyname>Roy</keyname><forenames>Gareth</forenames></author><author><keyname>Crooks</keyname><forenames>David</forenames></author></authors><title>Enabling Object Storage via shims for Grid Middleware</title><categories>physics.comp-ph cs.DC hep-ex</categories><comments>21st International Conference on Computing in High Energy and Nuclear
  Physics (CHEP2015)</comments><doi>10.1088/1742-6596/664/4/042052</doi><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  The Object Store model has quickly become the basis of most commercially
successful mass storage infrastructure, backing so-called &quot;Cloud&quot; storage such
as Amazon S3, but also underlying the implementation of most parallel
distributed storage systems. Many of the assumptions in Object Store design are
similar, but not identical, to concepts in the design of Grid Storage Elements,
although the requirement for &quot;POSIX-like&quot; filesystem structures on top of SEs
makes the disjunction seem larger. As modern Object Stores provide many
features that most Grid SEs do not (block level striping, parallel access,
automatic file repair, etc.), it is of interest to see how easily we can
provide interfaces to typical Object Stores via plugins and shims for Grid
tools, and how well experiments can adapt their data models to them. We present
evaluation of, and first-deployment experiences with, (for example) Xrootd-Ceph
interfaces for direct object-store access, as part of an initiative within
GridPP\cite{GridPP} hosted at RAL. Additionally, we discuss the tradeoffs and
experience of developing plugins for the currently-popular {\it Ceph} parallel
distributed filesystem for the GFAL2 access layer, at Glasgow.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00274</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00274</id><created>2015-12-01</created><updated>2016-02-11</updated><authors><author><keyname>Dubrova</keyname><forenames>Elena</forenames></author></authors><title>On Constructing Secure and Hardware-Efficient Invertible Mappings</title><categories>cs.CR</categories><comments>6 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our society becomes increasingly dependent on wireless communications. The
tremendous growth in the number and type of wirelessly connected devices in a
combination with the dropping cost for performing cyberattacks create new
challenges for assuring security of services and applications provided by the
next generation of wireless communication networks. The situation is
complicated even further by the fact that many end-point Internet of Things
(IoT) devices have very limited resources for implementing security
functionality. This paper addresses one of the aspects of this important,
many-faceted problem - the design of hardware-efficient cryptographic
primitives suitable for the protection of resource-constrained IoT devices. We
focus on cryptographic primitives based on the invertible mappings of type
$\{0,1,\ldots,2^n-1\} \rightarrow \{0,1,\ldots,2^n-1\}$. In order to check if a
given mapping is invertible or not, we generally need an exponential in $n$
number of steps. In this paper, we derive a sufficient condition for
invertibility which can be checked in $O(n^2 N)$ time, where $N$ is the size of
representation of the largest function in the mapping. Our results can be used
for constructing cryptographically secure invertible mappings which can be
efficiently implemented in hardware.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00296</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00296</id><created>2015-12-01</created><authors><author><keyname>Jayaram</keyname><forenames>Vinay</forenames></author><author><keyname>Alamgir</keyname><forenames>Morteza</forenames></author><author><keyname>Altun</keyname><forenames>Yasemin</forenames></author><author><keyname>Sch&#xf6;lkopf</keyname><forenames>Bernhard</forenames></author><author><keyname>Grosse-Wentrup</keyname><forenames>Moritz</forenames></author></authors><title>Transfer Learning in Brain-Computer Interfaces</title><categories>cs.HC q-bio.NC</categories><comments>To be published in IEEE Computational Intelligence Magazine, special
  BCI issue on January 15th online</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The performance of brain-computer interfaces (BCIs) improves with the amount
of available training data, the statistical distribution of this data, however,
varies across subjects as well as across sessions within individual subjects,
limiting the transferability of training data or trained models between them.
In this article, we review current transfer learning techniques in BCIs that
exploit shared structure between training data of multiple subjects and/or
sessions to increase performance. We then present a framework for transfer
learning in the context of BCIs that can be applied to any arbitrary feature
space, as well as a novel regression estimation method that is specifically
designed for the structure of a system based on the electroencephalogram (EEG).
We demonstrate the utility of our framework and method on subject-to-subject
transfer in a motor-imagery paradigm as well as on session-to-session transfer
in one patient diagnosed with amyotrophic lateral sclerosis (ALS), showing that
it is able to outperform other comparable methods on an identical dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00297</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00297</id><created>2015-12-01</created><authors><author><keyname>Iacovacci</keyname><forenames>Jacopo</forenames></author><author><keyname>Lacasa</keyname><forenames>Lucas</forenames></author></authors><title>Visibility graph motifs</title><categories>physics.data-an cs.LG nlin.CD</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Visibility algorithms transform time series into graphs and encode dynamical
information in their topology, paving the way for graph-theoretical time series
analysis as well as building a bridge between nonlinear dynamics and network
science. In this work we introduce and study the concept of visibility graph
motifs, smaller substructures that appear with characteristic frequencies. We
develop a theory to compute in an exact way the motif profiles associated to
general classes of deterministic and stochastic dynamics. We find that this
simple property is indeed a highly informative and computationally efficient
feature capable to distinguish among different dynamics and robust against
noise contamination. We finally confirm that it can be used in practice to
perform unsupervised learning, by extracting motif profiles from experimental
heart-rate series and being able, accordingly, to disentangle meditative from
other relaxation states. Applications of this general theory include the
automatic classification and description of physical, biological, and financial
time series.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00298</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00298</id><created>2015-12-01</created><authors><author><keyname>Burger</keyname><forenames>Martin</forenames></author><author><keyname>Dirks</keyname><forenames>Hendrik</forenames></author><author><keyname>Frerking</keyname><forenames>Lena</forenames></author></authors><title>On Optical Flow Models for Variational Motion Estimation</title><categories>math.NA cs.CV math.OC</categories><comments>27 pages, 3 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this paper is to discuss and evaluate total variation based
regularization methods for motion estimation, with particular focus on optical
flow models. In addition to standard $L^2$ and $L^1$ data fidelities we give an
overview of different variants of total variation regularization obtained from
combination with higher order models and a unified computational optimization
approach based on primal-dual methods. Moreover, we extend the models by
Bregman iterations and provide an inverse problems perspective to the analysis
of variational optical flow models. A particular focus of the paper is the
quantitative evaluation of motion estimation, which is a difficult and often
underestimated task. We discuss several approaches for quality measures of
motion estimation and apply them to compare the previously discussed
regularization approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00306</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00306</id><created>2015-12-01</created><authors><author><keyname>Du</keyname><forenames>Wei Lin</forenames></author><author><keyname>Capretz</keyname><forenames>Luiz Fernando</forenames></author><author><keyname>Nassif</keyname><forenames>Ali Bou</forenames></author><author><keyname>Ho</keyname><forenames>Danny</forenames></author></authors><title>A Hybrid Intelligent Model for Software Cost Estimation</title><categories>cs.SE cs.AI</categories><journal-ref>Journal of Computer Science, 9(11):1506-1513, 2013</journal-ref><doi>10.3844/ajbb.2013.1506-1513</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Accurate software development effort estimation is critical to the success of
software projects. Although many techniques and algorithmic models have been
developed and implemented by practitioners, accurate software development
effort prediction is still a challenging endeavor in the field of software
engineering, especially in handling uncertain and imprecise inputs and
collinear characteristics. In this paper, a hybrid in-telligent model combining
a neural network model integrated with fuzzy model (neuro-fuzzy model) has been
used to improve the accuracy of estimating software cost. The performance of
the proposed model is assessed by designing and conducting evaluation with
published project and industrial data. Results have shown that the proposed
model demonstrates the ability of improving the estimation accuracy by 18%
based on the Mean Magnitude of Relative Error (MMRE) criterion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00307</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00307</id><created>2015-12-01</created><authors><author><keyname>Engwirda</keyname><forenames>Darren</forenames></author></authors><title>Multi-resolution unstructured grid-generation for geophysical
  applications on the sphere</title><categories>physics.ao-ph cs.CG math.NA physics.comp-ph physics.flu-dyn</categories><comments>This paper was presented as a research note at the 24th International
  Meshing Roundtable, University of Texas at Austin, October, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An algorithm for the generation of non-uniform unstructured grids on
ellipsoidal geometries is described. This technique is designed to generate
high quality triangular and polygonal meshes appropriate for general
circulation modelling on the sphere, including applications to atmospheric and
ocean simulation, and numerical weather predication. Using a recently developed
Frontal-Delaunay-refinement technique, a method for the construction of
high-quality unstructured ellipsoidal Delaunay triangulations is introduced. A
dual polygonal grid, derived from the associated Voronoi diagram, is also
optionally generated as a by-product. Compared to existing techniques, it is
shown that the Frontal-Delaunay approach typically produces grids with
near-optimal element quality and smooth grading characteristics, while imposing
relatively low computational expense. Initial results are presented for a
selection of uniform and non-uniform ellipsoidal grids appropriate for
large-scale geophysical applications. The use of user-defined mesh-sizing
functions to generate smoothly graded, non-uniform grids is discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00308</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00308</id><created>2015-12-01</created><authors><author><keyname>Ahmed</keyname><forenames>Faheem</forenames></author><author><keyname>Campbell</keyname><forenames>Piers</forenames></author><author><keyname>Jaffar</keyname><forenames>Ahmad</forenames></author><author><keyname>Capretz</keyname><forenames>Luiz Fernando</forenames></author></authors><title>Managing Support Requests in Open Source Software Project: The Role of
  Online Forums</title><categories>cs.SE cs.CY</categories><comments>arXiv admin note: substantial text overlap with arXiv:1507.06927</comments><journal-ref>2nd IEEE International Conference on Computer Science and
  Information Technology, pp. 590-594, 2009</journal-ref><doi>10.1109/ICCSIT.2009.5234491</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of free and open source software is gaining momentum due to the ever
increasing availability and use of the Internet. Organizations are also now
adopting open source software, despite some reservations in particular
regarding the provision and availability of support. One of the greatest
concerns about free and open source software is the availability of post
release support and the handling of for support. A common belief is that there
is no appropriate support available for this class of software, while an
alternative argument is that due to the active involvement of Internet users in
online forums, there is in fact a large resource available that communicates
and manages the management of support requests. The research model of this
empirical investigation establishes and studies the relationship between open
source software support requests and online public forums. The results of this
empirical study provide evidence about the realities of support that is present
in open source software projects. We used a dataset consisting of 616 open
source software projects covering a broad range of categories in this
investigation. The results show that online forums play a significant role in
managing support requests in open source software, thus becoming a major source
of assistance in maintenance of the open source projects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00312</identifier>
 <datestamp>2015-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00312</id><created>2015-11-26</created><authors><author><keyname>Aristov</keyname><forenames>Anton</forenames></author></authors><title>The Quasi cellular nets-based models of transport and logistic systems</title><categories>cs.OH</categories><msc-class>68R99</msc-class><journal-ref>Reports of the XXIII XXIII International Scientific Symposium
  Miner's Week, 2015, Moscow : NUST MISIS., PP. 280-287</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  There are many systems in different subjects such as industry, medicine,
transport, social and others, can be discribed on their dynamic of flows.
Nowadays models of flows consist of micro- and macro-models. In practice there
is a problem of convertation from different levels of simulation. In the
different articles author descriptes quasi cellular nets. Quasi cellular nets
are new type of discrete structures without signature. It may be used for
simulation instruments. This structures can simulate flows on micro- and macro
levels on the single model structure. In this article described using quasi
cellular nets in transport and logistics of open-cast mining.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00313</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00313</id><created>2015-12-01</created><authors><author><keyname>Yamany</keyname><forenames>Hany F. El</forenames></author><author><keyname>Capretz</keyname><forenames>Miriam</forenames></author><author><keyname>Capretz</keyname><forenames>Luiz Fernando</forenames></author></authors><title>A Multi-Agent Framework for Testing Distributed Systems</title><categories>cs.DC cs.SE</categories><journal-ref>30th IEEE International Computer Software and Applications
  Conference (COMPSAC), Chicago, USA, Volume II, pp. 151-156, 2006</journal-ref><doi>10.1109/COMPSAC.2006.98</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software testing is a very expensive and time consuming process. It can
account for up to 50% of the total cost of the software development.
Distributed systems make software testing a daunting task. The research
described in this paper investigates a novel multi-agent framework for testing
3-tier distributed systems. This paper describes the framework architecture as
well as the communication mechanism among agents in the architecture. Web-based
application is examined as a case study to validate the proposed framework. The
framework is considered as a step forward to automate testing for distributed
systems in order to enhance their reliability within an acceptable range of
cost and time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00327</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00327</id><created>2015-12-01</created><authors><author><keyname>Wagner</keyname><forenames>Isabel</forenames></author><author><keyname>Eckhoff</keyname><forenames>David</forenames></author></authors><title>Technical Privacy Metrics: a Systematic Survey</title><categories>cs.CR cs.IT cs.PF math.IT</categories><comments>35 pages, submitted to ACM Computing Surveys</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of privacy metrics is to measure the degree of privacy enjoyed by
users in a system and the amount of protection offered by privacy-enhancing
technologies. In this way, privacy metrics contribute to improving user privacy
in the digital world. The diversity and complexity of privacy metrics in the
literature makes an informed choice of metrics challenging. As a result,
redundant new metrics are proposed frequently, and privacy studies are often
incomparable. In this survey we alleviate these problems by structuring the
landscape of privacy metrics. For this we explain and discuss a selection of
over eighty privacy metrics and introduce a categorization based on the aspect
of privacy they measure, their required inputs, and the type of data that needs
protection. In addition, we present a method on how to choose privacy metrics
based on eight questions that help identify the right privacy metrics for a
given scenario, and highlight topics where additional work on privacy metrics
is needed. Our survey spans multiple privacy domains and can be understood as a
general framework for privacy measurement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00333</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00333</id><created>2015-12-01</created><authors><author><keyname>Fluschnik</keyname><forenames>Till</forenames></author><author><keyname>Hermelin</keyname><forenames>Danny</forenames></author><author><keyname>Nichterlein</keyname><forenames>Andr&#xe9;</forenames></author><author><keyname>Niedermeier</keyname><forenames>Rolf</forenames></author></authors><title>Fractals for Kernelization Lower Bounds, With an Application to
  Length-Bounded Cut Problems</title><categories>cs.CC cs.DM cs.DS</categories><comments>27 pages, 9 figures</comments><msc-class>68Q17, 68Q25, 68W40, 68R10</msc-class><acm-class>F.1.3; F.2.2; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bodlaender et al.'s [SIDMA 2014] cross-composition technique is a popular
method for excluding polynomial-size problem kernels for NP-hard parameterized
problems. We present a new technique exploiting triangle-based fractal
structures for extending the range of applicability of cross-compositions. Our
technique makes it possible to prove new no-polynomial-kernel results for a
number of problems dealing with length-bounded cuts. In particular, answering
an open question of Golovach and Thilikos [Discrete Optim. 2011], we show that,
unless a collapse in the Polynomial Hierarchy occurs, the NP-hard
Length-Bounded Edge-Cut problem (delete at most $k$ edges such that the
resulting graph has no $s$-$t$ path of length shorter than $\ell$)
parameterized by the combined $k$ and $\ell$ has no polynomial-size problem
kernel. Our framework applies to planar as well as directed variants of the
basic problems and also applies to both edge and vertex deletion problems.
  Key words: Fixed-parameter tractability; polynomial kernels; kernelization;
kernel lower bounds; cross-compositions; graph modification problems; fractals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00344</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00344</id><created>2015-12-01</created><authors><author><keyname>Britton</keyname><forenames>Tom</forenames></author><author><keyname>Juher</keyname><forenames>David</forenames></author><author><keyname>Saldana</keyname><forenames>Joan</forenames></author></authors><title>A network epidemic model with preventive rewiring: comparative analysis
  of the initial phase</title><categories>q-bio.PE cs.SI physics.soc-ph</categories><comments>25 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is concerned with stochastic SIR and SEIR epidemic models on
random networks in which individuals may rewire away from infected individuals
at some rate w, so-called preventive rewiring. The models are denoted SIR-w and
SEIR-w, and we focus attention on the early stages of an outbreak, where we
derive expression for the basic reproduction number R0 and the expected degree
of the infectious nodes E(D_I) using two different approximation approaches.
The first approach approximates the early spread of an epidemic by a branching
process, whereas the second one uses pair approximation. The expressions are
compared with the corresponding empirical means obtained from stochastic
simulations of SIR-w and SEIR-w epidemics on Poisson and scale-free networks.
For SIR-w, and the SEIR-w case without rewiring of exposed nodes, both
approaches predict the same epidemic threshold and the same E(D_I), the latter
being very close to the observed mean degree in simulated epidemics over
Poisson networks. Above the epidemic threshold, pairwise models overestimate
the value of R0 obtained from the simulations, which turns out to be very close
to the one predicted by the branching process approximation. For SEIR-w where
exposed individuals also rewire (perhaps unaware of being infected), the two
approaches give different epidemic thresholds, with the branching process
approximation being more in agreement with simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00351</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00351</id><created>2015-11-27</created><authors><author><keyname>Kilcullen</keyname><forenames>Joseph</forenames></author></authors><title>Cryptography Based Solutions to Counterfeiting of Manufactured Goods</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Counterfeiting of manufactured goods is presented as the theft of
intellectual property, patents, copyright etc. accompanied by identity theft.
The purpose of the identity theft is to facilitate the intellectual property
theft. Without it the intellectual property theft would be obvious and the
products would be confiscated and destroyed. Authentication solutions, to
prevent identity theft, were then developed for the two categories of
manufactured goods i.e. goods which can be subjected to destructive screening
strategies and goods which cannot e.g. pharmaceutical drugs and currencies,
respectively. The solutions developed were found to be analogous to digital
signatures. Tamper proof packaging on pharmaceutical drugs is analogous to
encryption because it prevents Mallory from interfering with the product.
Breaking the tamper proof packaging is a one-way function. Concealed inside the
packaging a one-time password, which can be used to authenticate the product
over the internet. The name of the authentication website must be common
knowledge, just like a public key for authenticating digital signatures.
Otherwise the counterfeiters will specify their own authentication website.
This solution can be altered for currencies i.e. the one-way function,
equivalent to opening the tamper proof packaging, becomes the method of
manufacture of the currency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00355</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00355</id><created>2015-12-01</created><authors><author><keyname>Saha</keyname><forenames>Amrita</forenames></author><author><keyname>Indurthi</keyname><forenames>Sathish</forenames></author><author><keyname>Godbole</keyname><forenames>Shantanu</forenames></author><author><keyname>Rongali</keyname><forenames>Subendhu</forenames></author><author><keyname>Raykar</keyname><forenames>Vikas C.</forenames></author></authors><title>Taxonomy grounded aggregation of classifiers with different label sets</title><categories>cs.AI cs.LG</categories><comments>Under review by AISTATS 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe the problem of aggregating the label predictions of diverse
classifiers using a class taxonomy. Such a taxonomy may not have been available
or referenced when the individual classifiers were designed and trained, yet
mapping the output labels into the taxonomy is desirable to integrate the
effort spent in training the constituent classifiers. A hierarchical taxonomy
representing some domain knowledge may be different from, but partially
mappable to, the label sets of the individual classifiers. We present a
heuristic approach and a principled graphical model to aggregate the label
predictions by grounding them into the available taxonomy. Our model aggregates
the labels using the taxonomy structure as constraints to find the most likely
hierarchically consistent class. We experimentally validate our proposed method
on image and text classification tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00358</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00358</id><created>2015-12-01</created><authors><author><keyname>Aronov</keyname><forenames>Boris</forenames></author><author><keyname>Sharir</keyname><forenames>Micha</forenames></author></authors><title>Almost Tight Bounds for Eliminating Depth Cycles in Three Dimensions</title><categories>cs.CG</categories><comments>15 pages including appendix, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given $n$ non-vertical lines in 3-space, their vertical depth (above/below)
relation can contain cycles. We show that the lines can be cut into
$O(n^{3/2}\mathop{\mathrm{polylog}} n)$ pieces, such that the depth relation
among these pieces is now a proper partial order. This bound is nearly tight in
the worst case. As a consequence, we deduce that the number of \emph{pairwise
non-overlapping cycles}, namely, cycles whose $xy$-projections do not overlap,
is $O(n^{3/2}\mathop{\mathrm{polylog}} n)$; this bound too is almost tight in
the worst case.
  Previous results on this topic could only handle restricted cases of the
problem (such as handling only triangular cycles, by Aronov, Koltun, and
Sharir, or only cycles in grid-like patterns, by Chazelle et al.), and the
bounds were considerably weaker---much closer to quadratic.
  Our proof uses a recent variant of the polynomial partitioning technique, due
to Guth, and some simple tools from algebraic geometry. It is much more
straightforward than the previous &quot;purely combinatorial&quot; methods.
  Our technique extends to eliminating all cycles in the depth relation among
segments, and of constant-degree algebraic arcs. We hope that a suitable
extension of this technique could be used to handle the (much more difficult)
case of pairwise-disjoint triangles. Our results almost completely settle a
long-standing (35 years old) open problem in computational geometry, motivated
by hidden-surface removal in computer graphics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00360</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00360</id><created>2015-11-27</created><authors><author><keyname>Kott</keyname><forenames>Alexander</forenames></author><author><keyname>Alberts</keyname><forenames>David S.</forenames></author><author><keyname>Wang</keyname><forenames>Cliff</forenames></author></authors><title>War of 2050: a Battle for Information, Communications, and Computer
  Security</title><categories>cs.CR</categories><comments>A shorter version of this paper has been accepted for publication in
  IEEE Computer, December 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As envisioned in a recent future-casting workshop, warfare will continue to
be transformed by advances in information technologies. In fact, information
itself will become the decisive domain of warfare. Four developments will
significantly change the nature of the battle. The first of these will be a
proliferation of intelligent systems; the second, augmented humans; the third,
the decisive battle for the information domain; and the fourth, the
introduction of new, networked approaches to command and control. Each of these
new capabilities possesses the same critical vulnerability - attacks on the
information, communications and computers that will enable human-robot teams to
make sense of the battlefield and act decisively. Hence, the largely unseen
battle for information, communications and computer security will determine the
extent to which adversaries will be able to function and succeed on the
battlefield of 2050.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00363</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00363</id><created>2015-12-01</created><authors><author><keyname>Cho</keyname><forenames>Jung Rae</forenames></author><author><keyname>Park</keyname><forenames>Jeongmi</forenames></author><author><keyname>Sano</keyname><forenames>Yoshio</forenames></author></authors><title>T-partition systems and travel groupoids on a graph</title><categories>math.CO cs.DM</categories><comments>10 pages</comments><msc-class>20N02, 05C12, 05C05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The notion of travel groupoids was introduced by L. Nebesk\'y in 2006 in
connection with a study on geodetic graphs. A travel groupoid is a pair of a
set $V$ and a binary operation $*$ on $V$ satisfying two axioms. For a travel
groupoid, we can associate a graph. We say that a graph $G$ has a travel
groupoid if the graph associated with the travel groupoid is equal to $G$.
Nebesk\'y gave a characterization for finite graphs to have a travel groupoid.
In this paper, we introduce the notion of T-partition systems on a graph and
give a characterization of travel groupoids on a graph in terms of T-partition
systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00375</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00375</id><created>2015-12-01</created><authors><author><keyname>Knyazev</keyname><forenames>Andrew</forenames></author><author><keyname>Malyshev</keyname><forenames>Alexander</forenames></author></authors><title>Sparse preconditioning for model predictive control</title><categories>math.OC cs.SY</categories><comments>6 pages, 5 figures, submitted to the 2016 American Control
  Conference, July 6-8, Boston, MA, USA. arXiv admin note: text overlap with
  arXiv:1509.02861</comments><msc-class>49M15, 93B40</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose fast O(N) preconditioning, where N is the number of gridpoints on
the prediction horizon, for iterative solution of (non)-linear systems
appearing in model predictive control methods such as forward-difference
Newton-Krylov methods. The Continuation/GMRES method for nonlinear model
predictive control, suggested by T. Ohtsuka in 2004, is a specific application
of the Newton-Krylov method, which uses the GMRES iterative algorithm to solve
a forward difference approximation of the optimality equations on every time
step.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00378</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00378</id><created>2015-12-01</created><authors><author><keyname>Hon</keyname><forenames>Wing-Kai</forenames></author><author><keyname>Thankachan</keyname><forenames>Sharma V.</forenames></author><author><keyname>Xu</keyname><forenames>Bojian</forenames></author></authors><title>An In-place Framework for Exact and Approximate Shortest Unique
  Substring Queries</title><categories>cs.DS</categories><comments>15 pages. A preliminary version of this paper appears in Proceedings
  of the 26th International Symposium on Algorithms and Computation (ISAAC),
  Nagoya, Japan, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We revisit the exact shortest unique substring (SUS) finding problem, and
propose its approximate version where mismatches are allowed, due to its
applications in subfields such as computational biology. We design a generic
in-place framework that fits to solve both the exact and approximate
$k$-mismatch SUS finding, using the minimum $2n$ memory words plus $n$ bytes
space, where $n$ is the input string size. By using the in-place framework, we
can find the exact and approximate $k$-mismatch SUS for every string position
using a total of $O(n)$ and $O(n^2)$ time, respectively, regardless of the
value of $k$. Our framework does not involve any compressed or succinct data
structures and thus is practical and easy to implement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00389</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00389</id><created>2015-12-01</created><authors><author><keyname>Knyazev</keyname><forenames>Andrew</forenames></author><author><keyname>Malyshev</keyname><forenames>Alexander</forenames></author></authors><title>Accelerated non-linear denoising filters</title><categories>cs.CV math.NA</categories><comments>5 pages, 8 figures, submitted to the 41st IEEE International
  Conference on Acoustics, Speech and Signal Processing (ICASSP 2016) in
  Shanghai, China, March 20-25, 2016</comments><msc-class>65F30</msc-class><acm-class>I.4.3; G.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Self-guided denoising filters, such as bilateral, guided, and total variation
filters, may require multiple evaluations if noise is large and filter
parameters are tuned to preserve sharp edges. We formulate three acceleration
techniques of the resulted iterations: conjugate gradient, Nesterov, and heavy
ball methods. We numerically compare these techniques for image denoising and
demonstrate 5-13 times speed-up.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00397</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00397</id><created>2015-12-01</created><authors><author><keyname>Asgari</keyname><forenames>Ehsaneddin</forenames></author><author><keyname>Garakani</keyname><forenames>Kiavash</forenames></author><author><keyname>Mofrad</keyname><forenames>Mohammad R. K</forenames></author></authors><title>A New Approach for Scalable Analysis of Microbial Communities</title><categories>q-bio.GN cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Microbial communities play important roles in the function and maintenance of
various biosystems, ranging from human body to the environment. Current methods
for analysis of microbial communities are typically based on taxonomic
phylogenetic alignment using 16S rRNA metagenomic or Whole Genome Sequencing
data. In typical characterizations of microbial communities, studies deal with
billions of micobial sequences, aligning them to a phylogenetic tree. We
introduce a new approach for the efficient analysis of microbial communities.
Our new reference-free analysis tech- nique is based on n-gram sequence
analysis of 16S rRNA data and reduces the processing data size dramatically (by
105 fold), without requiring taxonomic alignment. The proposed approach is
applied to characterize phenotypic microbial community differ- ences in
different settings. Specifically, we applied this approach in classification of
microbial com- munities across different body sites, characterization of oral
microbiomes associated with healthy and diseased individuals, and
classification of microbial communities longitudinally during the develop- ment
of infants. Different dimensionality reduction methods are introduced that
offer a more scalable analysis framework, while minimizing the loss in
classification accuracies. Among dimensionality re- duction techniques, we
propose a continuous vector representation for microbial communities, which can
widely be used for deep learning applications in microbial informatics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00399</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00399</id><created>2015-12-01</created><authors><author><keyname>Ren</keyname><forenames>Mengqi</forenames></author><author><keyname>Niu</keyname><forenames>Ruixin</forenames></author></authors><title>Joint Group Testing of Time-varying Faulty Sensors and System State
  Estimation in Large Sensor Networks</title><categories>cs.SY</categories><comments>5 pages, 3 figures, and 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of faulty sensor detection is investigated in large sensor
networks where the sensor faults are sparse and time-varying, such as those
caused by attacks launched by an adversary. Group testing and the Kalman filter
are designed jointly to perform real time system state estimation and
time-varying faulty sensor detection with a small number of tests. Numerical
results show that the faulty sensors are efficiently detected and removed, and
the system state estimation performance is significantly improved via the
proposed method. Compared with an approach that tests sensors one by one, the
proposed approach reduces the number of tests significantly while maintaining a
similar fault detection performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00407</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00407</id><created>2015-11-29</created><authors><author><keyname>Kott</keyname><forenames>Alexander</forenames></author></authors><title>Science of Cyber Security as a System of Models and Problems</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Terms like &quot;Science of Cyber&quot; or &quot;Cyber Science&quot; have been appearing in
literature with growing frequency, and influential organizations initiated
research initiatives toward developing such a science even though it is not
clearly defined. We propose to define the domain of the science of cyber
security by noting the most salient artifact within cyber security -- malicious
software -- and defining the domain as comprised of phenomena that involve
malicious software (as well as legitimate software and protocols used
maliciously) used to compel a computing device or a network of computing
devices to perform actions desired by the perpetrator of malicious software
(the attacker) and generally contrary to the intent (the policy) of the
legitimate owner or operator (the defender) of the computing device(s). We
further define the science of cyber security as the study of relations --
preferably expressed as theoretically-grounded models -- between attributes,
structures and dynamics of: violations of cyber security policy; the network of
computing devices under attack; the defenders' tools and techniques; and the
attackers' tools and techniques where malicious software plays the central
role. We offer a simple formalism of these key objects within cyber science and
systematically derive a classification of primary problem classes within cyber
science.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00408</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00408</id><created>2015-11-29</created><authors><author><keyname>Ruelens</keyname><forenames>Frederik</forenames></author><author><keyname>Claessens</keyname><forenames>Bert</forenames></author><author><keyname>Quaiyum</keyname><forenames>Salman</forenames></author><author><keyname>De Schutter</keyname><forenames>Bart</forenames></author><author><keyname>Babuska</keyname><forenames>Robert</forenames></author><author><keyname>Belmans</keyname><forenames>Ronnie</forenames></author></authors><title>Reinforcement Learning Applied to an Electric Water Heater: From Theory
  to Practice</title><categories>cs.LG</categories><comments>Submitted to IEEE transaction on smart grid</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Electric water heaters have the ability to store energy in their water buffer
without impacting the comfort of the end user. This feature makes them a prime
candidate for residential demand response. However, the stochastic and
nonlinear dynamics of electric water heaters, makes it challenging to harness
their flexibility. Driven by this challenge, this paper formulates the
underlying sequential decision-making problem as a Markov decision process and
uses techniques from reinforcement learning. Specifically, we apply an
auto-encoder network to find a compact feature representation of the sensor
measurements, which helps to mitigate the curse of dimensionality. A wellknown
batch reinforcement learning technique, fitted Q-iteration, is used to find a
control policy, given this feature representation. In a simulation-based
experiment using an electric water heater with 50 temperature sensors, the
proposed method was able to achieve good policies much faster than when using
the full state information. In a lab experiment, we apply fitted Q-iteration to
an electric water heater with eight temperature sensors. Further reducing the
state vector did not improve the results of fitted Q-iteration. The results of
the lab experiment, spanning 40 days, indicate that compared to a thermostat
controller, the presented approach was able to reduce the total cost of energy
consumption of the electric water heater by 15%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00411</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00411</id><created>2015-12-01</created><authors><author><keyname>Tunali</keyname><forenames>Nihat Engin</forenames></author><author><keyname>Wu</keyname><forenames>Michael</forenames></author><author><keyname>Dick</keyname><forenames>Chris</forenames></author><author><keyname>Studer</keyname><forenames>Christoph</forenames></author></authors><title>Linear Large-Scale MIMO Data Detection for 5G Multi-Carrier Waveform
  Candidates</title><categories>cs.IT math.IT</categories><comments>Presented at the Asilomar Conference on Signals, Systems, and
  Computers</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fifth generation (5G) wireless systems are expected to combine emerging
transmission technologies, such as large-scale multiple-input multiple-output
(MIMO) and non-orthogonal multi-carrier waveforms, to improve the spectral
efficiency and to reduce out-of-band (OOB) emissions. This paper investigates
the efficacy of two promising multi-carrier waveforms that reduce OOB emissions
in combination with large-scale MIMO, namely filter bank multi-carrier (FBMC)
and generalized frequency division multiplexing (GFDM). We develop novel,
low-complexity data detection algorithms for both of these waveforms. We
investigate the associated performance/complexity trade-offs in the context of
large-scale MIMO, and we study the peak-to-average power ratio (PAPR). Our
results show that reducing the OOB emissions with FBMC and GFDM leads to higher
computational complexity and PAPR compared to that of orthogonal
frequency-division multiplexing (OFDM) and single-carrier frequency division
multiple access (SC-FDMA).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00413</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00413</id><created>2015-12-01</created><authors><author><keyname>Andrews</keyname><forenames>Jeffrey G.</forenames></author><author><keyname>Zhang</keyname><forenames>Xinchen</forenames></author><author><keyname>Durgin</keyname><forenames>Gregory D.</forenames></author><author><keyname>Gupta</keyname><forenames>Abhishek K.</forenames></author></authors><title>Are We Approaching the Fundamental Limits of Wireless Network
  Densification?</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The single most important factor behind the data rate increases experienced
by users of wireless networks over the past few decades has been densification,
namely adding more base stations and access points and thus getting more
spatial reuse of the spectrum. This trend is set to continue into 5G and
presumably beyond. However, at some point further densification will no longer
be able to provide exponentially increasing data rates. Like the end of Moore's
Law, this would have massive implications on the entire technology landscape,
which depends ever more heavily on wireless connectivity. When and why will
this happen? How might we prolong this from occurring for as long as possible?
These are the questions explored in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00421</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00421</id><created>2015-11-30</created><authors><author><keyname>Csat&#xf3;</keyname><forenames>L&#xe1;szl&#xf3;</forenames></author></authors><title>On the additivity of preference aggregation methods</title><categories>cs.GT</categories><comments>24 pages, 9 figures</comments><msc-class>15A06, 91B14</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper reviews some axioms of additivity concerning ranking methods used
for generalized tournaments with possible missing values and multiple
comparisons. It is shown that one of the most natural properties, called
consistency, has strong links to independence of irrelevant comparisons, an
axiom judged unfavourable when players have different opponents. Therefore some
directions of weakening consistency are suggested, and several ranking methods,
the score, generalized row sum and least squares as well as fair bets and its
two variants (one of them entirely new) are analysed whether they satisfy the
properties discussed. It turns out that least squares and generalized row sum
with an appropriate parameter choice preserve the relative ranking of two
objects if the ranking problems added have the same comparison structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00428</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00428</id><created>2015-11-28</created><authors><author><keyname>Pei</keyname><forenames>Jisheng</forenames></author><author><keyname>Wen</keyname><forenames>Lijie</forenames></author><author><keyname>Ye</keyname><forenames>Xiaojun</forenames></author></authors><title>Computation of Transition Adjacency Relations Based on Complete Prefix
  Unfolding (Technical Report)</title><categories>cs.OH</categories><comments>Related to recent submission to CAiSE 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An increasing number of works have devoted to the application of Transition
Adjacency Relation (TAR) as a means to capture behavioral features of business
process models. In this paper, we systematically study the efficient TAR
derivation from process models using unfolding technique which previously has
been used to address the state space explosion when dealing with concurrent
behaviors of a Petri net. We reveal and formally describe the equivalence
between TAR and Event Adjacency Relation (EAR), the manifestation of TAR in the
Complete Prefix Unfolding (CPU) of a Petri net. By computing TARs from CPU
using this equivalence, we can alleviate the concurrency caused state-explosion
issues. Furthermore, structural boosting rules are categorized, proved and
added to the TAR computing algorithm. Formal proofs of correctness and
generality of CPU-based TAR computation are provided for the first time by this
work, and they significantly expand the range of Petri nets from which TARs can
be efficiently derived. Experiments on both industrial and synthesized process
models show the effectiveness of proposed CPU-based algorithms as well as the
observation that they scale well with the increase in size and concurrency of
business process models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00433</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00433</id><created>2015-12-01</created><authors><author><keyname>H&#xe4;ger</keyname><forenames>Christian</forenames></author><author><keyname>Pfister</keyname><forenames>Henry D.</forenames></author><author><keyname>Amat</keyname><forenames>Alexandre Graell i</forenames></author><author><keyname>Br&#xe4;nnstr&#xf6;m</keyname><forenames>Fredrik</forenames></author></authors><title>Density Evolution for Deterministic Generalized Product Codes on the
  Binary Erasure Channel</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Generalized product codes (GPCs) are extensions of product codes (PCs) where
code symbols are protected by two component codes but not necessarily arranged
in a rectangular array. We consider a deterministic construction of GPCs (as
opposed to randomized code ensembles) and analyze the asymptotic performance
over the binary erasure channel under iterative decoding. Our code construction
encompasses several classes of GPCs previously proposed in the literature, such
as irregular PCs, block-wise braided codes, and staircase codes. It is assumed
that the component codes can correct a fixed number of erasures and that the
length of each component code tends to infinity. We show that this setup is
equivalent to studying the behavior of a peeling algorithm applied to a sparse
inhomogeneous random graph. Using a convergence result for these graphs, we
derive the density evolution equations that characterize the asymptotic
decoding performance. As an application, we discuss the design of irregular
GPCs employing a mixture of component codes with different erasure-correcting
capabilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00434</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00434</id><created>2015-12-01</created><authors><author><keyname>Abu-Lebdeh</keyname><forenames>Mohammad</forenames></author><author><keyname>Sahoo</keyname><forenames>Jagruti</forenames></author><author><keyname>Glitho</keyname><forenames>Roch</forenames></author><author><keyname>Tchouati</keyname><forenames>Constant Wette</forenames></author></authors><title>Cloudifying the 3GPP IP Multimedia Subsystem for 4G and Beyond: A Survey</title><categories>cs.NI</categories><comments>8 pages, 3 figures, 1 table, this paper has been accepted in IEEE
  Communications Magazine and will appear in January 2016 issue of the Network
  and Service Management Series</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  4G systems have been continuously evolving to cope with the emerging
challenges of human-centric and machine-to- machine (M2M) applications.
Research has also now started on 5G systems. Scenarios have been proposed and
initial requirements derived. 4G and beyond systems are expected to easily
deliver a wide range of human-centric and M2M applications and services in a
scalable, elastic, and cost efficient manner. The 3GPP IP multimedia subsystem
(IMS) was standardized as the service delivery platform for 3G networks.
Unfortunately, it does not meet several requirements for provisioning
applications and services in 4G and beyond systems. However, cloudifying it
will certainly pave the way for its use as a service delivery platform for 4G
and beyond. This article presents a critical overview of the architectures
proposed so far for cloudifying the IMS. There are two classes of approaches;
the first focuses on the whole IMS system, and the second deals with specific
IMS entities. Research directions are also discussed. IMS granularity and a
PaaS for the development and management of IMS functional entities are the two
key directions we currently foresee.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00442</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00442</id><created>2015-12-01</created><authors><author><keyname>Li</keyname><forenames>Ke</forenames></author><author><keyname>Malik</keyname><forenames>Jitendra</forenames></author></authors><title>Fast k-Nearest Neighbour Search via Dynamic Continuous Indexing</title><categories>cs.DS cs.AI cs.IR cs.LG stat.ML</categories><comments>11 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Existing methods for retrieving k-nearest neighbours suffer from the curse of
dimensionality. We argue this is caused in part by inherent deficiencies of
space partitioning, which is the underlying strategy used by almost all
existing methods. We devise a new strategy that avoids partitioning the vector
space and present a novel randomized algorithm that runs in time linear in
dimensionality and sub-linear in the size of the dataset and takes space
constant in dimensionality and linear in the size of the dataset. The proposed
algorithm allows fine-grained control over accuracy and speed on a per-query
basis, automatically adapts to variations in dataset density, supports dynamic
updates to the dataset and is easy-to-implement. We show appealing theoretical
properties and demonstrate empirically that the proposed algorithm outperforms
locality-sensitivity hashing (LSH) in terms of approximation quality and speed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00443</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00443</id><created>2015-12-01</created><updated>2016-02-29</updated><authors><author><keyname>Charles</keyname><forenames>Valentine</forenames><affiliation>International Rights Statements Working Group-Technical Working Group</affiliation></author><author><keyname>Cowles</keyname><forenames>Esm&#xe9;</forenames><affiliation>International Rights Statements Working Group-Technical Working Group</affiliation></author><author><keyname>Estlund</keyname><forenames>Karen</forenames><affiliation>International Rights Statements Working Group-Technical Working Group</affiliation></author><author><keyname>Isaac</keyname><forenames>Antoine</forenames><affiliation>International Rights Statements Working Group-Technical Working Group</affiliation></author><author><keyname>Johnson</keyname><forenames>Tom</forenames><affiliation>International Rights Statements Working Group-Technical Working Group</affiliation></author><author><keyname>Matienzo</keyname><forenames>Mark A.</forenames><affiliation>International Rights Statements Working Group-Technical Working Group</affiliation></author><author><keyname>Peiffer</keyname><forenames>Patrick</forenames><affiliation>International Rights Statements Working Group-Technical Working Group</affiliation></author><author><keyname>Urban</keyname><forenames>Richard J.</forenames><affiliation>International Rights Statements Working Group-Technical Working Group</affiliation></author><author><keyname>Zeinstra</keyname><forenames>Maarten</forenames><affiliation>International Rights Statements Working Group-Technical Working Group</affiliation></author></authors><title>Recommendations for the Technical Infrastructure for Standardized
  International Rights Statements</title><categories>cs.DL cs.CY</categories><comments>15 pages; released May 2015 at http://bit.ly/1QtmmmT 0 also available
  at
  http://rightsstatements.org/files/150701_recommendations_for_technical_infastructure_for_standardized_international_rights_statements.pdf</comments><acm-class>H.3.7; H.3.1</acm-class><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  This white paper is the product of a joint Digital Public Library of America
(DPLA)-Europeana working group organized to develop minimum rights statement
metadata standards for organizations that contribute to DPLA and Europeana.
This white paper deals specifically with the technical infrastructure of a
common namespace (rightsstatements.org) that hosts the rights statements to be
used by (at minimum) the DPLA and Europeana. These recommendations for a common
technical infrastructure for rights statements outline a simple, flexible, and
extensible framework to host the rights statements at rightsstatements.org.
This white paper specifically outlines the management of rights statements as
linked open data. The rights statements are published according to Best
Practices for Publishing RDF Vocabularies. They are encoded into
dereferenceable URIs, express further information encoded in RDF, and link to
existing vocabularies and standards. The rights statements adhere to
expressions of existing rights vocabularies. Furthermore the paper reviews the
publication and implementation to make the rights statements available through
human-readable web pages augmented with machine-readable formats.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00444</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00444</id><created>2015-12-01</created><updated>2015-12-02</updated><authors><author><keyname>Bach</keyname><forenames>Eric</forenames></author><author><keyname>Fernando</keyname><forenames>Rex</forenames></author></authors><title>Infinitely Many Carmichael Numbers for a Modified Miller-Rabin Prime
  Test</title><categories>math.NT cs.DS</categories><comments>17 pages (21 with appendix)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define a variant of the Miller-Rabin primality test, which is in between
Miller-Rabin and Fermat in terms of strength. We show that this test has
infinitely many &quot;Carmichael&quot; numbers. We show that the test can also be thought
of as a variant of the Solovay-Strassen test. We explore the growth of the
test's &quot;Carmichael&quot; numbers, giving some empirical results and a discussion of
one particularly strong pattern which appears in the results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00448</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00448</id><created>2015-11-30</created><authors><author><keyname>Costa</keyname><forenames>Mia</forenames></author><author><keyname>Desmarais</keyname><forenames>Bruce A.</forenames></author><author><keyname>Hird</keyname><forenames>John A.</forenames></author></authors><title>Science Use in Regulatory Impact Analysis: The Effects of Political
  Attention and Controversy</title><categories>cs.DL cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scholars, policymakers, and research sponsors have long sought to understand
the conditions under which scientific research is used in the policymaking
process. Recent research has identified a resource that can be used to trace
the use of science across time and many policy domains. US federal agencies are
mandated by executive order to justify all economically significant regulations
by regulatory impact analyses (RIAs), in which they present evidence of the
scientific underpinnings and consequences of the proposed rule. To gain new
insight into when and how regulators invoke science in their policy
justifications, we ask: does the political attention and controversy
surrounding a regulation affect the extent to which science is utilized in
RIAs? We examine scientific citation activity in all 101 economically
significant RIAs from 2008-2012 and evaluate the effects of attention -- from
the public, policy elites and the media -- on the degree of science use in
RIAs. Our main finding is that regulators draw more heavily on scientific
research when justifying rules subject to a high degree of attention from
outside actors. These findings suggest that scientific research plays an
important role in the justification of regulations, especially those that are
highly salient to the public and other policy actors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00481</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00481</id><created>2015-12-01</created><authors><author><keyname>Bringmann</keyname><forenames>Karl</forenames></author><author><keyname>Kozma</keyname><forenames>L&#xe1;szl&#xf3;</forenames></author><author><keyname>Moran</keyname><forenames>Shay</forenames></author><author><keyname>Narayanaswamy</keyname><forenames>N. S.</forenames></author></authors><title>Hitting Set in hypergraphs of low VC-dimension</title><categories>cs.DS math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the complexity of the Hitting Set problem in set systems
(hypergraphs) that avoid certain sub-structures. In particular, we characterize
the classical and parameterized complexity of the problem when the
Vapnik-Chervonenkis dimension (VC-dimension) of the input is small.
VC-dimension is a natural measure of complexity of set systems. Several
tractable instances of Hitting Set with a geometric or graph-theoretical flavor
are known to have low VC-dimension. In set systems of bounded VC-dimension,
Hitting Set is known to admit efficient and almost optimal approximation
algorithms (Br\&quot;onnimann and Goodrich, 1995; Even, Rawitz, and Shahar, 2005;
Agarwal and Pan, 2014).
  In contrast to these approximation-results, a low VC-dimension does not
necessarily imply tractability in the parameterized sense. In fact, we show
that Hitting Set is W[1]-hard already on inputs with VC-dimension 2, even if
the VC-dimension of the dual set system is also 2. Thus, Hitting Set is very
unlikely to be fixed-parameter tractable even in this arguably simple case.
This answers an open question raised by King in 2010. For set systems whose
(primal or dual) VC-dimension is 1, we show that Hitting Set is solvable in
polynomial time.
  To bridge the gap in complexity between the classes of inputs with
VC-dimension 1 and 2, we use a measure that is more fine-grained than
VC-dimension. In terms of this measure, we identify a sharp threshold where the
complexity of Hitting Set transitions from polynomial-time-solvable to NP-hard.
The tractable class that lies just under the threshold is a generalization of
Edge Cover, and thus extends the domain of polynomial-time tractability of
Hitting Set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00482</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00482</id><created>2015-12-01</created><authors><author><keyname>Fernau</keyname><forenames>Henning</forenames></author><author><keyname>Paramasivan</keyname><forenames>Meenakshi</forenames></author><author><keyname>Schmid</keyname><forenames>Markus L.</forenames></author><author><keyname>Vorel</keyname><forenames>Vojt&#x11b;ch</forenames></author></authors><title>Characterization and Complexity Results on Jumping Finite Automata</title><categories>cs.FL cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a jumping finite automaton, the input head can jump to an arbitrary
position within the remaining input after reading and consuming a symbol.
  We characterize the corresponding class of languages in terms of special
shuffle expressions and survey other equivalent notions from the existing
literature.
  Moreover, we present several results concerning computational hardness and
algorithms for parsing and other basic tasks concerning jumping finite
automata.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00486</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00486</id><created>2015-12-01</created><authors><author><keyname>Lapin</keyname><forenames>Maksim</forenames></author><author><keyname>Hein</keyname><forenames>Matthias</forenames></author><author><keyname>Schiele</keyname><forenames>Bernt</forenames></author></authors><title>Loss Functions for Top-k Error: Analysis and Insights</title><categories>stat.ML cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to push the performance on realistic computer vision tasks, the
number of classes in modern benchmark datasets has significantly increased in
recent years. This increase in the number of classes comes along with increased
ambiguity between the class labels, raising the question if top-1 error is the
right performance measure. In this paper, we provide an extensive comparison
and evaluation of established multiclass methods comparing their top-k
performance both from a practical as well as from a theoretical perspective.
Moreover, we introduce novel top-k loss functions as modifications of the
softmax and the multiclass SVM loss and provide efficient optimization schemes
for them. In the experiments, we compare on various datasets all of the
proposed and established methods for top-k error optimization. An interesting
insight of this paper is that the softmax loss yields competitive top-k
performance for all k simultaneously. For a specific top-k error, our new top-k
losses lead typically to further improvements while being faster to train than
the softmax.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00490</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00490</id><created>2015-12-01</created><authors><author><keyname>Bj&#xf6;rnson</keyname><forenames>Emil</forenames></author><author><keyname>de Carvalho</keyname><forenames>Elisabeth</forenames></author><author><keyname>Larsson</keyname><forenames>Erik G.</forenames></author><author><keyname>Popovski</keyname><forenames>Petar</forenames></author></authors><title>Random Access Protocol for Massive MIMO: Strongest-User Collision
  Resolution (SUCR)</title><categories>cs.IT math.IT</categories><comments>6 pages, 6 figures, submitted to ICC 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless networks with many antennas at the base stations and multiplexing of
many users, known as Massive MIMO systems, are key to handle the rapid growth
of data traffic. As the number of users increases, the random access in
contemporary networks will be flooded by user collisions. In this paper, we
propose a reengineered random access protocol, coined strongest-user collision
resolution (SUCR). It exploits the channel hardening feature of massive MIMO
channels to enable each user to detect collisions, determine how strong the
contenders' channels are, and only keep transmitting if it has the strongest
channel gain. The proposed SUCR protocol can quickly and distributively resolve
the vast majority of all pilot collisions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00500</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00500</id><created>2015-12-01</created><authors><author><keyname>Hosseini</keyname><forenames>Mohammad</forenames></author><author><keyname>Nagibolhosseini</keyname><forenames>Nooreddin</forenames></author><author><keyname>Barnoy</keyname><forenames>Amotz</forenames></author><author><keyname>Terlecky</keyname><forenames>Peter</forenames></author><author><keyname>Liu</keyname><forenames>Hengchang</forenames></author><author><keyname>Hu</keyname><forenames>Shaohan</forenames></author><author><keyname>Wang</keyname><forenames>Shiguang</forenames></author><author><keyname>Amin</keyname><forenames>Tanvir</forenames></author><author><keyname>Su</keyname><forenames>Lu</forenames></author><author><keyname>Wang</keyname><forenames>Dong</forenames></author><author><keyname>Govindan</keyname><forenames>Ramesh</forenames></author><author><keyname>Ganti</keyname><forenames>Raghu</forenames></author><author><keyname>Srivatsa</keyname><forenames>Mudhakar</forenames></author><author><keyname>Aggrawal</keyname><forenames>Charu</forenames></author><author><keyname>Abdelzaher</keyname><forenames>Tarek</forenames></author><author><keyname>Gu</keyname><forenames>Siyu</forenames></author><author><keyname>Pan</keyname><forenames>Chenji</forenames></author></authors><title>Joint Source Selection and Data Extrapolation in Social Sensing for
  Disaster Response</title><categories>cs.SI</categories><comments>24 pages, Technical Report, University of Illinois at
  Urbana-Champaign</comments><acm-class>C.2.2; H.3.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper complements the large body of social sensing literature by
developing means for augmenting sensing data with inference results that
&quot;fill-in&quot; missing pieces. It specifically explores the synergy between (i)
inference techniques used for filling-in missing pieces and (ii) source
selection techniques used to determine which pieces to retrieve in order to
improve inference results. We focus on prediction in disaster scenarios, where
disruptive trend changes occur. We first discuss our previous conference study
that compared a set of prediction heuristics and developed a hybrid prediction
algorithm. We then enhance the prediction scheme by considering algorithms for
sensor selection that improve inference quality. Our proposed source selection
and extrapolation algorithms are tested using data collected during the New
York City crisis in the aftermath of Hurricane Sandy in November 2012. The
evaluation results show that consistently good predictions are achieved. The
work is notable for addressing the bi-modal nature of damage propagation in
complex systems subjected to stress, where periods of calm are interspersed
with periods of severe change. It is novel in offering a new solution to the
problem that jointly leverages source selection and extrapolation components
thereby improving the results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00501</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00501</id><created>2015-12-01</created><authors><author><keyname>Bui</keyname><forenames>Dai Nguyen</forenames></author></authors><title>CacheDiff: Fast Random Sampling</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a sampling method called, CacheDiff, that has both time and space
complexity of O(k) to randomly select k items from a pool of N items, in which
N is known.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00504</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00504</id><created>2015-12-01</created><authors><author><keyname>Schiel</keyname><forenames>Jamie</forenames></author><author><keyname>Bainbridge-Smith</keyname><forenames>Andrew</forenames></author></authors><title>Efficient Edge Detection on Low-Cost FPGAs</title><categories>cs.AR cs.CV</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Improving the efficiency of edge detection in embedded applications, such as
UAV control, is critical for reducing system cost and power dissipation. Field
programmable gate arrays (FPGA) are a good platform for making improvements
because of their specialised internal structure. However, current FPGA edge
detectors do not exploit this structure well. A new edge detection architecture
is proposed that is better optimised for FPGAs. The basis of the architecture
is the Sobel edge kernels that are shown to be the most suitable because of
their separability and absence of multiplications. Edge intensities are
calculated with a new 4:2 compressor that consists of two custom-designed 3:2
compressors. Addition speed is increased by breaking carry propagation chains
with look-ahead logic. Testing of the design showed it gives a 28% increase in
speed and 4.4% reduction in area over previous equivalent designs, which
demonstrated that it will lower the cost of edge detection systems, dissipate
less power and still maintain high-speed control.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00506</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00506</id><created>2015-12-01</created><authors><author><keyname>Venugopal</keyname><forenames>Kiran</forenames></author><author><keyname>Valenti</keyname><forenames>Matthew C.</forenames></author><author><keyname>Heath</keyname><forenames>Robert W.</forenames><suffix>Jr</suffix></author></authors><title>Analysis of Millimeter Wave Networked Wearables in Crowded Environments</title><categories>cs.IT math.IT</categories><comments>5 pages, 7 figures. Presented at Asilomar Conference on Signals,
  Systems, and Computers, Nov. 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The millimeter wave (mmWave) band has the potential to provide high
throughput among wearable devices. When mmWave wearable networks are used in
crowded environments, such as on a bus or train, antenna directivity and
orientation hold the key to achieving Gbps rates. Previous work using
stochastic geometry often assumes an infinite number of interfering nodes drawn
from a Poisson Point Process (PPP). Since indoor wearable networks will be
isolated due to walls, a network with a finite number of nodes may be a more
suitable model. In this paper, we characterize the significant sources of
interference and develop closed-form expressions for the spatially averaged
performance of a typical user's wearable communication link. The effect of
human body blockage on the mmWave signals and the role of network density are
investigated to show that an increase in interferer density reduces the mean
number of significant interferers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00517</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00517</id><created>2015-12-01</created><authors><author><keyname>Leordeanu</keyname><forenames>Marius</forenames></author><author><keyname>Radu</keyname><forenames>Alexandra</forenames></author><author><keyname>Baluja</keyname><forenames>Shumeet</forenames></author><author><keyname>Sukthankar</keyname><forenames>Rahul</forenames></author></authors><title>Labeling the Features Not the Samples: Efficient Video Classification
  with Minimal Supervision</title><categories>cs.CV</categories><comments>arXiv admin note: text overlap with arXiv:1411.7714</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Feature selection is essential for effective visual recognition. We propose
an efficient joint classifier learning and feature selection method that
discovers sparse, compact representations of input features from a vast sea of
candidates, with an almost unsupervised formulation. Our method requires only
the following knowledge, which we call the \emph{feature sign}---whether or not
a particular feature has on average stronger values over positive samples than
over negatives. We show how this can be estimated using as few as a single
labeled training sample per class. Then, using these feature signs, we extend
an initial supervised learning problem into an (almost) unsupervised clustering
formulation that can incorporate new data without requiring ground truth
labels. Our method works both as a feature selection mechanism and as a fully
competitive classifier. It has important properties, low computational cost and
excellent accuracy, especially in difficult cases of very limited training
data. We experiment on large-scale recognition in video and show superior speed
and performance to established feature selection approaches such as AdaBoost,
Lasso, greedy forward-backward selection, and powerful classifiers such as SVM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00519</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00519</id><created>2015-11-30</created><authors><author><keyname>Knowles</keyname><forenames>Bryan A.</forenames></author><author><keyname>Atici</keyname><forenames>Mustafa</forenames></author></authors><title>Proposed Approximate Dynamic Programming for Pathfinding under Visible
  Uncertainty</title><categories>cs.DS</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Continuing our preleminary work \cite{knowles14}, we define the
safest-with-sight pathfinding problems and explore its solution using
techniques borrowed from measure-theoretic probability theory. We find a simple
recursive definition for the probability that an ideal pathfinder will select
an edge in a given scenario of an uncertain network where edges have
probabilities of failure and vertices provide &quot;vision&quot; of edges via
lines-of-sight. We propose an approximate solution based on our theoretical
findings that would borrow techniques from approximate dynamic programming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00521</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00521</id><created>2015-12-01</created><authors><author><keyname>Aguiar</keyname><forenames>Pedro M.</forenames></author><author><keyname>Hornby</keyname><forenames>Robert</forenames></author><author><keyname>McGarry</keyname><forenames>Cameron</forenames></author><author><keyname>O'Keefe</keyname><forenames>Simon</forenames></author><author><keyname>Sebald</keyname><forenames>Angelika</forenames></author></authors><title>Discrete and Continuous Systems of Logic in Nuclear Magnetic Resonance</title><categories>cs.ET</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We implement several non-binary logic systems using the spin dynamics of
nuclear spins in nuclear magnetic resonance (NMR). The NMR system is a suitable
test system because of its high degree of experimental control; findings from
NMR implementations are relevant for other computational platforms exploiting
particles with spin, such as electrons or photons. While we do not expect the
NMR system to become a practical computational device, it is uniquely useful to
explore strengths and weaknesses of unconventional computational approaches,
such as non-binary logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00524</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00524</id><created>2015-12-01</created><authors><author><keyname>Juarez</keyname><forenames>Marc</forenames></author><author><keyname>Imani</keyname><forenames>Mohsen</forenames></author><author><keyname>Perry</keyname><forenames>Mike</forenames></author><author><keyname>Diaz</keyname><forenames>Claudia</forenames></author><author><keyname>Wright</keyname><forenames>Matthew</forenames></author></authors><title>WTF-PAD: Toward an Efficient Website Fingerprinting Defense for Tor</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Website Fingerprinting attacks are conducted by a low-resource local
adversary who observes the (encrypted) traffic between an anonymity system and
one or more of its clients. With this attack, the adversary can recover the
user's otherwise anonymized web browsing activity by matching the observed
traffic with prerecorded web page traffic templates. The defenses that have
been proposed to counter these attacks are effective, but they are impractical
for deployment in real-world systems due to their high cost in terms of both
added delay to access webpages and bandwidth overhead. Further, these defenses
have been designed to counter attacks that, despite their high success rates,
have been criticized for assuming unrealistic attack conditions in the
evaluation setting.
  In this paper, we show that there are lightweight defenses that provide a
sufficient level of security against website fingerprinting, particularly in
realistic evaluation settings. In particular, we propose a novel defense based
on Adaptive Padding, a link-padding strategy for low-latency anonymity systems
that we have adapted for Tor. With this countermeasure in a closed-world
setting, we observe a reduction in the accuracy of the state-of-the-art attack
from 91% to 20%, while introducing zero latency overhead and less than 60%
bandwidth overhead. In an open-world setting with 50 pages that the attacker is
monitoring and 2,000 other websites that users visit, the attack precision is
just 1% and drops further as the number of sites grows. For the implementation
and evaluation of the defense, we have developed a tool for evaluating the
traffic analysis resistance properties of Tor's Pluggable Transports that we
hope will contribute to future research on Tor and traffic analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00531</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00531</id><created>2015-12-01</created><authors><author><keyname>Reagan</keyname><forenames>Andrew</forenames></author><author><keyname>Tivnan</keyname><forenames>Brian</forenames></author><author><keyname>Williams</keyname><forenames>Jake Ryland</forenames></author><author><keyname>Danforth</keyname><forenames>Christopher M.</forenames></author><author><keyname>Dodds</keyname><forenames>Peter Sheridan</forenames></author></authors><title>Benchmarking sentiment analysis methods for large-scale texts: A case
  for using continuum-scored words and word shift graphs</title><categories>cs.CL</categories><comments>34 pages, 30 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  The emergence and global adoption of social media has rendered possible the
real-time estimation of population-scale sentiment, bearing profound
implications for our understanding of human behavior. Given the growing
assortment of sentiment measuring instruments, comparisons between them are
evidently required. Here, we perform detailed tests of 6 dictionary-based
methods applied to 4 different corpora, and briefly examine a further 8
methods. We show that a dictionary-based method will only perform both reliably
and meaningfully if (1) the dictionary covers a sufficiently large enough
portion of a given text's lexicon when weighted by word usage frequency; and
(2) words are scored on a continuous scale.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00537</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00537</id><created>2015-12-01</created><authors><author><keyname>Gruenheid</keyname><forenames>Anja</forenames></author><author><keyname>Nushi</keyname><forenames>Besmira</forenames></author><author><keyname>Kraska</keyname><forenames>Tim</forenames></author><author><keyname>Gatterbauer</keyname><forenames>Wolfgang</forenames></author><author><keyname>Kossmann</keyname><forenames>Donald</forenames></author></authors><title>Fault-Tolerant Entity Resolution with the Crowd</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, crowdsourcing is increasingly applied as a means to enhance
data quality. Although the crowd generates insightful information especially
for complex problems such as entity resolution (ER), the output quality of
crowd workers is often noisy. That is, workers may unintentionally generate
false or contradicting data even for simple tasks. The challenge that we
address in this paper is how to minimize the cost for task requesters while
maximizing ER result quality under the assumption of unreliable input from the
crowd. For that purpose, we first establish how to deduce a consistent ER
solution from noisy worker answers as part of the data interpretation problem.
We then focus on the next-crowdsource problem which is to find the next task
that maximizes the information gain of the ER result for the minimal additional
cost. We compare our robust data interpretation strategies to alternative
state-of-the-art approaches that do not incorporate the notion of
fault-tolerance, i.e., the robustness to noise. In our experimental evaluation
we show that our approaches yield a quality improvement of at least 20% for two
real-world datasets. Furthermore, we examine task-to-worker assignment
strategies as well as task parallelization techniques in terms of their cost
and quality trade-offs in this paper. Based on both synthetic and crowdsourced
datasets, we then draw conclusions on how to minimize cost while maintaining
high quality ER results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00539</identifier>
 <datestamp>2015-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00539</id><created>2015-12-01</created><authors><author><keyname>Namvar</keyname><forenames>Nima</forenames></author><author><keyname>Saad</keyname><forenames>Walid</forenames></author><author><keyname>Maham</keyname><forenames>Behrouz</forenames></author></authors><title>Cell Selection in Wireless Two-Tier Networks: A Context-Aware Matching
  Game</title><categories>cs.NI cs.IT math.IT</categories><comments>11 pages, 11 figures, Journal article in ICST Wireless Spectrum, 2014</comments><doi>10.4108/ws.1.2.e2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The deployment of small cell networks is seen as a major feature of the next
generation of wireless networks. In this paper, a novel approach for cell
association in small cell networks is proposed. The proposed approach exploits
new types of information extracted from the users' devices and environment to
improve the way in which users are assigned to their serving base stations.
Examples of such context information include the devices' screen size and the
users' trajectory. The problem is formulated as a matching game with
externalities and a new, distributed algorithm is proposed to solve this game.
The proposed algorithm is shown to reach a stable matching whose properties are
studied. Simulation results show that the proposed context-aware matching
approach yields significant performance gains, in terms of the average utility
per user, when compared with a classical max-SINR approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00540</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00540</id><created>2015-12-01</created><authors><author><keyname>Kowalski</keyname><forenames>Dariusz R.</forenames></author><author><keyname>Mosteiro</keyname><forenames>Miguel A.</forenames></author><author><keyname>Zaki</keyname><forenames>Kevin</forenames></author></authors><title>Dynamic Multiple-Message Broadcast: Bounding Throughput in the
  Affectance Model</title><categories>cs.DC</categories><msc-class>68W15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a dynamic version of the Multiple-Message Broadcast problem, where
packets are continuously injected in network nodes for dissemination throughout
the network. Our performance metric is the ratio of the throughput of such
protocol against the optimal one, for any sufficiently long period of time
since startup. We present and analyze a dynamic Multiple-Message Broadcast
protocol that works under an affectance model, which parameterizes the
interference that other nodes introduce in the communication between a given
pair of nodes. As an algorithmic tool, we develop an efficient algorithm to
schedule a broadcast along a BFS tree under the affectance model. To provide a
rigorous and accurate analysis, we define two novel network characteristics
based on the network topology and the affectance function. The combination of
these characteristics influence the performance of broadcasting with affectance
(modulo a logarithmic function). We also carry out simulations of our protocol
under affectance. To the best of our knowledge, this is the first dynamic
Multiple-Message Broadcast protocol that provides throughput guarantees for
continuous injection of messages and works under the affectance model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00547</identifier>
 <datestamp>2015-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00547</id><created>2015-12-01</created><authors><author><keyname>Namvar</keyname><forenames>Nima</forenames></author><author><keyname>Bahadori</keyname><forenames>Niloofar</forenames></author><author><keyname>Afghah</keyname><forenames>Fatemeh</forenames></author></authors><title>Context-Aware D2D Peer Selection for Load Distribution in LTE Networks</title><categories>cs.NI cs.IT math.IT</categories><comments>49th Annual Asilomar Conference on Signals, Systems, and Computers,
  accepted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a novel context-aware approach for resource
allocation in device to device (D2D) communication networks which exploits
context information about the users velocity and size of their demanded data to
decide whether their traffic load can be transferred to D2D tier and which D2D
users should be paired. The problem is modeled as a matching game with
externalities and a novel algorithm is proposed to solve the game which
converges to a stable matching between the D2D users. Simulation results
demonstrate the effectiveness of our model in offloading the cellular networks
traffic to the D2D tier.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00550</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00550</id><created>2015-12-01</created><authors><author><keyname>Liu</keyname><forenames>Shichao</forenames></author><author><keyname>Jiang</keyname><forenames>Ying</forenames></author></authors><title>Value-passing CCS for Trees: A Theory for Concurrent Systems</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we extend the theory CCS for trees (CCTS) to value-passing
CCTS (VCCTS), of which symbols have the capacity for receiving and sending data
values, and a nonsequential semantics is proposed in an operational approach.
In this concurrent model, a weak barbed congruence and a localized early weak
bisimilarity are defined, and the latter relation is proved to be sufficient to
justify the former. As an illustration of potential applications of VCCTS, a
semantics based on VCCTS is given to a toy multi-threaded programming language
featuring a core of C/C++ concurrency; and a formalization based on the
operational semantics of VCCTS is proposed for some relaxed memory models, and
a DRF-guarantee property with respect to VCCTS is proved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00553</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00553</id><created>2015-12-01</created><authors><author><keyname>Hayashi</keyname><forenames>Yukio</forenames></author></authors><title>Asymptotic behavior of the node degrees in the ensemble average of
  adjacency matrix</title><categories>physics.soc-ph cs.SI nlin.AO</categories><comments>13 paghes, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Various important and useful quantities or measures that characterize the
topological network structure are usually investigated for a network, then they
are averaged over the samples. In this paper, we propose an explicit
representation by the beforehand averaged adjacency matrix over samples of
growing networks as a new general framework for investigating the
characteristic quantities. It is applied to some network models, and shows a
good approximation of degree distribution asymptotically. In particular, our
approach will be applicable through the numerical calculations instead of
intractable theoretical analysises, when the time-course of degree is a
monotone increasing function like power-law or logarithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00556</identifier>
 <datestamp>2015-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00556</id><created>2015-12-01</created><authors><author><keyname>Namvar</keyname><forenames>Nima</forenames></author><author><keyname>Afghah</keyname><forenames>Fatemeh</forenames></author></authors><title>Spectrum Sharing in Cooperative Cognitive Radio Networks: A Matching
  Game Framework</title><categories>cs.NI cs.IT math.IT</categories><comments>5 pages, 3 figures in Information Sciences and Systems (CISS), 2015
  49th Annual Conference on , vol., no., pp.1-5, 18-20 March 2015</comments><doi>10.1109/CISS.2015.7086843</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dynamic spectrum access allows the unlicensed wireless users (secondary
users) to dynamically access the licensed bands from legacy spectrum holders
(primary users) either on an opportunistic or a cooperative basis. In this
paper, we focus on cooperative spectrum sharing in a wireless network
consisting of multiple primary and multiple secondary users. In particular, we
study the partner-selection and resource-allocation problems within a matching
theory framework, in which the primary and secondary users aim at optimizing
their utilities in terms of transmission rate and power consumption. We propose
a distributed algorithm to find the solution of the developed matching game
that results in a stable matching between the sets of the primary and secondary
users. Both analytical and numerical results show that the proposed matching
model is a promising approach under which the utility functions of both primary
and secondary users are maximized.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00567</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00567</id><created>2015-12-01</created><updated>2015-12-11</updated><authors><author><keyname>Szegedy</keyname><forenames>Christian</forenames></author><author><keyname>Vanhoucke</keyname><forenames>Vincent</forenames></author><author><keyname>Ioffe</keyname><forenames>Sergey</forenames></author><author><keyname>Shlens</keyname><forenames>Jonathon</forenames></author><author><keyname>Wojna</keyname><forenames>Zbigniew</forenames></author></authors><title>Rethinking the Inception Architecture for Computer Vision</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convolutional networks are at the core of most state-of-the-art computer
vision solutions for a wide variety of tasks. Since 2014 very deep
convolutional networks started to become mainstream, yielding substantial gains
in various benchmarks. Although increased model size and computational cost
tend to translate to immediate quality gains for most tasks (as long as enough
labeled data is provided for training), computational efficiency and low
parameter count are still enabling factors for various use cases such as mobile
vision and big-data scenarios. Here we explore ways to scale up networks in
ways that aim at utilizing the added computation as efficiently as possible by
suitably factorized convolutions and aggressive regularization. We benchmark
our methods on the ILSVRC 2012 classification challenge validation set
demonstrate substantial gains over the state of the art: 21.2% top-1 and 5.6%
top-5 error for single frame evaluation using a network with a computational
cost of 5 billion multiply-adds per inference and with using less than 25
million parameters. With an ensemble of 4 models and multi-crop evaluation, we
report 3.5% top-5 error on the validation set (3.6% error on the test set) and
17.3% top-1 error on the validation set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00570</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00570</id><created>2015-12-01</created><authors><author><keyname>Yan</keyname><forenames>Xinchen</forenames></author><author><keyname>Yang</keyname><forenames>Jimei</forenames></author><author><keyname>Sohn</keyname><forenames>Kihyuk</forenames></author><author><keyname>Lee</keyname><forenames>Honglak</forenames></author></authors><title>Attribute2Image: Conditional Image Generation from Visual Attributes</title><categories>cs.LG cs.AI cs.CV</categories><comments>10 pages (main) and 1 page (supplementary material)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates a problem of generating images from visual
attributes. Given the prevalent research for image recognition, the conditional
image generation problem is relatively under-explored due to the challenges of
learning a good generative model and handling rendering uncertainties in
images. To address this, we propose a variety of attribute-conditioned deep
variational auto-encoders that enjoy both effective representation learning and
Bayesian modeling, from which images can be generated from specified attributes
and sampled latent factors. We experiment with natural face images and
demonstrate that the proposed models are capable of generating realistic faces
with diverse appearance. We further evaluate the proposed models by performing
attribute-conditioned image progression, transfer and retrieval. In particular,
our generation method achieves superior performance in the retrieval experiment
against traditional nearest-neighbor-based methods both qualitatively and
quantitatively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00573</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00573</id><created>2015-12-01</created><authors><author><keyname>Wong</keyname><forenames>Lawson L. S.</forenames></author><author><keyname>Kurutach</keyname><forenames>Thanard</forenames></author><author><keyname>Kaelbling</keyname><forenames>Leslie Pack</forenames></author><author><keyname>Lozano-P&#xe9;rez</keyname><forenames>Tom&#xe1;s</forenames></author></authors><title>Object-based World Modeling in Semi-Static Environments with Dependent
  Dirichlet-Process Mixtures</title><categories>cs.AI cs.LG cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To accomplish tasks in human-centric indoor environments, robots need to
represent and understand the world in terms of objects and their attributes. We
refer to this attribute-based representation as a world model, and consider how
to acquire it via noisy perception and maintain it over time, as objects are
added, changed, and removed in the world. Previous work has framed this as
multiple-target tracking problem, where objects are potentially in motion at
all times. Although this approach is general, it is computationally expensive.
We argue that such generality is not needed in typical world modeling tasks,
where objects only change state occasionally. More efficient approaches are
enabled by restricting ourselves to such semi-static environments.
  We consider a previously-proposed clustering-based world modeling approach
that assumed static environments, and extend it to semi-static domains by
applying a dependent Dirichlet-process (DDP) mixture model. We derive a novel
MAP inference algorithm under this model, subject to data association
constraints. We demonstrate our approach improves computational performance in
semi-static environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00576</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00576</id><created>2015-12-01</created><authors><author><keyname>Suhartono</keyname><forenames>Derwin</forenames></author></authors><title>Probabilistic Latent Semantic Analysis (PLSA) untuk Klasifikasi Dokumen
  Teks Berbahasa Indonesia</title><categories>cs.CL cs.IR</categories><comments>17 pages, 6 figures, 3 tables, Technical Report Program Studi Doktor
  Ilmu Komputer Universitas Indonesia</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One task that is included in managing documents is how to find substantial
information inside. Topic modeling is a technique that has been developed to
produce document representation in form of keywords. The keywords will be used
in the indexing process and document retrieval as needed by users. In this
research, we will discuss specifically about Probabilistic Latent Semantic
Analysis (PLSA). It will cover PLSA mechanism which involves Expectation
Maximization (EM) as the training algorithm, how to conduct testing, and obtain
the accuracy result.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00578</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00578</id><created>2015-12-01</created><authors><author><keyname>Suhartono</keyname><forenames>Derwin</forenames></author></authors><title>Klasifikasi Komponen Argumen Secara Otomatis pada Dokumen Teks berbentuk
  Esai Argumentatif</title><categories>cs.CL cs.IR</categories><comments>16 pages, 3 figures, 2 tables, Technical Report Program Studi Doktor
  Ilmu Komputer Universitas Indonesia</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  By automatically recognize argument component, essay writers can do some
inspections to texts that they have written. It will assist essay scoring
process objectively and precisely because essay grader is able to see how well
the argument components are constructed. Some reseachers have tried to do
argument detection and classification along with its implementation in some
domains. The common approach is by doing feature extraction to the text.
Generally, the features are structural, lexical, syntactic, indicator, and
contextual. In this research, we add new feature to the existing features. It
adopts keywords list by Knott and Dale (1993). The experiment result shows the
argument classification achieves 72.45% accuracy. Moreover, we still get the
same accuracy without the keyword lists. This concludes that the keyword lists
do not affect significantly to the features. All features are still weak to
classify major claim and claim, so we need other features which are useful to
differentiate those two kind of argument components.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00583</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00583</id><created>2015-12-02</created><authors><author><keyname>Yu</keyname><forenames>Pengqian</forenames></author><author><keyname>Yu</keyname><forenames>Jia Yuan</forenames></author><author><keyname>Xu</keyname><forenames>Huan</forenames></author></authors><title>Central-limit approach to risk-aware Markov decision processes</title><categories>math.OC cs.SY</categories><comments>arXiv admin note: text overlap with arXiv:1403.6530 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Whereas classical Markov decision processes maximize the expected reward, we
consider minimizing the risk. We propose to evaluate the risk associated to a
given policy over a long-enough time horizon with the help of a central limit
theorem. The proposed approach works whether the transition probabilities are
known or not. We also provide a gradient-based policy improvement algorithm
that converges to a local optimum of the risk objective.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00596</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00596</id><created>2015-12-02</created><authors><author><keyname>Kemelmacher-Shlizerman</keyname><forenames>Ira</forenames></author><author><keyname>Seitz</keyname><forenames>Steve</forenames></author><author><keyname>Miller</keyname><forenames>Daniel</forenames></author><author><keyname>Brossard</keyname><forenames>Evan</forenames></author></authors><title>The MegaFace Benchmark: 1 Million Faces for Recognition at Scale</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent face recognition experiments on a major benchmark LFW show stunning
performance--a number of algorithms achieve near to perfect score, surpassing
human recognition rates. In this paper, we advocate evaluations at the million
scale (LFW includes only 13K photos of 5K people). To this end, we have
assembled the MegaFace dataset and created the first MegaFace challenge. Our
dataset includes One Million photos that capture more than 690K different
individuals. The challenge evaluates performance of algorithms with increasing
numbers of distractors (going from 10 to 1M) in the gallery set. We present
both identification and verification performance, evaluate performance with
respect to pose and a person's age, and compare as a function of training data
size (number of photos and people). We report results of state of the art and
baseline algorithms. Our key observations are that testing at the million scale
reveals big performance differences (of algorithms that perform similarly well
on smaller scale) and that age invariant recognition as well as pose are still
challenging for most. The MegaFace dataset, baseline code, and evaluation
scripts, are all publicly released for further experimentations at:
megaface.cs.washington.edu.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00597</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00597</id><created>2015-12-02</created><authors><author><keyname>Sun</keyname><forenames>Sun</forenames></author><author><keyname>Dong</keyname><forenames>Min</forenames></author><author><keyname>Liang</keyname><forenames>Ben</forenames></author></authors><title>Distributed Real-Time Power Balancing in Renewable-Integrated Power
  Grids with Storage and Flexible Loads</title><categories>cs.SY</categories><comments>To appear in IEEE Transactions on Smart Grid</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The large-scale integration of renewable generation directly affects the
reliability of power grids. We investigate the problem of power balancing in a
general renewable-integrated power grid with storage and flexible loads. We
consider a power grid that is supplied by one conventional generator (CG) and
multiple renewable generators (RGs) each co-located with storage,and is
connected with external markets. An aggregator operates the power grid to
maintain power balance between supply and demand. Aiming at minimizing the
long-term system cost, we first propose a real-time centralized power balancing
solution, taking into account the uncertainty of the renewable generation,
loads, and energy prices. We then provide a distributed implementation
algorithm, significantly reducing both computational burden and communication
overhead. We demonstrate that our proposed algorithm is asymptotically optimal
as the storage capacity increases and the CG ramping constraint loosens.
Moreover, the distributed implementation enjoys a fast convergence rate, and
enables each RG and the aggregator to make their own decisions. Simulation
shows that our proposed algorithm outperforms alternatives and can achieve
near-optimal performance for a wide range of storage capacity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00607</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00607</id><created>2015-12-02</created><authors><author><keyname>Kato</keyname><forenames>Toshiyuki</forenames></author><author><keyname>Hino</keyname><forenames>Hideitsu</forenames></author><author><keyname>Murata</keyname><forenames>Noboru</forenames></author></authors><title>Double Sparse Multi-Frame Image Super Resolution</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A large number of image super resolution algorithms based on the sparse
coding are proposed, and some algorithms realize the multi-frame super
resolution. In multi-frame super resolution based on the sparse coding, both
accurate image registration and sparse coding are required. Previous study on
multi-frame super resolution based on sparse coding firstly apply block
matching for image registration, followed by sparse coding to enhance the image
resolution. In this paper, these two problems are solved by optimizing a single
objective function. The results of numerical experiments support the
effectiveness of the proposed approch.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00614</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00614</id><created>2015-12-02</created><authors><author><keyname>Maksimovic</keyname><forenames>Zoran</forenames></author></authors><title>A connected multidimensional maximum bisection problem</title><categories>cs.DM</categories><comments>arXiv admin note: text overlap with arXiv:1506.07731</comments><msc-class>90C11, 05C70</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The maximum graph bisection problem is a well known graph partition problem.
The problem has been proven to be NP-hard. In the maximum graph bisection
problem it is required that the set of vertices is divided into two partition
with equal number of vertices, and the weight of the edge cut is maximal.
  This work introduces a connected multidimensional generalization of the
maximum bisection problem. In this problem the weights on edges are vectors of
positive numbers rather than numbers and partitions should be connected. A
mixed integer linear programming formulation is proposed with the proof of its
correctness. The MILP formulation of the problem has polynomial number of
variables and constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00622</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00622</id><created>2015-12-02</created><authors><author><keyname>Boyali</keyname><forenames>Ali</forenames></author><author><keyname>Hashimoto</keyname><forenames>Naohisa</forenames></author><author><keyname>Kavakli</keyname><forenames>Manolya</forenames></author></authors><title>Continuous and Simultaneous Gesture and Posture Recognition for
  Commanding a Robotic Wheelchair; Towards Spotting the Signal Patterns</title><categories>cs.RO cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spotting signal patterns with varying lengths has been still an open problem
in the literature. In this study, we describe a signal pattern recognition
approach for continuous and simultaneous classification of a tracked hand's
posture and gestures and map them to steering commands for control of a robotic
wheelchair. The developed methodology not only affords 100\% recognition
accuracy on a streaming signal for continuous recognition, but also brings
about a new perspective for building a training dictionary which eliminates
human intervention to spot the gesture or postures on a training signal. In the
training phase we employ a state of art subspace clustering method to find the
most representative state samples. The recognition and training framework
reveal boundaries of the patterns on the streaming signal with a successive
decision tree structure intrinsically. We make use of the Collaborative ans
Block Sparse Representation based classification methods for continuous gesture
and posture recognition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00650</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00650</id><created>2015-12-02</created><authors><author><keyname>Guih&#xe9;neuf</keyname><forenames>Pierre-Antoine</forenames></author></authors><title>Model sets, almost periodic patterns, uniform density and linear maps</title><categories>math.DS cs.DM math.MG</categories><comments>17 pages. arXiv admin note: text overlap with arXiv:1510.00722</comments><msc-class>52C23, 52C07, 11H06</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article consists in two independent parts. In the first one, we
investigate the geometric properties of almost periodicity of model sets (or
cut-and-project sets, defined under the weakest hypotheses); in particular we
show that they are almost periodic patterns and thus possess a uniform density.
In the second part, we prove that the class of model sets and almost periodic
patterns are stable under discretizations of linear maps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00658</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00658</id><created>2015-12-02</created><authors><author><keyname>Fan</keyname><forenames>Li</forenames></author><author><keyname>Jin</keyname><forenames>Shi</forenames></author><author><keyname>Wen</keyname><forenames>Chao-Kai</forenames></author><author><keyname>Zhang</keyname><forenames>Haixia</forenames></author></authors><title>Uplink Achievable Rate for Massive MIMO with Low-Resolution ADC</title><categories>cs.IT math.IT</categories><comments>Accepted to appear in IEEE Communications Letters</comments><doi>10.1109/LCOMM.2015.2494600</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this letter, we derive an approximate analytical expression for the uplink
achievable rate of a massive multi-input multi-output (MIMO) antenna system
when finite precision analog-digital converters (ADCs) and the common maximal
ratio combining technique are used at the receivers. To obtain this expression,
we treat quantization noise as an additive quantization noise model.
Considering the obtained expression, we show that low-resolution ADCs lead to a
decrease in the achievable rate but the performance loss can be compensated by
increasing the number of receiving antennas. In addition, we investigate the
relation between the number of antennas and the ADC resolution, as well as the
power-scaling law. These discussions support the feasibility of equipping
highly economical ADCs with low resolution in practical massive MIMO systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00659</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00659</id><created>2015-12-02</created><authors><author><keyname>Govada</keyname><forenames>Aruna</forenames></author><author><keyname>Gauri</keyname><forenames>Bhavul</forenames></author><author><keyname>Sahay</keyname><forenames>S. K.</forenames></author></authors><title>Centroid Based Binary Tree Structured SVM for Multi Classification</title><categories>cs.LG</categories><comments>Presented in ICACCI, Kochi, India, 2015</comments><journal-ref>IEEE Xplore, Advances in Computing, Communications and Informatics
  (ICACCI), p.258 - 262, 2015</journal-ref><doi>10.1109/ICACCI.2015.7275618</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Support Vector Machines (SVMs) were primarily designed for 2-class
classification. But they have been extended for N-class classification also
based on the requirement of multiclasses in the practical applications.
Although N-class classification using SVM has considerable research attention,
getting minimum number of classifiers at the time of training and testing is
still a continuing research. We propose a new algorithm CBTS-SVM (Centroid
based Binary Tree Structured SVM) which addresses this issue. In this we build
a binary tree of SVM models based on the similarity of the class labels by
finding their distance from the corresponding centroids at the root level. The
experimental results demonstrates the comparable accuracy for CBTS with OVO
with reasonable gamma and cost values. On the other hand when CBTS is compared
with OVA, it gives the better accuracy with reduced training time and testing
time. Furthermore CBTS is also scalable as it is able to handle the large data
sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00661</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00661</id><created>2015-12-02</created><authors><author><keyname>Ambainis</keyname><forenames>Andris</forenames></author><author><keyname>Kokainis</keyname><forenames>Martins</forenames></author></authors><title>Almost quadratic gap between partition complexity and
  query/communication complexity</title><categories>cs.CC</categories><comments>17 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show nearly quadratic separations between two pairs of complexity
measures:
  1. We show that there is a Boolean function $f$ with
$D(f)=\Omega((D^{sc}(f))^{2-o(1)})$ where $D(f)$ is the deterministic query
complexity of $f$ and $D^{sc}$ is the subcube partition complexity of $f$;
  2. As a consequence, we obtain that there is a communication task $f(x, y)$
such that $D^{cc}(f)=\Omega(\log^{2-o(1)}\chi(f))$ where $D^{cc}(f)$ is the
deterministic 2-party communication complexity of $f$ (in the standard 2-party
model of communication) and $\chi(f)$ is the partition number of $f$.
  Both of those separations are nearly optimal: it is well known that
$D(f)=O((D^{sc}(f))^{2})$ and $D^{cc}(f)=O(\log^2\chi(f))$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00664</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00664</id><created>2015-12-02</created><authors><author><keyname>Govada</keyname><forenames>Aruna</forenames></author><author><keyname>Gauri</keyname><forenames>Bhavul</forenames></author><author><keyname>Sahay</keyname><forenames>S. K.</forenames></author></authors><title>Distributed Multi Class SVM for Large Data Sets</title><categories>cs.DC</categories><comments>Presente in the WCI, Kochi, India, 2015</comments><journal-ref>ACM Digital Library, Proceeding WCI '15 Proceedings of the Third
  International Symposium on Women in Computing and Informatics, PP. 54-58,
  2015</journal-ref><doi>10.1145/2791405.2791534</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data mining algorithms are originally designed by assuming the data is
available at one centralized site.These algorithms also assume that the whole
data is fit into main memory while running the algorithm. But in today's
scenario the data has to be handled is distributed even geographically.
Bringing the data into a centralized site is a bottleneck in terms of the
bandwidth when compared with the size of the data. In this paper for multiclass
SVM we propose an algorithm which builds a global SVM model by merging the
local SVMs using a distributed approach(DSVM). And the global SVM will be
communicated to each site and made it available for further classification. The
experimental analysis has shown promising results with better accuracy when
compared with both the centralized and ensemble method. The time complexity is
also reduced drastically because of the parallel construction of local SVMs.
The experiments are conducted by considering the data sets of size 100s to
hundred of 100s which also addresses the issue of scalability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00665</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00665</id><created>2015-12-02</created><authors><author><keyname>Wang</keyname><forenames>Weidong</forenames></author><author><keyname>Liao</keyname><forenames>Chunhua</forenames></author><author><keyname>Wang</keyname><forenames>Liqiang</forenames></author><author><keyname>Quinlan</keyname><forenames>Daniel J.</forenames></author><author><keyname>Lu</keyname><forenames>Wei</forenames></author></authors><title>HBTM: A Heartbeat-based Behavior Detection Mechanism for POSIX Threads
  and OpenMP Applications</title><categories>cs.DC</categories><comments>7 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Extreme-scale computing involves hundreds of millions of threads with
multi-level parallelism running on large-scale hierarchical and heterogeneous
hardware. In POSIX threads and OpenMP applications, some key behaviors
occurring in runtime such as thread failure, busy waiting, and exit need to be
accurately and timely detected. However, for the most of these applications,
there are lack of unified and efficient detection mechanisms to do this. In
this paper, a heartbeat-based behavior detection mechanism for POSIX threads
(Pthreads) and OpenMP applications (HBTM) is proposed. In the design, two types
of implementations are conducted, centralized and decentralized respectively.
In both implementations, unified API has been designed to guarantee the
generality of the mechanism. Meanwhile, a ring-based detection algorithm is
designed to ease the burden of the centra thread at runtime. To evaluate the
mechanism, the NAS Parallel Benchmarks (NPB) are used to test the performance
of the HBTM. The experimental results show that the HBTM supports detection of
behaviors of POSIX threads and OpenMP applications while acquiring a short
latency and near 1% overhead.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00682</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00682</id><created>2015-12-02</created><authors><author><keyname>Yavuz</keyname><forenames>Davut Deniz</forenames></author><author><keyname>Abul</keyname><forenames>Osman</forenames></author></authors><title>Implicit Location Sharing Detection in Social Media from Short Turkish
  Text</title><categories>cs.CY</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social media have become a significant venue for information sharing of live
updates. Users of social media are producing and sharing large amount of
personal data as a part of the live updates. A significant percentage of this
data contains location information that can be used by other people for many
purposes. Some of the social media users deliberately share their own location
information with other social network users. However, a large number of social
media users blindly or implicitly share their location without noticing it or
its possible consequences. Implicit location sharing is investigated in the
current paper. We perform a large scale study on implicit location sharing for
one of the most popular social media platform, namely Twitter. After a careful
study, we built a dataset of Turkish tweets and manually tagged them. Using
machine learning techniques we built classifiers that are able to classify
whether a given tweet contains implicit location sharing or not. The
classifiers are shown to be very accurate and efficient. Moreover, the best
classifier is employed as a browser add-on tool which warns the user whenever
an implicit location sharing is predicted from to be released tweet. The paper
provides the methodology and the technical analysis as well. Furthermore, it
discusses how these techniques can be extended to different social network
services and also to different languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00708</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00708</id><created>2015-12-02</created><authors><author><keyname>Biyanto</keyname><forenames>Totok Ruki</forenames></author><author><keyname>Fibrianto</keyname><forenames>Henokh Yernias</forenames></author><author><keyname>Nugroho</keyname><forenames>Gunawan</forenames></author><author><keyname>Listijorini</keyname><forenames>Erny</forenames></author><author><keyname>Budiati</keyname><forenames>Titik</forenames></author><author><keyname>Huda</keyname><forenames>Hairul</forenames></author></authors><title>Duelist Algorithm: An Algorithm Inspired by How Duelist Improve Their
  Capabilities in a Duel</title><categories>cs.NE</categories><comments>This paper under submission to the Journal of Swarm and Evolutionary
  Computation, consist of 7 pages and 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes an optimization algorithm based on how human fight and
learn from each duelist. Since this algorithm is based on population, the
proposed algorithm starts with an initial set of duelists. The duel is to
determine the winner and loser. The loser learns from the winner, while the
winner try their new skill or technique that may improve their fighting
capabilities. A few duelists with highest fighting capabilities are called as
champion. The champion train a new duelists such as their capabilities. The new
duelist will join the tournament as a representative of each champion. All
duelist are re-evaluated, and the duelists with worst fighting capabilities is
eliminated to maintain the amount of duelists. Two optimization problem is
applied for the proposed algorithm, together with genetic algorithm, particle
swarm optimization and imperialist competitive algorithm. The results show that
the proposed algorithm is able to find the better global optimum and faster
iteration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00717</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00717</id><created>2015-12-02</created><authors><author><keyname>Pyatykh</keyname><forenames>Stanislav</forenames></author><author><keyname>Hesser</keyname><forenames>J&#xfc;rgen</forenames></author></authors><title>MMSE Estimation for Poisson Noise Removal in Images</title><categories>cs.CV cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Poisson noise suppression is an important preprocessing step in several
applications, such as medical imaging, microscopy, and astronomical imaging. In
this work, we propose a novel patch-wise Poisson noise removal strategy, in
which the MMSE estimator is utilized in order to produce the denoising result
for each image patch. Fast and accurate computation of the MMSE estimator is
carried out using k-d tree search followed by search in the K-nearest neighbor
graph. Our experiments show that the proposed method is the preferable choice
for low signal-to-noise ratios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00720</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00720</id><created>2015-11-27</created><updated>2015-12-14</updated><authors><author><keyname>Bl&#xf6;mer</keyname><forenames>Johannes</forenames></author><author><keyname>Kohn</keyname><forenames>Kathl&#xe9;n</forenames></author></authors><title>Voronoi Cells of Lattices with Respect to Arbitrary Norms</title><categories>math.MG cs.CG math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by the deterministic single exponential time algorithm of
Micciancio and Voulgaris for solving the shortest and closest vector problem
for the Euclidean norm, we study the geometry and complexity of Voronoi cells
of lattices with respect to arbitrary norms. On the positive side, we show that
for strictly convex and smooth norms the geometry of Voronoi cells of lattices
in any dimension is similar to the Euclidean case, i.e., the Voronoi cells are
defined by the so-called Voronoi-relevant vectors and the facets of a Voronoi
cell are in one-to-one correspondence with these vectors. On the negative side,
we show that combinatorially Voronoi cells for arbitrary strictly convex and
smooth norms are much more complicated than in the Euclidean case. In
particular, we construct a family of three-dimensional lattices whose number of
Voronoi-relevant vectors with respect to the $\ell_3$-norm is unbounded. Since
the algorithm of Micciancio and Voulgaris and its run time analysis crucially
depend on the fact that for the Euclidean norm the number of Voronoi-relevant
vectors is single exponential in the lattice dimension, this indicates that the
techniques of Micciancio and Voulgaris cannot be extended to achieve
deterministic single exponential time algorithms for lattice problems with
respect to arbitrary $\ell_p$-norms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00727</identifier>
 <datestamp>2015-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00727</id><created>2015-12-02</created><updated>2015-12-03</updated><authors><author><keyname>Einziger</keyname><forenames>Gil</forenames></author><author><keyname>Friedman</keyname><forenames>Roy</forenames></author><author><keyname>Manes</keyname><forenames>Ben</forenames></author></authors><title>TinyLFU: A Highly Efficient Cache Admission Policy</title><categories>cs.OS</categories><comments>A much earlier and shorter version of this work appeared in the
  Euromicro PDP 2014 conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes to use a frequency based cache admission policy in order
to boost the effectiveness of caches subject to skewed access distributions.
Given a newly accessed item and an eviction candidate from the cache, our
scheme decides, based on the recent access history, whether it is worth
admitting the new item into the cache at the expense of the eviction candidate.
  Realizing this concept is enabled through a novel approximate LFU structure
called TinyLFU, which maintains an approximate representation of the access
frequency of a large sample of recently accessed items. TinyLFU is very compact
and light-weight as it builds upon Bloom filter theory.
  We study the properties of TinyLFU through simulations of both synthetic
workloads as well as multiple real traces from several sources. These
simulations demonstrate the performance boost obtained by enhancing various
replacement policies with the TinyLFU eviction policy. Also, a new combined
replacement and eviction policy scheme nicknamed W-TinyLFU is presented.
W-TinyLFU is demonstrated to obtain equal or better hit-ratios than other state
of the art replacement policies on these traces. It is the only scheme to
obtain such good results on all traces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00728</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00728</id><created>2015-12-02</created><authors><author><keyname>Massey</keyname><forenames>Philip</forenames></author><author><keyname>Xia</keyname><forenames>Patrick</forenames></author><author><keyname>Bamman</keyname><forenames>David</forenames></author><author><keyname>Smith</keyname><forenames>Noah A.</forenames></author></authors><title>Annotating Character Relationships in Literary Texts</title><categories>cs.CL</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  We present a dataset of manually annotated relationships between characters
in literary texts, in order to support the training and evaluation of automatic
methods for relation type prediction in this domain (Makazhanov et al., 2014;
Kokkinakis, 2013) and the broader computational analysis of literary character
(Elson et al., 2010; Bamman et al., 2014; Vala et al., 2015; Flekova and
Gurevych, 2015). In this work, we solicit annotations from workers on Amazon
Mechanical Turk for 109 texts ranging from Homer's _Iliad_ to Joyce's _Ulysses_
on four dimensions of interest: for a given pair of characters, we collect
judgments as to the coarse-grained category (professional, social, familial),
fine-grained category (friend, lover, parent, rival, employer), and affinity
(positive, negative, neutral) that describes their primary relationship in a
text. We do not assume that this relationship is static; we also collect
judgments as to whether it changes at any point in the course of the text.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00743</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00743</id><created>2015-12-02</created><authors><author><keyname>Gudi</keyname><forenames>Amogh</forenames></author></authors><title>Recognizing Semantic Features in Faces using Deep Learning</title><categories>cs.LG cs.CV stat.ML</categories><comments>Thesis, M.Sc. Artificial Intelligence, Universiteit van Amsterdam,
  2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The human face constantly conveys information, both consciously and
subconsciously. However, as basic as it is for humans to visually interpret
this information, it is quite a big challenge for machines. Conventional
semantic facial feature recognition and analysis techniques are already in use
and are based on physiological heuristics, but they suffer from lack of
robustness and high computation time. This thesis aims to explore ways for
machines to learn to interpret semantic information available in faces in an
automated manner without requiring manual design of feature detectors, using
the approach of Deep Learning. This thesis provides a study of the effects of
various factors and hyper-parameters of deep neural networks in the process of
determining an optimal network configuration for the task of semantic facial
feature recognition. This thesis explores the effectiveness of the system to
recognize the various semantic features (like emotions, age, gender, ethnicity
etc.) present in faces. Furthermore, the relation between the effect of
high-level concepts on low level features is explored through an analysis of
the similarities in low-level descriptors of different semantic features. This
thesis also demonstrates a novel idea of using a deep network to generate 3-D
Active Appearance Models of faces from real-world 2-D images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00747</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00747</id><created>2015-12-02</created><authors><author><keyname>Mosinska</keyname><forenames>Agata</forenames></author><author><keyname>Sznitman</keyname><forenames>Raphael</forenames></author><author><keyname>G&#x142;owacki</keyname><forenames>Przemys&#x142;aw</forenames></author><author><keyname>Fua</keyname><forenames>Pascal</forenames></author></authors><title>Active Learning for Delineation of Curvilinear Structures</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many recent delineation techniques owe much of their increased effectiveness
to path classification algorithms that make it possible to distinguish
promising paths from others. The downside of this development is that they
require annotated training data, which is tedious to produce.
  In this paper, we propose an Active Learning approach that considerably
speeds up the annotation process. Unlike standard ones, it takes advantage of
the specificities of the delineation problem. It operates on a graph and can
reduce the training set size by up to 80% without compromising the
reconstruction quality.
  We will show that our approach outperforms conventional ones on various
biomedical and natural image datasets, thus showing that it is broadly
applicable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00750</identifier>
 <datestamp>2015-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00750</id><created>2015-11-29</created><authors><author><keyname>Smith</keyname><forenames>Reginald D.</forenames></author></authors><title>A Mutual Information Approach to Calculating Nonlinearity</title><categories>cs.IT math.IT</categories><comments>13 pages, 2 figures</comments><journal-ref>STAT, 4, 291-303 (2015)</journal-ref><doi>10.1002/sta4.96</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new method to measure nonlinear dependence between two variables is
described using mutual information to analyze the separate linear and nonlinear
components of dependence. This technique, which gives an exact value for the
proportion of linear dependence, is then compared with another common test for
linearity, the Brock, Dechert and Scheinkman (BDS) test.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00757</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00757</id><created>2015-12-02</created><authors><author><keyname>Tan</keyname><forenames>Zilong</forenames></author><author><keyname>Babu</keyname><forenames>Shivnath</forenames></author></authors><title>Tempo: Robust and Self-Tuning Resource Management in Multi-tenant
  Parallel Databases</title><categories>cs.DB</categories><comments>14 pages, 12 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-tenant database systems have a component called the Resource Manager,
or RM that is responsible for allocating resources to tenants. RMs today do not
provide direct support for performance objectives such as: &quot;Average job
response time of tenant A must be less than two minutes&quot;, or &quot;No more than 5%
of tenant B's jobs can miss the deadline of 1 hour.&quot; Thus, DBAs have to tinker
with the RM's low-level configuration settings to meet such objectives. We
propose a framework called Tempo that brings simplicity, self-tuning, and
robustness to existing RMs. Tempo provides a simple interface for DBAs to
specify performance objectives declaratively, and optimizes the RM
configuration settings to meet these objectives. Tempo has a solid theoretical
foundation which gives key robustness guarantees. We report experiments done on
Tempo using production traces of data-processing workloads from companies such
as Facebook and Cloudera. These experiments demonstrate significant
improvements in meeting desired performance objectives over RM configuration
settings specified by human experts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00762</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00762</id><created>2015-12-02</created><authors><author><keyname>Azim</keyname><forenames>Zubair Al</forenames></author><author><keyname>Sengupta</keyname><forenames>Abhronil</forenames></author><author><keyname>Sarwar</keyname><forenames>Syed Shakib</forenames></author><author><keyname>Roy</keyname><forenames>Kaushik</forenames></author></authors><title>Spin-Torque Sensors for Energy Efficient High Speed Long Interconnects</title><categories>cs.ET</categories><comments>To appear in IEEE Transactions on Electron Devices</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a Spin-Torque (ST) based sensing scheme that can
enable energy efficient multi-bit long distance interconnect architectures.
Current-mode interconnects have recently been proposed to overcome the
performance degradations associated with conventional voltage mode Copper (Cu)
interconnects. However, the performance of current mode interconnects are
limited by analog current sensing transceivers and equalization circuits. As a
solution, we propose the use of ST based receivers that use Magnetic Tunnel
Junctions (MTJ) and simple digital components for current-to-voltage conversion
and do not require analog transceivers. We incorporate Spin-Hall Metal (SHM) in
our design to achieve high speed sensing. We show both single and multi-bit
operations that reveal major benefits at higher speeds. Our simulation results
show that the proposed technique consumes only 3.93-4.72 fJ/bit/mm energy while
operating at 1-2 Gbits/sec; which is considerably better than existing charge
based interconnects. In addition, Voltage Controlled Magnetic Anisotropy (VCMA)
can reduce the required current at the sensor. With the inclusion of VCMA, the
energy consumption can be further reduced to 2.02-4.02 fJ/bit/mm
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00764</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00764</id><created>2015-12-01</created><authors><author><keyname>Kernahan</keyname><forenames>Michael</forenames></author><author><keyname>Capretz</keyname><forenames>Miriam</forenames></author><author><keyname>Capretz</keyname><forenames>Luiz Fernando</forenames></author></authors><title>Extracting Traceability Information from C# Projects</title><categories>cs.SE</categories><comments>arXiv admin note: substantial text overlap with arXiv:1507.06949</comments><journal-ref>9th International Conference on Computers, Athens, Greece, pp.
  1-6, 2005</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The maintenance portion of the software lifecycle represents a major drain on
most software companys resources. The transition from programmers to
maintainers is high risk, since usually the maintainers have to learn the
system from scratch before they can begin modifying it appropriately. This
paper introduces a method for automatically extracting important traceability
information from a C# software projects source code. Using this traceability
information, maintainers (and programmers) are better able to evaluate the
impacts their actions will have on the entire project.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00765</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00765</id><created>2015-12-02</created><authors><author><keyname>De Boom</keyname><forenames>Cedric</forenames></author><author><keyname>Van Canneyt</keyname><forenames>Steven</forenames></author><author><keyname>Bohez</keyname><forenames>Steven</forenames></author><author><keyname>Demeester</keyname><forenames>Thomas</forenames></author><author><keyname>Dhoedt</keyname><forenames>Bart</forenames></author></authors><title>Learning Semantic Similarity for Very Short Texts</title><categories>cs.IR cs.CL</categories><comments>6 pages, 5 figures, 3 tables, ReLSD workshop at ICDM 15</comments><doi>10.1109/ICDMW.2015.86</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Levering data on social media, such as Twitter and Facebook, requires
information retrieval algorithms to become able to relate very short text
fragments to each other. Traditional text similarity methods such as tf-idf
cosine-similarity, based on word overlap, mostly fail to produce good results
in this case, since word overlap is little or non-existent. Recently,
distributed word representations, or word embeddings, have been shown to
successfully allow words to match on the semantic level. In order to pair short
text fragments - as a concatenation of separate words - an adequate distributed
sentence representation is needed, in existing literature often obtained by
naively combining the individual word representations. We therefore
investigated several text representations as a combination of word embeddings
in the context of semantic pair matching. This paper investigates the
effectiveness of several such naive techniques, as well as traditional tf-idf
similarity, for fragments of different lengths. Our main contribution is a
first step towards a hybrid method that combines the strength of dense
distributed representations - as opposed to sparse term matching - with the
strength of tf-idf based methods to automatically reduce the impact of less
informative terms. Our new approach outperforms the existing techniques in a
toy experimental set-up, leading to the conclusion that the combination of word
embeddings and tf-idf information might lead to a better model for semantic
content within very short text fragments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00766</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00766</id><created>2015-12-02</created><authors><author><keyname>Gesmundo</keyname><forenames>Fulvio</forenames></author></authors><title>Gemetric Aspects of Iterated Matrix Multiplication</title><categories>math.AG cs.CC</categories><comments>21 pages</comments><msc-class>68Q17, 15A86, 14L40, 16G20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies geometric properties of the Iterated Matrix Multiplication
polynomial and the hypersurface that it defines. We focus on geometric aspects
that may be relevant for complexity theory such as the symmetry group of the
polynomial, the dual variety and the Jacobian loci of the hypersurface, that
are computed with the aid of representation theory of quivers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00770</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00770</id><created>2015-12-02</created><authors><author><keyname>Grabowicz</keyname><forenames>Przemyslaw A.</forenames></author><author><keyname>Romero-Ferrero</keyname><forenames>Francisco</forenames></author><author><keyname>Lins</keyname><forenames>Theo</forenames></author><author><keyname>de Polavieja</keyname><forenames>Gonzalo G.</forenames></author><author><keyname>Benevenuto</keyname><forenames>Fabr&#xed;cio</forenames></author><author><keyname>Gummadi</keyname><forenames>Krishna P.</forenames></author></authors><title>An experimental study of opinion influenceability</title><categories>physics.soc-ph cs.CY cs.SI physics.data-an</categories><comments>21 pages, 19 figures</comments><acm-class>H.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Humans, like many other animal species, often make choices under social
influence. Experiments in ants and fishes have shown that individuals choose
according to estimations of which option to take given private and social
information. Principled approaches based on probabilistic estimations by agents
give mathematical formulas explaining experiments in these species. Here we
test whether the same principled approaches can explain social influence in
humans. We conduct a large online field experiment in which we measure opinion
influenced by public comments about short movies in the most popular
video-sharing website. We show that the basic principles of social influence in
other species also apply to humans, with the added complexity that humans are
heterogenous. We infer influenceability of each participant of the experiment,
finding that individuals prone to social influence tend to agree with social
feedback, read less comments, and are less educated than the persons who resist
influence. We believe that our results will help to build a mathematical theory
of social influence rooted in probabilistic reasoning that show commonalities
and differences between humans and other species.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00775</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00775</id><created>2015-12-02</created><authors><author><keyname>Tsitsvero</keyname><forenames>Mikhail</forenames></author><author><keyname>Barbarossa</keyname><forenames>Sergio</forenames></author><author><keyname>Di Lorenzo</keyname><forenames>Paolo</forenames></author></authors><title>Uncertainty Principle and Sampling of Signals Defined on Graphs</title><categories>cs.IT cs.DM math.IT math.SP</categories><comments>6 pages, 3 figures. Asilomar Conference on Signals, Systems, and
  Computers, 8-11 November, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many applications, from sensor to social networks, gene regulatory
networks or big data, observations can be represented as a signal defined over
the vertices of a graph. Building on the recently introduced Graph Fourier
Transform, the first contribution of this paper is to provide an uncertainty
principle for signals on graph. As a by-product of this theory, we show how to
build a dictionary of maximally concentrated signals on vertex/frequency
domains. Then, we establish a direct relation between uncertainty principle and
sampling, which forms the basis for a sampling theorem of signals defined on
graph. Based on this theory, we show that, besides sampling rate, the samples'
location plays a key role in the performance of signal recovery algorithms.
Hence, we suggest a few alternative sampling strategies and compare them with
recently proposed methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00779</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00779</id><created>2015-12-02</created><authors><author><keyname>Ipparthi</keyname><forenames>Dhananjay</forenames></author><author><keyname>Mastrangeli</keyname><forenames>Massimo</forenames></author><author><keyname>Winslow</keyname><forenames>Andrew</forenames></author></authors><title>Dipole Codes Attractively Encode Glue Functions</title><categories>cs.FL</categories><comments>To appear in Theoretical Computer Science</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dipole words are sequences of magnetic dipoles, in which alike elements repel
and opposite elements attract. Magnetic dipoles contrast with more general sets
of bonding types, called glues, in which pairwise bonding strength is specified
by a glue function. We prove that every glue function $g$ has a set of dipole
words, called a dipole code, that attractively encodes $g$: the pairwise
attractions (positive or non-positive bond strength) between the words are
identical to those of $g$. Moreover, we give such word sets of asymptotically
optimal length. Similar results are obtained for a commonly used subclass of
glue functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00783</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00783</id><created>2015-12-02</created><authors><author><keyname>Tsitsvero</keyname><forenames>Mikhail</forenames></author><author><keyname>Barbarossa</keyname><forenames>Sergio</forenames></author></authors><title>On the Degrees of Freedom of Signals on Graphs</title><categories>cs.IT cs.DM math.IT</categories><comments>5 pages, 2 figures; Presented on 23-rd European Signal Processing
  Conference (EUSIPCO), September, 2015, Nice, France</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Continuous-time signals are well known for not being perfectly localized in
both time and frequency domains. Conversely, a signal defined over the vertices
of a graph can be perfectly localized in both vertex and frequency domains. We
derive the conditions ensuring the validity of this property and then, building
on this theory, we provide the conditions for perfect reconstruction of a graph
signal from its samples. Next, we provide a finite step algorithm for the
reconstruction of a band-limited signal from its samples and then we show the
effect of sampling a non perfectly band-limited signal and show how to select
the bandwidth that minimizes the mean square reconstruction error.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00787</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00787</id><created>2015-12-02</created><authors><author><keyname>Torres</keyname><forenames>Surayne</forenames></author><author><keyname>Pinero</keyname><forenames>Yadenis</forenames></author><author><keyname>Pinero</keyname><forenames>Pedro</forenames></author><author><keyname>Capretz</keyname><forenames>Luiz Fernando</forenames></author></authors><title>Creation and Evaluation of Software Teams - A Social Approach</title><categories>cs.CY</categories><journal-ref>International Journal of Manufacturing Technology and Management,
  28(4/5/6):167-187, 2014</journal-ref><doi>10.1504/IJMTM.2014.066695</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work discusses an important issue in the area of human resource
management by proposing a novel model for creation and evaluation of software
teams. The model consists of several assessments, including a technical test, a
quality of life test and a psychological-sociological test. Since the technical
test requires particular organizational specifications and cannot be examined
without reference to a specific company, only the sociological test and the
quality of life tests are extensively discussed in this work. Two strategies
are discussed for assigning roles in a project. Initially, six software
projects were selected, and after extensive analysis of the projects, two
projects were chosen and correctives actions were applied. An empirical
evaluation was also conducted to assess the model effectiveness. The
experimental results demonstrate that the application of the model improved the
productivity of project teams.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00795</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00795</id><created>2015-12-02</created><authors><author><keyname>Wang</keyname><forenames>Xiaolong</forenames></author><author><keyname>Farhadi</keyname><forenames>Ali</forenames></author><author><keyname>Gupta</keyname><forenames>Abhinav</forenames></author></authors><title>Actions ~ Transformations</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  What defines an action like &quot;kicking ball&quot;? We argue that the true meaning of
an action lies in the change or transformation an action brings to the
environment. In this paper, we propose a novel representation for actions by
modeling action as a transformation which changes the state of the environment
before the action happens (precondition) to the state after the action
(effect). Motivated by the recent advancement of video representation using
deep learning, we design a Siamese network which models the action as the
transformation on a high-level feature space. We show that our model gives
improvements on standard action recognition datasets including UCF101 and
HMDB51. More importantly, our approach is able to generalize beyond learned
action categories and shows significant performance improvement on
cross-category generalization on our new ACT dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00818</identifier>
 <datestamp>2015-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00818</id><created>2015-12-02</created><updated>2015-12-15</updated><authors><author><keyname>Elhoseiny</keyname><forenames>Mohamed</forenames></author><author><keyname>Liu</keyname><forenames>Jingen</forenames></author><author><keyname>Cheng</keyname><forenames>Hui</forenames></author><author><keyname>Sawhney</keyname><forenames>Harpreet</forenames></author><author><keyname>Elgammal</keyname><forenames>Ahmed</forenames></author></authors><title>Zero-Shot Event Detection by Multimodal Distributional Semantic
  Embedding of Videos</title><categories>cs.CV cs.CL cs.LG</categories><comments>To appear in AAAI 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new zero-shot Event Detection method by Multi-modal
Distributional Semantic embedding of videos. Our model embeds object and action
concepts as well as other available modalities from videos into a
distributional semantic space. To our knowledge, this is the first Zero-Shot
event detection model that is built on top of distributional semantics and
extends it in the following directions: (a) semantic embedding of multimodal
information in videos (with focus on the visual modalities), (b) automatically
determining relevance of concepts/attributes to a free text query, which could
be useful for other applications, and (c) retrieving videos by free text event
query (e.g., &quot;changing a vehicle tire&quot;) based on their content. We embed videos
into a distributional semantic space and then measure the similarity between
videos and the event query in a free text form. We validated our method on the
large TRECVID MED (Multimedia Event Detection) challenge. Using only the event
title as a query, our method outperformed the state-of-the-art that uses big
descriptions from 12.6% to 13.5% with MAP metric and 0.73 to 0.83 with ROC-AUC
metric. It is also an order of magnitude faster.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00822</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00822</id><created>2015-12-02</created><authors><author><keyname>Arashloo</keyname><forenames>Mina Tahmasbi</forenames></author><author><keyname>Koral</keyname><forenames>Yaron</forenames></author><author><keyname>Greenberg</keyname><forenames>Michael</forenames></author><author><keyname>Rexford</keyname><forenames>Jennifer</forenames></author><author><keyname>Walker</keyname><forenames>David</forenames></author></authors><title>SNAP: Stateful Network-Wide Abstractions for Packet Processing</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Early programming languages for Software-Defined Networking (SDN) were built
on top of the simple match-action paradigm offered by OpenFlow 1.0. However,
emerging switches and middleboxes offer much more sophisticated support for
persistent state in the data plane, without involving a central controller. In
this paper, we introduce high-level programming abstractions and compiler
technology that exploit these low-level mechanisms.
  Our SNAP language allows programmers to mix stateful programming with pure
packet processing using global, persistent arrays indexed by packet-header
fields. This allows SNAP programs to learn about the network environment, store
per-flow information, and implement various stateful network functions. SNAP is
high-level and modular, allowing flexible composition of independently-written
programs on top of a &quot;one-big-switch&quot; abstraction. Our SNAP compiler analyzes
programs to discover dependencies and race conditions, and offers a simple
network transaction abstraction as a way to resolve the latter. A mixed
integer-linear program optimizes both placement of state variables on switches
and also routing traffic through them efficiently and in the right order. To
compile programs efficiently, our compiler uses a novel variant of binary
decision diagrams (BDDs), extended to incorporate stateful components. We have
implemented a prototype compiler, applied it to a range of programs and
topologies, and measured our techniques' scalability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00824</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00824</id><created>2015-12-02</created><updated>2016-01-09</updated><authors><author><keyname>Graves</keyname><forenames>Eric</forenames></author><author><keyname>Wong</keyname><forenames>Tan F.</forenames></author></authors><title>Equal-image-size source partitioning: Creating strong Fano's
  inequalities for multi-terminal discrete memoryless channels</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE trans info theory. Presented in part at the 2014
  International Symposium on Information Theory (ISIT) in Honolulu, HI, USA. 29
  pages, 1 example. Comments welcome. This version fixes a mistake in the
  application example and corrects a number of typos</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces equal-image-size source partitioning, a new tool for
analyzing channel and joint source-channel coding in a multi-terminal discrete
memoryless channel environment. Equal-image-size source partitioning divides
the source (combination of messages and codewords) into a sub-exponential
number of subsets. Over each of these subsets, the exponential orders of the
minimum image sizes of most messages are roughly equal to the same entropy
term. This property gives us the strength of minimum image sizes and the
flexibility of entropy terms. Using the method of equal-image-size source
partitioning, we prove separate necessary conditions for the existence of
average-error and maximum-error codes. These necessary conditions are much
stronger than the standard Fano's inequality, and can be weakened to render
versions of Fano's inequality that apply to codes with non-vanishing error
probabilities. To demonstrate the power of this new tool, we employ the
stronger average-error version of Fano's inequality to prove the strong
converse for the discrete memoryless wiretap channel with decaying leakage,
which heretofore has been an open problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00834</identifier>
 <datestamp>2015-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00834</id><created>2015-12-02</created><authors><author><keyname>Ballen</keyname><forenames>Peter</forenames></author><author><keyname>Guha</keyname><forenames>Sudipto</forenames></author></authors><title>Behavioral Intervention and Non-Uniform Bootstrap Percolation</title><categories>math.PR cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bootstrap percolation is an often used model to study the spread of diseases,
rumors, and information on sparse random graphs. The percolation process
demonstrates a critical value such that the graph is either almost completely
affected or almost completely unaffected based on the initial seed being larger
or smaller than the critical value.
  To analyze intervention strategies we provide the first analytic
determination of the critical value for basic bootstrap percolation in random
graphs when the vertex thresholds are nonuniform and provide an efficient
algorithm. This result also helps solve the problem of &quot;Percolation with
Coinflips&quot; when the infection process is not deterministic, which has been a
criticism about the model. We also extend the results to clustered random
graphs thereby extending the classes of graphs considered. In these graphs the
vertices are grouped in a small number of clusters, the clusters model a fixed
communication network and the edge probability is dependent if the vertices are
in close or far clusters. We present simulations for both basic percolation and
interventions that support our theoretical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00843</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00843</id><created>2015-12-01</created><updated>2015-12-10</updated><authors><author><keyname>Wang</keyname><forenames>Sheng</forenames></author><author><keyname>Peng</keyname><forenames>Jian</forenames></author><author><keyname>Ma</keyname><forenames>Jianzhu</forenames></author><author><keyname>Xu</keyname><forenames>Jinbo</forenames></author></authors><title>Protein secondary structure prediction using deep convolutional neural
  fields</title><categories>q-bio.BM cs.LG q-bio.QM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Protein secondary structure (SS) prediction is important for studying protein
structure and function. When only the sequence (profile) information is used as
input feature, currently the best predictors can obtain ~80% Q3 accuracy, which
has not been improved in the past decade. Here we present DeepCNF (Deep
Convolutional Neural Fields) for protein SS prediction. DeepCNF is a Deep
Learning extension of Conditional Neural Fields (CNF), which is an integration
of Conditional Random Fields (CRF) and shallow neural networks. DeepCNF can
model not only complex sequence-structure relationship by a deep hierarchical
architecture, but also interdependency between adjacent SS labels, so it is
much more powerful than CNF. Experimental results show that DeepCNF can obtain
~84% Q3 accuracy, ~85% SOV score, and ~72% Q8 accuracy, respectively, on the
CASP and CAMEO test proteins, greatly outperforming currently popular
predictors. As a general framework, DeepCNF can be used to predict other
protein structure properties such as contact number, disorder regions, and
solvent accessibility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00859</identifier>
 <datestamp>2015-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00859</id><created>2015-12-02</created><authors><author><keyname>Mandr&#xe0;</keyname><forenames>Salvatore</forenames></author><author><keyname>Guerreschi</keyname><forenames>Gian Giacomo</forenames></author><author><keyname>Aspuru-Guzik</keyname><forenames>Al&#xe1;n</forenames></author></authors><title>Faster than Classical Quantum Algorithm for dense Formulas of Exact
  Satisfiability and Occupation Problems</title><categories>quant-ph cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an exact quantum algorithm for solving the Exact Satisfiability
problem, which is known to belong to the important NP-complete complexity
class. The algorithm is based on an intuitive approach that can be divided into
two parts: The first step consists in the identification and efficient
characterization of a restricted subspace that contains all the valid
assignments (if any solution exists) of the Exact Satisfiability; while the
second part performs a quantum search in such restricted subspace. The quantum
algorithm can be used either to find a valid assignment (or to certify that no
solutions exist) or to count the total number of valid assignments. The query
complexity for the worst-case is, respectively, bounded by
$O(\sqrt{2^{n-M^{\prime}}})$ and $O(2^{n-M^{\prime}})$, where $n$ is the number
of variables and $M^{\prime}$ the number of linearly independent clauses.
Remarkably, the proposed quantum algorithm results to be faster than any known
exact classical algorithm to solve dense formulas of Exact Satisfiability. When
compared to heuristic techniques, the proposed quantum algorithm is faster than
the classical WalkSAT heuristic and Adiabatic Quantum Optimization for random
instances with a density of constraints close to the satisfiability threshold,
the regime in which instances are typically the hardest to solve. The quantum
algorithm that we propose can be straightforwardly extended to the generalized
version of the Exact Satisfiability known as Occupation problems. The general
version of the algorithm is presented and analyzed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00877</identifier>
 <datestamp>2015-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00877</id><created>2015-12-02</created><authors><author><keyname>Tuke</keyname><forenames>Jonathan</forenames></author><author><keyname>Roughan</keyname><forenames>Matthew</forenames></author></authors><title>All networks look the same to me: Testing for homogeneity in networks</title><categories>stat.ME cs.SI</categories><comments>19 pages, 7 figures, 5 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How can researchers test for heterogeneity in the local structure of a
network? In this paper, we present a framework that utilizes random sampling to
give subgraphs which are then used in a goodness of fit test to test for
heterogeneity. We illustrate how to use the goodness of fit test for an
analytically derived distribution as well as an empirical distribution. To
demonstrate our framework, we consider the simple case of testing for edge
probability heterogeneity. We examine the significance level, power and
computation time for this case with appropriate examples. Finally we outline
how to apply our framework to other heterogeneity problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00880</identifier>
 <datestamp>2015-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00880</id><created>2015-12-02</created><authors><author><keyname>Aerts</keyname><forenames>Diederik</forenames></author><author><keyname>de Bianchi</keyname><forenames>Massimiliano Sassoli</forenames></author></authors><title>The GTR-model: a universal framework for quantum-like measurements</title><categories>quant-ph cs.AI</categories><comments>33 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a very general geometrico-dynamical description of physical or
more abstract entities, called the 'general tension-reduction' (GTR) model,
where not only states, but also measurement-interactions can be represented,
and the associated outcome probabilities calculated. Underlying the model is
the hypothesis that indeterminism manifests as a consequence of unavoidable
fluctuations in the experimental context, in accordance with the
'hidden-measurements interpretation' of quantum mechanics. When the structure
of the state space is Hilbertian, and measurements are of the 'universal' kind,
i.e., are the result of an average over all possible ways of selecting an
outcome, the GTR-model provides the same predictions of the Born rule, and
therefore provides a natural completed version of quantum mechanics. However,
when the structure of the state space is non-Hilbertian and/or not all possible
ways of selecting an outcome are available to be actualized, the predictions of
the model generally differ from the quantum ones, especially when sequential
measurements are considered. Some paradigmatic examples will be discussed,
taken from physics and human cognition. Particular attention will be given to
some known psychological effects, like question order effects and response
replicability, which we show are able to generate non-Hilbertian statistics. We
also suggest a realistic interpretation of the GTR-model, when applied to human
cognition and decision, which we think could become the generally adopted
interpretative framework in quantum cognition research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00883</identifier>
 <datestamp>2015-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00883</id><created>2015-12-02</created><authors><author><keyname>Biyanto</keyname><forenames>Totok R.</forenames></author><author><keyname>Suganda</keyname><forenames>Sumitra Wira</forenames></author><author><keyname>Matraji</keyname></author><author><keyname>Susatio</keyname><forenames>Yerry</forenames></author><author><keyname>Justiono</keyname><forenames>Heri</forenames></author><author><keyname>Sarwono</keyname></author></authors><title>Cleaning Schedule Optimization of Heat Exchanger Networks Using Particle
  Swarm Optimization</title><categories>cs.NE</categories><comments>8 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Oil refinery is one of industries that require huge energy consumption. The
today technology advance requires energy saving. Heat integration is a method
used to minimize the energy comsumption though the implementation of Heat
Exchanger Network (HEN). CPT is one of types of Heat Exchanger Network (HEN)
that functions to recover the heat in the flow of product or waste. HEN
comprises a number of heat exchangers (HEs) that are serially connected.
However, the presence of fouling in the heat exchanger has caused the decline
of the performance of both heat exchangers and all heat exchanger networks.
Fouling can not be avoided. However, it can be mitigated. In industry, periodic
heat exchanger cleaning is the most effective and widely used mitigation
technique. On the other side, a very frequent cleaning of heat exchanger can be
much costly in maintenance and lost of production. In this way, an accurate
optimization technique of cleaning schedule interval of heat exchanger is very
essential. Commonly, this technique involves three elements: model to simulate
the heat exchanger network, representative fouling model to describe the
fouling behavior and suitable optimization algorithm to solve the problem of
clening schedule interval for heat exchanger network. This paper describe the
optimization of interval cleaning schedule of HEN within the 44-month period
using PSO (particle swarm optimization). The number of iteration used to
achieve the convergent is 100 iterations and the fitness value in PSO
correlated with the amount of heat recovery, cleaning cost, and additional
pumping cost. The saving after the optimization of cleaning schedule of HEN in
this research achieved at $ 1.236 millions or 23% of maximum potential savings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00894</identifier>
 <datestamp>2015-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00894</id><created>2015-12-02</created><authors><author><keyname>Yucesoy</keyname><forenames>Burcu</forenames></author><author><keyname>Barab&#xe1;si</keyname><forenames>Albert-L&#xe1;szl&#xf3;</forenames></author></authors><title>Untangling Performance from Success</title><categories>physics.soc-ph cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fame, popularity and celebrity status, frequently used tokens of success, are
often loosely related to, or even divorced from professional performance. This
dichotomy is partly rooted in the difficulty to distinguish performance, an
individual measure that captures the actions of a performer, from success, a
collective measure that captures a community's reactions to these actions. Yet,
finding the relationship between the two measures is essential for all areas
that aim to objectively reward excellence, from science to business. Here we
quantify the relationship between performance and success by focusing on
tennis, an individual sport where the two quantities can be independently
measured. We show that a predictive model, relying only on a tennis player's
performance in tournaments, can accurately predict an athlete's popularity,
both during a player's active years and after retirement. Hence the model
establishes a direct link between performance and momentary popularity. The
agreement between the performance-driven and observed popularity suggests that
in most areas of human achievement exceptional visibility may be rooted in
detectable performance measures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00901</identifier>
 <datestamp>2015-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00901</id><created>2015-12-02</created><authors><author><keyname>Yang</keyname><forenames>Mingrui</forenames></author><author><keyname>de Hoog</keyname><forenames>Frank</forenames></author><author><keyname>Fan</keyname><forenames>Yuqi</forenames></author><author><keyname>Hu</keyname><forenames>Wen</forenames></author></authors><title>Compressive hyperspectral imaging via adaptive sampling and dictionary
  learning</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a new sampling strategy for hyperspectral signals
that is based on dictionary learning and singular value decomposition (SVD).
Specifically, we first learn a sparsifying dictionary from training spectral
data using dictionary learning. We then perform an SVD on the dictionary and
use the first few left singular vectors as the rows of the measurement matrix
to obtain the compressive measurements for reconstruction. The proposed method
provides significant improvement over the conventional compressive sensing
approaches. The reconstruction performance is further improved by
reconditioning the sensing matrix using matrix balancing. We also demonstrate
that the combination of dictionary learning and SVD is robust by applying them
to different datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00907</identifier>
 <datestamp>2015-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00907</id><created>2015-12-02</created><authors><author><keyname>Rahmani</keyname><forenames>Mostafa</forenames></author><author><keyname>Atia</keyname><forenames>George</forenames></author></authors><title>Innovation Pursuit: A New Approach to Subspace Clustering</title><categories>cs.CV cs.IR cs.IT cs.LG math.IT stat.ML</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In subspace clustering, a group of data points belonging to a union of
subspaces are assigned membership to their respective subspaces. This paper
presents a new approach dubbed Innovation Pursuit (iPursuit) to the problem of
subspace clustering using a new geometrical idea whereby each subspace is
identified based on its novelty with respect to the other subspaces. The
proposed approach finds the subspaces consecutively by solving a series of
simple linear optimization problems, each searching for some direction in the
span of the data that is potentially orthogonal to all subspaces except for the
one to be identified in one step of the algorithm. A detailed mathematical
analysis is provided establishing sufficient conditions for the proposed
approach to correctly cluster the data points. Remarkably, the proposed
approach can provably yield exact clustering even when the subspaces have
significant intersections under mild conditions on the distribution of the data
points in the subspaces. Moreover, It is shown that the complexity of iPursuit
is almost independent of the dimension of the data. The numerical simulations
demonstrate that iPursuit can often outperform the state-of-the-art subspace
clustering algorithms, more so for subspaces with significant intersections.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00911</identifier>
 <datestamp>2015-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00911</id><created>2015-12-02</created><authors><author><keyname>Olsen</keyname><forenames>Eric B.</forenames></author></authors><title>Introduction of the Residue Number Arithmetic Logic Unit With Brief
  Computational Complexity Analysis</title><categories>cs.OH</categories><comments>20 pages, 2 figures, 5 tables, 3 graphs</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Digital System Research has pioneered the mathematics and design for a new
class of computing machine using residue numbers. Unlike prior art, the new
breakthrough provides methods and apparatus for general purpose computation
using several new residue based fractional representations. The result is that
fractional arithmetic may be performed without carry. Additionally, fractional
operations such as addition, subtraction and multiplication of a fraction by an
integer occur in a single clock period, regardless of word size. Fractional
multiplication is of the order O(p), where p equals the number of residues.
More significantly, complex operations, such as sum of products, may be
performed in an extended format, where fractional products are performed and
summed using single clock instructions, regardless of word width, and where a
normalization operation with an execution time of the order O(p) is performed
as a final step.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00932</identifier>
 <datestamp>2015-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00932</id><created>2015-12-02</created><authors><author><keyname>Happy</keyname><forenames>S L</forenames></author><author><keyname>Patnaik</keyname><forenames>Priyadarshi</forenames></author><author><keyname>Routray</keyname><forenames>Aurobinda</forenames></author><author><keyname>Guha</keyname><forenames>Rajlakshmi</forenames></author></authors><title>The Indian Spontaneous Expression Database for Emotion Recognition</title><categories>cs.CV</categories><comments>in IEEE Transactions on Affective Computing, 2016</comments><doi>10.1109/TAFFC.2015.2498174</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatic recognition of spontaneous facial expressions is a major challenge
in the field of affective computing. Head rotation, face pose, illumination
variation, occlusion etc. are the attributes that increase the complexity of
recognition of spontaneous expressions in practical applications. Effective
recognition of expressions depends significantly on the quality of the database
used. Most well-known facial expression databases consist of posed expressions.
However, currently there is a huge demand for spontaneous expression databases
for the pragmatic implementation of the facial expression recognition
algorithms. In this paper, we propose and establish a new facial expression
database containing spontaneous expressions of both male and female
participants of Indian origin. The database consists of 428 segmented video
clips of the spontaneous facial expressions of 50 participants. In our
experiment, emotions were induced among the participants by using emotional
videos and simultaneously their self-ratings were collected for each
experienced emotion. Facial expression clips were annotated carefully by four
trained decoders, which were further validated by the nature of stimuli used
and self-report of emotions. An extensive analysis was carried out on the
database using several machine learning algorithms and the results are provided
for future reference. Such a spontaneous database will help in the development
and validation of algorithms for recognition of spontaneous expressions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00933</identifier>
 <datestamp>2015-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00933</id><created>2015-12-02</created><authors><author><keyname>Briol</keyname><forenames>Fran&#xe7;ois-Xavier</forenames></author><author><keyname>Oates</keyname><forenames>Chris. J.</forenames></author><author><keyname>Girolami</keyname><forenames>Mark</forenames></author><author><keyname>Osborne</keyname><forenames>Michael A.</forenames></author><author><keyname>Sejdinovic</keyname><forenames>Dino</forenames></author></authors><title>Probabilistic Integration</title><categories>stat.ML cs.NA math.NA math.ST stat.CO stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Probabilistic numerical methods aim to model numerical error as a source of
epistemic uncertainty that is subject to probabilistic analysis and reasoning,
enabling the principled propagation of numerical uncertainty through a
computational pipeline. In this paper we focus on numerical methods for
integration. We present probabilistic (Bayesian) versions of both Markov chain
and Quasi Monte Carlo methods for integration and provide rigorous theoretical
guarantees for convergence rates, in both posterior mean and posterior
contraction. The performance of probabilistic integrators is guaranteed to be
no worse than non-probabilistic integrators and is, in many cases,
asymptotically superior. These probabilistic integrators therefore enjoy the
&quot;best of both worlds&quot;, leveraging the sampling efficiency of advanced Monte
Carlo methods whilst being equipped with valid probabilistic models for
uncertainty quantification. Several applications and illustrations are
provided, including examples from computer vision and system modelling using
non-linear differential equations. A survey of open challenges in probabilistic
integration is provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00939</identifier>
 <datestamp>2015-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00939</id><created>2015-12-02</created><authors><author><keyname>Choubey</keyname><forenames>Siddharth</forenames></author><author><keyname>Banchhor</keyname><forenames>Deepika</forenames></author></authors><title>A Literature Survey of various Fingerprint De-noising Techniques to
  justify the need of a new De-noising model based upon Pixel Component
  Analysis</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image Preprocessing is a vital step in the field of image processing for
biometric pattern recognition. This paper studies and reviews various classical
and modern fingerprint image de-noising models. The various model used for
de-noising ranges widely from transform matrix using frequency, histogram model
de-noising, de-noising by introducing Gabor filter and its types to enhance
fingerprint images.
  The output efficiency of various de-noising model proposed earlier is
calculated on the basis of SNR (signal to noise ratio) and MSE (mean square
error rate). Our simulated experimental results indicates that incorporating
the de-noising model based on Gabor filter inside domain of wavelet ranges with
composite method only betters MSE (Mean Square Error). Improved MSE without
significant improvement in SNR improves the fingerprint images only by a little
margin which is non-optimal in nature. Thus the objective of this research
paper is to build an optimal de-noising model for fingerprint images so that
its usage in biometric authentication can be more robust in nature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00961</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00961</id><created>2015-12-03</created><updated>2015-12-29</updated><authors><author><keyname>Gopalakrishnan</keyname><forenames>Roshan</forenames></author><author><keyname>Basu</keyname><forenames>Arindam</forenames></author></authors><title>Triplet Spike Time Dependent Plasticity: A floating-gate Implementation</title><categories>cs.NE</categories><comments>13 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Synapse plays an important role of learning in a neural network; the learning
rules which modify the synaptic strength based on the timing difference between
the pre- and post-synaptic spike occurrence is termed as Spike Time Dependent
Plasticity (STDP). The most commonly used rule posits weight change based on
time difference between one pre- and one post spike and is hence termed doublet
STDP (DSTDP). However, D-STDP could not reproduce results of many biological
experiments; a triplet STDP (T-STDP) that considers triplets of spikes as the
fundamental unit has been proposed recently to explain these observations. This
paper describes the compact implementation of a synapse using single
floating-gate (FG) transistor that can store a weight in a nonvolatile manner
and demonstrate the triplet STDP (T-STDP) learning rule by modifying drain
voltages according to triplets of spikes. We describe a mathematical procedure
to obtain control voltages for the FG device for T-STDP and also show
measurement results from a FG synapse fabricated in TSMC 0.35um CMOS process to
support the theory. Possible VLSI implementation of drain voltage waveform
generator circuits are also presented with simulation results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00964</identifier>
 <datestamp>2015-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00964</id><created>2015-12-03</created><authors><author><keyname>Nakahashi</keyname><forenames>Ryo</forenames></author><author><keyname>Baker</keyname><forenames>Chris L.</forenames></author><author><keyname>Tenenbaum</keyname><forenames>Joshua B.</forenames></author></authors><title>Modeling Human Understanding of Complex Intentional Action with a
  Bayesian Nonparametric Subgoal Model</title><categories>cs.AI</categories><comments>Accepted at AAAI 16</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most human behaviors consist of multiple parts, steps, or subtasks. These
structures guide our action planning and execution, but when we observe others,
the latent structure of their actions is typically unobservable, and must be
inferred in order to learn new skills by demonstration, or to assist others in
completing their tasks. For example, an assistant who has learned the subgoal
structure of a colleague's task can more rapidly recognize and support their
actions as they unfold. Here we model how humans infer subgoals from
observations of complex action sequences using a nonparametric Bayesian model,
which assumes that observed actions are generated by approximately rational
planning over unknown subgoal sequences. We test this model with a behavioral
experiment in which humans observed different series of goal-directed actions,
and inferred both the number and composition of the subgoal sequences
associated with each goal. The Bayesian model predicts human subgoal inferences
with high accuracy, and significantly better than several alternative models
and straightforward heuristics. Motivated by this result, we simulate how
learning and inference of subgoals can improve performance in an artificial
user assistance task. The Bayesian model learns the correct subgoals from fewer
observations, and better assists users by more rapidly and accurately inferring
the goal of their actions than alternative approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00965</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00965</id><created>2015-12-03</created><updated>2016-01-20</updated><authors><author><keyname>Yin</keyname><forenames>Pengcheng</forenames></author><author><keyname>Lu</keyname><forenames>Zhengdong</forenames></author><author><keyname>Li</keyname><forenames>Hang</forenames></author><author><keyname>Kao</keyname><forenames>Ben</forenames></author></authors><title>Neural Enquirer: Learning to Query Tables with Natural Language</title><categories>cs.AI cs.CL cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We proposed Neural Enquirer as a neural network architecture to execute a
natural language (NL) query on a knowledge-base (KB) for answers. Basically,
Neural Enquirer finds the distributed representation of a query and then
executes it on knowledge-base tables to obtain the answer as one of the values
in the tables. Unlike similar efforts in end-to-end training of semantic
parsers, Neural Enquirer is fully &quot;neuralized&quot;: it not only gives
distributional representation of the query and the knowledge-base, but also
realizes the execution of compositional queries as a series of differentiable
operations, with intermediate results (consisting of annotations of the tables
at different levels) saved on multiple layers of memory. Neural Enquirer can be
trained with gradient descent, with which not only the parameters of the
controlling components and semantic parsing component, but also the embeddings
of the tables and query words can be learned from scratch. The training can be
done in an end-to-end fashion, but it can take stronger guidance, e.g., the
step-by-step supervision for complicated queries, and benefit from it. Neural
Enquirer is one step towards building neural network systems which seek to
understand language by executing it on real-world. Our experiments show that
Neural Enquirer can learn to execute fairly complicated NL queries on tables
with rich structures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00971</identifier>
 <datestamp>2015-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00971</id><created>2015-12-03</created><authors><author><keyname>Rayguru</keyname><forenames>Madan Mohan</forenames></author><author><keyname>Kar</keyname><forenames>I N</forenames></author></authors><title>Contraction based stabilization of nonlinear singularly perturbed
  systems and application to high gain feedback</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent development of contraction theory based analysis of singularly
perturbed system has opened the door for inspecting differential behavior of
multi time-scale systems. In this paper a contraction theory based framework is
proposed for stabilization of singularly perturbed systems. The primary
objective is to design a feedback controller to achieve bounded tracking error
for both standard and non-standard singularly perturbed systems. This framework
provides relaxation over traditional quadratic Lyapunov based method as there
is no need to satisfy interconnection conditions during controller design
algorithm. Moreover, the stability bound does not depend on smallness of
singularly perturbed parameter. Combined with high gain scaling, the proposed
technique is shown to assure contraction of approximate feedback linearizable
systems. These findings extend the class of nonlinear systems which can be made
contracting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00977</identifier>
 <datestamp>2015-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00977</id><created>2015-12-03</created><authors><author><keyname>Liu</keyname><forenames>Feng</forenames></author><author><keyname>Shi</keyname><forenames>Yong</forenames></author></authors><title>A Study on Artificial Intelligence IQ and Standard Intelligent Model</title><categories>cs.AI</categories><comments>16 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Currently, potential threats of artificial intelligence (AI) to human have
triggered a large controversy in society, behind which, the nature of the issue
is whether the artificial intelligence (AI) system can be evaluated
quantitatively. This article analyzes and evaluates the challenges that the AI
development level is facing, and proposes that the evaluation methods for the
human intelligence test and the AI system are not uniform; and the key reason
for which is that none of the models can uniformly describe the AI system and
the beings like human. Aiming at this problem, a standard intelligent system
model is established in this study to describe the AI system and the beings
like human uniformly. Based on the model, the article makes an abstract
mathematical description, and builds the standard intelligent machine
mathematical model; expands the Von Neumann architecture and proposes the
Liufeng - Shiyong architecture; gives the definition of the artificial
intelligence IQ, and establishes the artificial intelligence scale and the
evaluation method; conduct the test on 50 search engines and three human
subjects at different ages across the world, and finally obtains the ranking of
the absolute IQ and deviation IQ ranking for artificial intelligence IQ 2014.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00984</identifier>
 <datestamp>2015-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00984</id><created>2015-12-03</created><authors><author><keyname>Yao</keyname><forenames>Quanming</forenames></author><author><keyname>Kwok</keyname><forenames>James T.</forenames></author><author><keyname>Zhong</keyname><forenames>Wenliang</forenames></author></authors><title>Fast Low-Rank Matrix Learning with Nonconvex Regularization</title><categories>cs.NA cs.LG</categories><comments>Long version of conference paper appeared ICDM 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Low-rank modeling has a lot of important applications in machine learning,
computer vision and social network analysis. While the matrix rank is often
approximated by the convex nuclear norm, the use of nonconvex low-rank
regularizers has demonstrated better recovery performance. However, the
resultant optimization problem is much more challenging. A very recent
state-of-the-art is based on the proximal gradient algorithm. However, it
requires an expensive full SVD in each proximal step. In this paper, we show
that for many commonly-used nonconvex low-rank regularizers, a cutoff can be
derived to automatically threshold the singular values obtained from the
proximal operator. This allows the use of power method to approximate the SVD
efficiently. Besides, the proximal operator can be reduced to that of a much
smaller matrix projected onto this leading subspace. Convergence, with a rate
of O(1/T) where T is the number of iterations, can be guaranteed. Extensive
experiments are performed on matrix completion and robust principal component
analysis. The proposed method achieves significant speedup over the
state-of-the-art. Moreover, the matrix solution obtained is more accurate and
has a lower rank than that of the traditional nuclear norm regularizer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.00994</identifier>
 <datestamp>2015-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.00994</id><created>2015-12-03</created><authors><author><keyname>Song</keyname><forenames>Hanqiang</forenames></author><author><keyname>Zhu</keyname><forenames>Zhuotun</forenames></author><author><keyname>Wang</keyname><forenames>Xinggang</forenames></author></authors><title>Bag Reference Vector for Multi-instance Learning</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-instance learning (MIL) has a wide range of applications due to its
distinctive characteristics. Although many state-of-the-art algorithms have
achieved decent performances, a plurality of existing methods solve the problem
only in instance level rather than excavating relations among bags. In this
paper, we propose an efficient algorithm to describe each bag by a
corresponding feature vector via comparing it with other bags. In other words,
the crucial information of a bag is extracted from the similarity between that
bag and other reference bags. In addition, we apply extensions of Hausdorff
distance to representing the similarity, to a certain extent, overcoming the
key challenge of MIL problem, the ambiguity of instances' labels in positive
bags. Experimental results on benchmarks and text categorization tasks show
that the proposed method outperforms the previous state-of-the-art by a large
margin.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01003</identifier>
 <datestamp>2015-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01003</id><created>2015-12-03</created><authors><author><keyname>Xie</keyname><forenames>Yuan</forenames></author><author><keyname>Gu</keyname><forenames>Shuhang</forenames></author><author><keyname>Liu</keyname><forenames>Yan</forenames></author><author><keyname>Zuo</keyname><forenames>Wangmeng</forenames></author><author><keyname>Zhang</keyname><forenames>Wensheng</forenames></author><author><keyname>Zhang</keyname><forenames>Lei</forenames></author></authors><title>Weighted Schatten $p$-Norm Minimization for Image Denoising and
  Background Subtraction</title><categories>cs.CV</categories><comments>13 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Low rank matrix approximation (LRMA), which aims to recover the underlying
low rank matrix from its degraded observation, has a wide range of applications
in computer vision. The latest LRMA methods resort to using the nuclear norm
minimization (NNM) as a convex relaxation of the nonconvex rank minimization.
However, NNM tends to over-shrink the rank components and treats the different
rank components equally, limiting its flexibility in practical applications. We
propose a more flexible model, namely the Weighted Schatten $p$-Norm
Minimization (WSNM), to generalize the NNM to the Schatten $p$-norm
minimization with weights assigned to different singular values. The proposed
WSNM not only gives better approximation to the original low-rank assumption,
but also considers the importance of different rank components. We analyze the
solution of WSNM and prove that, under certain weights permutation, WSNM can be
equivalently transformed into independent non-convex $l_p$-norm subproblems,
whose global optimum can be efficiently solved by generalized iterated
shrinkage algorithm. We apply WSNM to typical low-level vision problems, e.g.,
image denoising and background subtraction. Extensive experimental results
show, both qualitatively and quantitatively, that the proposed WSNM can more
effectively remove noise, and model complex and dynamic scenes compared with
state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01017</identifier>
 <datestamp>2015-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01017</id><created>2015-12-03</created><authors><author><keyname>Stotz</keyname><forenames>David</forenames></author><author><keyname>Riegler</keyname><forenames>Erwin</forenames></author><author><keyname>Agustsson</keyname><forenames>Eirikur</forenames></author><author><keyname>B&#xf6;lcskei</keyname><forenames>Helmut</forenames></author></authors><title>Almost lossless analog signal separation and probabilistic uncertainty
  relations</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Trans. on Inf. Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an information-theoretic framework for analog signal separation.
Specifically, we consider the problem of recovering two analog signals, modeled
as general random vectors, from the noiseless sum of linear measurements of the
signals. Our framework is inspired by the groundbreaking work of Wu and Verd\'u
(2010) on analog compression and encompasses, inter alia, inpainting,
declipping, superresolution, the recovery of signals corrupted by impulse
noise, and the separation of (e.g., audio or video) signals into two distinct
components. The main results we report are general achievability bounds for the
compression rate, i.e., the number of measurements relative to the dimension of
the ambient space the signals live in, under either measurability or H\&quot;older
continuity imposed on the separator. Furthermore, we find a matching converse
for sources of mixed discrete-continuous distribution. For measurable
separators our proofs are based on a new probabilistic uncertainty relation
which shows that the intersection of generic subspaces with general subsets of
sufficiently small Minkowski dimension is empty. H\&quot;older continuous separators
are dealt with by introducing the concept of regularized probabilistic
uncertainty relations. The probabilistic uncertainty relations we develop are
inspired by embedding results in dynamical systems theory due to Sauer et al.
(1991) and--conceptually--parallel classical Donoho-Stark and Elad-Bruckstein
uncertainty principles at the heart of compressed sensing theory.
Operationally, the new uncertainty relations take the theory of sparse signal
separation beyond traditional sparsity--as measured in terms of the number of
non-zero entries--to the more general notion of low description complexity as
quantified by Minkowski dimension. Finally, our approach also allows to
significantly strengthen key results in Wu and Verd\'u (2010).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01027</identifier>
 <datestamp>2015-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01027</id><created>2015-12-03</created><authors><author><keyname>Hamze</keyname><forenames>Firas</forenames></author><author><keyname>Andryash</keyname><forenames>Evgeny</forenames></author></authors><title>Discrete Equilibrium Sampling with Arbitrary Nonequilibrium Processes</title><categories>stat.CO cond-mat.dis-nn cond-mat.stat-mech cs.AI physics.comp-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel framework for performing statistical sampling, expectation
estimation, and partition function approximation using \emph{arbitrary}
heuristic stochastic processes defined over discrete state spaces. Using a
highly parallel construction we call the \emph{sequential constraining
process}, we are able to simultaneously generate states with the heuristic
process and accurately estimate their probabilities, even when they are far too
small to be realistically inferred by direct counting. After showing that both
theoretically correct importance sampling and Markov chain Monte Carlo are
possible using the sequential constraining process, we integrate it into a
methodology called \emph{state space sampling}, extending the ideas of state
space search from computer science to the sampling context. The methodology
comprises a dynamic data structure that constructs a robust Bayesian model of
the statistics generated by the heuristic process subject to an accuracy
constraint, the posterior Kullback-Leibler divergence. Sampling from the
dynamic structure will generally yield partial states, which are completed by
recursively calling the heuristic to refine the structure and resuming the
sampling. Our experiments on various Ising models suggest that state space
sampling enables heuristic state generation with accurate probability
estimates, demonstrated by illustrating the convergence of a simulated
annealing process to the Boltzmann distribution with increasing run length.
Consequently, heretofore unprecedented direct importance sampling using the
\emph{final} (marginal) distribution of a generic stochastic process is
allowed, potentially augmenting the range of algorithms at the Monte Carlo
practitioner's disposal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01030</identifier>
 <datestamp>2015-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01030</id><created>2015-12-03</created><authors><author><keyname>Veeravasarapu</keyname><forenames>V S R</forenames></author><author><keyname>Hota</keyname><forenames>Rudra Narayan</forenames></author><author><keyname>Rothkopf</keyname><forenames>Constantin</forenames></author><author><keyname>Visvanathan</keyname><forenames>Ramesh</forenames></author></authors><title>Simulations for Validation of Vision Systems</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As the computer vision matures into a systems science and engineering
discipline, there is a trend in leveraging latest advances in computer graphics
simulations for performance evaluation, learning, and inference. However, there
is an open question on the utility of graphics simulations for vision with
apparently contradicting views in the literature. In this paper, we place the
results from the recent literature in the context of performance
characterization methodology outlined in the 90's and note that insights
derived from simulations can be qualitative or quantitative depending on the
degree of fidelity of models used in simulation and the nature of the question
posed by the experimenter. We describe a simulation platform that incorporates
latest graphics advances and use it for systematic performance characterization
and trade-off analysis for vision system design. We verify the utility of the
platform in a case study of validating a generative model inspired vision
hypothesis, Rank-Order consistency model, in the contexts of global and local
illumination changes, and bad weather, and high-frequency noise. Our approach
establishes the link between alternative viewpoints, involving models with
physics based semantics and signal and perturbation semantics and confirms
insights in literature on robust change detection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01039</identifier>
 <datestamp>2015-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01039</id><created>2015-12-03</created><authors><author><keyname>Namvar</keyname><forenames>Nima</forenames></author><author><keyname>Saad</keyname><forenames>Walid</forenames></author><author><keyname>Maham</keyname><forenames>Behrouz</forenames></author><author><keyname>Valentin</keyname><forenames>Stefan</forenames></author></authors><title>A context-aware matching game for user association in wireless small
  cell networks</title><categories>cs.NI</categories><comments>2014 IEEE International Conference on Acoustics, Speech and Signal
  Processing (ICASSP), vol., no., pp.439-443, 4-9 May 2014</comments><journal-ref>Acoustics, Speech and Signal Processing (ICASSP), 2014 IEEE
  International Conference on, pp.439-443</journal-ref><doi>10.1109/ICASSP.2014.6853634</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Small cell networks are seen as a promising technology for boosting the
performance of future wireless networks. In this paper, we propose a novel
context-aware user-cell association approach for small cell networks that
exploits the information about the velocity and trajectory of the users while
also taking into account their quality of service (QoS) requirements. We
formulate the problem in the framework of matching theory with externalities in
which the agents, namely users and small cell base stations (SCBSs), have
strict interdependent preferences over the members of the opposite set. To
solve the problem, we propose a novel algorithm that leads to a stable matching
among the users and SCBSs. We show that the proposed approach can better
balance the traffic among the cells while also satisfying the QoS of the users.
Simulation results show that the proposed matching algorithm yields significant
performance advantages relative to traditional context-unaware approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01041</identifier>
 <datestamp>2015-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01041</id><created>2015-12-03</created><authors><author><keyname>Aguzzoli</keyname><forenames>Stefano</forenames></author><author><keyname>Codara</keyname><forenames>Pietro</forenames></author><author><keyname>Flaminio</keyname><forenames>Tommaso</forenames></author><author><keyname>Gerla</keyname><forenames>Brunella</forenames></author><author><keyname>Valota</keyname><forenames>Diego</forenames></author></authors><title>Querying with {\L}ukasiewicz logic</title><categories>cs.LO cs.AI cs.DB math.LO</categories><journal-ref>2015 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE),
  pp.1-8</journal-ref><doi>10.1109/FUZZ-IEEE.2015.7338061</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present, by way of case studies, a proof of concept, based
on a prototype working on a automotive data set, aimed at showing the potential
usefulness of using formulas of {\L}ukasiewicz propositional logic to query
databases in a fuzzy way. Our approach distinguishes itself for its stress on
the purely linguistic, contraposed with numeric, formulations of queries. Our
queries are expressed in the pure language of logic, and when we use (integer)
numbers, these stand for shortenings of formulas on the syntactic level, and
serve as linguistic hedges on the semantic one. Our case-study queries aim
first at showing that each numeric-threshold fuzzy query is simulated by a
{\L}ukasiewicz formula. Then they focus on the expressing power of
{\L}ukasiewicz logic which easily allows for updating queries by clauses and
for modifying them through a potentially infinite variety of linguistic hedges
implemented with a uniform syntactic mechanism. Finally we shall hint how,
already at propositional level, {\L}ukasiewicz natural semantics enjoys a
degree of reflection, allowing to write syntactically simple queries that
semantically work as meta-queries weighing the contribution of simpler ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01043</identifier>
 <datestamp>2015-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01043</id><created>2015-12-03</created><authors><author><keyname>Thakkar</keyname><forenames>Harsh</forenames></author><author><keyname>Patel</keyname><forenames>Dhiren</forenames></author></authors><title>Approaches for Sentiment Analysis on Twitter: A State-of-Art study</title><categories>cs.SI cs.CL cs.IR</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Microbloging is an extremely prevalent broadcast medium amidst the Internet
fraternity these days. People share their opinions and sentiments about variety
of subjects like products, news, institutions, etc., every day on microbloging
websites. Sentiment analysis plays a key role in prediction systems, opinion
mining systems, etc. Twitter, one of the microbloging platforms allows a limit
of 140 characters to its users. This restriction stimulates users to be very
concise about their opinion and twitter an ocean of sentiments to analyze.
Twitter also provides developer friendly streaming API for data retrieval
purpose allowing the analyst to search real time tweets from various users. In
this paper, we discuss the state-of-art of the works which are focused on
Twitter, the online social network platform, for sentiment analysis. We survey
various lexical, machine learning and hybrid approaches for sentiment analysis
on Twitter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01055</identifier>
 <datestamp>2015-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01055</id><created>2015-12-03</created><authors><author><keyname>Radwan</keyname><forenames>Ibrahim</forenames></author><author><keyname>Dhall</keyname><forenames>Abhinav</forenames></author><author><keyname>Goecke</keyname><forenames>Roland</forenames></author></authors><title>Occlusion-Aware Human Pose Estimation with Mixtures of Sub-Trees</title><categories>cs.CV</categories><comments>12 pages, 5 figures and 3 Tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the problem of learning a model for human pose
estimation as mixtures of compositional sub-trees in two layers of prediction.
This involves estimating the pose of a sub-tree followed by identifying the
relationships between sub-trees that are used to handle occlusions between
different parts. The mixtures of the sub-trees are learnt utilising both
geometric and appearance distances. The Chow-Liu (CL) algorithm is recursively
applied to determine the inter-relations between the nodes and to build the
structure of the sub-trees. These structures are used to learn the latent
parameters of the sub-trees and the inference is done using a standard belief
propagation technique. The proposed method handles occlusions during the
inference process by identifying overlapping regions between different
sub-trees and introducing a penalty term for overlapping parts. Experiments are
performed on three different datasets: the Leeds Sports, Image Parse and UIUC
People datasets. The results show the robustness of the proposed method to
occlusions over the state-of-the-art approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01058</identifier>
 <datestamp>2015-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01058</id><created>2015-12-03</created><authors><author><keyname>Brock</keyname><forenames>Anke</forenames><affiliation>Potioc</affiliation></author><author><keyname>Jouffrais</keyname><forenames>Christophe</forenames><affiliation>CNRS, IRIT</affiliation></author></authors><title>Interactive audio-tactile maps for visually impaired people</title><categories>cs.HC</categories><comments>\&amp;lt;http://dl.acm.org/citation.cfm?id=J956\&amp;CFID=730680571\&amp;CFTOKEN=17044974\&amp;gt;.
  \&amp;lt;10.1145/2850440.2850441\&amp;gt</comments><proxy>ccsd</proxy><journal-ref>ACM SIGACCESS Accessibility and Computing (ACM Digital Library),
  Association for Computing Machinery (ACM), 2015, pp.3-12.</journal-ref><doi>10.1145/2850440.2850441</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Visually impaired people face important challenges related to orientation and
mobility. Indeed, 56% of visually impaired people in France declared having
problems concerning autonomous mobility. These problems often mean that
visually impaired people travel less, which influences their personal and
professional life and can lead to exclusion from society. Therefore this issue
presents a social challenge as well as an important research area. Accessible
geographic maps are helpful for acquiring knowledge about a city's or
neighborhood's configuration, as well as selecting a route to reach a
destination. Traditionally, raised-line paper maps with braille text have been
used. These maps have proved to be efficient for the acquisition of spatial
knowledge by visually impaired people. Yet, these maps possess significant
limitations. For instance, due to the specificities of the tactile sense only a
limited amount of information can be displayed on a single map, which
dramatically increases the number of maps that are needed. For the same reason,
it is difficult to represent specific information such as distances. Finally,
braille labels are used for textual descriptions but only a small percentage of
the visually impaired population reads braille. In France 15% of blind people
are braille readers and only 10% can read and write. In the United States,
fewer than 10% of the legally blind people are braille readers and only 10% of
blind children actually learn braille. Recent technological advances have
enabled the design of interactive maps with the aim to overcome these
limitations. Indeed, interactive maps have the potential to provide a broad
spectrum of the population with spatial knowledge, irrespective of age,
impairment, skill level, or other factors. To this regard, they might be an
efficient means for providing visually impaired people with access to
geospatial information. In this paper we give an overview of our research on
making geographic maps accessible to visually impaired people.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01085</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01085</id><created>2015-12-03</created><updated>2015-12-08</updated><authors><author><keyname>Barton</keyname><forenames>Carl</forenames></author><author><keyname>Liu</keyname><forenames>Chang</forenames></author><author><keyname>Pissis</keyname><forenames>Solon P.</forenames></author></authors><title>Fast Average-Case Pattern Matching on Weighted Sequences</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A weighted string over an alphabet of size $\sigma$ is a string in which a
set of letters may occur at each position with respective occurrence
probabilities. Weighted strings, also known as position weight matrices or
uncertain sequences, naturally arise in many contexts. In this article, we
study the problem of weighted string matching with a special focus on
average-case analysis. Given a weighted pattern string $x$ of length $m$, a
text string $y$ of length $n&gt;m$, and a cumulative weight threshold $1/z$,
defined as the minimal probability of occurrence of factors in a weighted
string, we present an algorithm requiring average-case search time $o(n)$ for
pattern matching for weight ratio $\frac{z}{m} &lt; \min\{\frac{1}{\log
z},\frac{\log \sigma}{\log z (\log m + \log \log \sigma)}\}$. For a pattern
string $x$ of length $m$, a weighted text string $y$ of length $n&gt;m$, and a
cumulative weight threshold $1/z$, we present an algorithm requiring
average-case search time $o(\sigma n)$ for the same weight ratio. The
importance of these results lies on the fact that these algorithms work in
average-case sublinear search time in the size of the text, and in linear
preprocessing time and space in the size of the pattern, for these ratios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01088</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01088</id><created>2015-12-03</created><updated>2015-12-11</updated><authors><author><keyname>Kiani</keyname><forenames>Narsis A.</forenames></author><author><keyname>Zenil</keyname><forenames>Hector</forenames></author><author><keyname>Olczak</keyname><forenames>Jakub</forenames></author><author><keyname>Tegn&#xe9;r</keyname><forenames>Jesper</forenames></author></authors><title>Evaluating Network Inference Methods in Terms of Their Ability to
  Preserve the Topology and Complexity of Genetic Networks</title><categories>q-bio.MN cs.IT math.IT</categories><comments>main part: 18 pages. 21 pages with Sup Inf. Forthcoming in the
  journal of Seminars in Cell and Developmental Biology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network inference is a rapidly advancing field, with new methods being
proposed on a regular basis. Understanding the advantages and limitations of
different network inference methods is key to their effective application in
different circumstances. The common structural properties shared by diverse
networks naturally pose a challenge when it comes to devising accurate
inference methods, but surprisingly, there is a paucity of comparison and
evaluation methods. Historically, every new methodology has only been tested
against \textit{gold standard} (true values) purpose-designed synthetic and
real-world (validated) biological networks. In this paper we aim to assess the
impact of taking into consideration aspects of topological and information
content in the evaluation of the final accuracy of an inference procedure.
Specifically, we will compare the best inference methods, in both
graph-theoretic and information-theoretic terms, for preserving topological
properties and the original information content of synthetic and biological
networks. New methods for performance comparison are introduced by borrowing
ideas from gene set enrichment analysis and by applying concepts from
algorithmic complexity. Experimental results show that no individual algorithm
outperforms all others in all cases, and that the challenging and non-trivial
nature of network inference is evident in the struggle of some of the
algorithms to turn in a performance that is superior to random guesswork.
Therefore special care should be taken to suit the method to the purpose at
hand. Finally, we show that evaluations from data generated using different
underlying topologies have different signatures that can be used to better
choose a network reconstruction method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01090</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01090</id><created>2015-12-03</created><updated>2015-12-21</updated><authors><author><keyname>Zhou</keyname><forenames>Lin</forenames></author><author><keyname>Tan</keyname><forenames>Vincent Y. F.</forenames></author><author><keyname>Motani</keyname><forenames>Mehul</forenames></author></authors><title>Discrete Lossy Gray-Wyner Revisited: Second-Order Asymptotics, Large and
  Moderate Deviations</title><categories>cs.IT math.IT</categories><comments>32 pages, submitted to IEEE Transactions on Information Theory; Short
  version v_1 submitted to IEEE ISIT 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we revisit the discrete lossy Gray-Wyner problem. In
particular, we derive its optimal second-order coding rate region, its error
exponent (reliability function) and its moderate deviations constant under mild
conditions on the source. To obtain the second-order asymptotics, we extend
some ideas from Watanabe's work (2015). In particular, we leverage the
properties of an appropriate generalization of the conditional
distortion-tilted information density, which was first introduced by Kostina
and Verd\'u (2012). The converse part uses a perturbation argument by Gu and
Effros (2009) in their strong converse proof of the discrete Gray-Wyner
problem. The achievability part uses two novel elements: (i) a generalization
of various type covering lemmas; and (ii) the uniform continuity of the
conditional rate-distortion function in both the source (joint) distribution
and the distortion level. To obtain the error exponent, for the achievability
part, we use the same generalized type covering lemma and for the converse, we
use the strong converse together a change-of-measure technique. Finally, to
obtain the moderate deviations constant, we apply the moderate deviations
theorem to probabilities defined in terms of information spectrum quantities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01093</identifier>
 <datestamp>2015-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01093</id><created>2015-12-03</created><authors><author><keyname>Fonda</keyname><forenames>Carlo</forenames></author><author><keyname>Canessa</keyname><forenames>Enrique</forenames></author></authors><title>Making Ideas at Scientific Fabrication Laboratories</title><categories>physics.soc-ph cs.CY</categories><comments>8 pages, PDF</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Creativity, together with the making of ideas into fruition, is essential for
progress. Today the evolution from an idea to its application can be
facilitated by the implementation of Fabrication Laboratories, or FabLabs,
having affordable digital tools for prototyping. FabLabs aiming at scientific
research and invention are now starting to be established inside Universities
and Research Centers. We review the setting up of the ICTP Scientific FabLab in
Trieste, Italy and propose to replicate this class of multi-purpose workplaces
within academia as a support for science, education and development world-wide.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01100</identifier>
 <datestamp>2015-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01100</id><created>2015-12-03</created><authors><author><keyname>Tang</keyname><forenames>Duyu</forenames></author><author><keyname>Qin</keyname><forenames>Bing</forenames></author><author><keyname>Feng</keyname><forenames>Xiaocheng</forenames></author><author><keyname>Liu</keyname><forenames>Ting</forenames></author></authors><title>Target-Dependent Sentiment Classification with Long Short Term Memory</title><categories>cs.CL</categories><comments>7 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Target-dependent sentiment classification remains a challenge: modeling the
semantic relatedness of a target with its context words in a sentence.
Different context words have different influences on determining the sentiment
polarity of a sentence towards the target. Therefore, it is desirable to
integrate the connections between target word and context words when building a
learning system. In this paper, we develop two target dependent long short-term
memory (LSTM) models, where target information is automatically taken into
account. We evaluate our methods on a benchmark dataset from Twitter. Empirical
results show that modeling sentence representation with standard LSTM does not
perform well. Incorporating target information into LSTM can significantly
boost the classification accuracy. The target-dependent LSTM models achieve
state-of-the-art performances without using syntactic parser or external
sentiment lexicons.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01104</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01104</id><created>2015-12-03</created><updated>2016-01-28</updated><authors><author><keyname>Stavropoulos</keyname><forenames>Konstantinos</forenames></author></authors><title>On the Medianwidth of Graphs</title><categories>math.CO cs.DM</categories><comments>Corrected typos, improved introduction, added references, simplified
  proof of Thm 3.1 and fixed a gap in an earlier version of Thm 5.1</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A median graph is a connected graph, such that for any three vertices $u,v,w$
there is exactly one vertex $x$ that lies simultaneously on a shortest
$(u,v)$-path, a shortest $(v,w)$-path and a shortest $(w,u)$-path. Examples of
median graphs are trees and hypercubes.
  We introduce and study a generalisation of tree decompositions, to be called
median decompositions, where instead of decomposing a graph $G$ in a treelike
fashion, we use general median graphs as the underlying graph of the
decomposition. We show that the corresponding width parameter $\text{mw}(G)$,
the medianwidth of $G$, is equal to the clique number of the graph, while a
suitable variation of it is equal to the chromatic number of $G$.
  We study in detail the $i$-medianwidth $\text{mw}_i(G)$ of a graph, for which
we restrict the underlying median graph of a decomposition to be isometrically
embeddable to the Cartesian product of $i$ trees. For $i\geq 1$, the parameters
$\text{mw}_i$ constitute a hierarchy starting from treewidth and converging to
the clique number. We characterize the $i$-medianwidth of a graph to be,
roughly said, the largest &quot;intersection&quot; of the best choice of $i$ many tree
decompositions of the graph.
  Lastly, we extend the concept of tree and median decompositions and propose a
general framework of how to decompose a graph $G$ in any fixed graphlike
fashion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01110</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01110</id><created>2015-12-03</created><updated>2015-12-24</updated><authors><author><keyname>Song</keyname><forenames>Yang</forenames></author><author><keyname>Zhu</keyname><forenames>Jun</forenames></author></authors><title>Bayesian Matrix Completion via Adaptive Relaxed Spectral Regularization</title><categories>cs.NA cs.AI cs.LG</categories><comments>Accepted to AAAI 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bayesian matrix completion has been studied based on a low-rank matrix
factorization formulation with promising results. However, little work has been
done on Bayesian matrix completion based on the more direct spectral
regularization formulation. We fill this gap by presenting a novel Bayesian
matrix completion method based on spectral regularization. In order to
circumvent the difficulties of dealing with the orthonormality constraints of
singular vectors, we derive a new equivalent form with relaxed constraints,
which then leads us to design an adaptive version of spectral regularization
feasible for Bayesian inference. Our Bayesian method requires no parameter
tuning and can infer the number of latent factors automatically. Experiments on
synthetic and real datasets demonstrate encouraging results on rank recovery
and collaborative filtering, with notably good results for very sparse
matrices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01124</identifier>
 <datestamp>2015-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01124</id><created>2015-12-03</created><updated>2015-12-16</updated><authors><author><keyname>Sunehag</keyname><forenames>Peter</forenames></author><author><keyname>Evans</keyname><forenames>Richard</forenames></author><author><keyname>Dulac-Arnold</keyname><forenames>Gabriel</forenames></author><author><keyname>Zwols</keyname><forenames>Yori</forenames></author><author><keyname>Visentin</keyname><forenames>Daniel</forenames></author><author><keyname>Coppin</keyname><forenames>Ben</forenames></author></authors><title>Deep Reinforcement Learning with Attention for Slate Markov Decision
  Processes with High-Dimensional States and Actions</title><categories>cs.AI cs.HC cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many real-world problems come with action spaces represented as feature
vectors. Although high-dimensional control is a largely unsolved problem, there
has recently been progress for modest dimensionalities. Here we report on a
successful attempt at addressing problems of dimensionality as high as $2000$,
of a particular form. Motivated by important applications such as
recommendation systems that do not fit the standard reinforcement learning
frameworks, we introduce Slate Markov Decision Processes (slate-MDPs). A
Slate-MDP is an MDP with a combinatorial action space consisting of slates
(tuples) of primitive actions of which one is executed in an underlying MDP.
The agent does not control the choice of this executed action and the action
might not even be from the slate, e.g., for recommendation systems for which
all recommendations can be ignored. We use deep Q-learning based on feature
representations of both the state and action to learn the value of whole
slates. Unlike existing methods, we optimize for both the combinatorial and
sequential aspects of our tasks. The new agent's superiority over agents that
either ignore the combinatorial or sequential long-term value aspect is
demonstrated on a range of environments with dynamics from a real-world
recommendation system. Further, we use deep deterministic policy gradients to
learn a policy that for each position of the slate, guides attention towards
the part of the action space in which the value is the highest and we only
evaluate actions in this area. The attention is used within a sequentially
greedy procedure leveraging submodularity. Finally, we show how introducing
risk-seeking can dramatically improve the agents performance and ability to
discover more far reaching strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01129</identifier>
 <datestamp>2015-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01129</id><created>2015-12-03</created><authors><author><keyname>Garcia-Dorado</keyname><forenames>Jose Luis</forenames></author></authors><title>Bandwidth in the Cloud</title><categories>cs.NI cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The seek for the best quality of service has led Cloud infrastructure clients
to disseminate their services, contents and data over multiple cloud
data-centers often involving several Cloud Service Providers (CSPs). The
consequence of this is that a large amount of bytes must be transmitted across
the public Cloud. However, very little is known about its bandwidth dynamics.
To address this, we have conducted a measurement campaign for bandwidth between
eighteen data-centers of four major CSPs. Such extensive campaign allowed us to
characterize the resulting time series of bandwidth as the addition of a
stationary component and some infrequent excursions (typically, downtimes).
While the former provides a description of the bandwidth users can expect in
the Cloud, the latter is closely related to the robustness of the Cloud (i.e.,
the occurrence of downtimes is correlated). Both components have been studied
further by applying a factor analysis, specifically ANOVA, as a mechanism to
formally compare data-centers' behaviors and extract generalities. The results
show that the stationary process is closely related to data-center locations
and CSPs involved in transfers, which fortunately makes both the Cloud more
predictable and the set of reported measurements extrapolate. On the other
hand, although the correlation in the Cloud is low, i.e., only 10% of the
measured pair of paths showed some correlation, we have found evidence that
such correlation depends on the particular relationships between pairs of
data-centers with little link to more general factors. Positively, this implies
that data-centers either at the same area or CSP do not show qualitatively more
correlation than others data-centers, which eases the deployment of robust
infrastructures. On the downside, this metric is barely generalizable and,
consequently, calls for exhaustive monitoring.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01132</identifier>
 <datestamp>2015-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01132</id><created>2015-12-03</created><authors><author><keyname>Zhang</keyname><forenames>Lei M.</forenames></author><author><keyname>Truhachev</keyname><forenames>Dmitri</forenames></author><author><keyname>Kschischang</keyname><forenames>Frank</forenames></author></authors><title>Spatially-coupled Split-component Codes with Iterative Algebraic
  Decoding</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze a class of high performance, low decoding-data-flow
error-correcting codes suitable for high bit-rate optical-fiber communication
systems. A spatially-coupled split-component ensemble is defined, generalizing
from the most important codes of this class, staircase codes and braided block
codes, and preserving a deterministic partitioning of component-code bits over
code blocks. Our analysis focuses on low-complexity iterative algebraic
decoding, which, for the binary erasure channel, is equivalent to a
generalization of the peeling decoder. Using the differential equation method,
we derive a vector recursion that tracks the expected residual graph evolution
throughout the decoding process. The threshold of the recursion is found using
potential function analysis. We generalize the analysis to mixture ensembles
consisting of more than one type of component code, which provide increased
flexibility of ensemble parameters and can improve performance. The analysis
extends to the binary symmetric channel by assuming mis-correction-free
component-code decoding. Simple upper-bounds on the number of errors
correctable by the ensemble are derived. Finally, we analyze the threshold of
spatially-coupled split-component ensembles under beyond bounded-distance
component decoding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01150</identifier>
 <datestamp>2015-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01150</id><created>2015-12-03</created><updated>2015-12-04</updated><authors><author><keyname>Froese</keyname><forenames>Vincent</forenames></author><author><keyname>van Bevern</keyname><forenames>Ren&#xe9;</forenames></author><author><keyname>Niedermeier</keyname><forenames>Rolf</forenames></author><author><keyname>Sorge</keyname><forenames>Manuel</forenames></author></authors><title>Exploiting Hidden Structure in Selecting Dimensions that Distinguish
  Vectors</title><categories>cs.DM cs.CC</categories><comments>Accepted for publication in Journal of Computer and System Sciences
  (Elsevier)</comments><msc-class>15B36, 06B99, 05D05, 68R05, 68Q17, 68Q32, 68P15</msc-class><acm-class>F.2.2; G.2.1; I.1.2; I.5.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The NP-hard Distinct Vectors problem asks to delete as many columns as
possible from a matrix such that all rows in the resulting matrix are still
pairwise distinct. Our main result is that, for binary matrices, there is a
complexity dichotomy for Distinct Vectors based on the maximum (H) and the
minimum (h) pairwise Hamming distance between matrix rows: Distinct Vectors can
be solved in polynomial time if H &lt;= 2 ceil(h/2) + 1, and is NP-complete
otherwise. Moreover, we explore connections of Distinct Vectors to hitting
sets, thereby providing several fixed-parameter tractability and intractability
results also for general matrices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01157</identifier>
 <datestamp>2015-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01157</id><created>2015-12-03</created><authors><author><keyname>Barto</keyname><forenames>Libor</forenames></author><author><keyname>Kozik</keyname><forenames>Marcin</forenames></author></authors><title>Robustly Solvable Constraint Satisfaction Problems</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An algorithm for a constraint satisfaction problem is called robust if it
outputs an assignment satisfying at least $(1-g(\varepsilon))$-fraction of the
constraints given a $(1-\varepsilon)$-satisfiable instance, where
$g(\varepsilon) \rightarrow 0$ as $\varepsilon \rightarrow 0$. Guruswami and
Zhou conjectured a characterization of constraint languages for which the
corresponding constraint satisfaction problem admits an efficient robust
algorithm. This paper confirms their conjecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01173</identifier>
 <datestamp>2015-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01173</id><created>2015-12-03</created><authors><author><keyname>Shi</keyname><forenames>Jiaxin</forenames></author><author><keyname>Zhu</keyname><forenames>Jun</forenames></author></authors><title>Building Memory with Concept Learning Capabilities from Large-scale
  Knowledge Base</title><categories>cs.CL cs.AI cs.LG</categories><comments>Accepted to NIPS 2015 Cognitive Computation workshop (CoCo@NIPS 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new perspective on neural knowledge base (KB) embeddings, from
which we build a framework that can model symbolic knowledge in the KB together
with its learning process. We show that this framework well regularizes
previous neural KB embedding model for superior performance in reasoning tasks,
while having the capabilities of dealing with unseen entities, that is, to
learn their embeddings from natural language descriptions, which is very like
human's behavior of learning semantic concepts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01174</identifier>
 <datestamp>2015-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01174</id><created>2015-12-03</created><authors><author><keyname>Manikonda</keyname><forenames>Lydia</forenames></author><author><keyname>Venkatesan</keyname><forenames>Ragav</forenames></author><author><keyname>Kambhampati</keyname><forenames>Subbarao</forenames></author><author><keyname>Li</keyname><forenames>Baoxin</forenames></author></authors><title>Evolution of fashion brands on Twitter and Instagram</title><categories>cs.SI</categories><comments>6 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social media has become a popular platform for marketing and brand
advertisement especially for fashion brands. To promote their products and gain
popularity, different brands post their latest products and updates as photos
on different social networks. Little has been explored on how these fashion
brands use different social media to reach out to their customers and obtain
feedback on different products. Understanding this can help future consumers to
focus on their interested brands on specific social media for better
information and also can help the marketing staff of different brands to
understand how the other brands are utilizing the social media. In this article
we focus on the top-20 fashion brands and comparatively analyze how they target
their current and potential future customers on Twitter and Instagram. Using
both linguistic and deep image features, our work reveals an increasing
diversification of trends accompanied by a simultaneous concentration towards a
few selected trends. It provides insights about brand marketing strategies and
their respective competencies. Our investigations show that the brands are
using Twitter and Instagram in a distinctive manner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01187</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01187</id><created>2015-12-03</created><updated>2016-02-25</updated><authors><author><keyname>Brzozowski</keyname><forenames>Janusz</forenames></author><author><keyname>Jir&#xe1;skov&#xe1;</keyname><forenames>Galina</forenames></author><author><keyname>Liu</keyname><forenames>Bo</forenames></author><author><keyname>Rajasekaran</keyname><forenames>Aayush</forenames></author><author><keyname>Szyku&#x142;a</keyname><forenames>Marek</forenames></author></authors><title>On the State Complexity of the Shuffle of Regular Languages</title><categories>cs.FL</categories><comments>13 pages, 4 figures, 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the shuffle operation on regular languages represented by
complete deterministic finite automata. We prove that $f(m,n)=2^{mn-1} +
2^{(m-1)(n-1)}(2^{m-1}-1)(2^{n-1}-1)$ is an upper bound on the state complexity
of the shuffle of two regular languages having state complexities $m$ and $n$,
respectively. We also state partial results about the tightness of this bound.
We show that there exist witness languages meeting the bound if $2\le m\le 5$
and $n\ge2$, and also if $m=n=6$. Moreover, we prove that in the subset
automaton of the NFA accepting the shuffle, all $2^{mn}$ states can be
distinguishable, and an alphabet of size three suffices for that. It follows
that the bound can be met if all $f(m,n)$ states are reachable. We know that an
alphabet of size at least $mn$ is required provided that $m,n \ge 2$. The
question of reachability, and hence also of the tightness of the bound $f(m,n)$
in general, remains open.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01192</identifier>
 <datestamp>2015-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01192</id><created>2015-12-03</created><authors><author><keyname>Jetley</keyname><forenames>Saumya</forenames></author><author><keyname>Romera-Paredes</keyname><forenames>Bernardino</forenames></author><author><keyname>Jayasumana</keyname><forenames>Sadeep</forenames></author><author><keyname>Torr</keyname><forenames>Philip</forenames></author></authors><title>Prototypical Priors: From Improving Classification to Zero-Shot Learning</title><categories>cs.CV</categories><comments>12 Pages, 6 Figures, 2 Tables, in British Machine Vision Conference
  (BMVC), 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent works on zero-shot learning make use of side information such as
visual attributes or natural language semantics to define the relations between
output visual classes and then use these relationships to draw inference on new
unseen classes at test time. In a novel extension to this idea, we propose the
use of visual prototypical concepts as side information. For most real-world
visual object categories, it may be difficult to establish a unique prototype.
However, in cases such as traffic signs, brand logos, flags, and even natural
language characters, these prototypical templates are available and can be
leveraged for an improved recognition performance. The present work proposes a
way to incorporate this prototypical information in a deep learning framework.
Using prototypes as prior information, the deepnet pipeline learns the input
image projections into the prototypical embedding space subject to minimization
of the final classification loss. Based on our experiments with two different
datasets of traffic signs and brand logos, prototypical embeddings incorporated
in a conventional convolutional neural network improve the recognition
performance. Recognition accuracy on the Belga logo dataset is especially
noteworthy and establishes a new state-of-the-art. In zero-shot learning
scenarios, the same system can be directly deployed to draw inference on unseen
classes by simply adding the prototypical information for these new classes at
test time. Thus, unlike earlier approaches, testing on seen and unseen classes
is handled using the same pipeline, and the system can be tuned for a trade-off
of seen and unseen class performance as per task requirement. Comparison with
one of the latest works in the zero-shot learning domain yields top results on
the two datasets mentioned above.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01195</identifier>
 <datestamp>2015-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01195</id><created>2015-12-03</created><authors><author><keyname>Zhou</keyname><forenames>Yuchen</forenames></author><author><keyname>Baras</keyname><forenames>John S.</forenames></author></authors><title>Reachable Set Approach to Collision Avoidance for UAVs</title><categories>cs.SY cs.RO math.OC</categories><comments>CDC 2015 Final Draft</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a reachable set based collision avoidance algorithm
for unmanned aerial vehicles (UAVs). UAVs have been deployed for agriculture
research and management, surveillance and sensor coverage for threat detection
and disaster search and rescue operations. It is essential for the aircraft to
have on-board collision avoidance capability to guarantee safety. Instead of
the traditional approach of collision avoidance between trajectories, we
propose a collision avoidance scheme based on reachable sets and tubes. We then
formulate the problem as a convex optimization problem seeking suitable control
constraint sets for participating aircraft. We have applied the approach on a
case study of two quadrotors collision avoidance scenario.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01196</identifier>
 <datestamp>2015-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01196</id><created>2015-12-03</created><authors><author><keyname>Alaluna</keyname><forenames>Max</forenames></author><author><keyname>Ramos</keyname><forenames>Fernando M. V.</forenames></author><author><keyname>Neves</keyname><forenames>Nuno</forenames></author></authors><title>(Literally) above the clouds: virtualizing the network over multiple
  clouds</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent SDN-based solutions give cloud providers the opportunity to extend
their &quot;as-a-service&quot; model with the offer of complete network virtualization.
They provide tenants with the freedom to specify the network topologies and
addressing schemes of their choosing, while guaranteeing the required level of
isolation among them. These platforms, however, have been targeting the
datacenter of a single cloud provider with full control over the
infrastructure.
  This paper extends this concept further by supporting the creation of virtual
networks that span across several datacenters, which may belong to distinct
cloud providers, while including private facilities owned by the tenant. In
order to achieve this, we introduce a new network layer above the existing
cloud hypervisors, affording the necessary level of control over the
communications while hiding the heterogeneity of the clouds. The benefits of
this approach are various, such as enabling finer decisions on where to place
the virtual machines (e.g., to fulfill legal requirements), avoiding single
points of failure, and potentially decreasing costs. Although our focus in the
paper is on architecture design, we also present experimental results of a
first prototype of the proposed solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01210</identifier>
 <datestamp>2015-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01210</id><created>2015-12-03</created><authors><author><keyname>Kothari</keyname><forenames>Robin</forenames></author></authors><title>Nearly optimal separations between communication (or query) complexity
  and partitions</title><categories>cs.CC quant-ph</categories><comments>13 pages</comments><report-no>MIT-CTP #4746</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show a nearly quadratic separation between deterministic communication
complexity and the logarithm of the partition number, which is essentially
optimal. This improves upon a recent power 1.5 separation of G\&quot;o\&quot;os, Pitassi,
and Watson (FOCS 2015). In query complexity, we establish a nearly quadratic
separation between deterministic (and even randomized) query complexity and
subcube partition complexity, which is also essentially optimal. We also
establish a nearly power 1.5 separation between quantum query complexity and
subcube partition complexity, the first superlinear separation between the two
measures. Lastly, we show a quadratic separation between quantum query
complexity and one-sided subcube partition complexity.
  Our query complexity separations use the recent cheat sheet framework of
Aaronson, Ben-David, and the author. Our query functions are built up in stages
by alternating function composition with the cheat sheet construction. The
communication complexity separation follows from lifting the query separation
to communication complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01218</identifier>
 <datestamp>2015-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01218</id><created>2015-12-03</created><authors><author><keyname>Fortenbacher</keyname><forenames>Philipp</forenames></author><author><keyname>Zellner</keyname><forenames>Martin</forenames></author><author><keyname>Andersson</keyname><forenames>G&#xf6;ran</forenames></author></authors><title>Optimal Sizing and Placement of Distributed Storage in Low Voltage
  Networks</title><categories>cs.SY</categories><comments>submitted for 19th Power Systems Computation Conference 2016, Genoa,
  Italy</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a novel algorithm to optimally size and place storage in
low voltage (LV) networks based on a linearized multiperiod optimal power flow
method which we call forward backward sweep optimal power flow (FBS-OPF). We
show that this method has good convergence properties, its solution deviates
slightly from the optimum and makes the storage sizing and placement problem
tractable for longer investment horizons. We demonstrate the usefulness of our
method by assessing the economic viability of distributed and centralized
storage in LV grids with a high photovoltaic penetration (PV). As a main
result, we quantify that for the CIGRE LV test grid distributed storage
configurations are preferable, since they allow for less PV curtailment due to
grid constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01249</identifier>
 <datestamp>2015-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01249</id><created>2015-12-02</created><authors><author><keyname>Kerkvliet</keyname><forenames>Timber</forenames></author><author><keyname>Meester</keyname><forenames>Ronald</forenames></author></authors><title>Quantifying knowledge with a new calculus for belief functions - a
  generalization of probability theory</title><categories>math.PR cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We first show that there are practical situations in for instance forensic
and gambling settings, in which applying classical probability theory, that is,
based on the axioms of Kolmogorov, is problematic. We then introduce and
discuss Shafer belief functions. Technically, Shafer belief functions
generalize probability distributions. Philosophically, they pertain to
individual or shared knowledge of facts, rather than to facts themselves, and
therefore can be interpreted as generalizing epistemic probability, that is,
probability theory interpreted epistemologically. Belief functions are more
flexible and better suited to deal with certain types of uncertainty than
classical probability distributions. We develop a new calculus for belief
functions which does not use the much criticized Dempster's rule of
combination, by generalizing the classical notions of conditioning and
independence in a natural and uncontroversial way. Using this calculus, we
explain our rejection of Dempster's rule in detail. We apply the new theory to
a number of examples, including a gambling example and an example in a forensic
setting. We prove a law of large numbers for belief functions and offer a
betting interpretation similar to the Dutch Book Theorem for probability
distributions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01250</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01250</id><created>2015-12-02</created><authors><author><keyname>Kerkvliet</keyname><forenames>Timber</forenames></author><author><keyname>Meester</keyname><forenames>Ronald</forenames></author></authors><title>Assessing forensic evidence by computing belief functions - theory and
  applications</title><categories>math.PR cs.AI</categories><comments>arXiv admin note: text overlap with arXiv:1512.01249</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We first discuss certain problems with the classical probabilistic approach
for assessing forensic evidence, in particular its inability to distinguish
between lack of belief and disbelief, and its inability to model complete
ignorance within a given population. We then discuss Shafer belief functions, a
generalization of probability distributions, which can deal with both these
objections. We develop a new calculus of belief functions which does not use
the much criticized Dempster rule of combination, and which yields very natural
notions of conditioning and independence, consistent with their classical
counterparts. We then apply this calculus to some classical forensic problems
like the various island problems and the problem of parental identification. If
we impose no prior knowledge apart from assuming that the culprit or parent
belongs to a given population (something which is possible in our setting),
then our answers differ from the classical ones when uniform or other priors
are imposed. We can actually retrieve the classical answers by imposing the
relevant priors, so our setup can and should be interpreted as a generalization
of the classical methodology, allowing more flexibility. We show how our
calculus can be used to develop an analogue of Bayes' rule, with belief
functions instead of classical probabilities. We also discuss consequences of
our theory for legal practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01256</identifier>
 <datestamp>2015-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01256</id><created>2015-12-03</created><authors><author><keyname>Sinha</keyname><forenames>Gaurav</forenames></author></authors><title>Reconstruction of Real depth-3 Circuits with top fan-in 2</title><categories>cs.DS cs.CC cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a polynomial time randomized algorithm for reconstructing
$\Sigma\Pi\Sigma(2)$ circuits over $\mathbb{R}$, i.e. depth 3 circuits with fan
in 2 at the top addition gate and having real coefficients. The algorithm needs
only a blackbox query access to the polynomial $f\in
\mathbb{R}[x_1,\ldots,x_n]$ of degree d in n variables, computable by a
$\Sigma\Pi\Sigma(2)$ circuit C. In addition, we assume that the simple rank of
this polynomial (essential number of variables after removing the gcd of the
two multiplication gates) is bigger than a fixed constant. Our algorithm runs
in time $poly(n,d)$ and returns an equivalent $\Sigma\Pi\Sigma(2)$ circuit(with
high probability). Our main techniques are based on the use of Quantitative
Syslvester Gallai Theorems from the work of Barak et.al.([3]) to find a small
collection of nice subspaces to project onto. The heart of our paper lies in
subtle applications of the Quantitative Sylvester Gallai theorems to prove why
projections w.r.t. the nice subspaces can be glued. We also use Brills
Equations([8]) to construct a small set of candidate linear forms (containing
linear forms from both gates). Another important technique which comes very
handy is the polynomial time randomized algorithm for factoring multivariate
polynomials given by Kaltofen [14].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01260</identifier>
 <datestamp>2015-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01260</id><created>2015-12-03</created><authors><author><keyname>Civita</keyname><forenames>Andrea</forenames></author><author><keyname>Fiori</keyname><forenames>Simone</forenames></author><author><keyname>Romani</keyname><forenames>Giuseppe</forenames></author></authors><title>A Smartphone-Based Acquisition System for Hips Rotation Fluency
  Assessment</title><categories>cs.CY cs.HC</categories><comments>Technical report of the iSpLab, Department of Information
  Engineering, Universita' Politecnica delle Marche (Italy)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The present contribution is motivated by recent studies on the assessment of
the fluency of body movements during complex motor tasks. In particular, we
focus on the estimation of the Cartesian kinematic jerk (namely, the derivative
of the acceleration) of the hips' orientation during a full three-dimensional
movement. The kinematic jerk index is estimated on the basis of gyroscopic
signals acquired through a smartphone. A specific free mobile application
available for the Android mobile operating system, HyperIMU, is used to acquire
the gyroscopic signals and to transmit them to a personal computer via a User
Datagram Protocol (UDP) through a wireless network. The personal computer
elaborates the acquired data through a MATLAB script, either in real time or
offline, and returns the kinematic jerk index associated to a motor task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01263</identifier>
 <datestamp>2015-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01263</id><created>2015-12-03</created><authors><author><keyname>Berretti</keyname><forenames>Alberto</forenames></author><author><keyname>Ciccarone</keyname><forenames>Simone</forenames></author></authors><title>A Monte Carlo method for the spread of mobile malware</title><categories>cs.SI cs.CE</categories><comments>11 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new model for the spread of mobile malware based on proximity (i.e.
Bluetooth, ad-hoc WiFi or NFC) is introduced. The spread of malware is analyzed
using a Monte Carlo method and the results of the simulation are compared with
those from mean field theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01271</identifier>
 <datestamp>2015-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01271</id><created>2015-12-03</created><updated>2015-12-23</updated><authors><author><keyname>Venkatakrishnan</keyname><forenames>Shaileshh Bojja</forenames></author><author><keyname>Alizadeh</keyname><forenames>Mohammad</forenames></author><author><keyname>Viswanath</keyname><forenames>Pramod</forenames></author></authors><title>Costly Circuits, Submodular Schedules: Hybrid Switch Scheduling for Data
  Centers</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hybrid switching - in which a high bandwidth circuit switch (optical or
wireless) is used in conjunction with a low bandwidth packet switch - is a
promising alternative to interconnect servers in today's large scale
data-centers. Circuit switches offer a very high link rate, but incur a
non-trivial reconfiguration delay which makes their scheduling challenging. In
this paper, we demonstrate a lightweight, simple and nearly-optimal scheduling
algorithm that trades-off configuration costs with the benefits of
reconfiguration that match the traffic demands. The algorithm has strong
connections to submodular optimization, has performance at least half that of
the optimal schedule and strictly outperforms state of the art in a variety of
traffic demand settings. These ideas naturally generalize: we see that indirect
routing leads to exponential connectivity; this is another phenomenon of the
power of multi hop routing, distinct from the well-known load balancing
effects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01272</identifier>
 <datestamp>2015-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01272</id><created>2015-12-03</created><authors><author><keyname>Mansinghka</keyname><forenames>Vikash</forenames></author><author><keyname>Shafto</keyname><forenames>Patrick</forenames></author><author><keyname>Jonas</keyname><forenames>Eric</forenames></author><author><keyname>Petschulat</keyname><forenames>Cap</forenames></author><author><keyname>Gasner</keyname><forenames>Max</forenames></author><author><keyname>Tenenbaum</keyname><forenames>Joshua B.</forenames></author></authors><title>CrossCat: A Fully Bayesian Nonparametric Method for Analyzing
  Heterogeneous, High Dimensional Data</title><categories>cs.AI stat.CO stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is a widespread need for statistical methods that can analyze
high-dimensional datasets with- out imposing restrictive or opaque modeling
assumptions. This paper describes a domain-general data analysis method called
CrossCat. CrossCat infers multiple non-overlapping views of the data, each
consisting of a subset of the variables, and uses a separate nonparametric
mixture to model each view. CrossCat is based on approximately Bayesian
inference in a hierarchical, nonparamet- ric model for data tables. This model
consists of a Dirichlet process mixture over the columns of a data table in
which each mixture component is itself an independent Dirichlet process mixture
over the rows; the inner mixture components are simple parametric models whose
form depends on the types of data in the table. CrossCat combines strengths of
mixture modeling and Bayesian net- work structure learning. Like mixture
modeling, CrossCat can model a broad class of distributions by positing latent
variables, and produces representations that can be efficiently conditioned and
sampled from for prediction. Like Bayesian networks, CrossCat represents the
dependencies and independencies between variables, and thus remains accurate
when there are multiple statistical signals. Inference is done via a scalable
Gibbs sampling scheme; this paper shows that it works well in practice. This
paper also includes empirical results on heterogeneous tabular data of up to 10
million cells, such as hospital cost and quality measures, voting records,
unemployment rates, gene expression measurements, and images of handwritten
digits. CrossCat infers structure that is consistent with accepted findings and
common-sense knowledge in multiple domains and yields predictive accuracy
competitive with generative, discriminative, and model-free alternatives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01274</identifier>
 <datestamp>2015-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01274</id><created>2015-12-03</created><authors><author><keyname>Chen</keyname><forenames>Tianqi</forenames></author><author><keyname>Li</keyname><forenames>Mu</forenames></author><author><keyname>Li</keyname><forenames>Yutian</forenames></author><author><keyname>Lin</keyname><forenames>Min</forenames></author><author><keyname>Wang</keyname><forenames>Naiyan</forenames></author><author><keyname>Wang</keyname><forenames>Minjie</forenames></author><author><keyname>Xiao</keyname><forenames>Tianjun</forenames></author><author><keyname>Xu</keyname><forenames>Bing</forenames></author><author><keyname>Zhang</keyname><forenames>Chiyuan</forenames></author><author><keyname>Zhang</keyname><forenames>Zheng</forenames></author></authors><title>MXNet: A Flexible and Efficient Machine Learning Library for
  Heterogeneous Distributed Systems</title><categories>cs.DC cs.LG cs.MS cs.NE</categories><comments>In Neural Information Processing Systems, Workshop on Machine
  Learning Systems, 2016</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  MXNet is a multi-language machine learning (ML) library to ease the
development of ML algorithms, especially for deep neural networks. Embedded in
the host language, it blends declarative symbolic expression with imperative
tensor computation. It offers auto differentiation to derive gradients. MXNet
is computation and memory efficient and runs on various heterogeneous systems,
ranging from mobile devices to distributed GPU clusters.
  This paper describes both the API design and the system implementation of
MXNet, and explains how embedding of both symbolic expression and tensor
operation is handled in a unified fashion. Our preliminary experiments reveal
promising results on large scale deep neural network applications using
multiple GPU machines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01283</identifier>
 <datestamp>2015-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01283</id><created>2015-12-03</created><authors><author><keyname>Datla</keyname><forenames>Vivek</forenames></author><author><keyname>Vishnu</keyname><forenames>Abhinav</forenames></author></authors><title>Predicting the top and bottom ranks of billboard songs using Machine
  Learning</title><categories>cs.CL cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The music industry is a $130 billion industry. Predicting whether a song
catches the pulse of the audience impacts the industry. In this paper we
analyze language inside the lyrics of the songs using several computational
linguistic algorithms and predict whether a song would make to the top or
bottom of the billboard rankings based on the language features. We trained and
tested an SVM classifier with a radial kernel function on the linguistic
features. Results indicate that we can classify whether a song belongs to top
and bottom of the billboard charts with a precision of 0.76.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01289</identifier>
 <datestamp>2015-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01289</id><created>2015-12-03</created><authors><author><keyname>Grant</keyname><forenames>Edward</forenames></author><author><keyname>Sahm</keyname><forenames>Stephan</forenames></author><author><keyname>Zabihi</keyname><forenames>Mariam</forenames></author><author><keyname>van Gerven</keyname><forenames>Marcel</forenames></author></authors><title>Predicting psychological attributions from face photographs with a deep
  neural network</title><categories>cs.CV cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Judgements about personality based on facial appearance are strong effectors
in social decision making and are known to impact on areas from presidential
elections to jury decisions. Recent work has shown that it is possible to
predict perception of memorability, trustworthiness, intelligence and other
attributes in human face images. The most successful of these approaches
requires face images expertly annotated with key facial landmarks. We
demonstrate a Convolutional Neural Network (CNN) model that is able perform the
same task without the need for landmark features thereby greatly increasing
efficiency. The model has high accuracy, surpassing human level performance in
some cases. Furthermore, we use a deconvolutional approach to visualize
important features for perception of 22 attributes and show that these can be
described as a composites of their positive and negative components by
separately visualizing both.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01290</identifier>
 <datestamp>2015-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01290</id><created>2015-12-03</created><authors><author><keyname>Gupta</keyname><forenames>Abhishek K.</forenames></author><author><keyname>Andrews</keyname><forenames>Jeffrey G.</forenames></author><author><keyname>Heath</keyname><forenames>Robert W.</forenames><suffix>Jr</suffix></author></authors><title>On the Feasibility of Sharing Spectrum Licenses in mmWave Cellular
  Systems</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The highly directional and adaptive antennas used in mmWave communication
open up the possibility of uncoordinated sharing of spectrum licenses between
commercial cellular providers. There are several advantages to sharing
including a reduction in license costs and an increase in spectrum utilization.
In this paper, we establish the theoretical feasibility of spectrum license
sharing among mmWave cellular providers. We consider a heterogeneous
multi-network system containing multiple independent cellular networks. We then
compute the SINR and rate distribution for downlink mobile users of each
network. Using the analysis, we compare systems with fully shared licenses and
exclusive licenses for different access rules and explore the trade-offs
between system performance and spectrum cost. We show that sharing spectrum
licenses increases the per-user rate when antennas have narrow beams and is
also favored when there is a low density of users. We also consider a
multi-network system where BSs of all the networks are co-located to show that
the simultaneous sharing of spectrum and infrastructure is also feasible. We
show that all networks can share licenses with less bandwidth and still achieve
the same per-user median rate as if they each had an exclusive license to
spectrum with more bandwidth.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01293</identifier>
 <datestamp>2015-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01293</id><created>2015-12-03</created><authors><author><keyname>Yu</keyname><forenames>Huacheng</forenames></author></authors><title>Cell-probe Lower Bounds for Dynamic Problems via a New Communication
  Model</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we develop a new communication model to prove a data structure
lower bound for the dynamic interval union problem. The problem is to maintain
a multiset of intervals $\mathcal{I}$ over $[0, n]$ with integer coordinates,
supporting the following operations:
  - insert(a, b): add an interval $[a, b]$ to $\mathcal{I}$, provided that $a$
and $b$ are integers in $[0, n]$;
  - delete(a, b): delete a (previously inserted) interval $[a, b]$ from
$\mathcal{I}$;
  - query(): return the total length of the union of all intervals in
$\mathcal{I}$.
  It is related to the two-dimensional case of Klee's measure problem. We prove
that there is a distribution over sequences of operations with $O(n)$
insertions and deletions, and $O(n^{0.01})$ queries, for which any data
structure with any constant error probability requires $\Omega(n\log n)$ time
in expectation. Interestingly, we use the sparse set disjointness protocol of
H\aa{}stad and Wigderson [ToC'07] to speed up a reduction from a new kind of
nondeterministic communication games, for which we prove lower bounds.
  For applications, we prove lower bounds for several dynamic graph problems by
reducing them from dynamic interval union.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01294</identifier>
 <datestamp>2015-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01294</id><created>2015-12-03</created><authors><author><keyname>Ugrinovskii</keyname><forenames>V.</forenames></author></authors><title>Distributed robust estimation over randomly switching networks using
  $H_\infty$ consensus</title><categories>cs.SY</categories><journal-ref>Automatica, 49(1), pp. 160-168, 2013</journal-ref><doi>10.1016/j.automatica.2012.09.010</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper considers a distributed robust estimation problem over a network
with Markovian randomly varying topology. The objective is to deal with network
variations locally, by switching observer gains at affected nodes only. We
propose sufficient conditions which guarantee a suboptimal $H_\infty$ level of
relative disagreement of estimates in such observer networks. When the status
of the network is known globally, these sufficient conditions enable the
network gains to be computed by solving certain LMIs. When the nodes are to
rely on a locally available information about the network topology, additional
rank constraints are used to condition the gains, given this information. The
results are complemented by necessary conditions which relate properties of the
interconnection graph Laplacian to the mean-square detectability of the plant
through measurement and interconnection channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01314</identifier>
 <datestamp>2015-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01314</id><created>2015-12-03</created><authors><author><keyname>Roy</keyname><forenames>Subhrajit</forenames></author><author><keyname>Basu</keyname><forenames>Arindam</forenames></author></authors><title>An Online Unsupervised Structural Plasticity Algorithm for Spiking
  Neural Networks</title><categories>cs.NE</categories><comments>11 pages, 10 figures, journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we propose a novel Winner-Take-All (WTA) architecture
employing neurons with nonlinear dendrites and an online unsupervised
structural plasticity rule for training it. Further, to aid hardware
implementations, our network employs only binary synapses. The proposed
learning rule is inspired by spike time dependent plasticity (STDP) but differs
for each dendrite based on its activation level. It trains the WTA network
through formation and elimination of connections between inputs and synapses.
To demonstrate the performance of the proposed network and learning rule, we
employ it to solve two, four and six class classification of random Poisson
spike time inputs. The results indicate that by proper tuning of the inhibitory
time constant of the WTA, a trade-off between specificity and sensitivity of
the network can be achieved. We use the inhibitory time constant to set the
number of subpatterns per pattern we want to detect. We show that while the
percentage of successful trials are 92%, 88% and 82% for two, four and six
class classification when no pattern subdivisions are made, it increases to
100% when each pattern is subdivided into 5 or 10 subpatterns. However, the
former scenario of no pattern subdivision is more jitter resilient than the
later ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01320</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01320</id><created>2015-12-04</created><updated>2016-01-26</updated><authors><author><keyname>Borji</keyname><forenames>Ali</forenames></author><author><keyname>Izadi</keyname><forenames>Saeed</forenames></author><author><keyname>Itti</keyname><forenames>Laurent</forenames></author></authors><title>What can we learn about CNNs from a large scale controlled object
  dataset?</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tolerance to image variations (e.g. translation, scale, pose, illumination)
is an important desired property of any object recognition system, be it human
or machine. Moving towards increasingly bigger datasets has been trending in
computer vision specially with the emergence of highly popular deep learning
models. While being very useful for learning invariance to object inter- and
intra-class shape variability, these large-scale wild datasets are not very
useful for learning invariance to other parameters forcing researchers to
resort to other tricks for training a model. In this work, we introduce a
large-scale synthetic dataset, which is freely and publicly available, and use
it to answer several fundamental questions regarding invariance and selectivity
properties of convolutional neural networks. Our dataset contains two parts: a)
objects shot on a turntable: 16 categories, 8 rotation angles, 11 cameras on a
semicircular arch, 5 lighting conditions, 3 focus levels, variety of
backgrounds (23.4 per instance) generating 1320 images per instance (over 20
million images in total), and b) scenes: in which a robot arm takes pictures of
objects on a 1:160 scale scene. We study: 1) invariance and selectivity of
different CNN layers, 2) knowledge transfer from one object category to
another, 3) systematic or random sampling of images to build a train set, 4)
domain adaptation from synthetic to natural scenes, and 5) order of knowledge
delivery to CNNs. We also explore how our analyses can lead the field to
develop more efficient CNNs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01322</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01322</id><created>2015-12-04</created><updated>2016-01-23</updated><authors><author><keyname>Shin</keyname><forenames>Sungho</forenames></author><author><keyname>Hwang</keyname><forenames>Kyuyeon</forenames></author><author><keyname>Sung</keyname><forenames>Wonyong</forenames></author></authors><title>Fixed Point Performance Analysis of Recurrent Neural Networks</title><categories>cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recurrent neural networks have shown excellent performance in many
applications, however they require increased complexity in hardware or software
based implementations. The hardware complexity can be much lowered by
minimizing the word-length of weights and signals. This work analyzes the
fixed-point performance of recurrent neural networks using a retrain based
quantization method. The quantization sensitivity of each layer in RNNs is
studied, and the overall fixed-point optimization results minimizing the
capacity of weights while not sacrificing the performance are presented. A
language model and a phoneme recognition examples are used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01325</identifier>
 <datestamp>2015-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01325</id><created>2015-12-04</created><authors><author><keyname>Saleh</keyname><forenames>Babak</forenames></author><author><keyname>Elgammal</keyname><forenames>Ahmed</forenames></author><author><keyname>Feldman</keyname><forenames>Jacob</forenames></author><author><keyname>Farhadi</keyname><forenames>Ali</forenames></author></authors><title>Toward a Taxonomy and Computational Models of Abnormalities in Images</title><categories>cs.CV cs.AI cs.HC cs.IT cs.LG math.IT</categories><comments>To appear in the Thirtieth AAAI Conference on Artificial Intelligence
  (AAAI 2016)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The human visual system can spot an abnormal image, and reason about what
makes it strange. This task has not received enough attention in computer
vision. In this paper we study various types of atypicalities in images in a
more comprehensive way than has been done before. We propose a new dataset of
abnormal images showing a wide range of atypicalities. We design human subject
experiments to discover a coarse taxonomy of the reasons for abnormality. Our
experiments reveal three major categories of abnormality: object-centric,
scene-centric, and contextual. Based on this taxonomy, we propose a
comprehensive computational model that can predict all different types of
abnormality in images and outperform prior arts in abnormality recognition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01332</identifier>
 <datestamp>2015-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01332</id><created>2015-12-04</created><authors><author><keyname>Yoshida</keyname><forenames>Naoto</forenames></author></authors><title>Q-Networks for Binary Vector Actions</title><categories>cs.NE cs.LG</categories><comments>9 pages, 5 figures, accepted for Deep Reinforcement Learning
  Workshop, NIPS 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper reinforcement learning with binary vector actions was
investigated. We suggest an effective architecture of the neural networks for
approximating an action-value function with binary vector actions. The proposed
architecture approximates the action-value function by a linear function with
respect to the action vector, but is still non-linear with respect to the state
input. We show that this approximation method enables the efficient calculation
of greedy action selection and softmax action selection. Using this
architecture, we suggest an online algorithm based on Q-learning. The empirical
results in the grid world and the blocker task suggest that our approximation
architecture would be effective for the RL problems with large discrete action
sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01337</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01337</id><created>2015-12-04</created><updated>2016-02-05</updated><authors><author><keyname>Yin</keyname><forenames>Jun</forenames></author><author><keyname>Jiang</keyname><forenames>Xin</forenames></author><author><keyname>Lu</keyname><forenames>Zhengdong</forenames></author><author><keyname>Shang</keyname><forenames>Lifeng</forenames></author><author><keyname>Li</keyname><forenames>Hang</forenames></author><author><keyname>Li</keyname><forenames>Xiaoming</forenames></author></authors><title>Neural Generative Question Answering</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an end-to-end neural network model, named Neural
Generative Question Answering (genQA), that can generate answers to simple
factoid questions, both in natural language. More specifically, the model is
built on the encoder-decoder framework for sequence-to-sequence learning, while
equipped with the ability to access an embedded knowledge-base through an
attention-like mechanism. The model is trained on a corpus of question-answer
pairs, with their associated triples in the given knowledge-base. Empirical
study shows the proposed model can effectively deal with the language variation
of the question and generate a right answer by referring to the facts in the
knowledge-base. The experiment on question answering demonstrates that the
proposed model can outperform the embedding-based QA model as well as the
neural dialogue models trained on the same data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01344</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01344</id><created>2015-12-04</created><authors><author><keyname>Sikdar</keyname><forenames>Sandipan</forenames></author><author><keyname>Ganguly</keyname><forenames>Niloy</forenames></author><author><keyname>Mukherjee</keyname><forenames>Animesh</forenames></author></authors><title>Time series analysis of temporal networks</title><categories>cs.SI physics.soc-ph</categories><doi>10.1140/epjb/e2015-60654-7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An important feature of all real-world networks is that the network structure
changes over time. Due to this dynamic nature, it becomes difficult to propose
suitable growth models that can explain the various important characteristic
properties of these networks. In fact, in many application oriented studies
only knowing these properties is sufficient. We, in this paper show that even
if the network structure at a future time point is not available one can still
manage to estimate its properties. We propose a novel method to map a temporal
network to a set of time series instances, analyze them and using a standard
forecast model of time series, try to predict the properties of a temporal
network at a later time instance. We mainly focus on the temporal network of
human face- to-face contacts and observe that it represents a stochastic
process with memory that can be modeled as ARIMA. We use cross validation
techniques to find the percentage accuracy of our predictions. An important
observation is that the frequency domain properties of the time series obtained
from spectrogram analysis could be used to refine the prediction framework by
identifying beforehand the cases where the error in prediction is likely to be
high. This leads to an improvement of 7.96% (for error level &lt;= 20%) in
prediction accuracy on an average across all datasets. As an application we
show how such prediction scheme can be used to launch targeted attacks on
temporal networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01348</identifier>
 <datestamp>2015-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01348</id><created>2015-12-04</created><authors><author><keyname>Gadouleau</keyname><forenames>Maximilien</forenames></author></authors><title>On the possible values of the entropy of undirected graphs</title><categories>cs.IT cs.DM math.CO math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The entropy of a digraph is a fundamental measure which relates network
coding, information theory, and fixed points of finite dynamical systems. In
this paper, we focus on the entropy of undirected graphs. We prove that for any
integer $k$ the number of possible values of the entropy of an undirected graph
up to $k$ is finite. We also determine all the possible values for the entropy
of an undirected graph up to the value of four.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01355</identifier>
 <datestamp>2015-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01355</id><created>2015-12-04</created><authors><author><keyname>Bertinetto</keyname><forenames>Luca</forenames></author><author><keyname>Valmadre</keyname><forenames>Jack</forenames></author><author><keyname>Golodetz</keyname><forenames>Stuart</forenames></author><author><keyname>Miksik</keyname><forenames>Ondrej</forenames></author><author><keyname>Torr</keyname><forenames>Philip</forenames></author></authors><title>Staple: Complementary Learners for Real-Time Tracking</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Correlation Filter-based trackers have recently achieved excellent
performance, showing great robustness to challenging situations such as motion
blur and illumination changes. However, since the model that they learn depends
strongly on the spatial layout of the tracked object, they are notoriously
sensitive to deformation. Models based on colour statistics have complementary
traits: they cope well with variation in shape, but suffer when illumination is
not consistent throughout a sequence. Moreover, colour distributions alone can
be insufficiently discriminative. In this paper, we show that a simple tracker
combining complementary cues in a ridge regression framework can operate faster
than 90 FPS and outperform not only all entries in the popular VOT14
competition, but also recent and far more sophisticated trackers according to
multiple benchmarks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01362</identifier>
 <datestamp>2015-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01362</id><created>2015-12-04</created><authors><author><keyname>Leke</keyname><forenames>Collins</forenames></author><author><keyname>Marwala</keyname><forenames>Tshilidzi</forenames></author><author><keyname>Paul</keyname><forenames>Satyakama</forenames></author></authors><title>Proposition of a Theoretical Model for Missing Data Imputation using
  Deep Learning and Evolutionary Algorithms</title><categories>cs.NE cs.LG</categories><comments>14 Pages, 4 figures, journal, experiments will be added testing the
  hypotheses</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the last couple of decades, there has been major advancements in the
domain of missing data imputation. The techniques in the domain include amongst
others: Expectation Maximization, Neural Networks with Evolutionary Algorithms
or optimization techniques and K-Nearest Neighbor approaches to solve the
problem. The presence of missing data entries in databases render the tasks of
decision-making and data analysis nontrivial. As a result this area has
attracted a lot of research interest with the aim being to yield accurate and
time efficient and sensitive missing data imputation techniques especially when
time sensitive applications are concerned like power plants and winding
processes. In this article, considering arbitrary and monotone missing data
patterns, we hypothesize that the use of deep neural networks built using
autoencoders and denoising autoencoders in conjunction with genetic algorithms,
swarm intelligence and maximum likelihood estimator methods as novel data
imputation techniques will lead to better imputed values than existing
techniques. Also considered are the missing at random, missing completely at
random and missing not at random missing data mechanisms. We also intend to use
fuzzy logic in tandem with deep neural networks to perform the missing data
imputation tasks, as well as different building blocks for the deep neural
networks like Stacked Restricted Boltzmann Machines and Deep Belief Networks to
test our hypothesis. The motivation behind this article is the need for missing
data imputation techniques that lead to better imputed values than existing
methods with higher accuracies and lower errors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01364</identifier>
 <datestamp>2015-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01364</id><created>2015-12-04</created><authors><author><keyname>Sparavigna</keyname><forenames>A. C.</forenames></author><author><keyname>Marazzato</keyname><forenames>R.</forenames></author></authors><title>Using Google Ngram Viewer for Scientific Referencing and History of
  Science</title><categories>cs.DL cs.CY</categories><comments>Keywords: Computers and Society, Literary works, Time-series,
  Referencing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Today, several universal digital libraries exist such as Google Books,
Project Gutenberg, Internet Archive libraries, which possess texts from general
collections, and many other archives are available, concerning more specific
subjects. On the digitalized texts available from these libraries, we can
perform several analyses, from those typically used for time-series to those of
network theory. For what concerns time-series, an interesting tool provided by
Google Books exists, which can help us in bibliographical and reference
researches. This tool is the Ngram Viewer, based on yearly count of n-grams. As
we will show in this paper, although it seems suitable just for literary works,
it can be useful for scientific researches, not only for history of science,
but also for acquiring references often unknown to researchers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01370</identifier>
 <datestamp>2015-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01370</id><created>2015-12-04</created><authors><author><keyname>Jia</keyname><forenames>Yantao</forenames></author><author><keyname>Wang</keyname><forenames>Yuanzhuo</forenames></author><author><keyname>Lin</keyname><forenames>Hailun</forenames></author><author><keyname>Jin</keyname><forenames>Xiaolong</forenames></author><author><keyname>Cheng</keyname><forenames>Xueqi</forenames></author></authors><title>Locally Adaptive Translation for Knowledge Graph Embedding</title><categories>cs.AI cs.CL</categories><acm-class>I.2.4; I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Knowledge graph embedding aims to represent entities and relations in a
large-scale knowledge graph as elements in a continuous vector space. Existing
methods, e.g., TransE and TransH, learn embedding representation by defining a
global margin-based loss function over the data. However, the optimal loss
function is determined during experiments whose parameters are examined among a
closed set of candidates. Moreover, embeddings over two knowledge graphs with
different entities and relations share the same set of candidate loss
functions, ignoring the locality of both graphs. This leads to the limited
performance of embedding related applications. In this paper, we propose a
locally adaptive translation method for knowledge graph embedding, called
TransA, to find the optimal loss function by adaptively determining its margin
over different knowledge graphs. Experiments on two benchmark data sets
demonstrate the superiority of the proposed method, as compared to
the-state-of-the-art ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01375</identifier>
 <datestamp>2016-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01375</id><created>2015-12-04</created><updated>2016-03-08</updated><authors><author><keyname>Harks</keyname><forenames>Tobias</forenames></author><author><keyname>Timmermans</keyname><forenames>Veerle</forenames></author></authors><title>Uniqueness of Equilibria in Atomic Splittable Polymatroid Congestion
  Games</title><categories>cs.GT math.OC</categories><comments>17 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study uniqueness of Nash equilibria in atomic splittable congestion games
and derive a uniqueness result based on polymatroid theory: when the strategy
space of every player is a bidirectional flow polymatroid, then equilibria are
unique. Bidirectional flow polymatroids are introduced as a subclass of
polymatroids possessing certain exchange properties. We show that important
cases such as base orderable matroids can be recovered as a special case of
bidirectional flow polymatroids. On the other hand we show that matroidal set
systems are in some sense necessary to guarantee uniqueness of equilibria: for
every atomic splittable congestion game with at least three players and
nonmatroidal set systems per player, there is an isomorphic game having
multiple equilibria. Our results leave a gap between base orderable matroids
and general matroids for which we do not know whether equilibria are unique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01383</identifier>
 <datestamp>2015-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01383</id><created>2015-12-04</created><authors><author><keyname>M&#xf6;llenhoff</keyname><forenames>Thomas</forenames></author><author><keyname>Laude</keyname><forenames>Emanuel</forenames></author><author><keyname>Moeller</keyname><forenames>Michael</forenames></author><author><keyname>Lellmann</keyname><forenames>Jan</forenames></author><author><keyname>Cremers</keyname><forenames>Daniel</forenames></author></authors><title>Sublabel-Accurate Relaxation of Nonconvex Energies</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel spatially continuous framework for convex relaxations
based on functional lifting. Our method can be interpreted as a
sublabel-accurate solution to multilabel problems. We show that previously
proposed functional lifting methods optimize an energy which is linear between
two labels and hence require (often infinitely) many labels for a faithful
approximation. In contrast, the proposed formulation is based on a piecewise
convex approximation and therefore needs far fewer labels. In comparison to
recent MRF-based approaches, our method is formulated in a spatially continuous
setting and shows less grid bias. Moreover, in a local sense, our formulation
is the tightest possible convex relaxation. It is easy to implement and allows
an efficient primal-dual optimization on GPUs. We show the effectiveness of our
approach on several computer vision problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01384</identifier>
 <datestamp>2015-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01384</id><created>2015-12-04</created><authors><author><keyname>de Arruda</keyname><forenames>Henrique F.</forenames></author><author><keyname>Costa</keyname><forenames>Luciano da F.</forenames></author><author><keyname>Amancio</keyname><forenames>Diego R.</forenames></author></authors><title>Topic segmentation via community detection in complex networks</title><categories>cs.CL cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many real systems have been modelled in terms of network concepts, and
written texts are a particular example of information networks. In recent
years, the use of network methods to analyze language has allowed the discovery
of several interesting findings, including the proposition of novel models to
explain the emergence of fundamental universal patterns. While syntactical
networks, one of the most prevalent networked models of written texts, display
both scale-free and small-world properties, such representation fails in
capturing other textual features, such as the organization in topics or
subjects. In this context, we propose a novel network representation whose main
purpose is to capture the semantical relationships of words in a simple way. To
do so, we link all words co-occurring in the same semantic context, which is
defined in a threefold way. We show that the proposed representations favours
the emergence of communities of semantically related words, and this feature
may be used to identify relevant topics. The proposed methodology to detect
topics was applied to segment selected Wikipedia articles. We have found that,
in general, our methods outperform traditional bag-of-words representations,
which suggests that a high-level textual representation may be useful to study
semantical features of texts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01388</identifier>
 <datestamp>2015-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01388</id><created>2015-12-04</created><authors><author><keyname>Schneider</keyname><forenames>Jesper W.</forenames></author><author><keyname>Costas</keyname><forenames>Rodrigo</forenames></author></authors><title>Identifying potential breakthrough publications using refined citation
  analyses: Three related explorative approaches</title><categories>cs.DL</categories><comments>Accepted for publication in Journal of the Association for
  Information Science and Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The article presents three advanced citation-based methods used to detect
potential breakthrough papers among very highly cited papers. We approach the
detection of such papers from three different perspectives in order to provide
different typologies of breakthrough papers. In all three cases we use the
classification of scientific publications developed at CWTS based on direct
citation relationships. This classification establishes clusters of papers at
three levels of aggregation. Papers are clustered based on their similar
citation orientations and it is assumed that they are focused on similar
research interests. We use the clustering as the context for detecting
potential breakthrough papers. We utilize the Characteristics Scores and Scales
(CSS) approach to partition citation distributions and implement a specific
filtering algorithm to sort out potential highly-cited followers, papers not
considered breakthroughs in themselves. After invoking thresholds and
filtering, three methods are explored: A very exclusive one where only the
highest cited paper in a micro-cluster is considered as a potential
breakthrough paper (M1); as well as two conceptually different methods, one
that detects potential breakthrough papers among the two percent highest cited
papers according to CSS (M2a), and finally a more restrictive version where, in
addition to the CSS two percent filter, knowledge diffusion is also taken in as
an extra parameter (M2b). The advance citation-based methods are explored and
evaluated using specifically validated publication sets linked to different
Danish funding instruments including centres of excellence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01400</identifier>
 <datestamp>2015-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01400</id><created>2015-12-04</created><authors><author><keyname>Wu</keyname><forenames>Haibing</forenames></author><author><keyname>Gu</keyname><forenames>Xiaodong</forenames></author></authors><title>Max-Pooling Dropout for Regularization of Convolutional Neural Networks</title><categories>cs.LG cs.CV cs.NE</categories><comments>The journal version of this paper [arXiv:1512.00242] has been
  published in Neural Networks,
  http://www.sciencedirect.com/science/article/pii/S0893608015001446</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, dropout has seen increasing use in deep learning. For deep
convolutional neural networks, dropout is known to work well in fully-connected
layers. However, its effect in pooling layers is still not clear. This paper
demonstrates that max-pooling dropout is equivalent to randomly picking
activation based on a multinomial distribution at training time. In light of
this insight, we advocate employing our proposed probabilistic weighted
pooling, instead of commonly used max-pooling, to act as model averaging at
test time. Empirical evidence validates the superiority of probabilistic
weighted pooling. We also compare max-pooling dropout and stochastic pooling,
both of which introduce stochasticity based on multinomial distributions at
pooling stage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01401</identifier>
 <datestamp>2015-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01401</id><created>2015-12-04</created><authors><author><keyname>Veeravasarapu</keyname><forenames>V S R</forenames></author><author><keyname>Hota</keyname><forenames>Rudra Narayan</forenames></author><author><keyname>Rothkopf</keyname><forenames>Constantin</forenames></author><author><keyname>Visvanathan</keyname><forenames>Ramesh</forenames></author></authors><title>Model Validation for Vision Systems via Graphics Simulation</title><categories>cs.CV</categories><comments>arXiv admin note: text overlap with arXiv:1512.01030</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rapid advances in computation, combined with latest advances in computer
graphics simulations have facilitated the development of vision systems and
training them in virtual environments. One major stumbling block is in
certification of the designs and tuned parameters of these systems to work in
real world. In this paper, we begin to explore the fundamental question: Which
type of information transfer is more analogous to real world? Inspired from the
performance characterization methodology outlined in the 90's, we note that
insights derived from simulations can be qualitative or quantitative depending
on the degree of the fidelity of models used in simulations and the nature of
the questions posed by the experimenter. We adapt the methodology in the
context of current graphics simulation tools for modeling data generation
processes and, for systematic performance characterization and trade-off
analysis for vision system design leading to qualitative and quantitative
insights. In concrete, we examine invariance assumptions used in vision
algorithms for video surveillance settings as a case study and assess the
degree to which those invariance assumptions deviate as a function of
contextual variables on both graphics simulations and in real data. As computer
graphics rendering quality improves, we believe teasing apart the degree to
which model assumptions are valid via systematic graphics simulation can be a
significant aid to assisting more principled ways of approaching vision system
design and performance modeling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01406</identifier>
 <datestamp>2015-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01406</id><created>2015-12-04</created><authors><author><keyname>Cao</keyname><forenames>Yonglin</forenames></author><author><keyname>Cao</keyname><forenames>Yuan</forenames></author><author><keyname>Gao</keyname><forenames>Jian</forenames></author><author><keyname>Fu</keyname><forenames>Fang-wei</forenames></author></authors><title>Constacyclic codes of length $p^sn$ over
  $\mathbb{F}_{p^m}+u\mathbb{F}_{p^m}$</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $\mathbb{F}_{p^m}$ be a finite field of cardinality $p^m$ and
$R=\mathbb{F}_{p^m}[u]/\langle u^2\rangle=\mathbb{F}_{p^m}+u\mathbb{F}_{p^m}$
$(u^2=0)$, where $p$ is a prime and $m$ is a positive integer. For any
$\lambda\in \mathbb{F}_{p^m}^{\times}$, an explicit representation for all
distinct $\lambda$-constacyclic codes over $R$ of length $p^sn$ is given by a
canonical form decomposition for each code, where $s$ and $n$ are positive
integers satisfying ${\rm gcd}(p,n)=1$. For any such code, using its canonical
form decomposition the representation for the dual code of the code is
provided. Moreover, representations for all distinct negacyclic codes and their
dual codes of length $p^sn$ over $R$ are obtained, and self-duality for these
codes are determined. Finally, all distinct self-dual negacyclic codes over
$\mathbb{F}_5+u\mathbb{F}_5$ of length $2\cdot 5^s\cdot 3^t$ are listed for any
positive integer $t$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01409</identifier>
 <datestamp>2015-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01409</id><created>2015-12-04</created><authors><author><keyname>Cao</keyname><forenames>Mengyun</forenames></author><author><keyname>Tian</keyname><forenames>Jiao</forenames></author><author><keyname>Cheng</keyname><forenames>Dezhi</forenames></author><author><keyname>Liu</keyname><forenames>Jin</forenames></author><author><keyname>Sun</keyname><forenames>Xiaoping</forenames></author></authors><title>What Makes it Difficult to Understand a Scientific Literature?</title><categories>cs.CL</categories><comments>Accepted by SKG2015</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  In the artificial intelligence area, one of the ultimate goals is to make
computers understand human language and offer assistance. In order to achieve
this ideal, researchers of computer science have put forward a lot of models
and algorithms attempting at enabling the machine to analyze and process human
natural language on different levels of semantics. Although recent progress in
this field offers much hope, we still have to ask whether current research can
provide assistance that people really desire in reading and comprehension. To
this end, we conducted a reading comprehension test on two scientific papers
which are written in different styles. We use the semantic link models to
analyze the understanding obstacles that people will face in the process of
reading and figure out what makes it difficult for human to understand a
scientific literature. Through such analysis, we summarized some
characteristics and problems which are reflected by people with different
levels of knowledge on the comprehension of difficult science and technology
literature, which can be modeled in semantic link network. We believe that
these characteristics and problems will help us re-examine the existing machine
models and are helpful in the designing of new one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01413</identifier>
 <datestamp>2015-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01413</id><created>2015-12-04</created><authors><author><keyname>Bouman</keyname><forenames>Katherine L.</forenames></author><author><keyname>Johnson</keyname><forenames>Michael D.</forenames></author><author><keyname>Zoran</keyname><forenames>Daniel</forenames></author><author><keyname>Fish</keyname><forenames>Vincent L.</forenames></author><author><keyname>Doeleman</keyname><forenames>Sheperd S.</forenames></author><author><keyname>Freeman</keyname><forenames>William T.</forenames></author></authors><title>Computational Imaging for VLBI Image Reconstruction</title><categories>astro-ph.IM astro-ph.GA cs.CV</categories><comments>10 pages, project website: http://vlbiimaging.csail.mit.edu/</comments><acm-class>I.4; I.4.5; I.4.4; G.1.8; J.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Very long baseline interferometry (VLBI) is a technique for imaging celestial
radio emissions by simultaneously observing a source from telescopes
distributed across Earth. The challenges in reconstructing images from fine
angular resolution VLBI data are immense. The data is extremely sparse and
noisy, thus requiring statistical image models such as those designed in the
computer vision community. In this paper we present a novel Bayesian approach
for VLBI image reconstruction. While other methods require careful tuning and
parameter selection for different types of images, our method is robust and
produces good results under different settings such as low SNR or extended
emissions. The success of our method is demonstrated on realistic synthetic
experiments as well as publicly available real data. We present this problem in
a way that is accessible to members of the computer vision community, and
provide a dataset website (vlbiimaging.csail.mit.edu) to allow for controlled
comparisons across algorithms. This dataset can foster development of new
methods by making VLBI easily approachable to computer vision researchers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01416</identifier>
 <datestamp>2015-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01416</id><created>2015-12-04</created><authors><author><keyname>Bornat</keyname><forenames>Richard</forenames><affiliation>Middlesex University, London</affiliation></author><author><keyname>Alglave</keyname><forenames>Jade</forenames><affiliation>University College, London</affiliation><affiliation>Microsoft Research</affiliation></author><author><keyname>Parkinson</keyname><forenames>Matthew</forenames><affiliation>Microsoft Research</affiliation></author></authors><title>New Lace and Arsenic: adventures in weak memory with a program logic</title><categories>cs.LO</categories><comments>This paper reports the joint work of its authors. But the words in
  the paper were written by Richard Bornat. Any opprobrium, bug reports,
  complaints, and observations about sins of commission or omission should be
  directed at him. R.Bornat@mdx.ac.uk</comments><acm-class>F.3.1; D.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a program logic for weak memory (also known as relaxed memory).
The logic is based on Hoare logic within a thread, and rely/guarantee between
threads. It is presented via examples, giving proofs of many weak-memory litmus
tests. It extends to coherence but not yet to synchronised assignment
(compare-and-swap, load-logical/store-conditional). It deals with conditionals
and loops but not yet arrays or heap.
  The logic uses a version of Hoare logic within threads, and a version of
rely/guarantee between threads, with five stability rules to handle various
kinds of parallelism (external, internal, propagation-free and two kinds of
in-flight parallelism). There are $\mathbb{B}$ and $\mathbb{U}$ modalities to
regulate propagation, and temporal modalities $\mathsf{since}$,
$\mathbb{S}\mathsf{ofar}$ and $\mathbb{O}\mathsf{uat}$ to deal with global
coherence (SC per location).
  The logic is presented by example. Proofs and unproofs of about thirty
weak-memory examples, including many litmus tests in various guises, are dealt
with in detail. There is a proof of a version of the token ring.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01418</identifier>
 <datestamp>2015-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01418</id><created>2015-10-12</created><authors><author><keyname>Ye</keyname><forenames>Cheng</forenames></author><author><keyname>Comin</keyname><forenames>Cesar H.</forenames></author><author><keyname>Peron</keyname><forenames>Thomas K. DM.</forenames></author><author><keyname>Silva</keyname><forenames>Filipi N.</forenames></author><author><keyname>Rodrigues</keyname><forenames>Francisco A.</forenames></author><author><keyname>Costa</keyname><forenames>Luciano da F.</forenames></author><author><keyname>Torsello</keyname><forenames>Andrea</forenames></author><author><keyname>Hancock</keyname><forenames>Edwin R.</forenames></author></authors><title>Thermodynamic characterization of networks using graph polynomials</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>16 pages, 12 figures. Published 25 September 2015</comments><doi>10.1103/PhysRevE.92.032810</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a method for characterizing the evolution of
time-varying complex networks by adopting a thermodynamic representation of
network structure computed from a polynomial (or algebraic) characterization of
graph structure. Commencing from a representation of graph structure based on a
characteristic polynomial computed from the normalized Laplacian matrix, we
show how the polynomial is linked to the Boltzmann partition function of a
network. This allows us to compute a number of thermodynamic quantities for the
network, including the average energy and entropy. Assuming that the system
does not change volume, we can also compute the temperature, defined as the
rate of change of entropy with energy. All three thermodynamic variables can be
approximated using low-order Taylor series that can be computed using the
traces of powers of the Laplacian matrix, avoiding explicit computation of the
normalized Laplacian spectrum. These polynomial approximations allow a smoothed
representation of the evolution of networks to be constructed in the
thermodynamic space spanned by entropy, energy, and temperature. We show how
these thermodynamic variables can be computed in terms of simple network
characteristics, e.g., the total number of nodes and node degree statistics for
nodes connected by edges. We apply the resulting thermodynamic characterization
to real-world time-varying networks representing complex systems in the
financial and biological domains. The study demonstrates that the method
provides an efficient tool for detecting abrupt changes and characterizing
different stages in network evolution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01420</identifier>
 <datestamp>2015-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01420</id><created>2015-10-30</created><authors><author><keyname>Yurdakul</keyname><forenames>Nazli</forenames></author><author><keyname>Berker</keyname><forenames>A. Nihat</forenames></author></authors><title>A Comparative Study of Interdisciplinarity in Sciences in South Korea,
  Turkey, and USA</title><categories>physics.soc-ph cs.DL physics.hist-ph</categories><comments>6 pages, 1 figure, 6 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A comparative study is done of interdisciplinary citations in 2013 between
physics, chemistry, and molecular biology, in South Korea, Turkey, and USA.
Several surprising conclusions emerge from our tabular and graphical analysis:
The interscience citation rates are in general strikingly similar, between
South Korea, Turkey, and USA. One apparent exception is the comparatively more
tenuous relation between molecular biology and physics in the USA. Other slight
exceptions are the higher amount of citing of physicists by chemists in South
Korea, of chemists by molecular biologists in Turkey, and of molecular
biologists by chemists in USA. Chemists are, by a sizable margin, the most
interscience citing scientists in this group of three sciences. Physicist are,
again by a sizable margin, the least interscience citing scientists in this
group of three sciences. The strongest interscience citation is from chemistry
to physics. The weakest interscience citation is from physics to molecular
biology. Our findings are consistent with a V-shaped backbone connectivity, as
opposed to a $\Delta$ connectivity, as also found in a previous study of
earlier citation years.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01432</identifier>
 <datestamp>2015-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01432</id><created>2015-11-29</created><authors><author><keyname>Liu</keyname><forenames>Jian-Guo</forenames></author><author><keyname>Hou</keyname><forenames>Lei</forenames></author><author><keyname>Pan</keyname><forenames>Xue</forenames></author><author><keyname>Guo</keyname><forenames>Qiang</forenames></author><author><keyname>Zhou</keyname><forenames>Tao</forenames></author></authors><title>Stability of similarity measurements for bipartite networks</title><categories>physics.soc-ph cs.SI</categories><comments>11 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Similarity is a fundamental measure in network analyses and machine learning
algorithms, with wide applications ranging from personalized recommendation to
socio-economic dynamics. We argue that an effective similarity measurement
should guarantee the stability even under some information loss. With six
bipartite networks, we investigate the stabilities of fifteen similarity
measurements by comparing the similarity matrixes of two data samples which are
randomly divided from original data sets. Results show that, the fifteen
measurements can be well classified into three clusters according to their
stabilities, and measurements in the same cluster have similar mathematical
definitions. In addition, we develop a top-$n$-stability method for
personalized recommendation, and find that the unstable similarities would
recommend false information to users, and the performance of recommendation
would be largely improved by using stable similarity measurements. This work
provides a novel dimension to analyze and evaluate similarity measurements,
which can further find applications in link prediction, personalized
recommendation, clustering algorithms, community detection and so on.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01435</identifier>
 <datestamp>2015-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01435</id><created>2015-11-25</created><authors><author><keyname>Ritchie</keyname><forenames>Martin</forenames></author><author><keyname>Berthouze</keyname><forenames>Luc</forenames></author><author><keyname>Kiss</keyname><forenames>Istvan Z</forenames></author></authors><title>Generation and analysis of networks with a prescribed degree sequence
  and subgraph family: Higher-order structure matters</title><categories>physics.soc-ph cs.SI q-bio.PE</categories><comments>30 pages, 11 figures, Matlab source code available from GitHub</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Designing algorithms that generate networks with a given degree sequence
while varying both subgraph composition and distribution of subgraphs around
nodes is an important but challenging research problem. Current algorithms lack
control of key network parameters, the ability to specify to what subgraphs a
node belongs to, come at a considerable complexity cost or, critically, sample
from a limited ensemble of networks. To enable controlled investigations of the
impact and role of subgraphs, especially for epidemics, neuronal activity or
complex contagion, it is essential that the generation process be versatile and
the generated networks as diverse as possible. In this paper, we present two
new network generation algorithms that use subgraphs as building blocks to
construct networks preserving a given degree sequence. Additionally, these
algorithms provide control over clustering both at node and global level. In
both cases, we show that, despite being constrained by a degree sequence and
global clustering, generated networks have markedly different topologies as
evidenced by both subgraph prevalence and distribution around nodes, and
large-scale network structure metrics such as path length and betweenness
measures. Simulations of standard epidemic and complex contagion models on
those networks reveal that degree distribution and global clustering do not
always accurately predict the outcome of dynamical processes taking place on
them. We conclude by discussing the benefits and limitations of both methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01436</identifier>
 <datestamp>2015-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01436</id><created>2015-11-26</created><authors><author><keyname>Halappanavar</keyname><forenames>Mahantesh</forenames><affiliation>Henry</affiliation></author><author><keyname>Cotilla-Sanchez</keyname><forenames>Eduardo</forenames><affiliation>Henry</affiliation></author><author><keyname>Hogan</keyname><forenames>Emilie</forenames><affiliation>Henry</affiliation></author><author><keyname>Duncan</keyname><forenames>Daniel</forenames><affiliation>Henry</affiliation></author><author><keyname>Zhenyu</keyname><affiliation>Henry</affiliation></author><author><keyname>Huang</keyname></author><author><keyname>Hines</keyname><forenames>Paul D. H.</forenames></author></authors><title>A Network-of-Networks Model for Electrical Infrastructure Networks</title><categories>physics.soc-ph cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modeling power transmission networks is an important area of research with
applications such as vulnerability analysis, study of cascading failures, and
location of measurement devices. Graph-theoretic approaches have been widely
used to solve these problems, but are subject to several limitations. One of
the limitations is the ability to model a heterogeneous system in a consistent
manner using the standard graph-theoretic formulation. In this paper, we
propose a {\em network-of-networks} approach for modeling power transmission
networks in order to explicitly incorporate heterogeneity in the model. This
model distinguishes between different components of the network that operate at
different voltage ratings, and also captures the intra and inter-network
connectivity patterns. By building the graph in this fashion we present a
novel, and fundamentally different, perspective of power transmission networks.
Consequently, this novel approach will have a significant impact on the
graph-theoretic modeling of power grids that we believe will lead to a better
understanding of transmission networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01438</identifier>
 <datestamp>2015-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01438</id><created>2015-12-02</created><authors><author><keyname>Kiselyov</keyname><forenames>Oleg</forenames><affiliation>Tohoku University, Japan</affiliation></author><author><keyname>Garrigue</keyname><forenames>Jacques</forenames><affiliation>Nagoya University, Japan</affiliation></author></authors><title>Proceedings ML Family/OCaml Users and Developers workshops</title><categories>cs.PL</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 198, 2015</journal-ref><doi>10.4204/EPTCS.198</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume collects the extended versions of selected papers originally
presented at the two ACM SIGPLAN workshops: ML Family Workshop 2014 and OCaml
2014. Both were affiliated with ICFP 2014 and took place on two consecutive
days, on September 4 and 5, 2014 in Gothenburg, Sweden.
  The ML Family workshop aims to recognize the entire extended family of ML and
ML-like languages: languages that are Higher-order, Typed, Inferred, and
Strict. It provides the forum to discuss common issues, both practical
(compilation techniques, implementations of concurrency and parallelism,
programming for the Web) and theoretical (fancy types, module systems,
metaprogramming). The scope of the workshop includes all aspects of the design,
semantics, theory, application, implementation, and teaching of the members of
the ML family.
  The OCaml workshop is more specifically targeted at the OCaml community, with
an emphasis on new proposals and tools aiming to improve OCaml, its
environment, and the functioning of the community. As such, it is interested in
works on the type system, language extensions, compiler and optimizations,
applications, tools, and experience reports of exciting uses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01448</identifier>
 <datestamp>2015-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01448</id><created>2015-12-04</created><authors><author><keyname>Gadouleau</keyname><forenames>Maximilien</forenames></author></authors><title>Maximum Rank and Asymptotic Rank of Finite Dynamical Systems</title><categories>math.CO cs.DM math.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A finite dynamical system is a system of multivariate functions over a finite
alphabet. The main feature of a finite dynamical system is its interaction
graph, which indicates which local functions depend on which variables. The
rank of a finite dynamical system is the cardinality of its image; the
asymptotic rank is the number of its periodic points. In this paper, we
determine the maximum rank and the maximum asymptotic rank of a finite
dynamical system with a given interaction graph over any non-Boolean alphabet.
We also obtain a similar result for Boolean finite dynamical systems whose
interaction graphs are contained in a given digraph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01462</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01462</id><created>2015-12-01</created><authors><author><keyname>Koteich</keyname><forenames>Mohamad</forenames></author><author><keyname>Maloum</keyname><forenames>Abdelmalek</forenames></author><author><keyname>Duc</keyname><forenames>Gilles</forenames></author><author><keyname>Sandou</keyname><forenames>Guillaume</forenames></author></authors><title>Discussion on &quot;AC Drive Observability Analysis&quot;</title><categories>cs.SY math.OC</categories><doi>10.1109/TIE.2015.2438777</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the paper by Vaclavek et al. (IEEE Trans. Ind. Electron., vol. 60, no. 8,
pp. 3047-3059, Aug. 2013), the local observability of both induction machine
and permanent-magnet synchronous machine (PMSM) under motion-sensorless
operation is studied. In this letter, the &quot;slowly varying&quot; speed assumption is
discussed, and the PMSM observability condition at standstill is revisited.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01465</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01465</id><created>2015-12-04</created><updated>2016-02-08</updated><authors><author><keyname>Amor</keyname><forenames>Selma Belhadj</forenames></author><author><keyname>Perlaza</keyname><forenames>Samir M.</forenames></author><author><keyname>Krikidis</keyname><forenames>Ioannis</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Feedback Enhances Simultaneous Wireless Information and Energy
  Transmission in Multiple Access Channels</title><categories>cs.IT math.IT</categories><comments>INRIA REPORT N{\deg}8804, submitted to IEEE transactions on
  Information Theory, November 15, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this report, the fundamental limits of simultaneous information and energy
transmission in the two-user Gaussian multiple access channel (G-MAC) with and
without feedback are fully characterized. All the achievable information and
energy transmission rates (in bits per channel use and energy-units per channel
use, respectively) are identified. Thus, the information-energy capacity region
is defined in both cases. In the case without feedback, an achievability scheme
based on power-splitting and successive interference cancelation is shown to be
optimal. Alternatively, in the case with feedback (G-MAC-F), a simple yet
optimal achievability scheme based on power-splitting and Ozarow's capacity
achieving scheme is presented. Three of the most important observations in this
work are: (a) The information-energy capacity region of the G-MAC without
feedback is a proper subset of the information-energy capacity region of the
G-MAC-F; (b) Feedback can at most double the energy rate for a fixed
information rate; and (c) Time-sharing with power control is strictly
suboptimal in terms of sum-rate in the G-MAC without feedback.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01478</identifier>
 <datestamp>2015-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01478</id><created>2015-12-04</created><authors><author><keyname>Munari</keyname><forenames>Andrea</forenames></author><author><keyname>M&#xe4;h&#xf6;nen</keyname><forenames>Petri</forenames></author><author><keyname>Petrova</keyname><forenames>Marina</forenames></author></authors><title>A Stochastic Geometry Framework for Asynchronous Full-Duplex Networks</title><categories>cs.NI cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In-band full-duplex is emerging as a promising solution to enhance throughput
in wireless networks. Allowing nodes to simultaneously send and receive data
over the same bandwidth can potentially double the system capacity, and a good
degree of maturity has been reached in terms of physical layer design with
practical demonstrations in simple topologies. However, the true potential of
full-duplex at a system level is yet to be fully understood. In this paper, we
introduce an analytical framework based on stochastic geometry that captures
the behaviour of large full-duplex networks implementing an asynchronous random
access policy. Via closed-form expressions we discuss the key tradeoffs that
characterise these systems, exploring among the rest the role of transmission
duration, imperfect self- interference cancellation and fraction of full-duplex
nodes in the network. We also provide protocol design principles, and our
comparison with slotted systems sheds light on the performance loss induced by
the lack of synchronism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01479</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01479</id><created>2015-12-04</created><updated>2016-01-18</updated><authors><author><keyname>Guarnieri</keyname><forenames>Marco</forenames></author><author><keyname>Marinovic</keyname><forenames>Srdjan</forenames></author><author><keyname>Basin</keyname><forenames>David</forenames></author></authors><title>Strong and Provably Secure Database Access Control</title><categories>cs.CR cs.DB</categories><comments>A short version of this paper has been published in the proceedings
  of the 1st IEEE European Symposium on Security and Privacy (EuroS&amp;P 2016)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Existing SQL access control mechanisms are extremely limited. Attackers can
leak information and escalate their privileges using advanced database features
such as views, triggers, and integrity constraints. This is not merely a
problem of vendors lagging behind the state-of-the-art. The theoretical
foundations for database security lack adequate security definitions and a
realistic attacker model, both of which are needed to evaluate the security of
modern databases. We address these issues and present a provably secure access
control mechanism that prevents attacks that defeat popular SQL database
systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01485</identifier>
 <datestamp>2015-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01485</id><created>2015-12-04</created><authors><author><keyname>Bose</keyname><forenames>Prosenjit</forenames></author><author><keyname>Verdonschot</keyname><forenames>Sander</forenames></author></authors><title>Flips in Edge-Labelled Pseudo-Triangulations</title><categories>cs.CG</categories><comments>15 pages, 10 figures, invited to a CGTA special issue for CCCG 2015.
  The paper is a stand-alone version of results that appeared in the second
  author's PhD thesis (arXiv:1509.02563 [cs.CG])</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that $O(n^2)$ exchanging flips suffice to transform any edge-labelled
pointed pseudo-triangulation into any other with the same set of labels. By
using insertion, deletion and exchanging flips, we can transform any
edge-labelled pseudo-triangulation into any other with $O(n \log c + h \log h)$
flips, where $c$ is the number of convex layers and $h$ is the number of points
on the convex hull.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01503</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01503</id><created>2015-12-04</created><updated>2015-12-27</updated><authors><author><keyname>Ha</keyname><forenames>Quang Minh</forenames></author><author><keyname>Deville</keyname><forenames>Yves</forenames></author><author><keyname>Pham</keyname><forenames>Quang Dung</forenames></author><author><keyname>H&#xe0;</keyname><forenames>Minh Ho&#xe0;ng</forenames></author></authors><title>On the Min-cost Traveling Salesman Problem with Drone</title><categories>cs.AI</categories><comments>23 pages. arXiv admin note: text overlap with arXiv:1509.08764</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Once known to be used exclusively in military domain, unmanned aerial
vehicles (drones) have stepped up to become a part of new logistic method in
commercial sector called &quot;last-mile delivery&quot;. In this novel approach, small
unmanned aerial vehicles (UAV), also known as drones, are deployed alongside
with trucks to deliver goods to customers in order to improve the service
quality or reduce the transportation cost. It gives rise to a new variant of
the traveling salesman problem (TSP), of which we call TSP with drone (TSP-D).
In this article, we consider a variant of TSP-D where the main objective is to
minimize the total transportation cost. We also propose two heuristics: &quot;Drone
First, Truck Second&quot; (DFTS) and &quot;Truck First, Drone Second&quot; (TFDS), to
effectively solve the problem. The former constructs route for drone first
while the latter constructs route for truck first. We solve a TSP to generate
route for truck and propose a mixed integer programming (MIP) formulation with
different profit functions to build route for drone. Numerical results obtained
on many instances with different sizes and characteristics are presented.
Recommendations on promising algorithm choices are also provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01515</identifier>
 <datestamp>2015-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01515</id><created>2015-12-04</created><authors><author><keyname>Litany</keyname><forenames>Or</forenames></author><author><keyname>Remez</keyname><forenames>Tal</forenames></author><author><keyname>Freedman</keyname><forenames>Daniel</forenames></author><author><keyname>Shapira</keyname><forenames>Lior</forenames></author><author><keyname>Bronstein</keyname><forenames>Alex</forenames></author><author><keyname>Gal</keyname><forenames>Ran</forenames></author></authors><title>ASIST: Automatic Semantically Invariant Scene Transformation</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present ASIST, a technique for transforming point clouds by replacing
objects with their semantically equivalent counterparts. Transformations of
this kind have applications in virtual reality, repair of fused scans, and
robotics. ASIST is based on a unified formulation of semantic labeling and
object replacement; both result from minimizing a single objective. We present
numerical tools for the e?cient solution of this optimization problem. The
method is experimentally assessed on new datasets of both synthetic and real
point clouds, and is additionally compared to two recent works on object
replacement on data from the corresponding papers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01516</identifier>
 <datestamp>2015-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01516</id><created>2015-12-04</created><authors><author><keyname>Avendi</keyname><forenames>M. R.</forenames></author><author><keyname>Nguyen</keyname><forenames>Ha H.</forenames></author><author><keyname>Ha</keyname><forenames>Dac-Binh</forenames></author></authors><title>Differential Amplify-and-Forward Relaying Using Linear Combining in
  Time-Varying Channels</title><categories>cs.IT math.IT</categories><comments>IEEE International Conference on Computing, Management and
  Telecommunications (ComManTel), 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Differential encoding and decoding can be employed to circumvent channel
estimation in wireless relay networks. This article studies differential
amplify-and-forward relaying using linear combining with arbitrary fixed
combining weights in time-varying channels. An exact bit error rate (BER)
analysis is obtained for this system using DBPSK modulation and over
time-varying Rayleigh fading channels. The analysis is verified with simulation
results for several sets of combining weights and in various fading scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01525</identifier>
 <datestamp>2015-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01525</id><created>2015-12-04</created><authors><author><keyname>Yang</keyname><forenames>Yezhou</forenames></author><author><keyname>Aloimonos</keyname><forenames>Yiannis</forenames></author><author><keyname>Fermuller</keyname><forenames>Cornelia</forenames></author><author><keyname>Aksoy</keyname><forenames>Eren Erdal</forenames></author></authors><title>Learning the Semantics of Manipulation Action</title><categories>cs.RO cs.CL cs.CV</categories><journal-ref>The 53rd Annual Meeting of the Association for Computational
  Linguistics (ACL) 1 (2015) 676-686</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a formal computational framework for modeling
manipulation actions. The introduced formalism leads to semantics of
manipulation action and has applications to both observing and understanding
human manipulation actions as well as executing them with a robotic mechanism
(e.g. a humanoid robot). It is based on a Combinatory Categorial Grammar. The
goal of the introduced framework is to: (1) represent manipulation actions with
both syntax and semantic parts, where the semantic part employs
$\lambda$-calculus; (2) enable a probabilistic semantic parsing schema to learn
the $\lambda$-calculus representation of manipulation action from an annotated
action corpus of videos; (3) use (1) and (2) to develop a system that visually
observes manipulation actions and understands their meaning while it can reason
beyond observations using propositional logic and axiom schemata. The
experiments conducted on a public available large manipulation action dataset
validate the theoretical framework and our implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01533</identifier>
 <datestamp>2015-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01533</id><created>2015-12-04</created><authors><author><keyname>Goudeseune</keyname><forenames>Camille</forenames></author></authors><title>Motion trails from time-lapse video</title><categories>cs.CV</categories><comments>7 pages, 6 figures</comments><acm-class>I.3.3; I.4.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  From an image sequence captured by a stationary camera, background
subtraction can detect moving foreground objects in the scene. Distinguishing
foreground from background is further improved by various heuristics. Then each
object's motion can be emphasized by duplicating its positions as a motion
trail. These trails clarify the objects' spatial relationships. Also, adding
motion trails to a video before previewing it at high speed reduces the risk of
overlooking transient events.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01537</identifier>
 <datestamp>2015-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01537</id><created>2015-12-04</created><authors><author><keyname>Braylan</keyname><forenames>Alexander</forenames></author><author><keyname>Hollenbeck</keyname><forenames>Mark</forenames></author><author><keyname>Meyerson</keyname><forenames>Elliot</forenames></author><author><keyname>Miikkulainen</keyname><forenames>Risto</forenames></author></authors><title>Reuse of Neural Modules for General Video Game Playing</title><categories>cs.NE cs.AI</categories><comments>Accepted at AAAI 16</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A general approach to knowledge transfer is introduced in which an agent
controlled by a neural network adapts how it reuses existing networks as it
learns in a new domain. Networks trained for a new domain can improve their
performance by routing activation selectively through previously learned neural
structure, regardless of how or for what it was learned. A neuroevolution
implementation of this approach is presented with application to
high-dimensional sequential decision-making domains. This approach is more
general than previous approaches to neural transfer for reinforcement learning.
It is domain-agnostic and requires no prior assumptions about the nature of
task relatedness or mappings. The method is analyzed in a stochastic version of
the Arcade Learning Environment, demonstrating that it improves performance in
some of the more complex Atari 2600 games, and that the success of transfer can
be predicted based on a high-level characterization of game dynamics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01563</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01563</id><created>2015-12-04</created><authors><author><keyname>Liang</keyname><forenames>Yitao</forenames></author><author><keyname>Machado</keyname><forenames>Marlos C.</forenames></author><author><keyname>Talvitie</keyname><forenames>Erik</forenames></author><author><keyname>Bowling</keyname><forenames>Michael</forenames></author></authors><title>State of the Art Control of Atari Games Using Shallow Reinforcement
  Learning</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recently introduced Deep Q-Networks (DQN) algorithm has gained attention
as one of the first successful combinations of deep neural networks and
reinforcement learning. Its promise was demonstrated in the Arcade Learning
Environment (ALE), a challenging framework composed of dozens of Atari 2600
games used to evaluate general competency in AI. It achieved dramatically
better results than earlier approaches, showing that its ability to learn good
representations is quite robust and general. This paper attempts to understand
the principles that underly DQN's impressive performance and to better
contextualize its success. We systematically evaluate the importance of key
representational biases encoded by DQN's network by proposing simple linear
representations that make use of these concepts. Incorporating these
characteristics, we obtain a computationally practical feature set that
achieves competitive performance to DQN in the ALE. Besides offering insight
into the strengths and weaknesses of DQN, we provide a generic representation
for the ALE, significantly reducing the burden of learning a representation for
each game. Moreover, we also provide a simple, reproducible benchmark for the
sake of comparison to future work in the ALE.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01568</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01568</id><created>2015-12-02</created><authors><author><keyname>Govada</keyname><forenames>Aruna</forenames></author><author><keyname>Joshi</keyname><forenames>Pravin</forenames></author><author><keyname>Mittal</keyname><forenames>Sahil</forenames></author><author><keyname>Sahay</keyname><forenames>Sanjay K</forenames></author></authors><title>Hybrid Approach for Inductive Semi Supervised Learning using Label
  Propagation and Support Vector Machine</title><categories>cs.LG cs.DC</categories><comments>Presented in the 11th International Conference, MLDM, Germany, July
  20 - 21, 2015. Springer, Machine Learning and Data Mining in Pattern
  Recognition, LNAI Vol. 9166, p. 199-213, 2015</comments><doi>10.1007/978-3-319-21024-7_14</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Semi supervised learning methods have gained importance in today's world
because of large expenses and time involved in labeling the unlabeled data by
human experts. The proposed hybrid approach uses SVM and Label Propagation to
label the unlabeled data. In the process, at each step SVM is trained to
minimize the error and thus improve the prediction quality. Experiments are
conducted by using SVM and logistic regression(Logreg). Results prove that SVM
performs tremendously better than Logreg. The approach is tested using 12
datasets of different sizes ranging from the order of 1000s to the order of
10000s. Results show that the proposed approach outperforms Label Propagation
by a large margin with F-measure of almost twice on average. The parallel
version of the proposed approach is also designed and implemented, the analysis
shows that the training time decreases significantly when parallel version is
used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01569</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01569</id><created>2015-12-03</created><authors><author><keyname>Iacus</keyname><forenames>Stefano Maria</forenames></author><author><keyname>Porro</keyname><forenames>Giuseppe</forenames></author><author><keyname>Salini</keyname><forenames>Silvia</forenames></author><author><keyname>Siletti</keyname><forenames>Elena</forenames></author></authors><title>Social networks, happiness and health: from sentiment analysis to a
  multidimensional indicator of subjective well-being</title><categories>stat.AP cs.CY cs.SI physics.soc-ph</categories><comments>26 pages, 5 figure</comments><msc-class>91D30, 97K80</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper applies a novel technique of opinion analysis over social media
data with the aim of proposing a new indicator of perceived and subjective
well-being. This new index, namely SWBI, examines several dimension of
individual and social life. The indicator has been compared to some other
existing indexes of well-being and health conditions in Italy: the BES
(Benessere Equo Sostenibile), the incidence rate of influenza and the abundance
of PM10 in urban environments. SWBI is a daily measure available at province
level. BES data, currently available only for 2013 and 2014, are annual and
available at regional level. Flu data are weekly and distributed as regional
data and PM10 are collected daily for different cities. Due to the fact that
the time scale and space granularity of the different indexes varies, we apply
a novel statistical technique to discover nowcasting features and the classical
latent analysis to study the relationships among them. A preliminary analysis
suggest that the environmental and health conditions anticipate several
dimensions of the perception of well-being as measured by SWBI. Moreover, the
set of indicators included in the BES represent a latent dimension of
well-being which shares similarities with the latent dimension represented by
SWBI.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01573</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01573</id><created>2015-12-04</created><updated>2015-12-08</updated><authors><author><keyname>Ruet</keyname><forenames>Paul</forenames></author></authors><title>Negative local feedbacks in Boolean networks</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the relationships between negative local cycles and asymptotic
dynamical properties of Boolean networks. The two main results are the
following: we show that and-nets without local negative cycle may have no fixed
point, and that Boolean networks without local negative cycle may have
antipodal attractive cycles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01581</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01581</id><created>2015-12-04</created><authors><author><keyname>Iyengar</keyname><forenames>Anirudh</forenames></author><author><keyname>Ghosh</keyname><forenames>Swaroop</forenames></author></authors><title>Threshold Voltage-Defined Switches for Programmable Gates</title><categories>cs.CR cs.AR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Semiconductor supply chain is increasingly getting exposed to variety of
security attacks such as Trojan insertion, cloning, counterfeiting, reverse
engineering (RE), piracy of Intellectual Property (IP) or Integrated Circuit
(IC) and side-channel analysis due to involvement of untrusted parties. In this
paper, we propose transistor threshold voltage-defined switches to camouflage
the logic gate both logically and physically to resist against RE and IP
piracy. The proposed gate can function as NAND, AND, NOR, OR, XOR, XNOR, INV
and BUF robustly using threshold-defined switches. The camouflaged design
operates at nominal voltage and obeys conventional reliability limits. The
proposed gate can also be used to personalize the design during manufacturing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01585</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01585</id><created>2015-12-04</created><updated>2016-02-23</updated><authors><author><keyname>Thomas</keyname><forenames>Timothy A.</forenames></author><author><keyname>Rybakowski</keyname><forenames>Marcin</forenames></author><author><keyname>Sun</keyname><forenames>Shu</forenames></author><author><keyname>Rappaport</keyname><forenames>Theodore S.</forenames></author><author><keyname>Nguyen</keyname><forenames>Huan</forenames></author><author><keyname>Kovacs</keyname><forenames>Istvan Z.</forenames></author><author><keyname>Rodriguez</keyname><forenames>Ignacio</forenames></author></authors><title>A Prediction Study of Path Loss Models from 2-73.5 GHz in an Urban-Macro
  Environment</title><categories>cs.IT math.IT</categories><comments>5 pages, 8 figures,in 2016 IEEE 83rd Vehicular Technology Conference
  (Spring VTC-2016), May 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is becoming clear that 5G wireless systems will encompass frequencies from
around 500 MHz all the way to around 100 GHz. To adequately assess the
performance of 5G systems in these different bands, path loss (PL) models will
need to be developed across this wide frequency range. The PL models can
roughly be broken into two categories, ones that have some anchor in physics,
and ones that curve-match only over the data set without any physical anchor.
In this paper we use both real-world measurements from 2 to 28 GHz and
ray-tracing studies from 2 to 73.5 GHz, both in an urban-macro environment, to
assess the prediction performance of the two PL modeling techniques. In other
words, we look at how the two different PL modeling techniques perform when the
PL model is applied to a prediction set which is different in distance,
frequency, or environment from a measurement set where the parameters of the
respective models are determined. We show that a PL model with a physical
anchor point can be a better predictor of PL performance in the prediction sets
while also providing a parameterization which is more stable over a substantial
number of different measurement sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01587</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01587</id><created>2015-12-04</created><authors><author><keyname>Garg</keyname><forenames>Sahil</forenames></author><author><keyname>Galstyan</keyname><forenames>Aram</forenames></author><author><keyname>Hermjakob</keyname><forenames>Ulf</forenames></author><author><keyname>Marcu</keyname><forenames>Daniel</forenames></author></authors><title>Extracting Biomolecular Interactions Using Semantic Parsing of
  Biomedical Text</title><categories>cs.CL cs.AI cs.IR cs.IT cs.LG math.IT</categories><comments>Appearing in Proceedings of the Thirtieth AAAI Conference on
  Artificial Intelligence (AAAI-16)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We advance the state of the art in biomolecular interaction extraction with
three contributions: (i) We show that deep, Abstract Meaning Representations
(AMR) significantly improve the accuracy of a biomolecular interaction
extraction system when compared to a baseline that relies solely on surface-
and syntax-based features; (ii) In contrast with previous approaches that infer
relations on a sentence-by-sentence basis, we expand our framework to enable
consistent predictions over sets of sentences (documents); (iii) We further
modify and expand a graph kernel learning framework to enable concurrent
exploitation of automatically induced AMR (semantic) and dependency structure
(syntactic) representations. Our experiments show that our approach yields
interaction extraction systems that are more robust in environments where there
is a significant mismatch between training and test conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01594</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01594</id><created>2015-12-04</created><authors><author><keyname>Sommars</keyname><forenames>Jeff</forenames></author><author><keyname>Verschelde</keyname><forenames>Jan</forenames></author></authors><title>Exact Gift Wrapping to Prune the Tree of Edges of Newton Polytopes to
  Compute Pretropisms</title><categories>cs.CG</categories><comments>exact, gift wrapping, Newton polytope, pretropism, tree pruning</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pretropisms are candidates for the leading exponents of Puiseux series that
represent solutions of polynomial systems. To find pretropisms, we propose an
exact gift wrapping algorithm to prune the tree of edges of a tuple of Newton
polytopes. We prefer exact arithmetic not only because of the exact input and
the degrees of the output, but because of the often unpredictable growth of the
coordinates in the face normals, even for polytopes in generic position. We
provide experimental results with our preliminary implementation in Sage that
compare favorably with the pruning method that relies only on cone
intersections.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01596</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01596</id><created>2015-12-04</created><authors><author><keyname>Turchenko</keyname><forenames>Volodymyr</forenames></author><author><keyname>Luczak</keyname><forenames>Artur</forenames></author></authors><title>Creation of a Deep Convolutional Auto-Encoder in Caffe</title><categories>cs.NE cs.CV cs.LG</categories><comments>8 pages, 7 figures, 5 tables, 28 references in the list</comments><msc-class>68Txx</msc-class><acm-class>F.1.1; I.2.6; I.5.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The development of a deep (stacked) convolutional auto-encoder in the Caffe
deep learning framework is presented in this paper. We describe simple
principles which we used to create this model in Caffe. The proposed model of
convolutional auto-encoder does not have pooling/unpooling layers yet. The
results of our experimental research show comparable accuracy of dimensionality
reduction in comparison with a classic auto-encoder on the example of MNIST
dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01601</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01601</id><created>2015-12-04</created><authors><author><keyname>Ruan</keyname><forenames>H.</forenames></author><author><keyname>de Lamare</keyname><forenames>R. C.</forenames></author></authors><title>Study of Efficient Robust Adaptive Beamforming Algorithms Based on
  Shrinkage Techniques</title><categories>cs.IT math.IT</categories><comments>9 pages, 2 figures. arXiv admin note: text overlap with
  arXiv:1505.06788</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes low-complexity robust adaptive beamforming (RAB)
techniques based on shrinkage methods. We firstly briefly review a
Low-Complexity Shrinkage-Based Mismatch Estimation (LOCSME) batch algorithm to
estimate the desired signal steering vector mismatch, in which the
interference-plus-noise covariance (INC) matrix is also estimated with a
recursive matrix shrinkage method. Then we develop low complexity adaptive
robust version of the conjugate gradient (CG) algorithm to both estimate the
steering vector mismatch and update the beamforming weights. A computational
complexity study of the proposed and existing algorithms is carried out.
Simulations are conducted in local scattering scenarios and comparisons to
existing RAB techniques are provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01603</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01603</id><created>2015-12-04</created><authors><author><keyname>O'Donnell</keyname><forenames>Ryan</forenames></author><author><keyname>Zhao</keyname><forenames>Yu</forenames></author></authors><title>Polynomial bounds for decoupling, with applications</title><categories>cs.DM math.PR</categories><comments>19 pages, including bibliography</comments><msc-class>60C05, 68Q87, 68Q17</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let f(x) = f(x_1, ..., x_n) = \sum_{|S| &lt;= k} a_S \prod_{i \in S} x_i be an
n-variate real multilinear polynomial of degree at most k, where S \subseteq
[n] = {1, 2, ..., n}. For its &quot;one-block decoupled&quot; version,
  f~(y,z) = \sum_{|S| &lt;= k} a_S \sum_{i \in S} y_i \prod_{j \in S\i} z_j,
  we show tail-bound comparisons of the form
  Pr[|f~(y,z)| &gt; C_k t] &lt;= D_k Pr[f(x) &gt; t].
  Our constants C_k, D_k are significantly better than those known for &quot;full
decoupling&quot;. For example, when x, y, z are independent Gaussians we obtain C_k
= D_k = O(k); when x, y, z, Rademacher random variables we obtain C_k = O(k^2),
D_k = k^{O(k)}. By contrast, for full decoupling only C_k = D_k = k^{O(k)} is
known in these settings.
  We describe consequences of these results for query complexity (related to
conjectures of Aaronson and Ambainis) and for analysis of Boolean functions
(including an optimal sharpening of the DFKO Inequality).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01609</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01609</id><created>2015-12-04</created><authors><author><keyname>Wei</keyname><forenames>Xiaohan</forenames></author><author><keyname>Neely</keyname><forenames>Michael J.</forenames></author></authors><title>Data Center Server Provision: Distributed Asynchronous Control for
  Coupled Renewal Systems</title><categories>cs.DC cs.PF</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers a cost minimization problem for data centers with $N$
servers and randomly arriving service requests. A central router decides which
server to use for each new request. Each server has three types of states
(active, idle, setup) with different costs and time durations. The servers
operate asynchronously over their own states and can choose one of multiple
sleep modes when idle. We develop an online distributed control algorithm so
that each server makes its own decisions and the overall time average cost is
near optimal with probability 1. The algorithm does not need probability
information for the arrival rate or job sizes. Next, an improved algorithm that
uses a single queue is developed via a &quot;virtualization&quot; technique. The
improvement is shown to provide the same (near optimal) costs while
significantly reducing delay, as shown in simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01613</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01613</id><created>2015-12-04</created><updated>2015-12-08</updated><authors><author><keyname>Mao</keyname><forenames>Wei-Hao</forenames></author><author><keyname>Gao</keyname><forenames>Fei</forenames></author><author><keyname>Dong</keyname><forenames>Yi-Jin</forenames></author><author><keyname>Li</keyname><forenames>Wen-Ming</forenames></author></authors><title>A Novel Paradigm for Calculating Ramsey Number via Artificial Bee Colony
  Algorithm</title><categories>cs.AI cs.NE math.CO math.OC</categories><comments>25 pages, 6 figures</comments><msc-class>90C59, 05C55, 05C30, 68R10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Ramsey number is of vital importance in Ramsey's theorem. This paper
proposed a novel methodology for constructing Ramsey graphs about R(3,10),
which uses Artificial Bee Colony optimization(ABC) to raise the lower bound of
Ramsey number R(3,10). The r(3,10)-graph contains two limitations, that is,
neither complete graphs of order 3 nor independent sets of order 10. To resolve
these limitations, a special mathematical model is put in the paradigm to
convert the problems into discrete optimization whose smaller minimizers are
correspondent to bigger lower bound as approximation of inf R(3,10). To
demonstrate the potential of the proposed method, simulations are done to to
minimize the amount of these two types of graphs. For the first time, four
r(3,9,39) graphs with best approximation for inf R(3,10) are reported in
simulations to support the current lower bound for R(3,10). The experiments'
results show that the proposed paradigm for Ramsey number's calculation driven
by ABC is a successful method with the advantages of high precision and
robustness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01615</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01615</id><created>2015-12-04</created><updated>2016-01-31</updated><authors><author><keyname>Yan</keyname><forenames>Xiao-Yong</forenames></author><author><keyname>Wang</keyname><forenames>Wen-Xu</forenames></author><author><keyname>Chen</keyname><forenames>Guan-Rong</forenames></author><author><keyname>Shi</keyname><forenames>Ding-Hua</forenames></author></authors><title>Multiplex congruence network of natural numbers</title><categories>math.NT cs.SI physics.soc-ph</categories><comments>7 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Congruence theory has many applications in physical, social, biological and
technological systems. Congruence arithmetic has been a fundamental tool for
data security and computer algebra. However, much less attention was devoted to
the topological features of congruence relations among natural numbers. Here,
we explore the congruence relations in the setting of a multiplex network and
unveil some unique and outstanding properties of the multiplex congruence
network. Analytical results show that every layer therein is a sparse and
heterogeneous subnetwork with a scale-free topology. Counterintuitively, every
layer has an extremely strong controllability in spite of its scale-free
structure that is usually difficult to control. Another amazing feature is that
the controllability is robust against targeted attacks to critical nodes but
vulnerable to random failures, which also differs from normal scale-free
networks. The multi-chain structure with a small number of chain roots arising
from each layer accounts for the strong controllability and the abnormal
feature. The multiplex congruence network offers a graphical solution to the
simultaneous congruences problem, which may have implication in cryptography
based on simultaneous congruences. Our work also gains insight into the design
of networks integrating advantages of both heterogeneous and homogeneous
networks without inheriting their limitations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01621</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01621</id><created>2015-12-04</created><authors><author><keyname>Fomin</keyname><forenames>Fedor V.</forenames></author><author><keyname>Gaspers</keyname><forenames>Serge</forenames></author><author><keyname>Lokshtanov</keyname><forenames>Daniel</forenames></author><author><keyname>Saurabh</keyname><forenames>Saket</forenames></author></authors><title>Exact Algorithms via Monotone Local Search</title><categories>cs.DS cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a new general approach for designing exact exponential-time
algorithms for subset problems. In a subset problem the input implicitly
describes a family of sets over a universe of size n and the task is to
determine whether the family contains at least one set. Our approach is based
on &quot;monotone local search&quot;, where the goal is to extend a partial solution to a
solution by adding as few elements as possible. More formally, in the extension
problem we are also given as input a subset X of the universe and an integer k.
The task is to determine whether one can add at most k elements to X to obtain
a set in the (implicitly defined) family. Our main result is that an O*(c^k)
time algorithm for the extension problem immediately yields a randomized
algorithm for finding a solution of any size with running time O*((2-1/c)^n).
  In many cases, the extension problem can be reduced to simply finding a
solution of size at most k. Furthermore, efficient algorithms for finding small
solutions have been extensively studied in the field of parameterized
algorithms. Directly applying these algorithms, our theorem yields in one
stroke significant improvements over the best known exponential-time algorithms
for several well-studied problems, including d-Hitting Set, Feedback Vertex
Set, Node Unique Label Cover, and Weighted d-SAT. Our results demonstrate an
interesting and very concrete connection between parameterized algorithms and
exact exponential-time algorithms.
  We also show how to derandomize our algorithms at the cost of a
subexponential multiplicative factor in the running time. Our derandomization
is based on an efficient construction of a new pseudo-random object that might
be of independent interest. Finally, we extend our methods to establish new
combinatorial upper bounds and develop enumeration algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01625</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01625</id><created>2015-12-05</created><authors><author><keyname>Li</keyname><forenames>Songze</forenames></author><author><keyname>Maddah-Ali</keyname><forenames>Mohammad Ali</forenames></author><author><keyname>Avestimehr</keyname><forenames>A. Salman</forenames></author></authors><title>Coded MapReduce</title><categories>cs.DC cs.IT math.IT</categories><comments>16 pages, 6 figures. Parts of this work were presented in 53rd
  Allerton Conference, Sept. 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  MapReduce is a commonly used framework for executing data-intensive jobs on
distributed server clusters. We introduce a variant implementation of
MapReduce, namely &quot;Coded MapReduce&quot;, to substantially reduce the inter-server
communication load for the shuffling phase of MapReduce, and thus accelerating
its execution. The proposed Coded MapReduce exploits the repetitive mapping of
data blocks at different servers to create coding opportunities in the
shuffling phase to exchange (key,value) pairs among servers much more
efficiently. We demonstrate that Coded MapReduce can cut down the total
inter-server communication load by a multiplicative factor that grows linearly
with the number of servers in the system and it achieves the minimum
communication load within a constant multiplicative factor. We also analyze the
tradeoff between the &quot;computation load&quot; and the &quot;communication load&quot; of Coded
MapReduce.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01629</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01629</id><created>2015-12-05</created><authors><author><keyname>Chow</keyname><forenames>Yinlam</forenames></author><author><keyname>Ghavamzadeh</keyname><forenames>Mohammad</forenames></author><author><keyname>Janson</keyname><forenames>Lucas</forenames></author><author><keyname>Pavone</keyname><forenames>Marco</forenames></author></authors><title>Risk-Constrained Reinforcement Learning with Percentile Risk Criteria</title><categories>cs.AI cs.LG math.OC</categories><comments>arXiv admin note: substantial text overlap with arXiv:1406.3339</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many sequential decision-making problems one is interested in minimizing
an expected cumulative cost while taking into account \emph{risk}, i.e.,
increased awareness of events of small probability and high consequences.
Accordingly, the objective of this paper is to present efficient reinforcement
learning algorithms for risk-constrained Markov decision processes (MDPs),
where risk is represented via a chance constraint or a constraint on the
conditional value-at-risk (CVaR) of the cumulative cost. We collectively refer
to such problems as percentile risk-constrained MDPs.
  Specifically, we first derive a formula for computing the gradient of the
Lagrangian function for percentile risk-constrained MDPs. Then, we devise
policy gradient and actor-critic algorithms that (1) estimate such gradient,
(2) update the policy parameters in the descent direction, and (3) update the
Lagrange multiplier in the ascent direction. For these algorithms we prove
convergence to locally-optimal policies. Finally, we demonstrate the
effectiveness of our algorithms in an optimal stopping problem and an online
marketing application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01630</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01630</id><created>2015-12-05</created><authors><author><keyname>Zhang</keyname><forenames>Qian</forenames></author><author><keyname>Jiang</keyname><forenames>Ying</forenames></author><author><keyname>Wu</keyname><forenames>Peng</forenames></author></authors><title>Modelling and Analysis of Network Security - an Algebraic Approach</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Game theory has been applied to investigate network security. But different
security scenarios were often modeled via different types of games and analyzed
in an ad-hoc manner. In this paper, we propose an algebraic approach for
modeling and analyzing uniformly several types of network security games. This
approach is based on a probabilistic extension of the value-passing Calculus of
Communicating Systems (CCS) which is regarded as a Generative model for
Probabilistic Value-passing CCS (PVCCSG for short). Our approach gives a
uniform framework, called PVCCSG based security model, for the security
scenarios modeled via perfect and complete or incomplete information games. We
present then a uniform algorithm for computing the Nash equilibria strategies
of a network security game on its PVCCSG based security model. The algorithm
first generates a transition system for each of the PVCCSG based security
models, then simplifies this transition system through graph-theoretic
abstraction and bisimulation minimization. Then, a backward induction method,
which is only applicable to finite tree models, can be used to compute all the
Nash equilibria strategies of the (possibly infinite) security games. This
algorithm is implemented and can also be tuned smoothly for computing its
social optimal strategies. The effectiveness and efficiency of this approach
are further demonstrated with four detailed case studies from the field of
network security.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01639</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01639</id><created>2015-12-05</created><authors><author><keyname>Wo&#x142;k</keyname><forenames>Krzysztof</forenames></author><author><keyname>Marasek</keyname><forenames>Krzysztof</forenames></author></authors><title>PJAIT Systems for the IWSLT 2015 Evaluation Campaign Enhanced by
  Comparable Corpora</title><categories>cs.CL stat.ML</categories><journal-ref>Proceedings of the 12th International Workshop on Spoken Language
  Translation, Da Nang, Vietnam, December 3-4, 2015, p.101-104</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we attempt to improve Statistical Machine Translation (SMT)
systems on a very diverse set of language pairs (in both directions): Czech -
English, Vietnamese - English, French - English and German - English. To
accomplish this, we performed translation model training, created adaptations
of training settings for each language pair, and obtained comparable corpora
for our SMT systems. Innovative tools and data adaptation techniques were
employed. The TED parallel text corpora for the IWSLT 2015 evaluation campaign
were used to train language models, and to develop, tune, and test the system.
In addition, we prepared Wikipedia-based comparable corpora for use with our
SMT system. This data was specified as permissible for the IWSLT 2015
evaluation. We explored the use of domain adaptation techniques, symmetrized
word alignment models, the unsupervised transliteration models and the KenLM
language modeling tool. To evaluate the effects of different preparations on
translation results, we conducted experiments and used the BLEU, NIST and TER
metrics. Our results indicate that our approach produced a positive impact on
SMT quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01641</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01641</id><created>2015-12-05</created><authors><author><keyname>Wo&#x142;k</keyname><forenames>Krzysztof</forenames></author><author><keyname>Marasek</keyname><forenames>Krzysztof</forenames></author></authors><title>Unsupervised comparable corpora preparation and exploration for
  bi-lingual translation equivalents</title><categories>cs.CL stat.ML</categories><comments>arXiv admin note: text overlap with arXiv:1509.08639</comments><journal-ref>Proceedings of the 12th IWSLT, Da Nang, Vietnam, December 3-4,
  2015, p.118-125</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The multilingual nature of the world makes translation a crucial requirement
today. Parallel dictionaries constructed by humans are a widely-available
resource, but they are limited and do not provide enough coverage for good
quality translation purposes, due to out-of-vocabulary words and neologisms.
This motivates the use of statistical translation systems, which are
unfortunately dependent on the quantity and quality of training data. Such
systems have a very limited availability especially for some languages and very
narrow text domains. In this research we present our improvements to current
comparable corpora mining methodologies by re- implementation of the comparison
algorithms (using Needleman-Wunch algorithm), introduction of a tuning script
and computation time improvement by GPU acceleration. Experiments are carried
out on bilingual data extracted from the Wikipedia, on various domains. For the
Wikipedia itself, additional cross-lingual comparison heuristics were
introduced. The modifications made a positive impact on the quality and
quantity of mined data and on the translation quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01642</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01642</id><created>2015-12-05</created><authors><author><keyname>Lin</keyname><forenames>Liang</forenames></author><author><keyname>Wang</keyname><forenames>Keze</forenames></author><author><keyname>Zuo</keyname><forenames>Wangmeng</forenames></author><author><keyname>Wang</keyname><forenames>Meng</forenames></author><author><keyname>Luo</keyname><forenames>Jiebo</forenames></author><author><keyname>Zhang</keyname><forenames>Lei</forenames></author></authors><title>A Deep Structured Model with Radius-Margin Bound for 3D Human Activity
  Recognition</title><categories>cs.CV</categories><comments>16 pages, 9 figures, to appear in International Journal of Computer
  Vision 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding human activity is very challenging even with the recently
developed 3D/depth sensors. To solve this problem, this work investigates a
novel deep structured model, which adaptively decomposes an activity instance
into temporal parts using the convolutional neural networks (CNNs). Our model
advances the traditional deep learning approaches in two aspects. First, { we
incorporate latent temporal structure into the deep model, accounting for large
temporal variations of diverse human activities. In particular, we utilize the
latent variables to decompose the input activity into a number of temporally
segmented sub-activities, and accordingly feed them into the parts (i.e.
sub-networks) of the deep architecture}. Second, we incorporate a radius-margin
bound as a regularization term into our deep model, which effectively improves
the generalization performance for classification. For model training, we
propose a principled learning algorithm that iteratively (i) discovers the
optimal latent variables (i.e. the ways of activity decomposition) for all
training instances, (ii) { updates the classifiers} based on the generated
features, and (iii) updates the parameters of multi-layer neural networks. In
the experiments, our approach is validated on several complex scenarios for
human activity recognition and demonstrates superior performances over other
state-of-the-art approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01655</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01655</id><created>2015-12-05</created><updated>2015-12-08</updated><authors><author><keyname>Pezzotti</keyname><forenames>Nicola</forenames></author><author><keyname>Lelieveldt</keyname><forenames>Boudewijn P. F.</forenames></author><author><keyname>van der Maaten</keyname><forenames>Laurens</forenames></author><author><keyname>H&#xf6;llt</keyname><forenames>Thomas</forenames></author><author><keyname>Eisemann</keyname><forenames>Elmar</forenames></author><author><keyname>Vilanova</keyname><forenames>Anna</forenames></author></authors><title>Approximated and User Steerable tSNE for Progressive Visual Analytics</title><categories>cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Progressive Visual Analytics aims at improving the interactivity in existing
analytics techniques by means of visualization as well as interaction with
intermediate results. One key method for data analysis is dimensionality
reduction, for example, to produce 2D embeddings that can be visualized and
analyzed efficiently. t-Distributed Stochastic Neighbor Embedding (tSNE) is a
well-suited technique for the visualization of several high-dimensional data.
tSNE can create meaningful intermediate results but suffers from a slow
initialization that constrains its application in Progressive Visual Analytics.
We introduce a controllable tSNE approximation (A-tSNE), which trades off speed
and accuracy, to enable interactive data exploration. We offer real-time
visualization techniques, including a density-based solution and a Magic Lens
to inspect the degree of approximation. With this feedback, the user can decide
on local refinements and steer the approximation level during the analysis. We
demonstrate our technique with several datasets, in a real-world research
scenario and for the real-time analysis of high-dimensional streams to
illustrate its effectiveness for interactive data analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01668</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01668</id><created>2015-12-05</created><authors><author><keyname>Dong</keyname><forenames>Zhao Yu</forenames></author></authors><title>A Framework for Computing on Large Dynamic Graphs</title><categories>cs.DC</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This proposal presents a graph computing framework intending to support both
online and offline computing on large dynamic graphs efficiently. The framework
proposes a new data model to support rich evolving vertex and edge data types.
It employs a replica-coherence protocol to improve data locality thus can adapt
to data access patterns of different algorithms. A new computing model called
protocol dataflow is proposed to implement and integrate various programming
models for both online and offline computing on large dynamic graphs. A central
topic of the proposal is also the analysis of large real dynamic graphs using
our proposed framework. Our goal is to calculate the temporal patterns and
properties which emerge when the large graphs keep evolving. Thus we can
evaluate the capability of the proposed framework. Key words: Large dynamic
graph, programming model, distributed computing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01680</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01680</id><created>2015-12-05</created><authors><author><keyname>Afghah</keyname><forenames>Fatemeh</forenames></author><author><keyname>Razi</keyname><forenames>Abolfazl</forenames></author><author><keyname>Najarian</keyname><forenames>Kayvan</forenames></author></authors><title>A Shapley Value Solution to Game Theoretic-based Feature Reduction in
  False Alarm Detection</title><categories>cs.CV</categories><comments>Neural Information Processing Systems (NIPS'15), Workshop on Machine
  Learning in Healthcare</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  False alarm is one of the main concerns in intensive care units and can
result in care disruption, sleep deprivation, and insensitivity of care-givers
to alarms. Several methods have been proposed to suppress the false alarm rate
through improving the quality of physiological signals by filtering, and
developing more accurate sensors. However, significant intrinsic correlation
among the extracted features limits the performance of most currently available
data mining techniques, as they often discard the predictors with low
individual impact that may potentially have strong discriminatory power when
grouped with others. We propose a model based on coalition game theory that
considers the inter-features dependencies in determining the salient predictors
in respect to false alarm, which results in improved classification accuracy.
The superior performance of this method compared to current methods is shown in
simulation results using PhysionNet's MIMIC II database.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01681</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01681</id><created>2015-12-05</created><authors><author><keyname>Gogacz</keyname><forenames>Tomasz</forenames></author><author><keyname>Marcinkowski</keyname><forenames>Jerzy</forenames></author></authors><title>Red Spider Meets a Rainworm: Conjunctive Query Finite Determinacy Is
  Undecidable</title><categories>cs.DB</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  We solve a well known and long-standing open problem in database theory,
proving that Conjunctive Query Finite Determinacy Problem is undecidable. The
technique we use builds on the top of our Red Spider method which we developed
in our paper [GM15] to show undecidability of the same problem in the
&quot;unrestricted case&quot; -- when database instances are allowed to be infinite. We
also show a specific instance $Q_0$, ${\cal Q}= \{Q_1, Q_2, \ldots Q_k\}$ such
that the set $\cal Q$ of CQs does not determine CQ $Q_0$ but finitely
determines it. Finally, we claim that while $Q_0$ is finitely determined by
$\cal Q$, there is no FO-rewriting of $Q_0$, with respect to $\cal Q$, and we
outline a proof of this claim
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01683</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01683</id><created>2015-12-05</created><authors><author><keyname>Choi</keyname><forenames>Jun Won</forenames></author><author><keyname>Shim</keyname><forenames>Byonghyo</forenames></author></authors><title>Sparse Detection of Non-Sparse Signals for Large-Scale Wireless Systems</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce a new detection algorithm for large-scale
wireless systems, referred to as post sparse error detection (PSED) algorithm,
that employs a sparse error recovery algorithm to refine the estimate of a
symbol vector obtained by the conventional linear detector. The PSED algorithm
operates in two steps: 1) sparse transformation converting the original
non-sparse system into the sparse system whose input is an error vector caused
by the symbol slicing and 2) estimation of the error vector using the sparse
recovery algorithm. From the asymptotic mean square error (MSE) analysis and
empirical simulations performed on large-scale systems, we show that the PSED
algorithm brings significant performance gain over classical linear detectors
while imposing relatively small computational overhead.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01688</identifier>
 <datestamp>2016-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01688</id><created>2015-12-05</created><authors><author><keyname>Thelwall</keyname><forenames>Mike</forenames></author></authors><title>The precision of the arithmetic mean, geometric mean and percentiles for
  citation data: An experimental simulation modelling approach</title><categories>cs.DL</categories><comments>Thelwall, M. (in press). The precision of the arithmetic mean,
  geometric mean and percentiles for citation data: An experimental simulation
  modelling approach. Journal of Informetrics</comments><doi>10.1016/j.joi.2015.12.001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When comparing the citation impact of nations, departments or other groups of
researchers within individual fields, three approaches have been proposed:
arithmetic means, geometric means, and percentage in the top X%. This article
compares the precision of these statistics using 97 trillion experimentally
simulated citation counts from 6875 sets of different parameters (although all
having the same scale parameter) based upon the discretised lognormal
distribution with limits from 1000 repetitions for each parameter set. The
results show that the geometric mean is the most precise, closely followed by
the percentage of a country's articles in the top 50% most cited articles for a
field, year and document type. Thus the geometric mean citation count is
recommended for future citation-based comparisons between nations. The
percentage of a country's articles in the top 1% most cited is a particularly
imprecise indicator and is not recommended for international comparisons based
on individual fields. Moreover, whereas standard confidence interval formulae
for the geometric mean appear to be accurate, confidence interval formulae are
less accurate and consistent for percentile indicators. These recommendations
assume that the scale parameters of the samples are the same but the choice of
indicator is complex and partly conceptual if they are not.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01690</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01690</id><created>2015-12-05</created><authors><author><keyname>Soshnikov</keyname><forenames>Dmitri</forenames></author></authors><title>Using Functional Programming for Development of Distributed, Cloud and
  Web Applications in F#</title><categories>cs.PL</categories><comments>Presented at CEE-SECR 2011, Moscow, Russia, see
  http://2011.secr.ru/lang/en-en/talks/using-functional-programming-and-f</comments><acm-class>D.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we argue that modern functional programming languages - in
particular, FSharp on the .NET platform - are well suited for the development
of distributed, web and cloud applications on the Internet. We emphasize that
FSharp can be successfully used in a range of scenarios - starting from simple
ASP.NET web applications, and including cloud data processing tasks and
data-driven web applications. In particular, we show how some of the FSharp
features (eg. quotations) can be effectively used to develop a distributed web
system using single code-base, and describe the commercial WebSharper project
by Intellifactory for building distributed client-server web applications, as
well as research library that uses Windows Azure for parametric sweep
computational tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01691</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01691</id><created>2015-12-05</created><authors><author><keyname>Pandey</keyname><forenames>Rohit Kumar</forenames></author><author><keyname>Zhou</keyname><forenames>Yingbo</forenames></author><author><keyname>Kota</keyname><forenames>Bhargava Urala</forenames></author><author><keyname>Govindaraju</keyname><forenames>Venu</forenames></author></authors><title>Maximum Entropy Binary Encoding for Face Template Protection</title><categories>cs.CV</categories><comments>arXiv admin note: text overlap with arXiv:1506.04340</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a framework for secure identification using deep
neural networks, and apply it to the task of template protection for face
authentication. We use deep convolutional neural networks (CNNs) to learn a
mapping from face images to maximum entropy binary (MEB) codes. The mapping is
robust enough to tackle the problem of exact matching, yielding the same code
for new samples of a user as the code assigned during training. These codes are
then hashed using any hash function that follows the random oracle model (like
SHA-512) to generate protected face templates (similar to text based password
protection). The algorithm makes no unrealistic assumptions and offers high
template security, cancelability, and state-of-the-art matching performance.
The efficacy of the approach is shown on CMU-PIE, Extended Yale B, and
Multi-PIE face databases. We achieve high (~95%) genuine accept rates (GAR) at
zero false accept rate (FAR) with up to 1024 bits of template security.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01693</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01693</id><created>2015-12-05</created><authors><author><keyname>Sorokin</keyname><forenames>Ivan</forenames></author><author><keyname>Seleznev</keyname><forenames>Alexey</forenames></author><author><keyname>Pavlov</keyname><forenames>Mikhail</forenames></author><author><keyname>Fedorov</keyname><forenames>Aleksandr</forenames></author><author><keyname>Ignateva</keyname><forenames>Anastasiia</forenames></author></authors><title>Deep Attention Recurrent Q-Network</title><categories>cs.LG</categories><comments>7 pages, 5 figures, Deep Reinforcement Learning Workshop, NIPS 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A deep learning approach to reinforcement learning led to a general learner
able to train on visual input to play a variety of arcade games at the human
and superhuman levels. Its creators at the Google DeepMind's team called the
approach: Deep Q-Network (DQN). We present an extension of DQN by &quot;soft&quot; and
&quot;hard&quot; attention mechanisms. Tests of the proposed Deep Attention Recurrent
Q-Network (DARQN) algorithm on multiple Atari 2600 games show level of
performance superior to that of DQN. Moreover, built-in attention mechanisms
allow a direct online monitoring of the training process by highlighting the
regions of the game screen the agent is focusing on when making decisions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01699</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01699</id><created>2015-12-05</created><authors><author><keyname>Mehrabi</keyname><forenames>Saeed</forenames></author><author><keyname>Mehrabi</keyname><forenames>Abbas</forenames></author></authors><title>A Note on Approximating 2-Transmitters</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A k-transmitter in a simple orthogonal polygon P is a mobile guard that
travels back and forth along an orthogonal line segment s inside P. The
k-transmitter can see a point p in P if there exists a point q on s such that
the line segment pq is normal to s and pq intersects the boundary of P in at
most k points. In this paper, we give a 2-approximation algorithm for the
problem of guarding a monotone orthogonal polygon with the minimum number of
2-transmitters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01700</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01700</id><created>2015-12-05</created><authors><author><keyname>Bendich</keyname><forenames>Paul</forenames></author><author><keyname>Bubenik</keyname><forenames>Peter</forenames></author></authors><title>Stabilizing the output of persistent homology computations</title><categories>cs.CG math.AT</categories><comments>16 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a general technique for extracting a larger set of stable
information from persistent homology computations than is currently done. The
persistent homology algorithm is generally seen as a procedure which starts
with a filtered complex and ends with a persistence diagram. This procedure is
stable (at least to certain types of pertur- bations of the input), and so
using the diagram as a signature of the input - for example, deriving features
from it that are then used in machine learning - is a well-justified con- cept.
However, these computations also produce a great deal of other potentially
useful but unstable information: for example, each point in the diagram
corresponds to a specific positive simplex. In addition, the persistence
diagram is not stable with respect to other procedures that are employed in
practice, such as thresholding a point cloud by density. We recast these
problems as real-valued functions which are discontinuous but measur- able, and
then observe that convolving such a function with a Lipschitz function produces
a Lipschitz function. The resulting stable function is easily estimated. We
illustrate this approach with numerous examples and several experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01701</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01701</id><created>2015-12-05</created><authors><author><keyname>Ahmat</keyname><forenames>Kamal</forenames></author></authors><title>Emerging Cloud Computing Security Threats</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud computing is one of the latest emerging innovations of the modern
internet and technological landscape. With everyone from the White house to
major online technological leaders like Amazon and Google using or offering
cloud computing services it is truly presents itself as an exciting and
innovative method to store and use data on the internet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01708</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01708</id><created>2015-12-05</created><authors><author><keyname>De</keyname><forenames>Soham</forenames></author><author><keyname>Taylor</keyname><forenames>Gavin</forenames></author><author><keyname>Goldstein</keyname><forenames>Tom</forenames></author></authors><title>Variance Reduction for Distributed Stochastic Gradient Descent</title><categories>cs.LG cs.DC math.OC stat.ML</categories><comments>Preprint</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Variance reduction (VR) methods boost the performance of stochastic gradient
descent (SGD) by enabling the use of larger stepsizes and preserving linear
convergence rates. However, current variance reduced SGD methods require either
high memory usage or require a full pass over the (large) data set at the end
of each epoch to calculate the exact gradient of the objective function. This
makes current VR methods impractical in distributed or parallel settings. In
this paper, we propose a variance reduction method, called VR-lite, that does
not require full gradient computations or extra storage. We explore distributed
synchronous and asynchronous variants with both high and low communication
latency. We find that our distributed algorithms scale linearly with the number
of local workers and remain stable even with low communication frequency. We
empirically compare both the sequential and distributed algorithms to
state-of-the-art stochastic optimization methods, and find that our proposed
algorithms consistently converge faster than other stochastic methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01712</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01712</id><created>2015-12-05</created><authors><author><keyname>Lopyrev</keyname><forenames>Konstantin</forenames></author></authors><title>Generating News Headlines with Recurrent Neural Networks</title><categories>cs.CL cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe an application of an encoder-decoder recurrent neural network
with LSTM units and attention to generating headlines from the text of news
articles. We find that the model is quite effective at concisely paraphrasing
news articles. Furthermore, we study how the neural network decides which input
words to pay attention to, and specifically we identify the function of the
different neurons in a simplified attention mechanism. Interestingly, our
simplified attention mechanism performs better that the more complex attention
mechanism on a held out set of articles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01713</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01713</id><created>2015-12-05</created><authors><author><keyname>Rahul</keyname><forenames>Saladi</forenames></author></authors><title>Approximate Range Counting Revisited</title><categories>cs.CG cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work presents new results on approximate range counting. If the actual
count is $k$, then the data structures in this paper output a value, $\tau$,
lying in the range $[(1-\varepsilon)k,(1+\varepsilon)k]$. The main results are
the following:
  (1) A new technique for efficiently solving any approximate range counting
problem is presented. This technique can be viewed as an enhancement of Aronov
and Har-Peled's technique [SIAM Journal of Computing, 2008]. The key reasons
are the following:
  (a) The new technique is sensitive to the value of $k$: As an application,
this work presents a structure for approximate halfspace range counting in
$R^d, d\geq 4$, which occupies $O(n)$ space and solves the query in roughly
$O\left((n/k)^{1-1/\lfloor d/2\rfloor}\right)$ time. When $k=\Theta(n)$, then
the query time is roughly $\tilde{O}(1)$.
  (b) The new technique handles colored range searching problems: As an
application, the orthogonal colored range counting problem is solved. Existing
structures for exact counting use $O(n^d)$ space to answer the query in
$O(\text{polylog } n)$ query time. Therefore, if one is willing for an
approximation, an attractive result is obtained: an $O(n \text{ polylog } n)$
space data structure and an $O(\text{polylog } n)$ query time algorithm.
  (2) An optimal solution for some approximate rectangle stabbing counting
problems in $R^2$. This is achieved by a non-trivial reduction to planar point
location.
  (3) Finally, an efficient solution is obtained for $3$-sided orthogonal
colored range counting. The result is obtained by a non-trivial combination of
two different types of random sampling techniques and a reduction to
non-colored range searching problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01715</identifier>
 <datestamp>2015-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01715</id><created>2015-12-05</created><updated>2015-12-16</updated><authors><author><keyname>Qi</keyname><forenames>Hang</forenames></author><author><keyname>Wu</keyname><forenames>Tianfu</forenames></author><author><keyname>Lee</keyname><forenames>Mun-Wai</forenames></author><author><keyname>Zhu</keyname><forenames>Song-Chun</forenames></author></authors><title>A Restricted Visual Turing Test for Deep Scene and Event Understanding</title><categories>cs.CV cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a restricted visual Turing test (VTT) for story-line
based deep understanding in long-term and multi-camera captured videos. Given a
set of videos of a scene (such as a multi-room office, a garden, and a parking
lot.) and a sequence of story-line based queries, the task is to provide
answers either simply in binary form &quot;true/false&quot; (to a polar query) or in an
accurate natural language description (to a non-polar query). Queries, polar or
non-polar, consist of view-based queries which can be answered from a
particular camera view and scene-centered queries which involves joint
inference across different cameras. The story lines are collected to cover
spatial, temporal and causal understanding of input videos. The data and
queries distinguish our VTT from recently proposed visual question answering in
images and video captioning. A vision system is proposed to perform joint video
and query parsing which integrates different vision modules, a knowledge base
and a query engine. The system provides unified interfaces for different
modules so that individual modules can be reconfigured to test a new method. We
provide a benchmark dataset and a toolkit for ontology guided story-line query
generation which consists of about 93.5 hours videos captured in four different
locations and 3,426 queries split into 127 story lines. We also provide a
baseline implementation and result analyses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01717</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01717</id><created>2015-12-05</created><authors><author><keyname>Bartholdi</keyname><forenames>Laurent</forenames></author></authors><title>Algorithmic decidability of Engel's property for automaton groups</title><categories>cs.FL math.GR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider decidability problems associated with Engel's identity
($[\cdots[[x,y],y],\dots,y]=1$ for a long enough commutator sequence) in groups
generated by an automaton. We give a partial algorithm that decides, given
$x,y$, whether an Engel identity is satisfied. It succeeds, importantly, in
proving that Grigorchuk's $2$-group is not Engel. We consider next the problem
of recognizing Engel elements, namely elements $y$ such that the map
$x\mapsto[x,y]$ attracts to $\{1\}$. Although this problem seems intractable in
general, we prove that it is decidable for Grigorchuk's group: Engel elements
are precisely those of order at most $2$. Our computations were implemented
using the package FR within the computer algebra system GAP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01722</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01722</id><created>2015-12-05</created><authors><author><keyname>Borji</keyname><forenames>Ali</forenames></author><author><keyname>Feng</keyname><forenames>Mengyang</forenames></author></authors><title>Vanishing point attracts gaze in free-viewing and visual search tasks</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To investigate whether the vanishing point (VP) plays a significant role in
gaze guidance, we ran two experiments. In the first one, we recorded fixations
of 10 observers (4 female; mean age 22; SD=0.84) freely viewing 532 images, out
of which 319 had VP (shuffled presentation; each image for 4 secs). We found
that the average number of fixations at a local region (80x80 pixels) centered
at the VP is significantly higher than the average fixations at random
locations (t-test; n=319; p=1.8e-35). To address the confounding factor of
saliency, we learned a combined model of bottom-up saliency and VP. AUC score
of our model (0.85; SD=0.01) is significantly higher than the original saliency
model (e.g., 0.8 using AIM model by Bruce &amp; Tsotsos (2009), t-test; p=
3.14e-16) and the VP-only model (0.64, t-test; p= 4.02e-22). In the second
experiment, we asked 14 subjects (4 female, mean age 23.07, SD=1.26) to search
for a target character (T or L) placed randomly on a 3x3 imaginary grid
overlaid on top of an image. Subjects reported their answers by pressing one of
two keys. Stimuli consisted of 270 color images (180 with a single VP, 90
without). The target happened with equal probability inside each cell (15 times
L, 15 times T). We found that subjects were significantly faster (and more
accurate) when target happened inside the cell containing the VP compared to
cells without VP (median across 14 subjects 1.34 sec vs. 1.96; Wilcoxon
rank-sum test; p = 0.0014). Response time at VP cells were also significantly
lower than response time on images without VP (median 2.37; p= 4.77e-05). These
findings support the hypothesis that vanishing point, similar to face and text
(Cerf et al., 2009) as well as gaze direction (Borji et al., 2014) attracts
attention in free-viewing and visual search.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01725</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01725</id><created>2015-12-05</created><updated>2016-02-28</updated><authors><author><keyname>Heaberlin</keyname><forenames>Bradi</forenames></author><author><keyname>DeDeo</keyname><forenames>Simon</forenames></author></authors><title>The Evolution of Wikipedia's Norm Network</title><categories>cs.SI cs.CY physics.soc-ph q-bio.PE</categories><comments>25 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social norms have traditionally been difficult to quantify. In any particular
society, their sheer number and complex interdependencies often limit a
system-level analysis. One exception is that of the network of norms that
sustain the online Wikipedia community. We study the fifteen year evolution of
this network using the interconnected set of pages that establish, describe,
and interpret the community's norms. Despite Wikipedia's reputation for ad hoc
governance, we find that its normative evolution is highly conservative. The
earliest users create norms that both dominate the network and persist over
time. These core norms govern both content and interpersonal interactions using
abstract principles such as neutrality, verifiability, and assume good faith.
As the network grows, norm neighborhoods decouple topologically from each
other, while increasing in semantic coherence. Taken together, these results
suggest that the evolution of Wikipedia's norm network is akin to bureaucratic
systems that predate the information age.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01727</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01727</id><created>2015-12-05</created><authors><author><keyname>Shea</keyname><forenames>Daniel</forenames></author></authors><title>Solving the Subset Sum Problem with Heap-Ordered Subset Trees</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the field of computer science, one of the more well-known algorithmic
exercises is that of the subset sum problem. That is, given a set of integers,
determine whether one or more integers in the set can sum to a target value.
Aside from the brute-force approach of trying every possible combination of
integers, many creative solutions have emerged over the years, ranging from
clever uses of various data structures to computationally-efficient
approximation solutions. Here, a unique approach is discussed which builds upon
the existing min-heap solution for positive integers, extending the methodology
to encompass all integers using a variation of the binomial heap.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01728</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01728</id><created>2015-12-05</created><authors><author><keyname>Qian</keyname><forenames>Qi</forenames></author><author><keyname>Baytas</keyname><forenames>Inci M.</forenames></author><author><keyname>Jin</keyname><forenames>Rong</forenames></author><author><keyname>Jain</keyname><forenames>Anil</forenames></author><author><keyname>Zhu</keyname><forenames>Shenghuo</forenames></author></authors><title>Similarity Learning via Adaptive Regression and Its Application to Image
  Retrieval</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of similarity learning and its application to image
retrieval with large-scale data. The similarity between pairs of images can be
measured by the distances between their high dimensional representations, and
the problem of learning the appropriate similarity is often addressed by
distance metric learning. However, distance metric learning requires the
learned metric to be a PSD matrix, which is computational expensive and not
necessary for retrieval ranking problem. On the other hand, the bilinear model
is shown to be more flexible for large-scale image retrieval task, hence, we
adopt it to learn a matrix for estimating pairwise similarities under the
regression framework. By adaptively updating the target matrix in regression,
we can mimic the hinge loss, which is more appropriate for similarity learning
problem. Although the regression problem can have the closed-form solution, the
computational cost can be very expensive. The computational challenges come
from two aspects: the number of images can be very large and image features
have high dimensionality. We address the first challenge by compressing the
data by a randomized algorithm with the theoretical guarantee. For the high
dimensional issue, we address it by taking low rank assumption and applying
alternating method to obtain the partial matrix, which has a global optimal
solution. Empirical studies on real world image datasets (i.e., Caltech and
ImageNet) demonstrate the effectiveness and efficiency of the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01730</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01730</id><created>2015-11-30</created><authors><author><keyname>Benson</keyname><forenames>Karen</forenames></author><author><keyname>Rahman</keyname><forenames>Shawon</forenames></author></authors><title>Security Risks in Mechanical Engineering Industries</title><categories>cs.CR cs.CY</categories><doi>10.5121/ijcses.2011.2306</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inherent in any organization are security risks and barriers that must be
understood, analyzed, and minimized in order to prepare for and perpetuate
future growth and return on investment within the business. Likewise, company
leaders must determine the security health of the organization and routinely
review the potential threats that are ever changing in this new global economy.
Once these risks are outlined, the cost and potential damage must be weighed
before action is implemented. This paper will address the modern problems of
securing information technology (IT) of a mechanical engineering enterprise,
which can be applied to other modern industries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01731</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01731</id><created>2015-11-30</created><authors><author><keyname>Donahue</keyname><forenames>Kimmarie</forenames></author><author><keyname>Rahman</keyname><forenames>Shawon</forenames></author></authors><title>Healthcare IT: Is your Information at Risk?</title><categories>cs.CR cs.CY</categories><doi>10.5121/ijnsa.2012.4508</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Healthcare Information Technology (IT) has made great advances over the past
few years and while these advances have enable healthcare professionals to
provide higher quality healthcare to a larger number of individuals it also
provides the criminal element more opportunities to access sensitive
information, such as patient protected health information (PHI) and Personal
identification Information (PII). Having an Information Assurance (IA)
programallows for the protection of information and information systems and
ensures the organization is in compliance with all requires regulations, laws
and directive is essential. While most organizations have such a policy in
place, often it is inadequate to ensure the proper protection to prevent
security breaches. The increase of data breaches in the last few years
demonstrates the importance of an effective IA program. To ensure an effective
IA policy, the policy must manage the operational risk, including identifying
risks, assessment and mitigation of identified risks and ongoing monitoring to
ensure compliance
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01738</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01738</id><created>2015-11-03</created><updated>2016-01-26</updated><authors><author><keyname>Ghanem</keyname><forenames>Samah A. M.</forenames></author></authors><title>Network Coding: Connections Between Information Theory And Estimation
  Theory</title><categories>cs.IT math.IT</categories><comments>IEEE Wireless Communications and Networking Conference (WCNC), April,
  2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we prove the existence of fundamental relations between
information theory and estimation theory for network-coded flows. When the
network is represented by a directed graph G=(V, E) and under the assumption of
uncorrelated noise over information flows between the directed links connecting
transmitters, switches (relays), and receivers. We unveil that there yet exist
closed-form relations for the gradient of the mutual information with respect
to different components of the system matrix M. On the one hand, this result
opens a new class of problems casting further insights into effects of the
network topology, topological changes when nodes are mobile, and the impact of
errors and delays in certain links into the network capacity which can be
further studied in scenarios where one source multi-sinks multicasts and
multi-source multicast where the invertibility and the rank of matrix M plays a
significant role in the decoding process and therefore, on the network
capacity. On the other hand, it opens further research questions of finding
precoding solutions adapted to the network level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01748</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01748</id><created>2015-12-06</created><authors><author><keyname>Zhang</keyname><forenames>Ying</forenames></author></authors><title>Restricted Low-Rank Approximation via ADMM</title><categories>cs.NA cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The matrix low-rank approximation problem with additional convex constraints
can find many applications and has been extensively studied before. However,
this problem is shown to be nonconvex and NP-hard; most of the existing
solutions are heuristic and application-dependent. In this paper, we show that,
other than tons of application in current literature, this problem can be used
to recover a feasible solution for SDP relaxation. By some sophisticated
tricks, it can be equivalently posed in an appropriate form for the Alternating
Direction Method of Multipliers (ADMM) to solve. The two updates of ADMM
include the basic matrix low-rank approximation and projection onto a convex
set. Different from the general non-convex problems, the sub-problems in each
step of ADMM can be solved exactly and efficiently in spite of their
non-convexity. Moreover, the algorithm will converge exponentially under proper
conditions. The simulation results confirm its superiority over existing
solutions. We believe that the results in this paper provide a useful tool for
this important problem and will help to extend the application of ADMM to the
non-convex regime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01749</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01749</id><created>2015-12-06</created><authors><author><keyname>Viswanatha</keyname><forenames>Kumar</forenames></author><author><keyname>Akyol</keyname><forenames>Emrah</forenames></author><author><keyname>Rose</keyname><forenames>Kenneth</forenames></author></authors><title>Combinatorial Message Sharing and a New Achievable Region for Multiple
  Descriptions</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new achievable rate-distortion region for the general L
channel multiple descriptions problem. A well known general region for this
problem is due to Venkataramani, Kramer and Goyal (VKG) [1]. Their encoding
scheme is an extension of the El-Gamal-Cover (EC) and Zhang- Berger (ZB) coding
schemes to the L channel case and includes a combinatorial number of refinement
codebooks, one for each subset of the descriptions. As in ZB, the scheme also
allows for a single common codeword to be shared by all descriptions. This
paper proposes a novel encoding technique involving Combinatorial Message
Sharing (CMS), where every subset of the descriptions may share a distinct
common message. This introduces a combinatorial number of shared codebooks
along with the refinement codebooks of [1]. We derive an achievable
rate-distortion region for the proposed technique, and show that it subsumes
the VKG region for general sources and distortion measures. We further show
that CMS provides a strict improvement of the achievable region for any source
and distortion measures for which some 2-description subset is such that ZB
achieves points outside the EC region. We then show a more surprising result:
CMS outperforms VKG for a general class of sources and distortion measures,
including scenarios where the ZB and EC regions coincide for all 2-description
subsets. In particular, we show that CMS strictly improves on VKG, for the
L-channel quadratic Gaussian MD problem, for all L greater than or equal to 3,
despite the fact that the EC region is complete for the corresponding
2-descriptions problem. Using the encoding principles derived, we show that the
CMS scheme achieves the complete rate-distortion region for several asymmetric
cross-sections of the L-channel quadratic Gaussian MD problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01751</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01751</id><created>2015-12-06</created><authors><author><keyname>Johnny</keyname><forenames>Milad</forenames></author><author><keyname>Aref</keyname><forenames>Mohammad Reza</forenames></author></authors><title>Interference Alignment for the K-user Interference Channel with
  Imperfect CSI</title><categories>cs.IT math.IT</categories><comments>41 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we explore the information-theoretic aspects of interference
alignment and its relation to channel state information (CSI). For the $K-$user
interference channel using different changing patterns between different users,
we propose several methods to align some parts of interferences and to increase
what is achieved by time sharing method. For more practical case when all the
channel links connected to the same destination have the same changing pattern,
we find an upper-bound and analyze it for the large interference channel
network. This result shows that when the size of the network increases, the
upper-bound value goes to $\frac{\sqrt{K}}{2}$. For the fast fading channel
when all the channels have the same changing pattern, we show that when the
direct links have different characteristic functions (channel permutation or
memory), in the absence of half part of CSI (cross links) at both transmitters
and receivers, one can achieve $K/2$ degrees-of-freedom (DoF). Also by the
converse proof we show that this is the minimum channel information to achieve
maximum DoF of $\frac{K}{2}$. Throughout this work, this fact has been
pinpointed to prove statements about more general partial state CSI and
achievable DoF. In other words, for the 3-user fully connected interference
channel we find out while $\frac{3}{2}$ lies in achievable DoF, we don't need
to know half part of the CSI. Also, the result has been extended to a more
general form for $K-$user interference channel and through the converse proof,
its functionality on channel state is proved to be optimum.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01752</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01752</id><created>2015-12-06</created><authors><author><keyname>Ravi</keyname><forenames>Sujith</forenames></author><author><keyname>Diao</keyname><forenames>Qiming</forenames></author></authors><title>Large Scale Distributed Semi-Supervised Learning Using Streaming
  Approximation</title><categories>cs.LG cs.AI</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional graph-based semi-supervised learning (SSL) approaches, even
though widely applied, are not suited for massive data and large label
scenarios since they scale linearly with the number of edges $|E|$ and distinct
labels $m$. To deal with the large label size problem, recent works propose
sketch-based methods to approximate the distribution on labels per node thereby
achieving a space reduction from $O(m)$ to $O(\log m)$, under certain
conditions. In this paper, we present a novel streaming graph-based SSL
approximation that captures the sparsity of the label distribution and ensures
the algorithm propagates labels accurately, and further reduces the space
complexity per node to $O(1)$. We also provide a distributed version of the
algorithm that scales well to large data sizes. Experiments on real-world
datasets demonstrate that the new method achieves better performance than
existing state-of-the-art algorithms with significant reduction in memory
footprint. We also study different graph construction mechanisms for natural
language applications and propose a robust graph augmentation strategy trained
using state-of-the-art unsupervised deep learning architectures that yields
further significant quality gains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01755</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01755</id><created>2015-12-06</created><authors><author><keyname>Joshi</keyname><forenames>Sumedh M.</forenames></author><author><keyname>Diamessis</keyname><forenames>Peter J.</forenames></author><author><keyname>Steinmoeller</keyname><forenames>Derek T.</forenames></author><author><keyname>Stastna</keyname><forenames>Marek</forenames></author><author><keyname>Thomsen</keyname><forenames>Greg N.</forenames></author></authors><title>A post-processing technique for stabilizing the discontinuous pressure
  projection operator in marginally-resolved incompressible inviscid flow</title><categories>physics.comp-ph cs.CE math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A method for post-processing the velocity after a pressure projection is
developed that helps to maintain stability in an under-resolved, inviscid,
discontinuous element-based simulation for use in environmental fluid mechanics
process studies. The post-processing method is needed because of spurious
divergence growth at element interfaces due to the discontinuous nature of the
discretization used. This spurious divergence eventually leads to a numerical
instability. Previous work has shown that a discontinuous element-local
projection onto the space of divergence-free basis functions is capable of
stabilizing the projection method, but the discontinuity inherent in this
technique may lead to instability in under-resolved simulations. By enforcing
inter-element discontinuity and requiring a divergence-free result in the weak
sense only, a new post-processing technique is developed that simultaneously
improves smoothness and reduces divergence in the pressure-projected velocity
field at the same time. When compared against a non-post-processed velocity
field, the post-processed velocity field remains stable far longer and exhibits
better smoothness and conservation properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01764</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01764</id><created>2015-12-06</created><authors><author><keyname>Szczepa&#x144;ski</keyname><forenames>Piotr Lech</forenames></author></authors><title>Fast Algorithms for Game-Theoretic Centrality Measures</title><categories>cs.GT cs.AI cs.SI</categories><comments>Doctoral Dissertation at Warsaw University of Technology, Faculty of
  Electronics and Information Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this dissertation, we analyze the computational properties of
game-theoretic centrality measures. The key idea behind game-theoretic approach
to network analysis is to treat nodes as players in a cooperative game, where
the value of each coalition of nodes is determined by certain graph properties.
Next, the centrality of any individual node is determined by a chosen
game-theoretic solution concept (notably, the Shapley value) in the same way as
the payoff of a player in a cooperative game. On one hand, the advantage of
game-theoretic centrality measures is that nodes are ranked not only according
to their individual roles but also according to how they contribute to the role
played by all possible subsets of nodes. On the other hand, the disadvantage is
that the game-theoretic solution concepts are typically computationally
challenging. The main contribution of this dissertation is that we show that a
wide variety of game-theoretic solution concepts on networks can be computed in
polynomial time. Our focus is on centralities based on the Shapley value and
its various extensions, such as the Semivalues and Coalitional Semivalues.
Furthermore, we prove #P-hardness of computing the Shapley value in
connectivity games and propose an algorithm to compute it. Finally, we analyse
computational properties of generalized version of cooperative games in which
order of player matters. We propose a new representation for such games, called
generalized marginal contribution networks, that allows for polynomial
computation in the size of the representation of two dedicated extensions of
the Shapley value to this class of games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01767</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01767</id><created>2015-12-06</created><updated>2015-12-09</updated><authors><author><keyname>Jeong</keyname><forenames>Cheol</forenames></author><author><keyname>Shin</keyname><forenames>Won-Yong</forenames></author></authors><title>GreenInfra: Capacity of Large-Scale Hybrid Networks With Cost-Effective
  Infrastructure</title><categories>cs.IT cs.NI math.IT</categories><comments>25 pages, 13 figures, 3 tables, Under Review for Possible Publication
  in IEEE Journal on Selected Areas in Communications (Special Issue on Green
  Communications and Networking: Second Issue)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The cost-effective impact and fundamental limits of infrastructure support
with rate-limited wired backhaul links (i.e., GreenInfra support), directly
connecting base stations (BSs), are analyzed in a large-scale hybrid network of
unit node density, where multi-antenna BSs are deployed. We consider a general
scenario such that the rate of each BS-to-BS link scales at an arbitrary rate
relative to the number of randomly located wireless nodes, $n$. For the
operating regimes with respect to the number of BSs and the number of antennas
at each BS, we first analyze the minimum rate of each backhaul link,
$C_{\textrm{BS}}$, required to guarantee the same throughput scaling as in the
infinite-capacity backhaul link case. We then identify the operating regimes in
which the required rate $C_{\textrm{BS}}$ scales slower than $n^\epsilon$ for
an arbitrarily small $\epsilon&gt;0$ (i.e., the regimes where $C_{\textrm{BS}}$
does not need to be infinitely large). We also show the case where our network
with GreenInfra is fundamentally in the infrastructure-limited regime, in which
the performance is limited by the rate of backhaul links. In addition, we
derive a generalized throughput scaling law including the case where the rate
of each backhaul link scales slower than $C_{\textrm{BS}}$. To validate the
throughput scaling law for finite values of system parameters, numerical
evaluation is also shown via computer simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01768</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01768</id><created>2015-12-06</created><authors><author><keyname>Danish</keyname></author><author><keyname>Dahiya</keyname><forenames>Yogesh</forenames></author><author><keyname>Talukdar</keyname><forenames>Partha</forenames></author></authors><title>Want Answers? A Reddit Inspired Study on How to Pose Questions</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Questions form an integral part of our everyday communication, both offline
and online. Getting responses to our questions from others is fundamental to
satisfying our information need and in extending our knowledge boundaries. A
question may be represented using various factors such as social, syntactic,
semantic, etc. We hypothesize that these factors contribute with varying
degrees towards getting responses from others for a given question. We perform
a thorough empirical study to measure effects of these factors using a novel
question and answer dataset from the website Reddit.com. To the best of our
knowledge, this is the first such analysis of its kind on this important topic.
We also use a sparse nonnegative matrix factorization technique to
automatically induce interpretable semantic factors from the question dataset.
We also document various patterns on response prediction we observe during our
analysis in the data. For instance, we found that preference-probing questions
are scantily answered. Our method is robust to capture such latent response
factors. We hope to make our code and datasets publicly available upon
publication of the paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01769</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01769</id><created>2015-12-06</created><authors><author><keyname>Kozlenko</keyname><forenames>Mykola</forenames></author></authors><title>The interference immunity of the telemetric information data exchange
  with autonomous mobile robots</title><categories>cs.RO</categories><comments>in Ukrainian</comments><journal-ref>Naukovyi Visnyk Natsionalnoho Hirnychoho Universytetu, (1),
  107-113 (2015)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Purpose. To obtain the interference immunity of the data exchange by spread
spectrum signals with variable entropy of the telemetric information data
exchange with autonomous mobile robots.
  Methodology. The results have been obtained by the theoretical investigations
and have been confirmed by the modeling experiments.
  Findings. The interference immunity in form of dependence of bit error
probability on normalized signal/noise ratio of the data exchange by spread
spectrum signals with variable entropy has been obtained.It has been proved
that the interference immunity factor (needed normalized signal/noise ratio) is
at least 2 dB better under condition of equal time complexity as compared with
correlation processing methods of orthogonal signals.
  Originality. For the first time the interference immunity in form of
dependence of bit error probability on normalized signal/noise ratio of the
data exchange by spread spectrum signals with variable entropy has been
obtained.
  Practical value. The obtained results prove the feasibility of using variable
entropy spread spectrum signals data exchange method in the distributed
telemetric information processing systems in specific circumstances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01774</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01774</id><created>2015-12-06</created><authors><author><keyname>Litany</keyname><forenames>Or</forenames></author><author><keyname>Remez</keyname><forenames>Tal</forenames></author><author><keyname>Bronstein</keyname><forenames>Alex</forenames></author></authors><title>Image reconstruction from dense binary pixels</title><categories>cs.CV</categories><comments>Signal Processing with Adaptive Sparse Structured Representations
  (SPARS 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, the dense binary pixel Gigavision camera had been introduced,
emulating a digital version of the photographic film. While seems to be a
promising solution for HDR imaging, its output is not directly usable and
requires an image reconstruction process. In this work, we formulate this
problem as the minimization of a convex objective combining a
maximum-likelihood term with a sparse synthesis prior. We present MLNet - a
novel feed-forward neural network, producing acceptable output quality at a
fixed complexity and is two orders of magnitude faster than iterative
algorithms. We present state of the art results in the abstract.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01775</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01775</id><created>2015-12-06</created><authors><author><keyname>Bartal</keyname><forenames>Yair</forenames></author><author><keyname>Gottlieb</keyname><forenames>Lee-Ad</forenames></author></authors><title>Approximate nearest neighbor search for $\ell_p$-spaces ($2 &lt; p &lt;
  \infty$) via embeddings</title><categories>cs.CG cs.DS</categories><comments>arXiv admin note: substantial text overlap with arXiv:1408.1789</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While the problem of approximate nearest neighbor search has been
well-studied for Euclidean space and $\ell_1$, few non-trivial algorithms are
known for $\ell_p$ when ($2 &lt; p &lt; \infty$). In this paper, we revisit this
fundamental problem and present approximate nearest-neighbor search algorithms
which give the first non-trivial approximation factor guarantees in this
setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01780</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01780</id><created>2015-12-06</created><authors><author><keyname>Merhav</keyname><forenames>Neri</forenames></author></authors><title>The generalized likelihood decoder: random coding and expurgated bounds</title><categories>cs.IT math.IT</categories><comments>25 pages; 2 figures; submitted to the IEEE Transactions on
  Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The likelihood decoder is a stochastic decoder that selects the decoded
message at random, using the posterior distribution of the true underlying
message given the channel output. In this work, we study a generalized version
of this decoder where the posterior is proportional to a general function that
depends only on the joint empirical distribution of the output vector and the
codeword. This framework allows both mismatched versions and universal (MMI)
versions of the likelihood decoder, as well as the corresponding ordinary
deterministic decoders, among many others. We provide a direct analysis method
that yields the exact random coding exponent (as opposed to separate upper
bounds and lower bounds that turn out to be compatible, which were derived
earlier by Scarlett et al. We also extend the result from pure channel coding
to combined source and channel coding (random binning followed by random
channel coding) with side information available to the decoder. Finally,
returning to pure channel coding, we derive also an expurgated exponent for the
stochastic likelihood decoder, which turns out to be at least as tight (and in
some cases, strictly so) as the classical expurgated exponent of the maximum
likelihood decoder, even though the stochastic likelihood decoder is
suboptimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01781</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01781</id><created>2015-12-06</created><authors><author><keyname>Singh</keyname><forenames>Mohit</forenames></author><author><keyname>Zenklusen</keyname><forenames>Rico</forenames></author></authors><title>k-Trails: Recognition, Complexity, and Approximations</title><categories>cs.DS cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The notion of degree-constrained spanning hierarchies, also called k-trails,
was recently introduced in the context of network routing problems. They
describe graphs that are homomorphic images of connected graphs of degree at
most k. First results highlight several interesting advantages of k-trails
compared to previous routing approaches. However, so far, only little is known
regarding computational aspects of k-trails.
  In this work we aim to fill this gap by presenting how k-trails can be
analyzed using techniques from algorithmic matroid theory. Exploiting this
connection, we resolve several open questions about k-trails. In particular, we
show that one can recognize efficiently whether a graph is a k-trail.
Furthermore, we show that deciding whether a graph contains a k-trail is
NP-complete; however, every graph that contains a k-trail is a (k+1)-trail.
Moreover, further leveraging the connection to matroids, we consider the
problem of finding a minimum weight k-trail contained in a graph G. We show
that one can efficiently find a (2k-1)-trail contained in G whose weight is no
more than the cheapest k-trail contained in G, even when allowing negative
weights.
  The above results settle several open questions raised by Molnar, Newman, and
Sebo.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01784</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01784</id><created>2015-12-06</created><updated>2015-12-08</updated><authors><author><keyname>Tabatabaei</keyname><forenames>Azadeh</forenames></author><author><keyname>Ghodsi</keyname><forenames>Mohammad</forenames></author></authors><title>Randomized Strategy for Walking in Streets for a Simple Robot</title><categories>cs.CG cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of walking in an unknown street, for a robot that has
a minimal sensing capability. The robot is equipped with a sensor that only
detects the discontinuities in depth information (gaps) and can locate the
target point as enters in its visibility region. First, we propose an online
deterministic search strategy that generates an optimal search path for the
simple robot to reach the target t, starting from s. In contrast with
previously known research, the path is designed without memorizing any portion
of the scene has seen so far. Then, we present a randomized search strategy,
based on the deterministic strategy. We prove that the expected distance
traveled by the robot is at most a 5.33 times longer than the shortest path to
reach the target.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01789</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01789</id><created>2015-12-06</created><authors><author><keyname>Sheinin</keyname><forenames>Mark</forenames></author><author><keyname>Schechner</keyname><forenames>Yoav Y.</forenames></author></authors><title>The Next Best Underwater View</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To image in high resolution large and occlusion-prone scenes, a camera must
move above and around. Degradation of visibility due to geometric occlusions
and distances is exacerbated by scattering, when the scene is in a
participating medium. Moreover, underwater and in other media, artificial
lighting is needed. Overall, data quality depends on the observed surface,
medium and the time-varying poses of the camera and light source. This work
proposes to optimize camera/light poses as they move, so that the surface is
scanned efficiently and the descattered recovery has the highest quality. The
work generalizes the next best view concept of robot vision to scattering media
and cooperative movable lighting. It also extends descattering to platforms
that move optimally. The optimization criterion is information gain, taken from
information theory. We exploit the existence of a prior rough 3D model, since
underwater such a model is routinely obtained using sonar. We demonstrate this
principle in a scaled-down setup.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01795</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01795</id><created>2015-12-06</created><authors><author><keyname>Khuziev</keyname><forenames>I. M.</forenames></author><author><keyname>Vyalyi</keyname><forenames>M. N.</forenames></author></authors><title>Distributed protocols for spanning tree construction and leader election</title><categories>cs.DC</categories><comments>Submitted to CSR11</comments><msc-class>68W15</msc-class><acm-class>D.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present fast deterministic distributed protocols in synchronous networks
for leader election and spanning tree construction. The protocols are designed
under the assumption that nodes in a network have identifiers but the size of
an identifier is unlimited. So time bounds of protocols depend on the sizes of
identifiers. Depending on a type of a protocol, we present protocols running in
time $O(D\log L+L)$ or $O(D\log L+V+L)$, where $L$ is the size of the minimal
identifier, $V$ is the number of nodes in a network and $D$ is the diameter of
a network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01808</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01808</id><created>2015-12-06</created><authors><author><keyname>Gogacz</keyname><forenames>Tomasz</forenames></author><author><keyname>Toru&#x144;czyk</keyname><forenames>Szymon</forenames></author></authors><title>Entropy bounds for conjunctive queries with functional dependencies</title><categories>cs.DB</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  We study the problem of finding the worst-case bound for the size of the
result $Q(\mathbb{ D})$ of a fixed conjunctive query $Q$ applied to a database
$\mathbb{ D}$ satisfying given functional dependencies. We provide a precise
characterization of this bound in terms of entropy vectors, and in terms of
finite groups. In particular, we show that an upper bound provided by Gottlob,
Lee, Valiant and Valiant is tight, answering a question from their paper. Our
result generalizes the bound due to Atserias, Grohe and Marx, who consider the
case without functional dependencies. Our result shows that the problem of
computing the worst-case size bound, in the general case, is closely related to
difficult problems from information theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01809</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01809</id><created>2015-12-06</created><authors><author><keyname>Nguyen</keyname><forenames>Hy Quy</forenames></author><author><keyname>Lee</keyname><forenames>Siu Wa</forenames></author><author><keyname>Tian</keyname><forenames>Xiaohai</forenames></author><author><keyname>Dong</keyname><forenames>Minghui</forenames></author><author><keyname>Chng</keyname><forenames>Eng Siong</forenames></author></authors><title>High quality voice conversion using prosodic and high-resolution
  spectral features</title><categories>cs.SD</categories><doi>10.1007/s11042-015-3039-x</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Voice conversion methods have advanced rapidly over the last decade. Studies
have shown that speaker characteristics are captured by spectral feature as
well as various prosodic features. Most existing conversion methods focus on
the spectral feature as it directly represents the timbre characteristics,
while some conversion methods have focused only on the prosodic feature
represented by the fundamental frequency. In this paper, a comprehensive
framework using deep neural networks to convert both timbre and prosodic
features is proposed. The timbre feature is represented by a high-resolution
spectral feature. The prosodic features include F0, intensity and duration. It
is well known that DNN is useful as a tool to model high-dimensional features.
In this work, we show that DNN initialized by our proposed autoencoder
pretraining yields good quality DNN conversion models. This pretraining is
tailor-made for voice conversion and leverages on autoencoder to capture the
generic spectral shape of source speech. Additionally, our framework uses
segmental DNN models to capture the evolution of the prosodic features over
time. To reconstruct the converted speech, the spectral feature produced by the
DNN model is combined with the three prosodic features produced by the DNN
segmental models. Our experimental results show that the application of both
prosodic and high-resolution spectral features leads to quality converted
speech as measured by objective evaluation and subjective listening tests.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01812</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01812</id><created>2015-12-06</created><authors><author><keyname>Stankovic</keyname><forenames>Ljubisa</forenames></author><author><keyname>Stankovic</keyname><forenames>Isidora</forenames></author></authors><title>Reconstruction of Sparse and Nonsparse Signals from a Reduced Set of
  Samples</title><categories>cs.IT math.IT</categories><comments>23 pages, 5 figures, ETF Journal of Electrical Engineering, vol.21,
  no.1, Dec. 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Signals sparse in a transformation domain can be recovered from a reduced set
of randomly positioned samples by using compressive sensing algorithms. Simple
re- construction algorithms are presented in the first part of the paper. The
missing samples manifest themselves as a noise in this reconstruction. Once the
reconstruction conditions for a sparse signal are met and the reconstruction is
achieved, the noise due to missing samples does not influence the results in a
direct way. It influences the possibility to recover a signal only. Additive
input noise will remain in the resulting reconstructed signal. The accuracy of
the recovery results is related to the additive input noise. Simple derivation
of this relation is presented. If a reconstruction algorithm for a sparse
signal is used in the reconstruction of a nonsparse signal then the noise due
to missing samples will remain and behave as an additive input noise. An exact
relation for the mean square error of this error is derived for the partial DFT
matrix case in this paper and presented in form of a theorem. It takes into
account very important fact that if all samples are available then the error
will be zero, for both sparse and nonsparse recovered signals. Theory is
illustrated and checked on statistical examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01815</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01815</id><created>2015-12-06</created><authors><author><keyname>Gadot</keyname><forenames>David</forenames></author><author><keyname>Wolf</keyname><forenames>Lior</forenames></author></authors><title>PatchBatch: a Batch Augmented Loss for Optical Flow</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose new loss functions for learning patch based descriptors via deep
Convolutional Neural Networks. The learned descriptors are compared using the
L2 norm and do not require network processing of pairs of patches. The success
of the method is based on a few technical novelties, including an innovative
loss function that, for each training batch, computes higher moments of the
score distributions. Combined with an Approximate Nearest Neighbor patch
matching method and a flow interpolating method, state of the art performance
is obtained on the most challenging and competitive optical flow benchmarks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01818</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01818</id><created>2015-12-06</created><updated>2016-02-01</updated><authors><author><keyname>Ribeiro</keyname><forenames>Filipe Nunes</forenames></author><author><keyname>Ara&#xfa;jo</keyname><forenames>Matheus</forenames></author><author><keyname>Gon&#xe7;alves</keyname><forenames>Pollyanna</forenames></author><author><keyname>Benevenuto</keyname><forenames>Fabr&#xed;cio</forenames></author><author><keyname>Gon&#xe7;alves</keyname><forenames>Marcos Andr&#xe9;</forenames></author></authors><title>A Benchmark Comparison of State-of-the-Practice Sentiment Analysis
  Methods</title><categories>cs.CL cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the last few years thousands of scientific papers have explored sentiment
analysis, several startups that measures opinions on real data have emerged,
and a number of innovative products related to this theme have been developed.
There are multiple methods for measuring sentiments, including lexical-based
approaches and supervised machine learning methods. Despite the vast interest
on the theme and wide popularity of some methods, it is unclear which method is
better for identifying the polarity (i.e., positive or negative) of a message.
Thus, there is a strong need to conduct a thorough apple-to-apple comparison of
sentiment analysis methods, as they are used in practice, across multiple
datasets originated from different data sources. Such a comparison is key for
understanding the potential limitations, advantages, and disadvantages of
popular methods. This study aims at filling this gap by presenting a benchmark
comparison of twenty one popular sentiment analysis methods (which we call the
state-of-the-practice methods). Our evaluation is based on a benchmark of
twenty labeled datasets, covering messages posted on social networks, movie and
product reviews, as well as opinions and comments in news articles. Our results
highlight the extent to which the prediction performance of these methods
varies widely across datasets. Aiming at boosting the development of this
research area, we open the methods' codes and datasets used in this paper and
we deploy a benchmark system, which provides an open API for accessing and
comparing sentence-level sentiment analysis methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01829</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01829</id><created>2015-12-06</created><authors><author><keyname>Ene</keyname><forenames>Alina</forenames></author><author><keyname>Mnich</keyname><forenames>Matthias</forenames></author><author><keyname>Pilipczuk</keyname><forenames>Marcin</forenames></author><author><keyname>Risteski</keyname><forenames>Andrej</forenames></author></authors><title>On Routing Disjoint Paths in Bounded Treewidth Graphs</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of routing on disjoint paths in bounded treewidth graphs
with both edge and node capacities. The input consists of a capacitated graph
$G$ and a collection of $k$ source-destination pairs $\mathcal{M} = \{(s_1,
t_1), \dots, (s_k, t_k)\}$. The goal is to maximize the number of pairs that
can be routed subject to the capacities in the graph. A routing of a subset
$\mathcal{M}'$ of the pairs is a collection $\mathcal{P}$ of paths such that,
for each pair $(s_i, t_i) \in \mathcal{M}'$, there is a path in $\mathcal{P}$
connecting $s_i$ to $t_i$. In the Maximum Edge Disjoint Paths (MaxEDP) problem,
the graph $G$ has capacities $\mathrm{cap}(e)$ on the edges and a routing
$\mathcal{P}$ is feasible if each edge $e$ is in at most $\mathrm{cap}(e)$ of
the paths of $\mathcal{P}$. The Maximum Node Disjoint Paths (MaxNDP) problem is
the node-capacitated counterpart of MaxEDP.
  In this paper we obtain an $O(r^3)$ approximation for MaxEDP on graphs of
treewidth at most $r$ and a matching approximation for MaxNDP on graphs of
pathwidth at most $r$. Our results build on and significantly improve the work
by Chekuri et al. [ICALP 2013] who obtained an $O(r \cdot 3^r)$ approximation
for MaxEDP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01834</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01834</id><created>2015-12-06</created><authors><author><keyname>Chung</keyname><forenames>SueYeon</forenames></author><author><keyname>Lee</keyname><forenames>Daniel D.</forenames></author><author><keyname>Sompolinsky</keyname><forenames>Haim</forenames></author></authors><title>Classification of Manifolds by Single-Layer Neural Networks</title><categories>cond-mat.dis-nn cond-mat.stat-mech cs.NE q-bio.NC stat.ML</categories><comments>5 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The neuronal representation of objects exhibit enormous variability due to
changes in the object's physical features such as location, size, orientation,
and intensity. How the brain copes with the variability across these manifolds
of neuronal states and generates invariant perception of objects remains poorly
understood. Here we present a theory of neuronal classification of manifolds,
extending Gardner's replica theory of classification of isolated points by a
single layer perceptron. We evaluate how the perceptron capacity depends on the
dimensionality, size and shape of the classified manifolds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01837</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01837</id><created>2015-12-06</created><authors><author><keyname>Sterling</keyname><forenames>Jonathan</forenames></author></authors><title>Type Theory and its Meaning Explanations</title><categories>cs.LO cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  At the heart of intuitionistic type theory lies an intuitive semantics called
the &quot;meaning explanations&quot;; crucially, when meaning explanations are taken as
definitive for type theory, the core notion is no longer &quot;proof&quot; but
&quot;verification&quot;. We'll explore how type theories of this sort arise naturally as
enrichments of logical theories with further judgements, and contrast this with
modern proof-theoretic type theories which interpret the judgements and proofs
of logics, not their propositions and verifications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01839</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01839</id><created>2015-12-06</created><authors><author><keyname>Barrolleta</keyname><forenames>Roland D.</forenames></author><author><keyname>Villanueva</keyname><forenames>Merc&#xe8;</forenames></author></authors><title>Partial permutation decoding for binary linear and Z4-linear Hadamard
  codes</title><categories>cs.IT cs.DM math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Permutation decoding is a technique which involves finding a subset $S$,
called PD-set, of the permutation automorphism group of a code $C$ in order to
assist in decoding. A method to obtain $s$-PD-sets of minimum size $s+1$ for
partial permutation decoding for binary linear Hadamard codes $H_m$ of length
$2^m$, for all $m\geq 4$ and $1&lt;s\leq \left \lfloor{\frac{2^m-m-1}{1+m}} \right
\rfloor$, is described. Moreover, a recursive construction to obtain
$s$-PD-sets of size $l$ for $H_{m+1}$ of length $2^{m+1}$, from a given
$s$-PD-set of the same size for $H_m$, is also established. These results are
generalized to find $s$-PD-sets for (nonlinear) binary Hadamard codes of length
$2^m$, called $Z_4$-linear Hadamard codes, which are obtained as the Gray map
image of quaternary linear codes of length $2^{m-1}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01841</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01841</id><created>2015-12-06</created><authors><author><keyname>Nikulchev</keyname><forenames>Evgeny</forenames></author><author><keyname>Pluzhnik</keyname><forenames>Evgeniy</forenames></author><author><keyname>Biryukov</keyname><forenames>Dmitry</forenames></author><author><keyname>Lukyanchikov</keyname><forenames>Oleg</forenames></author></authors><title>Designing Applications in a Hybrid Cloud</title><categories>cs.DC cs.DS cs.SE</categories><comments>8 pages</comments><journal-ref>Contemporary Engineering Sciences 8 (2015) 963-970</journal-ref><doi>10.12988/ces.2015.57214</doi><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Designing applications for hybrid cloud has many features, including dynamic
virtualization management and route switching. This makes it impossible to
evaluate the query and hence the optimal distribution of data. In this paper,
we formulate the main challenges of designing and simulation, offer
installation for processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01843</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01843</id><created>2015-12-06</created><updated>2015-12-29</updated><authors><author><keyname>Keykhosravi</keyname><forenames>Kamran</forenames></author><author><keyname>Agrell</keyname><forenames>Erik</forenames></author><author><keyname>Durisi</keyname><forenames>Giuseppe</forenames></author></authors><title>A Monotonically Increasing Lower Bound on the Capacity of the
  Fiber-Optical Channel</title><categories>cs.IT math.IT physics.optics</categories><comments>This paper has been withdrawn by the authors due to a crucial error
  in the proof</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An achievable rate is derived for the fiber-optical channel, described by the
nonlinear Schr\&quot;odinger equation and discretized in time and space. The model
takes into account the effects of nonlinearity, dispersion, and noise. The
obtained achievable rate goes to infinity with a pre-log factor of one half as
the power grows large. Since any achievable rate is a lower bound on the
capacity of the same channel, the result proves that the capacity of the
discretized fiber-optical channel grows unboundedly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01845</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01845</id><created>2015-12-06</created><authors><author><keyname>Wu</keyname><forenames>Chao-Yuan</forenames></author><author><keyname>Beutel</keyname><forenames>Alex</forenames></author><author><keyname>Ahmed</keyname><forenames>Amr</forenames></author><author><keyname>Smola</keyname><forenames>Alexander J.</forenames></author></authors><title>Explaining reviews and ratings with PACO: Poisson Additive Co-Clustering</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Understanding a user's motivations provides valuable information beyond the
ability to recommend items. Quite often this can be accomplished by perusing
both ratings and review texts, since it is the latter where the reasoning for
specific preferences is explicitly expressed.
  Unfortunately matrix factorization approaches to recommendation result in
large, complex models that are difficult to interpret and give recommendations
that are hard to clearly explain to users. In contrast, in this paper, we
attack this problem through succinct additive co-clustering. We devise a novel
Bayesian technique for summing co-clusterings of Poisson distributions. With
this novel technique we propose a new Bayesian model for joint collaborative
filtering of ratings and text reviews through a sum of simple co-clusterings.
The simple structure of our model yields easily interpretable recommendations.
Even with a simple, succinct structure, our model outperforms competitors in
terms of predicting ratings with reviews.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01848</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01848</id><created>2015-12-06</created><authors><author><keyname>Fernando</keyname><forenames>Basura</forenames></author><author><keyname>Gavves</keyname><forenames>Efstratios</forenames></author><author><keyname>Oramas</keyname><forenames>Jose</forenames></author><author><keyname>Ghodrati</keyname><forenames>Amir</forenames></author><author><keyname>Tuytelaars</keyname><forenames>Tinne</forenames></author></authors><title>Rank Pooling for Action Recognition</title><categories>cs.CV</categories><comments>Under review for IEEE Transactions on Pattern Analysis and Machine
  Intelligence</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a function-based temporal pooling method that captures the latent
structure of the video sequence data - e.g. how frame-level features evolve
over time in a video. We show how the parameters of a function that has been
fit to the video data can serve as a robust new video representation. As a
specific example, we learn a pooling function via ranking machines. By learning
to rank the frame-level features of a video in chronological order, we obtain a
new representation that captures the video-wide temporal dynamics of a video,
suitable for action recognition. Other than ranking functions, we explore
different parametric models that could also explain the temporal changes in
videos. The proposed functional pooling methods, and rank pooling in
particular, is easy to interpret and implement, fast to compute and effective
in recognizing a wide variety of actions. We evaluate our method on various
benchmarks for generic action, fine-grained action and gesture recognition.
Results show that rank pooling brings an absolute improvement of 7-10 average
pooling baseline. At the same time, rank pooling is compatible with and
complementary to several appearance and local motion based methods and
features, such as improved trajectories and deep learning features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01858</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01858</id><created>2015-12-06</created><authors><author><keyname>Feng</keyname><forenames>Mengyang</forenames></author><author><keyname>Borji</keyname><forenames>Ali</forenames></author><author><keyname>Lu</keyname><forenames>Huchuan</forenames></author></authors><title>Fixation prediction with a combined model of bottom-up saliency and
  vanishing point</title><categories>cs.CV</categories><comments>arXiv admin note: text overlap with arXiv:1512.01722</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  By predicting where humans look in natural scenes, we can understand how they
perceive complex natural scenes and prioritize information for further
high-level visual processing. Several models have been proposed for this
purpose, yet there is a gap between best existing saliency models and human
performance. While many researchers have developed purely computational models
for fixation prediction, less attempts have been made to discover cognitive
factors that guide gaze. Here, we study the effect of a particular type of
scene structural information, known as the vanishing point, and show that human
gaze is attracted to the vanishing point regions. We record eye movements of 10
observers over 532 images, out of which 319 have vanishing points. We then
construct a combined model of traditional saliency and a vanishing point
channel and show that our model outperforms state of the art saliency models
using three scores on our dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01859</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01859</id><created>2015-12-06</created><authors><author><keyname>Marzen</keyname><forenames>Sarah E.</forenames></author><author><keyname>Crutchfield</keyname><forenames>James P.</forenames></author></authors><title>Statistical Signatures of Structural Organization: The case of long
  memory in renewal processes</title><categories>cond-mat.stat-mech cs.IT math.DS math.IT math.ST physics.data-an stat.TH</categories><comments>13 pages, 2 figures, 3 appendixes;
  http://csc.ucdavis.edu/~cmg/compmech/pubs/lrmrp.htm</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Identifying and quantifying memory are often critical steps in developing a
mechanistic understanding of stochastic processes. These are particularly
challenging and necessary when exploring processes that exhibit long-range
correlations. The most common signatures employed rely on second-order temporal
statistics and lead, for example, to identifying long memory in processes with
power-law autocorrelation function and Hurst exponent greater than $1/2$.
However, most stochastic processes hide their memory in higher-order temporal
correlations. Information measures---specifically, divergences in the mutual
information between a process' past and future (excess entropy) and minimal
predictive memory stored in a process' causal states (statistical
complexity)---provide a different way to identify long memory in processes with
higher-order temporal correlations. However, there are no ergodic stationary
processes with infinite excess entropy for which information measures have been
compared to autocorrelation functions and Hurst exponents. Here, we show that
fractal renewal processes---those with interevent distribution tails $\propto
t^{-\alpha}$---exhibit long memory via a phase transition at $\alpha = 1$.
Excess entropy diverges only there and statistical complexity diverges there
and for all $\alpha &lt; 1$. When these processes do have power-law
autocorrelation function and Hurst exponent greater than $1/2$, they do not
have divergent excess entropy. This analysis breaks the intuitive association
between these different quantifications of memory. We hope that the methods
used here, based on causal states, provide some guide as to how to construct
and analyze other long memory processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01862</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01862</id><created>2015-12-06</created><authors><author><keyname>Vasilescu</keyname><forenames>Bogdan</forenames></author><author><keyname>van Schuylenburg</keyname><forenames>Stef</forenames></author><author><keyname>Wulms</keyname><forenames>Jules</forenames></author><author><keyname>Serebrenik</keyname><forenames>Alexander</forenames></author><author><keyname>Brand</keyname><forenames>Mark G. J. van den</forenames></author></authors><title>Continuous integration in a social-coding world: Empirical evidence from
  GitHub. **Updated version with corrections**</title><categories>cs.SE</categories><comments>This is an updated and corrected version of our ICSME 2014 paper:
  http://dx.doi.org/10.1109/ICSME.2014.62</comments><doi>10.1109/ICSME.2014.62</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Continuous integration is a software engineering practice of frequently
merging all developer working copies with a shared main branch, e.g., several
times a day. With the advent of GitHub, a platform well known for its &quot;social
coding&quot; features that aid collaboration and sharing, and currently the largest
code host in the open source world, collaborative software development has
never been more prominent. In GitHub development one can distinguish between
two types of developer contributions to a project: direct ones, coming from a
typically small group of developers with write access to the main project
repository, and indirect ones, coming from developers who fork the main
repository, update their copies locally, and submit pull requests for review
and merger. In this paper we explore how GitHub developers use continuous
integration as well as whether the contribution type (direct versus indirect)
and different project characteristics (e.g., main programming language, or
project age) are associated with the success of the automatic builds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01871</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01871</id><created>2015-12-06</created><authors><author><keyname>Adamatzky</keyname><forenames>Andrew</forenames></author><author><keyname>Sirakoulis</keyname><forenames>Georgios Ch.</forenames></author></authors><title>Building exploration with leeches Hirudo verbana</title><categories>cs.ET</categories><journal-ref>Biosystems Volume 134, August 2015, Pages 48--55</journal-ref><doi>10.1016/j.biosystems.2015.06.004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Safe evacuation of people from building and outdoor environments, and search
and rescue operations, always will remain actual in course of all
socio-technological developments. Modern facilities offer a range of automated
systems to guide residents towards emergency exists. The systems are assumed to
be infallible. But what if they fail? How occupants not familiar with a
building layout will be looking for exits in case of very limited visibility
where tactile sensing is the only way to assess the environment? Analogous
models of human behaviour, and socio-dynamics in general, are provided to be
fruitful ways to explore alternative, or would-be scenarios. Crowd, or a single
person, dynamics could be imitated using particle systems, reaction-diffusion
chemical medium, electro-magnetic fields, or social insects. Each type of
analogous model offer unique insights on behavioural patterns of natural
systems in constrained geometries. In this particular paper we have chosen
leeches to analyse patterns of exploration. Reasons are two-fold. First, when
deprived from other stimuli leeches change their behavioural modes in an
automated regime in response to mechanical stimulation. Therefore leeches can
give us invaluable information on how human beings might behave under stress
and limited visibility. Second, leeches are ideal blueprints of future
soft-bodied rescue robots. Leeches have modular nervous circuitry with a rich
behavioral spectrum. Leeches are multi-functional, fault-tolerant with
autonomous inter-segment coordination and adaptive decision-making. We aim to
answer the question: how efficiently a real building can be explored and
whether there any dependencies on the pathways of exploration and geometrical
complexity of the building. In our case studies we use templates made on the
floor plan of real building.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01872</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01872</id><created>2015-12-06</created><authors><author><keyname>Rajpurkar</keyname><forenames>Pranav</forenames></author><author><keyname>Migimatsu</keyname><forenames>Toki</forenames></author><author><keyname>Kiske</keyname><forenames>Jeff</forenames></author><author><keyname>Cheng-Yue</keyname><forenames>Royce</forenames></author><author><keyname>Tandon</keyname><forenames>Sameep</forenames></author><author><keyname>Wang</keyname><forenames>Tao</forenames></author><author><keyname>Ng</keyname><forenames>Andrew</forenames></author></authors><title>Driverseat: Crowdstrapping Learning Tasks for Autonomous Driving</title><categories>cs.HC cs.AI cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While emerging deep-learning systems have outclassed knowledge-based
approaches in many tasks, their application to detection tasks for autonomous
technologies remains an open field for scientific exploration. Broadly, there
are two major developmental bottlenecks: the unavailability of comprehensively
labeled datasets and of expressive evaluation strategies. Approaches for
labeling datasets have relied on intensive hand-engineering, and strategies for
evaluating learning systems have been unable to identify failure-case
scenarios. Human intelligence offers an untapped approach for breaking through
these bottlenecks. This paper introduces Driverseat, a technology for embedding
crowds around learning systems for autonomous driving. Driverseat utilizes
crowd contributions for (a) collecting complex 3D labels and (b) tagging
diverse scenarios for ready evaluation of learning systems. We demonstrate how
Driverseat can crowdstrap a convolutional neural network on the lane-detection
task. More generally, crowdstrapping introduces a valuable paradigm for any
technology that can benefit from leveraging the powerful combination of human
and computer intelligence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01876</identifier>
 <datestamp>2016-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01876</id><created>2015-12-06</created><updated>2016-01-07</updated><authors><author><keyname>Agarwal</keyname><forenames>Pankaj K.</forenames></author><author><keyname>Fox</keyname><forenames>Kyle</forenames></author><author><keyname>Pan</keyname><forenames>Jiangwei</forenames></author><author><keyname>Ying</keyname><forenames>Rex</forenames></author></authors><title>Approximating Dynamic Time Warping and Edit Distance for a Pair of Point
  Sequences</title><categories>cs.CG cs.DS</categories><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give the first subquadratic-time approximation schemes for dynamic time
warping (DTW) and edit distance (ED) of several natural families of point
sequences in $\mathbb{R}^d$, for any fixed $d \ge 1$. In particular, our
algorithms compute $(1+\varepsilon)$-approximations of DTW and ED in time
near-linear for point sequences drawn from k-packed or k-bounded curves, and
subquadratic for backbone sequences. Roughly speaking, a curve is
$\kappa$-packed if the length of its intersection with any ball of radius $r$
is at most $\kappa \cdot r$, and a curve is $\kappa$-bounded if the sub-curve
between two curve points does not go too far from the two points compared to
the distance between the two points. In backbone sequences, consecutive points
are spaced at approximately equal distances apart, and no two points lie very
close together. Recent results suggest that a subquadratic algorithm for DTW or
ED is unlikely for an arbitrary pair of point sequences even for $d=1$. Our
algorithms work by constructing a small set of rectangular regions that cover
the entries of the dynamic programming table commonly used for these distance
measures. The weights of entries inside each rectangle are roughly the same, so
we are able to use efficient procedures to approximately compute the cheapest
paths through these rectangles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01878</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01878</id><created>2015-12-06</created><authors><author><keyname>Adamatzky</keyname><forenames>Andrew</forenames></author></authors><title>On exploration of geometrically constrained space by medicinal leeches
  Hirudo verbana</title><categories>cs.ET</categories><journal-ref>Biosystems. Volume 130, April 2015, Pages 28--36</journal-ref><doi>10.1016/j.biosystems.2015.02.005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Leeches are fascinating creatures: they have simple modular nervous circuitry
m yet exhibit a rich spectrum of behavioural modes. Leeches could be ideal
blue-prints for designing flexible soft robots which are modular,
multi-functional, fault-tolerant, easy to control, capable for navigating using
optical, mechanical and chemical sensorial inputs, have autonomous
inter-segmental coordination and adaptive decision-making. With future designs
of leech-robots in mind we study how leeches behave in geometrically
constrained spaces. Core results of the paper deal with leeches exploring a row
of rooms arranged along a narrow corridor. In laboratory experiments we find
that rooms closer to ends of the corridor are explored by leeches more often
than rooms in the middle of the corridor. Also, in series of scoping
experiments, we evaluate leeches capabilities to navigating in mazes towards
sources of vibration and chemo-attraction. We believe our results lay
foundation for future developments of robots mimicking behaviour of leeches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01881</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01881</id><created>2015-12-06</created><updated>2015-12-11</updated><authors><author><keyname>Chan</keyname><forenames>Cheng-Sheng</forenames></author><author><keyname>Chen</keyname><forenames>Shou-Zhong</forenames></author><author><keyname>Xie</keyname><forenames>Pei-Xuan</forenames></author><author><keyname>Chang</keyname><forenames>Chiung-Chih</forenames></author><author><keyname>Sun</keyname><forenames>Min</forenames></author></authors><title>Recognition from Hand Cameras</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose HandCam (Fig. 1), a novel wearable camera capturing activities of
hands, for recognizing human behaviors. HandCam has two main advantages over
egocentric systems [10, 5, 23]: (1) it avoids the need to detect hands and
manipulation regions; (2) it observes the activities of hands almost at all
time. These nice properties enable HandCam to recognize hand states (free v.s.
active hands, hand gestures, object categories), and discover object categories
through interaction with hands. We propose a new method to accomplish these
tasks by fusing per-frame state prediction with state changes estimation from a
pair of frames. We have collected one of the first HandCam dataset with 20
videos captured in three scenes. Experiments show that HandCam system
significantly outperforms a state-of- the-art HeadCam system (frame-based
re-implementation similar to [17]) in all tasks by at most 10.8% improvement in
accuracy. We also show that HandCam videos captured by different users can be
easily aligned to improve free v.s. active recognition accuracy (3.3%
improvement) in cross-scenes use case. Moreover, we observe that finetuning
Convolutional Neural Network [14] consistently improves accuracy (at most 4.9%
better). Finally, a method combining HandCam and HeadCam features achieves the
best performance. With more data, we believe a joint HandCam and HeadCam system
can robustly log hand states in our daily life.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01882</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01882</id><created>2015-12-06</created><updated>2015-12-10</updated><authors><author><keyname>Wang</keyname><forenames>Dong</forenames></author><author><keyname>Zhang</keyname><forenames>Xuewei</forenames></author></authors><title>THCHS-30 : A Free Chinese Speech Corpus</title><categories>cs.CL cs.SD</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Speech data is crucially important for speech recognition research. There are
quite some speech databases that can be purchased at prices that are reasonable
for most research institutes. However, for young people who just start research
activities or those who just gain initial interest in this direction, the cost
for data is still an annoying barrier. We support the `free data' movement in
speech recognition: research institutes (particularly supported by public
funds) publish their data freely so that new researchers can obtain sufficient
data to kick of their career. In this paper, we follow this trend and release a
free Chinese speech database THCHS-30 that can be used to build a full- edged
Chinese speech recognition system. We report the baseline system established
with this database, including the performance under highly noisy conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01883</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01883</id><created>2015-12-06</created><authors><author><keyname>Liebig</keyname><forenames>Jessica</forenames></author><author><keyname>Rao</keyname><forenames>Asha</forenames></author></authors><title>A fast and simple method of extracting the backbone of one-mode
  projections: An aid in the detection of communities</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a computationally inexpensive method of extracting the
backbone of one-mode networks projected from bipartite networks. We show that
the edge weights in one-mode projections are distributed according to a Poisson
binomial distribution. Finding the expected weight distribution of a one-mode
network projected from a random bipartite network only requires knowledge of
the bipartite degree distributions. Being able to extract the backbone of a
projection proves to be highly beneficial to filter out redundant information
in large complex networks and to narrow down the information in the one-mode
projection to the most relevant. We further demonstrate that the backbone of a
one-mode projection aids in the detection of communities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01885</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01885</id><created>2015-12-06</created><authors><author><keyname>Nobandegani</keyname><forenames>Ardavan Salehi</forenames></author><author><keyname>Psaromiligkos</keyname><forenames>Ioannis N.</forenames></author></authors><title>Probabilistic Structural Controllability in Causal Bayesian Networks</title><categories>cs.AI cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Humans routinely confront the following key question which could be viewed as
a probabilistic variant of the controllability problem: While faced with an
uncertain environment governed by causal structures, how should they practice
their autonomy by intervening on driver variables, in order to increase (or
decrease) the probability of attaining their desired (or undesired) state for
some target variable? In this paper, for the first time, the problem of
probabilistic controllability in Causal Bayesian Networks (CBNs) is studied.
More specifically, the aim of this paper is two-fold: (i) to introduce and
formalize the problem of probabilistic structural controllability in CBNs, and
(ii) to identify a sufficient set of driver variables for the purpose of
probabilistic structural controllability of a generic CBN. We also elaborate on
the nature of minimality the identified set of driver variables satisfies. In
this context, the term &quot;structural&quot; signifies the condition wherein solely the
structure of the CBN is known.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01886</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01886</id><created>2015-12-06</created><updated>2015-12-08</updated><authors><author><keyname>Thomas</keyname><forenames>Bryce</forenames></author><author><keyname>Jurdak</keyname><forenames>Raja</forenames></author><author><keyname>Zhao</keyname><forenames>Kun</forenames></author><author><keyname>Atkinson</keyname><forenames>Ian</forenames></author></authors><title>Diffusion in Colocation Contact Networks: the Impact of Nodal
  Spatiotemporal Dynamics</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Temporal contact networks are studied to understand dynamic spreading
phenomena such as communicable diseases or information dissemination. To
establish how spatiotemporal dynamics of nodes impact spreading potential in
colocation contact networks, we propose &quot;inducement-shuffling&quot; null models
which break one or more correlations between times, locations and nodes. By
reconfiguring the time and/or location of each node's presence in the network,
these models induce alternative sets of colocation events giving rise to
contact networks with varying spreading potential. This enables second-order
causal reasoning about how correlations in nodes' spatiotemporal preferences
not only lead to a given contact network but ultimately influence the network's
spreading potential. We find the correlation between nodes and times to be the
greatest impediment to spreading, while the correlation between times and
locations slightly catalyzes spreading. Under each of the presented null models
we measure both the number of contacts and infection prevalence as a function
of time, with the surprising finding that the two have no direct causality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01891</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01891</id><created>2015-12-06</created><authors><author><keyname>Sun</keyname><forenames>Yi</forenames></author><author><keyname>Wang</keyname><forenames>Xiaogang</forenames></author><author><keyname>Tang</keyname><forenames>Xiaoou</forenames></author></authors><title>Sparsifying Neural Network Connections for Face Recognition</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes to learn high-performance deep ConvNets with sparse
neural connections, referred to as sparse ConvNets, for face recognition. The
sparse ConvNets are learned in an iterative way, each time one additional layer
is sparsified and the entire model is re-trained given the initial weights
learned in previous iterations. One important finding is that directly training
the sparse ConvNet from scratch failed to find good solutions for face
recognition, while using a previously learned denser model to properly
initialize a sparser model is critical to continue learning effective features
for face recognition. This paper also proposes a new neural correlation-based
weight selection criterion and empirically verifies its effectiveness in
selecting informative connections from previously learned models in each
iteration. When taking a moderately sparse structure (26%-76% of weights in the
dense model), the proposed sparse ConvNet model significantly improves the face
recognition performance of the previous state-of-the-art DeepID2+ models given
the same training data, while it keeps the performance of the baseline model
with only 12% of the original parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01892</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01892</id><created>2015-12-06</created><authors><author><keyname>Kyng</keyname><forenames>Rasmus</forenames></author><author><keyname>Lee</keyname><forenames>Yin Tat</forenames></author><author><keyname>Peng</keyname><forenames>Richard</forenames></author><author><keyname>Sachdeva</keyname><forenames>Sushant</forenames></author><author><keyname>Spielman</keyname><forenames>Daniel A.</forenames></author></authors><title>Sparsified Cholesky and Multigrid Solvers for Connection Laplacians</title><categories>cs.DS</categories><comments>This article supersedes arXiv:1506.08204</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the sparsified Cholesky and sparsified multigrid algorithms for
solving systems of linear equations. These algorithms accelerate Gaussian
elimination by sparsifying the nonzero matrix entries created by the
elimination process. We use these new algorithms to derive the first nearly
linear time algorithms for solving systems of equations in connection
Laplacians, a generalization of Laplacian matrices that arise in many problems
in image and signal processing. We also prove that every connection Laplacian
has a linear sized approximate inverse. This is an LU factorization with a
linear number of nonzero entries that is a strong approximation of the original
matrix. Using such a factorization one can solve systems of equations in a
connection Laplacian in linear time. Such a factorization was unknown even for
ordinary graph Laplacians.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01895</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01895</id><created>2015-12-06</created><authors><author><keyname>White</keyname><forenames>Leo</forenames></author><author><keyname>Bour</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames></author><author><keyname>Yallop</keyname><forenames>Jeremy</forenames></author></authors><title>Modular implicits</title><categories>cs.PL</categories><comments>In Proceedings ML/OCaml 2014, arXiv:1512.01438</comments><proxy>EPTCS</proxy><acm-class>D.3.3</acm-class><journal-ref>EPTCS 198, 2015, pp. 22-63</journal-ref><doi>10.4204/EPTCS.198.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present modular implicits, an extension to the OCaml language for ad-hoc
polymorphism inspired by Scala implicits and modular type classes. Modular
implicits are based on type-directed implicit module parameters, and elaborate
straightforwardly into OCaml's first-class functors. Basing the design on
OCaml's modules leads to a system that naturally supports many features from
other languages with systematic ad-hoc overloading, including inheritance,
instance constraints, constructor classes and associated types.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01896</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01896</id><created>2015-12-06</created><authors><author><keyname>Petricek</keyname><forenames>Tomas</forenames><affiliation>University of Cambridge</affiliation></author><author><keyname>Syme</keyname><forenames>Don</forenames><affiliation>Microsoft Research</affiliation></author><author><keyname>Bray</keyname><forenames>Zach</forenames><affiliation>Type Inferred Ltd</affiliation></author></authors><title>In the Age of Web: Typed Functional-First Programming Revisited</title><categories>cs.PL cs.SE</categories><comments>In Proceedings ML/OCaml 2014, arXiv:1512.01438</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 198, 2015, pp. 64-79</journal-ref><doi>10.4204/EPTCS.198.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most programming languages were designed before the age of web. This matters
because the web changes many assumptions that typed functional language
designers take for granted. For example, programs do not run in a closed world,
but must instead interact with (changing and likely unreliable) services and
data sources, communication is often asynchronous or event-driven, and programs
need to interoperate with untyped environments.
  In this paper, we present how the F# language and libraries face the
challenges posed by the web. Technically, this comprises using type providers
for integration with external information sources and for integration with
untyped programming environments, using lightweight meta-programming for
targeting JavaScript and computation expressions for writing asynchronous code.
  In this inquiry, the holistic perspective is more important than each of the
features in isolation. We use a practical case study as a starting point and
look at how F# language and libraries approach the challenges posed by the web.
The specific lessons learned are perhaps less interesting than our attempt to
uncover hidden assumptions that no longer hold in the age of web.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01897</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01897</id><created>2015-12-06</created><authors><author><keyname>Chargu&#xe9;raud</keyname><forenames>Arthur</forenames></author></authors><title>Improving Type Error Messages in OCaml</title><categories>cs.PL</categories><comments>In Proceedings ML/OCaml 2014, arXiv:1512.01438</comments><proxy>EPTCS</proxy><acm-class>D.3.4</acm-class><journal-ref>EPTCS 198, 2015, pp. 80-97</journal-ref><doi>10.4204/EPTCS.198.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cryptic type error messages are a major obstacle to learning OCaml or other
ML-based languages. In many cases, error messages cannot be interpreted without
a sufficiently-precise model of the type inference algorithm. The problem of
improving type error messages in ML has received quite a bit of attention over
the past two decades, and many different strategies have been considered. The
challenge is not only to produce error messages that are both sufficiently
concise and systematically useful to the programmer, but also to handle a
full-blown programming language and to cope with large-sized programs
efficiently.
  In this work, we present a modification to the traditional ML type inference
algorithm implemented in OCaml that, by significantly reducing the
left-to-right bias, allows us to report error messages that are more helpful to
the programmer. Our algorithm remains fully predictable and continues to
produce fairly concise error messages that always help making some progress
towards fixing the code. We implemented our approach as a patch to the OCaml
compiler in just a few hundred lines of code. We believe that this patch should
benefit not just to beginners, but also to experienced programs developing
large-scale OCaml programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01898</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01898</id><created>2015-12-06</created><authors><author><keyname>Abe</keyname><forenames>Akinori</forenames></author><author><keyname>Sumii</keyname><forenames>Eijiro</forenames></author></authors><title>A Simple and Practical Linear Algebra Library Interface with Static Size
  Checking</title><categories>cs.PL</categories><comments>In Proceedings ML/OCaml 2014, arXiv:1512.01438</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 198, 2015, pp. 1-21</journal-ref><doi>10.4204/EPTCS.198.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linear algebra is a major field of numerical computation and is widely
applied. Most linear algebra libraries (in most programming languages) do not
statically guarantee consistency of the dimensions of vectors and matrices,
causing runtime errors. While advanced type systems--specifically, dependent
types on natural numbers--can ensure consistency among the sizes of collections
such as lists and arrays, such type systems generally require non-trivial
changes to existing languages and application programs, or tricky type-level
programming.
  We have developed a linear algebra library interface that verifies the
consistency (with respect to dimensions) of matrix operations by means of
generative phantom types, implemented via fairly standard ML types and module
system. To evaluate its usability, we ported to it a practical machine learning
library from a traditional linear algebra library. We found that most of the
changes required for the porting could be made mechanically, and changes that
needed human thought are minor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01904</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01904</id><created>2015-12-06</created><authors><author><keyname>Li</keyname><forenames>Chengtao</forenames></author><author><keyname>Sra</keyname><forenames>Suvrit</forenames></author><author><keyname>Jegelka</keyname><forenames>Stefanie</forenames></author></authors><title>Bounds on bilinear inverse forms via Gaussian quadrature with
  applications</title><categories>stat.ML cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address quadrature-based approximations of the bilinear inverse form
$u^\top A^{-1} u$, where $A$ is a real symmetric positive definite matrix, and
analyze properties of the Gauss, Gauss-Radau, and Gauss-Lobatto quadrature. In
particular, we establish monotonicity of the bounds given by these quadrature
rules, compare the tightness of these bounds, and derive associated convergence
rates. To our knowledge, this is the first work to establish these properties
of Gauss-type quadrature for computing bilinear inverse forms, thus filling a
theoretical gap regarding this classical topic. We illustrate the empirical
benefits of our theoretical results by applying quadrature to speed up two
Markov Chain sampling procedures for (discrete) determinantal point processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01907</identifier>
 <datestamp>2016-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01907</id><created>2015-12-06</created><updated>2016-01-05</updated><authors><author><keyname>Roychowdhury</keyname><forenames>Mrinal Kanti</forenames></author></authors><title>An algorithm for computing CVTs for any Cantor distribution</title><categories>cs.IT math.IT</categories><comments>arXiv admin note: text overlap with arXiv:1509.06037</comments><msc-class>60Exx, 94Axx, 28A80</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Centroidal Voronoi tessellations (CVTs) are Voronoi tessellations of a region
such that the generating points of the tessellations are also the centroids of
the corresponding Voronoi regions with respect to a given probability measure.
CVT is a fundamental notion that has a wide spectrum of applications in
computational science and engineering. In this paper, an algorithm is given to
obtain all the CVTs with $n$-generators, for any positive integer $n$, of any
Cantor set generated by a pair of self-similar mappings given by $S_1(x)=r_1x$
and $S_2(x)=r_2x+(1-r_2)$ for $x\in \mathbb R$, where $r_1, r_2&gt;0$ and
$r_1+r_2&lt;1$, with respect to any probability distribution $P$ such that $P=p_1
P\circ S_1^{-1}+p_2 P\circ S_2^{-1}$, where $p_1, p_2&gt;0$ and $p_1+p_2=1$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01914</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01914</id><created>2015-12-07</created><authors><author><keyname>Zhang</keyname><forenames>Xiao</forenames></author></authors><title>Rademacher Complexity of the Restricted Boltzmann Machine</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Boltzmann machine, as a fundamental construction block of deep belief network
and deep Boltzmann machines, is widely used in deep learning community and
great success has been achieved. However, theoretical understanding of many
aspects of it is still far from clear. In this paper, we studied the Rademacher
complexity of both the asymptotic restricted Boltzmann machine and the
practical implementation with single-step contrastive divergence (CD-1)
procedure. Our results disclose the fact that practical implementation training
procedure indeed increased the Rademacher complexity of restricted Boltzmann
machines. A further research direction might be the investigation of the VC
dimension of a compositional function used in the CD-1 procedure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01915</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01915</id><created>2015-12-07</created><authors><author><keyname>Jiang</keyname><forenames>Guifei</forenames></author><author><keyname>Zhang</keyname><forenames>Dongmo</forenames></author><author><keyname>Perrussel</keyname><forenames>Laurent</forenames></author></authors><title>Knowledge Sharing in Coalitions</title><categories>cs.AI</categories><comments>This version corrected errors in its previous version published at
  AI'15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this paper is to investigate the interplay between knowledge
shared by a group of agents and its coalition ability. We characterize this
relation in the standard context of imperfect information concurrent game. We
assume that whenever a set of agents form a coalition to achieve a goal, they
share their knowledge before acting. Based on this assumption, we propose new
semantics for alternating-time temporal logic with imperfect information and
perfect recall. It turns out that this semantics is sufficient to preserve all
the desirable properties of coalition ability in traditional coalitional
logics. Meanwhile, we investigate how knowledge sharing within a group of
agents contributes to its coalitional ability through the interplay of
epistemic and coalition modalities. This work provides a partial answer to the
question: which kind of group knowledge is required for a group to achieve
their goals in the context of imperfect information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01921</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01921</id><created>2015-12-07</created><authors><author><keyname>Farsad</keyname><forenames>Nariman</forenames></author><author><keyname>Guo</keyname><forenames>Weisi</forenames></author><author><keyname>Chae</keyname><forenames>Chan-Byoung</forenames></author><author><keyname>Eckford</keyname><forenames>Andrew</forenames></author></authors><title>Stable Distributions as Noise Models for Molecular Communication</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we consider diffusion-based molecular communication timing
channels. Three different timing channels are presented based on three
different modulation techniques, i.e., i) modulation of the release timing of
the information particles, ii) modulation on the time between two consecutive
information particles of the same type, and iii) modulation on the time between
two consecutive information particles of different types. We show that each
channel can be represented as an additive noise channel, where the noise
follows one of the subclasses of stable distributions. We provide expressions
for the probability density function of the noise terms, and numerical
evaluations for the probability density function and cumulative density
function. We also show that the tails are longer than Gaussian distribution, as
expected.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01926</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01926</id><created>2015-12-07</created><authors><author><keyname>Rocki</keyname><forenames>Kamil</forenames></author></authors><title>Thinking Required</title><categories>cs.LG cs.AI cs.CL</categories><comments>18 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There exists a theory of a single general-purpose learning algorithm which
could explain the principles its operation. It assumes the initial rough
architecture, a small library of simple innate circuits which are prewired at
birth. and proposes that all significant mental algorithms are learned. Given
current understanding and observations, this paper reviews and lists the
ingredients of such an algorithm from architectural and functional
perspectives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01927</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01927</id><created>2015-12-07</created><authors><author><keyname>Chen</keyname><forenames>Haoran</forenames></author><author><keyname>Sun</keyname><forenames>Yanfeng</forenames></author><author><keyname>Gao</keyname><forenames>Junbin</forenames></author><author><keyname>Hu</keyname><forenames>Yongli</forenames></author></authors><title>Fast Optimization Algorithm on Riemannian Manifolds and Its Application
  in Low-Rank Representation</title><categories>cs.NA cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper addresses the problem of optimizing a class of composite functions
on Riemannian manifolds and a new first order optimization algorithm (FOA) with
a fast convergence rate is proposed. Through the theoretical analysis for FOA,
it has been proved that the algorithm has quadratic convergence. The
experiments in the matrix completion task show that FOA has better performance
than other first order optimization methods on Riemannian manifolds. A fast
subspace pursuit method based on FOA is proposed to solve the low-rank
representation model based on augmented Lagrange method on the low rank matrix
variety. Experimental results on synthetic and real data sets are presented to
demonstrate that both FOA and SP-RPRG(ALM) can achieve superior performance in
terms of faster convergence and higher accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01952</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01952</id><created>2015-12-07</created><authors><author><keyname>Barylska</keyname><forenames>Kamila</forenames></author><author><keyname>Ochma&#x144;ski</keyname><forenames>Edward</forenames></author></authors><title>Hierarchy of persistence with respect to the length of action's
  disability</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The notion of persistence, based on the rule &quot;no action can disable another
one&quot; is one of the classical notions in concurrency theory. It is also one of
the issues discussed in the Petri net theory. We recall two ways of
generalization of this notion: the first is &quot;no action can kill another one&quot;
(called l/l-persistence) and the second &quot;no action can kill another enabled
one&quot; (called the delayed persistence, or shortly e/l-persistence). Afterwards
we introduce a more precise notion, called e/l-k-persistence, in which one
action disables another one for no longer than a specified number k of single
sequential steps. Then we consider an infinite hie\-rarchy of such e/l-k
persistencies. We prove that if an action is disabled, and not killed, by
another one, it can not be postponed indefinitely. Afterwards, we investigate
the set of markings in which two actions are enabled simultaneously, and also
the set of reachable markings with that feature. We show that the minimum of
the latter is finite and effectively computable. Finally we deal with decision
problems about e/l-k persistencies. We show that all the kinds of e/l-k
persistencies are decidable with respect to steps, markings and nets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01968</identifier>
 <datestamp>2016-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01968</id><created>2015-12-07</created><updated>2015-12-31</updated><authors><author><keyname>Harsha</keyname><forenames>Prahladh</forenames></author><author><keyname>Jain</keyname><forenames>Rahul</forenames></author><author><keyname>Radhakrishnan</keyname><forenames>Jaikumar</forenames></author></authors><title>Relaxed partition bound is quadratically tight for product distributions</title><categories>cs.CC</categories><comments>The proofs in the paper justify the main theorem in terms of
  partition bound and NOT relaxed partition bound as stated in the abstract and
  paper. A revised version of the paper (with the modified claims) will be
  posted shortly</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $f : \{0,1\}^n \times \{0,1\}^n \rightarrow \{0,1\}$ be a 2-party
function. For every product distribution $\mu$ on $\{0,1\}^n \times \{0,1\}^n$,
we show that $${\mathsf{CC}}^\mu_{0.49}(f) = O\left(\left(\log
{\mathsf{rprt}}_{1/4}(f) \cdot \log \log
{\mathsf{rprt}}_{1/4}(f)\right)^2\right),$$ where
${\mathsf{CC}}^\mu_\varepsilon(f)$ is the distributional communication
complexity with error at most $\varepsilon$ under the distribution $\mu$ and
${\mathsf{rprt}}_{1/4}(f)$ is the {\em relaxed partition bound} of the function
$f$, as introduced by Kerenidis et al [Proc. FOCS 2012]. A similar upper bound
for communication complexity for product distributions in terms of information
complexity was recently (and independently) obtained by Kol [ECCC Tech. Report
TR15-168].
  We show a similar result for query complexity under product distributions.
Let $g : \{0,1\}^n \rightarrow \{0,1\}$ be a function. For every bit-wise
product distribution $\mu$ on $\{0,1\}^n$, we show that
$${\mathsf{QC}}^\mu_{1/3}(g) = O\left(\left( \log {\mathsf{qrprt}}_{1/4}(g)
\log \log{\mathsf{qrprt}}_{1/4}(g))\right)^2 \right),$$ where
${\mathsf{QC}}^\mu_{1/3}(g)$ is the distributional query complexity with error
at most $1/3$ under the distribution $\mu$ and ${\mathsf{qrprt}}_{1/4}(g))$ is
the {\em relaxed query partition bound} of the function $g$.
  Recall that relaxed partition bounds were introduced (in both communication
complexity and query complexity models) to provide LP-based lower bounds to
randomized communication complexity and randomized query complexity. Our
results demonstrate that these lower bounds are polynomially tight for {\em
product} distributions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01976</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01976</id><created>2015-12-07</created><authors><author><keyname>Gizzie</keyname><forenames>Nina</forenames></author><author><keyname>Mayne</keyname><forenames>Richard</forenames></author><author><keyname>Yitzchaik</keyname><forenames>Shlomo</forenames></author><author><keyname>Ikbal</keyname><forenames>Muhamad</forenames></author><author><keyname>Adamatzky</keyname><forenames>Andrew</forenames></author></authors><title>Living Wires - Effects of Size and Coating of Gold Nanoparticles in
  Altering the Electrical Properties of Physarum polycephalum and Lettuce
  Seedlings</title><categories>cs.ET</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The manipulation of biological substrates is becoming more popular route
towards generating novel computing devices. Physarum polycephalum is used as a
model organism in biocomputing because it can create `wires' for use in hybrid
circuits; programmable growth by manipulation through external stimuli and the
ability withstanding a current and its tolerance to hybridisation with a
variety of nano/microparticles. Lettuce seedlings have also had previous
interest invested in them for generating plant wires, although currently there
is little information as to their suitability for such applications. In this
study both P. polycephalum and Lettuce seedlings were hybridised with gold
nanoparticles - functionalised and unfunctionalised - to explore their uptake,
toxicological effects and, crucially, any alterations in electrical properties
they bestow upon the organisms. Using various microscopy techniques it was
shown that P. polycephalum and lettuce seedlings are able to internalize
nanoparticles and assemble them in vivo, however some toxicological effects
were observed. The electrical resistance of both lettuce seedlings and P.
polycephalum was found to decrease, the most significant reduction being with
lettuce seedlings whose resistance reduced from 3MOhms to 0.5MOhms. We conclude
that gold is a suitable nanomaterial for biohybridisation specifically in
creating conductive pathways for more efficient biological wires in
self-growing hybrid circuitry.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01978</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01978</id><created>2015-12-07</created><authors><author><keyname>Lipari</keyname><forenames>Giuseppe</forenames></author><author><keyname>Palopoli</keyname><forenames>Luigi</forenames></author></authors><title>Real-Time scheduling: from hard to soft real-time systems</title><categories>cs.OS</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Real-time systems are traditionally classified into hard real-time and soft
real-time: in the first category we have safety critical real-time systems
where missing a deadline can have catastrophic consequences, whereas in the
second class we find systems or which we need to optimise the Quality of
service provided to the user. However, the frontier between these two classes
is thinner than one may think, and many systems that were considered as hard
real-time in the past should now be reconsidered under a different light. In
this paper we shall first recall the fundamental notion of time-predictability
and criticality, in order to understand where the real-time deadlines that we
use in our theoretical models come from. We shall then introduce the model of a
soft real-time system and present one popular method for scheduling hard and
soft real-time tasks, the resource reservation framework. Finally, we shall
show how resource reservation techniques can be successfully applied to the
design of classical control systems, thus adding robustness to the system and
increasing resource utilisation and performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01979</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01979</id><created>2015-12-07</created><authors><author><keyname>Cicone</keyname><forenames>Antonio</forenames></author><author><keyname>Liu</keyname><forenames>Jingfang</forenames></author><author><keyname>Zhou</keyname><forenames>Haomin</forenames></author></authors><title>Hyperspectral Chemical Plume Detection Algorithms Based On
  Multidimensional Iterative Filtering Decomposition</title><categories>math.NA cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Chemicals released in the air can be extremely dangerous for human beings and
the environment. Hyperspectral images can be used to identify chemical plumes,
however the task can be extremely challenging. Assuming we know a priori that
some chemical plume, with a known frequency spectrum, has been photographed
using a hyperspectral sensor, we can use standard techniques like the so called
matched filter or adaptive cosine estimator, plus a properly chosen threshold
value, to identify the position of the chemical plume. However, due to noise
and sensors fault, the accurate identification of chemical pixels is not easy
even in this apparently simple situation. In this paper we present a
post-processing tool that, in a completely adaptive and data driven fashion,
allows to improve the performance of any classification methods in identifying
the boundaries of a plume. This is done using the Multidimensional Iterative
Filtering (MIF) algorithm (arXiv:1411.6051, arXiv:1507.07173), which is a
non-stationary signal decomposition method like the pioneering Empirical Mode
Decomposition (EMD) method. Moreover, based on the MIF technique, we propose
also a pre-processing method that allows to decorrelate and mean-center a
hyperspectral dataset. The Cosine Similarity measure, which often fails in
practice, appears to become a successful and outperforming classifier when
equipped with such pre-processing method. We show some examples of the proposed
methods when applied to real life problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01980</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01980</id><created>2015-12-07</created><authors><author><keyname>Bhardwaj</keyname><forenames>Akashdeep</forenames></author><author><keyname>Subrahmanyam</keyname><forenames>G. V. B.</forenames></author><author><keyname>Avasthi</keyname><forenames>Vinay</forenames></author><author><keyname>Sastry</keyname><forenames>Hanumat</forenames></author></authors><title>Ransomware: A Rising Threat of new age Digital Extortion</title><categories>cs.CR cs.CY</categories><comments>6 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  What if someone stopped you from accessing your files or even using your
computer. what if they demanded an amount to get the access back to you. With
most financial and social interactions revolving around three critical aspects
firstly the use of digital data and files, secondly computer systems and last
the unsecure internet. This is where Ransomware using Bitcoin has become a
major cause of concern in form of a new age digital extortion threat to home
and corporate user alike. In this paper we discuss Ransomware on the ways and
methods adopted by cyber criminals to holding ransom an innocent users digital
data and systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01984</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01984</id><created>2015-12-07</created><authors><author><keyname>Abeni</keyname><forenames>Luca</forenames></author><author><keyname>Lipari</keyname><forenames>Giuseppe</forenames></author><author><keyname>Parri</keyname><forenames>Andrea</forenames></author><author><keyname>Sun</keyname><forenames>Youcheng</forenames></author></authors><title>Parallel and sequential reclaiming in multicore real-time global
  scheduling</title><categories>cs.OS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When integrating hard, soft and non-real-time tasks in general purpose
operating systems, it is necessary to provide temporal isolation so that the
timing properties of one task do not depend on the behaviour of the others.
However, strict budget enforcement can lead to inefficient use of the
computational resources in the presence of tasks with variable workload. Many
resource reclaiming algorithms have been proposed in the literature for single
processor scheduling, but not enough work exists for global scheduling in
multiprocessor systems. In this report, we propose two reclaiming algorithms
for multiprocessor global scheduling and we prove their correctness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01993</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01993</id><created>2015-12-07</created><authors><author><keyname>Govada</keyname><forenames>Aruna</forenames></author><author><keyname>Ranjani</keyname><forenames>Shree</forenames></author><author><keyname>Viswanathan</keyname><forenames>Aditi</forenames></author><author><keyname>Sahay</keyname><forenames>S. K.</forenames></author></authors><title>A Novel Approach to Distributed Multi-Class SVM</title><categories>cs.LG cs.DC</categories><comments>8 Pages</comments><journal-ref>Transactions on Machine Learning and Artificial Intelligence, Vol.
  2, No. 5, p. 72, 2014</journal-ref><doi>10.14738/tmlai.25.562</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With data sizes constantly expanding, and with classical machine learning
algorithms that analyze such data requiring larger and larger amounts of
computation time and storage space, the need to distribute computation and
memory requirements among several computers has become apparent. Although
substantial work has been done in developing distributed binary SVM algorithms
and multi-class SVM algorithms individually, the field of multi-class
distributed SVMs remains largely unexplored. This research proposes a novel
algorithm that implements the Support Vector Machine over a multi-class dataset
and is efficient in a distributed environment (here, Hadoop). The idea is to
divide the dataset into half recursively and thus compute the optimal Support
Vector Machine for this half during the training phase, much like a divide and
conquer approach. While testing, this structure has been effectively exploited
to significantly reduce the prediction time. Our algorithm has shown better
computation time during the prediction phase than the traditional sequential
SVM methods (One vs. One, One vs. Rest) and out-performs them as the size of
the dataset grows. This approach also classifies the data with higher accuracy
than the traditional multi-class algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01994</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01994</id><created>2015-12-07</created><authors><author><keyname>Levit</keyname><forenames>Vadim E.</forenames></author><author><keyname>Mandrescu</keyname><forenames>Eugen</forenames></author></authors><title>On Konig-Egervary Collections of Maximum Critical Independent Sets</title><categories>cs.DM math.CO</categories><comments>9 pages, 3 figures</comments><msc-class>05C69, 05C70 (Primary), 05A20(Secondary)</msc-class><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let G be a simple graph with vertex set V(G). A set S is independent if no
two vertices from S are adjacent. The graph G is known to be a Konig-Egervary
if alpha(G)+mu(G)= |V(G)|, where alpha(G) denotes the size of a maximum
independent set and mu(G) is the cardinality of a maximum matching. The number
d(X)= |X|-|N(X)| is the difference of X, and an independent set A is critical
if d(A) = max{d(I):I is an independent set in G} (Zhang; 1990). Let Omega(G)
denote the family of all maximum independent sets. Let us say that a family
Gamma of independent sets is a Konig-Egervary collection if |Union of Gamma| +
|Intersection of Gamma| = 2alpha(G) (Jarden, Levit, Mandrescu; 2015). In this
paper, we show that if the family of all maximum critical independent sets of a
graph G is a Konig-Egervary collection, then G is a Konig-Egervary graph. It
generalizes one of our conjectures recently validated in (Short; 2015).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01996</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01996</id><created>2015-12-07</created><authors><author><keyname>Grabowski</keyname><forenames>Szymon</forenames></author><author><keyname>Cis&#x142;ak</keyname><forenames>Aleksander</forenames></author></authors><title>A bloated FM-index reducing the number of cache misses during the search</title><categories>cs.DS</categories><comments>5 figures</comments><msc-class>68W32</msc-class><acm-class>H.3.3; I.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The FM-index is a well-known compressed full-text index, based on the
Burrows-Wheeler transform (BWT). During a pattern search, the BWT sequence is
accessed at &quot;random&quot; locations, which is cache-unfriendly. In this paper, we
are interested in speeding up the FM-index by working on $q$-grams rather than
individual characters, at the cost of using more space. The first presented
variant is related to an inverted index on $q$-grams, yet the occurrence lists
in our solution are in the sorted suffix order rather than text order in a
traditional inverted index. This variant obtains $O(m/|CL| + \log n \log m)$
cache misses in the worst case, where $n$ and $m$ are the text and pattern
lengths, respectively, and $|CL|$ is the CPU cache line size, in symbols
(typically 64 in modern hardware). This index is often several times faster
than the fastest known FM-indexes (especially for long patterns), yet the space
requirements are enormous, $O(n\log^2 n)$ bits in theory and about $80n$-$95n$
bytes in practice. For this reason, we dub our approach FM-bloated. The second
presented variant requires $O(n\log n)$ bits of space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.01998</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.01998</id><created>2015-12-07</created><authors><author><keyname>Hossain</keyname><forenames>M M Aftab</forenames></author><author><keyname>Cavdar</keyname><forenames>Cicek</forenames></author><author><keyname>Bj&#xf6;rnson</keyname><forenames>Emil</forenames></author><author><keyname>J&#xe4;ntti</keyname><forenames>Riku</forenames></author></authors><title>Energy Efficiency of Massive MIMO: Coping with Daily Load Variation</title><categories>cs.NI cs.IT math.IT</categories><comments>27 pages, 10 Figures. arXiv admin note: substantial text overlap with
  arXiv:1510.00649</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Massive MIMO is a promising technique to meet the exponential growth of
mobile traffic demand. However, contrary to the current systems, energy
consumption of next generation networks is required to be load adaptive as the
network load varies significantly throughout the day. In this paper, we propose
a load adaptive multi-cell massive MIMO system where each base station (BS)
adapts the number of antennas to the daily load profile (DLP) in order to
maximize the downlink energy efficiency (EE). In order to incorporate the DLP,
the load at each BS is modeled as an M/G/m/m state dependent queue under the
assumption that the network is dimensioned to serve a maximum number of users
at the peak load. The EE maximization problem is formulated in a game theoretic
framework where the number of antennas to be used by a BS is determined through
best response iteration. This load adaptive system achieves around 24% higher
EE and saves around 40% energy compared to a baseline system where the BSs
always run with the fixed number of antennas that is most energy efficient at
the peak load and that can be switched off when there is no traffic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02005</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02005</id><created>2015-12-07</created><authors><author><keyname>Bhardwaj</keyname><forenames>Akashdeep</forenames></author><author><keyname>Subrahmanyam</keyname><forenames>G. V. B.</forenames></author><author><keyname>Avasthi</keyname><forenames>Vinay</forenames></author><author><keyname>Sastry</keyname><forenames>Hanumat</forenames></author></authors><title>Three Tier Network Architecture to mitigate DDoS Attacks on Hybrid Cloud
  Environments</title><categories>cs.NI cs.CR</categories><comments>9 pages, 14 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the rise of cyber-attacks on cloud systems globally, Cloud Service
Providers, Data carrier and hosting providers are mandated to provide careful
consideration to the novel challenges posed and requirements for attacks and
more specifically DDoS protection in large hosting environment setups. For
Cloud Service providers, DDoS attack poses a high risk to their infrastructure
as well to the customer data in order to preserve service level agreements or
SLAs and avoid collateral damages. This paper proposes use of multi-tiered
network design based on Hybrid cloud solution that has an on premise solution
acceptable to the organizations IT security and Operations team as well as a
public cloud infrastructure capable of handling hurricane sized DDoS storms
that are targeted towards their servers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02006</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02006</id><created>2015-12-07</created><authors><author><keyname>Bhardwaj</keyname><forenames>Akashdeep</forenames></author><author><keyname>Subrahmanyam</keyname><forenames>GVB</forenames></author><author><keyname>Avasthi</keyname><forenames>Vinay</forenames></author><author><keyname>Sastry</keyname><forenames>Hanumat</forenames></author></authors><title>Security Algorithms for Cloud Computing Environment</title><categories>cs.CR</categories><comments>6 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With growing awareness and concerns regards to Cloud Computing and
Information Security, there is growing awareness and usage of Security
Algorithms into data systems and processes. This paper presents a brief
overview and comparison of Cryptographic algorithms, with an emphasis on
Symmetric algorithms which should be used for Cloud based applications and
services that require data and link encryption. In this paper we review
Symmetric and Asymmetric algorithms with emphasis on Symmetric Algorithms for
security consideration on which one should be used for Cloud based applications
and services that require data and link encryption
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02009</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02009</id><created>2015-12-07</created><authors><author><keyname>Chen</keyname><forenames>Bei</forenames></author><author><keyname>Zhu</keyname><forenames>Jun</forenames></author><author><keyname>Yang</keyname><forenames>Nan</forenames></author><author><keyname>Tian</keyname><forenames>Tian</forenames></author><author><keyname>Zhou</keyname><forenames>Ming</forenames></author><author><keyname>Zhang</keyname><forenames>Bo</forenames></author></authors><title>Jointly Modeling Topics and Intents with Global Order Structure</title><categories>cs.CL cs.IR cs.LG</categories><comments>Accepted by AAAI 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modeling document structure is of great importance for discourse analysis and
related applications. The goal of this research is to capture the document
intent structure by modeling documents as a mixture of topic words and
rhetorical words. While the topics are relatively unchanged through one
document, the rhetorical functions of sentences usually change following
certain orders in discourse. We propose GMM-LDA, a topic modeling based
Bayesian unsupervised model, to analyze the document intent structure
cooperated with order information. Our model is flexible that has the ability
to combine the annotations and do supervised learning. Additionally, entropic
regularization can be introduced to model the significant divergence between
topics and intents. We perform experiments in both unsupervised and supervised
settings, results show the superiority of our model over several
state-of-the-art baselines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02011</identifier>
 <datestamp>2016-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02011</id><created>2015-12-07</created><updated>2016-01-20</updated><authors><author><keyname>Fran&#xe7;ois-Lavet</keyname><forenames>Vincent</forenames></author><author><keyname>Fonteneau</keyname><forenames>Raphael</forenames></author><author><keyname>Ernst</keyname><forenames>Damien</forenames></author></authors><title>How to Discount Deep Reinforcement Learning: Towards New Dynamic
  Strategies</title><categories>cs.LG cs.AI</categories><comments>NIPS 2015 Deep Reinforcement Learning Workshop</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using deep neural nets as function approximator for reinforcement learning
tasks have recently been shown to be very powerful for solving problems
approaching real-world complexity. Using these results as a benchmark, we
discuss the role that the discount factor may play in the quality of the
learning process of a deep Q-network (DQN). When the discount factor
progressively increases up to its final value, we empirically show that it is
possible to significantly reduce the number of learning steps. When used in
conjunction with a varying learning rate, we empirically show that it
outperforms original DQN on several experiments. We relate this phenomenon with
the instabilities of neural networks when they are used in an approximate
Dynamic Programming setting. We also describe the possibility to fall within a
local optimum during the learning process, thus connecting our discussion with
the exploration/exploitation dilemma.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02013</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02013</id><created>2015-12-07</created><authors><author><keyname>Popescu</keyname><forenames>Adrian</forenames></author><author><keyname>Gadeski</keyname><forenames>Etienne</forenames></author><author><keyname>Borgne</keyname><forenames>Herv&#xe9; Le</forenames></author></authors><title>Scalable domain adaptation of convolutional neural networks</title><categories>cs.CV</categories><comments>technical report, 6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convolutional neural networks (CNNs) tend to become a standard approach to
solve a wide array of computer vision problems. Besides important theoretical
and practical advances in their design, their success is built on the existence
of manually labeled visual resources, such as ImageNet. The creation of such
datasets is cumbersome and here we focus on alternatives to manual labeling. We
hypothesize that new resources are of uttermost importance in domains which are
not or weakly covered by ImageNet, such as tourism photographs. We first
collect noisy Flickr images for tourist points of interest and apply automatic
or weakly-supervised reranking techniques to reduce noise. Then, we learn
domain adapted models with a standard CNN architecture and compare them to a
generic model obtained from ImageNet. Experimental validation is conducted with
publicly available datasets, including Oxford5k, INRIA Holidays and Div150Cred.
Results show that low-cost domain adaptation improves results compared to the
use of generic models but also compared to strong non-CNN baselines such as
triangulation embedding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02014</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02014</id><created>2015-12-07</created><authors><author><keyname>Fotouhi</keyname><forenames>Babak</forenames></author><author><keyname>Shirkoohi</keyname><forenames>Mehrdad Khani</forenames></author></authors><title>Temporal Dynamics of Connectivity and Epidemic Properties of Growing
  Networks</title><categories>physics.soc-ph cs.SI</categories><doi>10.1103/PhysRevE.93.012301</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional mathematical models of epidemic disease had for decades
conventionally considered static structure for contacts. Recently, an upsurge
of theoretical inquiry has strived towards rendering the models more realistic
by incorporating the temporal aspects of networks of contacts, societal and
online, that are of interest in the study of epidemics (and other similar
diffusion processes). However, temporal dynamics have predominantly focused on
link fluctuations and nodal activities, and less attention has been paid to the
growth of the underlying network. Many real networks grow: online networks are
evidently in constant growth, and societal networks can grow due to migration
flux and reproduction. The effect of network growth on the epidemic properties
of networks is hitherto unknown---mainly due to the predominant focus of the
network growth literature on the so-called steady-state. This paper takes a
step towards alleviating this gap. We analytically study the degree dynamics of
a given arbitrary network that is subject to growth. We use the theoretical
findings to predict the epidemic properties of the network as a function of
time. We observe that the introduction of new individuals into the network can
enhance or diminish its resilience against endemic outbreaks, and investigate
how this regime shift depends upon the connectivity of newcomers and on how
they establish connections to existing nodes. Throughout, theoretical findings
are corroborated with Monte Carlo simulations over synthetic and real networks.
The results shed light on the effects of network growth on the future epidemic
properties of networks, and offers insights for devising a-priori immunization
strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02016</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02016</id><created>2015-12-07</created><authors><author><keyname>Chen</keyname><forenames>Bei</forenames></author><author><keyname>Chen</keyname><forenames>Ning</forenames></author><author><keyname>Zhu</keyname><forenames>Jun</forenames></author><author><keyname>Song</keyname><forenames>Jiaming</forenames></author><author><keyname>Zhang</keyname><forenames>Bo</forenames></author></authors><title>Discriminative Nonparametric Latent Feature Relational Models with Data
  Augmentation</title><categories>cs.LG stat.ML</categories><comments>Accepted by AAAI 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a discriminative nonparametric latent feature relational model
(LFRM) for link prediction to automatically infer the dimensionality of latent
features. Under the generic RegBayes (regularized Bayesian inference)
framework, we handily incorporate the prediction loss with probabilistic
inference of a Bayesian model; set distinct regularization parameters for
different types of links to handle the imbalance issue in real networks; and
unify the analysis of both the smooth logistic log-loss and the piecewise
linear hinge loss. For the nonconjugate posterior inference, we present a
simple Gibbs sampler via data augmentation, without making restricting
assumptions as done in variational methods. We further develop an approximate
sampler using stochastic gradient Langevin dynamics to handle large networks
with hundreds of thousands of entities and millions of links, orders of
magnitude larger than what existing LFRM models can process. Extensive studies
on various real networks show promising performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02017</identifier>
 <datestamp>2016-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02017</id><created>2015-12-07</created><updated>2016-01-14</updated><authors><author><keyname>Mahendran</keyname><forenames>Aravindh</forenames></author><author><keyname>Vedaldi</keyname><forenames>Andrea</forenames></author></authors><title>Visualizing Deep Convolutional Neural Networks Using Natural Pre-Images</title><categories>cs.CV</categories><comments>A substantially extended version of
  http://www.robots.ox.ac.uk/~vedaldi/assets/pubs/mahendran15understanding.pdf.
  arXiv admin note: text overlap with arXiv:1412.0035</comments><msc-class>68T45</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Image representations, from SIFT and bag of visual words to Convolutional
Neural Networks (CNNs), are a crucial component of almost all computer vision
systems. However, our understanding of them remains limited. In this paper we
study several landmark representations, both shallow and deep, by a number of
complementary visualization techniques. These visualizations are based on the
concept of &quot;natural pre-image&quot;, namely a naturally-looking image whose
representation has some notable property. We study in particular three such
visualizations: inversion, in which the aim is to reconstruct an image from its
representation, activation maximization, in which we search for patterns that
maximally stimulate a representation component, and caricaturization, in which
the visual patterns that a representation detects in an image are exaggerated.
We pose this as a regularized energy-minimization framework and demonstrate its
generality and effectiveness. In particular, we show that this method can
invert representations such as HOG more accurately than recent alternatives
while being applicable to CNNs too. Among our findings, we show that several
layers in CNNs retain photographically accurate information about the image,
with different degrees of geometric and photometric invariance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02019</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02019</id><created>2015-12-07</created><updated>2016-02-17</updated><authors><author><keyname>DPHEP Collaboration</keyname></author><author><keyname>Amerio</keyname><forenames>Silvia</forenames></author><author><keyname>Barbera</keyname><forenames>Roberto</forenames></author><author><keyname>Berghaus</keyname><forenames>Frank</forenames></author><author><keyname>Blomer</keyname><forenames>Jakob</forenames></author><author><keyname>Branson</keyname><forenames>Andrew</forenames></author><author><keyname>Cancio</keyname><forenames>Germ&#xe1;n</forenames></author><author><keyname>Cartaro</keyname><forenames>Concetta</forenames></author><author><keyname>Chen</keyname><forenames>Gang</forenames></author><author><keyname>Dallmeier-Tiessen</keyname><forenames>S&#xfc;nje</forenames></author><author><keyname>Diaconu</keyname><forenames>Cristinel</forenames></author><author><keyname>Ganis</keyname><forenames>Gerardo</forenames></author><author><keyname>Gheata</keyname><forenames>Mihaela</forenames></author><author><keyname>Hara</keyname><forenames>Takanori</forenames></author><author><keyname>Herner</keyname><forenames>Ken</forenames></author><author><keyname>Hildreth</keyname><forenames>Mike</forenames></author><author><keyname>Jones</keyname><forenames>Roger</forenames></author><author><keyname>Kluth</keyname><forenames>Stefan</forenames></author><author><keyname>Kr&#xfc;cker</keyname><forenames>Dirk</forenames></author><author><keyname>Lassila-Perini</keyname><forenames>Kati</forenames></author><author><keyname>Maggi</keyname><forenames>Marcello</forenames></author><author><keyname>de Lucas</keyname><forenames>Jesus Marco</forenames></author><author><keyname>Mele</keyname><forenames>Salvatore</forenames></author><author><keyname>Pace</keyname><forenames>Alberto</forenames></author><author><keyname>Schr&#xf6;der</keyname><forenames>Matthias</forenames></author><author><keyname>Shamdasani</keyname><forenames>Jetendr</forenames></author><author><keyname>Shiers</keyname><forenames>Jamie</forenames></author><author><keyname>Smith</keyname><forenames>Tim</forenames></author><author><keyname>Sobie</keyname><forenames>Randall</forenames></author><author><keyname>South</keyname><forenames>David Michael</forenames></author><author><keyname>Verbytskyi</keyname><forenames>Andrii</forenames></author><author><keyname>Viljoen</keyname><forenames>Matthew</forenames></author><author><keyname>Wang</keyname><forenames>Lu</forenames></author><author><keyname>Zimmermann</keyname><forenames>Markus</forenames></author></authors><title>Status Report of the DPHEP Collaboration: A Global Effort for
  Sustainable Data Preservation in High Energy Physics</title><categories>hep-ex cs.DL</categories><comments>report, 60 pages</comments><doi>10.5281/zenodo.46158</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Data from High Energy Physics (HEP) experiments are collected with
significant financial and human effort and are mostly unique. An
inter-experimental study group on HEP data preservation and long-term analysis
was convened as a panel of the International Committee for Future Accelerators
(ICFA). The group was formed by large collider-based experiments and
investigated the technical and organizational aspects of HEP data preservation.
An intermediate report was released in November 2009 addressing the general
issues of data preservation in HEP and an extended blueprint paper was
published in 2012. In July 2014 the DPHEP collaboration was formed as a result
of the signature of the Collaboration Agreement by seven large funding agencies
(others have since joined or are in the process of acquisition) and in June
2015 the first DPHEP Collaboration Workshop and Collaboration Board meeting
took place.
  This status report of the DPHEP collaboration details the progress during the
period from 2013 to 2015 inclusive.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02033</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02033</id><created>2015-12-07</created><updated>2015-12-09</updated><authors><author><keyname>Karmon</keyname><forenames>Danny</forenames></author><author><keyname>Keshet</keyname><forenames>Joseph</forenames></author></authors><title>Risk Minimization in Structured Prediction using Orbit Loss</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new surrogate loss function called orbit loss in the
structured prediction framework, which has good theoretical and practical
advantages. While the orbit loss is not convex, it has a simple analytical
gradient and a simple perceptron-like learning rule. We analyze the new loss
theoretically and state a PAC-Bayesian generalization bound. We also prove that
the new loss is consistent in the strong sense; namely, the risk achieved by
the set of the trained parameters approaches the infimum risk achievable by any
linear decoder over the given features. Methods that are aimed at risk
minimization, such as the structured ramp loss, the structured probit loss and
the direct loss minimization require at least two inference operations per
training iteration. In this sense, the orbit loss is more efficient as it
requires only one inference operation per training iteration, while yields
similar performance. We conclude the paper with an empirical comparison of the
proposed loss function to the structured hinge loss, the structured ramp loss,
the structured probit loss and the direct loss minimization method on several
benchmark datasets and tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02047</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02047</id><created>2015-12-07</created><updated>2015-12-08</updated><authors><author><keyname>Dang</keyname><forenames>Duc-Cuong</forenames></author><author><keyname>Eremeev</keyname><forenames>Anton V.</forenames></author><author><keyname>Lehre</keyname><forenames>Per Kristian</forenames></author></authors><title>Level-Based Analysis of Genetic Algorithms for Combinatorial
  Optimization</title><categories>cs.NE</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper is devoted to upper bounds on run-time of Non-Elitist Genetic
Algorithms until some target subset of solutions is visited for the first time.
In particular, we consider the sets of optimal solutions and the sets of local
optima as the target subsets. Previously known upper bounds are improved by
means of drift analysis. Finally, we propose conditions ensuring that a
Non-Elitist Genetic Algorithm efficiently finds approximate solutions with
constant approximation ratio on the class of combinatorial optimization
problems with guaranteed local optima (GLO).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02054</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02054</id><created>2015-12-07</created><authors><author><keyname>Hoffmann</keyname><forenames>Isabella</forenames></author><author><keyname>Kurz</keyname><forenames>Sascha</forenames></author><author><keyname>Rambau</keyname><forenames>J&#xf6;rg</forenames></author></authors><title>The Maximum Scatter TSP on a Regular Grid</title><categories>cs.DM math.OC</categories><comments>6 pages, 2 figures; to appear in OR Proceedings 2015</comments><msc-class>68W25, 90B99, 68R99</msc-class><acm-class>F.2.1; G.1.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the maximum scatter traveling salesman problem the objective is to find a
tour that maximizes the shortest distance between any two consecutive nodes.
This model can be applied to manufacturing processes, particularly laser
melting processes. We extend an algorithm by Arkin et al. that yields optimal
solutions for nodes on a line to a regular $m \times n$-grid. The new algorithm
$\textsc{Weave}(m,n)$ takes linear time to compute an optimal tour in some
cases. It is asymptotically optimal and a $\frac{\sqrt{10}}{5}$-approximation
for the $3\times 4$-grid, which is the worst case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02059</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02059</id><created>2015-12-07</created><authors><author><keyname>Niesen</keyname><forenames>Urs</forenames></author><author><keyname>Ekambaram</keyname><forenames>Venkatesan N.</forenames></author><author><keyname>Jose</keyname><forenames>Jubin</forenames></author><author><keyname>Wu</keyname><forenames>Xinzhou</forenames></author></authors><title>Vehicular Ranging using Periodic Broadcasts</title><categories>cs.NI</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of precise range estimation between pairs of moving
vehicles using periodic broadcast messages. The transmitter and receivers are
not time synchronized and one needs to explicitly account for the clock offset
and clock drift in addition to the vehicle motion to obtain accurate range
estimates. We develop a range estimation algorithm based on a local polynomial
smoothing of the vehicle motion and experimentally verify that the performance
is close to that obtained using unicast round-trip time ranging. This broadcast
approach is of interest particularly in the context of dedicated short-range
communications (DSRC) wherein periodic broadcast safety messages are exchanged
between vehicles. We propose to exploit these broadcast messages to perform
ranging. Our scheme requires additional timestamp information to be transmitted
as part of the DSRC messages, and we develop a novel timestamp compression
algorithm to minimize the resulting overhead. We validate our proposed
algorithm on experimental data and show that it is able to achieve sub-meter
ranging accuracies in vehicular scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02062</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02062</id><created>2015-12-07</created><authors><author><keyname>Sanabria-Russo</keyname><forenames>Luis</forenames></author><author><keyname>Bellalta</keyname><forenames>Boris</forenames></author></authors><title>Traffic Differentiation in Dense Collision-free WLANs</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ability to perform traffic differentiation is a promising feature of the
current Medium Access Control (MAC) in Wireless Local Area Networks (WLANs).
The Enhaced Distributed Channel Access (EDCA) MAC protocol for WLANs proposes
up to four Access Categories (AC) that can be mapped to different traffic
priorities. High priority ACs are allowed to transmit more often than low
priority ACs, providing a way of prioritising delay sensitive traffic like
voice calls or video streaming. Further, EDCA also considers the intricacies
related to the management of multiple queues, virtual collisions and priority.
Nevertheless, EDCA falls short in efficiency when performing in dense WLAN
scenarios. Its collision-prone contention mechanism degrades the overall
throughput to the point of starving low priority ACs.
  Carrier Sense Multiple Access with Enhanced Collision Avoidance (CSMA/ECA) is
a compatible MAC protocol for WLANs which is also capable of providing traffic
differentiation. Contrary to EDCA, CSMA/ECA uses a contention mechanism with a
deterministic backoff technique which is capable of constructing collision-free
schedules for many nodes with multiple ACs, extending the network capacity
without starving low priority ACs, as experienced in EDCA. This work analyses
traffic differentiation with CSMA/ECA by describing the mechanisms used to
construct collision-free schedules with multiple queues. Furthermore, it
introduces a way to eliminate Virtual Collisions (VC), which also contribute to
the throughput degradation in EDCA WLANs. Finally, it provides simulation
results that show CSMA/ECA outperforming EDCA in different commonly-found
networking scenarios, including coexistence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02068</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02068</id><created>2015-12-07</created><authors><author><keyname>Mozes</keyname><forenames>Shay</forenames></author><author><keyname>Nikolaev</keyname><forenames>Cyril</forenames></author><author><keyname>Nussbaum</keyname><forenames>Yahav</forenames></author><author><keyname>Weimann</keyname><forenames>Oren</forenames></author></authors><title>Minimum Cut of Directed Planar Graphs in O(nloglogn) Time</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give an $O(n \log \log n)$ time algorithm for computing the minimum cut
(or equivalently, the shortest cycle) of a weighted directed planar graph. This
improves the previous fastest $O(n\log^2 n)$ solution [SODA'04]. Interestingly,
while in undirected planar graphs both min-cut and min $st$-cut have $O(n \log
\log n)$ solutions [ESA'11, STOC'11], in directed planar graphs our result
makes min-cut faster than min $st$-cut, which currently requires $O(n \log n)$
[J. ACM'09].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02072</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02072</id><created>2015-12-07</created><authors><author><keyname>P&#xfc;sp&#xf6;ki</keyname><forenames>Zsuzsanna</forenames></author><author><keyname>Ward</keyname><forenames>John Paul</forenames></author><author><keyname>Sage</keyname><forenames>Daniel</forenames></author><author><keyname>Unser</keyname><forenames>Michael</forenames></author></authors><title>On The Continuous Steering of the Scale of Tight Wavelet Frames</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In analogy with steerable wavelets, we present a general construction of
adaptable tight wavelet frames, with an emphasis on scaling operations. In
particular, the derived wavelets can be &quot;dilated&quot; by a procedure comparable to
the operation of steering steerable wavelets. The fundamental aspects of the
construction are the same: an admissible collection of Fourier multipliers is
used to extend a tight wavelet frame, and the &quot;scale&quot; of the wavelets is
adapted by scaling the multipliers. As an application, the proposed wavelets
can be used to improve the frequency localization. Importantly, the localized
frequency bands specified by this construction can be scaled efficiently using
matrix multiplication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02073</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02073</id><created>2015-12-07</created><authors><author><keyname>Lei</keyname><forenames>Yanjun</forenames></author><author><keyname>Jiang</keyname><forenames>Xin</forenames></author><author><keyname>Guo</keyname><forenames>Quantong</forenames></author><author><keyname>Ma</keyname><forenames>Yifang</forenames></author><author><keyname>Li</keyname><forenames>Meng</forenames></author><author><keyname>Zheng</keyname><forenames>Zhiming</forenames></author></authors><title>Contagion processes on the static and activity driven coupling networks</title><categories>physics.soc-ph cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The evolution of network structure and the spreading of epidemic are common
coexistent dynamical processes. In most cases, network structure is treated
either static or time-varying, supposing the whole network is observed in a
same time window. In this paper, we consider the epidemic spreading on a
network consisting of both static and time-varying structures. At meanwhile,
the time-varying part and the epidemic spreading are supposed to be of the same
time scale. We introduce a static and activity driven coupling (SADC) network
model to characterize the coupling between static (strong) structure and
dynamic (weak) structure. Epidemic thresholds of SIS and SIR model are studied
on SADC both analytically and numerically with various coupling strategies,
where the strong structure is of homogeneous or heterogeneous degree
distribution. Theoretical thresholds obtained from SADC model can both recover
and generalize the classical results in static and time-varying networks. It is
demonstrated that weak structures can make the epidemics break out much more
easily in homogeneous coupling but harder in heterogeneous coupling when
keeping same average degree in SADC networks. Furthermore, we show there exists
a threshold ratio of the weak structure to have substantive effects on the
breakout of the epidemics. This promotes our understanding of why epidemics can
still break out in some social networks even we restrict the flow of the
population.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02075</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02075</id><created>2015-12-07</created><authors><author><keyname>Wu</keyname><forenames>Jingbo</forenames></author><author><keyname>Ugrinovskii</keyname><forenames>Valery</forenames></author><author><keyname>Allg&#xf6;wer</keyname><forenames>Frank</forenames></author></authors><title>Cooperative Estimation for Synchronization of Heterogeneous Multi-Agent
  Systems Using Relative Information</title><categories>cs.SY</categories><comments>The short version of this paper was published in Proc. IFAC World
  Congress, pp. 4662-4667, Cape Town, South Africa, 2014</comments><journal-ref>Proc. IFAC World Congress, pp. 4662-4667, Cape Town, South Africa,
  2014</journal-ref><doi>10.3182/20140824-6-ZA-1003.01938</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a distributed estimation setup where local agents
estimate their states from relative measurements received from their
neighbours. In the case of heterogeneous multi-agent systems, where only
relative measurements are available, this is of high relevance. The objective
is to improve the scalability of the existing distributed estimation algorithms
by restricting the agents to estimating only their local states and those of
immediate neighbours. The presented estimation algorithm also guarantees robust
performance against model and measurement disturbances. It is shown that it can
be integrated into output synchronization algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02077</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02077</id><created>2015-12-07</created><updated>2016-01-04</updated><authors><author><keyname>Shemesh</keyname><forenames>Omri Har</forenames></author><author><keyname>Quax</keyname><forenames>Rick</forenames></author><author><keyname>Hoekstra</keyname><forenames>Alfons G.</forenames></author><author><keyname>Sloot</keyname><forenames>Peter M. A.</forenames></author></authors><title>Information geometric analysis of phase transitions in complex patterns:
  the case of the Gray-Scott reaction-diffusion model</title><categories>cond-mat.stat-mech cs.IT math.IT nlin.PS</categories><comments>13 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Fisher-Rao metric from Information Geometry is related to phase
transition phenomena in classical statistical mechanics. Several studies
propose to extend the use of Information Geometry to study more general phase
transitions in complex systems. However, it is unclear whether the Fisher-Rao
metric does indeed detect these more general transitions, especially in the
absence of a statistical model. In this paper we study the transitions between
patterns in the Gray-Scott reaction-diffusion model using Fisher information.
We describe the system by a probability density function that represents the
size distribution of blobs in the patterns and compute its Fisher information
with respect to changing the two rate parameters of the underlying model. We
estimate the distribution non-parametrically so that we do not assume any
statistical model. The resulting Fisher map can be interpreted as a phase-map
of the different patterns. Lines with high Fisher information can be considered
as boundaries between regions of parameter space where patterns with similar
characteristics appear. These lines of high Fisher information can be
interpreted as phase transitions between complex patterns.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02078</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02078</id><created>2015-12-07</created><authors><author><keyname>Li</keyname><forenames>Kai</forenames></author><author><keyname>Wang</keyname><forenames>Yanjing</forenames></author></authors><title>From rules to runs: A dynamic epistemic take on imperfect information
  games</title><categories>cs.AI cs.GT cs.LO</categories><comments>draft of a paper accepted by Studies in Logic (published by Sun
  Yat-Sen University)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the literature of game theory, the information sets of extensive form
games have different interpretations, which may lead to confusions and
paradoxical cases. We argue that the problem lies in the mix-up of two
interpretations of the extensive form game structures: game rules or game runs
which do not always coincide. In this paper, we try to separate and connect
these two views by proposing a dynamic epistemic framework in which we can
compute the runs step by step from the game rules plus the given assumptions of
the players. We propose a modal logic to describe players' knowledge and its
change during the plays, and provide a complete axiomatization. We also show
that, under certain conditions, the mix-up of the rules and the runs is not
harmful due to the structural similarity of the two.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02086</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02086</id><created>2015-12-07</created><updated>2015-12-08</updated><authors><author><keyname>Diaz</keyname><forenames>Giovanna</forenames></author><author><keyname>O'Rourke</keyname><forenames>Joseph</forenames></author></authors><title>Hypercube Unfoldings that Tile R^3 and R^2</title><categories>cs.CG</categories><comments>20 pages, 18 figures, 10 refs. Version 2: Corrected a typo, added a
  reference</comments><msc-class>51F, 52C20</msc-class><acm-class>F.2.2; G.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the hypercube has a face-unfolding that tiles space, and that
unfolding has an edge-unfolding that tiles the plane. So the hypercube is a
&quot;dimension-descending tiler.&quot; We also show that the hypercube cross unfolding
made famous by Dali tiles space, but we leave open the question of whether or
not it has an edge-unfolding that tiles the plane.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02090</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02090</id><created>2015-12-07</created><authors><author><keyname>Natarajan</keyname><forenames>Anand</forenames></author><author><keyname>Vidick</keyname><forenames>Thomas</forenames></author></authors><title>Constant-Soundness Interactive Proofs for Local Hamiltonians</title><categories>quant-ph cs.CC</categories><comments>33 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  $ \newcommand{\Xlin}{\mathcal{X}} \newcommand{\Zlin}{\mathcal{Z}}
\newcommand{\C}{\mathbb{C}} $We give a quantum multiprover interactive proof
system for the local Hamiltonian problem in which there is a constant number of
provers, questions are classical of length polynomial in the number of qubits,
and answers are of constant length. The main novelty of our protocol is that
the gap between completeness and soundness is directly proportional to the
promise gap on the (normalized) ground state energy of the Hamiltonian. This
result can be interpreted as a concrete step towards a quantum PCP theorem
giving entangled-prover interactive proof systems for QMA-complete problems.
  The key ingredient is a quantum version of the classical linearity test of
Blum, Luby, and Rubinfeld, where the function $f:\{0,1\}^n\to\{0,1\}$ is
replaced by a pair of functions $\Xlin, \Zlin:\{0,1\}^n\to \text{Obs}_d(\C)$,
the set of $d$-dimensional Hermitian matrices that square to identity. The test
enforces that (i) each function is exactly linear,
$\Xlin(a)\Xlin(b)=\Xlin(a+b)$ and $\Zlin(a) \Zlin(b)=\Zlin(a+b)$, and (ii) the
two functions are approximately complementary, $\Xlin(a)\Zlin(b)\approx
(-1)^{a\cdot b} \Zlin(b)\Xlin(a)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02097</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02097</id><created>2015-12-07</created><authors><author><keyname>Qiu</keyname><forenames>Teng</forenames></author><author><keyname>Li</keyname><forenames>Yongjie</forenames></author></authors><title>Clustering by Deep Nearest Neighbor Descent (D-NND): A Density-based
  Parameter-Insensitive Clustering Method</title><categories>stat.ML cs.CV cs.LG stat.CO stat.ME</categories><comments>28 pages, 14 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Most density-based clustering methods largely rely on how well the underlying
density is estimated. However, density estimation itself is also a challenging
problem, especially the determination of the kernel bandwidth. A large
bandwidth could lead to the over-smoothed density estimation in which the
number of density peaks could be less than the true clusters, while a small
bandwidth could lead to the under-smoothed density estimation in which spurious
density peaks, or called the &quot;ripple noise&quot;, would be generated in the
estimated density. In this paper, we propose a density-based hierarchical
clustering method, called the Deep Nearest Neighbor Descent (D-NND), which
could learn the underlying density structure layer by layer and capture the
cluster structure at the same time. The over-smoothed density estimation could
be largely avoided and the negative effect of the under-estimated cases could
be also largely reduced. Overall, D-NND presents not only the strong capability
of discovering the underlying cluster structure but also the remarkable
reliability due to its insensitivity to parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02100</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02100</id><created>2015-12-07</created><authors><author><keyname>Taylor</keyname><forenames>Tim</forenames></author><author><keyname>Dorin</keyname><forenames>Alan</forenames></author><author><keyname>Korb</keyname><forenames>Kevin</forenames></author></authors><title>Digital Genesis: Computers, Evolution and Artificial Life</title><categories>cs.NE</categories><comments>Extended abstract of talk presented at the 7th Munich-Sydney-Tilburg
  Philosophy of Science Conference: Evolutionary Thinking, University of
  Sydney, 20-22 March 2014. Presentation slides from talk available at
  http://www.tim-taylor.com/papers/digital-genesis-presentation.pdf</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The application of evolution in the digital realm, with the goal of creating
artificial intelligence and artificial life, has a history as long as that of
the digital computer itself. We illustrate the intertwined history of these
ideas, starting with the early theoretical work of John von Neumann and the
pioneering experimental work of Nils Aall Barricelli. We argue that
evolutionary thinking and artificial life will continue to play an integral
role in the future development of the digital world.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02102</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02102</id><created>2015-12-07</created><authors><author><keyname>Goodspeed</keyname><forenames>Ben</forenames></author></authors><title>Proof Driven Development</title><categories>cs.SE</categories><comments>23 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new workflow for software development (proof-driven development) is
presented. An extension of test-driven development, the new workflow utilizes
the paradigm of dependently typed programming. The differences in design,
complexity and provability of software are discussed, based on the technique
used to create the system. Furthermore, the difference in what properties can
be expressed in a proof-driven development workflow versus a traditional
test-driven development workflow or using test-last development.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02109</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02109</id><created>2015-11-26</created><updated>2016-01-28</updated><authors><author><keyname>Daskin</keyname><forenames>Anmer</forenames></author></authors><title>Obtaining A Linear Combination of the Principal Components of a Matrix
  on Quantum Computers</title><categories>quant-ph cs.LG math.ST stat.TH</categories><comments>The title of the paper is changed. A couple of sections are extended.
  8 pages and 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Principal component analysis is a multivariate statistical method frequently
used in science and engineering to reduce the dimension of a problem or extract
the most significant features from a dataset. In this paper, using a similar
notion to the quantum counting, we show how to apply the amplitude
amplification together with the phase estimation algorithm to an operator in
order to procure the eigenvectors of the operator associated to the eigenvalues
defined in the range $\left[a, b\right]$, where $a$ and $b$ are real and $0
\leq a \leq b \leq 1$. This makes possible to obtain a combination of the
eigenvectors associated to the largest eigenvalues and so can be used to do
principal component analysis on quantum computers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02110</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02110</id><created>2015-12-07</created><authors><author><keyname>Holodovsky</keyname><forenames>Vadim</forenames></author><author><keyname>Schechner</keyname><forenames>Yoav Y.</forenames></author><author><keyname>Levin</keyname><forenames>Anat</forenames></author><author><keyname>Levis</keyname><forenames>Aviad</forenames></author><author><keyname>Aides</keyname><forenames>Amit</forenames></author></authors><title>In-situ multi-scattering tomography</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To recover the three dimensional (3D) volumetric distribution of matter in an
object, images of the object are captured from multiple directions and
locations. Using these images tomographic computations extract the
distribution. In highly scattering media and constrained, natural irradiance,
tomography must explicitly account for off-axis scattering. Furthermore, the
tomographic model and recovery must function when imaging is done in-situ, as
occurs in medical imaging and ground-based atmospheric sensing. We formulate
tomography that handles arbitrary orders of scattering, using a monte-carlo
model. Moreover, the model is highly parallelizable in our formulation. This
enables large scale rendering and recovery of volumetric scenes having a large
number of variables. We solve stability and conditioning problems that stem
from radiative transfer (RT) modeling in-situ.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02114</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02114</id><created>2015-12-07</created><authors><author><keyname>Hoeller</keyname><forenames>Arliones</forenames><suffix>Jr.</suffix></author><author><keyname>Fr&#xf6;hlich</keyname><forenames>Ant&#xf4;nio Augusto</forenames></author></authors><title>Evaluation of Energy-Efficient Heuristics for ACO-based Routing in
  Mobile Wireless Sensor Networks</title><categories>cs.NI</categories><comments>in International Journal of advanced studies in Computer Science and
  Engineering, IJASCSE, Volume 4, Issue 6, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile ad-hoc networks demand routing algorithms able to adapt to network
topologies subject to constant change. Moreover, with the advent of the
Internet-of-Things (IoT), network nodes tend not only to show increased
mobility, but also impose further restrictions to these mobile network systems.
These restrictions include non-functional requirements of low-power
consumption, small-size, and low-cost. ADHOP (Ant-based Dynamic Hop
Optimization Protocol) is a routing algorithm based on ant-colony optimizations
that target such small-size and low-cost platforms, consuming little amounts of
memory and processing power. This paper elaborates on ADHOP to investigate the
use of energy-related heuristics to guide routing decisions. The goal is to
minimize network usage without jeopardizing network operation. We replace the
original ADHOP heuristic of network latency by two energy-related heuristics
based on the battery charge and the estimated node lifetime. Simulations
compare the energy-aware versions of ADHOP to its original version, to AODV and
to AOER. Results show that the proposed approaches are able to balance network
load among nodes, resulting in a lower number of failures due to battery
depletion. The energy-aware versions of ADHOP also deliver more packets than
their counter-parts in the simulated scenario, delivering 2x more packets than
the original ADHOP, and, respectively, 5x and 9x more packets than AOER and
AODV.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02125</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02125</id><created>2015-12-07</created><authors><author><keyname>and&#xe9;n</keyname><forenames>Joakim</forenames></author><author><keyname>Lostanlen</keyname><forenames>Vincent</forenames></author><author><keyname>Mallat</keyname><forenames>St&#xe9;phane</forenames></author></authors><title>Joint Time-Frequency Scattering for Audio Classification</title><categories>cs.SD</categories><comments>6 pages, 2 figures in IEEE 25th International Workshop on Machine
  Learning for Signal Processing (MLSP), 2015. Sept. 17-20. Boston, USA</comments><doi>10.1109/MLSP.2015.7324385</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the joint time-frequency scattering transform, a time shift
invariant descriptor of time-frequency structure for audio classification. It
is obtained by applying a two-dimensional wavelet transform in time and
log-frequency to a time-frequency wavelet scalogram. We show that this
descriptor successfully characterizes complex time-frequency phenomena such as
time-varying filters and frequency modulated excitations. State-of-the-art
results are achieved for signal reconstruction and phone segment classification
on the TIMIT dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02134</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02134</id><created>2015-12-07</created><authors><author><keyname>Mayer</keyname><forenames>Nikolaus</forenames></author><author><keyname>Ilg</keyname><forenames>Eddy</forenames></author><author><keyname>H&#xe4;usser</keyname><forenames>Philip</forenames></author><author><keyname>Fischer</keyname><forenames>Philipp</forenames></author><author><keyname>Cremers</keyname><forenames>Daniel</forenames></author><author><keyname>Dosovitskiy</keyname><forenames>Alexey</forenames></author><author><keyname>Brox</keyname><forenames>Thomas</forenames></author></authors><title>A Large Dataset to Train Convolutional Networks for Disparity, Optical
  Flow, and Scene Flow Estimation</title><categories>cs.CV cs.LG stat.ML</categories><comments>Includes supplementary material</comments><acm-class>I.2.6; I.2.10; I.4.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent work has shown that optical flow estimation can be formulated as a
supervised learning task and can be successfully solved with convolutional
networks. Training of the so-called FlowNet was enabled by a large
synthetically generated dataset. The present paper extends the concept of
optical flow estimation via convolutional networks to disparity and scene flow
estimation. To this end, we propose three synthetic stereo video datasets with
sufficient realism, variation, and size to successfully train large networks.
Our datasets are the first large-scale datasets to enable training and
evaluating scene flow methods. Besides the datasets, we present a convolutional
network for real-time disparity estimation that provides state-of-the-art
results. By combining a flow and disparity estimation network and training it
jointly, we demonstrate the first scene flow estimation with a convolutional
network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02138</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02138</id><created>2015-12-07</created><updated>2015-12-15</updated><authors><author><keyname>Asudeh</keyname><forenames>Abolfazl</forenames></author><author><keyname>Thirumuruganathan</keyname><forenames>Saravanan</forenames></author><author><keyname>Zhang</keyname><forenames>Nan</forenames></author><author><keyname>Das</keyname><forenames>Gautam</forenames></author></authors><title>Discovering the Skyline of Web Databases</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many web databases are &quot;hidden&quot; behind proprietary search interfaces that
enforce the top-$k$ output constraint, i.e., each query returns at most $k$ of
all matching tuples, preferentially selected and returned according to a
proprietary ranking function. In this paper, we initiate research into the
novel problem of skyline discovery over top-$k$ hidden web databases. Since
skyline tuples provide critical insights into the database and include the
top-ranked tuple for every possible ranking function following the monotonic
order of attribute values, skyline discovery from a hidden web database can
enable a wide variety of innovative third-party applications over one or
multiple web databases. Our research in the paper shows that the critical
factor affecting the cost of skyline discovery is the type of search interface
controls provided by the website. As such, we develop efficient algorithms for
three most popular types, i.e., one-ended range, free range and point
predicates, and then combine them to support web databases that feature a
mixture of these types. Rigorous theoretical analysis and extensive real-world
online and offline experiments demonstrate the effectiveness of our proposed
techniques and their superiority over baseline solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02140</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02140</id><created>2015-11-20</created><authors><author><keyname>Mani</keyname><forenames>A</forenames></author></authors><title>Contamination-Free Measures and Algebraic Operations</title><categories>cs.AI</categories><comments>Preprint of FUZZIEEE'2013 Conference Paper</comments><msc-class>03E99, 06F30, 03C90</msc-class><doi>10.1109/FUZZ-IEEE.2013.6622521</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An open concept of rough evolution and an axiomatic approach to granules was
also developed recently by the present author. Subsequently the concepts were
used in the formal framework of rough Y-systems (RYS) for developing on
granular correspondences by her. These have since been used for a new approach
towards comparison of rough algebraic semantics across different semantic
domains by way of correspondences that preserve rough evolution and try to
avoid contamination. In this research paper, new methods are proposed and a
semantics for handling possibly contaminated operations and structured bigness
is developed. These would also be of natural interest for relative consistency
of one collection of knowledge relative other.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02157</identifier>
 <datestamp>2016-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02157</id><created>2015-12-07</created><updated>2016-02-22</updated><authors><author><keyname>Agarwal</keyname><forenames>Udit</forenames></author><author><keyname>Ramachandran</keyname><forenames>Vijaya</forenames></author></authors><title>Finding $k$ Simple Shortest Paths and Cycles</title><categories>cs.DS cs.DM</categories><comments>The current version includes new results for undirected graphs. In
  Section 4, the notion of an (m,n) reduction is generalized to an f(m,n)
  reduction</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of finding multiple simple shortest paths in a weighted directed
graph $G=(V,E)$ has many applications, and is considerably more difficult than
the corresponding problem when cycles are allowed in the paths. Even for a
single source-sink pair, it is known that two simple shortest paths cannot be
found in time polynomially smaller than $n^3$ (where $n=|V|$) unless the
All-Pairs Shortest Paths problem can be solved in a similar time bound. The
latter is a well-known open problem in algorithm design. We consider the
all-pairs version of the problem, and we give a new algorithm to find $k$
simple shortest paths for all pairs of vertices. For $k=2$, our algorithm runs
in $O(mn + n^2 \log n)$ time (where $m=|E|$), which is almost the same bound as
for the single pair case, and for $k=3$ we improve earlier bounds. Our approach
is based on forming suitable path extensions to find simple shortest paths;
this method is different from the `detour finding' technique used in most of
the prior work on simple shortest paths, replacement paths, and distance
sensitivity oracles.
  Enumerating simple cycles is a well-studied classical problem. We present new
algorithms for generating simple cycles and simple paths in $G$ in
non-decreasing order of their weights; the algorithm for generating simple
paths is much faster, and uses another variant of path extensions. We also give
hardness results for sparse graphs, relative to the complexity of computing a
minimum weight cycle in a graph, for several variants of problems related to
finding $k$ simple paths and cycles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02159</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02159</id><created>2015-12-07</created><updated>2015-12-08</updated><authors><author><keyname>Pomorski</keyname><forenames>Mateusz</forenames></author><author><keyname>Krawczyk</keyname><forenames>Malgorzata J.</forenames></author><author><keyname>Kulakowski</keyname><forenames>Krzysztof</forenames></author><author><keyname>Kwapien</keyname><forenames>Jaroslaw</forenames></author><author><keyname>Ausloos</keyname><forenames>Marcel</forenames></author></authors><title>Inferring cultural regions from correlation networks of given baby names</title><categories>physics.soc-ph cs.SI</categories><journal-ref>Physica A 445 (2016) 169-175</journal-ref><doi>10.1016/j.physa.2015.11.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We report investigations on the statistical characteristics of the baby names
given between 1910 and 2010 in the United States of America. For each year, the
100 most frequent names in the USA are sorted out. For these names, the
correlations between the names profiles are calculated for all pairs of states
(minus Hawaii and Alaska). The correlations are used to form a weighted network
which is found to vary mildly in time. In fact, the structure of communities in
the network remains quite stable till about 1980. The goal is that the
calculated structure approximately reproduces the usually accepted geopolitical
regions: the North East, the South, and the &quot;Midwest + West&quot; as the third one.
Furthermore, the dataset reveals that the name distribution satisfies the Zipf
law, separately for each state and each year, i.e. the name frequency $f\propto
r^{-\alpha}$, where r is the name rank. Between 1920 and 1980, the exponent
alpha is the largest one for the set of states classified as 'the South', but
the smallest one for the set of states classified as &quot;Midwest + West&quot;. Our
interpretation is that the pool of selected names was quite narrow in the
Southern states. The data is compared with some related statistics of names in
Belgium, a country also with different regions, but having quite a different
scale than the USA. There, the Zipf exponent is low for young people and for
the Brussels citizens.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02160</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02160</id><created>2015-12-07</created><authors><author><keyname>Borowski</keyname><forenames>Holly P.</forenames></author><author><keyname>Marden</keyname><forenames>Jason R.</forenames></author><author><keyname>Shamma</keyname><forenames>Jeff S.</forenames></author></authors><title>Learning Efficient Correlated Equilibria</title><categories>cs.GT</categories><comments>11 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The majority of distributed learning literature focuses on convergence to
Nash equilibria. Correlated equilibria, on the other hand, can often
characterize more efficient collective behavior than even the best Nash
equilibrium. However, there are no existing distributed learning algorithms
that converge to specific correlated equilibria. In this paper, we provide one
such algorithm which guarantees that the agents' collective joint strategy will
constitute an efficient correlated equilibrium with high probability. The key
to attaining efficient correlated behavior through distributed learning
involves incorporating a common random signal into the learning environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02167</identifier>
 <datestamp>2015-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02167</id><created>2015-12-07</created><updated>2015-12-15</updated><authors><author><keyname>Zhou</keyname><forenames>Bolei</forenames></author><author><keyname>Tian</keyname><forenames>Yuandong</forenames></author><author><keyname>Sukhbaatar</keyname><forenames>Sainbayar</forenames></author><author><keyname>Szlam</keyname><forenames>Arthur</forenames></author><author><keyname>Fergus</keyname><forenames>Rob</forenames></author></authors><title>Simple Baseline for Visual Question Answering</title><categories>cs.CV cs.CL</categories><comments>One comparison method's scores are put into the correct column, and a
  new experiment of generating attention map is added</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a very simple bag-of-words baseline for visual question
answering. This baseline concatenates the word features from the question and
CNN features from the image to predict the answer. When evaluated on the
challenging VQA dataset [2], it shows comparable performance to many recent
approaches using recurrent neural networks. To explore the strength and
weakness of the trained model, we also provide an interactive web demo and
open-source code. .
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02171</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02171</id><created>2015-12-07</created><authors><author><keyname>Banderier</keyname><forenames>Cyril</forenames></author><author><keyname>Baril</keyname><forenames>Jean-Luc</forenames></author><author><keyname>Santos</keyname><forenames>C&#xe9;line Moreira Dos</forenames></author></authors><title>Right-jumps and pattern avoiding permutations</title><categories>cs.DM math.CO math.PR</categories><comments>Corresponds to a work presented at the conferences Analysis of
  Algorithms (AofA'15) and Permutation Patterns'15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the iteration of the process &quot;a particle jumps to the right&quot; in
permutations. We prove that the set of permutations obtained in this model
after a given number of iterations from the identity is a class of pattern
avoiding permutations. We characterize the elements of the basis of this class
and we enumerate these &quot;forbidden minimal patterns&quot; by giving their bivariate
exponential generating function: we achieve this via a catalytic variable, the
number of left-to-right maxima. We show that this generating function is a
D-finite function satisfying a nice differential equation of order~2. We give
some congruence properties for the coefficients of this generating function,
and we show that their asymptotics involves a rather unusual algebraic exponent
(the golden ratio $(1+\sqrt 5)/2$) and some unusual closed-form constants. We
end by proving a limit law: a forbidden pattern of length $n$ has typically
$(\ln n) /\sqrt{5}$ left-to-right maxima, with Gaussian fluctuations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02179</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02179</id><created>2015-12-07</created><authors><author><keyname>Chrobak</keyname><forenames>Marek</forenames></author><author><keyname>Costello</keyname><forenames>Kevin P.</forenames></author></authors><title>Faster Information Gathering in Ad-Hoc Radio Tree Networks</title><categories>cs.DS cs.DM</categories><comments>Full version; extended abstract to appear in LATIN '16</comments><acm-class>G.2.2; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study information gathering in ad-hoc radio networks. Initially, each node
of the network has a piece of information called a rumor, and the overall
objective is to gather all these rumors in the designated target node. The
ad-hoc property refers to the fact that the topology of the network is unknown
when the computation starts. Aggregation of rumors is not allowed, which means
that each node may transmit at most one rumor in one step.
  We focus on networks with tree topologies, that is we assume that the network
is a tree with all edges directed towards the root, but, being ad-hoc, its
actual topology is not known. We provide two deterministic algorithms for this
problem. For the model that does not assume any collision detection nor
acknowledgement mechanisms, we give an $O(n\log\log n)$-time algorithm,
improving the previous upper bound of $O(n\log n)$. We also show that this
running time can be further reduced to $O(n)$ if the model allows for
acknowledgements of successful transmissions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02181</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02181</id><created>2015-12-07</created><authors><author><keyname>Liu</keyname><forenames>Ji</forenames></author><author><keyname>Zhu</keyname><forenames>Xiaojin</forenames></author></authors><title>The Teaching Dimension of Linear Learners</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Teaching dimension is a learning theoretic quantity that specifies the
minimum training set size to teach a target model to a learner. Previous
studies on teaching dimension focused on version-space learners which maintain
all hypotheses consistent with the training data, and cannot be applied to
modern machine learners which select a specific hypothesis via optimization.
This paper presents the first known teaching dimension for ridge regression,
support vector machines, and logistic regression. We also exhibit optimal
training sets that match these teaching dimensions. Our approach generalizes to
other linear learners.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02183</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02183</id><created>2015-12-07</created><authors><author><keyname>Zhong</keyname><forenames>Shiyin</forenames><affiliation>Senior Member, IEEE</affiliation></author><author><keyname>Broadwater</keyname><forenames>Robert</forenames><affiliation>Senior Member, IEEE</affiliation></author><author><keyname>Steffel</keyname><forenames>Steve</forenames></author></authors><title>Wavelet Based Load Models from AMI Data</title><categories>cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A major challenge of using AMI data in power system analysis is the large
size of the data sets. For rapid analysis that addresses historical behavior of
systems consisting of a few hundred feeders, all of the AMI load data can be
loaded into memory and used in a power flow analysis. However, if a system
contains thousands of feeders then the handling of the AMI data in the analysis
becomes more challenging. The work here seeks to demonstrate that the
information contained in large AMI data sets can be compressed into accurate
load models using wavelets. Two types of wavelet based load models are
considered, the multi-resolution wavelet load model for each individual
customer and the classified wavelet load model for customers that share similar
load patterns. The multi-resolution wavelet load model compresses the data, and
the classified wavelet load model further compresses the data. The method of
grouping customers into classes using the wavelet based classification
technique is illustrated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02188</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02188</id><created>2015-12-07</created><authors><author><keyname>Oh</keyname><forenames>Tae-Hyun</forenames></author><author><keyname>Wipf</keyname><forenames>David</forenames></author><author><keyname>Matsushita</keyname><forenames>Yasuyuki</forenames></author><author><keyname>Kweon</keyname><forenames>In So</forenames></author></authors><title>New Design Criteria for Robust PCA and a Compliant Bayesian-Inspired
  Algorithm</title><categories>cs.CV cs.LG stat.ML</categories><comments>Submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Commonly used in computer vision and other applications, robust PCA
represents an algorithmic attempt to reduce the sensitivity of classical PCA to
outliers. The basic idea is to learn a decomposition of some data matrix of
interest into low rank and sparse components, the latter representing unwanted
outliers. Although the resulting optimization problem is typically NP-hard,
convex relaxations provide a computationally-expedient alternative with
theoretical support. However, in practical regimes performance guarantees break
down and a variety of non-convex alternatives, including Bayesian-inspired
models, have been proposed to boost estimation quality. Unfortunately though,
without additional a priori knowledge none of these methods can significantly
expand the critical operational range such that exact principal subspace
recovery is possible. Into this mix we propose a novel pseudo-Bayesian
algorithm that explicitly compensates for design weaknesses in many existing
non-convex approaches leading to state-of-the-art performance with a sound
analytical foundation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02194</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02194</id><created>2015-12-07</created><authors><author><keyname>Groen</keyname><forenames>Derek</forenames></author><author><keyname>Bhati</keyname><forenames>Agastya</forenames></author><author><keyname>Suter</keyname><forenames>James</forenames></author><author><keyname>Hetherington</keyname><forenames>James</forenames></author><author><keyname>Zasada</keyname><forenames>Stefan</forenames></author><author><keyname>Coveney</keyname><forenames>Peter</forenames></author></authors><title>FabSim: facilitating computational research through automation on
  large-scale and distributed e-infrastructures</title><categories>cs.DC physics.comp-ph</categories><comments>29 pages, 8 figures, 2 tables, submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present FabSim, a toolkit developed to simplify a range of computational
tasks for researchers in diverse disciplines. FabSim is flexible, adaptable,
and allows users to perform a wide range of tasks with ease. It also provides a
systematic way to automate the use of resourcess, including HPC and distributed
resources, and to make tasks easier to repeat by recording contextual
information. To demonstrate this, we present three use cases where FabSim has
enhanced our research productivity. These include simulating cerebrovascular
bloodflow, modelling clay-polymer nanocomposites across multiple scales, and
calculating ligand-protein binding affinities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02196</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02196</id><created>2015-12-07</created><authors><author><keyname>Pandya</keyname><forenames>Marmik</forenames></author></authors><title>Securing Cloud - The Quantum Way</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Confidentiality, Integrity, and Availability are basic goals of security
architecture. To ensure CIA, many authentication scheme has been introduced in
several years. Currently deployment of Public Key Infrastructure (PKI) is a
most significant solution. PKI involving exchange key using certificates via a
public channel to a authenticate users in the cloud infrastructure. It is
exposed to widespread security threats such as eavesdropping, the man in the
middle attack, masquerade et al. Quantum cryptography is of the most prominent
fields in the modern world of information security. Quantum cryptography is
considered to be a future replica of classical cryptography along with a vital
stance to break existing classical cryptography. This paper aims to look into
basic security architecture in place currently and further it tries to
introduce a new proposed security architecture for cloud computing environment,
which makes use of the knowledge of Quantum Mechanics and current advances in
research in Quantum Computing, to provide a more secure architecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02207</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02207</id><created>2015-12-07</created><authors><author><keyname>Bougeret</keyname><forenames>Marin</forenames></author><author><keyname>Ochem</keyname><forenames>Pascal</forenames></author></authors><title>The complexity of partitioning into disjoint cliques and a triangle-free
  graph</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by Chudnovsky's structure theorem of bull-free graphs, Abu-Khzam,
Feghali, and M\&quot;uller have recently proved that deciding if a graph has a
vertex partition into disjoint cliques and a triangle-free graph is NP-complete
for five graph classes. The problem is trivial for the intersection of these
five classes. We prove that the problem is NP-complete for the intersection of
two subsets of size four among the five classes. We also show NP-completeness
for other small classes, such as graphs with maximum degree 4 and line graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02215</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02215</id><created>2015-12-05</created><authors><author><keyname>Lisitsa</keyname><forenames>Alexei</forenames><affiliation>The University of Liverpool</affiliation></author><author><keyname>Nemytykh</keyname><forenames>Andrei P.</forenames><affiliation>Program Systems Institute of Russian Academy of Sciences</affiliation></author><author><keyname>Pettorossi</keyname><forenames>Alberto</forenames><affiliation>University of Roma Tor Vergata</affiliation></author></authors><title>Proceedings of the Third International Workshop on Verification and
  Program Transformation</title><categories>cs.LO cs.PL cs.SE</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 199, 2015</journal-ref><doi>10.4204/EPTCS.199</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains the papers selected among those which were presented at
the 3rd International Workshop on Verification and Program Transformation (VPT
2015) held in London, UK, on April 11th, 2015. Previous editions of the
Workshop were held at Saint-Petersburg (Russia) in 2013, and Vienna (Austria)
in 2014.
  Those papers show that methods and tools developed in the field of program
transformation such as partial evaluation and fold/unfold transformations, and
supercompilation, can be applied in the verification of software systems. They
also show how some program verification methods, such as model checking
techniques, abstract interpretation, SAT and SMT solving, and automated theorem
proving, can be used to enhance program transformation techniques, thereby
making these techniques more powerful and useful in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02240</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02240</id><created>2015-12-07</created><authors><author><keyname>Portmann</keyname><forenames>Christopher</forenames></author><author><keyname>Matt</keyname><forenames>Christian</forenames></author><author><keyname>Maurer</keyname><forenames>Ueli</forenames></author><author><keyname>Renner</keyname><forenames>Renato</forenames></author><author><keyname>Tackmann</keyname><forenames>Bj&#xf6;rn</forenames></author></authors><title>Causal Boxes: Quantum Information-Processing Systems Closed under
  Composition</title><categories>quant-ph cs.CR</categories><comments>42+18 pages, 14 figures. Comments welcome</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Complex information-processing systems, for example quantum circuits,
cryptographic protocols, or multi-player games, are naturally described as
networks composed of more basic information-processing systems. A modular
analysis of such systems requires a mathematical model of systems that is
closed under composition, i.e., a network of these objects is again an object
of the same type. We propose such a model and call the corresponding systems
causal boxes.
  Causal boxes capture superpositions of causal structures, e.g., messages sent
by a causal box A can be in a superposition of different orders or in a
superposition of being sent to box B and box C. Furthermore, causal boxes can
model systems whose behavior depends on time. By instantiating the Abstract
Cryptography framework with causal boxes, we obtain the first composable
security framework that can handle arbitrary quantum protocols and relativistic
protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02251</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02251</id><created>2015-12-07</created><authors><author><keyname>Ye</keyname><forenames>Shanglin</forenames></author><author><keyname>Aboutanios</keyname><forenames>Elias</forenames></author></authors><title>Efficient and Accurate Frequency Estimation of Multiple Superimposed
  Exponentials in Noise</title><categories>math.NA cs.IT math.IT math.OC math.SP</categories><comments>10 pages, 10 figures, submitted to the IEEE Transactions on Signal
  Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The estimation of the frequencies of multiple superimposed exponentials in
noise is an important research problem due to its various applications from
engineering to chemistry. In this paper, we propose an efficient and accurate
algorithm that estimates the frequency of each component iteratively and
consecutively by combining an estimator with a leakage subtraction scheme.
During the iterative process, the proposed method gradually reduces estimation
error and improves the frequency estimation accuracy. We give theoretical
analysis where we derive the theoretical bias and variance of the frequency
estimates and discuss the convergence behaviour of the estimator. We show that
the algorithm converges to the asymptotic fixed point where the estimation is
asymptotically unbiased and the variance is just slightly above the Cramer-Rao
lower bound. We then verify the theoretical results and estimation performance
using extensive simulation. The simulation results show that the proposed
algorithm is capable of obtaining more accurate estimates than state-of-art
methods with only a few iterations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02254</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02254</id><created>2015-12-07</created><authors><author><keyname>Bansal</keyname><forenames>Nikhil</forenames></author><author><keyname>Nagarajan</keyname><forenames>Viswanath</forenames></author></authors><title>Approximation-Friendly Discrepancy Rounding</title><categories>cs.DS</categories><comments>19 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rounding linear programs using techniques from discrepancy is a recent
approach that has been very successful in certain settings. However this method
also has some limitations when compared to approaches such as randomized and
iterative rounding. We provide an extension of the discrepancy-based rounding
algorithm due to Lovett-Meka that (i) combines the advantages of both
randomized and iterated rounding, (ii) makes it applicable to settings with
more general combinatorial structure such as matroids. As applications of this
approach, we obtain new results for various classical problems such as linear
system rounding, degree-bounded matroid basis and low congestion routing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02257</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02257</id><created>2015-12-07</created><authors><author><keyname>De Carufel</keyname><forenames>Jean-Lou</forenames></author><author><keyname>Grimm</keyname><forenames>Carsten</forenames></author><author><keyname>Maheshwari</keyname><forenames>Anil</forenames></author><author><keyname>Smid</keyname><forenames>Michiel</forenames></author></authors><title>Minimizing the Continuous Diameter when Augmenting Paths and Cycles with
  Shortcuts</title><categories>cs.CG</categories><acm-class>F.2.2; G.2.2; I.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We seek to augment a geometric network in the Euclidean plane with shortcuts
to minimize its continuous diameter, i.e., the largest network distance between
any two points on the augmented network. Unlike in the discrete setting where a
shortcut connects two vertices and the diameter is measured between vertices,
we take all points along the edges of the network into account when placing a
shortcut and when measuring distances in the augmented network.
  We study this network augmentation problem for paths and cycles. For paths,
we determine an optimal shortcut in linear time. For cycles, we show that a
single shortcut never decreases the continuous diameter and that two shortcuts
always suffice to reduce the continuous diameter. Furthermore, we characterize
optimal pairs of shortcuts for convex and non-convex cycles. Finally, we
develop a linear time algorithm that produces an optimal pair of shortcuts for
convex cycles. Apart from the algorithms, our results extend to rectifiable
curves.
  Our work reveals some of the underlying challenges that must be overcome when
addressing the discrete version of this network augmentation problem, where we
minimize the discrete diameter of a network with shortcuts that connect only
vertices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02266</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02266</id><created>2015-12-07</created><authors><author><keyname>Leonelli</keyname><forenames>Manuele</forenames></author><author><keyname>G&#xf6;rgen</keyname><forenames>Christiane</forenames></author><author><keyname>Smith</keyname><forenames>Jim Q.</forenames></author></authors><title>Sensitivity analysis, multilinearity and beyond</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sensitivity methods for the analysis of the outputs of discrete Bayesian
networks have been extensively studied and implemented in different software
packages. These methods usually focus on the study of sensitivity functions and
on the impact of a parameter change to the Chan-Darwiche distance. Although not
fully recognized, the majority of these results heavily rely on the multilinear
structure of atomic probabilities in terms of the conditional probability
parameters associated with this type of network. By defining a statistical
model through the polynomial expression of its associated defining conditional
probabilities, we develop a unifying approach to sensitivity methods applicable
to a large suite of models including extensions of Bayesian networks, for
instance context-specific and dynamic ones, and chain event graphs. By then
focusing on models whose defining polynomial is multilinear, our algebraic
approach enables us to prove that the Chan-Darwiche distance is minimized for a
certain class of multi-parameter contemporaneous variations when parameters are
proportionally covaried.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02285</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02285</id><created>2015-12-07</created><authors><author><keyname>Cole</keyname><forenames>Richard</forenames></author><author><keyname>Rao</keyname><forenames>Shravas</forenames></author></authors><title>Applications of $\alpha$-strongly regular distributions to Bayesian
  auctions</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two classes of distributions that are widely used in the analysis of Bayesian
auctions are the Monotone Hazard Rate (MHR) and Regular distributions. They can
both be characterized in terms of the rate of change of the associated virtual
value functions: for MHR distributions the condition is that for values $v &lt;
v'$, $\phi(v') - \phi(v) \ge v' - v$, and for regular distributions, $\phi(v')
- \phi(v) \ge 0$. Cole and Roughgarden introduced the interpolating class of
$\alpha$-Strongly Regular distributions ($\alpha$-SR distributions for short),
for which $\phi(v') - \phi(v) \ge \alpha(v' - v)$, for $0 \le \alpha \le 1$.
  In this paper, we investigate five distinct auction settings for which good
expected revenue bounds are known when the bidders' valuations are given by MHR
distributions. In every case, we show that these bounds degrade gracefully when
extended to $\alpha$-SR distributions. For four of these settings, the auction
mechanism requires knowledge of these distribution(s) (in the other setting,
the distributions are needed only to ensure good bounds on the expected
revenue). In these cases we also investigate what happens when the
distributions are known only approximately via samples, specifically how to
modify the mechanisms so that they remain effective and how the expected
revenue depends on the number of samples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02300</identifier>
 <datestamp>2016-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02300</id><created>2015-12-07</created><updated>2016-01-14</updated><authors><author><keyname>Ma</keyname><forenames>Will</forenames></author><author><keyname>Simchi-Levi</keyname><forenames>David</forenames></author></authors><title>Reaping the Benefits of Bundling under High Production Costs</title><categories>cs.GT</categories><comments>35 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It has been known in the economics literature that selling different goods in
a single bundle can increase revenue, even when the valuations for the goods
are independent. However, bundling is no longer profitable if the goods have
high production costs. To overcome this issue, we introduce Pure Bundling with
Disposal for Cost (PBDC), where after buying the bundle, we allow the customer
to return any subset of items for their production cost. We demonstrate using
classical examples that PBDC captures the concentration effects of bundling
while allowing for the flexibility of individual sales, extracting all of the
consumer welfare in situations where previous simple mechanisms could not.
  Furthermore, we prove a theoretical guarantee on the performance of PBDC that
holds for independent distributions, using techniques from the mechanism design
literature. We transform the problem with costs to a problem with negative
valuations, extend the mechanism design techniques to negative valuations, and
use the Core-Tail decomposition of Babaioff et al. from [BILW14] to show that
either PBDC or individual sales will obtain at least $\frac{1}{5.2}$ of the
optimal profit. This also improves the bound of $\frac{1}{6}$ from [BILW14]. We
advance the upper bound as well, constructing two IID items with zero cost
where pure bundling and individual sales can only earn
$\frac{3+\ln2}{3+2\ln2}\approx\frac{1}{1.188}$ as much revenue as mixed
bundling.
  Our numerical experiments show that PBDC outperforms all other simple pricing
schemes, including the Bundle-Size Pricing (BSP) introduced by Chu et al. in
[CLS08], under the same families of distributions used in [CLS08]. We also
provide the first theoretical explanation for some of the great experimental
successes in [CLS08]. All in all, our work establishes PBDC as a robust,
computationally-minimal heuristic that is easy to market to consumers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02311</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02311</id><created>2015-12-07</created><authors><author><keyname>Narihira</keyname><forenames>Takuya</forenames></author><author><keyname>Maire</keyname><forenames>Michael</forenames></author><author><keyname>Yu</keyname><forenames>Stella X.</forenames></author></authors><title>Direct Intrinsics: Learning Albedo-Shading Decomposition by
  Convolutional Regression</title><categories>cs.CV</categories><comments>International Conference on Computer Vision (ICCV), 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new approach to intrinsic image decomposition, the task of
decomposing a single image into albedo and shading components. Our strategy,
which we term direct intrinsics, is to learn a convolutional neural network
(CNN) that directly predicts output albedo and shading channels from an input
RGB image patch. Direct intrinsics is a departure from classical techniques for
intrinsic image decomposition, which typically rely on physically-motivated
priors and graph-based inference algorithms.
  The large-scale synthetic ground-truth of the MPI Sintel dataset plays a key
role in training direct intrinsics. We demonstrate results on both the
synthetic images of Sintel and the real images of the classic MIT intrinsic
image dataset. On Sintel, direct intrinsics, using only RGB input, outperforms
all prior work, including methods that rely on RGB+Depth input. Direct
intrinsics also generalizes across modalities; it produces quite reasonable
decompositions on the real images of the MIT dataset. Our results indicate that
the marriage of CNNs with synthetic training data may be a powerful new
technique for tackling classic problems in computer vision.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02317</identifier>
 <datestamp>2015-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02317</id><created>2015-12-07</created><updated>2015-12-16</updated><authors><author><keyname>Dubey</keyname><forenames>Pradeep</forenames></author><author><keyname>Sahi</keyname><forenames>Siddhartha</forenames></author><author><keyname>Shubik</keyname><forenames>Martin</forenames></author></authors><title>Money as Minimal Complexity</title><categories>cs.GT math.CO</categories><comments>34 pages, v2, fixed typos/references</comments><msc-class>91B64</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider mechanisms that provide traders the opportunity to exchange
commodity $i$ for commodity $j$, for certain ordered pairs $ij$. Given any
connected graph $G$ of opportunities, we show that there is a unique mechanism
$M_{G}$ that satisfies some natural conditions of &quot;fairness&quot; and &quot;convenience&quot;.
Let $\mathfrak{M}(m)$ denote the class of mechanisms $M_{G}$ obtained by
varying $G$ on the commodity set $\left\{1,\ldots,m\right\} $. We define the
complexity of a mechanism $M$ in $\mathfrak{M(m)}$ to be a certain pair of
integers $\tau(M),\pi(M)$ which represent the time required to exchange $i$ for
$j$ and the information needed to determine the exchange ratio (each in the
worst case scenario, across all $i\neq j$). This induces a quasiorder $\preceq$
on $\mathfrak{M}(m)$ by the rule \[ M\preceq
M^{\prime}\text{if}\tau(M)\leq\tau(M^{\prime})\text{and}\pi(M)\leq\pi(M^{\prime}).
\] We show that, for $m&gt;3$, there are precisely three $\preceq$-minimal
mechanisms $M_{G}$ in $\mathfrak{M}(m)$, where $G$ corresponds to the star,
cycle and complete graphs. The star mechanism has a distinguished commodity --
the money -- that serves as the sole medium of exchange and mediates trade
between decentralized markets for the other commodities.
  Our main result is that, for any weights $\lambda,\mu&gt;0,$ the star mechanism
is the unique minimizer of $\lambda\tau(M)+\mu\pi(M)$ on $\mathfrak{M}(m)$ for
large enough $m$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02319</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02319</id><created>2015-12-07</created><authors><author><keyname>Li</keyname><forenames>Di</forenames></author><author><keyname>Kar</keyname><forenames>Soummya</forenames></author><author><keyname>Alsaadi</keyname><forenames>Fuad E.</forenames></author><author><keyname>Cui</keyname><forenames>Shuguang</forenames></author></authors><title>Distributed Bayesian Quickest Change Detection in Sensor Networks via
  Two-layer Large Deviation Analysis</title><categories>cs.IT math.IT</categories><comments>12 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a distributed Bayesian quickest change detection algorithm for
sensor networks, based on a random gossip inter-sensor communication structure.
Without a control or fusion center, each sensor executes its local change
detection procedure in a parallel and distributed fashion, interacting with its
neighbor sensors via random inter-sensor communications to propagate
information. By modeling the information propagation dynamics in the network as
a Markov process, two-layer large deviation analysis is presented to analyze
the performance of the proposed algorithm. The first-layer analysis shows that
the relation between the probability of false alarm and the conditional
averaged detection delay satisfies the large deviation principle, implying that
the probability of false alarm according to a rare event decays to zero at an
exponentially fast rate when the conditional averaged detection decay
increases, where the Kullback-Leibler information number is established as a
crucial factor. The second-layer analysis shows that the probability of the
rare event that not all observations are available at a sensor decays to zero
at an exponentially fast rate when the averaged number of communications
increases, where the large deviation upper and lower bounds for this rate are
also derived, based on which we show that the performance of the distributed
algorithm converges exponentially fast to that of the centralized one, by
proving that the defined distributed Kullback-Leibler information number
converges to the centralized Kullback-Leibler information number.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02325</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02325</id><created>2015-12-07</created><authors><author><keyname>Liu</keyname><forenames>Wei</forenames></author><author><keyname>Anguelov</keyname><forenames>Dragomir</forenames></author><author><keyname>Erhan</keyname><forenames>Dumitru</forenames></author><author><keyname>Szegedy</keyname><forenames>Christian</forenames></author><author><keyname>Reed</keyname><forenames>Scott</forenames></author></authors><title>SSD: Single Shot MultiBox Detector</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a method for detecting objects in images using a single deep
neural network. Our approach, named SSD, discretizes the output space of
bounding boxes into a set of bounding box priors over different aspect ratios
and scales per feature map location. At prediction time, the network generates
confidences that each prior corresponds to objects of interest and produces
adjustments to the prior to better match the object shape. Additionally, the
network combines predictions from multiple feature maps with different
resolutions to naturally handle objects of various sizes. Our SSD model is
simple relative to methods that requires object proposals, such as R-CNN and
MultiBox, because it completely discards the proposal generation step and
encapsulates all the computation in a single network. This makes SSD easy to
train and straightforward to integrate into systems that require a detection
component. Experimental results on ILSVRC DET and PASCAL VOC dataset confirm
that SSD has comparable performance with methods that utilize an additional
object proposal step and yet is 100-1000x faster. Compared to other single
stage methods, SSD has similar or better performance, while providing a unified
framework for both training and inference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02326</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02326</id><created>2015-12-07</created><authors><author><keyname>Shao</keyname><forenames>Jie</forenames></author><author><keyname>Wang</keyname><forenames>Dequan</forenames></author><author><keyname>Xue</keyname><forenames>Xiangyang</forenames></author><author><keyname>Zhang</keyname><forenames>Zheng</forenames></author></authors><title>Learning to Point and Count</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes the problem of point-and-count as a test case to break
the what-and-where deadlock. Different from the traditional detection problem,
the goal is to discover key salient points as a way to localize and count the
number of objects simultaneously. We propose two alternatives, one that counts
first and then point, and another that works the other way around.
Fundamentally, they pivot around whether we solve &quot;what&quot; or &quot;where&quot; first. We
evaluate their performance on dataset that contains multiple instances of the
same class, demonstrating the potentials and their synergies. The experiences
derive a few important insights that explains why this is a much harder problem
than classification, including strong data bias and the inability to deal with
object scales robustly in state-of-art convolutional neural networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02328</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02328</id><created>2015-12-08</created><authors><author><keyname>Ji</keyname><forenames>Bo</forenames></author><author><keyname>Gupta</keyname><forenames>Gagan R.</forenames></author><author><keyname>Sang</keyname><forenames>Yu</forenames></author></authors><title>Node-based Service-Balanced Scheduling for Provably Guaranteed
  Throughput and Evacuation Time Performance</title><categories>cs.NI</categories><comments>To appear in IEEE INFOCOM 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper focuses on the design of \emph{provably efficient online} link
scheduling algorithms for multi-hop wireless networks. We consider single-hop
flows and the one-hop interference model. The objective is twofold: 1)
\emph{maximize the throughput} when the flow sources continuously inject
packets into the network, and 2) \emph{minimize the evacuation time} when there
are no future packet arrivals. The prior works mostly employ the link-based
approach, which leads to throughput-efficient algorithms but often does not
guarantee satisfactory evacuation time performance. In this paper, we adopt a
novel \emph{node-based approach} and propose a \emph{service-balanced online}
scheduling algorithm, called NSB, which gives balanced scheduling opportunities
to the nodes with heavy workload. We rigorously prove that NSB guarantees to
achieve an efficiency ratio no worse (or no smaller) than $2/3$ for the
throughput and an approximation ratio no worse (or no greater) than $3/2$ for
the evacuation time. It is remarkable that NSB is both throughput-optimal and
evacuation-time-optimal if the underlying network graph is bipartite. Further,
we develop a lower-complexity NSB algorithm, called LC-NSB, which provides the
same performance guarantees as NSB. Finally, we conduct numerical experiments
to elucidate our theoretical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02329</identifier>
 <datestamp>2015-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02329</id><created>2015-12-08</created><updated>2015-12-15</updated><authors><author><keyname>Wang</keyname><forenames>Qifei</forenames></author></authors><title>Computational Models for Multiview Dense Depth Maps of Dynamic Scene</title><categories>cs.CV</categories><comments>4 pages, IEEE COMSOC MMTC E-Letter 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper reviews the recent progresses of the depth map generation for
dynamic scene and its corresponding computational models. This paper mainly
covers the homogeneous ambiguity models in depth sensing, resolution models in
depth processing, and consistency models in depth optimization. We also
summarize the future work in the depth map generation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02332</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02332</id><created>2015-12-08</created><authors><author><keyname>Raza</keyname><forenames>Zahid</forenames></author><author><keyname>Rana</keyname><forenames>Amrina</forenames></author></authors><title>$(1-2u^k)$-constacyclic codes over
  $\mathbb{F}_p+u\mathbb{F}_p+u^2\mathbb{F}_+u^{3}\mathbb{F}_{p}+\dots+u^{k}\mathbb{F}_{p}$</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $\mathbb{F}_p$ be a finite field and $u$ be an indeterminate. This
article studies $(1-2u^k)$-constacyclic codes over the ring
$\mathcal{R}=\mathbb{F}_p+u\mathbb{F}_p+u^2\mathbb{F}_p+u^{3}\mathbb{F}_{p}+\cdots+u^{k}\mathbb{F}_{p}$
where $u^{k+1}=u$. We illustrate the generator polynomials and investigate the
structural properties of these codes via decomposition theorem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02337</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02337</id><created>2015-12-08</created><updated>2016-02-03</updated><authors><author><keyname>Hopkins</keyname><forenames>Samuel B.</forenames></author><author><keyname>Schramm</keyname><forenames>Tselil</forenames></author><author><keyname>Shi</keyname><forenames>Jonathan</forenames></author><author><keyname>Steurer</keyname><forenames>David</forenames></author></authors><title>Fast spectral algorithms from sum-of-squares proofs: tensor
  decomposition and planted sparse vectors</title><categories>cs.DS cs.CC cs.LG stat.ML</categories><comments>62 pages, title changed, to appear at STOC 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider two problems that arise in machine learning applications: the
problem of recovering a planted sparse vector in a random linear subspace and
the problem of decomposing a random low-rank overcomplete 3-tensor. For both
problems, the best known guarantees are based on the sum-of-squares method. We
develop new algorithms inspired by analyses of the sum-of-squares method. Our
algorithms achieve the same or similar guarantees as sum-of-squares for these
problems but the running time is significantly faster.
  For the planted sparse vector problem, we give an algorithm with running time
nearly linear in the input size that approximately recovers a planted sparse
vector with up to constant relative sparsity in a random subspace of $\mathbb
R^n$ of dimension up to $\tilde \Omega(\sqrt n)$. These recovery guarantees
match the best known ones of Barak, Kelner, and Steurer (STOC 2014) up to
logarithmic factors.
  For tensor decomposition, we give an algorithm with running time close to
linear in the input size (with exponent $\approx 1.086$) that approximately
recovers a component of a random 3-tensor over $\mathbb R^n$ of rank up to
$\tilde \Omega(n^{4/3})$. The best previous algorithm for this problem due to
Ge and Ma (RANDOM 2015) works up to rank $\tilde \Omega(n^{3/2})$ but requires
quasipolynomial time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02341</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02341</id><created>2015-12-08</created><authors><author><keyname>Imamori</keyname><forenames>Daichi</forenames></author><author><keyname>Tajima</keyname><forenames>Keishi</forenames></author></authors><title>Predicting Popularity of Twitter Accounts through the Discovery of
  Link-Propagating Early Adopters</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a method of ranking recently created Twitter
accounts according to their prospective popularity. Early detection of new
promising accounts is useful for trend prediction, viral marketing, user
recommendation, and so on. New accounts are, however, difficult to evaluate
because they have not established their reputations, and we cannot apply
existing link-based or other popularity-based account evaluation methods. Our
method first finds &quot;early adopters&quot;, i.e., users who often find new good
information sources earlier than others. Our method then regards new accounts
followed by good early adopters as promising, even if they do not have many
followers now. In order to find good early adopters, we estimate the frequency
of link propagation from each account, i.e., how many times the follow links
from the account have been copied by its followers. If its followers have
copied many of its follow links in the past, the account must be an early
adopter, who find good information sources earlier than its followers. We
develop a method of inferring which links are created by copying which links.
One advantage of our method is that our method only uses information that can
be easily obtained only by crawling neighbors of the target accounts in the
current Twitter graph. We evaluated our method by an experiment on Twitter
data. We chose then-new accounts from an old snapshot of Twitter, compute their
ranking by our method, and compare it with the number of followers the accounts
currently have. The result shows that our method produces better rankings than
various baseline methods, especially for new accounts that have only a few
followers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02347</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02347</id><created>2015-12-08</created><authors><author><keyname>Bostanci</keyname><forenames>Erkan</forenames></author></authors><title>Medical Wearable Technologies: Applications, Problems and Solutions</title><categories>cs.CY</categories><comments>4 pages, conference, in Turkish</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The focus of this paper is on wearable technologies which are increasingly
being employed in the medical field. From smart watches to smart glasses, from
electronic textile to data gloves; several gadgets are playing important roles
in diagnosis and treatment of various medical conditions. The threats posed by
these technologies are another matter of concern that must be seriously taken
into account. Numerous threats ranging from data privacy to big data problems
are facing us as adverse effects of these technologies. The paper analyses the
application areas and challenges of wearable technologies from a technical and
ethical point of view and presents solutions to possible threats.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02355</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02355</id><created>2015-12-08</created><authors><author><keyname>Bostanci</keyname><forenames>Erkan</forenames></author></authors><title>Is Hamming distance the only way for matching binary image feature
  descriptors?</title><categories>cs.CV</categories><comments>2 pages, journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Brute force matching of binary image feature descriptors is conventionally
performed using the Hamming distance. This paper assesses the use of
alternative metrics in order to see whether they can produce feature
correspondences that yield more accurate homography matrices. Two statistical
tests, namely ANOVA (Analysis of Variance) and McNemar's test were employed for
evaluation. Results show that Jackard-Needham and Dice metrics can display
better performance for some descriptors. Yet, these performance differences
were not found to be statistically significant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02356</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02356</id><created>2015-12-08</created><authors><author><keyname>Sadhu</keyname><forenames>Sanjib</forenames></author><author><keyname>Roy</keyname><forenames>Sasanka</forenames></author><author><keyname>Nandi</keyname><forenames>Soumen</forenames></author><author><keyname>Maheswari</keyname><forenames>Anil</forenames></author><author><keyname>Nandy</keyname><forenames>Subhas C.</forenames></author></authors><title>Approximation algorithms for the two-center problem of convex polygon</title><categories>cs.CG</categories><comments>27 pages, 18 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a convex polygon $P$ with $n$ vertices, the two-center problem is to
find two congruent closed disks of minimum radius such that they completely
cover $P$. We propose an algorithm for this problem in the streaming setup,
where the input stream is the vertices of the polygon in clockwise order. It
produces a radius $r$ satisfying $r\leq2r_{opt}$ using $O(1)$ space, where
$r_{opt}$ is the optimum solution. Next, we show that in non-streaming setup,
we can improve the approximation factor by $r\leq 1.84 r_{opt}$, maintaining
the time complexity of the algorithm to $O(n)$, and using $O(1)$ extra space in
addition to the space required for storing the input.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02357</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02357</id><created>2015-12-08</created><authors><author><keyname>Aghamohamadian-Sharbaf</keyname><forenames>Masoud</forenames></author><author><keyname>Heravi</keyname><forenames>Ahmadreza</forenames></author><author><keyname>Pourreza</keyname><forenames>Hamidreza</forenames></author></authors><title>Towards the Application of Linear Programming Methods For Multi-Camera
  Pose Estimation</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We presented a separation based optimization algorithm which, rather than
optimization the entire variables altogether, This would allow us to employ: 1)
a class of nonlinear functions with three variables and 2) a convex quadratic
multivariable polynomial, for minimization of reprojection error. Neglecting
the inversion required to minimize the nonlinear functions, in this paper we
demonstrate how separation allows eradication of matrix inversion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02363</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02363</id><created>2015-12-08</created><authors><author><keyname>Forster</keyname><forenames>Christian</forenames></author><author><keyname>Carlone</keyname><forenames>Luca</forenames></author><author><keyname>Dellaert</keyname><forenames>Frank</forenames></author><author><keyname>Scaramuzza</keyname><forenames>Davide</forenames></author></authors><title>On-Manifold Preintegration Theory for Fast and Accurate Visual-Inertial
  Navigation</title><categories>cs.RO</categories><comments>18 pages, 15 figures, submitted to IEEE Transactions on Robotics
  (TRO) and currently under review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current approaches for visual-inertial navigation (VIN) are able to attain
highly accurate state estimation via nonlinear optimization. However, real-time
optimization quickly becomes infeasible as the trajectory grows over time; this
problem is further emphasized by the fact that inertial measurements come at
high rate, hence leading to fast growth of the number of variables in the
optimization. In this paper, we address this issue by preintegrating inertial
measurements between selected keyframes into single relative motion
constraints. Our first contribution is a preintegration theory that properly
addresses the manifold structure of the rotation group. We formally discuss the
generative measurement model as well as the nature of the rotation noise and
derive the expression for the maximum a posteriori state estimator. Our
theoretical development enables the computation of all necessary Jacobians for
the optimization and a-posteriori bias correction in analytic form. The second
contribution is to show that the preintegrated IMU model can be seamlessly
integrated into a visual-inertial pipeline under the unifying framework of
factor graphs. This enables the application of incremental-smoothing algorithms
and the use of a structureless model for visual measurements, which avoids
optimizing over the 3D points, further accelerating the computation. We perform
an extensive evaluation of our monocular VIN pipeline on real and simulated
datasets. The results confirm that our modelling effort leads to accurate state
estimation in real-time, outperforming state-of-the-art approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02372</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02372</id><created>2015-12-08</created><authors><author><keyname>Khalil</keyname><forenames>Nahla</forenames></author></authors><title>The 3D virtual environment online for real shopping</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The development of information technology and Internet has led to rapidly
progressed in e-commerce and online shopping, due to the convenience that they
provide consumers. E-commerce and online shopping are still not able to fully
replace onsite shopping. In contrast, conventional online shopping websites
often cannot provide enough information about a product for the customer to
make an informed decision before checkout. 3D virtual shopping environment show
great potential for enhancing e-commerce systems and provide customers
information about a product and real shopping environment. This paper presents
a new type of e-commerce system, which obviously brings virtual environment
online with an active 3D model that allows consumers to access products into
real physical environments for user interaction. Such system with easy process
can helps customers make better purchasing decisions that allows users to
manipulate 3D virtual models online. The stores participate in the 3D virtual
mall by communicating with a mall management. The 3D virtual mall allows
shoppers to perform actions across multiple stores simultaneously such as
viewing product availability. The mall management can authenticate clients on
all stores participating in the 3D virtual mall while only requiring clients to
provide authentication information once. 3D virtual shopping online mall
convenient and easy process allow consumers directly buy goods or services from
a seller in real-time, without an intermediary service, over the Internet. The
virtual mall with an active 3D model is implemented by using 3D Language (VRML)
and asp.net as the script language for shopping online pages
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02379</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02379</id><created>2015-12-08</created><updated>2016-02-18</updated><authors><author><keyname>Hlin&#x11b;n&#xfd;</keyname><forenames>Petr</forenames></author><author><keyname>Der&#x148;&#xe1;r</keyname><forenames>Marek</forenames></author></authors><title>Crossing Number is Hard for Kernelization</title><categories>cs.CC math.CO</categories><msc-class>05C10</msc-class><acm-class>F.2.2; F.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The graph crossing number problem, cr(G)&lt;=k, asks for a drawing of a graph G
in the plane with at most k edge crossings. Although this problem is in general
notoriously difficult, it is fixed- parameter tractable for the parameter k
[Grohe]. This suggests a closely related question of whether this problem has a
polynomial kernel, meaning whether every instance of cr(G)&lt;=k can be in
polynomial time reduced to an equivalent instance of size polynomial in k (and
independent of |G|). We answer this question in the negative. Along the proof
we show that the tile crossing number problem of twisted planar tiles is
NP-hard, which has been an open problem for some time, too, and then employ the
complexity technique of cross-composition. Our result holds already for the
special case of graphs obtained from planar graphs by adding one edge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02381</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02381</id><created>2015-12-08</created><authors><author><keyname>Esperet</keyname><forenames>Louis</forenames></author></authors><title>Box representations of embedded graphs</title><categories>math.CO cs.CG cs.DM</categories><comments>15 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A $d$-box is the cartesian product of $d$ intervals of $\mathbb{R}$ and a
$d$-box representation of a graph $G$ is a representation of $G$ as the
intersection graph of a set of $d$-boxes in $\mathbb{R}^d$. It was proved by
Thomassen in 1986 that every planar graph has a 3-box representation. In this
paper we prove that every graph embedded in a fixed orientable surface, without
short non-contractible cycles, has a 5-box representation. This directly
implies that there is a function $f$, such that in every graph of genus $g$, a
set of at most $f(g)$ vertices can be removed so that the resulting graph has a
5-box representation. We show that such a function $f$ can be made linear in
$g$. Finally, we prove that for any proper minor-closed class $\mathcal{F}$,
there is a constant $c(\mathcal{F})$ such that every graph of $\mathcal{F}$
without cycles of length less than $c(\mathcal{F})$ has a 3-box representation,
which is best possible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02385</identifier>
 <datestamp>2016-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02385</id><created>2015-12-08</created><updated>2016-02-10</updated><authors><author><keyname>Ugur</keyname><forenames>Yigit</forenames></author><author><keyname>Awan</keyname><forenames>Zohaib Hassan</forenames></author><author><keyname>Sezgin</keyname><forenames>Aydin</forenames></author></authors><title>Cloud Radio Access Networks with Coded Caching</title><categories>cs.IT math.IT</categories><comments>5 pages, 3 figures, WSA 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A cloud radio access network (C-RAN) is considered as a candidate to meet the
expectations of higher data rate de- mands in wireless networks. In C-RAN, low
energy base stations (BSs) are deployed over a small geography and are allowed
to connect to the cloud via finite capacity backhaul links where the
information is processed. A conventional C-RAN, however, requires high capacity
backhaul links, since the requested files need to be transferred first from the
cloud to the BS before conveying them to the users. One approach to overcome
the limitations of the backhaul links is to introduce local storage caches at
the BSs, in which the popular files are stored locally in order to reduce the
load of the backhaul links. Furthermore, we utilize coded caching with the goal
to minimize the total network cost, i.e., the transmit power and the cost
associated with the backhaul links. The initial formulation of the optimization
problem for this model is non-convex. We first reformulate and then convexify
the problem through some relaxation techniques. In comparison to the uncoded
caching at the BSs, our results highlight the benefits associated with coded
caching and show that it decreases the backhaul cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02390</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02390</id><created>2015-12-08</created><authors><author><keyname>Stiller</keyname><forenames>Joerg</forenames></author></authors><title>Nonuniformly weighted Schwarz smoothers for spectral element multigrid</title><categories>cs.NA math.NA</categories><comments>Multigrid method; Schwarz methods; spectral element method; p-version
  finite element method</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A hybrid Schwarz/multigrid method for spectral element solvers to the Poisson
equation in $\mathbb R^2$ is presented. It is based on an additive Schwarz
method studied by J. Lottes and P. Fischer (J. Sci. Comput. 24:45--78, 2005),
which makes use of the inverse counting matrix for weighting overlapping
Schwarz updates. It is shown that by introducing a nonuniform weighting
function resembling a smoothed jump, the logarithmic multigrid convergence rate
is improved by a factor of 1.5 to 3, leading to a corresponding reduction of
the iteration count in comparison to the original method. Further, the
influence of the overlap width, smoothing strategies, additive versus
multiplicative Schwarz methods, and Krylov acceleration on robustness and
efficiency is investigated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02393</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02393</id><created>2015-12-08</created><authors><author><keyname>Zhu</keyname><forenames>Changbo</forenames></author><author><keyname>Xu</keyname><forenames>Huan</forenames></author><author><keyname>Yan</keyname><forenames>Shuicheng</forenames></author></authors><title>Online Crowdsourcing</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the success of modern internet based platform, such as Amazon Mechanical
Turk, it is now normal to collect a large number of hand labeled samples from
non-experts. The Dawid- Skene algorithm, which is based on Expectation-
Maximization update, has been widely used for inferring the true labels from
noisy crowdsourced labels. However, Dawid-Skene scheme requires all the data to
perform each EM iteration, and can be infeasible for streaming data or large
scale data. In this paper, we provide an online version of Dawid- Skene
algorithm that only requires one data frame for each iteration. Further, we
prove that under mild conditions, the online Dawid-Skene scheme with projection
converges to a stationary point of the marginal log-likelihood of the observed
data. Our experiments demonstrate that the online Dawid- Skene scheme achieves
state of the art performance comparing with other methods based on the Dawid-
Skene scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02394</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02394</id><created>2015-12-08</created><authors><author><keyname>Zhu</keyname><forenames>Changbo</forenames></author><author><keyname>Xu</keyname><forenames>Huan</forenames></author></authors><title>Online Gradient Descent in Function Space</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many problems in machine learning and operations research, we need to
optimize a function whose input is a random variable or a probability density
function, i.e. to solve optimization problems in an in?nite dimensional space.
On the other hand, online learning has the advantage of dealing with streaming
examples, and better model a changing environ- ment. In this paper, we extend
the celebrated online gradient descent algorithm to Hilbert spaces (function
spaces), and analyze the convergence guarantee of the algorithm. Finally, we
demonstrate that our algorithms can be useful in several important problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02406</identifier>
 <datestamp>2015-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02406</id><created>2015-12-08</created><updated>2015-12-15</updated><authors><author><keyname>Chen</keyname><forenames>Yi-Chun</forenames></author><author><keyname>Wheeler</keyname><forenames>Tim Allan</forenames></author><author><keyname>Kochenderfer</keyname><forenames>Mykel John</forenames></author></authors><title>Learning Discrete Bayesian Networks from Continuous Data</title><categories>cs.AI cs.LG</categories><comments>This work has been submitted to Machine Learning (Springer journal)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Real data often contains a mixture of discrete and continuous variables, but
many Bayesian network structure learning and inference algorithms assume all
random variables are discrete. Continuous variables are often discretized, but
the choice of discretization policy has significant impact on the accuracy,
speed, and interpretability of the resulting models. This paper introduces a
principled Bayesian discretization method for continuous variables in Bayesian
networks with quadratic complexity instead of the cubic complexity of other
standard techniques. Empirical demonstrations show that the proposed method is
superior to the state of the art. In addition, this paper shows how to
incorporate existing methods into the structure learning process to discretize
all continuous variables and simultaneously learn Bayesian network structures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02413</identifier>
 <datestamp>2016-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02413</id><created>2015-12-08</created><updated>2016-01-13</updated><authors><author><keyname>Wolf</keyname><forenames>Steffen</forenames></author><author><keyname>Hamprecht</keyname><forenames>Fred A.</forenames></author><author><keyname>Yarkony</keyname><forenames>Julian</forenames></author></authors><title>Tracking Objects with Higher Order Interactions using Delayed Column
  Generation</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new approach to tracking a large number of objects.
Specifically we consider tracking in the context of higher order Markov
interactions which can not be modeled using network flow techniques as
currently developed. Our approach relies on delayed column generation and is
inspired by the corresponding approach to the cutting stock problem. Columns
can be generated exactly using dynamic programming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02425</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02425</id><created>2015-12-08</created><authors><author><keyname>Fridman</keyname><forenames>Lex</forenames></author><author><keyname>Wildman</keyname><forenames>Jeffrey</forenames></author><author><keyname>Weber</keyname><forenames>Steven</forenames></author></authors><title>On the joint impact of bias and power control on downlink spectral
  efficiency in cellular networks</title><categories>cs.IT cs.NI cs.PF math.IT</categories><comments>14 pages, 9 figures, submitted on December 8, 2015 to IEEE/ACM
  Transactions on Networking, extension of Crowncom 2013 submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cell biasing and downlink transmit power are two controls that may be used to
improve the spectral efficiency of cellular networks. With cell biasing, each
mobile user associates with the base station offering, say, the highest biased
signal to interference plus noise ratio. Biasing affects the cell association
decisions of mobile users, but not the received instantaneous downlink
transmission rates. Adjusting the collection of downlink transmission powers
can likewise affect the cell associations, but in contrast with biasing, it
also directly affects the instantaneous rates. This paper investigates the
joint use of both cell biasing and transmission power control and their
(individual and joint) effects on the statistical properties of the collection
of per-user spectral efficiencies. Our analytical results and numerical
investigations demonstrate in some cases a significant performance improvement
in the Pareto efficient frontiers of both a mean-variance and
throughput-fairness tradeoff from using both bias and power controls over using
either control alone.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02430</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02430</id><created>2015-12-08</created><authors><author><keyname>Labai</keyname><forenames>Nadia</forenames></author><author><keyname>Makowsky</keyname><forenames>Johann A.</forenames></author></authors><title>Hankel Matrices for Weighted Visibly Pushdown Automata</title><categories>cs.FL</categories><comments>Accepted for publication in the 10th International Conference on
  Language and Automata Theory and Applications (LATA 2016)</comments><doi>10.1007/978-3-319-30000-9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hankel matrices (aka connection matrices) of word functions and graph
parameters have wide applications in automata theory, graph theory, and machine
learning. We give a characterization of real-valued functions on nested words
recognized by weighted visibly pushdown automata in terms of Hankel matrices on
nested words. This complements C. Mathissen's characterization in terms of
weighted monadic second order logic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02433</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02433</id><created>2015-12-08</created><updated>2015-12-09</updated><authors><author><keyname>Shen</keyname><forenames>Shiqi</forenames></author><author><keyname>Cheng</keyname><forenames>Yong</forenames></author><author><keyname>He</keyname><forenames>Zhongjun</forenames></author><author><keyname>He</keyname><forenames>Wei</forenames></author><author><keyname>Wu</keyname><forenames>Hua</forenames></author><author><keyname>Sun</keyname><forenames>Maosong</forenames></author><author><keyname>Liu</keyname><forenames>Yang</forenames></author></authors><title>Minimum Risk Training for Neural Machine Translation</title><categories>cs.CL</categories><comments>Some typos corrected; the case-sensitiveness of BLEU scores are given</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose minimum risk training for end-to-end neural machine translation.
Unlike conventional maximum likelihood estimation, minimum risk training is
capable of optimizing model parameters directly with respect to evaluation
metrics. Experiments on Chinese-English and English-French translation show
that our approach achieves significant improvements over maximum likelihood
estimation on a state-of-the-art neural machine translation system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02443</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02443</id><created>2015-12-08</created><authors><author><keyname>Akbari</keyname><forenames>Saieed</forenames></author><author><keyname>Etemadi</keyname><forenames>Khashayar</forenames></author><author><keyname>Ezzati</keyname><forenames>Peyman</forenames></author><author><keyname>Ghadiri</keyname><forenames>Mehrdad</forenames></author></authors><title>Even and Odd Cycles Passing a Given Edge or a Vertex</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we provide some sufficient conditions for the existence of an
odd or even cycle that passing a given vertex or an edge in $2$-connected or
$2$-edge connected graphs. We provide some similar conditions for the existence
of an odd or even circuit that passing a given vertex or an edge in 2-edge
connected graphs. We show that if $G$ is a $2$-connected $k$-regular graph, $k
\geq 3$, then every edge of $G$ is contained in an even cycle. We also prove
that in a $2$-edge connected graph, if a vertex has odd degree, then there is
an even cycle containing this vertex.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02456</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02456</id><created>2015-12-08</created><authors><author><keyname>Das</keyname><forenames>Pragna</forenames></author><author><keyname>Xirgo</keyname><forenames>Llu&#xed;s Ribas</forenames></author></authors><title>A study of Time-varying Cost Parameter Estimation Methods in Traffic
  Networks for Mobile Robots</title><categories>cs.RO</categories><comments>Submitted to ICRA'2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Industrial robust controlling systems built using automated guided vehicles
(AGVs) requires planning which depends on cost parameters like time and energy
of the mobile robots functioning in the system. This work addresses the problem
of on-line traversal time identification and estimation for proper mobility of
mobile robots on systems' traffic networks. Several filtering and estimation
methods have been investigated with respect to proper identification of
traversal time of arcs of systems' transportation graphs. We have found that
traversal times vary along time due to a variety of factors, including the
battery charge of the robot, and that the best method to predict the next value
must account not only for these but for a high variance. Results show that path
planning and navigation for each of the mobile robots can be made much more
efficient with this approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02461</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02461</id><created>2015-12-08</created><authors><author><keyname>Mondragon</keyname><forenames>R J</forenames></author></authors><title>Network partition via a bound of the spectral radius</title><categories>physics.soc-ph cs.SI</categories><comments>12 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Based on the density of connections between the nodes of high degree, we
introduce two bounds of the spectral radius. We use these bounds to split a
network into two sets, one of these sets contains the high degree nodes, we
refer to this set as the spectral--core. The degree of the nodes of the
subnetwork formed by the spectral--core gives an approximation to the top
entries of the leading eigenvector of the whole network. We also present some
numerical examples showing the dependancy of the spectral--core with the
assortativity coefficient, its evaluation in several real networks and how the
properties of the spectral--core can be used to reduce the spectral radius.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02479</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02479</id><created>2015-12-08</created><authors><author><keyname>Montavon</keyname><forenames>Gr&#xe9;goire</forenames></author><author><keyname>Bach</keyname><forenames>Sebastian</forenames></author><author><keyname>Binder</keyname><forenames>Alexander</forenames></author><author><keyname>Samek</keyname><forenames>Wojciech</forenames></author><author><keyname>M&#xfc;ller</keyname><forenames>Klaus-Robert</forenames></author></authors><title>Explaining NonLinear Classification Decisions with Deep Taylor
  Decomposition</title><categories>cs.LG stat.ML</categories><comments>20 pages, 15 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nonlinear methods such as Deep Neural Networks (DNNs) are the gold standard
for various challenging machine learning problems, e.g., image classification,
natural language processing or human action recognition. Although these methods
perform impressively well, they have a significant disadvantage, the lack of
transparency, limiting the interpretability of the solution and thus the scope
of application in practice. Especially DNNs act as black boxes due to their
multilayer nonlinear structure. In this paper we introduce a novel methodology
for interpreting generic multilayer neural networks by decomposing the network
classification decision into contributions of its input elements. Although our
focus is on image classification, the method is applicable to a broad set of
input data, learning tasks and network architectures. Our method is based on
deep Taylor decomposition and efficiently utilizes the structure of the network
by backpropagating the explanations from the output to the input layer. We
evaluate the proposed method empirically on the MNIST and ILSVRC data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02483</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02483</id><created>2015-12-08</created><authors><author><keyname>Walter</keyname><forenames>Xavier Alexis</forenames></author><author><keyname>Horsfield</keyname><forenames>Ian</forenames></author><author><keyname>Mayne</keyname><forenames>Richard</forenames></author><author><keyname>Ieropoulos</keyname><forenames>Ioannis A.</forenames></author><author><keyname>Adamatzky</keyname><forenames>Andrew</forenames></author></authors><title>On hybrid circuits exploiting thermistive properties of slime mould</title><categories>cs.ET</categories><comments>9 pages, 6 figures &amp; 1 Table, submitted to Scientific Reports</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Slime mould Physarum polycephalum is a single cell visible by unaided eye.
Let the slime mould span two electrodes with a single protoplasmic tube: if the
tube is heated to approximately 40{\deg}C, the electrical resistance of the
protoplasmic tube increases from 3 M{\Omega} to approximatively 10'000
M{\Omega}. The organism's resistance is not proportional nor correlated to the
temperature of its environment. Slime mould can therefore not be considered as
a thermistor but rather as a thermic switch. We employ the P. polycephalum
thermic switch to prototype hybrid electrical analog summator, NAND gates, and
cascade the gates into Flip-Flop latch. Computing operations performed on this
bio-hybrid computing circuitry feature high repeatability, reproducibility and
comparably low propagation delays
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02496</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02496</id><created>2015-12-08</created><updated>2016-01-16</updated><authors><author><keyname>Wang</keyname><forenames>Tao</forenames></author></authors><title>Light subgraphs in the graphs with average degree at most four</title><categories>math.CO cs.DM</categories><comments>12 pages, 18 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A graph $H$ is said to be {\em light} in a family $\mathfrak{G}$ of graphs if
at least one member of $\mathfrak{G}$ contains a copy of $H$ and there exists
an integer $\lambda(H, \mathfrak{G})$ such that each member $G$ of
$\mathfrak{G}$ with a copy of $H$ also has a copy $K$ of $H$ such that
$\deg_{G}(v) \leq \lambda(H, \mathfrak{G})$ for all $v \in V(K)$. In this
paper, we study the light graphs in the class of graphs with small average
degree, including the plane graphs with some restrictions on girth.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02497</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02497</id><created>2015-12-08</created><authors><author><keyname>Massa</keyname><forenames>Francisco</forenames></author><author><keyname>Russell</keyname><forenames>Bryan</forenames></author><author><keyname>Aubry</keyname><forenames>Mathieu</forenames></author></authors><title>Deep Exemplar 2D-3D Detection by Adapting from Real to Rendered Views</title><categories>cs.CV cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an end-to-end convolutional neural network (CNN) for
2D-3D exemplar detection. We demonstrate that the ability to adapt the features
of natural images to better align with those of CAD rendered views is critical
to the success of our technique. We show that the adaptation can be learned by
compositing rendered views of textured object models on natural images. Our
approach can be naturally incorporated into a CNN detection pipeline and
extends the accuracy and speed benefits from recent advances in deep learning
to 2D-3D exemplar detection. We applied our method to two tasks: instance
detection, where we evaluated on the IKEA dataset, and object category
detection, where we out-perform Aubry et al. for &quot;chair&quot; detection on a subset
of the Pascal VOC dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02504</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02504</id><created>2015-12-08</created><authors><author><keyname>Jain</keyname><forenames>Kartik</forenames></author><author><keyname>Roller</keyname><forenames>Sabine</forenames></author><author><keyname>Mardal</keyname><forenames>Kent-Andre</forenames></author></authors><title>Transitional flow in intracranial aneurysms - a space and time
  refinement study below the Kolmogorov scales using Lattice Boltzmann Method</title><categories>physics.flu-dyn cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most Computational Fluid Dynamics (CFD) studies of hemodynamics in
intracranial aneurysms are based on the assumption of laminar flow due to a
relatively low (below 500) parent artery Reynolds number. A few studies have
recently demonstrated the occurrence of transitional flow in aneurysms, but
these studies employed special finite element schemes tailored to capture
transitional nature of flow. In this study we investigate the occurrence of
transition using a standard Lattice Boltzmann method (LBM). The LBM is used
because of its computational efficiency, which in the present study allowed us
to perform simulations at a higher resolution than has been done in the context
of aneurysms before. The high space-time resolutions of 8{\mu}m and 0.11 {\mu}s
resulted in nearly one billion cells and 9 million time steps per second and
allowed us to quantify the turbulent kinetic energy at resolutions below the
Kolmogorov scales. We perform an in-depth space and time refinement study on 2
aneurysms; one was previously reported laminar, while the other was reported
transitional. Furthermore, we investigate the critical Reynolds number at which
the flow transitions in aneurysms under time constant inflow conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02505</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02505</id><created>2015-12-08</created><authors><author><keyname>Angelini</keyname><forenames>Patrizio</forenames></author><author><keyname>Bekos</keyname><forenames>Michael A.</forenames></author><author><keyname>Kaufmann</keyname><forenames>Michael</forenames></author><author><keyname>Roselli</keyname><forenames>Vincenzo</forenames></author></authors><title>Vertex-Coloring with Star-Defects</title><categories>cs.DS math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Defective coloring is a variant of traditional vertex-coloring, according to
which adjacent vertices are allowed to have the same color, as long as the
monochromatic components induced by the corresponding edges have a certain
structure. Due to its important applications, as for example in the
bipartisation of graphs, this type of coloring has been extensively studied,
mainly with respect to the size, degree, and acyclicity of the monochromatic
components.
  In this paper we focus on defective colorings in which the monochromatic
components are acyclic and have small diameter, namely, they form stars. For
outerplanar graphs, we give a linear-time algorithm to decide if such a
defective coloring exists with two colors and, in the positive case, to
construct one. Also, we prove that an outerpath (i.e., an outerplanar graph
whose weak-dual is a path) always admits such a two-coloring. Finally, we
present NP-completeness results for non-planar and planar graphs of bounded
degree for the cases of two and three colors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02510</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02510</id><created>2015-12-08</created><authors><author><keyname>Hols</keyname><forenames>Eva-Maria C.</forenames></author><author><keyname>Kratsch</keyname><forenames>Stefan</forenames></author></authors><title>A randomized polynomial kernel for Subset Feedback Vertex Set</title><categories>cs.DS cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Subset Feedback Vertex Set problem generalizes the classical Feedback
Vertex Set problem and asks, for a given undirected graph $G=(V,E)$, a set $S
\subseteq V$, and an integer $k$, whether there exists a set $X$ of at most $k$
vertices such that no cycle in $G-X$ contains a vertex of $S$. It was
independently shown by Cygan et al. (ICALP '11, SIDMA '13) and Kawarabayashi
and Kobayashi (JCTB '12) that Subset Feedback Vertex Set is fixed-parameter
tractable for parameter $k$. Cygan et al. asked whether the problem also admits
a polynomial kernelization.
  We answer the question of Cygan et al. positively by giving a randomized
polynomial kernelization for the equivalent version where $S$ is a set of
edges. In a first step we show that Edge Subset Feedback Vertex Set has a
randomized polynomial kernel parameterized by $|S|+k$ with $O(|S|^2k)$
vertices. For this we use the matroid-based tools of Kratsch and Wahlstr\&quot;om
(FOCS '12) that for example were used to obtain a polynomial kernel for
$s$-Multiway Cut. Next we present a preprocessing that reduces the given
instance $(G,S,k)$ to an equivalent instance $(G',S',k')$ where the size of
$S'$ is bounded by $O(k^4)$. These two results lead to a polynomial kernel for
Subset Feedback Vertex Set with $O(k^9)$ vertices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02511</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02511</id><created>2015-12-08</created><authors><author><keyname>Sassioui</keyname><forenames>Redouane</forenames></author><author><keyname>Pierre-Doray</keyname><forenames>Etienne</forenames></author><author><keyname>Szczecinski</keyname><forenames>Leszek</forenames></author><author><keyname>Pelletier</keyname><forenames>Benoit</forenames></author></authors><title>Modelling Decoding Errors in HARQ</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we address the issues of probabilistic modelling of the decoding
errors in hybrid ARQ (HARQ) rounds. In particular we i) claim that the
assumption of independence of decoding errors, used implicitly in various works
on this subject, is an approximation, and ii) propose equally simple but much
more accurate method to calculate the probability of the sequence of decoding
errors. The model we propose is useful from the point of view of performance
evaluation, system-level simulation, and/or link adaptation. Its simplicity
leads also to closed form expression for the outage probability and for the
average number of transmissions in block-fading channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02512</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02512</id><created>2015-11-30</created><updated>2016-03-07</updated><authors><author><keyname>Eriksson</keyname><forenames>Tobias A.</forenames></author><author><keyname>Fehenberger</keyname><forenames>Tobias</forenames></author><author><keyname>Andrekson</keyname><forenames>Peter A.</forenames></author><author><keyname>Karlsson</keyname><forenames>Magnus</forenames></author><author><keyname>Hanik</keyname><forenames>Norbert</forenames></author><author><keyname>Agrell</keyname><forenames>Erik</forenames></author></authors><title>Impact of 4D channel distribution on the achievable rates in coherent
  optical communication experiments</title><categories>cs.IT math.IT</categories><doi>10.1109/JLT.2016.2528550</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We experimentally investigate mutual information and generalized mutual
information for coherent optical transmission systems. The impact of the
assumed channel distribution on the achievable rate is investigated for
distributions in up to four dimensions. Single channel and wavelength division
multiplexing (WDM) transmission over transmission links with and without inline
dispersion compensation are studied. We show that for conventional WDM systems
without inline dispersion compensation, a circularly symmetric complex Gaussian
distribution is a good approximation of the channel. For other channels, such
as with inline dispersion compensation, this is no longer true and gains in the
achievable information rate are obtained by considering more sophisticated
four-dimensional (4D) distributions. We also show that for nonlinear channels,
gains in the achievable information rate can also be achieved by estimating the
mean values of the received constellation in four dimensions. The highest gain
for such channels is seen for a 4D correlated Gaussian distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02515</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02515</id><created>2015-12-08</created><authors><author><keyname>Kumar</keyname><forenames>M. Ashok</forenames></author><author><keyname>Sason</keyname><forenames>Igal</forenames></author></authors><title>Projection Theorems for the R\'enyi Divergence on $\alpha$-Convex Sets</title><categories>cs.IT math.IT math.PR math.ST stat.TH</categories><comments>24 pages. Submitted to the IEEE Trans. on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies forward and reverse projections for the R\'{e}nyi
divergence of order $\alpha \in (0, \infty)$ on $\alpha$-convex sets. The
forward projection on such a set is motivated by some works of Tsallis {\em et
al.} in statistical physics, and the reverse projection is motivated by robust
statistics. In a recent work, van Erven and Harremo\&quot;es proved a Pythagorean
inequality for R\'{e}nyi divergences on $\alpha$-convex sets under the
assumption that the forward projection exists. Continuing this study, a
sufficient condition for the existence of forward projection is proved for
probability measures on a general alphabet. For $\alpha \in (1, \infty)$, the
proof relies on a new Apollonius theorem for the Hellinger divergence, and for
$\alpha \in (0,1)$, the proof relies on the Banach-Alaoglu theorem from
functional analysis. Further projection results are then obtained in the finite
alphabet setting. These include a projection theorem on a specific
$\alpha$-convex set, which is termed an {\em $\alpha$-linear family},
generalizing a result by Csisz\'ar for $\alpha \neq 1$. The solution to this
problem yields a parametric family of probability measures which turns out to
be an extension of the exponential family, and it is termed an {\em
$\alpha$-exponential family}. An orthogonality relationship between the
$\alpha$-exponential and $\alpha$-linear families is established, and it is
used to turn the reverse projection on an $\alpha$-exponential family into a
forward projection on a $\alpha$-linear family. This paper also proves a
convergence result of an iterative procedure used to calculate the forward
projection on an intersection of a finite number of $\alpha$-linear families.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02520</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02520</id><created>2015-12-08</created><authors><author><keyname>Healy</keyname><forenames>C. T.</forenames></author><author><keyname>Uchoa</keyname><forenames>A. G. D.</forenames></author><author><keyname>de Lamare</keyname><forenames>R. C.</forenames></author></authors><title>Study of Structured Root-LDPC Codes and PEG Techniques for Block-Fading
  Channels</title><categories>cs.IT math.IT</categories><comments>7 figures, 13 pages. arXiv admin note: text overlap with
  arXiv:1402.5692</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we propose structured Root-Low-Density Parity-Check (LDPC)
codes and design techniques for block-fading channels. In particular,
Quasi-Cyclic Root-LDPC codes, Irregular repeat-accumulate Root-LDPC codes and
Controlled Doping Root-LDPC codes based on Progressive Edge Growth (PEG)
techniques for block-fading channels are proposed. The proposed Root-LDPC codes
are both suitable for channels under $F = 2, 3$ and $4$ independent fading per
codeword. The performance of the proposed codes is investigated in terms of
Frame Error Rate (FER). The proposed Root-LDPC codes are capable of achieving
the channel diversity and outperform standard LDPC codes. For block-fading
channel with $F = 2$ our proposed PEG-based Root-LDPC codes outperform
PEG-based LDPC codes by $7.5$dB at a FER close to $10^{-3}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02545</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02545</id><created>2015-12-08</created><authors><author><keyname>Kuang</keyname><forenames>Sen</forenames></author><author><keyname>Dong</keyname><forenames>Daoyi</forenames></author><author><keyname>Petersen</keyname><forenames>Ian R.</forenames></author></authors><title>Rapid Lyapunov control of finite-dimensional quantum systems</title><categories>quant-ph cs.SY</categories><comments>15 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rapid state control of quantum systems is significant in reducing the
influence of relaxation or decoherence caused by the environment and enhancing
the capability in dealing with uncertainties in the model and control process.
Bang-bang Lyapunov control can speed up the control process, but cannot
guarantee convergence to a target state. This paper proposes two classes of new
Lyapunov control methods that can achieve rapidly convergent control for
quantum states. One class is switching Lyapunov control where the control law
is designed by switching between bang-bang Lyapunov control and standard
Lyapunov control. The other class is approximate bang-bang Lyapunov control
where we propose two special control functions which are continuously
differentiable and yet have a bang-bang type property. Related stability
results are given and a construction method for the degrees of freedom in the
Lyapunov function is presented to guarantee rapid convergence to a target
eigenstate being isolated in the invariant set. Several numerical examples
demonstrate that the proposed methods can achieve improved performance for
rapid state control of quantum systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02546</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02546</id><created>2015-12-08</created><authors><author><keyname>Hambardzumyan</keyname><forenames>Lianna</forenames></author><author><keyname>Mkrtchyan</keyname><forenames>Vahan</forenames></author></authors><title>Cubic Graphs, Disjoint Matchings and Some Inequalities</title><categories>cs.DM math.CO</categories><comments>21 pages, 17 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For $k = 2,3$ and a cubic graph $G$ let $\nu_k(G)$ denote the size of a
maximum $k$-edge-colorable subgraph of $G$. Mkrtchyan, Petrosyan and Vardanyan
proved that $\nu_2(G)\geq \frac45\cdot |V(G)|$, $\nu_3(G)\geq \frac76\cdot
|V(G)|$ ~\cite{samvel:2010}. They were also able to show that
$\nu_2(G)+\nu_3(G)\geq 2\cdot |V(G)|$ ~\cite{samvel:2014} and $\nu_2(G) \leq
\frac{|V(G)| + 2\cdot \nu_3(G)}{4}$ ~\cite{samvel:2010}. In the present work,
we show that the last two inequalities imply the first two of them.
  Moreover, we show that $\nu_2(G) \geq \alpha \cdot \frac{|V(G)| + 2\cdot
\nu_3(G)}{4} $, where
  $\alpha=\frac{16}{17}$, if $G$ is a cubic graph,
  $\alpha=\frac{20}{21}$, if $G$ is a cubic graph containing a perfect
matching,
  $\alpha=\frac{44}{45}$, if $G$ is a bridgeless cubic graph. \\
  Finally, we investigate the parameters $\nu_2(G)$ and $\nu_3(G)$ in the class
of claw-free cubic graphs. We improve the lower bounds for $\nu_2(G)$ and
$\nu_3(G)$ for claw-free bridgeless cubic graphs to $\nu_2(G)\geq
\frac{29}{30}\cdot |V(G)|$, $\nu_3(G)\geq \frac{43}{45}\cdot |E(G)|$. We also
show that $\nu_2(G) \geq \frac{35}{36} \cdot |V(G)|$ when $n \geq 48$. On the
basis of these inequalities we are able to improve the coefficient $\alpha$ for
bridgeless claw-free cubic graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02548</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02548</id><created>2015-12-08</created><authors><author><keyname>Engsig-Karup</keyname><forenames>Allan Peter</forenames></author><author><keyname>Eskilsson</keyname><forenames>Claes</forenames></author><author><keyname>Bigoni</keyname><forenames>Daniele</forenames></author></authors><title>A Stabilised Nodal Spectral Element Method for Fully Nonlinear Water
  Waves</title><categories>physics.comp-ph cs.CE math.NA</categories><acm-class>G.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an arbitrary-order spectral element method for general-purpose
simulation of non-overturning water waves, described by fully nonlinear
potential theory. The method can be viewed as a high-order extension of the
classical finite element method proposed by Cai et al (1998)
\cite{CaiEtAl1998}, although the numerical implementation differs greatly.
Features of the proposed spectral element method include: nodal Lagrange basis
functions, a general quadrature-free approach and gradient recovery using
global $L^2$ projections. The quartic nonlinear terms present in the Zakharov
form of the free surface conditions can cause severe aliasing problems and
consequently numerical instability for marginally resolved or very steep waves.
We show how the scheme can be stabilised through a combination of
over-integration of the Galerkin projections and a mild spectral filtering on a
per element basis. This effectively removes any aliasing driven instabilities
while retaining the high-order accuracy of the numerical scheme. The additional
computational cost of the over-integration is found insignificant compared to
the cost of solving the Laplace problem. The model is applied to several
benchmark cases in two dimensions. The results confirm the high order accuracy
of the model (exponential convergence), and demonstrate the potential for
accuracy and speedup. The results of numerical experiments are in excellent
agreement with both analytical and experimental results for strongly nonlinear
and irregular dispersive wave propagation. The benefit of using a high-order --
possibly adapted -- spatial discretization for accurate water wave propagation
over long times and distances is particularly attractive for marine
hydrodynamics applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02560</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02560</id><created>2015-12-08</created><authors><author><keyname>Ghahabi</keyname><forenames>Omid</forenames></author><author><keyname>Hernando</keyname><forenames>Javier</forenames></author></authors><title>Deep Learning for Single and Multi-Session i-Vector Speaker Recognition</title><categories>cs.SD cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The promising performance of Deep Learning (DL) in speech recognition has
motivated the use of DL in other speech technology applications such as speaker
recognition. Given i-vectors as inputs, the authors proposed an impostor
selection algorithm and a universal model adaptation process in a hybrid system
based on Deep Belief Networks (DBN) and Deep Neural Networks (DNN) to
discriminatively model each target speaker. In order to have more insight into
the behavior of DL techniques in both single and multi-session speaker
enrollment tasks, some experiments have been carried out in this paper in both
scenarios. Additionally, the parameters of the global model, referred to as
universal DBN (UDBN), are normalized before adaptation. UDBN normalization
facilitates training DNNs specifically with more than one hidden layer.
Experiments are performed on the NIST SRE 2006 corpus. It is shown that the
proposed impostor selection algorithm and UDBN adaptation process enhance the
performance of conventional DNNs 8-20 % and 16-20 % in terms of EER for the
single and multi-session tasks, respectively. In both scenarios, the proposed
architectures outperform the baseline systems obtaining up to 17 % reduction in
EER.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02567</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02567</id><created>2015-12-08</created><authors><author><keyname>Hajiabadi</keyname><forenames>Mojtaba</forenames></author></authors><title>Distributed Adaptive LMF Algorithm for Sparse Parameter Estimation in
  Gaussian Mixture Noise</title><categories>cs.IT cs.CL math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A distributed adaptive algorithm for estimation of sparse unknown parameters
in the presence of nonGaussian noise is proposed in this paper based on
normalized least mean fourth (NLMF) criterion. At the first step, local
adaptive NLMF algorithm is modified by zero norm in order to speed up the
convergence rate and also to reduce the steady state error power in sparse
conditions. Then, the proposed algorithm is extended for distributed scenario
in which more improvement in estimation performance is achieved due to
cooperation of local adaptive filters. Simulation results show the superiority
of the proposed algorithm in comparison with conventional NLMF algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02568</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02568</id><created>2015-12-08</created><authors><author><keyname>Kathuria</keyname><forenames>Tarun</forenames></author><author><keyname>Sudarshan</keyname><forenames>S.</forenames></author></authors><title>Greedy Awakens : Efficient and Provable Multi-Query Optimization</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Complex queries for massive data analysis jobs have become increasingly
commonplace. Many such queries contain common sub-expressions, either within a
single query or among multiple queries submitted as a batch. Conventional query
optimizers do not exploit these common sub-expressions and produce sub-optimal
plans. The problem of multi-query optimization (MQO) is to generate an optimal
\textit{combined} evaluation plan by computing common sub-expressions once and
reusing them. Exhaustive algorithms for MQO explore an $\mathcal{O}(n^n)$
search space. Thus, this problem has primarily been tackled using various
heuristic algorithms, without providing any theoretical guarantees on the
quality of the solution obtained.
  In this paper, instead of the conventional cost minimization problem, we
treat the problem as maximizing a linear transformation of the cost function.
We propose a greedy algorithm for this transformed formulation of the problem,
which under weak, intuitive assumptions, gives an approximation factor with
respect to the optimal solution of this formulation. We go on to show that this
factor is optimal, unless $\mathsf{P} = \mathsf{NP}$. Another noteworthy point
about our algorithm is that it can be easily incorporated into existing
transformation-based optimizers. We finally propose optimizations which can be
used to improve the efficiency of our algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02573</identifier>
 <datestamp>2015-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02573</id><created>2015-12-08</created><updated>2015-12-15</updated><authors><author><keyname>El-Mawass</keyname><forenames>Nour</forenames></author><author><keyname>Alaboodi</keyname><forenames>Saad</forenames></author></authors><title>Hunting for Spammers: Detecting Evolved Spammers on Twitter</title><categories>cs.IR cs.CR cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Once an email problem, spam has nowadays branched into new territories with
disruptive effects. In particular, spam has established itself over the recent
years as a ubiquitous, annoying, and sometimes threatening aspect of online
social networks. Due to its prevalent existence, many works have tackled spam
on Twitter from different angles. Spam is, however, a moving target. The new
generation of spammers on Twitter has evolved into online creatures that are
not easily recognizable by old detection systems. With the strong tangled
spamming community, automatic tweeting scripts, and the ability to massively
create Twitter accounts with a negligible cost, spam on Twitter is becoming
smarter, fuzzier and harder to detect. Our own analysis of spam content on
Arabic trending hashtags in Saudi Arabia results in an estimate of about three
quarters of the total generated content. This alarming rate makes the
development of adaptive spam detection techniques a very real and pressing
need. In this paper, we analyze the spam content of trending hashtags on Saudi
Twitter, and assess the performance of previous spam detection systems on our
recently gathered dataset. Due to the escalating manipulation that
characterizes newer spamming accounts, simple manual labeling currently leads
to inaccurate results. In order to get reliable ground-truth data, we propose
an updated manual classification algorithm that avoids the deficiencies of
older manual approaches. We also adapt the previously proposed features to
respond to spammers evading techniques, and use these features to build a new
data-driven detection system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02592</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02592</id><created>2015-12-08</created><authors><author><keyname>Krotov</keyname><forenames>Denis</forenames><affiliation>Sobolev Institute of Mathematics, Novosibirsk, Russia</affiliation></author></authors><title>The minimum volume of subspace trades</title><categories>cs.DM cs.IT math.CO math.IT</categories><comments>8 pages</comments><msc-class>05B05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A subspace bitrade of type $T_q(t,k,v)$ is a pair $(T_0,T_1)$ of two disjoint
nonempty collections (trades) of $k$-dimensional subspaces of a $v$-dimensional
space $F^v$ over the finite field of order $q$ such that every $t$-dimensional
subspace of $V$ is covered by the same number of subspaces from $T_0$ and
$T_1$. In a previous paper, the minimum cardinality of a subspace
$T_q(t,t+1,v)$ bitrade was establish. We generalize that result by showing that
for admissible $v$, $t$, and $k$, the minimum cardinality of a subspace
$T_q(t,k,v)$ bitrade does not depend on $k$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02594</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02594</id><created>2015-12-08</created><authors><author><keyname>Brito</keyname><forenames>Italo</forenames></author><author><keyname>Figueiredo</keyname><forenames>Gustavo B.</forenames></author></authors><title>Suporte \`a Mobilidade em Redes Mesh Sem Fio: estrat\'egias comuns
  versus SDN</title><categories>cs.NI</categories><comments>in Portuguese</comments><report-no>TR-PGCOMP-003/2015</report-no><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Wireless mesh networks have been presented as a robust, scalable and low cost
solution to provide connectivity in long distance areas. However, given its
nature, routing strategies must support seamless mobility of nodes while
enabling operation with a good performance and fast self-recovery from links
fault. The routing approaches that meet these requirements are based on
protocols usually used in ad-hoc networks (e.g. OLSR and B.A.T.M.A.N.) and,
more recently, in the SDN paradigm (e.g. OpenWiMesh), each one of them having
its own pros and cons. Therefore, it is important to evaluate the benefits and
impacts of each approach, taking into account models and metrics inherent to
the mobility. In this paper, we propose a simplified implementation of mobility
support in OpenWiMesh and evaluate its performance compared to other protocols,
using differents mobility and data traffic models. The chosen metrics are based
on packet loss, occupation of the links with control traffic and delay.
Simulated results show that the SDN approach perform better than the classic
protocols, beyond flexibility and programmability of SDN itself.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02595</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02595</id><created>2015-12-08</created><authors><author><keyname>Amodei</keyname><forenames>Dario</forenames></author><author><keyname>Anubhai</keyname><forenames>Rishita</forenames></author><author><keyname>Battenberg</keyname><forenames>Eric</forenames></author><author><keyname>Case</keyname><forenames>Carl</forenames></author><author><keyname>Casper</keyname><forenames>Jared</forenames></author><author><keyname>Catanzaro</keyname><forenames>Bryan</forenames></author><author><keyname>Chen</keyname><forenames>Jingdong</forenames></author><author><keyname>Chrzanowski</keyname><forenames>Mike</forenames></author><author><keyname>Coates</keyname><forenames>Adam</forenames></author><author><keyname>Diamos</keyname><forenames>Greg</forenames></author><author><keyname>Elsen</keyname><forenames>Erich</forenames></author><author><keyname>Engel</keyname><forenames>Jesse</forenames></author><author><keyname>Fan</keyname><forenames>Linxi</forenames></author><author><keyname>Fougner</keyname><forenames>Christopher</forenames></author><author><keyname>Han</keyname><forenames>Tony</forenames></author><author><keyname>Hannun</keyname><forenames>Awni</forenames></author><author><keyname>Jun</keyname><forenames>Billy</forenames></author><author><keyname>LeGresley</keyname><forenames>Patrick</forenames></author><author><keyname>Lin</keyname><forenames>Libby</forenames></author><author><keyname>Narang</keyname><forenames>Sharan</forenames></author><author><keyname>Ng</keyname><forenames>Andrew</forenames></author><author><keyname>Ozair</keyname><forenames>Sherjil</forenames></author><author><keyname>Prenger</keyname><forenames>Ryan</forenames></author><author><keyname>Raiman</keyname><forenames>Jonathan</forenames></author><author><keyname>Satheesh</keyname><forenames>Sanjeev</forenames></author><author><keyname>Seetapun</keyname><forenames>David</forenames></author><author><keyname>Sengupta</keyname><forenames>Shubho</forenames></author><author><keyname>Wang</keyname><forenames>Yi</forenames></author><author><keyname>Wang</keyname><forenames>Zhiqian</forenames></author><author><keyname>Wang</keyname><forenames>Chong</forenames></author><author><keyname>Xiao</keyname><forenames>Bo</forenames></author><author><keyname>Yogatama</keyname><forenames>Dani</forenames></author><author><keyname>Zhan</keyname><forenames>Jun</forenames></author><author><keyname>Zhu</keyname><forenames>Zhenyao</forenames></author></authors><title>Deep Speech 2: End-to-End Speech Recognition in English and Mandarin</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that an end-to-end deep learning approach can be used to recognize
either English or Mandarin Chinese speech--two vastly different languages.
Because it replaces entire pipelines of hand-engineered components with neural
networks, end-to-end learning allows us to handle a diverse variety of speech
including noisy environments, accents and different languages. Key to our
approach is our application of HPC techniques, resulting in a 7x speedup over
our previous system. Because of this efficiency, experiments that previously
took weeks now run in days. This enables us to iterate more quickly to identify
superior architectures and algorithms. As a result, in several cases, our
system is competitive with the transcription of human workers when benchmarked
on standard datasets. Finally, using a technique called Batch Dispatch with
GPUs in the data center, we show that our system can be inexpensively deployed
in an online setting, delivering low latency when serving users at scale.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02615</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02615</id><created>2015-12-08</created><authors><author><keyname>Berta</keyname><forenames>Mario</forenames></author><author><keyname>Fawzi</keyname><forenames>Omar</forenames></author><author><keyname>Tomamichel</keyname><forenames>Marco</forenames></author></authors><title>On Variational Expressions for Quantum Relative Entropies</title><categories>quant-ph cs.IT math-ph math.IT math.MP</categories><comments>21 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distance measures between quantum states like the trace distance and the
fidelity can naturally be defined by optimizing a classical distance measure
over all measurement statistics that can be obtained from the respective
quantum states. In contrast, Petz showed that the measured relative entropy,
defined as a maximization of the Kullback-Leibler divergence over projective
measurement statistics, is strictly smaller than Umegaki's quantum relative
entropy whenever the states do not commute. We extend this result in two ways.
First, we show that Petz' conclusion remains true if we allow general positive
valued measures. To do this, we prove that maximizing the Kullback-Leibler
divergence over general positive valued measures results in the measured
relative entropy. Second, we extend the result to R\'enyi relative entropies
and show that for non-commuting states the sandwiched R\'enyi relative entropy
is strictly larger than the measured R\'enyi relative entropy for $\alpha \in
(\frac12, \infty)$, and strictly smaller for $\alpha \in [0,\frac12)$. The
latter statement provides counterexamples for the data-processing inequality of
the sandwiched R\'enyi relative entropy for $\alpha &lt; \frac12$. Our main tool
is a new variational expression for the measured R\'enyi relative entropy,
which we further exploit to show that certain lower bounds on quantum
conditional mutual information are superadditive.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02620</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02620</id><created>2015-12-08</created><authors><author><keyname>Alc&#xe1;zar</keyname><forenames>Juan Gerardo</forenames></author><author><keyname>Hermoso</keyname><forenames>Carlos</forenames></author><author><keyname>Muntingh</keyname><forenames>Georg</forenames></author></authors><title>Detecting similarity of rational space curves</title><categories>math.AG cs.CG cs.SC</categories><msc-class>14Q05, 68W30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide an algorithm to check whether two rational space curves are
related by a similarity. The algorithm exploits the relationship between the
curvatures and torsions of two similar curves, which is formulated in a
computer algebra setting. Helical curves, where curvature and torsion are
proportional, need to be distinguished as a special case. The algorithm is easy
to implement, as it involves only standard computer algebra techniques, such as
greatest common divisors and resultants, and Gr\&quot;obner basis for the special
case of helical curves. Details on the implementation and experimentation
carried out on the computer algebra system Maple 18 are provided. Timings show
that the algorithm is efficient for moderate degrees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02624</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02624</id><created>2015-12-07</created><authors><author><keyname>Venkataram</keyname><forenames>Hamsa Shwetha</forenames></author><author><keyname>Kruthika</keyname></author><author><keyname>MK</keyname><forenames>Prarthana</forenames></author><author><keyname>C</keyname><forenames>Shivani</forenames></author><author><keyname>Purama</keyname><forenames>Siva Naga Suresh</forenames></author><author><keyname>SP</keyname><forenames>Muthukumar</forenames></author></authors><title>'healthWISE' An Android Application For Personal Health And Nutrition
  Management</title><categories>cs.CY</categories><comments>Presented in International Conference on Current Trends in
  Engineering and Management(ICCTEM 2012) 12-14 July 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Now-a-days, people are getting more health conscious and tend to keep a check
on nutritional gain from the packed food products they consume. This
application christened 'healthWISE' helps a mobile user to scan/enter the
bar-code on the packed food product, know the nutritional information, and upon
entering the quantity to be consumed, it displays the energy the individual can
consume. If the user has exceeded the stipulated amount of calories, then it
suggests the exercise to burn the extra number of calories.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02627</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02627</id><created>2015-12-08</created><authors><author><keyname>Subbaraman</keyname><forenames>Anantharaman</forenames></author><author><keyname>Benosman</keyname><forenames>Mouhacine</forenames></author></authors><title>Extremum Seeking-based Iterative Learning Model Predictive Control
  (ESILC-MPC)</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study a tracking control problem for linear time-invariant
systems, with model parametric uncertainties, under input and states
constraints. We apply the idea of modular design introduced in Benosman et al.
2014, to solve this problem in the model predictive control (MPC) framework. We
propose to design an MPC with input-to-state stability (ISS) guarantee, and
complement it with an extremum seeking (ES) algorithm to iteratively learn the
model uncertainties. The obtained MPC algorithms can be classified as iterative
learning control (ILC)-MPC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02665</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02665</id><created>2015-12-08</created><updated>2015-12-10</updated><authors><author><keyname>Zhou</keyname><forenames>Feng</forenames></author><author><keyname>Lin</keyname><forenames>Yuanqing</forenames></author></authors><title>Fine-grained Image Classification by Exploring Bipartite-Graph Labels</title><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Given a food image, can a fine-grained object recognition engine tell &quot;which
restaurant which dish&quot; the food belongs to? Such ultra-fine grained image
recognition is the key for many applications like search by images, but it is
very challenging because it needs to discern subtle difference between classes
while dealing with the scarcity of training data. Fortunately, the ultra-fine
granularity naturally brings rich relationships among object classes. This
paper proposes a novel approach to exploit the rich relationships through
bipartite-graph labels (BGL). We show how to model BGL in an overall
convolutional neural networks and the resulting system can be optimized through
back-propagation. We also show that it is computationally efficient in
inference thanks to the bipartite structure. To facilitate the study, we
construct a new food benchmark dataset, which consists of 37,885 food images
collected from 6 restaurants and totally 975 menus. Experimental results on
this new food and three other datasets demonstrates BGL advances previous works
in fine-grained object recognition. An online demo is available at
http://www.f-zhou.com/fg_demo/.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02668</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02668</id><created>2015-11-23</created><authors><author><keyname>Zeng</keyname><forenames>William</forenames></author><author><keyname>Zahn</keyname><forenames>Philipp</forenames></author></authors><title>Contextuality and the Weak Axiom in the Theory of Choice</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent work on the logical structure of non-locality has constructed
scenarios where observations of multi-partite systems cannot be adequately
described by compositions of non-signaling subsystems. In this paper we apply
these frameworks to economics. First we construct a empirical model of choice,
where choices are understood as observable outcomes in a certain sense. An
analysis of contextuality within this framework allows us to characterize which
scenarios allow for the possible construction of an adequate global choice
rule. In essence, we mathematically characterize when it makes sense to
consider the choices of a group as composed of individual choices. We then map
out the logical space of some relevant empirical principles, relating
properties of these contextual choice scenarios to no-signalling theories and
to the weak axiom of revealed preference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02671</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02671</id><created>2015-12-08</created><authors><author><keyname>Martinsson</keyname><forenames>Per-Gunnar</forenames></author><author><keyname>Quintana-Orti</keyname><forenames>Gregorio</forenames></author><author><keyname>Heavner</keyname><forenames>Nathan</forenames></author><author><keyname>van de Geijn</keyname><forenames>Robert</forenames></author></authors><title>Householder QR Factorization: Adding Randomization for Column Pivoting.
  FLAME Working Note #78</title><categories>math.NA cs.NA</categories><report-no>FLAME Working Note #78</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A fundamental problem when adding column pivoting to the Householder QR
factorization is that only about half of the computation can be cast in terms
of high performing matrix-matrix multiplication, which greatly limits the
benefits of so-called blocked algorithms. This paper describes a technique for
selecting groups of pivot vectors by means of randomized projections. It is
demonstrated that the asymptotic flop count for the proposed method is $2mn^2 -
(2/3)n^3$ for an $m\times n$ matrix with $m \geq n$, identical to that of the
best classical Householder QR factorization (with or without pivoting).
Experiments demonstrate improvements in speed of substantial integer factors
(between a factor of 3 and 5) relative to LAPACK's geqp3 implementation on a
modern CPU.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02673</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02673</id><created>2015-12-08</created><updated>2015-12-10</updated><authors><author><keyname>Lee</keyname><forenames>Kangwook</forenames></author><author><keyname>Lam</keyname><forenames>Maximilian</forenames></author><author><keyname>Pedarsani</keyname><forenames>Ramtin</forenames></author><author><keyname>Papailiopoulos</keyname><forenames>Dimitris</forenames></author><author><keyname>Ramchandran</keyname><forenames>Kannan</forenames></author></authors><title>Speeding Up Distributed Machine Learning Using Codes</title><categories>cs.DC cs.IT cs.LG cs.PF math.IT</categories><comments>In Neural Information Processing Systems, Workshop on Machine
  Learning Systems, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Codes are widely used in many engineering applications to offer some form of
reliability and fault tolerance. The high-level idea of coding is to exploit
resource redundancy to deliver higher robustness against system noise. In
large-scale systems there are several types of &quot;noise&quot; that can affect the
performance of distributed machine learning algorithms: straggler nodes, system
failures, or communication bottlenecks. Moreover, redundancy is abundant: a
plethora of nodes, a lot of spare storage, etc.
  In this work, scratching the surface of &quot;codes for distributed computation,&quot;
we provide theoretical insights on how coded solutions can achieve significant
gains compared to uncoded ones. We focus on two of the most basic building
blocks of distributed learning algorithms: matrix multiplication and data
shuffling. For matrix multiplication, we use codes to leverage the plethora of
nodes and alleviate the effects of stragglers. We show that if the number of
workers is $n$, and the runtime of each subtask has an exponential tail, the
optimal coded matrix multiplication is $\Theta(\log n)$ times faster than the
uncoded matrix multiplication. In data shuffling, we use codes to exploit the
excess in storage and reduce communication bottlenecks. We show that when
$\alpha$ is the fraction of the data matrix that can be cached at each worker,
and $n$ is the number of workers, coded shuffling reduces the communication
cost by a factor $\Theta(\alpha \gamma(n))$ compared to uncoded shuffling,
where $\gamma(n)$ is the ratio of the cost of unicasting $n$ messages to $n$
users to broadcasting a common message (of the same size) to $n$ users. Our
synthetic and Open MPI experiments on Amazon EC2 show that coded distributed
algorithms can achieve significant speedups of up to 40% compared to uncoded
distributed algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02684</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02684</id><created>2015-12-08</created><authors><author><keyname>Swaminathan</keyname><forenames>Meenupriya</forenames></author><author><keyname>Muncuk</keyname><forenames>Ufuk</forenames></author><author><keyname>Chowdhury</keyname><forenames>Kaushik R.</forenames></author></authors><title>Topology Optimization for Galvanic Coupled Wireless Intra-body
  Communication</title><categories>cs.NI</categories><comments>This paper has been accepted for inclusion in the IEEE INFOCOM 2016
  technical program. This is an author copy</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Implanted sensors and actuators in the human body promise in-situ health
monitoring and rapid advancements in personalized medicine. We propose a new
paradigm where such implants may communicate wirelessly through a technique
called as galvanic coupling, which uses weak electrical signals and the
conduction properties of body tissues. While galvanic coupling overcomes the
problem of massive absorption of RF waves in the body, the unique intra-body
channel raises several questions on the topology of the implants and the
external (i.e., on skin) data collection nodes. This paper makes the first
contributions towards (i) building an energy-efficient topology through optimal
placement of data collection points/relays using measurement-driven tissue
channel models, and (ii) balancing the energy consumption over the entire
implant network so that the application needs are met. We achieve this via a
two-phase iterative clustering algorithm for the implants and formulate an
optimization problem that decides the position of external data-gathering
points. Our theoretical results are validated via simulations and experimental
studies on real tissues, with demonstrated increase in the network lifetime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02693</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02693</id><created>2015-12-08</created><authors><author><keyname>Jameson</keyname><forenames>John W.</forenames></author></authors><title>Reinforcement Control with Hierarchical Backpropagated Adaptive Critics</title><categories>cs.NE cs.LG cs.SY</categories><comments>16 pages, 5 figures</comments><acm-class>I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Present incremental learning methods are limited in the ability to achieve
reliable credit assignment over a large number time steps (or events). However,
this situation is typical for cases where the dynamical system to be controlled
requires relatively frequent control updates in order to maintain stability or
robustness yet has some action-consequences which must be established over
relatively long periods of time. To address this problem, the learning
capabilities of a control architecture comprised of two Backpropagated Adaptive
Critics (BACs) in a two-level hierarchy with continuous actions are explored.
The high-level BAC updates less frequently than the low-level BAC and controls
the latter to some degree. The response of the low-level to high-level signals
can either be determined a priori or it can emerge during learning. A general
approach called Response Induction Learning is introduced to address the latter
case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02698</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02698</id><created>2015-12-08</created><updated>2015-12-10</updated><authors><author><keyname>Kearns</keyname><forenames>Michael</forenames></author><author><keyname>Pai</keyname><forenames>Mallesh M.</forenames></author><author><keyname>Rogers</keyname><forenames>Ryan</forenames></author><author><keyname>Roth</keyname><forenames>Aaron</forenames></author><author><keyname>Ullman</keyname><forenames>Jonathan</forenames></author></authors><title>Robust Mediators in Large Games</title><categories>cs.GT</categories><comments>This work unifies and subsumes the two papers &quot;Mechanism design in
  large games: incentives and privacy&quot; ITCS'14 (arXiv:1207.4084) and
  &quot;Asymptotically truthful equilibrium selection in large congestion games&quot; EC
  '14 (arXiv:1311.2625)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A mediator is a mechanism that can only suggest actions to players, as a
function of all agents' reported types, in a given game of incomplete
information. We study what is achievable by two kinds of mediators, &quot;strong&quot;
and &quot;weak.&quot; Players can choose to opt-out of using a strong mediator but cannot
misrepresent their type if they opt-in. Such a mediator is &quot;strong&quot; because we
can view it as having the ability to verify player types. Weak mediators lack
this ability--- players are free to misrepresent their type to a weak mediator.
We show a striking result---in a prior-free setting, assuming only that the
game is large and players have private types, strong mediators can implement
approximate equilibria of the complete-information game. If the game is a
congestion game, then the same result holds using only weak mediators. Our
result follows from a novel application of differential privacy, in particular,
a variant we propose called joint differential privacy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02707</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02707</id><created>2015-12-08</created><authors><author><keyname>Radziwill</keyname><forenames>Nicole</forenames></author><author><keyname>Romano</keyname><forenames>Jessica</forenames></author><author><keyname>Shorter</keyname><forenames>Diane</forenames></author><author><keyname>Benton</keyname><forenames>Morgan</forenames></author></authors><title>The Ethics of Hacking: Should It Be Taught?</title><categories>cs.CY</categories><journal-ref>Software Quality Professional, 18(1), p. 11-15 (December 2015)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Poor software quality can adversely affect application security by increasing
the potential for a malicious breach of a system. Because computer security and
cybersecurity are becoming such relevant topics for practicing software
engineers, the need for educational opportunities in this area is steadily
increasing. Universities and colleges have recognized this, and have started to
offer programs in cybersecurity. At face value, these new programs may not
appear controversial, but developing their curriculum requires answering a
complex ethical question: Should programs teach hacking to their students? Even
though there are different types of hackers, media reports of cybersecurity
incidents tend to reserve the &quot;hacker&quot; label for cyber criminals, which
overlooks the value in hacking (and, by extension, teaching students to hack).
This article examines the full spectrum of hacking behavior, as well as
arguments for and against including hacking in education programs, and
recommends that hacking skills be considered an essential component of an
education and practice in software quality assurance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02708</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02708</id><created>2015-12-08</created><authors><author><keyname>Radziwill</keyname><forenames>Nicole</forenames></author><author><keyname>Benton</keyname><forenames>Morgan</forenames></author><author><keyname>Boadu</keyname><forenames>Kenneth</forenames></author><author><keyname>Perdomo</keyname><forenames>Wilson</forenames></author></authors><title>A Case-Based Look at Integrating Social Context into Software Quality</title><categories>cs.SE</categories><journal-ref>Software Quality Professional, 18(1), p. 16-22 (December 2015)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ensuring high-quality software requires considering the social climate within
which the applications will be deployed and used. This can be done by designing
quality goals and objectives that are consistent with changing social and
ethical landscapes. Using principles of technological determinism, this article
presents three cases that illustrate why it is becoming even more important to
integrate these concerns into software design and quality assurance. With these
examples in mind, this article explains how to consider technological
determinism in software design and quality assurance practices to achieve this
enhanced sensitivity on a practical level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02714</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02714</id><created>2015-12-08</created><authors><author><keyname>Zhu</keyname><forenames>Rong</forenames></author><author><keyname>Zou</keyname><forenames>Zhaonian</forenames></author><author><keyname>Li</keyname><forenames>Jianzhong</forenames></author></authors><title>SimRank Computation on Uncertain Graphs</title><categories>cs.DB</categories><comments>14 pages, under review by ICDE'16</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  SimRank is a similarity measure between vertices in a graph, which has become
a fundamental technique in graph analytics. Recently, many algorithms have been
proposed for efficient evaluation of SimRank similarities. However, the
existing SimRank computation algorithms either overlook uncertainty in graph
structures or is based on an unreasonable assumption (Du et al). In this paper,
we study SimRank similarities on uncertain graphs based on the possible world
model of uncertain graphs. Following the random-walk-based formulation of
SimRank on deterministic graphs and the possible worlds model of uncertain
graphs, we define random walks on uncertain graphs for the first time and show
that our definition of random walks satisfies Markov's property. We formulate
the SimRank measure based on random walks on uncertain graphs. We discover a
critical difference between random walks on uncertain graphs and random walks
on deterministic graphs, which makes all existing SimRank computation
algorithms on deterministic graphs inapplicable to uncertain graphs. To
efficiently compute SimRank similarities, we propose three algorithms, namely
the baseline algorithm with high accuracy, the sampling algorithm with high
efficiency, and the two-phase algorithm with comparable efficiency as the
sampling algorithm and about an order of magnitude smaller relative error than
the sampling algorithm. The extensive experiments and case studies verify the
effectiveness of our SimRank measure and the efficiency of our SimRank
computation algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02719</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02719</id><created>2015-12-08</created><authors><author><keyname>Swaminathan</keyname><forenames>Meenupriya</forenames></author><author><keyname>Cabrera</keyname><forenames>Ferran Simon</forenames></author><author><keyname>Pujol</keyname><forenames>Joan Sebastia</forenames></author><author><keyname>Muncuk</keyname><forenames>Ufuk</forenames></author><author><keyname>Schirner</keyname><forenames>Gunar</forenames></author><author><keyname>Chowdhury</keyname><forenames>Kaushik R.</forenames></author></authors><title>Multi-path Model and Sensitivity Analysis for Galvanic Coupled
  Intra-body Communication through Layered Tissue</title><categories>cs.NI cs.ET</categories><comments>This paper has been accepted for publication in IEEE Transaction on
  Biomedical Circuits and Systems and will appear in March 2016 issue. This is
  an author copy</comments><doi>10.1109/TBCAS.2015.2412548</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  New medical procedures promise continuous patient monitoring and drug
delivery through implanted sensors and actuators. When over the air wireless
radio frequency (OTA-RF) links are used for intra-body implant communication,
the network incurs heavy energy costs owing to absorption within the human
tissue. With this motivation, we explore an alternate form of intra-body
communication that relies on weak electrical signals, instead of OTA-RF. To
demonstrate the feasibility of this new paradigm for enabling communication
between sensors and actuators embedded within the tissue, or placed on the
surface of the skin, we develop a rigorous analytical model based on galvanic
coupling of low energy signals. The main contributions in this paper are: (i)
developing a suite of analytical expressions for modeling the resulting
communication channel for weak electrical signals in a three dimensional
multi-layered tissue structure, (ii) validating and verifying the model through
extensive finite element simulations, published measurements in existing
literature, and experiments conducted with porcine tissue, (iii) designing the
communication framework with safety considerations, and analyzing the influence
of different network and hardware parameters such as transmission frequency and
electrode placements. Our results reveal a close agreement between theory,
simulation, literature and experimental findings, pointing to the suitability
of the model for quick and accurate channel characterization and parameter
estimation for networked and implanted sensors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02723</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02723</id><created>2015-12-08</created><authors><author><keyname>Lang</keyname><forenames>Guangming</forenames></author></authors><title>Incremental approaches to knowledge reduction of covering decision
  information systems with variations of coverings</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In practical situations, calculating approximations of concepts is the
central step for knowledge reduction of dynamic covering decision information
system, which has received growing interests of researchers in recent years. In
this paper, the second and sixth lower and upper approximations of sets in
dynamic covering information systems with variations of coverings are computed
from the perspective of matrix using incremental approaches. Especially,
effective algorithms are designed for calculating the second and sixth lower
and upper approximations of sets in dynamic covering information systems with
the immigration of coverings. Experimental results demonstrate that the
designed algorithms provide an efficient and effective method for constructing
the second and sixth lower and upper approximations of sets in dynamic covering
information systems. Two examples are explored to illustrate the process of
knowledge reduction of dynamic covering decision information systems with the
covering immigration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02726</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02726</id><created>2015-12-08</created><updated>2015-12-14</updated><authors><author><keyname>Mao</keyname><forenames>Lingxiang</forenames></author></authors><title>Information Resources Management Framework for Virtual Enterprise</title><categories>cs.CY cs.DC</categories><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Virtual enterprise is a new form of organization in recent years which adapt
to the IT environment. Information resources management implemented in the
virtual enterprise is determined by the form of business organization and
information exchange mechanisms. According to the present characteristics of
virtual enterprise management, it puts forward the strategies and measures of
information resources management framework for virtual enterprise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02727</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02727</id><created>2015-12-08</created><authors><author><keyname>Aydin</keyname><forenames>Kevin</forenames></author><author><keyname>Bateni</keyname><forenames>MohammadHossein</forenames></author><author><keyname>Mirrokni</keyname><forenames>Vahab</forenames></author></authors><title>Distributed Balanced Partitioning via Linear Embedding</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Balanced partitioning is often a crucial first step in solving large-scale
graph optimization problems, e.g., in some cases, a big graph can be chopped
into pieces that fit on one machine to be processed independently before
stitching the results together. In other cases, links between different parts
may show up in the running time and/or network communications cost. We study a
distributed balanced partitioning problem where the goal is to partition the
vertices of a given graph into k pieces so as to minimize the total cut size.
Our algorithm is composed of a few steps that are easily implementable in
distributed computation frameworks. The algorithm first embeds nodes of the
graph onto a line, and then processes nodes in a distributed manner guided by
the linear embedding order. We examine various ways to find the first
embedding, e.g., via a hierarchical clustering or Hilbert curves. Then we apply
four different techniques including local swaps, minimum cuts on the boundaries
of partitions, as well as contraction and dynamic programming. As our empirical
study, we compare the above techniques with each other, and also to previous
work in distributed graph algorithms, e.g., a label propagation method, FENNEL
and Spinner. We report our results both on a private map graph and several
public social networks, and show that our results beat previous distributed
algorithms: e.g., compared to the label propagation algorithm, we report an
improvement of 15-25% in the cut value. We also observe that our algorithms
allow for scalable distributed implementation for any number of partitions.
Finally, we apply our techniques for the Google Maps Driving Directions to
minimize the number of multi-shard queries with the goal of saving in CPU
usage. During live experiments, we observe an ~40% drop in the number of
multi-shard queries when comparing our method with a standard geography-based
method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02728</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02728</id><created>2015-12-08</created><updated>2015-12-11</updated><authors><author><keyname>Kumar</keyname><forenames>Abhimanu</forenames></author><author><keyname>Xie</keyname><forenames>Pengtao</forenames></author><author><keyname>Yin</keyname><forenames>Junming</forenames></author><author><keyname>Xing</keyname><forenames>Eric P.</forenames></author></authors><title>Distributed Training of Deep Neural Networks with Theoretical Analysis:
  Under SSP Setting</title><categories>stat.ML cs.LG math.OC</categories><comments>The paper needs more refinement</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a distributed approach to train deep neural networks (DNNs), which
has guaranteed convergence theoretically and great scalability empirically:
close to 6 times faster on instance of ImageNet data set when run with 6
machines. The proposed scheme is close to optimally scalable in terms of number
of machines, and guaranteed to converge to the same optima as the undistributed
setting. The convergence and scalability of the distributed setting is shown
empirically across di?erent datasets (TIMIT and ImageNet) and machine learning
tasks (image classi?cation and phoneme extraction). The convergence analysis
provides novel insights into this complex learning scheme, including: 1)
layerwise convergence, and 2) convergence of the weights in probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02730</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02730</id><created>2015-12-08</created><authors><author><keyname>Biniaz</keyname><forenames>Ahmad</forenames></author><author><keyname>Bose</keyname><forenames>Prosenjit</forenames></author><author><keyname>Maheshwari</keyname><forenames>Anil</forenames></author><author><keyname>Smid</keyname><forenames>Michiel</forenames></author></authors><title>Plane Bichromatic Trees of Low Degree</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $R$ and $B$ be two disjoint sets of points in the plane such that
$|B|\leqslant |R|$, and no three points of $R\cup B$ are collinear. We show
that the geometric complete bipartite graph $K(R,B)$ contains a non-crossing
spanning tree whose maximum degree is at most $\max\left\{3, \left\lceil
\frac{|R|-1}{|B|}\right\rceil + 1\right\}$; this is the best possible upper
bound on the maximum degree. This solves an open problem posed by Abellanas et
al. at the Graph Drawing Symposium, 1996.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02734</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02734</id><created>2015-12-08</created><authors><author><keyname>Chandrasekaran</keyname><forenames>Karthekeyan</forenames></author><author><keyname>Gandikota</keyname><forenames>Venkata</forenames></author><author><keyname>Grigorescu</keyname><forenames>Elena</forenames></author></authors><title>Deciding Orthogonality in Construction-A Lattices</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lattices are discrete mathematical objects with widespread applications to
integer programs as well as modern cryptography. A fundamental problem in both
domains is the Closest Vector Problem (popularly known as CVP). It is
well-known that CVP can be easily solved in lattices that have an orthogonal
basis \emph{if} the orthogonal basis is specified. This motivates the
orthogonality decision problem: verify whether a given lattice has an
orthogonal basis. Surprisingly, the orthogonality decision problem is not known
to be either NP-complete or in P.
  In this paper, we focus on the orthogonality decision problem for a
well-known family of lattices, namely Construction-A lattices. These are
lattices of the form $C+q\mathbb{Z}^n$, where $C$ is an error-correcting
$q$-ary code, and are studied in communication settings. We provide a complete
characterization of lattices obtained from binary and ternary codes using
Construction-A that have an orthogonal basis. We use this characterization to
give an efficient algorithm to solve the orthogonality decision problem. Our
algorithm also finds an orthogonal basis if one exists for this family of
lattices. We believe that these results could provide a better understanding of
the complexity of the orthogonality decision problem for general lattices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02736</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02736</id><created>2015-12-08</created><authors><author><keyname>Zeng</keyname><forenames>Xingyu</forenames></author><author><keyname>Ouyang</keyname><forenames>Wanli</forenames></author><author><keyname>Wang</keyname><forenames>Xiaogang</forenames></author></authors><title>Window-Object Relationship Guided Representation Learning for Generic
  Object Detections</title><categories>cs.CV cs.LG cs.MM</categories><comments>9 pages, including 1 reference page, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In existing works that learn representation for object detection, the
relationship between a candidate window and the ground truth bounding box of an
object is simplified by thresholding their overlap. This paper shows
information loss in this simplification and picks up the relative location/size
information discarded by thresholding. We propose a representation learning
pipeline to use the relationship as supervision for improving the learned
representation in object detection. Such relationship is not limited to object
of the target category, but also includes surrounding objects of other
categories. We show that image regions with multiple contexts and multiple
rotations are effective in capturing such relationship during the
representation learning process and in handling the semantic and visual
variation caused by different window-object configurations. Experimental
results show that the representation learned by our approach can improve the
object detection accuracy by 6.4% in mean average precision (mAP) on
ILSVRC2014. On the challenging ILSVRC2014 test dataset, 48.6% mAP is achieved
by our single model and it is the best among published results. On PASCAL VOC,
it outperforms the state-of-the-art result of Fast RCNN by 3.3% in absolute
mAP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02737</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02737</id><created>2015-12-08</created><authors><author><keyname>Simhadri</keyname><forenames>Harsha Vardhan</forenames></author></authors><title>Using Symmetry to Schedule Classical Matrix Multiplication</title><categories>cs.DC</categories><acm-class>F.2; D.2.8; D.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Presented with a new machine with a specific interconnect topology, algorithm
designers use intuition about the symmetry of the algorithm to design time and
communication-efficient schedules that map the algorithm to the machine. Is
there a systematic procedure for designing schedules? We present a new
technique to design schedules for algorithms with no non-trivial dependencies,
focusing on the classical matrix multiplication algorithm.
  We model the symmetry of algorithm with the set of instructions $X$ as the
action of the group formed by the compositions of bijections from the set $X$
to itself. We model the machine as the action of the group $N\times \Delta$,
where $N$ and $\Delta$ represent the interconnect topology and time increments
respectively, on the set $P\times T$ of processors iterated over time steps. We
model schedules as symmetry-preserving equivariant maps between the set $X$ and
a subgroup of its symmetry and the set $P\times T$ with the symmetry
$N\times\Delta$. Such equivariant maps are the solutions of a set of algebraic
equations involving group homomorphisms. We associate time and communication
costs with the solutions to these equations.
  We solve these equations for the classical matrix multiplication algorithm
and show that equivariant maps correspond to time- and communication-efficient
schedules for many topologies. We recover well known variants including the
Cannon's algorithm and the communication-avoiding &quot;2.5D&quot; algorithm for toroidal
interconnects, systolic computation for planar hexagonal VLSI arrays, recursive
algorithms for fat-trees, the cache-oblivious algorithm for the ideal cache
model, and the space-bounded schedule for the parallel memory hierarchy model.
This suggests that the design of a schedule for a new class of machines can be
motivated by solutions to algebraic equations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02742</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02742</id><created>2015-12-08</created><updated>2016-02-12</updated><authors><author><keyname>Baez</keyname><forenames>John C.</forenames></author><author><keyname>Pollard</keyname><forenames>Blake S.</forenames></author></authors><title>Relative Entropy in Biological Systems</title><categories>cs.IT math.IT math.PR q-bio.QM</categories><comments>20 pages</comments><journal-ref>Entropy 18(2) (2016), 46</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we review various information-theoretic characterizations of
the approach to equilibrium in biological systems. The replicator equation,
evolutionary game theory, Markov processes and chemical reaction networks all
describe the dynamics of a population or probability distribution. Under
suitable assumptions, the distribution will approach an equilibrium with the
passage of time. Relative entropy - that is, the Kullback--Leibler divergence,
or various generalizations of this - provides a quantitative measure of how far
from equilibrium the system is. We explain various theorems that give
conditions under which relative entropy is nonincreasing. In biochemical
applications these results can be seen as versions of the Second Law of
Thermodynamics, stating that free energy can never increase with the passage of
time. In ecological applications, they make precise the notion that a
population gains information from its environment as it approaches equilibrium.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02743</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02743</id><created>2015-12-08</created><authors><author><keyname>Itoh</keyname><forenames>Yuki</forenames></author><author><keyname>Duarte</keyname><forenames>Marco F.</forenames></author><author><keyname>Parente</keyname><forenames>Mario</forenames></author></authors><title>Perfect Recovery Conditions For Non-Negative Sparse Modeling</title><categories>cs.IT cs.LG math.IT</categories><comments>10 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse modeling has been widely and successfully used in many applications
such as computer vision, machine learning, and pattern recognition and,
accompanied with those applications, significant research has studied the
theoretical limits and algorithm design for convex relaxations in sparse
modeling. However, only little has been done for theoretical limits of
non-negative versions of sparse modeling. The behavior is expected to be
similar as the general sparse modeling, but a precise analysis has not been
explored. This paper studies the performance of non-negative sparse modeling,
especially for non-negativity constrained and $\ell_1$-penalized least squares,
and gives an exact bound for which this problem can recover the correct signal
elements. We pose two conditions to guarantee the correct signal recovery:
minimum coefficient condition (MCC) and non-linearity vs. subset coherence
condition (NSCC). The former defines the minimum weight for each of the correct
atoms present in the signal and the latter defines the tolerable deviation from
the linear model relative to the positive subset coherence (PSC), a novel type
of &quot;coherence&quot; metric. We provide rigorous performance guarantees based on
these conditions and experimentally verify their precise predictive power in a
hyperspectral data unmixing application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02752</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02752</id><created>2015-12-08</created><updated>2016-01-17</updated><authors><author><keyname>Mao</keyname><forenames>Qi</forenames></author><author><keyname>Wang</keyname><forenames>Li</forenames></author><author><keyname>Tsang</keyname><forenames>Ivor W.</forenames></author><author><keyname>Sun</keyname><forenames>Yijun</forenames></author></authors><title>A Novel Regularized Principal Graph Learning Framework on Explicit Graph
  Representation</title><categories>cs.AI cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many scientific datasets are of high dimension, and the analysis usually
requires visual manipulation by retaining the most important structures of
data. Principal curve is a widely used approach for this purpose. However, many
existing methods work only for data with structures that are not
self-intersected, which is quite restrictive for real applications. A few
methods can overcome the above problem, but they either require complicated
human-made rules for a specific task with lack of convergence guarantee and
adaption flexibility to different tasks, or cannot obtain explicit structures
of data. To address these issues, we develop a new regularized principal graph
learning framework that captures the local information of the underlying graph
structure based on reversed graph embedding. As showcases, models that can
learn a spanning tree or a weighted undirected $\ell_1$ graph are proposed, and
a new learning algorithm is developed that learns a set of principal points and
a graph structure from data, simultaneously. The new algorithm is simple with
guaranteed convergence. We then extend the proposed framework to deal with
large-scale data. Experimental results on various synthetic and six real world
datasets show that the proposed method compares favorably with baselines and
can uncover the underlying structure correctly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02754</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02754</id><created>2015-12-09</created><authors><author><keyname>Xu</keyname><forenames>Jie</forenames></author><author><keyname>Duan</keyname><forenames>Lingjie</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author></authors><title>Proactive Eavesdropping via Cognitive Jamming in Fading Channels</title><categories>cs.IT math.IT</categories><comments>Submitted for possible journal publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To enhance the national security, there is a growing need for government
agencies to legitimately monitor suspicious communication links for preventing
intended crimes and terror attacks. In this paper, we propose a new wireless
information surveillance paradigm by investigating a scenario where a
legitimate monitor aims to intercept a suspicious wireless communication link
over fading channels. The legitimate monitor can successfully eavesdrop
(decode) the information of the suspicious link at each fading state only when
its achievable data rate is no smaller than that at the suspicious receiver. In
practice, such legitimate eavesdropping is particularly challenging, since the
legitimate monitor may be far away from the suspicious transmitter and cannot
eavesdrop efficiently. To overcome this issue, we propose a new approach,
namely proactive eavesdropping via cognitive jamming, in which the legitimate
monitor purposely jams the receiver so as to change the suspicious
communication (e.g., to a smaller data rate) for overhearing more efficiently.
In particular, we consider delay-sensitive and delay-tolerant applications for
the suspicious data communications, under which the legitimate monitor
maximizes the eavesdropping non-outage probability for event-based monitoring
and the relative eavesdropping rate for content analysis, respectively, by
optimizing its jamming power allocation over different fading states subject to
an average power constraint. Numerical results show that the proposed proactive
eavesdropping via cognitive jamming approach greatly outperforms the
conventional passive eavesdropping without jamming and the proactive
eavesdropping with constant-power jamming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02756</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02756</id><created>2015-12-09</created><authors><author><keyname>Su</keyname><forenames>Zibin</forenames></author><author><keyname>Yuan</keyname><forenames>Jing</forenames></author></authors><title>Robustness enhancement of cloud computing network based on coupled
  networks model</title><categories>cs.CR</categories><comments>6 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As a novel technology, cloud computing attracts more and more people
including technology enthusiasts and malicious users. Different from the
classical network architecture, cloud environment has many its own features
which make the traditional defense mechanism invalid. To make the network more
robust against a malicious attack, we introduce a new method to mitigate this
risk efficiently and systematically. In this paper, we first propose a coupled
networks model which adequately considers the interactions between physical
layer and virtual layer in a practical cloud computing environment. Based on
this new model and our systematical method, we show that with the addition of
protection of some specific nodes in the network structure, the robustness of
cloud computing's network can be significantly improved whereas their
functionality remains unchanged. Our results demonstrate that our new method
can effectively settle the hard problems which cloud computing now is facing
without much cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02758</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02758</id><created>2015-12-09</created><authors><author><keyname>Bostanci</keyname><forenames>Erkan</forenames></author></authors><title>Motion model transitions in GPS-IMU sensor fusion for user tracking in
  augmented reality</title><categories>cs.OH</categories><comments>8 pages, journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Finding the position of the user is an important processing step for
augmented reality (AR) applications. This paper investigates the use of
different motion models in order to choose the most suitable one, and
eventually reduce the Kalman filter errors in sensor fusion for such
applications where the accuracy of user tracking is crucial. A Deterministic
Finite Automaton (DFA) was employed using the innovation parameters of the
filter. Results show that the approach presented here reduces the filter error
compared to a static model and prevents filter divergence. The approach was
tested on a simple AR game in order to justify the accuracy and performance of
the algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02759</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02759</id><created>2015-12-09</created><authors><author><keyname>Dobi</keyname><forenames>Sonila</forenames></author><author><keyname>Gleirscher</keyname><forenames>Mario</forenames></author><author><keyname>Spichkova</keyname><forenames>Maria</forenames></author><author><keyname>Struss</keyname><forenames>Peter</forenames></author></authors><title>Model-based Hazard and Impact Analysis</title><categories>cs.SE</categories><report-no>TUM I1333</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hazard and impact analysis is an indispensable task during the specification
and development of safety-critical technical systems, and particularly of their
software-intensive control parts. There is a lack of methods supporting an
effective (reusable, automated) and integrated (cross-disciplinary) way to
carry out such analyses.
  This report was motivated by an industrial project whose goal was to survey
and propose methods and models for documentation and analysis of a system and
its environment to support hazard and impact analysis as an important task of
safety engineering and system development. We present and investigate three
perspectives of how to properly encode safety-relevant domain knowledge for
better reuse and automation, identify and assess all relevant hazards, as well
as pre-process this information and make it easily accessible for reuse in
other safety and systems engineering activities and, moreover, in similar
engineering projects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02766</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02766</id><created>2015-12-09</created><authors><author><keyname>Bostanci</keyname><forenames>Erkan</forenames></author><author><keyname>Bostanci</keyname><forenames>Betul</forenames></author><author><keyname>Kanwal</keyname><forenames>Nadia</forenames></author><author><keyname>Clark</keyname><forenames>Adrian F.</forenames></author></authors><title>Sensor Fusion of Camera, GPS and IMU using Fuzzy Adaptive Multiple
  Motion Models</title><categories>cs.RO cs.CV</categories><comments>14 pages, journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A tracking system that will be used for Augmented Reality (AR) applications
has two main requirements: accuracy and frame rate. The first requirement is
related to the performance of the pose estimation algorithm and how accurately
the tracking system can find the position and orientation of the user in the
environment. Accuracy problems of current tracking devices, considering that
they are low-cost devices, cause static errors during this motion estimation
process. The second requirement is related to dynamic errors (the end-to-end
system delay; occurring because of the delay in estimating the motion of the
user and displaying images based on this estimate. This paper investigates
combining the vision-based estimates with measurements from other sensors, GPS
and IMU, in order to improve the tracking accuracy in outdoor environments. The
idea of using Fuzzy Adaptive Multiple Models (FAMM) was investigated using a
novel fuzzy rule-based approach to decide on the model that results in improved
accuracy and faster convergence for the fusion filter. Results show that the
developed tracking system is more accurate than a conventional GPS-IMU fusion
approach due to additional estimates from a camera and fuzzy motion models. The
paper also presents an application in cultural heritage context.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02767</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02767</id><created>2015-12-09</created><authors><author><keyname>Maire</keyname><forenames>Michael</forenames></author><author><keyname>Narihira</keyname><forenames>Takuya</forenames></author><author><keyname>Yu</keyname><forenames>Stella X.</forenames></author></authors><title>Affinity CNN: Learning Pixel-Centric Pairwise Relations for
  Figure/Ground Embedding</title><categories>cs.CV cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spectral embedding provides a framework for solving perceptual organization
problems, including image segmentation and figure/ground organization. From an
affinity matrix describing pairwise relationships between pixels, it clusters
pixels into regions, and, using a complex-valued extension, orders pixels
according to layer. We train a convolutional neural network (CNN) to directly
predict the pairwise relationships that define this affinity matrix. Spectral
embedding then resolves these predictions into a globally-consistent
segmentation and figure/ground organization of the scene. Experiments
demonstrate significant benefit to this direct coupling compared to prior works
which use explicit intermediate stages, such as edge detection, on the pathway
from image to affinities. Our results suggest spectral embedding as a powerful
alternative to the conditional random field (CRF)-based globalization schemes
typically coupled to deep neural networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02769</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02769</id><created>2015-12-09</created><authors><author><keyname>Difrawi</keyname><forenames>Samouriq</forenames></author></authors><title>Scheduling on Grid with communication Delay</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Parallel processing, the core of High Performance Computing (HPC), was and
still the most effective way in improving the speed of computer systems. For
the past few years, the substantial developments in the computing power of
processors and the network speed have strikingly changed the landscape of HPC.
Geography distributed heterogeneous systems can now cooperate and share
resources to execute one application. This computing infrastructure is known as
computational Grid or Grid Computing. Grid can be viewed as a distributed
large-scale cluster computing. From other perspective, it constitutes the major
part of Cloud Computing Systems in addition to thin clients and utility
computing [1,2, 3]. Hence, Grid computing has attracted many researchers [4].
The interest in Grid computing has gone beyond the paradigm of traditional Grid
computing to a Wireless Grid computing [5,6].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02782</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02782</id><created>2015-12-09</created><updated>2015-12-10</updated><authors><author><keyname>Tajima</keyname><forenames>Masato</forenames></author></authors><title>Algebraic Construction of Tail-Biting Trellises for Linear Block Codes</title><categories>cs.IT math.IT</categories><comments>13 pages, 10 figures, submitted to the IEEE Transactions on
  Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present an algebraic construction of tail-biting trellises.
The proposed method is based on the state space expressions, i.e., the state
space is the image of the set of information sequences under the associated
state matrix. Then combining with the homomorphism theorem, an algebraic
trellis construction is obtained. We show that a tail-biting trellis
constructed using the proposed method is isomorphic to the associated
Koetter-Vardy (KV) trellis and tail-biting Bahl-Cocke-Jelinek-Raviv (BCJR)
trellis. We also evaluate the complexity of the obtained tail-biting trellises.
On the other hand, a matrix consisting of linearly independent rows of the
characteristic matrix is regarded as a generalization of minimal-span generator
matrices. Then we show that a KV trellis is constructed based on an extended
minimal-span generator matrix. It is shown that this construction is a natural
extension of the method proposed by McEliece (1996).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02791</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02791</id><created>2015-12-09</created><authors><author><keyname>Bernard</keyname><forenames>Sophie</forenames><affiliation>MARELLE</affiliation></author><author><keyname>Bertot</keyname><forenames>Yves</forenames><affiliation>MARELLE</affiliation></author><author><keyname>Rideau</keyname><forenames>Laurence</forenames><affiliation>MARELLE</affiliation></author><author><keyname>Strub</keyname><forenames>Pierre-Yves</forenames></author></authors><title>Formal Proofs of Transcendence for e and $\pi$ as an Application of
  Multivariate and Symmetric Polynomials</title><categories>cs.LO</categories><comments>in Jeremy Avigad and Adam Chlipala. Certified Programs and Proofs,
  Jan 2016, St Petersburg, Florida, United States. ACM Press, pp.12, 2016</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe the formalisation in Coq of a proof that the numbers e and $\pi$
are transcendental. This proof lies at the interface of two domains of
mathematics that are often considered separately: calculus (real and elementary
complex analysis) and algebra. For the work on calculus, we rely on the
Coquelicot library and for the work on algebra, we rely on the Mathematical
Components library. Moreover, some of the elements of our formalized proof
originate in the more ancient library for real numbers included in the Coq
distribution. The case of $\pi$ relies extensively on properties of
multivariate polynomials and this experiment was also an occasion to put to
test a newly developed library for these multivariate polynomials.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02794</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02794</id><created>2015-12-09</created><authors><author><keyname>Althoff</keyname><forenames>Matthias</forenames></author></authors><title>On Computing the Minkowski Difference of Zonotopes</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Zonotopes are becoming an increasingly popular set representation for formal
verification techniques. This is mainly due to their efficient representation
and the favorable computational complexity of important operations in
high-dimensional spaces. In particular, zonotopes are closed under Minkowski
addition and linear maps, which can be very efficiently implemented. Besides
those operations, zonotopes are also closed under Minkowski difference.
However, to the best knowledge of the author, no algorithm for computing the
Minkowski difference of zonotopes has been published so far. This paper
presents a detailed description on how to compute the Minkowski difference of
zonotopes. The efficiency of the proposed solution is demonstrated by numerical
experiments, which show that the computation time is significantly reduced
compared to computing the Minkowski difference of the corresponding halfspace
representation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02797</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02797</id><created>2015-12-09</created><authors><author><keyname>Heam</keyname><forenames>Pierre-Cyrille</forenames><affiliation>CASSIS</affiliation></author><author><keyname>Joly</keyname><forenames>Jean-Luc</forenames><affiliation>CASSIS</affiliation></author></authors><title>On the Uniform Random Generation of Non Deterministic Automata Up to
  Isomorphism</title><categories>cs.FL</categories><comments>Frank Drewes. CIAA 2015, Aug 2015, Umea, Sweden. Springer, 9223,
  pp.12, 2015, Implementation and Application of Automata - 20th International
  Conference</comments><proxy>ccsd</proxy><doi>10.1007/978-3-319-22360-5_12</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we address the problem of the uniform random generation of non
deterministic automata (NFA) up to isomorphism. First, we show how to use a
Monte-Carlo approach to uniformly sample a NFA. Secondly, we show how to use
the Metropolis-Hastings Algorithm to uniformly generate NFAs up to isomorphism.
Using labeling techniques, we show that in practice it is possible to move into
the modified Markov Chain efficiently, allowing the random generation of NFAs
up to isomorphism with dozens of states. This general approach is also applied
to several interesting subclasses of NFAs (up to isomorphism), such as NFAs
having a unique initial states and a bounded output degree. Finally, we prove
that for these interesting subclasses of NFAs, moving into the Metropolis
Markov chain can be done in polynomial time. Promising experimental results
constitute a practical contribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02802</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02802</id><created>2015-12-09</created><authors><author><keyname>Sadowski</keyname><forenames>Przemys&#x142;aw</forenames></author><author><keyname>Miszczak</keyname><forenames>Jaros&#x142;aw Adam</forenames></author><author><keyname>Ostaszewski</keyname><forenames>Mateusz</forenames></author></authors><title>Lively quantum walks on cycles</title><categories>quant-ph cs.ET cs.NI</categories><comments>12 pages, 3 figures</comments><msc-class>81P45, 94A05, 05C81</msc-class><acm-class>C.2.1; I.6.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a family of quantum walks on cycles parametrized by their
liveliness, defined as the ability to execute a long-range move. We investigate
the behavior of the probability distribution and time-averaged probability
distribution. We show that the liveliness parameter has a direct impact on the
periodicity of the limiting distribution. We also show that the introduced
model provides a simple recipe for improving the efficiency of the network
exploration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02819</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02819</id><created>2015-12-09</created><authors><author><keyname>Ferrett</keyname><forenames>Terry</forenames></author><author><keyname>Valenti</keyname><forenames>Matthew C.</forenames></author></authors><title>Reduced Complexity Detection for Network-Coded Slotted ALOHA using
  Sphere Decoding</title><categories>cs.IT math.IT</categories><comments>5 pages, submitted to Asilomar conference 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network-coded slotted ALOHA (NCSA) is a re- finement to the classic slotted
ALOHA protocol which im- proves throughput by enabling multiple source
transmissions per ALOHA slot using physical-layer network coding (PNC). The
receiver detects the network-coded combination of bits during every slot and
recovers information bits by solving a system of linear equations. This work
develops a receiver capable of detecting the network-coded combination of bits
during a slot considering an arbitrary number of sources, orthogonal
modulation, and a block fading channel. Maximum-likelihood detection of the
network-coded symbol at the receiver becomes complex as the number of sources
is increased. To reduce this complexity, sphere decoding is applied at the
receiver to limit the number of constellation symbols the receiver must
consider for detection. The system is simulated for two modulation orders and
two through five sources, and error-rate performance results are provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02831</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02831</id><created>2015-12-09</created><authors><author><keyname>Gieseke</keyname><forenames>Fabian</forenames></author><author><keyname>Oancea</keyname><forenames>Cosmin Eugen</forenames></author><author><keyname>Mahabal</keyname><forenames>Ashish</forenames></author><author><keyname>Igel</keyname><forenames>Christian</forenames></author><author><keyname>Heskes</keyname><forenames>Tom</forenames></author></authors><title>Bigger Buffer k-d Trees on Multi-Many-Core Systems</title><categories>cs.DC cs.DS cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A buffer k-d tree is a k-d tree variant for massively-parallel nearest
neighbor search. While providing valuable speed-ups on modern many-core devices
in case both a large number of reference and query points are given, buffer k-d
trees are limited by the amount of points that can fit on a single device. In
this work, we show how to modify the original data structure and the associated
workflow to make the overall approach capable of dealing with massive data
sets. We further provide a simple yet efficient way of using multiple devices
given in a single workstation. The applicability of the modified framework is
demonstrated in the context of astronomy, a field that is faced with huge
amounts of data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02832</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02832</id><created>2015-12-09</created><authors><author><keyname>Michail</keyname><forenames>Othon</forenames></author><author><keyname>Spirakis</keyname><forenames>Paul G.</forenames></author></authors><title>Connectivity Preserving Network Transformers</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Population Protocol model is a distributed model that concerns systems of
very weak computational entities that cannot control the way they interact. The
model of Network Constructors is a variant of Population Protocols capable of
(algorithmically) constructing abstract networks. Both models are characterized
by a fundamental inability to terminate. In this work, we investigate the
minimal strengthenings of the latter that could overcome this inability. Our
main conclusion is that initial connectivity of the communication topology
combined with the ability of the protocol to transform the communication
topology plus a few other local and realistic assumptions are sufficient to
guarantee not only termination but also the maximum computational power that
one can hope for in this family of models. The technique is to transform any
initial connected topology to a less symmetric and detectable topology without
ever breaking its connectivity during the transformation. The target topology
of all of our transformers is the spanning line and we call Terminating Line
Transformation the corresponding problem. We first study the case in which
there is a pre-elected unique leader and give a time-optimal protocol for
Terminating Line Transformation. We then prove that dropping the leader without
additional assumptions leads to a strong impossibility result. In an attempt to
overcome this, we equip the nodes with the ability to tell, during their
pairwise interactions, whether they have at least one neighbor in common.
Interestingly, it turns out that this local and realistic mechanism is
sufficient to make the problem solvable. In particular, we give a very
efficient protocol that solves Terminating Line Transformation when all nodes
are initially identical. The latter implies that the model computes with
termination any symmetric predicate computable by a Turing Machine of space
$\Theta(n^2)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02866</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02866</id><created>2015-12-09</created><authors><author><keyname>Rosenski</keyname><forenames>Jonathan</forenames></author><author><keyname>Shamir</keyname><forenames>Ohad</forenames></author><author><keyname>Szlak</keyname><forenames>Liran</forenames></author></authors><title>Multi-Player Bandits -- a Musical Chairs Approach</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a variant of the stochastic multi-armed bandit problem, where
multiple players simultaneously choose from the same set of arms and may
collide, receiving no reward. This setting has been motivated by problems
arising in cognitive radio networks, and is especially challenging under the
realistic assumption that communication between players is limited. We provide
a communication-free algorithm (Musical Chairs) which attains constant regret
with high probability, as well as a sublinear-regret, communication-free
algorithm (Dynamic Musical Chairs) for the more difficult setting of players
dynamically entering and leaving throughout the game. Moreover, both algorithms
do not require prior knowledge of the number of players. To the best of our
knowledge, these are the first communication-free algorithms with these types
of formal guarantees. We also rigorously compare our algorithms to previous
works, and complement our theoretical findings with experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02881</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02881</id><created>2015-12-08</created><authors><author><keyname>Krishnamoorthi</keyname><forenames>Shankarjee</forenames></author><author><keyname>Srivastava</keyname><forenames>Gaurav</forenames></author><author><keyname>Mandhyan</keyname><forenames>Amar</forenames></author></authors><title>Web application for size and topology optimization of trusses and gusset
  plates</title><categories>cs.OH</categories><comments>17 pages, 8 figures, submitted to Structural Engineering and
  Mechanics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With its ever growing popularity, providing Internet based applications tuned
towards practical applications is on the rise. Advantages such as no external
plugins and additional software, ease of use, updating and maintenance have
increased the popularity of web applications. In this work, a web-based
application has been developed which can perform size optimization of truss
structure as a whole as well as topology optimization of individual gusset
plate of each joint based on specified joint displacements and load conditions.
This application is developed using cutting-edge web technologies such as
Three.js and HTML5. The client side boasts of an intuitive interface which in
addition to its modeling capabilities also recommends configurations based on
user input, provides analysis options and finally displays the results. The
server side, using a combination of Scilab and DAKOTA, computes solution and
also provides the user with comparisons of the optimal design with that
conforming to Indian Standard (IS 800-2007). It is a freely available one-stop
web-based application to perform optimal and/or code based design of trusses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02888</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02888</id><created>2015-12-09</created><updated>2015-12-14</updated><authors><author><keyname>Viejo</keyname><forenames>Alexandre</forenames></author><author><keyname>S&#xe1;nchez</keyname><forenames>David</forenames></author></authors><title>Enforcing Transparent Access to Private Content in Social Networks by
  Means of Automatic Sanitization</title><categories>cs.CR cs.SI</categories><comments>Removed by arXiv administrators because submission violated the terms
  of arXiv's license agreement</comments><journal-ref>Expert Systems with Applications 42(23): 9366-9378 (2015)</journal-ref><doi>10.1016/j.eswa.2015.08.014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social networks have become an essential meeting point for millions of
individuals willing to publish and consume huge quantities of heterogeneous
information. Some studies have shown that the data published in these platforms
may contain sensitive personal information and that external entities can
gather and exploit this knowledge for their own benefit. Even though some
methods to preserve the privacy of social networks users have been proposed,
they generally apply rigid access control measures to the protected content
and, even worse, they do not enable the users to understand which contents are
sensitive. Last but not least, most of them require the collaboration of social
network operators or they fail to provide a practical solution capable of
working with well-known and already deployed social platforms (e.g., Twitter).
In this paper, we propose a new scheme that addresses all these issues. The new
system is envisaged as an independent piece of software that does not depend on
the social network in use and that can be transparently applied to most
existing ones. According to a set of privacy requirements intuitively defined
by the users of a social network, the proposed scheme is able to: (i)
automatically detect sensitive data in users' publications; (ii) construct
sanitized versions of such data; and (iii) provide privacy-preserving
transparent access to sensitive contents by disclosing more or less information
to readers according to their credentials toward the owner of the publications.
We also study the applicability of the proposed system in general and
illustrate its behavior in two well-known social networks (i.e., Twitter and
PatientsLikeMe).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02895</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02895</id><created>2015-12-09</created><authors><author><keyname>Zhang</keyname><forenames>Xiaofan</forenames></author><author><keyname>Zhou</keyname><forenames>Feng</forenames></author><author><keyname>Lin</keyname><forenames>Yuanqing</forenames></author><author><keyname>Zhang</keyname><forenames>Shaoting</forenames></author></authors><title>Embedding Label Structures for Fine-Grained Feature Representation</title><categories>cs.CV</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Recent algorithms in convolutional neural networks (CNN) considerably advance
the fine-grained image classification, which aims to differentiate subtle
differences among subordinate classes. However, previous studies have rarely
focused on learning a fined-grained and structured feature representation that
is able to locate similar images at different levels of relevance, e.g.,
discovering cars from the same make or the same model, both of which require
high precision. In this paper, we propose two main contributions to tackle this
problem. 1) A multi-task learning framework is designed to effectively learn
fine-grained feature representations by jointly optimizing both classification
and similarity constraints. 2) To model the multi-level relevance, label
structures such as hierarchy or shared attributes are seamlessly embedded into
the framework by generalizing the triplet loss. Extensive and thorough
experiments have been conducted on three fine-grained datasets, i.e., the
Stanford car, the car-333, and the food datasets, which contain either
hierarchical labels or shared attributes. Our proposed method has achieved very
competitive performance, i.e., among state-of-the-art classification accuracy.
More importantly, it significantly outperforms previous fine-grained feature
representations for image retrieval at different levels of relevance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02896</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02896</id><created>2015-12-09</created><authors><author><keyname>Naini</keyname><forenames>Farid M.</forenames></author><author><keyname>Unnikrishnan</keyname><forenames>Jayakrishnan</forenames></author><author><keyname>Thiran</keyname><forenames>Patrick</forenames></author><author><keyname>Vetterli</keyname><forenames>Martin</forenames></author></authors><title>Where You Are Is Who You Are: User Identification by Matching Statistics</title><categories>cs.LG cs.CR cs.SI stat.AP stat.ML</categories><doi>10.1109/TIFS.2015.2498131</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most users of online services have unique behavioral or usage patterns. These
behavioral patterns can be exploited to identify and track users by using only
the observed patterns in the behavior. We study the task of identifying users
from statistics of their behavioral patterns. Specifically, we focus on the
setting in which we are given histograms of users' data collected during two
different experiments. We assume that, in the first dataset, the users'
identities are anonymized or hidden and that, in the second dataset, their
identities are known. We study the task of identifying the users by matching
the histograms of their data in the first dataset with the histograms from the
second dataset. In recent works, the optimal algorithm for this user
identification task is introduced. In this paper, we evaluate the effectiveness
of this method on three different types of datasets and in multiple scenarios.
Using datasets such as call data records, web browsing histories, and GPS
trajectories, we show that a large fraction of users can be easily identified
given only histograms of their data; hence these histograms can act as users'
fingerprints. We also verify that simultaneous identification of users achieves
better performance compared to one-by-one user identification. We show that
using the optimal method for identification gives higher identification
accuracy than heuristics-based approaches in practical scenarios. The accuracy
obtained under this optimal method can thus be used to quantify the maximum
level of user identification that is possible in such settings. We show that
the key factors affecting the accuracy of the optimal identification algorithm
are the duration of the data collection, the number of users in the anonymized
dataset, and the resolution of the dataset. We analyze the effectiveness of
k-anonymization in resisting user identification attacks on these datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02897</identifier>
 <datestamp>2015-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02897</id><created>2015-12-09</created><updated>2015-12-16</updated><authors><author><keyname>S&#xe1;nchez</keyname><forenames>David</forenames></author><author><keyname>Domingo-Ferrer</keyname><forenames>Josep</forenames></author><author><keyname>Mart&#xed;nez</keyname><forenames>Sergio</forenames></author><author><keyname>Soria-Comas</keyname><forenames>Jordi</forenames></author></authors><title>Utility-Preserving Differentially Private Data Releases Via Individual
  Ranking Microaggregation</title><categories>cs.CR</categories><journal-ref>Information Fusion 30:1-14 (2016)</journal-ref><doi>10.1016/j.inffus.2015.11.002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Being able to release and exploit open data gathered in information systems
is crucial for researchers, enterprises and the overall society. Yet, these
data must be anonymized before release to protect the privacy of the subjects
to whom the records relate. Differential privacy is a privacy model for
anonymization that offers more robust privacy guarantees than previous models,
such as $k$-anonymity and its extensions. However, it is often disregarded that
the utility of differentially private outputs is quite limited, either because
of the amount of noise that needs to be added to obtain them or because utility
is only preserved for a restricted type and/or a limited number of queries. On
the contrary, $k$-anonymity-like data releases make no assumptions on the uses
of the protected data and, thus, do not restrict the number and type of doable
analyses. Recently, some authors have proposed mechanisms to offer
general-purpose differentially private data releases. This paper extends such
works with a specific focus on the preservation of the utility of the protected
data. Our proposal builds on microaggregation-based anonymization, which is
more flexible and utility-preserving than alternative anonymization methods
used in the literature, in order to reduce the amount of noise needed to
satisfy differential privacy. In this way, we improve the utility of
differentially private data releases. Moreover, the noise reduction we achieve
does not depend on the size of the data set, but just on the number of
attributes to be protected, which is a more desirable behavior for large data
sets. The utility benefits brought by our proposal are empirically evaluated
and compared with related works for several data sets and metrics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02898</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02898</id><created>2015-12-09</created><authors><author><keyname>De Nart</keyname><forenames>Dario</forenames></author><author><keyname>Degl'Innocenti</keyname><forenames>Dante</forenames></author><author><keyname>Peressotti</keyname><forenames>Marco</forenames></author></authors><title>Well-Stratified Linked Data for Well-Behaved Data Citation</title><categories>cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we analyse the functional requirements of linked data citation
and identify a minimal set of operations and primitives needed to realize such
task. Citing linked data implies solving a series of data provenance issues and
finding a way to identify data subsets. Those two tasks can be handled defining
a simple type system inside data and verifying it with a type checker, which is
significantly less complex than interpreting reified RDF statements and can be
implemented in a non data invasive way. Finally we suggest that data citation
should be handled outside of the data, possibly with an ad-hoc language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02902</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02902</id><created>2015-12-09</created><authors><author><keyname>Tapaswi</keyname><forenames>Makarand</forenames></author><author><keyname>Zhu</keyname><forenames>Yukun</forenames></author><author><keyname>Stiefelhagen</keyname><forenames>Rainer</forenames></author><author><keyname>Torralba</keyname><forenames>Antonio</forenames></author><author><keyname>Urtasun</keyname><forenames>Raquel</forenames></author><author><keyname>Fidler</keyname><forenames>Sanja</forenames></author></authors><title>MovieQA: Understanding Stories in Movies through Question-Answering</title><categories>cs.CV cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the MovieQA dataset which aims to evaluate automatic story
comprehension from both video and text. The dataset consists of 7702 questions
about 294 movies with high semantic diversity. The questions range from simpler
&quot;Who&quot; did &quot;What&quot; to &quot;Whom&quot;, to &quot;Why&quot; and &quot;How&quot; certain events occurred. Each
question comes with a set of five possible answers; a correct one and four
deceiving answers provided by human annotators. Our dataset is unique in that
it contains multiple sources of information -- full-length movies, plots,
subtitles, scripts and for a subset DVS. We analyze our data through various
statistics and intelligent baselines. We further extend existing QA techniques
to show that question-answering with such open-ended semantics is hard. We plan
to create a benchmark with an active leader board, to encourage inspiring work
in this challenging domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02909</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02909</id><created>2015-12-09</created><authors><author><keyname>Soria-Comas</keyname><forenames>Jordi</forenames></author><author><keyname>Domingo-Ferrer</keyname><forenames>Josep</forenames></author><author><keyname>S&#xe1;nchez</keyname><forenames>David</forenames></author><author><keyname>Mart&#xed;nez</keyname><forenames>Sergio</forenames></author></authors><title>t-Closeness through Microaggregation: Strict Privacy with Enhanced
  Utility Preservation</title><categories>cs.CR</categories><journal-ref>IEEE Transactions on Knowledge &amp; Data Engineering 27(11):
  3098-3110 (2015)</journal-ref><doi>10.1109/TKDE.2015.2435777</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Microaggregation is a technique for disclosure limitation aimed at protecting
the privacy of data subjects in microdata releases. It has been used as an
alternative to generalization and suppression to generate $k$-anonymous data
sets, where the identity of each subject is hidden within a group of $k$
subjects. Unlike generalization, microaggregation perturbs the data and this
additional masking freedom allows improving data utility in several ways, such
as increasing data granularity, reducing the impact of outliers and avoiding
discretization of numerical data. $k$-Anonymity, on the other side, does not
protect against attribute disclosure, which occurs if the variability of the
confidential values in a group of $k$ subjects is too small. To address this
issue, several refinements of $k$-anonymity have been proposed, among which
$t$-closeness stands out as providing one of the strictest privacy guarantees.
Existing algorithms to generate $t$-close data sets are based on generalization
and suppression (they are extensions of $k$-anonymization algorithms based on
the same principles). This paper proposes and shows how to use microaggregation
to generate $k$-anonymous $t$-close data sets. The advantages of
microaggregation are analyzed, and then several microaggregation algorithms for
$k$-anonymous $t$-closeness are presented and empirically evaluated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02910</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02910</id><created>2015-12-09</created><authors><author><keyname>Prados-Garzon</keyname><forenames>Jonathan</forenames></author><author><keyname>Ramos-Munoz</keyname><forenames>Juan J.</forenames></author><author><keyname>Ameigeiras</keyname><forenames>Pablo</forenames></author><author><keyname>Andres-Maldonado</keyname><forenames>Pilar</forenames></author><author><keyname>Soler</keyname><forenames>Juan M. Lopez</forenames></author></authors><title>Latency Evaluation of a Virtualized MME</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network Virtualization is one of the key technologies for developing the
future mobile networks. However, the performance of virtual mobile entities may
not be sufficient for delivering the service required for future networks in
terms of throughput or service time. In addition, to take advantage of the
virtualization capabilities, a criterion to decide when to scale out the number
of instances is a must.
  In this paper we propose an LTE virtualized Mobility Management Entity queue
model to evaluate its service time for a given signaling workload. The
estimation of this latency can serve to decide how many processing instances
should be deployed to provide a target service. Additionally, we provide a
compound data traffic model for the future mobile applications, and we predict
theoretically the control workload that it will generate. Finally, we evaluate
the virtualized Mobility Management Entity overall delay by simulation,
providing insights for selecting the number of virtual instances for a given
number of users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02914</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02914</id><created>2015-12-09</created><authors><author><keyname>Marcum</keyname><forenames>Christopher Steven</forenames></author></authors><title>Yet Another Statistical Analysis of Bob Ross Paintings</title><categories>stat.AP cs.CV</categories><comments>This version based off of arXiv compliant pdflatex source (which
  result in lossy data). Original R-code, Sweave source, and data files are
  available upon request</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we analyze a sample of clippings from paintings by the late
artist Bob Ross. Previous work focused on the qualitative themes of his
paintings (Hickey, 2014); here, we expand on that line of research by
considering the colorspace and luminosity values as our data. Our results
demonstrate the subtle aesthetics of the average Ross painting, the common
variation shared by his paintings, and the structure of the relationships
between each painting in our sample. We reveal, for the first time, renderings
of the average paintings and introduce &quot;eigenross&quot; components to identify and
evaluate shared variance. Additionally, all data and code are embedded in this
document to encourage future research, and, in the spirit of Bob Ross, to teach
others how to do so.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02918</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02918</id><created>2015-12-09</created><updated>2015-12-12</updated><authors><author><keyname>Gao</keyname><forenames>Zhen</forenames></author><author><keyname>Dai</keyname><forenames>Linglong</forenames></author><author><keyname>Dai</keyname><forenames>Wei</forenames></author><author><keyname>Shim</keyname><forenames>Byonghyo</forenames></author><author><keyname>Wang</keyname><forenames>Zhaocheng</forenames></author></authors><title>Structured Compressive Sensing Based Spatio-Temporal Joint Channel
  Estimation for FDD Massive MIMO</title><categories>cs.IT math.IT</categories><comments>16 pages; 12 figures;submitted to IEEE Trans. Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Massive MIMO is a promising technique for future 5G communications due to its
high spectrum and energy efficiency. To realize its potential performance gain,
accurate channel estimation is essential. However, due to massive number of
antennas at the base station (BS), the pilot overhead required by conventional
channel estimation schemes will be unaffordable, especially for frequency
division duplex (FDD) massive MIMO. To overcome this problem, we propose a
structured compressive sensing (SCS)-based spatio-temporal joint channel
estimation scheme to reduce the required pilot overhead, whereby the
spatio-temporal common sparsity of delay-domain MIMO channels is leveraged.
Particularly, we first propose the non-orthogonal pilots at the BS under the
framework of CS theory to reduce the required pilot overhead. Then, an adaptive
structured subspace pursuit (ASSP) algorithm at the user is proposed to jointly
estimate channels associated with multiple OFDM symbols from the limited number
of pilots, whereby the spatio-temporal common sparsity of MIMO channels is
exploited to improve the channel estimation accuracy. Moreover, by exploiting
the temporal channel correlation, we propose a space-time adaptive pilot scheme
to further reduce the pilot overhead. Additionally, we discuss the proposed
channel estimation scheme in multi-cell scenario. Simulation results
demonstrate that the proposed scheme can accurately estimate channels with the
reduced pilot overhead, and it is capable of approaching the optimal oracle
least squares estimator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02921</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02921</id><created>2015-12-09</created><authors><author><keyname>Sra</keyname><forenames>Misha</forenames></author><author><keyname>Schmandt</keyname><forenames>Chris</forenames></author></authors><title>Design Strategies for Playful Technologies to Support Light-intensity
  Physical Activity in the Workplace</title><categories>cs.HC</categories><comments>11 pages, 5 figures. Video:
  http://living.media.mit.edu/projects/see-saw/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Moderate to vigorous intensity physical activity has an established
preventative role in obesity, cardiovascular disease, and diabetes. However
recent evidence suggests that sitting time affects health negatively
independent of whether adults meet prescribed physical activity guidelines.
Since many of us spend long hours daily sitting in front of a host of
electronic screens, this is cause for concern. In this paper, we describe a set
of three prototype digital games created for encouraging light-intensity
physical activity during short breaks at work. The design of these kinds of
games is a complex process that must consider motivation strategies,
interaction methodology, usability and ludic aspects. We present design
guidelines for technologies that encourage physical activity in the workplace
that we derived from a user evaluation using the prototypes. Although the
design guidelines can be seen as general principles, we conclude that they have
to be considered differently for different workplace cultures and workspaces.
Our study was conducted with users who have some experience playing casual
games on their mobile devices and were able and willing to increase their
physical activity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02922</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02922</id><created>2015-12-09</created><authors><author><keyname>Sra</keyname><forenames>Misha</forenames></author><author><keyname>Schmandt</keyname><forenames>Chris</forenames></author></authors><title>MetaSpace II: Object and full-body tracking for interaction and
  navigation in social VR</title><categories>cs.HC</categories><comments>10 pages, 9 figures. Video:
  http://living.media.mit.edu/projects/metaspace-ii/</comments><acm-class>H.5.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  MetaSpace II (MS2) is a social Virtual Reality (VR) system where multiple
users can not only see and hear but also interact with each other, grasp and
manipulate objects, walk around in space, and get tactile feedback. MS2 allows
walking in physical space by tracking each user's skeleton in real-time and
allows users to feel by employing passive haptics i.e., when users touch or
manipulate an object in the virtual world, they simultaneously also touch or
manipulate a corresponding object in the physical world. To enable these
elements in VR, MS2 creates a correspondence in spatial layout and object
placement by building the virtual world on top of a 3D scan of the real world.
Through the association between the real and virtual world, users are able to
walk freely while wearing a head-mounted device, avoid obstacles like walls and
furniture, and interact with people and objects. Most current virtual reality
(VR) environments are designed for a single user experience where interactions
with virtual objects are mediated by hand-held input devices or hand gestures.
Additionally, users are only shown a representation of their hands in VR
floating in front of the camera as seen from a first person perspective. We
believe, representing each user as a full-body avatar that is controlled by
natural movements of the person in the real world (see Figure 1d), can greatly
enhance believability and a user's sense immersion in VR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02924</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02924</id><created>2015-12-09</created><updated>2016-02-11</updated><authors><author><keyname>Ozcan</keyname><forenames>Gozde</forenames></author><author><keyname>Gursoy</keyname><forenames>M. Cenk</forenames></author><author><keyname>Tran</keyname><forenames>Nghi</forenames></author><author><keyname>Tang</keyname><forenames>Jian</forenames></author></authors><title>Energy-Efficient Power Allocation in Cognitive Radio Systems with
  Imperfect Spectrum Sensing</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies energy-efficient power allocation schemes for secondary
users in sensing-based spectrum sharing cognitive radio systems. It is assumed
that secondary users first perform channel sensing possibly with errors and
then initiate data transmission with different power levels based on sensing
decisions. The circuit power is taken into account in total power consumption.
In this setting, the optimization problem is to maximize energy efficiency (EE)
subject to peak/average transmission power constraints and peak/average
interference constraints. By exploiting quasiconcave property of EE
maximization problem, the original problem is transformed into an equivalent
parameterized concave problem and an iterative power allocation algorithm based
on Dinkelbach's method is proposed. The optimal power levels are identified in
the presence of different levels of channel side information (CSI) regarding
the transmission and interference links at the secondary transmitter, namely
perfect CSI of both transmission and interference links, perfect CSI of the
transmission link and imperfect CSI of the interference link, imperfect CSI of
both links or only statistical CSI of both links. Through numerical results,
the impact of sensing performance, different types of CSI availability, and
transmit and interference power constraints on the EE of the secondary users is
analyzed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02930</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02930</id><created>2015-12-09</created><authors><author><keyname>Mostafa</keyname><forenames>Hesham</forenames></author><author><keyname>Indiveri</keyname><forenames>Giacomo</forenames></author></authors><title>Stochastic Interpretation of Quasi-periodic Event-based Systems</title><categories>cs.NE cs.ET q-bio.NC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many networks used in machine learning and as models of biological neural
networks make use of stochastic neurons or neuron-like units. We show that
stochastic artificial neurons can be realized on silicon chips by exploiting
the quasi-periodic behavior of mismatched analog oscillators to approximate the
neuron's stochastic activation function. We represent neurons by finite state
machines (FSMs) that communicate using digital events and whose transitions are
event-triggered. The event generation times of each neuron are controlled by an
analog oscillator internal to that neuron/FSM and the frequencies of the
oscillators in different FSMs are incommensurable. We show that within this
quasi-periodic system, the transition graph of a FSM can be interpreted as the
transition graph of a Markov chain and we show that by using different FSMs, we
can obtain approximations of different stochastic activation functions. We
investigate the quality of the stochastic interpretation of such a
deterministic system and we use the system to realize and sample from a
restricted Boltzmann machine. We implemented the quasi-periodic event-based
system on a custom silicon chip and we show that the chip behavior can be used
to closely approximate a stochastic sampling task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02945</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02945</id><created>2015-12-01</created><updated>2015-12-10</updated><authors><author><keyname>Talarico</keyname><forenames>Salvatore</forenames></author><author><keyname>Valenti</keyname><forenames>Matthew C.</forenames></author></authors><title>Frequency Hopping on a 5G Millimeter-Wave Uplink</title><categories>cs.IT math.IT</categories><comments>5 pages, 1 table and 4 figures, accepted to IEEE Asilomar Conf. on
  Signals, Sys, &amp; Computers (Asilomar), 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to overcome the anticipated tremendous growth in the volume of
mobile data traffic, the next generation of cellular networks will need to
exploit the large bandwidth offered by the millimeter-wave (mmWave) band. A key
distinguishing characteristic of mmWave is its use of highly directional and
steerable antennas. In addition, future networks will be highly densified
through the proliferation of base stations and their supporting infrastructure.
With the aim of further improving the overall throughput of the network by
mitigating the effect of frequency-selective fading and co-channel
interference, 5G cellular networks are also expected to aggressively use
frequency-hopping. This paper outlines an analytical framework that captures
the main characteristics of a 5G cellular uplink. This framework is used to
emphasize the benefits of network infrastructure densification, antenna
directivity, mmWave propagation characteristics, and frequency hopping.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02949</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02949</id><created>2015-12-09</created><authors><author><keyname>Shetty</keyname><forenames>Rakshith</forenames></author><author><keyname>Laaksonen</keyname><forenames>Jorma</forenames></author></authors><title>Video captioning with recurrent networks based on frame- and video-level
  features and visual content classification</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we describe the system for generating textual descriptions of
short video clips using recurrent neural networks (RNN), which we used while
participating in the Large Scale Movie Description Challenge 2015 in ICCV 2015.
Our work builds on static image captioning systems with RNN based language
models and extends this framework to videos utilizing both static image
features and video-specific features. In addition, we study the usefulness of
visual content classifiers as a source of additional information for caption
generation. With experimental results we show that utilizing keyframe based
features, dense trajectory video features and content classifier outputs
together gives better performance than any one of them individually.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02951</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02951</id><created>2015-12-09</created><authors><author><keyname>Memmi</keyname><forenames>Gerard</forenames></author><author><keyname>Kapusta</keyname><forenames>Katarzyna</forenames></author><author><keyname>Qiu</keyname><forenames>Han</forenames></author></authors><title>Data Protection: Combining Fragmentation, Encryption, and Dispersion, an
  Intermediary report</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hardening data protection using multiple methods rather than just encryption
is of paramount importance when considering continuous and powerful attacks to
spy or even destroy private and confidential information. Our purpose is to
look at cost effective data protection by way of combining fragmentation,
encryption, and then dispersion. This means to derive general schemes to
protect data everywhere throughout a network of machines where they are being
processed, transmitted, and stored during their entire life cycle. In this
report, we first present a general description of what should be a
fragmentation system including a number of high level requirements. Then, we
focus on fragmentation of two kinds of data. First, a bitmap image is split in
two fragments a public one and a private one. We describe two processes and
address the question of performance. Then, we analyze works first, on general
dispersion systems in a brute force manner; second on fragmentation of
information considering data stored in a structured database. We conclude that
combining data fragmentation, encryption, and dispersion constitutes a potent
process to protect data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02960</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02960</id><created>2015-12-09</created><authors><author><keyname>Kisil</keyname><forenames>Vladimir V.</forenames></author></authors><title>Ensembles of Cycles Programmed in GiNaC</title><categories>cs.CG cs.MS cs.SC math.DG</categories><comments>LaTeX 104pp, including 7 EPS graphic files and program code</comments><msc-class>51B25, 51N25, 68U05, 11E88, 68W30</msc-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This library manipulates ensembles of cycles (quadrics), which are
interrelated through certain geometric relations (to be orthogonal, to be
tangent, etc.). The code operates with numeric and symbolic data of cycles in
spaces of arbitrary dimensionality and metrics with any signatures. In the
two-dimensional case illustrations and animations can be produced.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02961</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02961</id><created>2015-12-09</created><authors><author><keyname>Gonz&#xe1;lez-Coma</keyname><forenames>Jos&#xe9; P.</forenames></author><author><keyname>Joham</keyname><forenames>Michael</forenames></author><author><keyname>Castro</keyname><forenames>Paula M.</forenames></author><author><keyname>Castedo</keyname><forenames>Luis</forenames></author></authors><title>QoS Constrained Power Minimization in the MISO Broadcast Channel with
  Imperfect CSI</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the design of linear precoders and receivers in a Multiple-Input
Single-Output (MISO) Broadcast Channel (BC). We aim at minimizing the transmit
power while fullfiling a set of per-user Quality-of-Service (QoS) constraints
expressed in terms of per-user average rate requirements. The Channel State
Information (CSI) is assumed to be perfectly known at the receivers but only
partially at the transmitter. To solve the problem we transform the QoS
constraints into Minimum Mean Square Error (MMSE) constraints. We then leverage
the MSE duality between the BC and the Multiple Access Channel (MAC), as well
as standard interference functions in the dual MAC, to perform power
minimization by means of an Alternating Optimization (AO) algorithm. Problem
feasibility is also studied to determine whether the QoS constraints can be
fulfilled or not. Finally, we present an algorithm to balance the average rates
and manage situations that may be unfeasible or lead to an unacceptably high
transmit power.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02963</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02963</id><created>2015-12-09</created><authors><author><keyname>Kozma</keyname><forenames>L&#xe1;szl&#xf3;</forenames></author><author><keyname>M&#xf6;mke</keyname><forenames>Tobias</forenames></author></authors><title>A PTAS for Euclidean Maximum Scatter TSP</title><categories>cs.DS cs.CG</categories><acm-class>F.2.2; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of finding a tour of $n$ points in $\mathbb{R}^d$ in
which every edge is long. More precisely, we wish to find a tour that maximizes
the length of the shortest edge in the tour. The problem is known as Maximum
Scatter TSP, and it was introduced by Arkin et al. (SODA 1997), motivated by
applications in manufacturing and medical imaging. Arkin et al. gave a
$2$-approximation for the metric version of the problem and showed that this is
the best possible ratio achievable in polynomial time (assuming $P \neq NP$).
They raised the question of whether one can obtain a better approximation ratio
in the planar Euclidean case. We answer this question in the affirmative in a
more general setting, by giving a polynomial-time approximation scheme (PTAS)
for Maximum Scatter TSP in an arbitrary fixed-dimensional Euclidean space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02968</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02968</id><created>2015-12-09</created><authors><author><keyname>Ranganath</keyname><forenames>Suhas</forenames></author><author><keyname>Morstatter</keyname><forenames>Fred</forenames></author><author><keyname>Hu</keyname><forenames>Xia</forenames></author><author><keyname>Tang</keyname><forenames>Jiliang</forenames></author><author><keyname>Liu</keyname><forenames>Huan</forenames></author></authors><title>Predicting Online Protest Participation of Social Media Users</title><categories>cs.SI</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Social media has emerged to be a popular platform for people to express their
viewpoints on political protests like the Arab Spring. Millions of people use
social media to communicate and mobilize their viewpoints on protests. Hence,
it is a valuable tool for organizing social movements. However, the mechanisms
by which protest affects the population is not known, making it difficult to
estimate the number of protestors. In this paper, we are inspired by
sociological theories of protest participation and propose a framework to
predict from the user's past status messages and interactions whether the next
post of the user will be a declaration of protest. Drawing concepts from these
theories, we model the interplay between the user's status messages and
messages interacting with him over time and predict whether the next post of
the user will be a declaration of protest. We evaluate the framework using data
from the social media platform Twitter on protests during the recent Nigerian
elections and demonstrate that it can effectively predict whether the next post
of a user is a declaration of protest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02970</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02970</id><created>2015-12-09</created><authors><author><keyname>De</keyname><forenames>Soham</forenames></author><author><keyname>Taylor</keyname><forenames>Gavin</forenames></author><author><keyname>Goldstein</keyname><forenames>Tom</forenames></author></authors><title>Scaling Up Distributed Stochastic Gradient Descent Using Variance
  Reduction</title><categories>cs.LG cs.DC math.OC stat.ML</categories><comments>preprint</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Variance reduction stochastic gradient descent methods enable minimization of
model fitting problems involving big datasets with low iteration complexity and
fast asymptotic convergence rates. However, they scale poorly in distributed
settings. In this paper, we propose a highly parallel variance reduction
method, CentralVR, with performance that scales linearly with the number of
worker nodes. We also propose distributed versions of popular variance
reduction methods that support a high degree of parallelization. Unlike
existing distributed stochastic gradient schemes, CentralVR exhibits linear
performance gains up to thousands of cores for massive datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02972</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02972</id><created>2015-12-09</created><authors><author><keyname>Ortiz</keyname><forenames>Jorge</forenames><affiliation>IBM Reserch</affiliation></author><author><keyname>Huang</keyname><forenames>Chien-Chin</forenames><affiliation>NYU Computer Science</affiliation></author><author><keyname>Chakraborty</keyname><forenames>Supriyo</forenames><affiliation>IBM Research</affiliation></author></authors><title>Get More With Less: Near Real-Time Image Clustering on Mobile Phones</title><categories>cs.CV cs.DC cs.PF</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Machine learning algorithms, in conjunction with user data, hold the promise
of revolutionizing the way we interact with our phones, and indeed their
widespread adoption in the design of apps bear testimony to this promise.
However, currently, the computationally expensive segments of the learning
pipeline, such as feature extraction and model training, are offloaded to the
cloud, resulting in an over-reliance on the network and under-utilization of
computing resources available on mobile platforms. In this paper, we show that
by combining the computing power distributed over a number of phones, judicious
optimization choices, and contextual information it is possible to execute the
end-to-end pipeline entirely on the phones at the edge of the network,
efficiently. We also show that by harnessing the power of this combination, it
is possible to execute a computationally expensive pipeline at near real-time.
  To demonstrate our approach, we implement an end-to-end image-processing
pipeline -- that includes feature extraction, vocabulary learning,
vectorization, and image clustering -- on a set of mobile phones. Our results
show a 75% improvement over the standard, full pipeline implementation running
on the phones without modification -- reducing the time to one minute under
certain conditions. We believe that this result is a promising indication that
fully distributed, infrastructure-less computing is possible on networks of
mobile phones; enabling a new class of mobile applications that are less
reliant on the cloud.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02977</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02977</id><created>2015-12-09</created><authors><author><keyname>Yildirim</keyname><forenames>Kasim Sinan</forenames></author></authors><title>Gradient Descent Algorithm Inspired Adaptive Time Synchronization in
  Wireless Sensor Networks</title><categories>cs.DC</categories><comments>Submitted for possible publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our motivation in this paper is to take another step forward from complex and
heavyweight synchronization protocols to the easy-to-implement and lightweight
synchronization protocols in WSNs. To this end, we present GraDeS, a novel
multi-hop time synchronization protocol based upon gradient descent algorithm.
We give details about our implementation of GraDeS and present its experimental
evaluation in our testbed of MICAz sensor nodes. Our observations indicate that
GraDeS is scalable, it has identical memory and processing overhead, better
convergence time and comparable synchronization performance as compared to
existing lightweight solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02985</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02985</id><created>2015-12-09</created><authors><author><keyname>Bandyapadhyay</keyname><forenames>Sayan</forenames></author><author><keyname>Varadarajan</keyname><forenames>Kasturi</forenames></author></authors><title>On Variants of k-means Clustering</title><categories>cs.CG cs.DS</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  \textit{Clustering problems} often arise in the fields like data mining,
machine learning etc. to group a collection of objects into similar groups with
respect to a similarity (or dissimilarity) measure. Among the clustering
problems, specifically \textit{$k$-means} clustering has got much attention
from the researchers. Despite the fact that $k$-means is a very well studied
problem its status in the plane is still an open problem. In particular, it is
unknown whether it admits a PTAS in the plane. The best known approximation
bound in polynomial time is $9+\eps$.
  In this paper, we consider the following variant of $k$-means. Given a set
$C$ of points in $\mathcal{R}^d$ and a real $f &gt; 0$, find a finite set $F$ of
points in $\mathcal{R}^d$ that minimizes the quantity $f*|F|+\sum_{p\in C}
\min_{q \in F} {||p-q||}^2$. For any fixed dimension $d$, we design a local
search PTAS for this problem. We also give a &quot;bi-criterion&quot; local search
algorithm for $k$-means which uses $(1+\eps)k$ centers and yields a solution
whose cost is at most $(1+\eps)$ times the cost of an optimal $k$-means
solution. The algorithm runs in polynomial time for any fixed dimension.
  The contribution of this paper is two fold. On the one hand, we are being
able to handle the square of distances in an elegant manner, which yields near
optimal approximation bound. This leads us towards a better understanding of
the $k$-means problem. On the other hand, our analysis of local search might
also be useful for other geometric problems. This is important considering that
very little is known about the local search method for geometric approximation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02989</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02989</id><created>2015-12-09</created><authors><author><keyname>Ewaisha</keyname><forenames>Ahmed</forenames></author><author><keyname>Tepedelenlioglu</keyname><forenames>Cihan</forenames></author></authors><title>Scheduling in Instantaneous-Interference-Limited CR Networks with Delay
  Guarantees</title><categories>cs.IT math.IT</categories><comments>arXiv admin note: substantial text overlap with arXiv:1410.7460</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study an uplink multi secondary user (SU) cognitive radio system having
average delay constraints as well as an instantaneous interference constraint
to the primary user (PU). If the interference channels from the SUs to the PU
have independent but not identically distributed fading coefficients, then the
SUs will experience heterogeneous delay performances. This is because SUs
causing low interference to the PU will be scheduled more frequently, and/or
allocated more transmission power than those causing high interference. We
propose a dynamic scheduling-and-power-control algorithm that can provide the
required average delay guarantees to all SUs as well as protecting the PU from
interference. Using the Lyapunov technique, we show that our algorithm is
asymptotically delay optimal while satisfying the delay and interference
constraints. We support our findings by extensive system simulations and show
the robustness of the proposed algorithm against channel estimation errors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02990</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02990</id><created>2015-12-09</created><authors><author><keyname>Bitar</keyname><forenames>Rawad</forenames></author><author><keyname>Rouayheb</keyname><forenames>Salim El</forenames></author></authors><title>Staircase Codes for Secret Sharing with Optimal Communication and Read
  Overheads</title><categories>cs.IT math.IT</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the communication efficient secret sharing (CESS) problem introduced
by Huang, Langberg, Kliewer and Bruck. A classical threshold secret sharing
scheme randomly encodes a secret into $n$ shares given to $n$ parties, such
that any set of at least $t$, $t&lt;n$, parties can reconstruct the secret, and
any set of at most $z$, $z&lt;t$, parties cannot obtain any information about the
secret. Recently, Huang et al. characterized the achievable minimum
communication overhead (CO) necessary for a legitimate user to decode the
secret when contacting $d\geq t$ parties and presented explicit code
constructions achieving minimum CO for $d=n$. The intuition behind the possible
savings on CO is that the user is only interested in decoding the secret and
does not have to decode the random keys involved in the encoding process. In
this paper, we introduce a new class of linear CESS codes called Staircase
Codes over any field $GF(q)$, for any prime power $q&gt; n$. We describe two
explicit constructions of Staircase codes that achieve minimum communication
and read overheads respectively for a fixed $d$, and universally for all
possible values of $d, t\leq d\leq n$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.02995</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.02995</id><created>2015-12-09</created><updated>2015-12-14</updated><authors><author><keyname>Salikhmetov</keyname><forenames>Anton</forenames></author></authors><title>A token-passing net implementation of optimal reduction with embedded
  read-back</title><categories>cs.LO cs.PL</categories><comments>7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce a new interaction net implementation of optimal
reduction for pure untyped lambda calculus. Unlike others, our implementation
allows to reach normal form regardless of interaction net reduction strategy
using the approach of so-called token-passing nets. Another new feature is the
read-back mechanism also implemented without leaving the formalism of
interaction nets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03007</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03007</id><created>2015-12-09</created><authors><author><keyname>Heath</keyname><forenames>Robert W.</forenames><suffix>Jr.</suffix></author><author><keyname>Gonzalez-Prelcic</keyname><forenames>Nuria</forenames></author><author><keyname>Rangan</keyname><forenames>Sundeep</forenames></author><author><keyname>Roh</keyname><forenames>Wonil</forenames></author><author><keyname>Sayeed</keyname><forenames>Akbar</forenames></author></authors><title>An Overview of Signal Processing Techniques for Millimeter Wave MIMO
  Systems</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Journal of Selected Topics in Signal Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Communication at millimeter wave (mmWave) frequencies is defining a new era
of wireless communication. The mmWave band offers higher bandwidth
communication channels versus those presently used in commercial wireless
systems. The applications of mmWave are immense: wireless local and personal
area networks in the unlicensed band, 5G cellular systems, not to mention
vehicular area networks, ad hoc networks, and wearables. Signal processing is
critical for enabling the next generation of mmWave communication. Due to the
use of large antenna arrays at the transmitter and receiver, combined with
radio frequency and mixed signal power constraints, new multiple-input
multiple-output (MIMO) communication signal processing techniques are needed.
Because of the wide bandwidths, low complexity transceiver algorithms become
important. There are opportunities to exploit techniques like compressed
sensing for channel estimation and beamforming. This article provides an
overview of signal processing challenges in mmWave wireless systems, with an
emphasis on those faced by using MIMO communication at higher carrier
frequencies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03012</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03012</id><created>2015-12-09</created><authors><author><keyname>Chang</keyname><forenames>Angel X.</forenames></author><author><keyname>Funkhouser</keyname><forenames>Thomas</forenames></author><author><keyname>Guibas</keyname><forenames>Leonidas</forenames></author><author><keyname>Hanrahan</keyname><forenames>Pat</forenames></author><author><keyname>Huang</keyname><forenames>Qixing</forenames></author><author><keyname>Li</keyname><forenames>Zimo</forenames></author><author><keyname>Savarese</keyname><forenames>Silvio</forenames></author><author><keyname>Savva</keyname><forenames>Manolis</forenames></author><author><keyname>Song</keyname><forenames>Shuran</forenames></author><author><keyname>Su</keyname><forenames>Hao</forenames></author><author><keyname>Xiao</keyname><forenames>Jianxiong</forenames></author><author><keyname>Yi</keyname><forenames>Li</forenames></author><author><keyname>Yu</keyname><forenames>Fisher</forenames></author></authors><title>ShapeNet: An Information-Rich 3D Model Repository</title><categories>cs.GR cs.AI cs.CG cs.CV cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present ShapeNet: a richly-annotated, large-scale repository of shapes
represented by 3D CAD models of objects. ShapeNet contains 3D models from a
multitude of semantic categories and organizes them under the WordNet taxonomy.
It is a collection of datasets providing many semantic annotations for each 3D
model such as consistent rigid alignments, parts and bilateral symmetry planes,
physical sizes, keywords, as well as other planned annotations. Annotations are
made available through a public web-based interface to enable data
visualization of object attributes, promote data-driven geometric analysis, and
provide a large-scale quantitative benchmark for research in computer graphics
and vision. At the time of this technical report, ShapeNet has indexed more
than 3,000,000 models, 220,000 models out of which are classified into 3,135
categories (WordNet synsets). In this report we describe the ShapeNet effort as
a whole, provide details for all currently available datasets, and summarize
future plans.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03019</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03019</id><created>2015-12-09</created><authors><author><keyname>Radu</keyname><forenames>Alexandra Maria</forenames></author></authors><title>Minimally Supervised Feature Selection for Classification (Master's
  Thesis, University Politehnica of Bucharest)</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the context of the highly increasing number of features that are available
nowadays we design a robust and fast method for feature selection. The method
tries to select the most representative features that are independent from each
other, but are strong together. We propose an algorithm that requires very
limited labeled data (as few as one labeled frame per class) and can
accommodate as many unlabeled samples. We also present here the supervised
approach from which we started. We compare our two formulations with
established methods like AdaBoost, SVM, Lasso, Elastic Net and FoBa and show
that our method is much faster and it has constant training time. Moreover, the
unsupervised approach outperforms all the methods with which we compared and
the difference might be quite prominent. The supervised approach is in most
cases better than the other methods, especially when the number of training
shots is very limited. All that the algorithm needs is to choose from a pool of
positively correlated features. The methods are evaluated on the
Youtube-Objects dataset of videos and on MNIST digits dataset, while at
training time we also used features obtained on CIFAR10 dataset and others
pre-trained on ImageNet dataset. Thereby, we also proved that transfer learning
is useful, even though the datasets differ very much: from low-resolution
centered images from 10 classes, to high-resolution images with objects from
1000 classes occurring in different regions of the images or to very difficult
videos with very high intraclass variance. 7
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03020</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03020</id><created>2015-12-09</created><authors><author><keyname>Chinaei</keyname><forenames>Hamidreza</forenames></author><author><keyname>Rais-Ghasem</keyname><forenames>Mohsen</forenames></author><author><keyname>Rudzicz</keyname><forenames>Frank</forenames></author></authors><title>Learning measures of semi-additive behaviour</title><categories>cs.AI</categories><comments>7 pages, 11 figures, 5 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In business analytics, measure values, such as sales numbers or volumes of
cargo transported, are often summed along values of one or more corresponding
categories, such as time or shipping container. However, not every measure
should be added by default (e.g., one might more typically want a mean over the
heights of a set of people); similarly, some measures should only be summed
within certain constraints (e.g., population measures need not be summed over
years). In systems such as Watson Analytics, the exact additive behaviour of a
measure is often determined by a human expert. In this work, we propose a small
set of features for this issue. We use these features in a case-based reasoning
approach, where the system suggests an aggregation behaviour, with 86% accuracy
in our collected dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03022</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03022</id><created>2015-12-08</created><authors><author><keyname>Avin</keyname><forenames>Chen</forenames></author><author><keyname>Els&#xe4;sser</keyname><forenames>Robert</forenames></author></authors><title>Breaking the log n barrier on rumor spreading</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  $O(\log n)$ rounds has been a well known upper bound for rumor spreading
using push&amp;pull in the random phone call model (i.e., uniform gossip in the
complete graph). A matching lower bound of $\Omega(\log n)$ is also known for
this special case. Under the assumption of this model and with a natural
addition that nodes can call a partner once they learn its address (e.g., its
IP address) we present a new distributed, address-oblivious and robust
algorithm that uses push&amp;pull with pointer jumping to spread a rumor to all
nodes in only $O(\sqrt{\log n})$ rounds, w.h.p. This algorithm can also cope
with $F= O(n/2^{\sqrt{\log n}})$ node failures, in which case all but $O(F)$
nodes become informed within $O(\sqrt{\log n})$ rounds, w.h.p.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03024</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03024</id><created>2015-12-09</created><authors><author><keyname>Pauly</keyname><forenames>Arno</forenames></author><author><keyname>Steinberg</keyname><forenames>Florian</forenames></author></authors><title>Representations of analytic functions and Weihrauch degrees</title><categories>cs.LO</categories><msc-class>03F60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers several representations of the analytic functions on the
unit disk and their mutual translations. All translations that are not already
computable are shown to be Weihrauch equivalent to closed choice on the natural
numbers. Subsequently some similar considerations are carried out for
representations of polynomials. In this case in addition to closed choice the
Weihrauch degree $\textrm{LPO}^*$ shows up as the difficulty of finding the
degree or the zeros.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03025</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03025</id><created>2015-12-09</created><authors><author><keyname>Zintchenko</keyname><forenames>Ilia</forenames></author><author><keyname>Hastings</keyname><forenames>Matthew</forenames></author><author><keyname>Wiebe</keyname><forenames>Nathan</forenames></author><author><keyname>Brown</keyname><forenames>Ethan</forenames></author><author><keyname>Troyer</keyname><forenames>Matthias</forenames></author></authors><title>Partial Reinitialisation for Optimisers</title><categories>stat.ML cs.LG cs.NE math.OC</categories><comments>8 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Heuristic optimisers which search for an optimal configuration of variables
relative to an objective function often get stuck in local optima where the
algorithm is unable to find further improvement. The standard approach to
circumvent this problem involves periodically restarting the algorithm from
random initial configurations when no further improvement can be found. We
propose a method of partial reinitialization, whereby, in an attempt to find a
better solution, only sub-sets of variables are re-initialised rather than the
whole configuration. Much of the information gained from previous runs is hence
retained. This leads to significant improvements in the quality of the solution
found in a given time for a variety of optimisation problems in machine
learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03031</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03031</id><created>2015-12-09</created><authors><author><keyname>Narasimha</keyname><forenames>Murali</forenames></author><author><keyname>Bagheri</keyname><forenames>Hossein</forenames></author></authors><title>Network Coding Applications for 5G Millimeter-Wave Communications</title><categories>cs.NI cs.IT math.IT</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The millimeter-wave bands have been attracting significant interest as a
means to achieve major improvements in data rates and network efficiencies. One
significant limitation for use of the millimeter-wave bands for cellular
communication is that the communication suffers from much higher path-loss
compared to the microwave bands. Millimeter-wave links have also been shown to
change rapidly, causing links between devices and access points to switch among
line-of-sight, non-line-of-sight and outage states. We propose using Random
Linear Network Coding to overcome the unreliability of individual communication
links in such millimeter-wave systems. Our system consists of devices
transmitting and receiving network-coded packets through several access points.
For downlink communication, network-coded packets are transmitted to a device
via multiple access points. For uplink communication, the access points perform
network coding of packets of several devices, and then send the network-coded
packets to the core network. We compare our approach against a naive approach
in which non-network coded packets are transmitted via multiple access points
(&quot;Forwarding&quot; approach). We find that the network coding approach significantly
outperforms the forwarding approach. In particular, network coding greatly
improves the efficiency of the air-interface transmissions and the efficiency
of the backhaul transmissions for the downlink and uplink communication,
respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03032</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03032</id><created>2015-12-09</created><authors><author><keyname>M&#xe9;ndez-Rial</keyname><forenames>Roi</forenames></author><author><keyname>Rusu</keyname><forenames>Cristian</forenames></author><author><keyname>Gonz&#xe1;lez-Prelcic</keyname><forenames>Nuria</forenames></author><author><keyname>Alkhateeb</keyname><forenames>Ahmed</forenames></author><author><keyname>Heath</keyname><forenames>Robert W.</forenames><suffix>Jr</suffix></author></authors><title>Hybrid MIMO Architectures for Millimeter Wave Communications: Phase
  Shifters or Switches?</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Access</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hybrid analog/digital MIMO architectures were recently proposed as an
alternative for fully-digitalprecoding in millimeter wave (mmWave) wireless
communication systems. This is motivated by the possible reduction in the
number of RF chains and analog-to-digital converters. In these architectures,
the analog processing network is usually based on variable phase shifters. In
this paper, we propose hybrid architectures based on switching networks to
reduce the complexity and the power consumption of the structures based on
phase shifters. We define a power consumption model and use it to evaluate the
energy efficiency of both structures. To estimate the complete MIMO channel, we
propose an open loop compressive channel estimation technique which is
independent of the hardware used in the analog processing stage. We analyze the
performance of the new estimation algorithm for hybrid architectures based on
phase shifters and switches. Using the estimated, we develop two algorithms for
the design of the hybrid combiner based on switches and analyze the achieved
spectral efficiency. Finally, we study the trade-offs between power
consumption, hardware complexity, and spectral efficiency for hybrid
architectures based on phase shifting networks and switching networks.
Numerical results show that architectures based on switches obtain equal or
better channel estimation performance to that obtained using phase shifters,
while reducing hardware complexity and power consumption. For equal power
consumption, all the hybrid architectures provide similar spectral
efficiencies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03048</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03048</id><created>2015-12-09</created><updated>2016-02-08</updated><authors><author><keyname>Krotov</keyname><forenames>Denis S.</forenames><affiliation>Sobolev Institute of Mathematics, Novosibirsk, Russia</affiliation></author><author><keyname>Vasil'eva</keyname><forenames>Anastasia Yu.</forenames><affiliation>Sobolev Institute of Mathematics, Novosibirsk, Russia</affiliation></author></authors><title>On 1-perfect codes that do not include Preparata codes</title><categories>cs.IT math.IT</categories><comments>English: 4pp; Russian: 5pp. v.2: English version added</comments><msc-class>94B25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that for every length of form $4^k-1$, there exists a binary
$1$-perfect code that does not include any Preparata-like code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03075</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03075</id><created>2015-12-09</created><updated>2016-01-19</updated><authors><author><keyname>Bartholdi</keyname><forenames>Laurent</forenames></author></authors><title>The rational homology of the outer automorphism group of $F_7$</title><categories>math.GR cs.DM math.AT</categories><comments>4 pages text, computer code. Mistakes corrected in signs of
  coefficients of w_cycle</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We compute the homology groups $H_*(\operatorname{Out}(F_7);\mathbb Q)$ of
the outer automorphism group of the free group of rank $7$.
  We produce in this manner the first rational homology classes of
$\operatorname{Out}(F_n)$ that are neither constant ($*=0$) nor Morita classes
($*=2n-4$).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03077</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03077</id><created>2015-12-08</created><authors><author><keyname>Raitoharju</keyname><forenames>Matti</forenames></author><author><keyname>Pich&#xe9;</keyname><forenames>Robert</forenames></author></authors><title>A Survey of Code Optimization Methods for Kalman Filter Extensions</title><categories>cs.SY math.OC math.PR</categories><acm-class>G.3; G.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Kalman filter extensions are commonly used algorithms for nonlinear state
estimation in time series. The structure of the state and measurement models in
the estimation problem can be exploited to reduce the computational demand of
the algorithms. We review algorithms that use different forms of structure and
show how they can be combined. We show also that the exploitation of the
structure of the problem can lead to improved accuracy of the estimates while
reducing the computational load.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03087</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03087</id><created>2015-10-08</created><authors><author><keyname>Kunwar</keyname><forenames>Bharat</forenames></author><author><keyname>Simini</keyname><forenames>Filippo</forenames></author><author><keyname>Johansson</keyname><forenames>Anders</forenames></author></authors><title>Evacuation time estimate for a total pedestrian evacuation using queuing
  network model and volunteered geographic information</title><categories>physics.soc-ph cs.MA</categories><comments>6 pages, 8 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Estimating city evacuation time is a non-trivial problem due to the
interaction between thousands of individual agents, giving rise to various
collective phenomena, such as bottleneck formation, intermittent flow and
stop-and-go waves. We present a mean field approach to draw relationships
between road network spatial attributes, number of evacuees and resultant
evacuation time estimate (ETE). We divide $50$ medium sized UK cities into a
total of $697$ catchment areas which we define as an area where all agents
share the same nearest exit node. In these catchment areas, 90% of agents are
within $5.4$ km of their designated exit node. We establish a characteristic
flow rate from catchment area attributes (population, distance to exit node and
exit node width) and a mean flow rate in free-flow regime by simulating total
evacuations using an agent based `queuing network' model. We use these
variables to determine a relationship between catchment area attributes and
resultant ETE. This relationship could enable emergency planners to make rapid
appraisal of evacuation strategies and help support decisions in the run up to
a crisis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03088</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03088</id><created>2015-10-17</created><authors><author><keyname>Scala</keyname><forenames>Antonio</forenames></author><author><keyname>Lucentini</keyname><forenames>Pier Giorgio De Sanctis</forenames></author><author><keyname>Caldarelli</keyname><forenames>Guido</forenames></author><author><keyname>D'Agostino</keyname><forenames>Gregorio</forenames></author></authors><title>Cascades in interdependent flow networks</title><categories>physics.soc-ph cs.SY</categories><comments>submitted to Physica D</comments><doi>10.1016/j.physd.2015.10.010</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the abrupt breakdown behavior of coupled distribution grids
under load growth. This scenario mimics the ever-increasing customer demand and
the foreseen introduction of energy hubs interconnecting the different energy
vectors. We extend an analytical model of cascading behavior due to line
overloads to the case of interdependent networks and find evidence of first
order transitions due to the long-range nature of the flows. Our results
indicate that the foreseen increase in the couplings between the grids has two
competing effects: on the one hand, it increases the safety region where grids
can operate without withstanding systemic failures; on the other hand, it
increases the possibility of a joint systems' failure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03099</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03099</id><created>2015-12-07</created><authors><author><keyname>Veitch</keyname><forenames>Victor</forenames></author><author><keyname>Roy</keyname><forenames>Daniel M.</forenames></author></authors><title>The Class of Random Graphs Arising from Exchangeable Random Measures</title><categories>math.ST cs.SI math.CO stat.TH</categories><comments>52 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a class of random graphs that we argue meets many of the
desiderata one would demand of a model to serve as the foundation for a
statistical analysis of real-world networks. The class of random graphs is
defined by a probabilistic symmetry: invariance of the distribution of each
graph to an arbitrary relabelings of its vertices. In particular, following
Caron and Fox, we interpret a symmetric simple point process on
$\mathbb{R}_+^2$ as the edge set of a random graph, and formalize the
probabilistic symmetry as joint exchangeability of the point process. We give a
representation theorem for the class of random graphs satisfying this symmetry
via a straightforward specialization of Kallenberg's representation theorem for
jointly exchangeable random measures on $\mathbb{R}_+^2$. The distribution of
every such random graph is characterized by three (potentially random)
components: a nonnegative real $I \in \mathbb{R}_+$, an integrable function $S:
\mathbb{R}_+ \to \mathbb{R}_+$, and a symmetric measurable function $W:
\mathbb{R}_+^2 \to [0,1]$ that satisfies several weak integrability conditions.
We call the triple $(I,S,W)$ a graphex, in analogy to graphons, which
characterize the (dense) exchangeable graphs on $\mathbb{N}$. Indeed, the model
we introduce here contains the exchangeable graphs as a special case, as well
as the &quot;sparse exchangeable&quot; model of Caron and Fox. We study the structure of
these random graphs, and show that they can give rise to interesting structure,
including sparse graph sequences. We give explicit equations for expectations
of certain graph statistics, as well as the limiting degree distribution. We
also show that certain families of graphexes give rise to random graphs that,
asymptotically, contain an arbitrarily large fraction of the vertices in a
single connected component.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03122</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03122</id><created>2015-12-09</created><authors><author><keyname>Ghazanfari</keyname><forenames>Amin</forenames></author><author><keyname>Tabassum</keyname><forenames>Hina</forenames></author><author><keyname>Hossain</keyname><forenames>Ekram</forenames></author></authors><title>Ambient RF Energy Harvesting in Ultra-Dense Small Cell Networks:
  Performance and Trade-offs</title><categories>cs.NI</categories><comments>IEEE Wireless Communications, to appear</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to minimize electric grid power consumption, energy harvesting from
ambient RF sources is considered as a promising technique for wireless charging
of low-power devices. To illustrate the design considerations of RF-based
ambient energy harvesting networks, this article first points out the primary
challenges of implementing and operating such networks, including
non-deterministic energy arrival patterns, energy harvesting mode selection,
energy-aware cooperation among base stations (BSs), etc. A brief overview of
the recent advancements and a summary of their shortcomings are then provided
to highlight existing research gaps and possible future research directions. To
this end, we investigate the feasibility of implementing RF-based ambient
energy harvesting in ultra-dense small cell networks (SCNs) and examine the
related trade-offs in terms of the energy efficiency and
signal-to-interference-plus-noise ratio (SINR) outage probability of a typical
user in the downlink. Numerical results demonstrate the significance of
deploying a mixture of on-grid small base stations (SBSs)~(powered by electric
grid) and off-grid SBSs~(powered by energy harvesting) and optimizing their
corresponding proportions as a function of the intensity of active SBSs in the
network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03126</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03126</id><created>2015-12-09</created><authors><author><keyname>Hepburn</keyname><forenames>I.</forenames></author><author><keyname>Chen</keyname><forenames>W.</forenames></author><author><keyname>De Schutter</keyname><forenames>E.</forenames></author></authors><title>Accurate Reaction-Diffusion Operator Splitting on Tetrahedral Meshes for
  Parallel Stochastic Molecular Simulations</title><categories>q-bio.QM cs.DC physics.bio-ph physics.chem-ph q-bio.BM</categories><comments>33 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spatial stochastic molecular simulations in biology are limited by the
intense computation required to track molecules in space either in a discrete
time or discrete space framework, meaning that the serial limit has already
been reached in sub-cellular models. This calls for parallel simulations that
can take advantage of the power of modern supercomputers; however exact methods
are known to be inherently serial. We introduce an operator splitting
implementation for irregular grids with a novel method to improve accuracy, and
demonstrate potential for scalable parallel simulations in an initial MPI
version. We foresee that this groundwork will enable larger scale, whole-cell
stochastic simulations in the near future.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03127</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03127</id><created>2015-12-09</created><authors><author><keyname>Jackson</keyname><forenames>Marcel</forenames></author></authors><title>Flexible constraint satisfiability and a problem in semigroup theory</title><categories>math.LO cs.CC cs.LO math.CO</categories><msc-class>68Q17, 20M07, 03C05, 08B99, 08C15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We examine some flexible notions of constraint satisfaction, observing some
relationships between model theoretic notions of universal Horn class
membership and robust satisfiability. We show the \texttt{NP}-completeness of
$2$-robust monotone 1-in-3 3SAT in order to give very small examples of finite
algebras with \texttt{NP}-hard variety membership problem. In particular we
give a $3$-element algebra with this property, and solve a widely stated
problem by showing that the $6$-element Brandt monoid has \texttt{NP}-hard
variety membership problem. These are the smallest possible sizes for a general
algebra and a semigroup to exhibit \texttt{NP}-hardness for the membership
problem of finite algebras in finitely generated varieties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03128</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03128</id><created>2015-12-09</created><updated>2016-01-22</updated><authors><author><keyname>Hashemi</keyname><forenames>Seyyed Ali</forenames></author><author><keyname>Balatsoukas-Stimming</keyname><forenames>Alexios</forenames></author><author><keyname>Giard</keyname><forenames>Pascal</forenames></author><author><keyname>Thibeault</keyname><forenames>Claude</forenames></author><author><keyname>Gross</keyname><forenames>Warren J.</forenames></author></authors><title>Partitioned Successive-Cancellation List Decoding of Polar Codes</title><categories>cs.AR cs.IT math.IT</categories><comments>4 pages, 6 figures, to appear at IEEE ICASSP 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Successive-cancellation list (SCL) decoding is an algorithm that provides
very good error-correction performance for polar codes. However, its hardware
implementation requires a large amount of memory, mainly to store intermediate
results. In this paper, a partitioned SCL algorithm is proposed to reduce the
large memory requirements of the conventional SCL algorithm. The decoder tree
is broken into partitions that are decoded separately. We show that with
careful selection of list sizes and number of partitions, the proposed
algorithm can outperform conventional SCL while requiring less memory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03131</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03131</id><created>2015-12-09</created><authors><author><keyname>Wang</keyname><forenames>Li</forenames></author><author><keyname>Sng</keyname><forenames>Dennis</forenames></author></authors><title>Deep Learning Algorithms with Applications to Video Analytics for A
  Smart City: A Survey</title><categories>cs.CV</categories><comments>8 pages, 18 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep learning has recently achieved very promising results in a wide range of
areas such as computer vision, speech recognition and natural language
processing. It aims to learn hierarchical representations of data by using deep
architecture models. In a smart city, a lot of data (e.g. videos captured from
many distributed sensors) need to be automatically processed and analyzed. In
this paper, we review the deep learning algorithms applied to video analytics
of smart city in terms of different research topics: object detection, object
tracking, face recognition, image classification and scene labeling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03132</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03132</id><created>2015-12-09</created><authors><author><keyname>Zhou</keyname><forenames>Xichuan</forenames></author><author><keyname>Li</keyname><forenames>Qin</forenames></author><author><keyname>Zhao</keyname><forenames>Han</forenames></author><author><keyname>Li</keyname><forenames>Shengli</forenames></author><author><keyname>Yu</keyname><forenames>Lei</forenames></author><author><keyname>Tang</keyname><forenames>Fang</forenames></author><author><keyname>Hu</keyname><forenames>Shengdong</forenames></author><author><keyname>Li</keyname><forenames>Guojun</forenames></author><author><keyname>Feng</keyname><forenames>Yujie</forenames></author></authors><title>Assessing Google Correlate Queries for Influenza H1N1 Surveillance in
  Asian Developing Countries</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  So far, Google Trend data have been used for influenza surveillance in many
European and American countries; however, there are few attempts to apply the
low-cost surveillance method in Asian developing countries. To investigate the
correlation between the search trends and the influenza activity in Asia, we
examined the Google query data of four Asian developing countries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03139</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03139</id><created>2015-12-10</created><authors><author><keyname>Prolubnikov</keyname><forenames>Alexander</forenames></author></authors><title>Reduction of the graph isomorphism problem to equality checking of
  $n$-variables polynomials and the algorithms that use the reduction</title><categories>cs.DM</categories><comments>12 pages</comments><acm-class>F.2.2; G.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The graph isomorphism problem is considered. We assign modified
characteristic polynomials for graphs and reduce the graph isomorphism problem
to the following one. It is reqired to find out, is there such an enumeration
of the graphs vertices that the polynomials of the graphs are equal. We present
algorithms that use the reduction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03143</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03143</id><created>2015-12-10</created><authors><author><keyname>Ge</keyname><forenames>Xiaohu</forenames></author><author><keyname>Tu</keyname><forenames>Song</forenames></author><author><keyname>Mao</keyname><forenames>Guoqiang</forenames></author><author><keyname>Wang</keyname><forenames>Cheng-Xiang</forenames></author><author><keyname>Han</keyname><forenames>Tao</forenames></author></authors><title>5G Ultra-Dense Cellular Networks</title><categories>cs.NI</categories><comments>14 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional ultra-dense wireless networks are recommended as a complement for
cellular networks and are deployed in partial areas, such as hotspot and indoor
scenarios. Based on the massive multiple-input multi-output (MIMO) antennas and
the millimeter wavecommunication technologies, the 5G ultra-dense cellular
network is proposed to deploy in overall cellular scenarios. Moreover, a
distribution network architecture is presented for 5G ultra-dense cellular
networks. Furthermore, the backhaul network capacity and the backhaul energy
efficiency of ultra-dense cellular networks are investigated to answer an
important question, i.e., how much densification can be deployed for 5G
ultra-dense cellular networks. Simulation results reveal that there exist
densification limits for 5G ultra-dense cellualr networks with backhaul network
capacity and backhaul energy efficiency constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03149</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03149</id><created>2015-12-10</created><authors><author><keyname>Ge</keyname><forenames>Xiaohu</forenames></author><author><keyname>Ye</keyname><forenames>Junliang</forenames></author><author><keyname>Yang</keyname><forenames>Yang</forenames></author><author><keyname>Li</keyname><forenames>Qiang</forenames></author></authors><title>User Mobility Evaluation for 5G Small Cell Networks Based on Individual
  Mobility Model</title><categories>cs.NI</categories><comments>15 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With small cell networks becoming core parts of the fifth generation (5G)
cellular networks, it is an important problem to evaluate the impact of user
mobility on 5G small cell networks. However, the tendency and clustering habits
in human activities have not been considered in traditional user mobility
models. In this paper, human tendency and clustering behaviors are first
considered to evaluate the user mobility performance for 5G small cell networks
based on individual mobility model (IMM). As key contributions, user pause
probability, user arrival and departure probabilities are derived in this paper
for evaluat-ing the user mobility performance in a hotspot-type 5G small cell
network. Furthermore, coverage probabilities of small cell and macro cell BSs
are derived for all users in 5G small cell networks, respectively. Compared
with the traditional random waypoint (RWP) model, IMM provides a different
viewpoint to investigate the impact of human tendency and clustering behaviors
on the performance of 5G small cell networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03152</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03152</id><created>2015-12-10</created><authors><author><keyname>Ge</keyname><forenames>Xiaohu</forenames></author><author><keyname>Chen</keyname><forenames>Jiaqi</forenames></author><author><keyname>Wang</keyname><forenames>Cheng-xiang</forenames></author><author><keyname>Thompson</keyname><forenames>John</forenames></author><author><keyname>Zhang</keyname><forenames>Jing</forenames></author></authors><title>5G green cellular networks considering power allocation schemes</title><categories>cs.NI cs.IT math.IT</categories><comments>14 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is important to assess the effect of transmit power allocation schemes on
the energy consumption on random cellular networks. The energy efficiency of 5G
green cellular networks with average and water-filling power allocation schemes
is studied in this paper. Based on the proposed interference and achievable
rate model, an energy efficiency model is proposed for MIMO random cellular
networks. Furthermore, the energy efficiency with average and water-filling
power allocation schemes are presented, respectively. Numerical results
indicate that the maximum limits of energy efficiency are always there for MIMO
random cellular networks with different intensity ratios of mobile stations
(MSs) to base stations (BSs) and channel conditions. Compared with the average
power allocation scheme, the water-filling scheme is shown to improve the
energy efficiency of MIMO random cellular networks when channel state
information (CSI) is attainable for both transmitters and receivers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03155</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03155</id><created>2015-12-10</created><authors><author><keyname>Bostanci</keyname><forenames>Erkan</forenames></author></authors><title>Enhanced image feature coverage: Key-point selection using genetic
  algorithms</title><categories>cs.CV</categories><comments>14 pages, journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Coverage of image features play an important role in many vision algorithms
since their distribution affect the estimated homography. This paper presents a
Genetic Algorithm (GA) in order to select the optimal set of features yielding
maximum coverage of the image which is measured by a robust method based on
spatial statistics. It is shown with statistical tests on two datasets that the
metric yields better coverage and this is also confirmed by an accuracy test on
the computed homography for the original set and the newly selected set of
features. Results have demonstrated that the new set has similar performance in
terms of the accuracy of the computed homography with the original one with an
extra benefit of using fewer number of features ultimately reducing the time
required for descriptor calculation and matching.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03156</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03156</id><created>2015-12-10</created><authors><author><keyname>Bostanci</keyname><forenames>Erkan</forenames></author></authors><title>3D Reconstruction of Crime Scenes and Design Considerations for an
  Interactive Investigation Tool</title><categories>cs.CV</categories><comments>9 pages, journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Crime Scene Investigation (CSI) is a carefully planned systematic process
with the purpose of acquiring physical evidences to shed light upon the
physical reality of the crime and eventually detect the identity of the
criminal. Capturing images and videos of the crime scene is an important part
of this process in order to conduct a deeper analysis on the digital evidence
for possible hints. This work brings this idea further to use the acquired
footage for generating a 3D model of the crime scene. Results show that
realistic reconstructions can be obtained using sophisticated computer vision
techniques. The paper also discusses a number of important design
considerations describing key features that should be present in a powerful
interactive CSI analysis tool.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03165</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03165</id><created>2015-12-10</created><authors><author><keyname>Alshari</keyname><forenames>Eissa M.</forenames></author></authors><title>Semantic Arabic Information Retrieval Framework</title><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The continuous increasing in the amount of the published and stored
information requires a special Information Retrieval (IR) frameworks to search
and get information accurately and speedily. Currently, keywords-based
techniques are commonly used in information retrieval. However, a major
drawback of the keywords approach is its inability of handling the polysemy and
synonymy phenomenon of the natural language. For instance, the meanings of
words and understanding of concepts differ in different communities. Same word
use for different concepts (polysemy) or use different words for the same
concept (synonymy). Most of information retrieval frameworks have a weakness to
deal with the semantics of the words in term of (indexing, Boolean model,
Latent Semantic Analysis (LSA) , Latent semantic Index (LSI) and semantic
ranking, etc.). Traditional Arabic Information Retrieval (AIR) models
performance insufficient with semantic queries, which deal with not only the
keywords but also with the context of these keywords. Therefore, there is a
need for a semantic information retrieval model with a semantic index structure
and ranking algorithm based on semantic index.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03167</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03167</id><created>2015-12-10</created><authors><author><keyname>Elabd</keyname><forenames>Emad</forenames></author><author><keyname>Alshari</keyname><forenames>Eissa</forenames></author><author><keyname>Abdulkader</keyname><forenames>Hatem</forenames></author></authors><title>Semantic Boolean Arabic Information Retrieval</title><categories>cs.IR</categories><comments>in The International Arab Journal of Information Technology, Vol. 12,
  No. 3, May 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Arabic language is one of the most widely spoken languages. This language has
a complex morphological structure and is considered as one of the most prolific
languages in terms of article linguistic. Therefore, Arabic Information
Retrieval (AIR) models need specific techniques to deal with this complex
morphological structure. This paper aims to develop an integrate AIR
frameworks. It lists and analysis the different Information Retrieval (IR)
methods and techniques such as query processing, stemming and indexing which
are used in AIR systems. We conclude that AIR frameworks have a weakness to
deal with semantic in term of indexing, Boolean model, Latent Semantic Analysis
(LSA), Latent Semantic Index (LSI) and semantic ranking. Therefore, semantic
Boolean IR framework is proposed in this paper. This model is implemented and
the precision, recall and run time are measured and compared with the
traditional IR model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03169</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03169</id><created>2015-12-10</created><authors><author><keyname>Szab&#xf3;</keyname><forenames>D&#xe1;vid</forenames></author><author><keyname>K&#x151;r&#xf6;si</keyname><forenames>Attila</forenames></author><author><keyname>B&#xed;r&#xf3;</keyname><forenames>J&#xf3;zsef</forenames></author><author><keyname>Guly&#xe1;s</keyname><forenames>Andr&#xe1;s</forenames></author></authors><title>Deductive Way of Reasoning about the Internet AS Level Topology</title><categories>cs.NI</categories><acm-class>C.2.1</acm-class><journal-ref>Chin. Phys. B . 2015, 24(11): 118901</journal-ref><doi>10.1088/1674-1056/24/11/118901</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our current understanding about the AS level topology of the Internet is
based on measurements and inductive-type models which set up rules describing
the behavior (node and edge dynamics) of the individual ASes and generalize the
consequences of these individual actions for the complete AS ecosystem using
induction. In this paper we suggest a third, deductive approach in which we
have premises for the whole AS system and the consequences of these premises
are determined through deductive reasoning. We show that such a deductive
approach can give complementary insights into the topological properties of the
AS graph. While inductive models can mostly reflect high level statistics (e.g.
degree distribution, clustering, diameter), deductive reasoning can identify
omnipresent subgraphs and peering likelihood. We also propose a model, called
YEAS, incorporating our deductive analytical findings that produces topologies
contain both traditional and novel metrics for the AS level Internet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03184</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03184</id><created>2015-12-10</created><authors><author><keyname>Iyengar</keyname><forenames>S. R. S.</forenames></author><author><keyname>Parasuram</keyname><forenames>Aishwarya</forenames></author><author><keyname>Saini</keyname><forenames>Jaspal Singh</forenames></author></authors><title>Social Network Analysis of the Caste-Based Reservation System in India</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It has been argued that the reservation system in India, which has existed
since the time of Indian Independence (1947), has caused more havoc and
degradation than progress. This being a popular public opinion, these notions
have not been based on any rigorous scientific study or research. In this
paper, we revisit the cultural divide among the Indian population from a purely
social networks based approach. We study the reservation system in detail,
starting from its past and observing its effect on the people. Through a
survey, we analyze the variation in behavioural characteristics exhibited
towards members of the same caste group versus members from another caste
group. We study the distinct cluster formation that takes place in the Indian
community, and find that this is largely due to the effect of caste-based
homophily. To study the social capital associated with each individual in the
backward class, we define a new parameter called social distance. We study the
changes that take place with regard to the average social distance of a cluster
as well as network formation when a new link is established between the
clusters which in its essence, is what the reservation system is accomplishing.
Our extensive study calls for the change in the mindset of people in India.
Although the animosity towards the reservation system could be rooted due to
historical influence, hero worship and herd mentality, our results make it
clear that the system has had a considerable impact in the country's overall
development by bridging the gap between the conflicting social groups.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03189</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03189</id><created>2015-12-10</created><authors><author><keyname>Zheng</keyname><forenames>Yuanshi</forenames></author><author><keyname>Ma</keyname><forenames>Jingying</forenames></author><author><keyname>Wang</keyname><forenames>Long</forenames></author></authors><title>Consensus of Hybrid Multi-agent Systems</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the consensus problem of hybrid multi-agent
system. First, the hybrid multi-agent system is proposed which is composed of
continuous-time and discrete-time dynamic agents. Then, three kinds of
consensus protocols are presented for hybrid multi-agent system. The analysis
tool developed in this paper is based on the matrix theory and graph theory.
With different restrictions of the sampling period, some necessary and
sufficient conditions are established for solving the consensus of hybrid
multi-agent system. The consensus states are also obtained under different
protocols. Finally, simulation examples are provided to demonstrate the
effectiveness of our theoretical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03199</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03199</id><created>2015-12-10</created><authors><author><keyname>Mayer</keyname><forenames>Michael</forenames></author><author><keyname>van der Zypen</keyname><forenames>Dominic</forenames></author></authors><title>Graph-theoretic autofill</title><categories>cs.HC math.CO</categories><comments>13 pages</comments><msc-class>05C20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Imagine a website that asks the user to fill in a web form and -- based on
the input values -- derives a relevant figure, for instance an expected salary,
a medical diagnosis or the market value of a house. How to deal with missing
input values at run-time? Besides using fixed defaults, a more sophisticated
approach is to use predefined dependencies (logical or correlational) between
different fields to autofill missing values in an iterative way. Directed
loopless graphs (in which cycles are allowed) are the ideal mathematical model
to formalize these dependencies. We present two new graph-theoretic approaches
to filling missing values at run-time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03201</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03201</id><created>2015-12-10</created><authors><author><keyname>Sigaud</keyname><forenames>Olivier</forenames></author><author><keyname>Masson</keyname><forenames>Cl&#xe9;ment</forenames></author><author><keyname>Filliat</keyname><forenames>David</forenames></author><author><keyname>Stulp</keyname><forenames>Freek</forenames></author></authors><title>Gated networks: an inventory</title><categories>cs.LG</categories><comments>Unpublished manuscript, 17 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gated networks are networks that contain gating connections, in which the
outputs of at least two neurons are multiplied. Initially, gated networks were
used to learn relationships between two input sources, such as pixels from two
images. More recently, they have been applied to learning activity recognition
or multi-modal representations. The aims of this paper are threefold: 1) to
explain the basic computations in gated networks to the non-expert, while
adopting a standpoint that insists on their symmetric nature. 2) to serve as a
quick reference guide to the recent literature, by providing an inventory of
applications of these networks, as well as recent extensions to the basic
architecture. 3) to suggest future research directions and applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03207</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03207</id><created>2015-12-10</created><authors><author><keyname>Bolz</keyname><forenames>Carl Friedrich</forenames></author><author><keyname>Kurilova</keyname><forenames>Darya</forenames></author><author><keyname>Tratt</keyname><forenames>Laurence</forenames></author></authors><title>Making an Embedded DBMS JIT-friendly</title><categories>cs.PL cs.DB</categories><comments>24 pages, 8 figures</comments><acm-class>D.3.4</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  While DataBase Management Systems (DBMSs) are highly optimized, interactions
across the Programming Language (PL) / DBMS boundary are costly, even for
in-process embedded DBMSs. In this paper we show that programs that interact
with the widely-used embedded DBMS SQLite can be significantly optimized - by a
factor of 3.4 in our benchmarks - by inlining across the PL / DBMS boundary. We
achieved this speed-up by replacing parts of SQLite's C interpreter with
RPython code and composing the resulting meta-tracing VM - called SQPyte - with
the PyPy VM. SQPyte does not compromise stand-alone SQL performance: it is 2.2%
faster than SQLite on the widely used TPC-H benchmark suite.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03211</identifier>
 <datestamp>2015-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03211</id><created>2015-12-10</created><authors><author><keyname>Vassio</keyname><forenames>Luca</forenames></author><author><keyname>Metwalley</keyname><forenames>Hassan</forenames></author><author><keyname>Giordano</keyname><forenames>Danilo</forenames></author></authors><title>The Exploitation of Web Navigation Data: Ethical Issues and Alternative
  Scenarios</title><categories>cs.CY cs.CR cs.NI</categories><comments>11 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, the users' browsing activity on the Internet is not completely
private due to many entities that collect and use such data, either for
legitimate or illegal goals.
  The implications are serious, from a person who exposes his private
information to an unknown third party entity, without any knowledge, to a
company that is unable to control the flow of its information to the outside
world. As a result, users have lost control over their private data in the
Internet.
  In this paper, we present the entities involved in users' data collection and
usage. Then, we highlight what are the ethical issues that arise for users,
companies, scientists and governments. Finally, we present some alternative
scenarios and suggestions for the entities to address such ethical issues.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03219</identifier>
 <datestamp>2015-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03219</id><created>2015-12-10</created><updated>2015-12-15</updated><authors><author><keyname>Malyshkin</keyname><forenames>Vladislav Gennadievich</forenames></author></authors><title>Norm-Free Radon-Nikodym Approach to Machine Learning</title><categories>cs.LG stat.ML</categories><comments>Cluster localization measure added. Quantum mechanics analogy
  improved and expanded (density matrix exact expression added). Coverage
  calculation via matrix spectrum added</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For Machine Learning (ML) classification problem, where a vector of
$\mathbf{x}$--observations (values of attributes) is mapped to a single $y$
value (class label), a generalized Radon--Nikodym type of solution is proposed.
Quantum--mechanics --like probability states $\psi^2(\mathbf{x})$ are
considered and &quot;Cluster Centers&quot;, corresponding to the extremums of
$&lt;y\psi^2(\mathbf{x})&gt;/&lt;\psi^2(\mathbf{x})&gt;$, are found from generalized
eigenvalues problem. The eigenvalues give possible $y^{[i]}$ outcomes and
corresponding to them eigenvectors $\psi^{[i]}(\mathbf{x})$ define &quot;Cluster
Centers&quot;. The projection of a $\psi$ state, localized at given $\mathbf{x}$ to
classify, on these eigenvectors define the probability of $y^{[i]}$ outcome,
thus avoiding using a norm ($L^2$ or other types), required for &quot;quality
criteria&quot; in a typical Machine Learning technique. A coverage of each `Cluster
Center&quot; is calculated, what potentially allows to separate system properties
(described by $y^{[i]}$ outcomes) and system testing conditions (described by
$C^{[i]}$ coverage). As an example of such application $y$ distribution
estimator is proposed in a form of pairs $(y^{[i]},C^{[i]})$, that can be
considered as Gauss quadratures generalization. This estimator allows to
perform $y$ probability distribution estimation in a strongly non--Gaussian
case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03220</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03220</id><created>2015-12-10</created><authors><author><keyname>Beretta</keyname><forenames>Stefano</forenames></author><author><keyname>Castelli</keyname><forenames>Mauro</forenames></author><author><keyname>Dondi</keyname><forenames>Riccardo</forenames></author></authors><title>Parameterized Tractability of the Maximum-Duo Preservation String
  Mapping Problem</title><categories>cs.DS cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we investigate the parameterized complexity of the Maximum-Duo
Preservation String Mapping Problem, the complementary of the Minimum Common
String Partition Problem. We show that this problem is fixed-parameter
tractable when parameterized by the number k of conserved duos, by first giving
a parameterized algorithm based on the color-coding technique and then
presenting a reduction to a kernel of size O(k^6 ).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03221</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03221</id><created>2015-12-10</created><authors><author><keyname>Gu</keyname><forenames>Yifan</forenames></author><author><keyname>Chen</keyname><forenames>He</forenames></author><author><keyname>Li</keyname><forenames>Yonghui</forenames></author><author><keyname>Vucetic</keyname><forenames>Branka</forenames></author></authors><title>A Discrete Time-Switching Protocol for Wireless-Powered Communications
  with Energy Accumulation</title><categories>cs.IT math.IT</categories><comments>Presented at Globecom'15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates a wireless-powered communication network (WPCN) setup
with one multi-antenna access point (AP) and one single-antenna source. It is
assumed that the AP is connected to an external power supply, while the source
does not have an embedded energy supply. But the source could harvest energy
from radio frequency (RF) signals sent by the AP and store it for future
information transmission. We develop a discrete time-switching (DTS) protocol
for the considered WPCN. In the proposed protocol, either energy harvesting
(EH) or information transmission (IT) operation is performed during each
transmission block. Specifically, based on the channel state information (CSI)
between source and AP, the source can determine the minimum energy required for
an outage-free IT operation. If the residual energy of the source is
sufficient, the source will start the IT phase. Otherwise, EH phase is invoked
and the source accumulates the harvested energy. To characterize the
performance of the proposed protocol, we adopt a discrete Markov chain (MC) to
model the energy accumulation process at the source battery. A closed-form
expression for the average throughput of the DTS protocol is derived. Numerical
results validate our theoretical analysis and show that the proposed DTS
protocol considerably outperforms the existing harvest-then-transmit protocol
when the battery capacity at the source is large.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03223</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03223</id><created>2015-12-10</created><authors><author><keyname>van Ommen</keyname><forenames>Thijs</forenames></author><author><keyname>Koolen</keyname><forenames>Wouter M.</forenames></author><author><keyname>Feenstra</keyname><forenames>Thijs E.</forenames></author><author><keyname>Gr&#xfc;nwald</keyname><forenames>Peter D.</forenames></author></authors><title>Worst-case Optimal Probability Updating</title><categories>stat.ME cs.GT</categories><comments>Preprint</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper discusses an alternative to conditioning that may be used when the
probability distribution is not fully specified. It does not require any
assumptions (such as CAR: coarsening at random) on the unknown distribution.
The well-known Monty Hall problem is the simplest scenario where neither naive
conditioning nor the CAR assumption suffice to determine an updated probability
distribution. This paper thus addresses a generalization of that problem to
arbitrary distributions on finite outcome spaces, arbitrary sets of `messages',
and (almost) arbitrary loss functions, and provides existence and
characterization theorems for worst-case optimal probability updating
strategies. We find that for logarithmic loss, optimality is characterized by
an elegant condition, which we call RCAR (reverse coarsening at random). Under
certain conditions, the same condition also characterizes optimality for a much
larger class of loss functions, and we obtain an objective and general answer
to how one should update probabilities in the light of new information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03224</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03224</id><created>2015-12-10</created><authors><author><keyname>Fang</keyname><forenames>Jun</forenames></author><author><keyname>Yang</keyname><forenames>Linxiao</forenames></author><author><keyname>Li</keyname><forenames>Hongbin</forenames></author></authors><title>Spectral Compressed Sensing via CANDECOMP/PARAFAC Decomposition of
  Incomplete Tensors</title><categories>cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the line spectral estimation problem which aims to recover a
mixture of complex sinusoids from a small number of randomly observed time
domain samples. Compressed sensing methods formulates line spectral estimation
as a sparse signal recovery problem by discretizing the continuous frequency
parameter space into a finite set of grid points. Discretization, however,
inevitably incurs errors and leads to deteriorated estimation performance. In
this paper, we propose a new method which leverages recent advances in tensor
decomposition. Specifically, we organize the observed data into a structured
tensor and cast line spectral estimation as a CANDECOMP/PARAFAC (CP)
decomposition problem with missing entries. The uniqueness of the CP
decomposition allows the frequency components to be super-resolved with
infinite precision. Simulation results show that the proposed method provides a
competitive estimate accuracy compared with existing state-of-the-art
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03225</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03225</id><created>2015-12-10</created><authors><author><keyname>Shen</keyname><forenames>Wenqian</forenames></author><author><keyname>Dai</keyname><forenames>Linglong</forenames></author><author><keyname>Shim</keyname><forenames>Byonghyo</forenames></author><author><keyname>Mumtaz</keyname><forenames>Shahid</forenames></author><author><keyname>Wang</keyname><forenames>Zhaocheng</forenames></author></authors><title>Joint CSIT Acquisition Based on Low-Rank Matrix Completion for FDD
  Massive MIMO Systems</title><categories>cs.IT math.IT</categories><journal-ref>IEEE Communications Letters, vol. 19, no. 12, pp. 2178-2181, Dec.
  2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Channel state information at the transmitter (CSIT) is essential for
frequency-division duplexing (FDD) massive MIMO systems, but conventional
solutions involve overwhelming overhead both for downlink channel training and
uplink channel feedback. In this letter, we propose a joint CSIT acquisition
scheme to reduce the overhead. Particularly, unlike conventional schemes where
each user individually estimates its own channel and then feed it back to the
base station (BS), we propose that all scheduled users directly feed back the
pilot observation to the BS, and then joint CSIT recovery can be realized at
the BS. We further formulate the joint CSIT recovery problem as a low-rank
matrix completion problem by utilizing the low-rank property of the massive
MIMO channel matrix, which is caused by the correlation among users. Finally,
we propose a hybrid low-rank matrix completion algorithm based on the singular
value projection to solve this problem. Simulations demonstrate that the
proposed scheme can provide accurate CSIT with lower overhead than conventional
schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03230</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03230</id><created>2015-12-10</created><authors><author><keyname>Shen</keyname><forenames>Wenqian</forenames></author><author><keyname>Dai</keyname><forenames>Linglong</forenames></author><author><keyname>Shi</keyname><forenames>Yi</forenames></author><author><keyname>Shim</keyname><forenames>Byonghyo</forenames></author><author><keyname>Wang</keyname><forenames>Zhaocheng</forenames></author></authors><title>Joint Channel Training and Feedback for FDD Massive MIMO Systems</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Massive multiple-input multiple-output (MIMO) is widely recognized as a
promising technology for future 5G wireless communication systems. To achieve
the theoretical performance gains in massive MIMO systems, accurate channel
state information at the transmitter (CSIT) is crucial. Due to the overwhelming
pilot signaling and channel feedback overhead, however, conventional downlink
channel estimation and uplink channel feedback schemes might not be suitable
for frequency-division duplexing (FDD) massive MIMO systems. In addition, these
two topics are usually separately considered in the literature. In this paper,
we propose a joint channel training and feedback scheme for FDD massive MIMO
systems. Specifically, we firstly exploit the temporal correlation of
time-varying channels to propose a differential channel training and feedback
scheme, which simultaneously reduces the overhead for downlink training and
uplink feedback. We next propose a structured compressive sampling matching
pursuit (S-CoSaMP) algorithm to acquire a reliable CSIT by exploiting the
structured sparsity of wireless MIMO channels. Simulation results demonstrate
that the proposed scheme can achieve substantial reduction in the training and
feedback overhead.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03236</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03236</id><created>2015-12-10</created><authors><author><keyname>Cardinal</keyname><forenames>Jean</forenames></author><author><keyname>Payne</keyname><forenames>Michael S.</forenames></author><author><keyname>Solomon</keyname><forenames>Noam</forenames></author></authors><title>Ramsey-type theorems for lines in 3-space</title><categories>math.CO cs.CG</categories><comments>18 pages including appendix</comments><msc-class>05D10, 52C35</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove geometric Ramsey-type statements on collections of lines in 3-space.
These statements give guarantees on the size of a clique or an independent set
in (hyper)graphs induced by incidence relations between lines, points, and
reguli in 3-space. Among other things, we prove that: (1) The intersection
graph of n lines in R^3 has a clique or independent set of size Omega(n^{1/3}).
(2) Every set of n lines in R^3 has a subset of n^{1/2} lines that are all
stabbed by one line, or a subset of Omega((n/log n)^{1/5}) such that no
6-subset is stabbed by one line. (3) Every set of n lines in general position
in R^3 has a subset of Omega(n^{2/3}) lines that all lie on a regulus, or a
subset of Omega(n^{1/3}) lines such that no 4-subset is contained in a regulus.
The proofs of these statements all follow from geometric incidence bounds --
such as the Guth-Katz bound on point-line incidences in R^3 -- combined with
Tur\'an-type results on independent sets in sparse graphs and hypergraphs.
Although similar Ramsey-type statements can be proved using existing generic
algebraic frameworks, the lower bounds we get are much larger than what can be
obtained with these methods. The proofs directly yield polynomial-time
algorithms for finding subsets of the claimed size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03242</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03242</id><created>2015-12-10</created><authors><author><keyname>Mogilnykh</keyname><forenames>I. Yu.</forenames></author><author><keyname>Solov'eva</keyname><forenames>F. I.</forenames></author></authors><title>On maximum components of a class of perfect codes</title><categories>math.CO cs.IT math.IT</categories><comments>The paper is in Russian. submitted to Sibirian Electronic
  Mathematical Reports</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the paper we show the existence of a large class of extended perfect
binary codes containing maximum ij-components.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03245</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03245</id><created>2015-12-10</created><authors><author><keyname>Mogilnykh</keyname><forenames>I. Yu.</forenames></author></authors><title>On the extention of propelinear structures of Nordstrom-Robinson code to
  Hamming code</title><categories>math.CO cs.IT math.IT</categories><comments>The paper is in Russian. Submitted to Problems of Information
  Transmission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A code is called propelinear if its automorphism group contains a subgroup
that acts regularly on its codewords, which is called a propelinear structure
on the code. In the paper a classification of the propelinear structures on the
Nordstrom-Robinson code is obtained and the question of extension of these
structures to propelinear structures of the Hamming code, that contains the
Nordstrom-Robinson code. The result partially relies on a representation of all
partitions of the Hamming code into codes with parameters of Nordstrom-Robinson
code via Fano planes which is given in the paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03246</identifier>
 <datestamp>2015-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03246</id><created>2015-12-10</created><authors><author><keyname>Mnich</keyname><forenames>Matthias</forenames></author><author><keyname>R&#xf6;glin</keyname><forenames>Heiko</forenames></author><author><keyname>R&#xf6;sner</keyname><forenames>Clemens</forenames></author></authors><title>New Deterministic Algorithms for Solving Parity Games</title><categories>cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study parity games in which one of the two players controls only a small
number $k$ of nodes and the other player controls the $n-k$ other nodes of the
game. Our main result is a fixed-parameter algorithm that solves bipartite
parity games in time $k^{O(\sqrt{k})}\cdot O(n^3)$, and general parity games in
time $(p+k)^{O(\sqrt{k})} \cdot O(pnm)$, where $p$ is the number of distinct
priorities and $m$ is the number of edges. For all games with $k = o(n)$ this
improves the previously fastest algorithm by Jurdzi{\'n}ski, Paterson, and
Zwick (SICOMP 2008). We also obtain novel kernelization results and an improved
deterministic algorithm for graphs with small average degree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03251</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03251</id><created>2015-12-10</created><authors><author><keyname>Petrushin</keyname><forenames>V. N.</forenames></author><author><keyname>Nikulchev</keyname><forenames>E. V.</forenames></author><author><keyname>Korolev</keyname><forenames>D. A.</forenames></author></authors><title>Histogram Arithmetic under Uncertainty of Probability Density Function</title><categories>cs.NA stat.CO</categories><comments>10 pages</comments><journal-ref>Applied Mathematical Sciences 9(2015) 7043-7052</journal-ref><doi>10.12988/ams.2015.510644</doi><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  In this article we propose a method of performing arithmetic operations on
varia-bles with unknown distribution. The approach to the evaluation results of
arithme-tic operations can select probability intervals of the algebraic
equations and their systems solutions, of differential equations and their
systems in case of histogram evaluation of the empirical density distributions
of random parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03253</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03253</id><created>2015-12-10</created><authors><author><keyname>Loizou</keyname><forenames>Nicolas</forenames></author></authors><title>Distributionally Robust Game Theory</title><categories>cs.GT</categories><comments>MSc Thesis, Imperial College London</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The classical, complete-information two-player games assume that the problem
data (in particular the payoff matrix) is known exactly by both players. In a
now famous result, Nash has shown that any such game has an equilibrium in
mixed strategies. This result was later extended to a class of
incomplete-information two-player games by Harsanyi, who assumed that the
payoff matrix is not known exactly but rather represents a random variable that
is governed by a probability distribution known to both players.
  In 2006, Bertsimas and Aghassi proposed a new class of distribution-free
two-player games where the payoff matrix is only known to belong to a given
uncertainty set. This model relaxes the distributional assumptions of
Harsanyi's Bayesian games, and it gives rise to an alternative
distribution-free equilibrium concept.
  In this thesis we present a new model of incomplete information games without
private information in which the players use a distributionally robust
optimization approach to cope with the payoff uncertainty. With some specific
restrictions, we show that our &quot;Distributionally Robust Game&quot; constitutes a
true generalization of the three aforementioned finite games (Nash games,
Bayesian Games and Robust Games). Subsequently, we prove that the set of
equilibria of an arbitrary distributionally robust game with specified
ambiguity set can be computed as the component-wise projection of the solution
set of a multi-linear system of equations and inequalities. Finally, we
demonstrate the applicability of our new model of games and highlight its
importance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03257</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03257</id><created>2015-12-10</created><updated>2016-02-29</updated><authors><author><keyname>Hansen</keyname><forenames>A.</forenames></author><author><keyname>Montina</keyname><forenames>A.</forenames></author><author><keyname>Wolf</keyname><forenames>S.</forenames></author></authors><title>Simple algorithm for computing the communication complexity of quantum
  communication processes</title><categories>quant-ph cs.IT math.IT math.OC</categories><comments>Corrected few typos</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A two-party quantum communication process with classical inputs and outcomes
can be simulated by replacing the quantum channel with a classical one. The
minimal amount of classical communication required to reproduce the statistics
of the quantum process is called its communication complexity. In the case of
many instances simulated in parallel, the minimal communication cost per
instance is called the asymptotic communication complexity. Previously, we
reduced the computation of the asymptotic communication complexity to a convex
minimization problem. In most cases, the objective function does not have an
explicit analytic form, as the function is defined as the maximum over an
infinite set of convex functions. Therefore, the overall problem takes the form
of a minimax problem and cannot directly be solved by standard optimization
methods. In this paper, we introduce a simple algorithm to compute the
asymptotic communication complexity. For some special cases with an analytic
objective function one can employ available convex-optimization libraries. In
the tested cases our method turned out to be notably faster. Finally, using our
method we obtain 1.238 bits as a lower bound on the asymptotic communication
complexity of a noiseless quantum channel with the capacity of 1 qubit. This
improves the previous bound of 1.208 bits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03261</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03261</id><created>2015-12-10</created><updated>2015-12-12</updated><authors><author><keyname>Salvati</keyname><forenames>Daniele</forenames></author><author><keyname>Drioli</keyname><forenames>Carlo</forenames></author><author><keyname>Foresti</keyname><forenames>Gian Luca</forenames></author></authors><title>Exploiting a Geometrically Sampled Grid in the SRP-PHAT for Localization
  Improvement and Power Response Sensitivity Analysis</title><categories>cs.SD</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The steered response power phase transform (SRP-PHAT) is a beamformer method
very attractive in acoustic localization applications due to its robustness in
reverberant environments. This paper presents a spatial grid design procedure,
called the geometrically sampled grid (GSG), which aims at computing the
spatial grid by taking into account the discrete sampling of time difference of
arrival (TDOA) functions and the desired spatial resolution. A new SRP-PHAT
localization algorithm based on the GSG method is also introduced. The proposed
method exploits the intersections of the discrete hyperboloids representing the
TDOA information domain of the sensor array, and projects the whole TDOA
information on the space search grid. The GSG method thus allows to design the
sampled spatial grid which represents the best search grid for a given sensor
array, it allows to perform a sensitivity analysis of the array and to
characterize its spatial localization accuracy, and it may assist the system
designer in the reconfiguration of the array. Experimental results using both
simulated data and real recordings show that the localization accuracy is
substantially improved both for high and for low spatial resolution, and that
it is closely related to the proposed power response sensitivity measure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03267</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03267</id><created>2015-12-10</created><authors><author><keyname>Komenda</keyname><forenames>Jan</forenames></author><author><keyname>Masopust</keyname><forenames>Tom&#xe1;&#x161;</forenames></author></authors><title>On the Computation of Controllable and Coobservable Sublanguages in
  Decentralized Supervisory Control</title><categories>math.OC cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In decentralized supervisory control, several local control agents
(supervisors) cooperate to achieve a common goal expressed by a safety
specification and/or nonblockingness. Coobservability is the key condition to
achieve the specification as the resulting language of the controlled system.
One of the most important problems is thus to compute a coobservable
sublanguage of the specification. In this paper, we show how to compute a
controllable and coobservable sublanguage of the specification in a
computationally efficient way. The method is motivated by recent results in
coordination control of modular discrete-event systems, namely by the notion of
conditional decomposability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03274</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03274</id><created>2015-12-10</created><authors><author><keyname>Maggi</keyname><forenames>Lorenzo</forenames></author><author><keyname>Gkatzikis</keyname><forenames>Lazaros</forenames></author><author><keyname>Paschos</keyname><forenames>Georgios</forenames></author><author><keyname>Leguay</keyname><forenames>J&#xe9;r&#xe9;mie</forenames></author></authors><title>Adapting Caching to Audience Retention Rate: Which Video Chunk to Store?</title><categories>cs.NI</categories><comments>11 pages, under submission</comments><msc-class>68M10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rarely do users watch online contents entirely. We study how to take this
into account to improve the performance of cache systems for video-on-demand
and video-sharing platforms in terms of traffic reduction on the core network.
We exploit the notion of &quot;Audience retention rate&quot;, introduced by mainstream
online content platforms and measuring the popularity of different parts of the
same video content. We first characterize the performance limits of a cache
able to store parts of videos, when the popularity and the audience retention
rate of each video are available to the cache manager. We then relax the
assumption of known popularity and we propose a LRU (Least Recently Used) cache
replacement policy that operates on the first chunks of each video. We
characterize its performance by extending the well-known Che's approximation to
this case. We prove that, by refining the chunk granularity, the chunk-LRU
policy increases its performance. It is shown numerically that even for a small
number of chunks (N=20), the gains of chunk-LRU are still significant in
comparison to standard LRU policy that caches entire files, and they are almost
optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03280</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03280</id><created>2015-12-10</created><authors><author><keyname>Gahi</keyname><forenames>Youssef</forenames></author><author><keyname>Guennoun</keyname><forenames>Mouhcine</forenames></author><author><keyname>Guennoun</keyname><forenames>Zouhair</forenames></author><author><keyname>El-Khatib</keyname><forenames>Khalil</forenames></author></authors><title>An Encrypted Trust-Based Routing Protocol</title><categories>cs.CR</categories><doi>10.1109/ICOS.2012.6417643</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The interest in trust-based routing protocols has grown with the advancements
achieved in ad-hoc wireless networks.However, regardless of the many security
approaches and trust metrics available, trust-based routing still faces some
security challenges, especially with respect to privacy. In this paper, we
propose a novel trust-based routing protocol based on a fully homomorphic
encryption scheme. The new protocol allows nodes, which collaborate in a
dynamic environment, to evaluate their knowledge on the trustworthiness of
specific routes and securely share this knowledge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03306</identifier>
 <datestamp>2015-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03306</id><created>2015-12-10</created><authors><author><keyname>Korman</keyname><forenames>Amos</forenames><affiliation>GANG, LIAFA</affiliation></author><author><keyname>Sereni</keyname><forenames>Jean-S&#xe9;bastien</forenames><affiliation>MASCOTTE</affiliation></author><author><keyname>Viennot</keyname><forenames>Laurent</forenames><affiliation>GANG, LIAFA, LINCS</affiliation></author></authors><title>Toward more localized local algorithms: removing assumptions concerning
  global knowledge</title><categories>cs.DC cs.DS</categories><proxy>ccsd</proxy><journal-ref>Distributed Computing, Springer Verlag, 2013, 26 (5-6)</journal-ref><doi>10.1007/s00446-012-0174-8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Numerous sophisticated local algorithm were suggested in the literature for
various fundamental problems. Notable examples are the MIS and
$(\Delta+1)$-coloring algorithms by Barenboim and Elkin [6], by Kuhn [22], and
by Panconesi and Srinivasan [34], as well as the $O(\Delta 2)$-coloring
algorithm by Linial [28]. Unfortunately, most known local algorithms
(including, in particular, the aforementioned algorithms) are non-uniform, that
is, local algorithms generally use good estimations of one or more global
parameters of the network, e.g., the maximum degree $\Delta$ or the number of
nodes n. This paper provides a method for transforming a non-uniform local
algorithm into a uniform one. Furthermore , the resulting algorithm enjoys the
same asymp-totic running time as the original non-uniform algorithm. Our method
applies to a wide family of both deterministic and randomized algorithms.
Specifically, it applies to almost all state of the art non-uniform algorithms
for MIS and Maximal Matching, as well as to many results concerning the
coloring problem. (In particular, it applies to all aforementioned algorithms.)
To obtain our transformations we introduce a new distributed tool called
pruning algorithms, which we believe may be of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03315</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03315</id><created>2015-12-10</created><authors><author><keyname>Czumaj</keyname><forenames>Artur</forenames></author><author><keyname>Deligkas</keyname><forenames>Argyrios</forenames></author><author><keyname>Fasoulakis</keyname><forenames>Michail</forenames></author><author><keyname>Fearnley</keyname><forenames>John</forenames></author><author><keyname>Jurdzi&#x144;ski</keyname><forenames>Marcin</forenames></author><author><keyname>Savani</keyname><forenames>Rahul</forenames></author></authors><title>Distributed Methods for Computing Approximate Equilibria</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new, distributed method to compute approximate Nash equilibria
in bimatrix games. In contrast to previous approaches that analyze the two
payoff matrices at the same time (for example, by solving a single LP that
combines the two players payoffs), our algorithm first solves two independent
LPs, each of which is derived from one of the two payoff matrices, and then
compute approximate Nash equilibria using only limited communication between
the players.
  Our method has several applications for improved bounds for efficient
computations of approximate Nash equilibria in bimatrix games. First, it yields
a best polynomial-time algorithm for computing \emph{approximate well-supported
Nash equilibria (WSNE)}, which guarantees to find a 0.6528-WSNE in polynomial
time. Furthermore, since our algorithm solves the two LPs separately, it can be
used to improve upon the best known algorithms in the limited communication
setting: the algorithm can be implemented to obtain a randomized
expected-polynomial-time algorithm that uses poly-logarithmic communication and
finds a 0.6528-WSNE. The algorithm can also be carried out to beat the best
known bound in the query complexity setting, requiring $O(n \log n)$ payoff
queries to compute a 0.6528-WSNE. Finally, our approach can also be adapted to
provide the best known communication efficient algorithm for computing
\emph{approximate Nash equilibria}: it uses poly-logarithmic communication to
find a 0.382-approximate Nash equilibrium.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03319</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03319</id><created>2015-12-10</created><authors><author><keyname>Dragoicea</keyname><forenames>Monica</forenames></author></authors><title>Diversity and Intelligence in Multi-robot Teams</title><categories>cs.RO</categories><journal-ref>Proceedings of the 15th Int. Conference on Control Systems and
  Computer Science - CSCS15, 25-27 Mai, Bucuresti, Romania, pag. 452-457, 2005</journal-ref><doi>10.13140/RG.2.1.4177.8000</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This research proposes new tools for investigation of behavioral diversity in
multi-robot systems and a significant body of results using these tools in
simulated and real mobile robot experiments. The experiments specifically
describe a framework of defining behavior-based strategies for multi-robot
tasks as robot foraging, robot soccer and robot formation. The research focuses
specifically on motor schema-based multi-robot systems, which are an important
example of behavior-based control.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03324</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03324</id><created>2015-12-10</created><authors><author><keyname>Liu</keyname><forenames>Yunshu</forenames></author><author><keyname>Walsh</keyname><forenames>John MacLaren</forenames></author></authors><title>Mapping the Region of Entropic Vectors with Support Enumeration &amp;
  Information Geometry</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The region of entropic vectors is a convex cone that has been shown to be at
the core of many fundamental limits for problems in multiterminal data
compression, network coding, and multimedia transmission. This cone has been
shown to be non-polyhedral for four or more random variables, however its
boundary remains unknown for four or more discrete random variables. Methods
for specifying probability distributions that are in faces and on the boundary
of the convex cone are derived, then utilized to map optimized inner bounds to
the unknown part of the entropy region. The first method utilizes tools and
algorithms from abstract algebra to efficiently determine those supports for
the joint probability mass functions for four or more random variables that
can, for some appropriate set of non-zero probabilities, yield entropic vectors
in the gap between the best known inner and outer bounds. These supports are
utilized, together with numerical optimization over non-zero probabilities, to
provide inner bounds to the unknown part of the entropy region. Next,
information geometry is utilized to parameterize and study the structure of
probability distributions on these supports yielding entropic vectors in the
faces of entropy and in the unknown part of the entropy region.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03335</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03335</id><created>2015-12-10</created><authors><author><keyname>Fern&#xe1;ndez-Pend&#xe1;s</keyname><forenames>Mario</forenames></author><author><keyname>Akhmatskaya</keyname><forenames>Elena</forenames></author><author><keyname>Sanz-Serna</keyname><forenames>J. M.</forenames></author></authors><title>Adaptive multi-stage integrators for optimal energy conservation in
  molecular simulations</title><categories>cs.NA physics.comp-ph</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We introduce a new Adaptive Integration Approach (AIA) to be used in a wide
range of molecular simulations. Given a simulation problem and a step size, the
method automatically chooses the optimal scheme out of an available family of
numerical integrators. Although we focus on two-stage splitting integrators,
the idea may be used with more general families. In each instance, the
system-specific integrating scheme identified by our approach is optimal in the
sense that it provides the best conservation of energy for harmonic forces. The
AIA method has been implemented in the BCAM-modified GROMACS software package.
Numerical tests in molecular dynamics and hybrid Monte Carlo simulations of
constrained and unconstrained physical systems show that the method
successfully realises the fail-safe strategy. In all experiments, and for each
of the criteria employed, the AIA is at least as good as, and often
significantly outperforms the standard Verlet scheme, as well as fixed
parameter, optimized two-stage integrators. In particular, the sampling
efficiency found in simulations using the AIA is up to 5 times better than the
one achieved with other tested schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03338</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03338</id><created>2015-12-10</created><authors><author><keyname>Banani</keyname><forenames>S. Alireza</forenames></author><author><keyname>Eckford</keyname><forenames>Andrew W.</forenames></author><author><keyname>Adve</keyname><forenames>Raviraj S.</forenames></author></authors><title>Analyzing the Impact of Access Point Density on the Performance of
  Finite-Area Networks</title><categories>cs.IT math.IT</categories><comments>This article has been accepted for publication in a future issue of
  the journal of IEEE Transactions on Communications, but has not been fully
  edited. Content may change prior to final publication</comments><doi>10.1109/TCOMM.2015.2481887</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Assuming a network of infinite extent, several researchers have analyzed
small-cell networks using a Poisson point process (PPP) location model, leading
to simple analytic expressions. The general assumption has been that these
results apply to finite-area networks as well. However, do the results of
infinite-area networks apply to finite-area networks? In this paper, we answer
this question by obtaining an accurate approximation for the achievable
signal-to-interference-plus-noise ratio (SINR) and user capacity in the
downlink of a \textit{finite-area} network with \textit{a fixed number of}
access points (APs). The APs are uniformly distributed within the area of
interest. Our analysis shows that, crucially, the results of infinite-area
networks are very different from those for finite-area networks of
low-to-medium AP density. Comprehensive simulations are used to illustrate the
accuracy of our analysis. For practical values of signal transmit powers and AP
densities, the analytic expressions capture the behavior of the system well. As
an added benefit, the formulations developed here can be used in parametric
studies for network design. Here, the analysis is used to obtain the required
number of APs to guarantee a desired target capacity in a finite-area network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03345</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03345</id><created>2015-12-10</created><authors><author><keyname>Dumitrache</keyname><forenames>Ioan</forenames></author><author><keyname>Dragoicea</keyname><forenames>Monica</forenames></author></authors><title>Mobile Robots Adaptive Control Using Neural Networks</title><categories>cs.RO cs.SY</categories><journal-ref>Proceedings of the 13th Int. Conference on Control Systems and
  Computer Science CSCS13, Bucuresti, Romania, pp:176-181, 2001</journal-ref><doi>10.13140/RG.2.1.2801.5443</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper proposes a feed-forward control strategy for mobile robot control
that accounts for a non-linear model of the vehicle with interaction between
inputs and outputs. It is possible to include specific model uncertainties in
the dynamic model of the mobile robot in order to see how the control problem
should be addressed taking into consideration the complete dynamic mobile robot
model. By means of a neural network feed-forward controller a real non-linear
mathematical model of the vehicle can be taken into consideration. The
classical velocity control strategy can be extended using artificial neural
networks in order to compensate for the modelling uncertainties. It is possible
to develop an intelligent strategy for mobile robot control.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03351</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03351</id><created>2015-12-10</created><authors><author><keyname>Dragoicea</keyname><forenames>Monica</forenames></author><author><keyname>Dumitrache</keyname><forenames>Ioan</forenames></author><author><keyname>Constantin</keyname><forenames>Nicolae</forenames></author></authors><title>Adaptive Neural Control for Mobile Robots Autonomous Navigation</title><categories>cs.RO cs.SY</categories><comments>in Proceedings of the 7th Int. Symposium on Automatic Control and
  Computer Science SACCS 2001, Iasi, Romania, CD ISBN 973-8292-11-5, 2001</comments><doi>10.13140/RG.2.1.4112.2647</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a combined strategy for tracking a non-holonomic mobile
robot which works under certain operating conditions for system parameters and
disturbances. The strategy includes kinematic steering and velocity dynamics
learning of mobile robot system simultaneously. In the learning controller
(neural network based controller) the velocity dynamics learning control takes
part in tracking of the reference velocity trajectory by learning the inverse
function of robot dynamics while the reference velocity control input plays a
role in stabilizing the kinematic steering system to the desired reference
model of kinematic system even without using the assumption of perfect velocity
tracking.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03357</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03357</id><created>2015-12-10</created><authors><author><keyname>Dierkes</keyname><forenames>Thomas</forenames></author></authors><title>Construction of ODE systems from time series data by a highly flexible
  modelling approach</title><categories>math.NA cs.NA</categories><comments>20 pages, 7 figures</comments><msc-class>65L09, 37M10, 92C42, 92D25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a down-to-earth approach to purely data-based modelling of
unknown dynamical systems is presented. Starting from a classical, explicit ODE
formulation $y' = f(t,y)$ of a dynamical system, a method determining the
unknown right-hand side $f(t,y)$ from some trajectory data $y_{k}(t_{j})$,
possibly very sparse, is given. As illustrative examples, a semi-standard
predator-prey model is reconstructed from a data set describing the population
numbers of hares and lynxes over a period of twenty years, and a simple damped
pendulum system with a highly non-linear right-hand side is recovered from some
artificial but very sparse data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03359</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03359</id><created>2015-12-10</created><authors><author><keyname>Maheshwari</keyname><forenames>Anil</forenames></author><author><keyname>Sack</keyname><forenames>J&#xf6;rg-R&#xfc;diger</forenames></author><author><keyname>Scheffer</keyname><forenames>Christian</forenames></author></authors><title>Approximating the Integral Fr\'echet Distance</title><categories>cs.CG</categories><msc-class>F.2.2</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A pseudo-polynomial time $(1 + \varepsilon)$-approximation algorithm is
presented for computing the integral and average Fr\'{e}chet distance between
two given polygonal curves $T_1$ and $T_2$. In particular, the running time is
upper-bounded by $\mathcal{O}( \zeta^{4}n^4/\varepsilon^{2})$ where $n$ is the
complexity of $T_1$ and $T_2$ and $\zeta$ is the maximal ratio of the lengths
of any pair of segments from $T_1$ and $T_2$. The Fr\'{e}chet distance captures
the minimal cost of a continuous deformation of $T_1$ into $T_2$ and vice versa
and defines the cost of a deformation as the maximal distance between two
points that are related. The integral Fr\'{e}chet distance defines the cost of
a deformation as the integral of the distances between points that are related.
The average Fr\'{e}chet distance is defined as the integral Fr\'{e}chet
distance divided by the lengths of $T_1$ and $T_2$.
  Furthermore, we give relations between weighted shortest paths inside a
single parameter cell $C$ and the monotone free space axis of $C$. As a result
we present a simple construction of weighted shortest paths inside a parameter
cell. Additionally, such a shortest path provides an optimal solution for the
partial Fr\'{e}chet similarity of segments for all leash lengths. These two
aspects are related to each other and are of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03361</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03361</id><created>2015-12-10</created><authors><author><keyname>Bespalov</keyname><forenames>Evgeny</forenames><affiliation>Sobolev Institute of Mathematics, Novosibirsk, Russia</affiliation></author><author><keyname>Krotov</keyname><forenames>Denis</forenames><affiliation>Sobolev Institute of Mathematics, Novosibirsk, Russia</affiliation></author></authors><title>MDS codes in the Doob graphs</title><categories>math.CO cs.IT math.IT</categories><comments>In Russian, 30 pp</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Doob graph $D(m,n)$, where $m&gt;0$, is the direct product of $m$ copies of
The Shrikhande graph and $n$ copies of the complete graph $K_4$ on $4$
vertices. The Doob graph $D(m,n)$ is a distance-regular graph with the same
parameters as the Hamming graph $H(2m+n,4)$. In this paper we consider MDS
codes in Doob graphs with code distance $d \ge 3$. We prove that if $2m+n&gt;6$
and $2&lt;d&lt;2m+n$, then there are no MDS codes with code distance $d$. We
characterize all MDS codes with code distance $d \ge 3$ in Doob graphs $D(m,n)$
when $2m+n \le 6$. We characterize all MDS codes in $D(m,n)$ with code distance
$d=2m+n$ for all values of $m$ and $n$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03375</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03375</id><created>2015-12-10</created><authors><author><keyname>Jin</keyname><forenames>Peter H.</forenames></author><author><keyname>Keutzer</keyname><forenames>Kurt</forenames></author></authors><title>Convolutional Monte Carlo Rollouts in Go</title><categories>cs.LG cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we present a MCTS-based Go-playing program which uses
convolutional networks in all parts. Our method performs MCTS in batches,
explores the Monte Carlo search tree using Thompson sampling and a
convolutional network, and evaluates convnet-based rollouts on the GPU. We
achieve strong win rates against open source Go programs and attain competitive
results against state of the art convolutional net-based Go-playing programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03384</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03384</id><created>2015-12-10</created><authors><author><keyname>Han</keyname><forenames>Xintong</forenames></author><author><keyname>Singh</keyname><forenames>Bharat</forenames></author><author><keyname>Morariu</keyname><forenames>Vlad I.</forenames></author><author><keyname>Davis</keyname><forenames>Larry S.</forenames></author></authors><title>Fast Automatic Video Retrieval using Web Images</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a real-time video retrieval framework based on short text input
in which weakly labelled training samples from the web are obtained, after the
query is known. Concept discovery methods in such a setting train hundreds of
detectors at test time and apply them to every frame in the video database.
Hence, they are not practical for use in a text based video retrieval setting.
We show that an efficient visual representation for a new query can be
constructed on-line that enables matching against the test set in real-time. We
evaluate a few combinations of encoding, pooling, and matching schemes that are
efficient and find that such a system can be built with surprisingly simple and
well-known components. We are not only able to construct and apply query models
in real-time, but with the help of a re-ranking scheme, we also outperform
state-of-the-art methods by a significant margin.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03385</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03385</id><created>2015-12-10</created><authors><author><keyname>He</keyname><forenames>Kaiming</forenames></author><author><keyname>Zhang</keyname><forenames>Xiangyu</forenames></author><author><keyname>Ren</keyname><forenames>Shaoqing</forenames></author><author><keyname>Sun</keyname><forenames>Jian</forenames></author></authors><title>Deep Residual Learning for Image Recognition</title><categories>cs.CV</categories><comments>Tech report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deeper neural networks are more difficult to train. We present a residual
learning framework to ease the training of networks that are substantially
deeper than those used previously. We explicitly reformulate the layers as
learning residual functions with reference to the layer inputs, instead of
learning unreferenced functions. We provide comprehensive empirical evidence
showing that these residual networks are easier to optimize, and can gain
accuracy from considerably increased depth. On the ImageNet dataset we evaluate
residual nets with a depth of up to 152 layers---8x deeper than VGG nets but
still having lower complexity. An ensemble of these residual nets achieves
3.57% error on the ImageNet test set. This result won the 1st place on the
ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100
and 1000 layers.
  The depth of representations is of central importance for many visual
recognition tasks. Solely due to our extremely deep representations, we obtain
a 28% relative improvement on the COCO object detection dataset. Deep residual
nets are foundations of our submissions to ILSVRC &amp; COCO 2015 competitions,
where we also won the 1st places on the tasks of ImageNet detection, ImageNet
localization, COCO detection, and COCO segmentation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03390</identifier>
 <datestamp>2016-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03390</id><created>2015-12-10</created><updated>2016-03-07</updated><authors><author><keyname>Pickering</keyname><forenames>William</forenames></author><author><keyname>Szymanski</keyname><forenames>Boleslaw K.</forenames></author><author><keyname>Lim</keyname><forenames>Chjan</forenames></author></authors><title>Opinion Diversity and the Stability of Social Systems: Implications from
  a Model of Social Influence</title><categories>physics.soc-ph cs.SI</categories><comments>8 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The naming game has become an archetype for linguistic evolution and
mathematical social behavioral analysis. In the model presented here, there are
$N$ individuals and $K$ words. Our contribution is developing a robust method
that handles the case when $K = O(N)$. The initial condition plays a crucial
role in the ordering of the system. We find that the system with high Shannon
entropy has a higher consensus time and a lower critical fraction of zealots
compared to low entropy states. We also show that the critical number of
committed agents decreases with the number of opinions, and grows with the
community size for each word. These results complement earlier published
conclusions that diversity of opinion is essential for evolution; without it,
the system stagnates in the status quo. In contrast, our results suggest that
committed minorities can more easily conquer highly diverse systems, showing
them to be inherently unstable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03393</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03393</id><created>2015-12-10</created><authors><author><keyname>Derksen</keyname><forenames>Harm</forenames></author><author><keyname>Makam</keyname><forenames>Visu</forenames></author></authors><title>Polynomial degree bounds for matrix semi-invariants</title><categories>math.RT cs.CC</categories><comments>16 pages</comments><msc-class>13A50 (Primary), 14L24, 16G20 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the left-right action of $\operatorname{SL}_n \times
\operatorname{SL}_n$ on $m$-tuples of $n \times n$ matrices with entries in an
infinite field $K$. We show that invariants of degree $n^2- n$ define the null
cone. Consequently, invariants of degree $\leq n^6$ generate the ring of
invariants if $\operatorname{char}(K)=0$. We also prove that for $m \gg 0$,
invariants of degree at least $n\lfloor \sqrt{n+1}\rfloor$ are required to
define the null cone. We generalize our results to matrix invariants of
$m$-tuples of $p\times q$ matrices, and to rings of semi-invariants for
quivers. For the proofs, we use new techniques such as the regularity lemma by
Ivanyos, Qiao and Subrahmanyam, and the concavity property of the tensor
blow-ups of matrix spaces. We will discuss several applications to algebraic
complexity theory, such as a deterministic polynomial time algorithm for
non-commutative rational identity testing, and the existence of small
division-free formulas for non-commutative polynomials.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03396</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03396</id><created>2015-12-10</created><authors><author><keyname>Ma</keyname><forenames>Yuting</forenames></author><author><keyname>Zheng</keyname><forenames>Tian</forenames></author></authors><title>Boosted Sparse Non-linear Distance Metric Learning</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a boosting-based solution addressing metric learning
problems for high-dimensional data. Distance measures have been used as natural
measures of (dis)similarity and served as the foundation of various learning
methods. The efficiency of distance-based learning methods heavily depends on
the chosen distance metric. With increasing dimensionality and complexity of
data, however, traditional metric learning methods suffer from poor scalability
and the limitation due to linearity as the true signals are usually embedded
within a low-dimensional nonlinear subspace. In this paper, we propose a
nonlinear sparse metric learning algorithm via boosting. We restructure a
global optimization problem into a forward stage-wise learning of weak learners
based on a rank-one decomposition of the weight matrix in the Mahalanobis
distance metric. A gradient boosting algorithm is devised to obtain a sparse
rank-one update of the weight matrix at each step. Nonlinear features are
learned by a hierarchical expansion of interactions incorporated within the
boosting algorithm. Meanwhile, an early stopping rule is imposed to control the
overall complexity of the learned metric. As a result, our approach guarantees
three desirable properties of the final metric: positive semi-definiteness, low
rank and element-wise sparsity. Numerical experiments show that our learning
model compares favorably with the state-of-the-art methods in the current
literature of metric learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03419</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03419</id><created>2015-12-10</created><authors><author><keyname>DeDeo</keyname><forenames>Simon</forenames></author></authors><title>Major Transitions in Political Order</title><categories>physics.soc-ph cs.MA cs.SI q-bio.PE</categories><comments>26 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present three major transitions that occur on the way to the elaborate and
diverse societies of the modern era. Our account links the worlds of social
animals such as pigtail macaques and monk parakeets to examples from human
history, including 18th Century London and the contemporary online phenomenon
of Wikipedia. From the first awareness and use of group-level social facts to
the emergence of norms and their self-assembly into normative bundles, each
transition represents a new relationship between the individual and the group.
At the center of this relationship is the use of coarse-grained information
gained via lossy compression. The role of top-down causation in the origin of
society parallels that conjectured to occur in the origin and evolution of life
itself.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03423</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03423</id><created>2015-12-09</created><authors><author><keyname>Orimaye</keyname><forenames>Sylvester Olubolu</forenames></author><author><keyname>Leong</keyname><forenames>Foo Chuan</forenames></author><author><keyname>Lee</keyname><forenames>Chen Hui</forenames></author><author><keyname>Ng</keyname><forenames>Eddy Cheng Han</forenames></author></authors><title>Predicting proximity with ambient mobile sensors for non-invasive health
  diagnostics</title><categories>cs.CY cs.LG</categories><comments>Accepted and presented at the 12th IEEE Malaysia International
  Conference on Communications, 23-25 November, 2015, Kuching, Sarawak,
  Malaysia</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern smart phones are becoming helpful in the areas of Internet-Of-Things
(IoT) and ambient health intelligence. By learning data from several mobile
sensors, we detect nearness of the human body to a mobile device in a
three-dimensional space with no physical contact with the device for
non-invasive health diagnostics. We show that the human body generates wave
patterns that interact with other naturally occurring ambient signals that
could be measured by mobile sensors, such as, temperature, humidity, magnetic
field, acceleration, gravity, and light. This interaction consequentially
alters the patterns of the naturally occurring signals, and thus, exhibits
characteristics that could be learned to predict the nearness of the human body
to a mobile device, hence provide diagnostic information for medical
practitioners. Our prediction technique achieved 88.75% accuracy and 88.3%
specificity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03424</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03424</id><created>2015-12-10</created><authors><author><keyname>Rezazadegan</keyname><forenames>Fahimeh</forenames></author><author><keyname>Shirazi</keyname><forenames>Sareh</forenames></author><author><keyname>Milford</keyname><forenames>Michael</forenames></author><author><keyname>Upcroft</keyname><forenames>Ben</forenames></author></authors><title>Evaluation of Object Detection Proposals Under Condition Variations</title><categories>cs.CV</categories><comments>2 pages, 6 figures, CVPR Workshop, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Object detection is a fundamental task in many computer vision applications,
therefore the importance of evaluating the quality of object detection is well
acknowledged in this domain. This process gives insight into the capabilities
of methods in handling environmental changes. In this paper, a new method for
object detection is introduced that combines the Selective Search and
EdgeBoxes. We tested these three methods under environmental variations. Our
experiments demonstrate the outperformance of the combination method under
illumination and view point variations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03426</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03426</id><created>2015-12-10</created><authors><author><keyname>Lavery</keyname><forenames>Domanic</forenames></author><author><keyname>Maher</keyname><forenames>Robert</forenames></author><author><keyname>Millar</keyname><forenames>David</forenames></author><author><keyname>Alvarado</keyname><forenames>Alex</forenames></author><author><keyname>Savory</keyname><forenames>Seb J.</forenames></author><author><keyname>Bayvel</keyname><forenames>Polina</forenames></author></authors><title>Why compensating fibre nonlinearity will never meet capacity demands</title><categories>physics.optics cs.IT math.IT</categories><comments>4 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current research efforts are focussed on overcoming the apparent limits of
communication in single mode optical fibre resulting from distortion due to
fibre nonlinearity. It has been experimentally demonstrated that this Kerr
nonlinearity limit is not a fundamental limit; thus it is pertinent to review
where the fundamental limits of optical communications lie, and direct future
research on this basis. This paper details recently presented results. The work
herein briefly reviews the intrinsic limits of optical communication over
standard single mode optical fibre (SMF), and shows that the empirical limits
of silica fibre power handling and transceiver design both introduce a
practical upper bound to the capacity of communication using SMF, on the order
of 1 Pbit/s. Transmission rates exceeding 1 Pbit/s are shown to be possible,
however, with currently available optical fibres, attempts to transmit beyond
this rate by simply increasing optical power will lead to an asymptotically
zero fractional increase in capacity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03440</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03440</id><created>2015-12-10</created><authors><author><keyname>Mediwaththe</keyname><forenames>Chathurika P.</forenames></author><author><keyname>Stephens</keyname><forenames>Edward R.</forenames></author><author><keyname>Smith</keyname><forenames>David B.</forenames></author><author><keyname>Mahanti</keyname><forenames>Anirban</forenames></author></authors><title>Competitive Energy Trading Framework for Demand-side Management in
  Neighborhood Area Networks</title><categories>cs.GT</categories><comments>10 pages, 4 figures, revision under review for IEEE Transactions on
  Control of Network Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we analyze three energy trading systems for demand-side
management of a residential neighborhood area network using a community energy
storage (CES) device and consumer-owned photovoltaic (PV) systems. We consider
a fully-competitive CES operator in a non-cooperative Stackelberg game, a
benevolent CES operator that has socially favorable regulations with
competitive consumers, and a centralized cooperative CES operator that
minimizes the total community energy cost. The two former game-theoretic
systems consider that the CES operator first maximizes their revenue by setting
a pricing signal and trading energy with the grid. Then the users with PV
panels play a non-cooperative repeated game following the actions of the CES
operator to trade energy with the CES device and the grid to minimize energy
costs. The centralized CES operator cooperates with the participating users to
minimize the total community energy cost without appropriate incentives. The
non-cooperative Stackelberg game with the fully-competitive CES operator has a
unique Stackelberg equilibrium at which the CES operator maximizes revenue and
users obtain unique Pareto-optimal Nash equilibrium CES energy trading
strategies while ensuring the balance between social costs of the two entities.
Both CES operator and participating users are simultaneously benefited in the
fully-competitive system and the system requires less energy storage resources
to give optimal and peak economic benefits than the other two alternative CES
operator models. Numerical evaluations prove that the benefits gained thorough
the fully-competitive system are robust to imperfect forecasts of PV power
generation and user demand of the following day.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03443</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03443</id><created>2015-12-10</created><authors><author><keyname>Kumar</keyname><forenames>Abhimanu</forenames></author><author><keyname>Palakodety</keyname><forenames>Shriphani</forenames></author><author><keyname>Wang</keyname><forenames>Chong</forenames></author><author><keyname>Rose</keyname><forenames>Carolyn P.</forenames></author><author><keyname>Xing</keyname><forenames>Eric P.</forenames></author><author><keyname>Wen</keyname><forenames>Miaomiao</forenames></author></authors><title>Scalable Modeling of Conversational-role based Self-presentation
  Characteristics in Large Online Forums</title><categories>stat.ML cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online discussion forums are complex webs of overlapping subcommunities
(macrolevel structure, across threads) in which users enact different roles
depending on which subcommunity they are participating in within a particular
time point (microlevel structure, within threads). This sub-network structure
is implicit in massive collections of threads. To uncover this structure, we
develop a scalable algorithm based on stochastic variational inference and
leverage topic models (LDA) along with mixed membership stochastic block (MMSB)
models. We evaluate our model on three large-scale datasets,
Cancer-ThreadStarter (22K users and 14.4K threads), Cancer-NameMention(15.1K
users and 12.4K threads) and StackOverFlow (1.19 million users and 4.55 million
threads). Qualitatively, we demonstrate that our model can provide useful
explanations of microlevel and macrolevel user presentation characteristics in
different communities using the topics discovered from posts. Quantitatively,
we show that our model does better than MMSB and LDA in predicting user reply
structure within threads. In addition, we demonstrate via synthetic data
experiments that the proposed active sub-network discovery model is stable and
recovers the original parameters of the experimental setup with high
probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03452</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03452</id><created>2015-12-10</created><authors><author><keyname>Dawy</keyname><forenames>Zaher</forenames></author><author><keyname>Saad</keyname><forenames>Walid</forenames></author><author><keyname>Ghosh</keyname><forenames>Arunabha</forenames></author><author><keyname>Andrews</keyname><forenames>Jeffrey G.</forenames></author><author><keyname>Yaacoub</keyname><forenames>Elias</forenames></author></authors><title>Towards Massive Machine Type Cellular Communications</title><categories>cs.IT math.IT</categories><comments>accepted and to appear in the IEEE Wireless Communications Magazine</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cellular networks have been engineered and optimized to carrying
ever-increasing amounts of mobile data, but over the last few years, a new
class of applications based on machine-centric communications has begun to
emerge. Automated devices such as sensors, tracking devices, and meters - often
referred to as machine-to-machine (M2M) or machine-type communications (MTC) -
introduce an attractive revenue stream for mobile network operators, if a
massive number of them can be efficiently supported. The novel technical
challenges posed by MTC applications include increased overhead and control
signaling as well as diverse application-specific constraints such as ultra-low
complexity, extreme energy efficiency, critical timing, and continuous data
intensive uploading. This paper explains the new requirements and challenges
that large-scale MTC applications introduce, and provides a survey on key
techniques for overcoming them. We focus on the potential of 4.5G and 5G
networks to serve both the high data rate needs of conventional human-type
communications (HTC) subscribers and the forecasted billions of new MTC
devices. We also opine on attractive economic models that will enable this new
class of cellular subscribers to grow to its full potential.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03460</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03460</id><created>2015-12-10</created><authors><author><keyname>Yang</keyname><forenames>Yezhou</forenames></author><author><keyname>Li</keyname><forenames>Yi</forenames></author><author><keyname>Fermuller</keyname><forenames>Cornelia</forenames></author><author><keyname>Aloimonos</keyname><forenames>Yiannis</forenames></author></authors><title>Neural Self Talk: Image Understanding via Continuous Questioning and
  Answering</title><categories>cs.CV cs.CL cs.RO</categories><acm-class>I.2.10</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider the problem of continuously discovering image
contents by actively asking image based questions and subsequently answering
the questions being asked. The key components include a Visual Question
Generation (VQG) module and a Visual Question Answering module, in which
Recurrent Neural Networks (RNN) and Convolutional Neural Network (CNN) are
used. Given a dataset that contains images, questions and their answers, both
modules are trained at the same time, with the difference being VQG uses the
images as input and the corresponding questions as output, while VQA uses
images and questions as input and the corresponding answers as output. We
evaluate the self talk process subjectively using Amazon Mechanical Turk, which
show effectiveness of the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03464</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03464</id><created>2015-12-10</created><authors><author><keyname>Parikh</keyname><forenames>Anup</forenames></author><author><keyname>Kamalapurkar</keyname><forenames>Rushikesh</forenames></author><author><keyname>Dixon</keyname><forenames>Warren E.</forenames></author></authors><title>Integral Concurrent Learning: Adaptive Control with Parameter
  Convergence without PE or State Derivatives</title><categories>cs.SY</categories><comments>4 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Concurrent learning is a recently developed adaptive update scheme that can
be used to guarantee parameter convergence without requiring persistent
excitation. However, this technique requires knowledge of state derivatives,
which are usually not directly sensed and therefore must be estimated. A novel
integral concurrent learning method is developed in this paper that removes the
need to estimate state derivatives while maintaining parameter convergence
properties. A Monte Carlo simulation illustrates improved robustness to noise
compared to the traditional derivative formulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03465</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03465</id><created>2015-12-10</created><authors><author><keyname>Shalaby</keyname><forenames>Walid</forenames></author><author><keyname>Zadrozny</keyname><forenames>Wlodek</forenames></author></authors><title>Measuring Semantic Relatedness using Mined Semantic Analysis</title><categories>cs.CL</categories><comments>7 pages</comments><acm-class>H.3.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mined Semantic Analysis (MSA) is a novel distributional semantics approach
which employs data mining techniques. MSA embraces knowledge-driven analysis of
natural languages. It uncovers implicit relations between concepts by mining
for their associations in target encyclopedic corpora. MSA exploits not only
target corpus content but also its knowledge graph (e.g., &quot;See also&quot; link graph
of Wikipedia). Empirical results show competitive performance of MSA compared
to prior state-of-the-art methods for measuring semantic relatedness on
benchmark data sets. Additionally, we introduce the first analytical study to
examine statistical significance of results reported by different semantic
relatedness methods. Our study shows that, top performing results could be
statistically equivalent though mathematically different. The study positions
MSA as one of state-of-the-art methods for measuring semantic relatedness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03466</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03466</id><created>2015-12-10</created><authors><author><keyname>Santana</keyname><forenames>Roberto</forenames></author><author><keyname>Mendiburu</keyname><forenames>Alexander</forenames></author><author><keyname>Lozano</keyname><forenames>Jose A.</forenames></author></authors><title>Computing factorized approximations of Pareto-fronts using
  mNM-landscapes and Boltzmann distributions</title><categories>cs.NE</categories><comments>Accepted for CAEPIA-2015 conference, Albacete, Spain. 11 pages, 3
  figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  NM-landscapes have been recently introduced as a class of tunable rugged
models. They are a subset of the general interaction models where all the
interactions are of order less or equal $M$. The Boltzmann distribution has
been extensively applied in single-objective evolutionary algorithms to
implement selection and study the theoretical properties of model-building
algorithms. In this paper we propose the combination of the multi-objective
NM-landscape model and the Boltzmann distribution to obtain Pareto-front
approximations. We investigate the joint effect of the parameters of the
NM-landscapes and the probabilistic factorizations in the shape of the Pareto
front approximations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03473</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03473</id><created>2015-12-10</created><authors><author><keyname>Stein</keyname><forenames>Manuel</forenames></author><author><keyname>Nossek</keyname><forenames>Josef A.</forenames></author><author><keyname>Barb&#xe9;</keyname><forenames>Kurt</forenames></author></authors><title>Fisher Information Bounds with Applications in Nonlinear Learning,
  Compression and Inference</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem how to derive a generic lower bound for the Fisher information
measure is considered. We review a recent approach by two examples and identify
a connection between the construction of strong Fisher information bounds and
the sufficient statistics of the underlying system model. In order to present
the problem of such information bounds within a broad scope, we discuss the
properties of the Fisher information measure for distributions belonging to the
exponential family. Under this restriction, we establish an identity connecting
Fisher information, the natural parameters and the sufficient statistics of the
system model. Replacing an arbitrary system model by an equivalent distribution
within the exponential family, we then derive a general lower bound for the
Fisher information measure. With the optimum estimation theoretic model
matching rule we show how to obtain a strong version of the information bound.
We then demonstrate different applications of the proposed conservative
likelihood framework and the derived Fisher information bound. In particular,
we discuss how to determine the minimum guaranteed inference capability of a
memoryless system with unknown statistical output model and show how to achieve
this pessimistic performance assessment with a root-n consistent estimator
operating on a nonlinear compressed version of the observed data. Finally, we
identify that the derived conservative maximum-likelihood algorithm can be
formulated as a special version of Hansen's generalized method of moments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03476</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03476</id><created>2015-12-10</created><updated>2015-12-21</updated><authors><author><keyname>Guimarans</keyname><forenames>Daniel</forenames></author><author><keyname>Harabor</keyname><forenames>Daniel</forenames></author><author><keyname>van Hentenryck</keyname><forenames>Pascal</forenames></author></authors><title>Simulation and Analysis of Container Freight Train Operations at Port
  Botany</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over two million containers crossed the docks at Sydney's Port Botany in
2011/12; a figure that is forecast increase more than threefold by the end of
the next decade. To cope with such large growth in volumes the NSW Government
plans to double rail mode share at the port by the year 2020. Conventional
wisdom from industry and the media says that existing infrastructure cannot
handle such volumes. In this paper we use a combination of data analytics and
simulation to examine operations at the port and evaluate the efficacy of
current infrastructure to handle projected growth in volumes. Contrary to
conventional wisdom we find that current rail resources appear distinctly
under-utilised. Moreover: (i) the peak rail capacity of Port Botany is 1.78
million TEU per annum; over six times higher than 2011/12 rail volumes; (ii)
there are no infrastructural impediments to the achievement of peak rail
capacity; (iii) operational changes, not infrastructural investment, are the
key to unlocking the potential of the port; (iv) Port Botany is well positioned
to handle projected increases in container volumes over the next decade and
beyond, including the 28% rail mode share target established by the New South
Wales State Government.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03484</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03484</id><created>2015-12-10</created><authors><author><keyname>Mamageishvili</keyname><forenames>Akaki</forenames></author><author><keyname>Penna</keyname><forenames>Paolo</forenames></author></authors><title>Tighter Bounds on the Inefficiency Ratio of Stable Equilibria in Load
  Balancing Games</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the inefficiency ratio of stable equilibria in load
balancing games introduced by Asadpour and Saberi [3]. We prove tighter lower
and upper bounds of 7/6 and 4/3, respectively. This improves over the best
known bounds in problem (19/18 and 3/2, respectively). Equivalently, the
results apply to the question of how well the optimum for the $L_2$ -norm can
approximate the $L_{\infty}$-norm (makespan) in identical machines scheduling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03485</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03485</id><created>2015-12-10</created><authors><author><keyname>Tushar</keyname><forenames>Wayes</forenames></author><author><keyname>Yuen</keyname><forenames>Chau</forenames></author><author><keyname>Smith</keyname><forenames>David</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Price discrimination for energy trading in smart grid: A game theoretic
  approach</title><categories>cs.SY cs.GT</categories><comments>Journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pricing schemes are an important smart grid feature to affect typical energy
usage behavior of energy users (EUs). However, most existing schemes use the
assumption that a buyer pays the same price per unit of energy to all suppliers
at any particular time when energy is bought. By contrast, here a discriminate
pricing technique using game theory is studied. A cake cutting game is
investigated, in which participating EUs in a smart community decide on the
price per unit of energy to charge a shared facility controller (SFC) in order
to sell surplus energy. The focus is to study fairness criteria to maximize sum
benefits to EUs and ensure an envy-free energy trading market. A benefit
function is designed that leverages generation of discriminate pricing by each
EU, according to the amount of surplus energy that an EU trades with the SFC
and the EU's sensitivity to price. It is shown that the game possesses a
socially optimal, and hence also Pareto optimal, solution. Further, an
algorithm that can be implemented by each EU in a distributed manner to reach
the optimal solution is proposed. Numerical case studies are given that
demonstrate beneficial properties of the scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03487</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03487</id><created>2015-12-10</created><authors><author><keyname>Boyle</keyname><forenames>Peter</forenames></author><author><keyname>Yamaguchi</keyname><forenames>Azusa</forenames></author><author><keyname>Cossu</keyname><forenames>Guido</forenames></author><author><keyname>Portelli</keyname><forenames>Antonin</forenames></author></authors><title>Grid: A next generation data parallel C++ QCD library</title><categories>hep-lat cs.DC cs.MS</categories><comments>14 pages, Lattice 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this proceedings we discuss the motivation, implementation details, and
performance of a new physics code base called Grid. It is intended to be more
performant, more general, but similar in spirit to QDP++\cite{QDP}. Our
approach is to engineer the basic type system to be consistently fast, rather
than bolt on a few optimised routines, and we are attempt to write all our
optimised routines directly in the Grid framework. It is hoped this will
deliver best known practice performance across the next generation of
supercomputers, which will provide programming challenges to traditional scalar
codes.
  We illustrate the programming patterns used to implement our goals, and
advances in productivity that have been enabled by using new features in C++11.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03489</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03489</id><created>2015-12-10</created><authors><author><keyname>Preciado</keyname><forenames>Victor M.</forenames></author><author><keyname>Rahimian</keyname><forenames>M. Amin</forenames></author></authors><title>Moment-Based Spectral Analysis of Random Graphs with Given Expected
  Degrees</title><categories>math.ST cs.SI math.PR physics.soc-ph stat.AP stat.TH</categories><msc-class>05C80, 60B20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we analyze the limiting spectral distribution of the adjacency
matrix of a random graph ensemble, proposed by Chung and Lu, in which a given
expected degree sequence $\bar{w}_n^{^{T}} = (w^{(n)}_1,\ldots,w^{(n)}_n)$ is
prescribed on the ensemble. Let $\mathbf{a}_{i,j} =1$ if there is an edge
between the nodes $\{i,j\}$ and zero otherwise, and consider the normalized
random adjacency matrix of the graph ensemble: $\mathbf{A}_n$ $=$ $
[\mathbf{a}_{i,j}/\sqrt{n}]_{i,j=1}^{n}$. The empirical spectral distribution
of $\mathbf{A}_n$ denoted by $\mathbf{F}_n(\mathord{\cdot})$ is the empirical
measure putting a mass $1/n$ at each of the $n$ real eigenvalues of the
symmetric matrix $\mathbf{A}_n$. Under some technical conditions on the
expected degrees sequence, we show that with probability one,
$\mathbf{F}_n(\mathord{\cdot})$ converges weakly to a deterministic
distribution $F(\mathord{\cdot})$. Furthermore, we fully characterize this
distribution by providing explicit expressions for the moments of
$F(\mathord{\cdot})$
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03497</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03497</id><created>2015-12-10</created><authors><author><keyname>Ponomarenko-Timofeev</keyname><forenames>Aleksei</forenames></author><author><keyname>Pyattaev</keyname><forenames>Alexander</forenames></author><author><keyname>Andreev</keyname><forenames>Sergey</forenames></author><author><keyname>Koucheryavy</keyname><forenames>Yevgeni</forenames></author><author><keyname>Mueck</keyname><forenames>Markus</forenames></author><author><keyname>Karls</keyname><forenames>Ingolf</forenames></author></authors><title>Highly Dynamic Spectrum Management within Licensed Shared Access
  Regulatory Framework</title><categories>cs.NI</categories><comments>9 pages, 5 figures, 1 table, 15 references, to appear in IEEE
  Communications Magazine, Open Call</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Historical fragmentation in spectrum access models accentuates the need for
novel concepts that allow for efficient sharing of already available but
underutilized spectrum. The emerging Licensed Shared Access (LSA) regulatory
framework is expected to enable more advanced spectrum sharing between a
limited number of users while guaranteeing their much needed interference
protection. However, the ultimate benefits of LSA may in practice be
constrained by space-time availability of the LSA bands. Hence, more dynamic
LSA spectrum management is required to leverage such real-time variability and
sustain reliability when e.g., the original spectrum user suddenly revokes the
previously granted frequency bands as they are required again. In this article,
we maintain the vision of highly dynamic LSA architecture and rigorously study
its future potential: from reviewing market opportunities and discussing
available technology implementations to conducting performance evaluation of
LSA dynamics and outlining the standardization landscape. Our investigations
are based on a comprehensive system-level evaluation framework, which has been
specifically designed to assess highly dynamic LSA deployments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03498</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03498</id><created>2015-12-10</created><authors><author><keyname>Gahi</keyname><forenames>Youssef</forenames></author><author><keyname>Guennoun</keyname><forenames>Mouhcine</forenames></author><author><keyname>El-Khatib</keyname><forenames>Khalil</forenames></author></authors><title>A Secure Database System using Homomorphic Encryption Schemes</title><categories>cs.CR</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Cloud computing emerges as an attractive solution that can be delegated to
store and process confidential data. However, several security risks are
encountered with such a system as the securely encrypted data should be
decrypted before processing them. Therefore, the decrypted data is susceptible
to reading and alterations. As a result, processing encrypted data has been a
research subject since the publication of the RSA encryption scheme in 1978. In
this paper we present a relational database system based on homomorphic
encryption schemes to preserve the integrity and confidentiality of the data.
Our system executes SQL queries over encrypted data. We tested our system with
a recently developed homomorphic scheme that enables the execution of
arithmetic operations on ciphertexts. We show that the proposed system performs
accurate SQL operations, yet its performance discourages a practical
implementation of this system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03501</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03501</id><created>2015-12-10</created><authors><author><keyname>Rizoiu</keyname><forenames>Marian-Andrei</forenames></author><author><keyname>Velcin</keyname><forenames>Julien</forenames></author><author><keyname>Bonnevay</keyname><forenames>St&#xe9;phane</forenames></author><author><keyname>Lallich</keyname><forenames>St&#xe9;phane</forenames></author></authors><title>ClusPath: A Temporal-driven Clustering to Infer Typical Evolution Paths</title><categories>cs.DB cs.DS</categories><doi>10.1007/s10618-015-0445-7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose ClusPath, a novel algorithm for detecting general evolution
tendencies in a population of entities. We show how abstract notions, such as
the Swedish socio-economical model (in a political dataset) or the companies
fiscal optimization (in an economical dataset) can be inferred from low-level
descriptive features. Such high-level regularities in the evolution of entities
are detected by combining spatial and temporal features into a spatio-temporal
dissimilarity measure and using semi-supervised clustering techniques. The
relations between the evolution phases are modeled using a graph structure,
inferred simultaneously with the partition, by using a &quot;slow changing world&quot;
assumption. The idea is to ensure a smooth passage for entities along their
evolution paths, which catches the long-term trends in the dataset.
Additionally, we also provide a method, based on an evolutionary algorithm, to
tune the parameters of ClusPath to new, unseen datasets. This method assesses
the fitness of a solution using four opposed quality measures and proposes a
balanced compromise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03503</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03503</id><created>2015-12-10</created><authors><author><keyname>Jeannerod</keyname><forenames>Claude-Pierre</forenames></author><author><keyname>Neiger</keyname><forenames>Vincent</forenames></author><author><keyname>Schost</keyname><forenames>&#xc9;ric</forenames></author><author><keyname>Villard</keyname><forenames>Gilles</forenames></author></authors><title>Computing minimal interpolation bases</title><categories>cs.SC cs.IT math.IT</categories><comments>57 pages, 14 figures (problems and algorithms), uses elsarticle.cls</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of computing univariate polynomial matrices over a
field that represent minimal solution bases for a general interpolation
problem, some forms of which are the vector M-Pad\'e approximation problem in
[Van Barel and Bultheel, Numerical Algorithms 3, 1992] and the rational
interpolation problem in [Beckermann and Labahn, SIAM J. Matrix Anal. Appl. 22,
2000]. Particular instances of this problem include the bivariate interpolation
steps of Guruswami-Sudan hard-decision and K\&quot;otter-Vardy soft-decision
decodings of Reed-Solomon codes, the multivariate interpolation step of
list-decoding of folded Reed-Solomon codes, and Hermite-Pad\'e approximation.
  In the mentioned references, the problem is solved using iterative algorithms
based on recurrence relations. Here, we discuss a fast, divide-and-conquer
version of this recurrence, taking advantage of fast matrix computations over
the scalars and over the polynomials. This new algorithm is deterministic, and
for computing shifted minimal bases of relations between $m$ vectors of size
$\sigma$ it uses $O~( m^{\omega-1} (\sigma + |s|) )$ field operations, where
$\omega$ is the exponent of matrix multiplication, and $|s|$ is the sum of the
entries of the input shift $s$, with $\min(s) = 0$. This complexity bound
improves in particular on earlier algorithms in the case of bivariate
interpolation for soft decoding, while matching fastest existing algorithms for
simultaneous Hermite-Pad\'e approximation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03512</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03512</id><created>2015-12-10</created><authors><author><keyname>Divakaran</keyname><forenames>Srikrishnan</forenames></author></authors><title>A Fast Heuristic for Exact String Matching</title><categories>cs.DS</categories><comments>arXiv admin note: substantial text overlap with arXiv:1509.09228</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a pattern string $P$ of length $n$ consisting of $\delta$ distinct
characters and a query string $T$ of length $m$, where the characters of $P$
and $T$ are drawn from an alphabet $\Sigma$ of size $\Delta$, the {\em exact
string matching} problem consists of finding all occurrences of $P$ in $T$. For
this problem, we present a randomized heuristic that in $O(n\delta)$ time
preprocesses $P$ to identify $sparse(P)$, a rarely occurring substring of $P$,
and then use it to find all occurrences of $P$ in $T$ efficiently. This
heuristic has an expected search time of $O( \frac{m}{min(|sparse(P)|,
\Delta)})$, where $|sparse(P)|$ is at least $\delta$. We also show that for a
pattern string $P$ whose characters are chosen uniformly at random from an
alphabet of size $\Delta$, $E[|sparse(P)|]$ is $\Omega(\Delta log
(\frac{2\Delta}{2\Delta-\delta}))$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03516</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03516</id><created>2015-12-10</created><authors><author><keyname>Rao</keyname><forenames>A. M. Mohan</forenames></author></authors><title>Subsumptive reflection in SNOMED CT: a large description logic-based
  terminology for diagnosis</title><categories>cs.AI</categories><comments>8 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Description logic (DL) based biomedical terminology (SNOMED CT) is used
routinely in medical practice. However, diagnostic inference using such
terminology is precluded by its complexity. Here we propose a model that
simplifies these inferential components. We propose three concepts that
classify clinical features and examined their effect on inference using SNOMED
CT. We used PAIRS (Physician Assistant Artificial Intelligence Reference
System) database (1964 findings for 485 disorders, 18 397 disease feature
links) for our analysis. We also use a 50-million medical word corpus for
estimating the vectors of disease-feature links. Our major results are 10% of
finding-disorder links are concomitant in both assertion and negation where as
90% are either concomitant in assertion or negation. Logical implications of
PAIRS data on SNOMED CT include 70% of the links do not share any common system
while 18% share organ and 12% share both system and organ. Applications of
these principles for inference are discussed and suggestions are made for
deriving a diagnostic process using SNOMED CT. Limitations of these processes
and suggestions for improvements are also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03518</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03518</id><created>2015-12-10</created><authors><author><keyname>Zhou</keyname><forenames>Zirui</forenames></author><author><keyname>So</keyname><forenames>Anthony Man-Cho</forenames></author></authors><title>A Unified Approach to Error Bounds for Structured Convex Optimization
  Problems</title><categories>math.OC cs.LG math.NA stat.ML</categories><comments>32 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Error bounds, which refer to inequalities that bound the distance of vectors
in a test set to a given set by a residual function, have proven to be
extremely useful in analyzing the convergence rates of a host of iterative
methods for solving optimization problems. In this paper, we present a new
framework for establishing error bounds for a class of structured convex
optimization problems, in which the objective function is the sum of a smooth
convex function and a general closed proper convex function. Such a class
encapsulates not only fairly general constrained minimization problems but also
various regularized loss minimization formulations in machine learning, signal
processing, and statistics. Using our framework, we show that a number of
existing error bound results can be recovered in a unified and transparent
manner. To further demonstrate the power of our framework, we apply it to a
class of nuclear-norm regularized loss minimization problems and establish a
new error bound for this class under a strict complementarity-type regularity
condition. We then complement this result by constructing an example to show
that the said error bound could fail to hold without the regularity condition.
Consequently, we obtain a rather complete answer to a question raised by Tseng.
We believe that our approach will find further applications in the study of
error bounds for structured convex optimization problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03521</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03521</id><created>2015-12-10</created><authors><author><keyname>Malaney</keyname><forenames>Robert</forenames></author></authors><title>The Quantum Car</title><categories>quant-ph cs.CR cs.IT math.IT</categories><comments>4 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  I explore the use of quantum information as a security enabler for the future
driverless vehicle. Specifically, I investigate the role combined classical and
quantum information can have on the most important characteristic of the
driverless vehicle paradigm - the vehicle location. By using
information-theoretic verification frameworks, coupled with emerging
quantum-based location-verification procedures, I show how vehicle positions
can be authenticated with a probability of error simply not attainable in
classical-only networks. I also discuss how other quantum applications can be
seamlessly encapsulated within the same vehicular communication infrastructure
required for location verification. The two technology enablers required for
the driverless quantum vehicle are an increase in current quantum memory
timescales (likely) and wide-scale deployment of classical vehicular
communication infrastructure (underway). I argue the enhanced safety features
delivered by the `Quantum Car' mean its eventual deployment is inevitable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03523</identifier>
 <datestamp>2015-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03523</id><created>2015-12-11</created><updated>2015-12-16</updated><authors><author><keyname>Rizoiu</keyname><forenames>Marian-Andrei</forenames></author><author><keyname>Xie</keyname><forenames>Lexing</forenames></author><author><keyname>Caetano</keyname><forenames>Tiberio</forenames></author><author><keyname>Cebrian</keyname><forenames>Manuel</forenames></author></authors><title>Evolution of Privacy Loss in Wikipedia</title><categories>cs.SI</categories><doi>10.1145/2835776.2835798</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The cumulative effect of collective online participation has an important and
adverse impact on individual privacy. As an online system evolves over time,
new digital traces of individual behavior may uncover previously hidden
statistical links between an individual's past actions and her private traits.
To quantify this effect, we analyze the evolution of individual privacy loss by
studying the edit history of Wikipedia over 13 years, including more than
117,523 different users performing 188,805,088 edits. We trace each Wikipedia's
contributor using apparently harmless features, such as the number of edits
performed on predefined broad categories in a given time period (e.g.
Mathematics, Culture or Nature). We show that even at this unspecific level of
behavior description, it is possible to use off-the-shelf machine learning
algorithms to uncover usually undisclosed personal traits, such as gender,
religion or education. We provide empirical evidence that the prediction
accuracy for almost all private traits consistently improves over time.
Surprisingly, the prediction performance for users who stopped editing after a
given time still improves. The activities performed by new users seem to have
contributed more to this effect than additional activities from existing (but
still active) users. Insights from this work should help users, system
designers, and policy makers understand and make long-term design choices in
online content creation systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03526</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03526</id><created>2015-12-11</created><authors><author><keyname>Erichson</keyname><forenames>N. Benjamin</forenames></author><author><keyname>Donovan</keyname><forenames>Carl</forenames></author></authors><title>Randomized Low-Rank Dynamic Mode Decomposition for Motion Detection</title><categories>cs.CV</categories><comments>Preprint submitted to Journal of Computer Vision and Image
  Understanding</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a fast algorithm for randomized computation of a
low-rank Dynamic Mode Decomposition (DMD) of a matrix. Here we consider this
matrix to represent the development of a spatial grid through time e.g. data
from a static video source. DMD was originally introduced in the fluid
mechanics community, but is also suitable for motion detection in video streams
and its use for background subtraction has received little previous
investigation. In this study we present a comprehensive evaluation of
background subtraction, using the randomized DMD and compare the results with
leading robust principal component analysis algorithms. The results are
convincing and show the random DMD is an efficient and powerful approach for
background modeling, allowing processing of high resolution videos in
real-time. Supplementary materials include implementations of the algorithms in
Python.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03531</identifier>
 <datestamp>2016-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03531</id><created>2015-12-11</created><updated>2016-01-19</updated><authors><author><keyname>Ivanyos</keyname><forenames>G&#xe1;bor</forenames></author><author><keyname>Qiao</keyname><forenames>Youming</forenames></author><author><keyname>Subrahmanyam</keyname><forenames>K. V.</forenames></author></authors><title>Constructive noncommutative rank computation in deterministic polynomial
  time over fields of arbitrary characteristics</title><categories>cs.CC cs.DS math.AC math.RT</categories><comments>14 pages. Version 3 includes a new, simple method that achieves the
  main result</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We extend our techniques developed in our manuscript mentioned in the
subtitle to obtain a deterministic polynomial time algorithm for computing the
non-commutative rank together with certificates of linear spaces of matrices
over sufficiently large base fields.
  The key new idea is a reduction procedure that keeps the blow-up parameter
small, and there are two methods to implement this idea: the first one is a
greedy argument that removes certain rows and columns, and the second one is an
efficient algorithmic version of a result of Derksen and Makam. Both methods
rely crucially on the regularity lemma in the aforementioned manuscript, and in
this note we improve that lemma by removing a coprime condition there.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03532</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03532</id><created>2015-12-11</created><authors><author><keyname>Parsonage</keyname><forenames>Eric</forenames></author><author><keyname>Roughan</keyname><forenames>Matthew</forenames></author></authors><title>Fast Generation of Spatially Embedded Random Networks</title><categories>cs.DS cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spatially Embedded Random Networks such as the Waxman random graph have been
used in a variety of settings for synthesizing networks. However, little
thought has been put into fast generation of these networks. Existing
techniques are $O(n^2)$ where $n$ is the number of nodes in the graph. In this
paper we present an $O(n + e)$ algorithm, where $e$ is the number of edges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03542</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03542</id><created>2015-12-11</created><authors><author><keyname>Che</keyname><forenames>Zhengping</forenames></author><author><keyname>Purushotham</keyname><forenames>Sanjay</forenames></author><author><keyname>Khemani</keyname><forenames>Robinder</forenames></author><author><keyname>Liu</keyname><forenames>Yan</forenames></author></authors><title>Distilling Knowledge from Deep Networks with Applications to Healthcare
  Domain</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Exponential growth in Electronic Healthcare Records (EHR) has resulted in new
opportunities and urgent needs for discovery of meaningful data-driven
representations and patterns of diseases in Computational Phenotyping research.
Deep Learning models have shown superior performance for robust prediction in
computational phenotyping tasks, but suffer from the issue of model
interpretability which is crucial for clinicians involved in decision-making.
In this paper, we introduce a novel knowledge-distillation approach called
Interpretable Mimic Learning, to learn interpretable phenotype features for
making robust prediction while mimicking the performance of deep learning
models. Our framework uses Gradient Boosting Trees to learn interpretable
features from deep learning models such as Stacked Denoising Autoencoder and
Long Short-Term Memory. Exhaustive experiments on a real-world clinical
time-series dataset show that our method obtains similar or better performance
than the deep learning models, and it provides interpretable phenotypes for
clinical decision making.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03543</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03543</id><created>2015-12-11</created><authors><author><keyname>Bhaskar</keyname><forenames>Umang</forenames></author><author><keyname>Cheng</keyname><forenames>Yu</forenames></author><author><keyname>Ko</keyname><forenames>Young Kun</forenames></author><author><keyname>Swamy</keyname><forenames>Chaitanya</forenames></author></authors><title>Near-Optimal Hardness Results for Signaling in Bayesian Games</title><categories>cs.GT cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the optimization problem faced by a perfectly informed principal in
a Bayesian game, who reveals information to the players about the state of
nature to obtain a desirable equilibrium. This signaling problem is the natural
design question motivated by uncertainty in games and has attracted much recent
attention. We present almost-optimal hardness results for signaling problems in
(a) Bayesian two-player zero-sum games, and (b) Bayesian network routing games.
  For Bayesian zero-sum games, when the principal seeks to maximize the
equilibrium utility of a player, we show that it is \nphard to obtain an FPTAS.
Previous results ruled out an FPTAS assuming the hardness of the planted clique
problem, which is an average-case assumption about the hardness of recovering a
planted clique from an Erd\&quot;os-R\'enyi random graph. Our hardness proof
exploits duality and the equivalence of separation and optimization in a novel,
unconventional way. Further, we preclude a PTAS assuming planted-clique
hardness; previously, a PTAS was ruled out (assuming planted-clique hardness)
only for games with quasi-polynomial-size strategy sets. Complementing these,
we obtain a PTAS for a structured class of zero-sum games (where signaling is
still NP-hard) when the payoff matrices obey a Lipschitz condition.
  For Bayesian network routing games, wherein the principal seeks to minimize
the average latency of the Nash flow, NP-hard to obtain an approximation ratio
better than 4/3, even for linear latency functions. This is the optimal
inapproximability result for linear latencies, since we show that full
revelation achieves a 4/3-approximation for linear latencies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03547</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03547</id><created>2015-12-11</created><updated>2016-01-19</updated><authors><author><keyname>Babai</keyname><forenames>L&#xe1;szl&#xf3;</forenames></author></authors><title>Graph Isomorphism in Quasipolynomial Time</title><categories>cs.DS cs.CC math.CO math.GR</categories><comments>89 pages</comments><msc-class>68Q25, 68R10, 20B25, 20B15, 05E18, 05C65, 20L05</msc-class><acm-class>F.2.2; G.2.2</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We show that the Graph Isomorphism (GI) problem and the related problems of
String Isomorphism (under group action) (SI) and Coset Intersection (CI) can be
solved in quasipolynomial ($\exp((\log n)^{O(1)})$) time. The best previous
bound for GI was $\exp(O(\sqrt{n\log n}))$, where $n$ is the number of vertices
(Luks, 1983); for the other two problems, the bound was similar,
$\exp(\tilde{O}(\sqrt{n}))$, where $n$ is the size of the permutation domain
(Babai, 1983).
  The algorithm builds on Luks's SI framework and attacks the barrier
configurations for Luks's algorithm by group theoretic &quot;local certificates&quot; and
combinatorial canonical partitioning techniques. We show that in a well-defined
sense, Johnson graphs are the only obstructions to effective canonical
partitioning.
  Luks's barrier situation is characterized by a homomorphism {\phi} that maps
a given permutation group $G$ onto $S_k$ or $A_k$, the symmetric or alternating
group of degree $k$, where $k$ is not too small. We say that an element $x$ in
the permutation domain on which $G$ acts is affected by {\phi} if the
{\phi}-image of the stabilizer of $x$ does not contain $A_k$. The
affected/unaffected dichotomy underlies the core &quot;local certificates&quot; routine
and is the central divide-and-conquer tool of the algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03549</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03549</id><created>2015-12-11</created><authors><author><keyname>Singh</keyname><forenames>Pranjal</forenames></author><author><keyname>Mukerjee</keyname><forenames>Amitabha</forenames></author></authors><title>Words are not Equal: Graded Weighting Model for building Composite
  Document Vectors</title><categories>cs.CL cs.LG cs.NE</categories><comments>10 Pages, 2 Figures, 11 Tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite the success of distributional semantics, composing phrases from word
vectors remains an important challenge. Several methods have been tried for
benchmark tasks such as sentiment classification, including word vector
averaging, matrix-vector approaches based on parsing, and on-the-fly learning
of paragraph vectors. Most models usually omit stop words from the composition.
Instead of such an yes-no decision, we consider several graded schemes where
words are weighted according to their discriminatory relevance with respect to
its use in the document (e.g., idf). Some of these methods (particularly
tf-idf) are seen to result in a significant improvement in performance over
prior state of the art. Further, combining such approaches into an ensemble
based on alternate classifiers such as the RNN model, results in an 1.6%
performance improvement on the standard IMDB movie review dataset, and a 7.01%
improvement on Amazon product reviews. Since these are language free models and
can be obtained in an unsupervised manner, they are of interest also for
under-resourced languages such as Hindi as well and many more languages. We
demonstrate the language free aspects by showing a gain of 12% for two review
datasets over earlier results, and also release a new larger dataset for future
testing (Singh,2015).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03551</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03551</id><created>2015-12-11</created><authors><author><keyname>Jafarizadeh</keyname><forenames>Saber</forenames></author></authors><title>Optimizing the Gossip Algorithm with Non-Uniform Clock Distribution over
  Classical &amp; Quantum Networks</title><categories>cs.SY</categories><comments>39 Pages, 2 Figures, 5 Tables. arXiv admin note: text overlap with
  arXiv:1509.05823</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed gossip algorithm has been studied in literature for practical
implementation of the distributed consensus algorithm as a fundamental
algorithm for the purpose of in-network collaborative processing. This paper
focuses on optimizing the convergence rate of the gossip algorithm for both
classical and quantum networks. A novel model of the gossip algorithm with
non-uniform clock distribution is proposed which can reach the optimal
convergence rate of the continuous-time consensus algorithm. It is described
that how the non-uniform clock distribution is achievable by modifying the rate
of the Poisson process modeling the clock of the gossip algorithm. The
minimization problem for optimizing the asymptotic convergence rate of the
proposed gossip algorithm and its corresponding semidefinite programming
formulation is addressed analytically. It is shown that the optimal results
obtained for uniform clock distribution are suboptimal compared to those of the
non-uniform one and for non-uniform distribution the optimal answer is not
unique i.e. there is more than one set of probabilities that can achieve the
optimal convergence rate. Based on the optimal continuous-time consensus
algorithm and the detailed balance property, an effective method of obtaining
one of these optimal answers is proposed. Regarding quantum gossip algorithm,
by expanding the density matrix in terms of the generalized Gell-Mann matrices,
the evolution equation of the quantum gossip algorithm is transformed to the
state update equation of the classical gossip algorithm. By defining the
quantum gossip operator, the original optimization problem is formulated as a
convex optimization problem, which can be addressed by semidefinite
programming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03564</identifier>
 <datestamp>2015-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03564</id><created>2015-12-11</created><updated>2015-12-15</updated><authors><author><keyname>Brucato</keyname><forenames>Matteo</forenames></author><author><keyname>Beltran</keyname><forenames>Juan Felipe</forenames></author><author><keyname>Abouzied</keyname><forenames>Azza</forenames></author><author><keyname>Meliou</keyname><forenames>Alexandra</forenames></author></authors><title>Scalable Package Queries in Relational Database Systems</title><categories>cs.DB</categories><comments>Extended version of PVLDB 2016 submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional database queries follow a simple model: they define constraints
that each tuple in the result must satisfy. This model is computationally
efficient, as the database system can evaluate the query conditions on each
tuple individually. However, many practical, real-world problems require a
collection of result tuples to satisfy constraints collectively, rather than
individually. In this paper, we present package queries, a new query model that
extends traditional database queries to handle complex constraints and
preferences over answer sets. We develop a full-fledged package query system,
implemented on top of a traditional database engine. Our work makes several
contributions. First, we design PaQL, a SQL-based query language that supports
the declarative specification of package queries. We prove that PaQL is as
least as expressive as integer linear programming, and therefore, evaluation of
package queries is in general NP-hard. Second, we present a fundamental
evaluation strategy that combines the capabilities of databases and constraint
optimization solvers to derive solutions to package queries. The core of our
approach is a set of translation rules that transform a package query to an
integer linear program. Third, we introduce an offline data partitioning
strategy allowing query evaluation to scale to large data sizes. Fourth, we
introduce SketchRefine, a scalable algorithm for package evaluation, with
strong approximation guarantees ($(1 \pm\epsilon)^6$-factor approximation).
Finally, we present extensive experiments over real-world and benchmark data.
The results demonstrate that SketchRefine is effective at deriving high-quality
package results, and achieves runtime performance that is an order of magnitude
faster than directly using ILP solvers over large datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03565</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03565</id><created>2015-12-11</created><authors><author><keyname>Cevik</keyname><forenames>Taner</forenames></author><author><keyname>Gunagwera</keyname><forenames>Alex</forenames></author><author><keyname>Cevik</keyname><forenames>Nazife</forenames></author></authors><title>A Survey of multimedia streaming in wireless sensor networks: progress,
  issues and design challenges</title><categories>cs.NI</categories><doi>10.5121/ijcnc.2015.7508</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Advancements in Complementary Metal Oxide Semiconductor (CMOS) technology
have enabled Wireless Sensor Networks (WSN) to gather, process and transport
multimedia (MM) data as well and not just limited to handling ordinary scalar
data anymore. This new generation of WSN type is called Wireless Multimedia
Sensor Networks (WMSNs). Better and yet relatively cheaper sensors that are
able to sense both scalar data and multimedia data with more advanced
functionalities such as being able to handle rather intense computations easily
have sprung up. In this paper, the applications, architectures, challenges and
issues faced in the design of WMSNs are explored. Security and privacy issues,
over all requirements, proposed and implemented solutions so far, some of the
successful achievements and other related works in the field are also
highlighted. Open research areas are pointed out and a few solution suggestions
to the still persistent problems are made, which, to the best of my knowledge,
so far have not been explored yet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03568</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03568</id><created>2015-12-11</created><authors><author><keyname>Cevik</keyname><forenames>Taner</forenames></author><author><keyname>Yilmaz</keyname><forenames>Serdar</forenames></author></authors><title>An overview of visible light communication systems</title><categories>cs.NI</categories><doi>10.5121/ijcnc.2015.7610</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Visible Light Communication (VLC) has gained great interest in the last
decade due to the rapid developments in Light Emitting Diodes (LEDs)
fabrication. Efficiency, durability and long life span of LEDs make them a
promising residential lighting equipment as well as an alternative cheap and
fast data transfer equipment. Appliance of visual light in data communication
by means of LEDs has been densely searched in academia. In this paper, we
explore the fundamentals and challenges of indoor VLC systems. Basics of
optical transmission such as transmitter, receiver, and links are investigated.
Moreover, characteristics of channel models in indoor VLC systems are
identified and theoretical details about channel modelling are presented in
detail.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03576</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03576</id><created>2015-12-11</created><authors><author><keyname>Bel</keyname><forenames>Albert</forenames></author><author><keyname>Adame</keyname><forenames>Toni</forenames></author><author><keyname>Bellalta</keyname><forenames>Boris</forenames></author></authors><title>An Energy Consumption Model for IEEE 802.11ah WLANs</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the main challenges when designing a new self-powered wireless sensor
network (WSN) technology is the vast operational dependence on its scarce
energy resources. Therefore, a thorough identification and characterisation of
the main energy consumption processes may lay the foundation for developing
further mechanisms aimed to make a more efficient use of devices' batteries.
This paper provides an energy consumption model for IEEE 802.11ah WLANs
operating in power saving mode, which are expected to become one of the
technology drivers in the development of the Internet of Things (IoT) in the
next years. Given the network characteristics, the presented analytical model
is able to provide an estimation of the average energy consumed by a station as
well as to predict its battery lifetime. Once the model has been validated, we
use it to obtain the optimal IEEE 802.11ah power saving parameters in several
IoT key scenarios, validating that the parameters provided by the IEEE 802.11ah
Task Group are already a very good choice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03580</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03580</id><created>2015-12-11</created><authors><author><keyname>Cevik</keyname><forenames>Taner</forenames></author><author><keyname>Ozyurt</keyname><forenames>Fatih</forenames></author></authors><title>Impacts of structural factors on energy consumption in cluster-based
  wireless sensor networks: a comprehensive analysis</title><categories>cs.NI</categories><doi>10.5121/ijasuc.2015.6101</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Limited energy is the major driving factor for research on wireless sensor
networks. Clustering alleviates this energy shortage problem by reducing data
traffic conveyed over the network and therefore several clustering methods are
proposed in the literature. Researchers put forward their methods by making
serious assumptions such as always locating single sink at one side of the
topology or making clusters near to the sink with smaller sizes. However, to
the best of our knowledge, there is no comprehensive research that investigates
the effects of various structural alternatives on energy consumption of
wireless sensor networks. In this paper, we thoroughly analyse the impact of
various structural approaches such as cluster size, number of tiers in the
topology, node density, position and number of sinks. Extensive simulation
results are provided. The results show that the best performance about
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03589</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03589</id><created>2015-12-11</created><updated>2016-01-09</updated><authors><author><keyname>Canas</keyname><forenames>Guillermo D.</forenames></author><author><keyname>Gortler</keyname><forenames>Steven J.</forenames></author></authors><title>On the Embeddability of Delaunay Triangulations in Anisotropic, Normed,
  and Bregman Spaces</title><categories>cs.CG</categories><comments>40 pages, 18 figures</comments><acm-class>I.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a two-dimensional space endowed with a divergence function that is
convex in the first argument, continuously differentiable in the second, and
satisfies suitable regularity conditions at Voronoi vertices, we show that
orphan-freedom (the absence of disconnected Voronoi regions) is sufficient to
ensure that Voronoi edges and vertices are also connected, and that the dual is
a simple planar graph. We then prove that the straight-edge dual of an
orphan-free Voronoi diagram (with sites as the first argument of the
divergence) is always an embedded triangulation.
  Among the divergences covered by our proofs are Bregman divergences,
anisotropic divergences, as well as all distances derived from strictly convex
$\mathcal{C}^1$ norms (including the $L_p$ norms with $1&lt; p &lt; \infty$). While
Bregman diagrams of the {first kind} are simply affine diagrams, and their
duals ({weighted} Delaunay triangulations) are always embedded, we show that
duals of orphan-free Bregman diagrams of the \emph{second kind} are always
embedded.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03591</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03591</id><created>2015-12-11</created><authors><author><keyname>H&#xe4;fner</keyname><forenames>Stephan</forenames></author><author><keyname>Thom&#xe4;</keyname><forenames>Reiner</forenames></author></authors><title>Estimation of Radio Channel Parameters in Case of an Unknown Transmitter</title><categories>cs.IT math.IT</categories><comments>5 figures, 5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the estimation of radio channel parameters from
receiver data, whereby the transmitter is fully unknown. We use a multipath
model to describe the radio channel between transmitter and receiver. According
to this model, we discuss the accessibility of parameters for estimation. Based
on the Maximum-Likelihood principle, we derive a cost function. A second cost
function is derived from the cross relation between the receiver channels. To
estimate the parameters, we seek for the minimum of these cost functions. The
performance of the presented cost functions are compared in simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03607</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03607</id><created>2015-12-11</created><authors><author><keyname>Ramya</keyname><forenames>C.</forenames></author><author><keyname>Rao</keyname><forenames>B. V. Raghavendra</forenames></author></authors><title>Limitations of sum of products of Read-Once Polynomials</title><categories>cs.CC</categories><comments>Submitted to a conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study limitations of polynomials computed by depth two circuits built over
read-once polynomials (ROPs) and depth three syntactically multi-linear
formulas.
  We prove an exponential lower bound for the size of the
$\Sigma\Pi^{[N^{1/30}]}$ arithmetic circuits built over syntactically
multi-linear $\Sigma\Pi\Sigma^{[N^{8/15}]}$ arithmetic circuits computing a
product of variable disjoint linear forms on $N$ variables. We extend the
result to the case of $\Sigma\Pi^{[N^{1/30}]}$ arithmetic circuits built over
ROPs of unbounded depth, where the number of variables with $+$ gates as a
parent in an proper sub formula is bounded by $N^{1/2+1/30}$. We show that the
same lower bound holds for the permanent polynomial. Finally we obtain an
exponential lower bound for the sum of ROPs computing a polynomial in ${\sf
VP}$ defined by Raz and Yehudayoff.
  Our results demonstrate a class of formulas of unbounded depth with
exponential size lower bound against the permanent and can be seen as an
exponential improvement over the multilinear formula size lower bounds given by
Raz for a sub-class of multi-linear and non-multi-linear formulas.
  Our proof techniques are built on the one developed by Raz and later extended
by Kumar et. al.\cite{KMS13} and are based on non-trivial analysis of ROPs
under random partitions. Further, our results exhibit strengths and limitations
of the lower bound techniques introduced by Raz\cite{Raz04a}.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03614</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03614</id><created>2015-12-11</created><authors><author><keyname>Perrone</keyname><forenames>Paolo</forenames></author><author><keyname>Ay</keyname><forenames>Nihat</forenames></author></authors><title>Hierarchical Quantification of Synergy in Channels</title><categories>cs.IT math.IT</categories><comments>20 pages, 12 figures, Front. Robot. AI - Computational Intelligence
  2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The decomposition of channel information into synergies of different order is
an open, active problem in the theory of complex systems. Most approaches to
the problem are based on information theory, and propose decompositions of
mutual information between inputs and outputs in se\-veral ways, none of which
is generally accepted yet.
  We propose a new point of view on the topic. We model a multi-input channel
as a Markov kernel. We can project the channel onto a series of exponential
families which form a hierarchical structure. This is carried out with tools
from information geometry, in a way analogous to the projections of probability
distributions introduced by Amari. A Pythagorean relation leads naturally to a
decomposition of the mutual information between inputs and outputs into terms
which represent single node information, pairwise interactions, and in general
n-node interactions.
  The synergy measures introduced in this paper can be easily evaluated by an
iterative scaling algorithm, which is a standard procedure in information
geometry.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03617</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03617</id><created>2015-12-11</created><authors><author><keyname>Ren</keyname><forenames>Wei-Ya</forenames></author></authors><title>Robust Dictionary based Data Representation</title><categories>cs.CV</categories><comments>8 pages. 2015.12.10</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The robustness to noise and outliers is an important issue in linear
representation in real applications. We focus on the problem that samples are
grossly corrupted, which is also the 'sample specific' corruptions problem. A
reasonable assumption is that corrupted samples cannot be represented by the
dictionary while clean samples can be well represented. This assumption is
enforced in this paper by investigating the coefficients of corrupted samples.
Concretely, we require the coefficients of corrupted samples be zero. In this
way, the representation quality of clean data can be assured without the effect
of corrupted data. At last, a robust dictionary based data representation
approach and its sparse representation version are proposed, which have
directive significance for future applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03622</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03622</id><created>2015-12-11</created><authors><author><keyname>Ding</keyname><forenames>Shengyong</forenames></author><author><keyname>Lin</keyname><forenames>Liang</forenames></author><author><keyname>Wang</keyname><forenames>Guangrun</forenames></author><author><keyname>Chao</keyname><forenames>Hongyang</forenames></author></authors><title>Deep Feature Learning with Relative Distance Comparison for Person
  Re-identification</title><categories>cs.CV</categories><comments>29 pages, 9 figures, The code has been released.
  http://vision.sysu.edu.cn/projects/deepreid/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Identifying the same individual across different scenes is an important yet
difficult task in intelligent video surveillance. Its main difficulty lies in
how to preserve similarity of the same person against large appearance and
structure variation while discriminating different individuals. In this paper,
we present a scalable distance driven feature learning framework based on the
deep neural network for person re-identification, and demonstrate its
effectiveness to handle the existing challenges. Specifically, given the
training images with the class labels (person IDs), we first produce a large
number of triplet units, each of which contains three images, i.e. one person
with a matched reference and a mismatched reference. Treating the units as the
input, we build the convolutional neural network to generate the layered
representations, and follow with the $L2$ distance metric. By means of
parameter optimization, our framework tends to maximize the relative distance
between the matched pair and the mismatched pair for each triplet unit.
Moreover, a nontrivial issue arising with the framework is that the triplet
organization cubically enlarges the number of training triplets, as one image
can be involved into several triplet units. To overcome this problem, we
develop an effective triplet generation scheme and an optimized gradient
descent algorithm, making the computational load mainly depends on the number
of original images instead of the number of triplets. On several challenging
databases, our approach achieves very promising results and outperforms other
state-of-the-art approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03623</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03623</id><created>2015-12-11</created><authors><author><keyname>Yayimli</keyname><forenames>Aysegul</forenames></author></authors><title>A Practical Layered Model for Flexible-grid Optical Networks to Reduce
  the Routing Complexity</title><categories>cs.NI</categories><comments>3 pages, 2 figures</comments><acm-class>C.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article proposes a new layered model to represent the spectrum
assignment on flexible-grid optical networks. This model can reduce the
time-complexity of existing routing and spectrum assignment methods by
providing a data structure that captures the current spectrum usage of the
network links.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03631</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03631</id><created>2015-12-11</created><updated>2016-01-26</updated><authors><author><keyname>Betts</keyname><forenames>Robert J</forenames></author></authors><title>How to find the least upper bound on the van der Waerden Number $W(r,
  k)$ that is some integer Power of the coloring Integer $r$</title><categories>cs.DM</categories><comments>25 pages, one Table, no figures. Small addition to Section 1. Slight
  revision of Theorem 3.1, Section 2. Typo correction for Theorem 7.2, Section
  7, where \(\frac{k}{r} = o(1)\) ought to have been \(\frac{k}{r^{n}} = o(1)\)
  and \(k \ll r^{n}\)</comments><msc-class>11P99 (Primary), 68R01 (Secondary)</msc-class><acm-class>G.2.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  What is a least integer upper bound on van der Waerden number $W(r, k)$ among
the powers of the integer $r$? We show how this can be found by expanding the
integer $W(r, k)$ into powers of $r$. Doing this enables us to find both a
least upper bound and a greatest lower bound on $W(r, k)$ that are some powers
of $r$ and where the greatest lower bound is equal to or smaller than $W(r,
k)$. A finite series expansion of each $W(r, k)$ into integer powers of $r$
then helps us to find also a greatest real lower bound on any $k$ for which a
conjecture posed by R. Graham is true, following immediately as a particular
case of the overall result.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03632</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03632</id><created>2015-12-11</created><authors><author><keyname>Yakut</keyname><forenames>Ibrahim</forenames></author><author><keyname>Turkoglu</keyname><forenames>Tugba</forenames></author><author><keyname>Yakut</keyname><forenames>Fikriye</forenames></author></authors><title>Understanding Customers' Evaluations Through Mining Airline Reviews</title><categories>cs.SI</categories><comments>International Journal of Data Mining &amp; Knowledge Management Process
  (IJDKP) Vol.5, No.6, November 2015</comments><doi>10.5121/ijdkp.2015.5601</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data mining can be evaluated as a strategic tool to determine the customer
profiles in order to learn customer expectations and requirements. Airline
customers have different characteristics and if passenger reviews about their
trip experiences are correctly analyzed, companies can increase customer
satisfaction by improving provided services. In this study, we investigate
customer review data for in-flight services of airline companies and draw
customer models with respect to such data. In this sense, we apply two
approaches as feature-based and clustering-based modelling. In feature-based
modelling, customers are grouped into categories based on features such as
cabin flown types, experienced airline companies. In clustering-based
modelling, customers are first clustered via k-means clustering and then
modeled. We apply multivariate regression analysis to model customer groups in
both cases. During this, we try to understand how customers evaluate the given
services and what dominant characteristics of in-flight services can be from
the customer viewpoint.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03655</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03655</id><created>2015-12-11</created><authors><author><keyname>Derpich</keyname><forenames>Milan S.</forenames></author><author><keyname>M&#xfc;ller</keyname><forenames>Mat&#xed;as</forenames></author><author><keyname>&#xd8;stergaard</keyname><forenames>Jan</forenames></author></authors><title>The Entropy Gain of Linear Time-Invariant Filters and Some of its
  Implications</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory. This version of
  the manuscript has a shorter abstract (due to arxiv restrictions)</comments><msc-class>94A17, 94A12, 93E20</msc-class><acm-class>H.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the increase in per-sample differential entropy rate of random
sequences and processes after being passed through a non minimum-phase (NMP)
discrete-time, linear time-invariant (LTI) filter G. For such filters and
random processes, it has long been established that this entropy gain, Gain(G),
equals the integral of log|G(exp(jw))|. It is also known that, if the first
sample of the impulse response of G has unit-magnitude, then this integral
equals the sum of the logarithm of the magnitudes of the non-minimum phase
zeros of G, say B(G). In this note, we begin by showing that existing
time-domain proofs of these results, which consider finite length-n sequences
and then let n tend to infinity, have neglected significant mathematical terms
and, therefore, are inaccurate. We discuss some of the implications of this
oversight when considering random processes. We then present a rigorous
time-domain analysis of the entropy gain of LTI filters for random processes.
In particular, we show that the entropy gain between equal-length input and
output sequences is upper bounded by B(G) and arises if and only if there
exists an output additive disturbance with finite differential entropy (no
matter how small) or a random initial state. Instead, when comparing the input
differential entropy to that of the entire (longer) output of G, the entropy
gain equals B(G) without the need for additional exogenous random signals. We
illustrate some of the consequences of these results by presenting their
implications in three different problems. Specifically: a simple derivation of
the rate-distortion function for Gaussian non-stationary sources, conditions
for equality in an information inequality of importance in networked control
problems, and an observation on the capacity of auto-regressive Gaussian
channels with feedback.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03667</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03667</id><created>2015-12-08</created><authors><author><keyname>Steinmetz</keyname><forenames>Jason W.</forenames></author></authors><title>An Intuitively Complete Analysis of Godel's Incompleteness</title><categories>math.LO cs.LO</categories><comments>36 pages</comments><msc-class>03F40 (Primary) 03F50, 03A99 (Secondary)</msc-class><acm-class>F.4.1; I.2.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A detailed and rigorous analysis of Godel's proof of his first incompleteness
theorem is presented. The purpose of this analysis is two-fold. The first is to
reveal what Godel actually proved to provide a clear and solid foundation upon
which to base future research. The second is to construct a coherent
explication of Godel's proof that is not only approachable by the
non-specialist, but also brings to light the core principles underlying Godel's
proof.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03685</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03685</id><created>2015-12-09</created><authors><author><keyname>Chen</keyname><forenames>Hsien-Pu</forenames></author><author><keyname>Mohammad</keyname><forenames>Muneer</forenames></author><author><keyname>Kish</keyname><forenames>Laszlo B.</forenames></author></authors><title>Current Injection Attack against the KLJN Secure Key Exchange</title><categories>cs.CR cs.ET physics.class-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Kirchhoff-law-Johnson-noise (KLJN) scheme is a statistical/physical
secure key exchange system based on the laws of classical statistical physics
to provide unconditional security. We used the LTSPICE industrial cable and
circuit simulator to emulate one of the major active (invasive) attacks, the
current injection attack, against the ideal and a practical KLJN system,
respectively. We show that two security enhancement techniques, namely, the
instantaneous voltage/current comparison method, and a simple privacy
amplification scheme, independently and effectively eliminate the information
leak and successfully preserve the system's unconditional security.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03696</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03696</id><created>2015-12-11</created><authors><author><keyname>Delacourt</keyname><forenames>Martin</forenames></author><author><keyname>de Menibus</keyname><forenames>Benjamin Hellouin</forenames></author></authors><title>Characterisation of limit measures of higher-dimensional cellular
  automata</title><categories>math.DS cs.FL nlin.CG</categories><comments>25 pages, submitted to Journal of Computer Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We characterise by computability conditions the set of all possible limit
measures of cellular automata of any dimension when starting from the uniform
measure. The limit measure describes the typical asymptotic behaviour of the
cellular automaton, getting rid of exceptional cases, when the initial
configuration is chosen uniformly at random. This generalises known results in
dimension one and (partially) two.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03697</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03697</id><created>2015-12-11</created><updated>2016-01-19</updated><authors><author><keyname>Skwerer</keyname><forenames>Sean</forenames></author><author><keyname>Zhang</keyname><forenames>Heping</forenames></author></authors><title>Computing Affine Combinations, Distances, and Correlations for Recursive
  Partition Functions</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recursive partitioning is the core of several statistical methods including
CART, random forest, and boosted trees. Despite the popularity of tree based
methods, to date, there did not exist methods for combining multiple trees into
a single tree, or methods for systematically quantifying the discrepancy
between two trees. Taking advantage of the recursive structure in trees we
formulated fast algorithms for computing affine combinations, distances and
correlations in a vector subspace of recursive partition functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03706</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03706</id><created>2015-12-11</created><authors><author><keyname>Hossu</keyname><forenames>Andrei</forenames></author><author><keyname>Andone</keyname><forenames>Daniela</forenames></author></authors><title>A New Approach of Gray Images Binarization with Threshold Methods</title><categories>cs.CV</categories><doi>10.13140/RG.2.1.4391.8165</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper presents some aspects of the (gray level) image binarization
methods used in artificial vision systems. It is introduced a new approach of
gray level image binarization for artificial vision systems dedicated to
industrial automation temporal thresholding. In the first part of the paper are
extracted some limitations of using the global optimum thresholding in gray
level image binarization. In the second part of this paper are presented some
aspects of the dynamic optimum thresholding method for gray level image
binarization. Starting from classic methods of global and dynamic optimal
thresholding of the gray level images in the next section are introduced the
concepts of temporal histogram and temporal thresholding. In the final section
are presented some practical aspects of the temporal thresholding method in
artificial vision applications form the moving scene in robotic automation
class; pointing out the influence of the acquisition frequency on the methods
results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03740</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03740</id><created>2015-12-11</created><authors><author><keyname>Lan</keyname><forenames>Zhenzhong</forenames></author><author><keyname>Yu</keyname><forenames>Shoou-I</forenames></author><author><keyname>Hauptmann</keyname><forenames>Alexander G.</forenames></author></authors><title>Improving Human Activity Recognition Through Ranking and Re-ranking</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose two well-motivated ranking-based methods to enhance the
performance of current state-of-the-art human activity recognition systems.
First, as an improvement over the classic power normalization method, we
propose a parameter-free ranking technique called rank normalization (RaN). RaN
normalizes each dimension of the video features to address the sparse and
bursty distribution problems of Fisher Vectors and VLAD. Second, inspired by
curriculum learning, we introduce a training-free re-ranking technique called
multi-class iterative re-ranking (MIR). MIR captures relationships among action
classes by separating easy and typical videos from difficult ones and
re-ranking the prediction scores of classifiers accordingly. We demonstrate
that our methods significantly improve the performance of state-of-the-art
motion features on six real-world datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03744</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03744</id><created>2015-12-11</created><updated>2015-12-18</updated><authors><author><keyname>MacCurdy</keyname><forenames>Robert</forenames></author><author><keyname>Katzschmann</keyname><forenames>Robert</forenames></author><author><keyname>Kim</keyname><forenames>Youbin</forenames></author><author><keyname>Rus</keyname><forenames>Daniela</forenames></author></authors><title>Printable Hydraulics: A Method for Fabricating Robots by 3D Co-Printing
  Solids and Liquids</title><categories>cs.RO</categories><comments>This paper was submitted prematurely and is incomplete. Please do not
  link to or distribute this version as it is not yet ready for a broad
  audience. We are revising</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work introduces a novel technique for fabricating functional robots
using 3D printers. Simultaneously depositing photopolymers and a non-curing
liquid allows complex, pre-filled fluidic channels to be fabricated. This new
printing capability enables complex hydraulically actuated robots and robotic
components to be automatically built, with no assembly required. The technique
is showcased by printing linear bellows actuators, gear pumps, soft grippers
and a hexapod robot, using a commercially-available 3D printer. We detail the
steps required to modify the printer and describe the design constraints
imposed by this new fabrication approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03770</identifier>
 <datestamp>2015-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03770</id><created>2015-12-11</created><updated>2015-12-16</updated><authors><author><keyname>Kellerer</keyname><forenames>Wolfgang</forenames></author><author><keyname>Basta</keyname><forenames>Arsany</forenames></author><author><keyname>Blenk</keyname><forenames>Andreas</forenames></author></authors><title>Flexibility of Networks: a new measure for network design space
  analysis?</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Flexibility is often claimed as a competitive advantage when proposing new
network designs. However, most proposals provide only qualitative arguments for
their improved support of flexibility. Quantitative arguments vary a lot among
different proposals. A general understanding for flexibility is not yet clearly
defined, leaving it to the reader to draw the right conclusions based on
background information. The term flexibility is commonly defined as the ability
to adapt to changes. For networks, flexibility would refer to the ability to
adapt the available network resources, such as flows or topology, to changes of
design requirements, e.g., shorter latency budgets or different traffic
distributions. Recent concepts such as Software Defined Networking, Network
Virtualization and Network Function Virtualization have emerged claiming to
provide more flexibility in networks. Nevertheless, a deeper understanding of
what flexibility means and how it could be quantified to compare different
network designs remains open. In this paper, we ask whether flexibility can be
a new measure for network design space analysis. As it is quite challenging to
formulate a flexibility measure that covers all network aspects, we propose an
initial set of flexibility aspects to start grounding guidelines. Our initial
selection is backed up by an analysis of Software Defined Networking, Network
Virtualization and Network Function Virtualization for their support of the
selected flexibility aspects. Our research methodology is based on a systematic
approach that leads to network design guidelines with respect to flexibility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03790</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03790</id><created>2015-11-22</created><authors><author><keyname>Zamanipour</keyname><forenames>Makan</forenames></author><author><keyname>Mohammadi</keyname><forenames>Mohammadali</forenames></author></authors><title>A New Coordinated Beamformer for MIMO-based Ad Hoc Networks</title><categories>cs.IT cs.NI math.IT</categories><comments>IEEE AIMS 2015 (2-4 Dec)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The study proposes a new scheme for MIMO-based ad hoc networks. This is
accomplished, while using the Interference Driving Technique (IDT) over
Nakagami-m fading channels with perfect channel state information at both the
transmitter and receiver. The use of this technique is proposed to decrease the
impact of all the unwanted interferences, routinely caused by the overlap of
the defined radio transmission ranges related to the used nodes. Indeed, IDT is
utilized as a coordinated beamformer in a cooperative scheme, according to mean
squared error criterion. The methodology and also analytical results are
conducted to prove the aptitude of the paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03794</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03794</id><created>2015-12-07</created><authors><author><keyname>Haddley</keyname><forenames>Joel</forenames></author><author><keyname>Worsley</keyname><forenames>Stephen</forenames></author></authors><title>Infinite families of monohedral disk tilings</title><categories>math.MG cs.CG math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper gives new solutions to the problem: 'Can we construct monohedral
tilings of the disk such that a neighbourhood of the origin has trivial
intersection with at least one tile?'
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03796</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03796</id><created>2015-12-11</created><authors><author><keyname>Streit</keyname><forenames>Ananda Gorck</forenames></author><author><keyname>Rodrigues</keyname><forenames>Carlo Kleber da Silva</forenames></author></authors><title>Improving BitTorrent's Peer Selection For Multimedia Content On-Demand
  Delivery</title><categories>cs.NI</categories><comments>International Journal of Computer Networks &amp; Communications(IJCNC)
  Vol.7, No.6, November 2015</comments><doi>10.5121/ijcnc.2015.7608</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The great efficiency achieved by the BitTorrent protocol for the distribution
of large amounts of data inspired its adoption to provide multimedia content
on-demand delivery over the Internet. As it is not designed for this purpose,
some adjustments have been proposed in order to meet the related QoS
requirements like low startup delay and smooth playback continuity.
Accordingly, this paper introduces a BitTorrent-like proposal named as
Quota-Based Peer Selection (QBPS). This proposal is mainly based on the
adaptation of the original peer-selection policy of the BitTorrent protocol.
Its validation is achieved by means of simulations and competitive analysis.
The final results show that QBPS outperforms other recent proposals of the
literature. For instance, it achieves a throughput optimization of up to 48.0%
in low-provision capacity scenarios where users are very interactive.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03798</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03798</id><created>2015-12-11</created><authors><author><keyname>Ikenmeyer</keyname><forenames>Christian</forenames></author><author><keyname>Panova</keyname><forenames>Greta</forenames></author></authors><title>Rectangular Kronecker coefficients and plethysms in geometric complexity
  theory</title><categories>cs.CC math.CO math.RT</categories><comments>20 pages</comments><msc-class>20C30, 20G05, 68Q17</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that in the geometric complexity theory program the vanishing of
rectangular Kronecker coefficients cannot be used to prove superpolynomial
determinantal complexity lower bounds for the permanent polynomial.
  Moreover, we prove the positivity of rectangular Kronecker coefficients for a
large class of partitions where the side lengths of the rectangle are at least
quadratic in the length of the partition. We also compare rectangular Kronecker
coefficients with their corresponding plethysm coefficients, which leads to a
new lower bound for rectangular Kronecker coefficients. Moreover, we prove that
the saturation of the rectangular Kronecker semigroup is trivial, we show that
the rectangular Kronecker positivity stretching factor is 2 for a long first
row, and we completely classify the positivity of rectangular limit Kronecker
coefficients that were introduced by Manivel in 2011.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03824</identifier>
 <datestamp>2015-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03824</id><created>2015-12-11</created><authors><author><keyname>Bocharov</keyname><forenames>Alex</forenames></author><author><keyname>Cui</keyname><forenames>Shawn X.</forenames></author><author><keyname>Roetteler</keyname><forenames>Martin</forenames></author><author><keyname>Svore</keyname><forenames>Krysta M.</forenames></author></authors><title>Improved Quantum Ternary Arithmetics</title><categories>quant-ph cs.ET</categories><comments>17 pages, 13 figures, 5 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Qutrit (or ternary) structures arise naturally in many quantum systems,
particularly in certain non-abelian anyon systems. We present efficient
circuits for ternary reversible and quantum arithmetics. Our main result is the
derivation of circuits for two families of ternary quantum adders, namely
ripple carry adders and carry look-ahead adders. The main difference to the
binary case is the more complicated form of the ternary carry, which leads to
higher resource counts for implementations over a universal ternary gate set.
Our ternary ripple adder circuit has a circuit depth of $O(n)$ and uses only
$1$ ancilla, making it more efficient in both, circuit depth and width than
previous constructions. Our ternary carry lookahead circuit has a circuit depth
of only $O(\log\,n)$, while using with $O(n)$ ancillas. Our approach works on
two levels of abstraction: at the first level, descriptions of arithmetic
circuits are given in terms of gates sequences that use various types of
non-Clifford reflections. At the second level, we break down these reflections
further by deriving them either from the two-qutrit Clifford gates and the
non-Clifford gate $C(X): |i,j\rangle \mapsto |i, j + \delta_{i,2} \mod
3\rangle$ or from the two-qutrit Clifford gates and the non-Clifford gate
$P_9=\mbox{diag}(1,e^{2 \pi \, i/9},e^{4 \pi \, i/9})$. The two choices of
elementary gate sets correspond to two possible mappings onto two different
prospective quantum computing architectures which we call the metaplectic and
the supermetaplectic basis, respectively. Finally, we develop a method to
factor diagonal unitaries using multi-variate polynomial over the ternary
finite field which allows to characterize classes of gates that can be
implemented exactly over the supermetaplectic basis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03839</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03839</id><created>2015-12-11</created><authors><author><keyname>Tan</keyname><forenames>Le Thanh</forenames></author><author><keyname>Le</keyname><forenames>Long Bao</forenames></author></authors><title>Design and Optimal Configuration of Full-Duplex MAC Protocol for
  Cognitive Radio Networks Considering Self-Interference</title><categories>cs.NI cs.IT math.IT</categories><comments>To Appear, IEEE Access, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose an adaptive Medium Access Control (MAC) protocol
for full-duplex (FD) cognitive radio networks in which FD secondary users (SUs)
perform channel contention followed by concurrent spectrum sensing and
transmission, and transmission only with maximum power in two different stages
(called the FD sensing and transmission stages, respectively) in each
contention and access cycle. The proposed FD cognitive MAC (FDC-MAC) protocol
does not require synchronization among SUs and it efficiently utilizes the
spectrum and mitigates the self-interference in the FD transceiver. We then
develop a mathematical model to analyze the throughput performance of the
FDC-MAC protocol where both half-duplex (HD) transmission (HDTx) and FD
transmission (FDTx) modes are considered in the transmission stage. Then, we
study the FDC-MAC configuration optimization through adaptively controlling the
spectrum sensing duration and transmit power level in the FD sensing stage
where we prove that there exists optimal sensing time and transmit power to
achieve the maximum throughput and we develop an algorithm to configure the
proposed FDC-MAC protocol. Extensive numerical results are presented to
illustrate the characteristic of the optimal FDC-MAC configuration and the
impacts of protocol parameters and the self-interference cancellation quality
on the throughput performance. Moreover, we demonstrate the significant
throughput gains of the FDC-MAC protocol with respect to existing half-duplex
MAC (HD MAC) and single-stage FD MAC protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03844</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03844</id><created>2015-12-11</created><authors><author><keyname>Shafiee</keyname><forenames>Mohammad Javad</forenames></author><author><keyname>Siva</keyname><forenames>Parthipan</forenames></author><author><keyname>Fieguth</keyname><forenames>Paul</forenames></author><author><keyname>Wong</keyname><forenames>Alexander</forenames></author></authors><title>Efficient Deep Feature Learning and Extraction via StochasticNets</title><categories>cs.LG stat.ML</categories><comments>10 pages. arXiv admin note: substantial text overlap with
  arXiv:1508.05463</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep neural networks are a powerful tool for feature learning and extraction
given their ability to model high-level abstractions in highly complex data.
One area worth exploring in feature learning and extraction using deep neural
networks is efficient neural connectivity formation for faster feature learning
and extraction. Motivated by findings of stochastic synaptic connectivity
formation in the brain as well as the brain's uncanny ability to efficiently
represent information, we propose the efficient learning and extraction of
features via StochasticNets, where sparsely-connected deep neural networks can
be formed via stochastic connectivity between neurons. To evaluate the
feasibility of such a deep neural network architecture for feature learning and
extraction, we train deep convolutional StochasticNets to learn abstract
features using the CIFAR-10 dataset, and extract the learned features from
images to perform classification on the SVHN and STL-10 datasets. Experimental
results show that features learned using deep convolutional StochasticNets,
with fewer neural connections than conventional deep convolutional neural
networks, can allow for better or comparable classification accuracy than
conventional deep neural networks: relative test error decrease of ~4.5% for
classification on the STL-10 dataset and ~1% for classification on the SVHN
dataset. Furthermore, it was shown that the deep features extracted using deep
convolutional StochasticNets can provide comparable classification accuracy
even when only 10% of the training data is used for feature learning. Finally,
it was also shown that significant gains in feature extraction speed can be
achieved in embedded applications using StochasticNets. As such, StochasticNets
allow for faster feature learning and extraction performance while facilitate
for better or comparable accuracy performances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03850</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03850</id><created>2015-12-11</created><authors><author><keyname>Pastor</keyname><forenames>Marissa</forenames></author><author><keyname>Song</keyname><forenames>Juyong</forenames></author><author><keyname>Hoang</keyname><forenames>Danh-Tai</forenames></author><author><keyname>Jo</keyname><forenames>Junghyo</forenames></author></authors><title>Minimal Perceptrons for Memorizing Complex Patterns</title><categories>q-bio.NC cs.NE</categories><comments>14 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Feedforward neural networks have been investigated to understand learning and
memory, as well as applied to numerous practical problems in pattern
classification. It is a rule of thumb that more complex tasks require larger
networks. However, the design of optimal network architectures for specific
tasks is still an unsolved fundamental problem. In this study, we consider
three-layered neural networks for memorizing binary patterns. We developed a
new complexity measure of binary patterns, and estimated the minimal network
size for memorizing them as a function of their complexity. We formulated the
minimal network size for regular, random, and complex patterns. In particular,
the minimal size for complex patterns, which are neither ordered nor
disordered, was predicted by measuring their Hamming distances from known
ordered patterns. Our predictions agreed with simulations based on the
back-propagation algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03851</identifier>
 <datestamp>2015-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03851</id><created>2015-12-11</created><authors><author><keyname>Ashe</keyname><forenames>Subrata</forenames></author></authors><title>A Design of Endurance Queue for Co-Existing Systems in Multi-Programmed
  Environments</title><categories>cs.PF cs.DM cs.SE</categories><comments>4 pages. 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  These days enterprise applications try to integrate online processing and
batch jobs into a common software stack for seamless monitoring and driverless
operations. Continuous integration of these systems results in choking of the
poorly performing sub-systems, when the service demand and throughput are not
synchronized. A poorly performing sub-system may become a serious performance
bottleneck for the entire system if its serviceability and the capacity is over
utilized by increased service demand from upstream systems. From all the
integrated sub-systems, queuing systems are majorly categorized as choking
elements due to their limited service length and lack of processing details.
The situation becomes more pronounced in multiprogramming environments where
the queue performance exponentially degrades with increased degree of
multiprogramming at upstream levels. This paper presents an approach to compute
the queue length and devise a distribution model such that the queue length is
dynamically adjusted depending on the sudden growth or decline of transmission
packets. The idea is to design a heat map of the memory and correlate it with
the queue length distribution. With each degree of multi-programmability, the
data processing logic is adjusted by the distribution model to arrive at an
endurance level queue for long term service under variable load conditions. It
will take away the current implementation of using delayed processing logic
and/or batch processing of data at downstream systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03853</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03853</id><created>2015-12-11</created><authors><author><keyname>Chang</keyname><forenames>Young Hwan</forenames></author><author><keyname>Hu</keyname><forenames>Qie</forenames></author><author><keyname>Tomlin</keyname><forenames>Claire J.</forenames></author></authors><title>Secure Estimation based Kalman Filter for Cyber-Physical Systems against
  Adversarial Attacks</title><categories>cs.SY</categories><comments>arXiv admin note: text overlap with arXiv:1205.5073 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cyber-physical systems (CPSs) are found in many applications such as power
networks, manufacturing processes, and air and ground transportation systems.
Maintaining security of these systems under cyber attacks is an important and
challenging task, since these attacks can be erratic and thus difficult to
model. Secure estimation problems study how to estimate the true system states
when measurements are corrupted and/or control inputs are compromised by
attackers. The authors in [1] proposed a secure estimation method when the set
of attacked nodes (sensors, controllers) is fixed. In this paper, we extend
these results to scenarios in which the set of attacked nodes can change over
time. We formulate this secure estimation problem into the classical error
correction problem [2] and we show that accurate decoding can be guaranteed
under a certain condition. Furthermore, we propose a combined secure estimation
method with our proposed secure estimator above and the Kalman Filter (KF) for
improved practical performance. Finally, we demonstrate the performance of our
method through simulations of two scenarios where an unmanned aerial vehicle is
under adversarial attack.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03859</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03859</id><created>2015-12-11</created><authors><author><keyname>Lisitsa</keyname><forenames>Alexei P.</forenames><affiliation>Department of Computer Science, The University of Liverpool</affiliation></author><author><keyname>Nemytykh</keyname><forenames>Andrei P.</forenames><affiliation>Program Systems Institute, Russian Academy of Sciences</affiliation></author></authors><title>Finite Countermodel Based Verification for Program Transformation (A
  Case Study)</title><categories>cs.SE cs.PL</categories><comments>In Proceedings VPT 2015, arXiv:1512.02215</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 199, 2015, pp. 15-32</journal-ref><doi>10.4204/EPTCS.199.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Both automatic program verification and program transformation are based on
program analysis. In the past decade a number of approaches using various
automatic general-purpose program transformation techniques (partial deduction,
specialization, supercompilation) for verification of unreachability properties
of computing systems were introduced and demonstrated. On the other hand, the
semantics based unfold-fold program transformation methods pose themselves
diverse kinds of reachability tasks and try to solve them, aiming at improving
the semantics tree of the program being transformed. That means some
general-purpose verification methods may be used for strengthening program
transformation techniques. This paper considers the question how finite
countermodels for safety verification method might be used in Turchin's
supercompilation method. We extract a number of supercompilation sub-algorithms
trying to solve reachability problems and demonstrate use of an external
countermodel finder for solving some of the problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03860</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03860</id><created>2015-12-11</created><authors><author><keyname>Hamilton</keyname><forenames>Geoff</forenames><affiliation>School of Computing, Dublin City University</affiliation></author></authors><title>Verifying Temporal Properties of Reactive Systems by Transformation</title><categories>cs.LO cs.PL</categories><comments>In Proceedings VPT 2015, arXiv:1512.02215. This work was supported,
  in part, by Science Foundation Ireland grant 10/CE/I1855 to Lero - the Irish
  Software Engineering Research Centre (www.lero.ie), and by the School of
  Computing, Dublin City University</comments><proxy>EPTCS</proxy><acm-class>I.2.2, F.3.2</acm-class><journal-ref>EPTCS 199, 2015, pp. 33-49</journal-ref><doi>10.4204/EPTCS.199.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show how program transformation techniques can be used for the
verification of both safety and liveness properties of reactive systems. In
particular, we show how the program transformation technique distillation can
be used to transform reactive systems specified in a functional language into a
simplified form that can subsequently be analysed to verify temporal properties
of the systems. Example systems which are intended to model mutual exclusion
are analysed using these techniques with respect to both safety (mutual
exclusion) and liveness (non-starvation), with the errors they contain being
correctly identified.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03861</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03861</id><created>2015-12-11</created><authors><author><keyname>Lester</keyname><forenames>Martin</forenames><affiliation>Department of Computer Science, University of Oxford</affiliation></author></authors><title>Control Flow Analysis for SF Combinator Calculus</title><categories>cs.PL</categories><comments>In Proceedings VPT 2015, arXiv:1512.02215</comments><proxy>EPTCS</proxy><acm-class>F.3.2; F.3.3</acm-class><journal-ref>EPTCS 199, 2015, pp. 51-67</journal-ref><doi>10.4204/EPTCS.199.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Programs that transform other programs often require access to the internal
structure of the program to be transformed. This is at odds with the usual
extensional view of functional programming, as embodied by the lambda calculus
and SK combinator calculus. The recently-developed SF combinator calculus
offers an alternative, intensional model of computation that may serve as a
foundation for developing principled languages in which to express intensional
computation, including program transformation. Until now there have been no
static analyses for reasoning about or verifying programs written in
SF-calculus. We take the first step towards remedying this by developing a
formulation of the popular control flow analysis 0CFA for SK-calculus and
extending it to support SF-calculus. We prove its correctness and demonstrate
that the analysis is invariant under the usual translation from SK-calculus
into SF-calculus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03862</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03862</id><created>2015-12-11</created><authors><author><keyname>Kafle</keyname><forenames>Bishoksan</forenames><affiliation>Roskilde University, Denmark</affiliation></author><author><keyname>Gallagher</keyname><forenames>John P.</forenames><affiliation>Roskilde University, Denmark and IMDEA Software Institute, Spain</affiliation></author><author><keyname>Ganty</keyname><forenames>Pierre</forenames><affiliation>IMDEA Software Institute, Spain</affiliation></author></authors><title>Decomposition by tree dimension in Horn clause verification</title><categories>cs.LO</categories><comments>In Proceedings VPT 2015, arXiv:1512.02215</comments><proxy>EPTCS</proxy><acm-class>Verification</acm-class><journal-ref>EPTCS 199, 2015, pp. 1-14</journal-ref><doi>10.4204/EPTCS.199.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we investigate the use of the concept of tree dimension in Horn
clause analysis and verification. The dimension of a tree is a measure of its
non-linearity - for example a list of any length has dimension zero while a
complete binary tree has dimension equal to its height. We apply this concept
to trees corresponding to Horn clause derivations. A given set of Horn clauses
P can be transformed into a new set of clauses P=&lt;k, whose derivation trees are
the subset of P's derivation trees with dimension at most k. Similarly, a set
of clauses P&gt;k can be obtained from P whose derivation trees have dimension at
least k + 1. In order to prove some property of all derivations of P, we
systematically apply these transformations, for various values of k, to
decompose the proof into separate proofs for P=&lt;k and P&gt;k (which could be
executed in parallel). We show some preliminary results indicating that
decomposition by tree dimension is a potentially useful proof technique. We
also investigate the use of existing automatic proof tools to prove some
interesting properties about dimension(s) of feasible derivation trees of a
given program.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03866</identifier>
 <datestamp>2015-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03866</id><created>2015-12-11</created><updated>2015-12-22</updated><authors><author><keyname>Wang</keyname><forenames>Qiuyan</forenames></author><author><keyname>Li</keyname><forenames>Fei</forenames></author><author><keyname>Lin</keyname><forenames>Dongdai</forenames></author></authors><title>A Class of Linear Codes With Three Weights</title><categories>cs.IT math.IT</categories><comments>11 pages,2 tables</comments><msc-class>94A05, 94A15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linear codes have been an interesting subject of study for many years.
Recently, linear codes with few weights have been constructed and extensively
studied. In this paper, for an odd prime p, a class of three-weight linear
codes over Fp are constructed. The weight distributions of the linear codes are
settled. These codes have applications in authentication codes, association
schemes and data storage systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03868</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03868</id><created>2015-12-11</created><authors><author><keyname>Bukatin</keyname><forenames>Michael A.</forenames></author></authors><title>Mathematics of Domains</title><categories>cs.LO</categories><comments>135 pages, PhD Thesis, 2002, Department of Computer Science, Brandeis
  University</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two groups of naturally arising questions in the mathematical theory of
domains for denotational semantics are addressed. Domains are equipped with
Scott topology and represent data types. Scott continuous functions represent
computable functions and form the most popular continuous model of
computations.
  Covariant Logic of Domains: Domains are represented as sets of theories, and
Scott continuous functions are represented as input-output inference engines.
The questions addressed are: A. What constitutes a subdomain? Do subdomains of
a given domain $A$ form a domain? B. Which retractions are finitary? C. What is
the essence of generalizations of information systems based on non-reflexive
logics? Are these generalizations restricted to continuous domains?
  Analysis on Domains:
  D. How to describe Scott topologies via generalized distance functions
satisfying the requirement of Scott continuity (&quot;abstract computability&quot;)? The
answer is that the axiom $\rho (x, x) = 0$ is incompatible with Scott
continuity of distance functions. The resulting \bf relaxed metrics are
studied.
  E. Is it possible to obtain Scott continuous relaxed metrics via measures of
domain subsets representing positive and negative information about domain
elements? The positive answer is obtained via the discovery of the novel class
of co-continuous valuations on the systems of Scott open sets.
  Some of these natural questions were studied earlier. However, in each case a
novel approach is presented, and the answers are supplied with much more
compelling and clear justifications, than were known before.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03871</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03871</id><created>2015-12-11</created><updated>2016-02-26</updated><authors><author><keyname>Qi</keyname><forenames>Minglong</forenames></author><author><keyname>Xiong</keyname><forenames>Shengwu</forenames></author><author><keyname>Yuan</keyname><forenames>Jingling</forenames></author><author><keyname>Rao</keyname><forenames>Wenbi</forenames></author><author><keyname>Zhong</keyname><forenames>Luo</forenames></author></authors><title>On the Linear Complexity of Generalized Cyclotomic Quaternary Sequences
  with Length $2pq$</title><categories>cs.CR</categories><comments>38pages</comments><doi>10.1587/transfun.E98.A.1569</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the linear complexity over $\mathbf{GF}(r)$ of generalized
cyclotomic quaternary sequences with period $2pq$ is determined, where $ r $ is
an odd prime such that $r \ge 5$ and $r\notin \lbrace p,q\rbrace$. The minimal
value of the linear complexity is equal to $\tfrac{5pq+p+q+1}{4}$ which is
greater than the half of the period $2pq$. According to the Berlekamp-Massey
algorithm, these sequences are viewed as enough good for the use in
cryptography. We show also that if the character of the extension field
$\mathbf{GF}(r^{m})$, $r$, is chosen so that $\bigl(\tfrac{r}{p}\bigr) =
\bigl(\tfrac{r}{q}\bigr) = -1$, $r\nmid 3pq-1$, and $r\nmid 2pq-4$, then the
linear complexity can reach the maximal value equal to the length of the
sequences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03874</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03874</id><created>2015-12-12</created><authors><author><keyname>Wu</keyname><forenames>Ren</forenames></author></authors><title>Correlating Features and Code by Dynamic and Semantic Analysis</title><categories>cs.SE</categories><comments>15 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One major problem in maintaining a software system is to understand how many
functional features in the system and how these features are implemented. In
this paper a novel approach for locating features in code by semantic and
dynamic analysis is proposed. The method process consists of three steps: The
first uses the execution traces as text corpus and the method calls involved in
the traces as terms of document. The second ranks the method calls in order to
filter out omnipresent methods by setting a threshold. And the third step
treats feature-traces as first class entities and extracts identifiers from the
rest method source code and a trace-by-identifier matrix is generated. Then a
semantic analysis model-LDA is applied on the matrix to extract topics, which
act as functional features. Through building several corresponding matrices,
the relations between features and code can be obtained for comprehending the
system functional intents. A case study is presented and the execution results
of this approach can be used to guide future research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03878</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03878</id><created>2015-12-12</created><authors><author><keyname>Warsi</keyname><forenames>Naqueeb Ahmad</forenames></author><author><keyname>Coon</keyname><forenames>Justin</forenames></author></authors><title>Coding for classical-quantum channels with rate limited side information
  at the encoder: An information-spectrum approach</title><categories>cs.IT math.IT quant-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the hybrid classical-quantum version of the channel coding problem
for the famous Gel'fand-Pinsker channel. In the classical setting for this
channel the conditional distribution of the channel output given the channel
input is a function of a random parameter called the channel state. We study
this problem when a rate limited version of the channel state is available at
the encoder for the classical-quantum Gel'fand-Pinsker channel. We establish
the capacity region for this problem in the information-spectrum setting. The
capacity region is quantified in terms of spectral-sup classical mutual
information rate and spectral-inf quantum mutual information rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03880</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03880</id><created>2015-12-12</created><authors><author><keyname>Gao</keyname><forenames>Jinyang</forenames></author><author><keyname>Jagadish</keyname><forenames>H. V.</forenames></author><author><keyname>Ooi</keyname><forenames>Beng Chin</forenames></author></authors><title>Active Sampler: Light-weight Accelerator for Complex Data Analytics at
  Scale</title><categories>cs.DB cs.LG stat.ML</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent years have witnessed amazing outcomes from &quot;Big Models&quot; trained by
&quot;Big Data&quot;. Most popular algorithms for model training are iterative. Due to
the surging volumes of data, we can usually afford to process only a fraction
of the training data in each iteration. Typically, the data are either
uniformly sampled or sequentially accessed.
  In this paper, we study how the data access pattern can affect model
training. We propose an Active Sampler algorithm, where training data with more
&quot;learning value&quot; to the model are sampled more frequently. The goal is to focus
training effort on valuable instances near the classification boundaries,
rather than evident cases, noisy data or outliers. We show the correctness and
optimality of Active Sampler in theory, and then develop a light-weight
vectorized implementation. Active Sampler is orthogonal to most approaches
optimizing the efficiency of large-scale data analytics, and can be applied to
most analytics models trained by stochastic gradient descent (SGD) algorithm.
Extensive experimental evaluations demonstrate that Active Sampler can speed up
the training procedure of SVM, feature selection and deep learning, for
comparable training quality by 1.6-2.2x.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03897</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03897</id><created>2015-12-12</created><authors><author><keyname>Jin</keyname><forenames>Kai</forenames></author></authors><title>Maximal Parallelograms in Convex Polygons</title><categories>cs.CG</categories><comments>60pages, 40figures, novel structure</comments><msc-class>52-06</msc-class><acm-class>I.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a convex polygon $P$ of $n$ vertices in the plane, we consider the
problem of finding the maximum area parallelogram (MAP) inside $P$. Previously,
the best algorithm for this problem runs in time $O(n^2)$, and this was
achieved by utilizing some nontrivial properties of the MAP. In this paper, we
exhibit an algorithm for finding the MAP in time $O(n\log^2n)$, greatly
improving the previous result. The main technical ingredient of our algorithm
is a new geometric structure of a convex polygon $P$, which we call $Nest(P)$.
Roughly speaking, $Nest(P)$ is an arrangement of certain line segments, each of
which is parallel to an edge of $P$. It enjoys several nice properties, e.g.,
it is a planar division over the exterior of $P$, and one natural subdivision
of it has a monotone property with the boundary of $P$. Indeed, $Nest(P)$
captures the essential nature of the MAPs; we reduce the optimization problem
of finding the MAPs in $P$ to a location query problem comprising $O(n)$ basic
location queries about $Nest(P)$. Each of these queries can be solved in
$O(\log^2n)$ time without building $Nest(P)$ explicitly. Putting these together
we obtain an $O(n\log^2n)$-time algorithm.
  We believe that the techniques developed in this paper will be useful for
solving other related problems, and the structure $Nest(P)$ is of independent
interest in convex geometry.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03899</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03899</id><created>2015-12-12</created><authors><author><keyname>Joseph</keyname><forenames>Mathew</forenames></author><author><keyname>Kuper</keyname><forenames>Gabriel</forenames></author><author><keyname>Mossakowski</keyname><forenames>Till</forenames></author><author><keyname>Serafini</keyname><forenames>Luciano</forenames></author></authors><title>Query Answering over Contextualized RDF/OWL Knowledge with
  Forall-Existential Bridge Rules: Decidable Finite Extension Classes (Post
  Print)</title><categories>cs.DB cs.AI cs.LO</categories><journal-ref>Semantic Web (IOS Press) Vol 7:1 Pages 25-61. 2016</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The proliferation of contextualized knowledge in the Semantic Web (SW) has
led to the popularity of knowledge formats such as \emph{quads} in the SW
community. A quad is an extension of an RDF triple with contextual information
of the triple. In this paper, we study the problem of query answering over
quads augmented with forall-existential bridge rules that enable
interoperability of reasoning between triples in various contexts. We call a
set of quads together with such expressive bridge rules, a quad-system. Query
answering over quad-systems is undecidable, in general. We derive decidable
classes of quad-systems, for which query answering can be done using forward
chaining. Sound, complete and terminating procedures, which are adaptations of
the well known chase algorithm, are provided for these classes for deciding
query entailment. Safe, msafe, and csafe class of quad-systems restrict the
structure of blank nodes generated during the chase computation process to be
directed acyclic graphs (DAGs) of bounded depth. RR and restricted RR classes
do not allow the generation of blank nodes during the chase computation
process. Both data and combined complexity of query entailment has been
established for the classes derived. We further show that quad-systems are
equivalent to forall-existential rules whose predicates are restricted to
ternary arity, modulo polynomial time translations. We subsequently show that
the technique of safety, strictly subsumes in expressivity, some of the well
known and expressive techniques, such as joint acyclicity and model faithful
acyclicity, used for decidability guarantees in the realm of forall-existential
rules.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03901</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03901</id><created>2015-12-12</created><authors><author><keyname>Rodriguez</keyname><forenames>Jose Israel</forenames></author><author><keyname>Tang</keyname><forenames>Xiaoxian</forenames></author></authors><title>A Probabilistic Algorithm for Computing Data-Discriminants of Likelihood
  Equations</title><categories>cs.SC</categories><comments>4 tables. arXiv admin note: substantial text overlap with
  arXiv:1501.00334</comments><acm-class>G.3; I.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An algebraic approach to the maximum likelihood estimation problem is to
solve a very structured parameterized polynomial system called likelihood
equations that have finitely many complex (real or non-real) solutions. The
only solutions that are statistically meaningful are the real solutions with
positive coordinates. In order to classify the parameters (data) according to
the number of real/positive solutions, we study how to efficiently compute the
discriminants, say data-discriminants (DD), of the likelihood equations. We
develop a probabilistic algorithm with three different strategies for computing
DDs. Our implemented probabilistic algorithm based on Maple and FGb is more
efficient than our previous version presented in ISSAC2015, and is also more
efficient than the standard elimination for larger benchmarks.
  By applying RAGlib to a DD we compute, we give the real root classification
of 3 by 3 symmetric matrix model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03911</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03911</id><created>2015-12-12</created><authors><author><keyname>Sobers</keyname><forenames>Andre</forenames></author></authors><title>BYOD and the Mobile Enterprise - Organisational challenges and solutions
  to adopt BYOD</title><categories>cs.CY</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bring Your Own Device, also known under the term BOYD refers to the trend in
employees bringing their personal mobile devices into organisations to use as a
primary device for their daily work activities. With the rapid development in
computing technology in smartphones and tablet computers and innovations in
mobile software and applications, mobile devices are becoming ever more
powerful tools for consumers to access information. Consumers are becoming more
inseparable from their personal mobile devices and development in mobile
technologies within the consumer space has led to the significance of
Consumerization. Enterprises everywhere want to introduce BYOD strategies to
improve mobility and productivity of their employees. However making the
necessary organizational changes to adopt BYOD may require a shift away from
centralized systems towards more open enterprise systems and this change can
present challenges to enterprises in particular over security, control,
technology and policy to the traditional IT model within organisations. This
paper explores some of the present challenges and solutions in relation to
mobile security, technology and policy that enterprise systems within
organisations can encounter. This paper also reviews real-life studies where
such changes were made in organisations aiming to implement BYOD. This paper
proposes a mobile enterprise model that aims to address security concerns and
the challenges of technology and policy change. This paper ends with looking
ahead to the future of mobile enterprise systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03916</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03916</id><created>2015-12-12</created><authors><author><keyname>Liu</keyname><forenames>Weiyi</forenames></author><author><keyname>Jiang</keyname><forenames>Qing</forenames></author><author><keyname>Fei</keyname><forenames>Gaolei</forenames></author><author><keyname>Yuan</keyname><forenames>Mingkai</forenames></author><author><keyname>Hu</keyname><forenames>Guangmin</forenames></author></authors><title>A Novel Methodologyof Router-To-ASMapping inspired by Community
  Discovery</title><categories>cs.NI cs.SI</categories><comments>8 pages, 9 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In the last decade many works has been done on the Internet topology at
router or autonomous system (AS) level. As routers is the essential composition
of ASes while ASes dominate the behavior of their routers. It is no doubt that
identifying the affiliation between routers and ASes can let us gain a deeper
understanding on the topology. However, the existing methods that assign a
router to an AS just based on the origin ASes of its IP addresses, which does
not make full use of information in our hand. In this paper, we propose a
methodology to assign routers to their owner ASes based on community discovery
tech. First, we use the origin ASes information along with router-pairs
similarities to construct a weighted router level topology, secondly, for
enormous topology data (more than 2M nodes and 19M edges) from CAIDA ITDK
project, we propose a fast hierarchy clustering which time and space complex
are both linear to do ASes community discovery, last we do router-to-AS mapping
based on these ASes communities. Experiments show that combining with ASes
communities our methodology discovers, the best accuracy rate of router-to-AS
mapping can reach to 82.62%, which is drastically high comparing to prior works
that stagnate on 65.44%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03921</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03921</id><created>2015-12-12</created><authors><author><keyname>Afrati</keyname><forenames>Foto</forenames></author><author><keyname>Stasinopoulos</keyname><forenames>Nikos</forenames></author><author><keyname>Ullman</keyname><forenames>Jeffrey D.</forenames></author><author><keyname>Vassilakopoulos</keyname><forenames>Angelos</forenames></author></authors><title>SharesSkew: An Algorithm to Handle Skew for Joins in MapReduce</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the problem of computing a multiway join in one
round of MapReduce when the data may be skewed. We optimize on communication
cost, i.e., the amount of data that is transferred from the mappers to the
reducers. We identify join attributes values that appear very frequently, Heavy
Hitters (HH). We distribute HH valued records to reducers avoiding skew by
using an adaptation of the Shares~\cite{AfUl} algorithm to achieve minimum
communication cost. Our algorithm is implemented for experimentation and is
offered as open source software. Furthermore, we investigate a class of
multiway joins for which a simpler variant of the algorithm can handle skew. We
offer closed forms for computing the parameters of the algorithm for chain and
symmetric joins.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03929</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03929</id><created>2015-12-12</created><authors><author><keyname>Zhao</keyname><forenames>Zhikuan</forenames></author><author><keyname>Fitzsimons</keyname><forenames>Jack K.</forenames></author><author><keyname>Fitzsimons</keyname><forenames>Joseph F.</forenames></author></authors><title>Quantum assisted Gaussian process regression</title><categories>quant-ph cs.LG stat.ML</categories><comments>4 pages. Comments welcome</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gaussian processes (GP) are a widely used model for regression problems in
supervised machine learning. Implementation of GP regression typically requires
$O(n^3)$ logic gates. We show that the quantum linear systems algorithm [Harrow
et al., Phys. Rev. Lett. 103, 150502 (2009)] can be applied to Gaussian process
regression (GPR), leading to an exponential reduction in computation time in
some instances. We show that even in some cases not ideally suited to the
quantum linear systems algorithm, a polynomial increase in efficiency still
occurs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03950</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03950</id><created>2015-12-12</created><authors><author><keyname>Sarkar</keyname><forenames>Kamal</forenames></author></authors><title>A Hidden Markov Model Based System for Entity Extraction from Social
  Media English Text at FIRE 2015</title><categories>cs.CL</categories><comments>FIRE 2015 Task:Entity Extraction from Social Media Text - Indian
  Languages (ESM-IL) - See more at:
  http://fire.irsi.res.in/fire/home#sthash.HpgiwjP5.dpuf. arXiv admin note:
  substantial text overlap with arXiv:1405.7397</comments><msc-class>68T50</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the experiments carried out by us at Jadavpur University
as part of the participation in FIRE 2015 task: Entity Extraction from Social
Media Text - Indian Languages (ESM-IL). The tool that we have developed for the
task is based on Trigram Hidden Markov Model that utilizes information like
gazetteer list, POS tag and some other word level features to enhance the
observation probabilities of the known tokens as well as unknown tokens. We
submitted runs for English only. A statistical HMM (Hidden Markov Models) based
model has been used to implement our system. The system has been trained and
tested on the datasets released for FIRE 2015 task: Entity Extraction from
Social Media Text - Indian Languages (ESM-IL). Our system is the best performer
for English language and it obtains precision, recall and F-measures of 61.96,
39.46 and 48.21 respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03953</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03953</id><created>2015-12-12</created><authors><author><keyname>Ghadiri</keyname><forenames>Mehrdad</forenames></author><author><keyname>Aghaee</keyname><forenames>Amin</forenames></author><author><keyname>Baghshah</keyname><forenames>Mahdieh Soleymani</forenames></author></authors><title>Active Distance-Based Clustering using K-medoids</title><categories>cs.LG</categories><comments>12 pages, 3 figures, PAKDD 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  k-medoids algorithm is a partitional, centroid-based clustering algorithm
which uses pairwise distances of data points and tries to directly decompose
the dataset with $n$ points into a set of $k$ disjoint clusters. However,
k-medoids itself requires all distances between data points that are not so
easy to get in many applications. In this paper, we introduce a new method
which requires only a small proportion of the whole set of distances and makes
an effort to estimate an upper-bound for unknown distances using the inquired
ones. This algorithm makes use of the triangle inequality to calculate an
upper-bound estimation of the unknown distances. Our method is built upon a
recursive approach to cluster objects and to choose some points actively from
each bunch of data and acquire the distances between these prominent points
from oracle. Experimental results show that the proposed method using only a
small subset of the distances can find proper clustering on many real-world and
synthetic datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03958</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03958</id><created>2015-12-12</created><authors><author><keyname>Lev</keyname><forenames>Guy</forenames></author><author><keyname>Sadeh</keyname><forenames>Gil</forenames></author><author><keyname>Klein</keyname><forenames>Benjamin</forenames></author><author><keyname>Wolf</keyname><forenames>Lior</forenames></author></authors><title>RNN Fisher Vectors for Action Recognition and Image Annotation</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recurrent Neural Networks (RNNs) have had considerable success in classifying
and predicting sequences. We demonstrate that RNNs can be effectively used in
order to encode sequences and provide effective representations. The
methodology we use is based on Fisher Vectors, where the RNNs are the
generative probabilistic models and the partial derivatives are computed using
backpropagation. State of the art results are obtained in two central but
distant tasks, which both rely on sequences: video action recognition and image
annotation. We also show a surprising transfer learning result from the task of
image annotation to the task of video action recognition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03965</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03965</id><created>2015-12-12</created><updated>2015-12-24</updated><authors><author><keyname>Eldan</keyname><forenames>Ronen</forenames></author><author><keyname>Shamir</keyname><forenames>Ohad</forenames></author></authors><title>The Power of Depth for Feedforward Neural Networks</title><categories>cs.LG cs.NE stat.ML</categories><comments>Revised discussion of related work</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that there are simple functions on $\mathbb{R}^d$, expressible by
small 3-layer feedforward neural networks, which cannot be approximated by any
2-layer network, to more than a certain constant accuracy, unless its width is
exponential in the dimension. The result holds for most continuous activation
functions, including rectified linear units and sigmoids, and is a formal
demonstration that depth -- even if increased by 1 -- can be exponentially more
valuable than width for standard feedforward neural networks. Moreover,
compared to related results in the context of Boolean functions, our result
requires fewer assumptions, and the proof techniques and construction are very
different.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03980</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03980</id><created>2015-12-12</created><authors><author><keyname>Ravanbakhsh</keyname><forenames>Mahdyar</forenames></author><author><keyname>Mousavi</keyname><forenames>Hossein</forenames></author><author><keyname>Rastegari</keyname><forenames>Mohammad</forenames></author><author><keyname>Murino</keyname><forenames>Vittorio</forenames></author><author><keyname>Davis</keyname><forenames>Larry S.</forenames></author></authors><title>Action Recognition with Image Based CNN Features</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most of human actions consist of complex temporal compositions of more simple
actions. Action recognition tasks usually relies on complex handcrafted
structures as features to represent the human action model. Convolutional
Neural Nets (CNN) have shown to be a powerful tool that eliminate the need for
designing handcrafted features. Usually, the output of the last layer in CNN (a
layer before the classification layer -known as fc7) is used as a generic
feature for images. In this paper, we show that fc7 features, per se, can not
get a good performance for the task of action recognition, when the network is
trained only on images. We present a feature structure on top of fc7 features,
which can capture the temporal variation in a video. To represent the temporal
components, which is needed to capture motion information, we introduced a
hierarchical structure. The hierarchical model enables to capture sub-actions
from a complex action. At the higher levels of the hierarchy, it represents a
coarse capture of action sequence and lower levels represent fine action
elements. Furthermore, we introduce a method for extracting key-frames using
binary coding of each frame in a video, which helps to improve the performance
of our hierarchical model. We experimented our method on several action
datasets and show that our method achieves superior results compared to other
state-of-the-arts methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03982</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03982</id><created>2015-12-12</created><authors><author><keyname>Chen</keyname><forenames>Zhi</forenames></author><author><keyname>Fan</keyname><forenames>Pinyi</forenames></author><author><keyname>Letaief</keyname><forenames>Khaled Ben</forenames></author></authors><title>Physical-Layer and Digital Network Coding Switching Scheme Over Two-Way
  Relaying</title><categories>cs.IT math.IT</categories><comments>11 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we consider a typical three-node, two-way relaying network
(TWRN) over fading channels. The aim is to minimize the entire system energy
usage for a TWRN in the long run, while satisfying the required average
symmetric exchange rate between the two source nodes. To this end, the energy
usage of the physical-layer network coding (PNC) or the superposition coding
based digital network coding (SPC-DNC) is analyzed. The rule on selection of
both strategies is then derived by comparison. Based on the observed rule, we
then design a scheme by switching between PNC and SPC-DNC for each channel
realization. The associated optimization problem, through PNC/DNC switching,as
well as power allocation on the uplink and the downlink for each channel
realization is formulated and solved via an iterative algorithm. It is
demonstrated that this switching scheme outperforms the schemes solely
employing PNC or SPC-DNC through both theoretical analysis and simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03985</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03985</id><created>2015-12-12</created><authors><author><keyname>Chen</keyname><forenames>Zhi</forenames></author><author><keyname>Fan</keyname><forenames>Pinyi</forenames></author><author><keyname>Letaief</keyname><forenames>Khale Ben</forenames></author></authors><title>Joint Transmit and Receive Energy Minimization Over Multi-Carrier
  Two-Way Relay Networks</title><categories>cs.IT math.IT</categories><comments>12 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, a three-node, two-way relaying network (TWRN) over a
multi-carrier system is considered. The aim is to minimize total energy usage
of such a multi-carrier TWRN with the constraint of rate requirements on both
sides, with both the transmit energy usage as well as the receive energy usage
into account. For comparison, two strategies including digital network coding
(DNC) and physical-layer network coding (PNC) are considered. The associated
total-energy-minimization problems for both strategies are formulated to be
nonconvex optimizations. To achieve global optimality, they are then
reformulated to be convex optimization problems and efficiently solved. It is
observed in numerical results that, PNC generally outperforms DNC but performs
worse in the very low SNR regime, in terms of energy usage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.03993</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.03993</id><created>2015-12-12</created><authors><author><keyname>Hahn</keyname><forenames>Meera</forenames></author><author><keyname>Chen</keyname><forenames>Si</forenames></author><author><keyname>Dehghan</keyname><forenames>Afshin</forenames></author></authors><title>Deep Tracking: Visual Tracking Using Deep Convolutional Networks</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study a discriminatively trained deep convolutional network
for the task of visual tracking. Our tracker utilizes both motion and
appearance features that are extracted from a pre-trained dual stream deep
convolution network. We show that the features extracted from our dual-stream
network can provide rich information about the target and this leads to
competitive performance against state of the art tracking methods on a visual
tracking benchmark.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04004</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04004</id><created>2015-12-13</created><authors><author><keyname>Gogineni</keyname><forenames>Vinay Chakravarthi</forenames></author><author><keyname>Mula</keyname><forenames>Subrahmanyam</forenames></author></authors><title>Convergence Analysis of Proportionate-type Least Mean Square Algorithms</title><categories>cs.SY</categories><comments>7 pages, 3 figures. Under Communication. arXiv admin note: text
  overlap with arXiv:1509.03203</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present the convergence analysis of proportionate-type
least mean square (Pt-LMS) algorithm that identifies the sparse system
effectively and more suitable for real time VLSI applications. Both first and
second order convergence analysis of Pt-LMS algorithm is studied. Optimum
convergence behavior of Pt-LMS algorithm is studied from the second order
convergence analysis provided in this paper. Simulation results were conducted
to verify the analytical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04009</identifier>
 <datestamp>2016-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04009</id><created>2015-12-13</created><updated>2016-01-14</updated><authors><author><keyname>Ying</keyname><forenames>Shenggang</forenames></author><author><keyname>Ying</keyname><forenames>Mingsheng</forenames></author><author><keyname>Feng</keyname><forenames>Yuan</forenames></author></authors><title>Quantum Privacy-Preserving Data Mining</title><categories>quant-ph cs.CR cs.DB cs.LG</categories><comments>5 pages. Comments are welcome</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data mining is a key technology in big data analytics and it can discover
understandable knowledge (patterns) hidden in large data sets. Association rule
is one of the most useful knowledge patterns, and a large number of algorithms
have been developed in the data mining literature to generate association rules
corresponding to different problems and situations. Privacy becomes a vital
issue when data mining is used to sensitive data sets like medical records,
commercial data sets and national security. In this Letter, we present a
quantum protocol for mining association rules on vertically partitioned
databases. The quantum protocol can improve the privacy level preserved by
known classical protocols and at the same time it can exponentially reduce the
computational complexity and communication cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04010</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04010</id><created>2015-12-13</created><authors><author><keyname>Khabbaz</keyname><forenames>Mohammad</forenames></author></authors><title>Finding HeavyPaths in Weighted Graphs and a Case-Study on Community
  Detection</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A heavy path in a weighted graph represents a notion of connectivity and
ordering that goes beyond two nodes. The heaviest path of length l in the
graph, simply means a sequence of nodes with edges between them, such that the
sum of edge weights is maximum among all paths of length l. It is trivial to
state the heaviest edge in the graph is the heaviest path of length 1, that
represents a heavy connection between (any) two existing nodes. This can be
generalized in many different ways for more than two nodes, one of which is
finding the heavy weight paths in the graph. In an influence network, this
represents a highway for spreading information from a node to one of its
indirect neighbors at distance l. Moreover, a heavy path implies an ordering of
nodes. For instance, we can discover which ordering of songs (tourist spots) on
a playlist (travel itinerary) is more pleasant to a user or a group of users
who enjoy all songs (tourist spots) on the playlist (itinerary). This can also
serve as a hard optimization problem, maximizing different types of quantities
of a path such as score, flow, probability or surprise, defined as edge weight.
Therefore, if one can solve the Heavy Path Problem (HPP) efficiently, they can
as well use HPP for modeling and reduce other complex problems to it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04011</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04011</id><created>2015-12-13</created><authors><author><keyname>Smith</keyname><forenames>Virginia</forenames></author><author><keyname>Forte</keyname><forenames>Simone</forenames></author><author><keyname>Jordan</keyname><forenames>Michael I.</forenames></author><author><keyname>Jaggi</keyname><forenames>Martin</forenames></author></authors><title>L1-Regularized Distributed Optimization: A Communication-Efficient
  Primal-Dual Framework</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite the importance of sparsity in many big data applications, there are
few existing methods for efficient distributed optimization of
sparsely-regularized objectives. In this paper, we present a
communication-efficient framework for L1-regularized optimization in
distributed environments. By taking a non-traditional view of classical
objectives as part of a more general primal-dual setting, we obtain a new class
of methods that can be efficiently distributed and is applicable to common
L1-regularized regression and classification objectives, such as Lasso, sparse
logistic regression, and elastic net regression. We provide convergence
guarantees for this framework and demonstrate strong empirical performance as
compared to other state-of-the-art methods on several real-world distributed
datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04013</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04013</id><created>2015-12-13</created><authors><author><keyname>Santosa</keyname><forenames>Andrew E.</forenames></author></authors><title>Comparing Weakest Precondition and Weakest Liberal Precondition</title><categories>cs.PL cs.LO</categories><acm-class>D.3.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article we investigate the relationships between the classical
notions of weakest precondition and weakest liberal precondition, and provide
several results, namely that in general, weakest liberal precondition is
neither stronger nor weaker than weakest precondition, however, given a
deterministic and terminating sequential while program and a postcondition,
they are equivalent. Hence, in such situation, it does not matter which
definition is used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04016</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04016</id><created>2015-12-13</created><authors><author><keyname>Aaronson</keyname><forenames>Scott</forenames></author><author><keyname>Ben-David</keyname><forenames>Shalev</forenames></author></authors><title>Sculpting Quantum Speedups</title><categories>quant-ph cs.CC</categories><comments>30 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a problem which is intractable for both quantum and classical
algorithms, can we find a sub-problem for which quantum algorithms provide an
exponential advantage? We refer to this problem as the &quot;sculpting problem.&quot; In
this work, we give a full characterization of sculptable functions in the query
complexity setting. We show that a total function f can be restricted to a
promise P such that Q(f|_P)=O(polylog(N)) and R(f|_P)=N^{Omega(1)}, if and only
if f has a large number of inputs with large certificate complexity. The proof
uses some interesting techniques: for one direction, we introduce new
relationships between randomized and quantum query complexity in various
settings, and for the other direction, we use a recent result from
communication complexity due to Klartag and Regev. We also characterize
sculpting for other query complexity measures, such as R(f) vs. R_0(f) and
R_0(f) vs. D(f).
  Along the way, we prove some new relationships for quantum query complexity:
for example, a nearly quadratic relationship between Q(f) and D(f) whenever the
promise of f is small. This contrasts with the recent super-quadratic query
complexity separations, showing that the maximum gap between classical and
quantum query complexities is indeed quadratic in various settings - just not
for total functions!
  Lastly, we investigate sculpting in the Turing machine model. We show that if
there is any BPP-bi-immune language in BQP, then every language outside BPP can
be restricted to a promise which places it in PromiseBQP but not in PromiseBPP.
Under a weaker assumption, that some problem in BQP is hard on average for
P/poly, we show that every paddable language outside BPP is sculptable in this
way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04017</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04017</id><created>2015-12-13</created><authors><author><keyname>Penna</keyname><forenames>Paolo</forenames></author></authors><title>The price of anarchy and stability in general noisy best-response
  dynamics</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Logit-response dynamics (Alos-Ferrer and Netzer, Games and Economic Behavior
2010) are a rich and natural class of noisy best-response dynamics. In this
work we revise the price of anarchy and the price of stability by considering
the quality of long-run equilibria in these dynamics. Our results show that
prior studies on simpler dynamics of this type can strongly depend on a
synchronous schedule of the players' moves. In particular, a small noise by
itself is not enough to improve the quality of equilibria as soon as other very
natural schedules are used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04021</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04021</id><created>2015-12-13</created><authors><author><keyname>Governatori</keyname><forenames>Guido</forenames></author><author><keyname>Olivieri</keyname><forenames>Francesco</forenames></author><author><keyname>Scannapieco</keyname><forenames>Simone</forenames></author><author><keyname>Rotolo</keyname><forenames>Antonino</forenames></author><author><keyname>Cristani</keyname><forenames>Matteo</forenames></author></authors><title>The Rationale behind the Concept of Goal</title><categories>cs.LO cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper proposes a fresh look at the concept of goal and advances that
motivational attitudes like desire, goal and intention are just facets of the
broader notion of (acceptable) outcome. We propose to encode the preferences of
an agent as sequences of &quot;alternative acceptable outcomes&quot;. We then study how
the agent's beliefs and norms can be used to filter the mental attitudes out of
the sequences of alternative acceptable outcomes. Finally, we formalise such
intuitions in a novel Modal Defeasible Logic and we prove that the resulting
formalisation is computationally feasible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04036</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04036</id><created>2015-12-13</created><authors><author><keyname>Zhong</keyname><forenames>Yangxin</forenames></author><author><keyname>Liu</keyname><forenames>Shixia</forenames></author><author><keyname>Wang</keyname><forenames>Xiting</forenames></author><author><keyname>Xiao</keyname><forenames>Jiannan</forenames></author><author><keyname>Song</keyname><forenames>Yangqiu</forenames></author></authors><title>Tracking Idea Flows between Social Groups</title><categories>cs.SI cs.LG</categories><comments>8 pages, AAAI 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many applications, ideas that are described by a set of words often flow
between different groups. To facilitate users in analyzing the flow, we present
a method to model the flow behaviors that aims at identifying the lead-lag
relationships between word clusters of different user groups. In particular, an
improved Bayesian conditional cointegration based on dynamic time warping is
employed to learn links between words in different groups. A tensor-based
technique is developed to cluster these linked words into different clusters
(ideas) and track the flow of ideas. The main feature of the tensor
representation is that we introduce two additional dimensions to represent both
time and lead-lag relationships. Experiments on both synthetic and real
datasets show that our method is more effective than methods based on
traditional clustering techniques and achieves better accuracy. A case study
was conducted to demonstrate the usefulness of our method in helping users
understand the flow of ideas between different user groups on social media
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04038</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04038</id><created>2015-12-13</created><authors><author><keyname>Liu</keyname><forenames>Mengchen</forenames></author><author><keyname>Liu</keyname><forenames>Shixia</forenames></author><author><keyname>Zhu</keyname><forenames>Xizhou</forenames></author><author><keyname>Liao</keyname><forenames>Qinying</forenames></author><author><keyname>Wei</keyname><forenames>Furu</forenames></author><author><keyname>Pan</keyname><forenames>Shimei</forenames></author></authors><title>An Uncertainty-Aware Approach for Exploratory Microblog Retrieval</title><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although there has been a great deal of interest in analyzing customer
opinions and breaking news in microblogs, progress has been hampered by the
lack of an effective mechanism to discover and retrieve data of interest from
microblogs. To address this problem, we have developed an uncertainty-aware
visual analytics approach to retrieve salient posts, users, and hashtags. We
extend an existing ranking technique to compute a multifaceted retrieval
result: the mutual reinforcement rank of a graph node, the uncertainty of each
rank, and the propagation of uncertainty among different graph nodes. To
illustrate the three facets, we have also designed a composite visualization
with three visual components: a graph visualization, an uncertainty glyph, and
a flow map. The graph visualization with glyphs, the flow map, and the
uncertainty analysis together enable analysts to effectively find the most
uncertain results and interactively refine them. We have applied our approach
to several Twitter datasets. Qualitative evaluation and two real-world case
studies demonstrate the promise of our approach for retrieving high-quality
microblog data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04039</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04039</id><created>2015-12-13</created><authors><author><keyname>Ma</keyname><forenames>Chenxin</forenames></author><author><keyname>Kone&#x10d;n&#xfd;</keyname><forenames>Jakub</forenames></author><author><keyname>Jaggi</keyname><forenames>Martin</forenames></author><author><keyname>Smith</keyname><forenames>Virginia</forenames></author><author><keyname>Jordan</keyname><forenames>Michael I.</forenames></author><author><keyname>Richt&#xe1;rik</keyname><forenames>Peter</forenames></author><author><keyname>Tak&#xe1;&#x10d;</keyname><forenames>Martin</forenames></author></authors><title>Distributed Optimization with Arbitrary Local Solvers</title><categories>cs.LG math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the growth of data and necessity for distributed optimization methods,
solvers that work well on a single machine must be re-designed to leverage
distributed computation. Recent work in this area has been limited by focusing
heavily on developing highly specific methods for the distributed environment.
These special-purpose methods are often unable to fully leverage the
competitive performance of their well-tuned and customized single machine
counterparts. Further, they are unable to easily integrate improvements that
continue to be made to single machine methods. To this end, we present a
framework for distributed optimization that both allows the flexibility of
arbitrary solvers to be used on each (single) machine locally, and yet
maintains competitive performance against other state-of-the-art
special-purpose distributed methods. We give strong primal-dual convergence
rate guarantees for our framework that hold for arbitrary local solvers. We
demonstrate the impact of local solver selection both theoretically and in an
extensive experimental comparison. Finally, we provide thorough implementation
details for our framework, highlighting areas for practical performance gains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04042</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04042</id><created>2015-12-13</created><authors><author><keyname>Liu</keyname><forenames>Shixia</forenames></author><author><keyname>Yin</keyname><forenames>Jialun</forenames></author><author><keyname>Wang</keyname><forenames>Xiting</forenames></author><author><keyname>Cui</keyname><forenames>Weiwei</forenames></author><author><keyname>Cao</keyname><forenames>Kelei</forenames></author><author><keyname>Pei</keyname><forenames>Jian</forenames></author></authors><title>Online Visual Analytics of Text Streams</title><categories>cs.IR cs.HC</categories><comments>IEEE TVCG 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an online visual analytics approach to helping users explore and
understand hierarchical topic evolution in high-volume text streams. The key
idea behind this approach is to identify representative topics in incoming
documents and align them with the existing representative topics that they
immediately follow (in time). To this end, we learn a set of streaming tree
cuts from topic trees based on user-selected focus nodes. A dynamic Bayesian
network model has been developed to derive the tree cuts in the incoming topic
trees to balance the fitness of each tree cut and the smoothness between
adjacent tree cuts. By connecting the corresponding topics at different times,
we are able to provide an overview of the evolving hierarchical topics. A
sedimentation-based visualization has been designed to enable the interactive
analysis of streaming text data from global patterns to local details. We
evaluated our method on real-world datasets and the results are generally
favorable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04047</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04047</id><created>2015-12-13</created><authors><author><keyname>van Bevern</keyname><forenames>Ren&#xe9;</forenames></author><author><keyname>Froese</keyname><forenames>Vincent</forenames></author><author><keyname>Komusiewicz</keyname><forenames>Christian</forenames></author></authors><title>Parameterizing edge modification problems above lower bounds</title><categories>cs.DM cs.DS</categories><msc-class>05C85</msc-class><acm-class>G.2.2; F.2.2; G.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a fixed graph $F$, we study the parameterized complexity of a variant of
the $F$-Free Editing problem: Given a graph $G$ and a natural number $k$, is it
possible to modify at most $k$ edges in $G$ so that the resulting graph
contains no induced subgraph isomorphic to $F$? In our variant, the input
additionally contains a vertex-disjoint packing $\mathcal{H}$ of induced
subgraphs of $G$, which provides a lower bound $h(\mathcal{H})$ on the number
of edge modifications required to transform $G$ into an $F$-free graph. While
earlier works used the number $k$ as parameter or structural parameters of the
input graph $G$, we consider the parameter $\ell:=k-h(\mathcal{H})$ instead,
that is, the number of edge modifications above the lower bound
$h(\mathcal{H})$. We show fixed-parameter tractability with respect to $\ell$
for $K_3$-Free Editing, Feedback Arc Set in Tournaments, and Cluster Editing
when the packing $\mathcal{H}$ contains subgraphs with bounded modification
cost. For $K_3$-Free Editing, we also prove NP-hardness in case of
edge-disjoint packings of $K_3$s and $\ell=0$, while for $K_q$-Free Editing and
$q\ge 6$, NP-hardness for $\ell=0$ even holds for vertex-disjoint packings of
$K_q$s.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04052</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04052</id><created>2015-12-13</created><authors><author><keyname>Murtagh</keyname><forenames>Fionn</forenames></author></authors><title>Big Data Scaling through Metric Mapping: Exploiting the Remarkable
  Simplicity of Very High Dimensional Spaces using Correspondence Analysis</title><categories>stat.ML cs.LG</categories><comments>13 pages, 3 figures</comments><msc-class>62H25</msc-class><acm-class>E.0; G.3; H.3.3; I.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present new findings in regard to data analysis in very high dimensional
spaces. We use dimensionalities up to around one million. A particular benefit
of Correspondence Analysis is its suitability for carrying out an orthonormal
mapping, or scaling, of power law distributed data. Power law distributed data
are found in many domains. Correspondence factor analysis provides a latent
semantic or principal axes mapping. Our experiments use data from digital
chemistry and finance, and other statistically generated data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04057</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04057</id><created>2015-12-13</created><authors><author><keyname>Shokri-Ghadikolaei</keyname><forenames>Hossein</forenames></author><author><keyname>Fischione</keyname><forenames>Carlo</forenames></author></authors><title>The Transitional Behavior of Interference in Millimeter Wave Networks
  and Its Impact on Medium Access Control</title><categories>cs.IT cs.PF math.IT</categories><comments>18 pages, 8 figures, 1 table, to appear in IEEE Transactions on
  Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Millimeter wave (mmWave) communication systems use large number of antenna
elements that can potentially overcome severe channel attenuation by narrow
beamforming. Narrow-beam operation in mmWave networks also reduces multiuser
interference, introducing the concept of noise-limited wireless networks as
opposed to interference-limited ones. The noise-limited or interference-limited
regime heavily reflects on the medium access control (MAC) layer throughput and
on proper resource allocation and interference management strategies. Yet,
these regimes are ignored in current approaches to mmWave MAC layer design,
with the potential disastrous consequences on the communication performance. In
this paper, we investigate these regimes in terms of collision probability and
throughput. We derive tractable closed-form expressions for the collision
probability and MAC layer throughput of mmWave ad hoc networks, operating under
slotted ALOHA. The new analysis reveals that mmWave networks may exhibit a
non-negligible transitional behavior from a noise-limited regime to an
interference-limited one, depending on the density of the transmitters, density
and size of obstacles, transmission probability, operating beamwidth, and
transmission power. Such transitional behavior necessitates a new framework of
adaptive hybrid resource allocation procedure, containing both contention-based
and contention-free phases with on-demand realization of the contention-free
phase. Moreover, the conventional collision avoidance procedure in the
contention-based phase should be revisited, due to the transitional behavior of
interference, to maximize throughput/delay performance of mmWave networks. We
conclude that, unless proper hybrid schemes are investigated, the severity of
the transitional behavior may significantly reduce throughput/delay performance
of mmWave networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04065</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04065</id><created>2015-12-13</created><authors><author><keyname>Kalantidis</keyname><forenames>Yannis</forenames></author><author><keyname>Mellina</keyname><forenames>Clayton</forenames></author><author><keyname>Osindero</keyname><forenames>Simon</forenames></author></authors><title>Cross-dimensional Weighting for Aggregated Deep Convolutional Features</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a simple and straightforward way of creating powerful image
representations via cross-dimensional weighting and aggregation of deep
convolutional neural network layer outputs. We first present a generalized
framework that encompasses a broad family of approaches and includes
cross-dimensional pooling and weighting steps. We then propose specific
non-parametric schemes for both spatial- and channel-wise weighting, that boost
the effect of highly active spatial responses and at the same time regulate
burstiness effects. We experiment on four public datasets for image search and
unsupervised fine-grained classification and show that our approach
consistently outperforms the current state-of-the-art by a large margin.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04077</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04077</id><created>2015-12-13</created><updated>2016-01-12</updated><authors><author><keyname>Mutny</keyname><forenames>Mojmir</forenames></author><author><keyname>Nair</keyname><forenames>Rahul</forenames></author><author><keyname>Gottfried</keyname><forenames>Jens-Malte</forenames></author></authors><title>Learning the Correction for Multi-Path Deviations in Time-of-Flight
  Cameras</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Multipath effect in Time-of-Flight(ToF) cameras still remains to be a
challenging problem that hinders further processing of 3D data information.
Based on the evidence from previous literature, we explored the possibility of
using machine learning techniques to correct this effect. Firstly, we created
two new datasets of of ToF images rendered via ToF simulator of LuxRender.
These two datasets contain corners in multiple orientations and with different
material properties. We chose scenes with corners as multipath effects are most
pronounced in corners. Secondly, we used this dataset to construct a learning
model to predict real valued corrections to the ToF data using Random Forests.
We found out that in our smaller dataset we were able to predict real valued
correction and improve the quality of depth images significantly by removing
multipath bias. With our algorithm, we improved relative per-pixel error from
average value of 19% to 3%. Additionally, variance of the error was lowered by
an order of magnitude.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04086</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04086</id><created>2015-12-13</created><updated>2016-02-16</updated><authors><author><keyname>Kumar</keyname><forenames>Neeraj</forenames></author><author><keyname>Karmakar</keyname><forenames>Animesh</forenames></author><author><keyname>Sharma</keyname><forenames>Ranti Dev</forenames></author><author><keyname>Mittal</keyname><forenames>Abhinav</forenames></author><author><keyname>Sethi</keyname><forenames>Amit</forenames></author></authors><title>Deep Learning-Based Image Kernel for Inductive Transfer</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a method to classify images from target classes with a small
number of training examples based on transfer learning from non-target classes.
Without using any more information than class labels for samples from
non-target classes, we train a Siamese net to estimate the probability of two
images to belong to the same class. With some post-processing, output of the
Siamese net can be used to form a gram matrix of a Mercer kernel. Coupled with
a support vector machine (SVM), such a kernel gave reasonable classification
accuracy on target classes without any fine-tuning. When the Siamese net was
only partially fine-tuned using a small number of samples from the target
classes, the resulting classifier outperformed the state-of-the-art and other
alternatives. We share class separation capabilities and insights into the
learning process of such a kernel on MNIST, Dogs vs. Cats, and CIFAR-10
datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04087</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04087</id><created>2015-12-13</created><authors><author><keyname>van Seijen</keyname><forenames>Harm</forenames></author><author><keyname>Mahmood</keyname><forenames>A. Rupam</forenames></author><author><keyname>Pilarski</keyname><forenames>Patrick M.</forenames></author><author><keyname>Machado</keyname><forenames>Marlos C.</forenames></author><author><keyname>Sutton</keyname><forenames>Richard S.</forenames></author></authors><title>True Online Temporal-Difference Learning</title><categories>cs.AI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The temporal-difference methods TD($\lambda$) and Sarsa($\lambda$) form a
core part of modern reinforcement learning. Their appeal comes from their good
performance, low computational cost, and their simple interpretation, given by
their forward view. Recently, new versions of these methods were introduced,
called true online TD($\lambda$) and true online Sarsa($\lambda$), respectively
(van Seijen and Sutton, 2014). Algorithmically, these true online methods only
make two small changes to the update rules of the regular methods, and the
extra computational cost is negligible in most cases. However, they follow the
ideas underlying the forward view much more closely. In particular, they
maintain an exact equivalence with the forward view at all times, whereas the
traditional versions only approximate it for small step-sizes. We hypothesize
that these true online methods not only have better theoretical properties, but
also dominate the regular methods empirically. In this article, we put this
hypothesis to the test by performing an extensive empirical comparison.
Specifically, we compare the performance of true online
TD($\lambda$)/Sarsa($\lambda$) with regular TD($\lambda$)/Sarsa($\lambda$) on
random MRPs, a real-world myoelectric prosthetic arm, and a domain from the
Arcade Learning Environment. We use linear function approximation with tabular,
binary, and non-binary features. Our results suggest that the true online
methods indeed dominate the regular methods. Across all domains/representations
the learning speed of the true online methods are often better, but never worse
than that of the regular methods. An additional advantage is that no choice
between traces has to be made for the true online methods. We show that new
true online temporal-difference methods can be derived by making changes to the
real-time forward view and then rewriting the update equations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04089</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04089</id><created>2015-12-13</created><authors><author><keyname>Doost-Mohammady</keyname><forenames>Rahman</forenames></author><author><keyname>Naderi</keyname><forenames>M. Yousof</forenames></author><author><keyname>Chowdhury</keyname><forenames>Kaushik Roy</forenames></author></authors><title>Performance Analysis of CSMA/CA based Medium Access in Full Duplex
  Wireless Communications</title><categories>cs.NI</categories><comments>14 pages, 16 figures, IEEE Transaction on Mobile Computing</comments><doi>10.1109/TMC.2015.2462832</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Full duplex communication promises a paradigm shift in wireless networks by
allowing simultaneous packet transmission and reception within the same
channel. While recent prototypes indicate the feasibility of this concept,
there is a lack of rigorous theoretical development on how full duplex impacts
medium access control (MAC) protocols in practical wireless networks. In this
paper, we formulate the first analytical model of a CSMA/CA based full duplex
MAC protocol for a wireless LAN network composed of an access point serving
mobile clients. There are two major contributions of our work: First, our
Markov chain-based approach results in closed form expressions of throughput
for both the access point and the clients for this new class of networks.
Second, our study provides quantitative insights on how much of the classical
hidden terminal problem can be mitigated through full duplex. We specifically
demonstrate that the improvement in the network throughput is up to 35-40\%
over the half duplex case. Our analytical models are verified through packet
level simulations in ns-2. Our results also reveal the benefit of full duplex
under varying network configuration parameters, such as number of hidden
terminals, client density, and contention window size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04092</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04092</id><created>2015-12-13</created><authors><author><keyname>Mehta</keyname><forenames>Sanket</forenames></author><author><keyname>Sodhani</keyname><forenames>Shagun</forenames></author></authors><title>Stack Exchange Tagger</title><categories>cs.CL cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of our project is to develop an accurate tagger for questions posted
on Stack Exchange. Our problem is an instance of the more general problem of
developing accurate classifiers for large scale text datasets. We are tackling
the multilabel classification problem where each item (in this case, question)
can belong to multiple classes (in this case, tags). We are predicting the tags
(or keywords) for a particular Stack Exchange post given only the question text
and the title of the post. In the process, we compare the performance of
Support Vector Classification (SVC) for different kernel functions, loss
function, etc. We found linear SVC with Crammer Singer technique produces best
results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04097</identifier>
 <datestamp>2015-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04097</id><created>2015-12-13</created><updated>2015-12-15</updated><authors><author><keyname>Calautti</keyname><forenames>Marco</forenames></author><author><keyname>Greco</keyname><forenames>Sergio</forenames></author><author><keyname>Molinaro</keyname><forenames>Cristian</forenames></author><author><keyname>Trubitsyna</keyname><forenames>Irina</forenames></author></authors><title>Using Linear Constraints for Logic Program Termination Analysis</title><categories>cs.AI</categories><comments>Under consideration in Theory and Practice of Logic Programming
  (TPLP)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is widely acknowledged that function symbols are an important feature in
answer set programming, as they make modeling easier, increase the expressive
power, and allow us to deal with infinite domains. The main issue with their
introduction is that the evaluation of a program might not terminate and
checking whether it terminates or not is undecidable. To cope with this
problem, several classes of logic programs have been proposed where the use of
function symbols is restricted but the program evaluation termination is
guaranteed. Despite the significant body of work in this area, current
approaches do not include many simple practical programs whose evaluation
terminates. In this paper, we present the novel classes of rule-bounded and
cycle-bounded programs, which overcome different limitations of current
approaches by performing a more global analysis of how terms are propagated
from the body to the head of rules. Results on the correctness, the complexity,
and the expressivity of the proposed approach are provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04103</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04103</id><created>2015-12-13</created><authors><author><keyname>Souri</keyname><forenames>Yaser</forenames></author><author><keyname>Noury</keyname><forenames>Erfan</forenames></author><author><keyname>Adeli-Mosabbeb</keyname><forenames>Ehsan</forenames></author></authors><title>Deep Relative Attributes</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Visual attributes are great means of describing images or scenes, in a way
both humans and computers understand. In order to establish a correspondence
between images and to be able to compare the strength of each property between
images, relative attributes were introduced. However, since their introduction,
hand-crafted and engineered features were used to learn complex models for the
problem of relative attributes. This limits the applicability of those methods
for more realistic cases. We introduce a two part deep learning architecture
for the task of relative attribute prediction. A convolutional neural network
(ConvNet) architecture is adopted to learn the features with addition of an
additional layer (ranking layer) that learns to rank the images based on these
features. Also an appropriate ranking loss is adapted to train the whole
network in an end-to-end fashion. Our proposed method outperforms the baseline
and state-of-the-art methods in relative attribute prediction on various
datasets. Our qualitative results also show that the network is able to learn
effective features for the task. Furthermore, we use our trained models to
visualize saliency maps for each attribute.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04105</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04105</id><created>2015-12-13</created><authors><author><keyname>Lehnert</keyname><forenames>Lucas</forenames></author><author><keyname>Precup</keyname><forenames>Doina</forenames></author></authors><title>Policy Gradient Methods for Off-policy Control</title><categories>cs.AI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Off-policy learning refers to the problem of learning the value function of a
way of behaving, or policy, while following a different policy. Gradient-based
off-policy learning algorithms, such as GTD and TDC/GQ, converge even when
using function approximation and incremental updates. However, they have been
developed for the case of a fixed behavior policy. In control problems, one
would like to adapt the behavior policy over time to become more greedy with
respect to the existing value function. In this paper, we present the first
gradient-based learning algorithms for this problem, which rely on the
framework of policy gradient in order to modify the behavior policy. We present
derivations of the algorithms, a convergence theorem, and empirical evidence
showing that they compare favorably to existing approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04106</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04106</id><created>2015-12-13</created><authors><author><keyname>Mirsadeghi</keyname><forenames>Seyed Hessam</forenames></author><author><keyname>Khonsari</keyname><forenames>Ahmad</forenames></author><author><keyname>Talebi</keyname><forenames>Mohammad Sadegh</forenames></author><author><keyname>Khodabandeloo</keyname><forenames>Behnam</forenames></author></authors><title>Scalable and Fair Admission Control for On-Chip Nanophotonic Crossbars</title><categories>cs.ET</categories><comments>17 pages, under submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Advances in CMOS-compatible photonic elements have made it plausible to
exploit nanophotonic communication to overcome the limitations of traditional
NoCs. Amongst the architectures proposed to exploit nanophotonic technology for
on-chip communication networks, optical crossbars have been shown to provide
high performance in terms of bandwidth and latency. Generally, optical
crossbars provide a huge volume of network resources that are shared among
cores. In this paper, we present a fair and efficient admission control
mechanism for shared wavelengths and buffer space in an optical crossbar. We
model the buffer management and wavelength assignment as a utility-based convex
optimization problem, whose solution determines the admission control policy.
Thanks to efficient convex optimization techniques, we obtain the globally
optimal solution of the admission control optimization problem using simple and
yet efficient iterative algorithms. Then, we cast our solution procedure as an
iterative algorithm to be implemented inside a central admission controller.
Our experimental results corroborate the efficacy of using such an admission
controller to manage the shared resources of the system. It also confirms that
the proposed admission control algorithm works well for various traffic
patterns and parameters, and promisingly evinces tractable scalability
properties as the number of cores in the crossbar increases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04107</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04107</id><created>2015-12-13</created><authors><author><keyname>Hraiech</keyname><forenames>Zeineb</forenames></author><author><keyname>Siala</keyname><forenames>Mohamed</forenames></author><author><keyname>Adelkefi</keyname><forenames>Fatma</forenames></author></authors><title>Discrete-Time Ping-pong Optimized Pulse Shaping-OFDM (POPS-OFDM)
  Operating on Time and Frequency Dispersive Channels for 5G Systems</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Fourth Generation (4G) of mobile communication systems was optimized to
offer high data rates with high terminal mobility by ensuring strict
synchronism and perfect orthogonality. However, the trend for novel
applications, that had not been feasible a few years back, reveals major limits
of this strict synchronism and imposes new challenges and severe requirements.
But, coarse synchronization can dramatically damage the waveforms orthogonality
in the Orthogonal Frequency Division Multiplexing (OFDM) signals, which results
in oppressive Inter-Carrier Interference (ICI) and Inter-Symbol Interference
(ISI). As a consequence, the use of non-orthogonal waveforms becomes further
essential in order to meet the upcoming requirements. In this context, we
propose here a novel waveform construction, referred to Ping-pong Optimized
Pulse Shaping-OFDM (POPS-OFDM), which is believed to be an attractive candidate
for the optimization of the radio interface of next 5G mobile communication
systems. Through a maximization of the Signal to Interference plus Noise Ratio
(SINR), this approach allows optimal and straightforward waveform design for
multicarrier systems at the Transmitter (Tx) and Receiver (Rx) sides. In this
paper, we analyze several characteristics of the proposed waveforms and shed
light on relevant features, which make it a powerful candidate for the design
of 5G system radio interface waveforms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04108</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04108</id><created>2015-12-13</created><authors><author><keyname>Munch</keyname><forenames>Elizabeth</forenames></author><author><keyname>Wang</keyname><forenames>Bei</forenames></author></authors><title>Convergence between Categorical Representations of Reeb Space and Mapper</title><categories>cs.CG math.AT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Reeb space, which generalizes the notion of a Reeb graph, is one of the
few tools in topological data analysis and visualization suitable for the study
of multivariate scientific datasets. First introduced by Edelsbrunner et al.
(Edelsbrunner et al. 2008), it compresses the components of the level sets of a
multivariate mapping and obtains a summary representation of their
relationships. A related construction called the mapper (Singh et al. 2007),
and a special case of mapper called the Joint Contour Net (Carr et al. 2014)
have been shown to be effective in visual analytics. Mapper and JCN are
intuitively regarded as discrete approximations of the Reeb space, however
without formal proofs or approximation guarantees. An open question has been
proposed by Dey et al. (Dey et al. 2015) as to whether the mapper converges to
the Reeb space in the limit.
  In this paper, we are interested in developing the theoretical understanding
of the relationship between the Reeb space and its discrete approximations to
support its use in practical data analysis. Using tools from category theory,
we formally prove the convergence between the Reeb space and mapper in terms of
an interleaving distance between their categorical representations. Given a
sequence of refined discretizations, we prove that these approximations
converge to the Reeb space in the interleaving distance; this also helps to
quantify the approximation quality of the discretization at a fixed resolution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04114</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04114</id><created>2015-12-13</created><updated>2016-02-19</updated><authors><author><keyname>Melis</keyname><forenames>Luca</forenames></author><author><keyname>Pyrgelis</keyname><forenames>Apostolos</forenames></author><author><keyname>De Cristofaro</keyname><forenames>Emiliano</forenames></author></authors><title>Building and Measuring Privacy-Preserving Predictive Blacklists</title><categories>cs.CR cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Collaborative approaches to network defense are increasingly used to predict
and speed up detection of attacks. In this paper, we focus on highly predictive
blacklisting, i.e., forecasting attack sources based on alerts contributed by
multiple organizations. While collaboration allows to discover groups of
correlated attacks targeting similar victims, it also raises important security
and privacy challenges. We propose a scalable privacy-friendly system,
featuring a semi-trusted authority that clusters organizations based on the
similarity of their logs. Entities in the same cluster then securely share
relevant logs and can build more accurate predictive blacklists. We present an
extensive set of measurements using real-world alerts from DShield.org and show
that available centralized algorithms for predictive blacklisting actually
achieve poor accuracy as they increase the number of false positives and
negatives. Then, we demonstrate that minimizing/optimizing information shared
across organizations improves the quality of predictions as privacy protection
does not actually limit this improvement. In fact, our methods markedly
outperform non privacy-preserving tools both in terms of precision and recall.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04115</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04115</id><created>2015-12-13</created><authors><author><keyname>Wang</keyname><forenames>Qifei</forenames></author><author><keyname>Kurillo</keyname><forenames>Gregorij</forenames></author><author><keyname>Ofli</keyname><forenames>Ferda</forenames></author><author><keyname>Bajcsy</keyname><forenames>Ruzena</forenames></author></authors><title>Unsupervised Temporal Segmentation of Repetitive Human Actions Based on
  Kinematic Modeling and Frequency Analysis</title><categories>cs.CV</categories><comments>9 pages, International Conference on 3D Vision 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a method for temporal segmentation of human
repetitive actions based on frequency analysis of kinematic parameters,
zero-velocity crossing detection, and adaptive k-means clustering. Since the
human motion data may be captured with different modalities which have
different temporal sampling rate and accuracy (e.g., optical motion capture
systems vs. Microsoft Kinect), we first apply a generic full-body kinematic
model with an unscented Kalman filter to convert the motion data into a unified
representation that is robust to noise. Furthermore, we extract the most
representative kinematic parameters via the primary frequency analysis. The
sequences are segmented based on zero-velocity crossing of the selected
parameters followed by an adaptive k-means clustering to identify the
repetition segments. Experimental results demonstrate that for the motion data
captured by both the motion capture system and the Microsoft Kinect, our
proposed algorithm obtains robust segmentation of repetitive action sequences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04116</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04116</id><created>2015-12-13</created><authors><author><keyname>Guri</keyname><forenames>Mordechai</forenames></author><author><keyname>Poliak</keyname><forenames>Yuri</forenames></author><author><keyname>Shapira</keyname><forenames>Bracha</forenames></author><author><keyname>Elovici</keyname><forenames>Yuval</forenames></author></authors><title>JoKER: Trusted Detection of Kernel Rootkits in Android Devices via JTAG
  Interface</title><categories>cs.CR</categories><comments>IEEE TrustCom 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Smartphones and tablets have become prime targets for malware, due to the
valuable private and corporate information they hold. While Anti-Virus (AV)
program may successfully detect malicious applications (apps), they remain
ineffective against low-level rootkits that evade detection mechanisms by
masking their own presence. Furthermore, any detection mechanism run on the
same physical device as the monitored OS can be compromised via application,
kernel or boot-loader vulnerabilities. Consequentially, trusted detection of
kernel rootkits in mobile devices is a challenging task in practice. In this
paper we present JoKER - a system which aims at detecting rootkits in the
Android kernel by utilizing the hardware's Joint Test Action Group (JTAG)
interface for trusted memory forensics. Our framework consists of components
that extract areas of a kernel's memory and reconstruct it for further
analysis. We present the overall architecture along with its implementation,
and demonstrate that the system can successfully detect the presence of
stealthy rootkits in the kernel. The results show that although JTAG's main
purpose is system testing, it can also be used for malware detection where
traditional methods fail.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04118</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04118</id><created>2015-12-13</created><authors><author><keyname>Liu</keyname><forenames>Jiongxin</forenames></author><author><keyname>Li</keyname><forenames>Yinxiao</forenames></author><author><keyname>Allen</keyname><forenames>Peter</forenames></author><author><keyname>Belhumeur</keyname><forenames>Peter</forenames></author></authors><title>Articulated Pose Estimation Using Hierarchical Exemplar-Based Models</title><categories>cs.CV</categories><comments>8 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Exemplar-based models have achieved great success on localizing the parts of
semi-rigid objects. However, their efficacy on highly articulated objects such
as humans is yet to be explored. Inspired by hierarchical object representation
and recent application of Deep Convolutional Neural Networks (DCNNs) on human
pose estimation, we propose a novel formulation that incorporates both
hierarchical exemplar-based models and DCNNs in the spatial terms.
Specifically, we obtain more expressive spatial models by assuming independence
between exemplars at different levels in the hierarchy; we also obtain stronger
spatial constraints by inferring the spatial relations between parts at the
same level. As our method strikes a good balance between expressiveness and
strength of spatial models, it is both effective and generalizable, achieving
state-of-the-art results on different benchmarks: Leeds Sports Dataset and
CUB-200-2011.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04122</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04122</id><created>2015-12-13</created><authors><author><keyname>Abah</keyname><forenames>Joshua</forenames></author><author><keyname>O.</keyname><forenames>Waziri</forenames><suffix>V</suffix></author><author><keyname>B</keyname><forenames>Abdullahi M.</forenames></author><author><keyname>M</keyname><forenames>Arthur U.</forenames></author><author><keyname>S</keyname><forenames>Adewale O.</forenames></author></authors><title>A machine learning approach to anomaly-based detection on Android
  platforms</title><categories>cs.CR</categories><comments>This is a 21 pages paper that reports research findings</comments><journal-ref>International Journal of Network Security &amp; Its Applications,
  Vol.7,No.6, pp.15-35 (2015)</journal-ref><doi>10.5121/ijnsa.2015.7602</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The emergence of mobile platforms with increased storage and computing
capabilities and the pervasive use of these platforms for sensitive
applications such as online banking, e-commerce and the storage of sensitive
information on these mobile devices have led to increasing danger associated
with malware targeted at these devices. Detecting such malware presents
inimitable challenges as signature-based detection techniques available today
are becoming inefficient in detecting new and unknown malware. In this
research, a machine learning approach for the detection of malware on Android
platforms is presented. The detection system monitors and extracts features
from the applications while in execution and uses them to perform in-device
detection using a trained K-Nearest Neighbour classifier. Results shows high
performance in the detection rate of the classifier with accuracy of 93.75%,
low error rate of 6.25% and low false positive rate with ability of detecting
real Android malware.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04127</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04127</id><created>2015-12-13</created><authors><author><keyname>Chen</keyname><forenames>Shuoshuo</forenames></author><author><keyname>Mizero</keyname><forenames>Fabrice</forenames></author></authors><title>A Survey on Security in Named Data Networking</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the past three decades, since its invention, the Internet has evolved in
both its sheer volume and usage. The Internet's core protocol, Internet
Protocol (IP), has proven its usability and effectiveness to support a
communication network. However, current Internet usage requires more than a
communication network due to a shift in the nature of Internet applications
from simple email application to large content producers such as NetFlix,
Google, Amazon, etc. Named Data Networking (NDN) is one of the few
initiatives/projects addressing the shortcomings of the current Internet
architecture and intends to move the Internet toward a content distribution
architecture. In this paper, we conduct a brief survey of security
topics/problems inherent to the NDN architecture. Specifically, we describe
current known problems and propose solutions to major security problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04133</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04133</id><created>2015-12-13</created><authors><author><keyname>Cushen</keyname><forenames>George</forenames></author></authors><title>A Person Re-Identification System For Mobile Devices</title><categories>cs.CV cs.CR cs.IR</categories><comments>Appearing in Proceedings of the 11th IEEE/ACM International
  Conference on Signal Image Technology &amp; Internet Systems (SITIS 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Person re-identification is a critical security task for recognizing a person
across spatially disjoint sensors. Previous work can be computationally
intensive and is mainly based on low-level cues extracted from RGB data and
implemented on a PC for a fixed sensor network (such as traditional CCTV). We
present a practical and efficient framework for mobile devices (such as smart
phones and robots) where high-level semantic soft biometrics are extracted from
RGB and depth data. By combining these cues, our approach attempts to provide
robustness to noise, illumination, and minor variations in clothing. This
mobile approach may be particularly useful for the identification of persons in
areas ill-served by fixed sensors or for tasks where the sensor position and
direction need to dynamically adapt to a target. Results on the BIWI dataset
are preliminary but encouraging. Further evaluation and demonstration of the
system will be available on our website.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04134</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04134</id><created>2015-12-13</created><authors><author><keyname>Wang</keyname><forenames>Qifei</forenames></author><author><keyname>Kurillo</keyname><forenames>Gregorij</forenames></author><author><keyname>Ofli</keyname><forenames>Ferda</forenames></author><author><keyname>Bajcsy</keyname><forenames>Ruzena</forenames></author></authors><title>Evaluation of Pose Tracking Accuracy in the First and Second Generations
  of Microsoft Kinect</title><categories>cs.CV cs.AI</categories><comments>10 pages, IEEE International Conference on Healthcare Informatics
  2015 (ICHI 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Microsoft Kinect camera and its skeletal tracking capabilities have been
embraced by many researchers and commercial developers in various applications
of real-time human movement analysis. In this paper, we evaluate the accuracy
of the human kinematic motion data in the first and second generation of the
Kinect system, and compare the results with an optical motion capture system.
We collected motion data in 12 exercises for 10 different subjects and from
three different viewpoints. We report on the accuracy of the joint localization
and bone length estimation of Kinect skeletons in comparison to the motion
capture. We also analyze the distribution of the joint localization offsets by
fitting a mixture of Gaussian and uniform distribution models to determine the
outliers in the Kinect motion data. Our analysis shows that overall Kinect 2
has more robust and more accurate tracking of human pose as compared to Kinect
1.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04138</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04138</id><created>2015-12-13</created><authors><author><keyname>Stephens-Davidowitz</keyname><forenames>Noah</forenames></author></authors><title>Search-to-Decision Reductions for Lattice Problems with Approximation
  Factors (Slightly) Greater Than One</title><categories>cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show the first dimension-preserving search-to-decision reductions for
approximate SVP and CVP. In particular, for any $\gamma \leq 1 + O(\log n/n)$
and any constant $C &gt; 0$, we obtain an efficient dimension-preserving reduction
from $\gamma^{C n/\log n}$-SVP to $\gamma$-GapSVP and an efficient
dimension-preserving reduction from $\gamma^{Cn}$-CVP to $\gamma$-GapCVP. These
results generalize the known equivalence of the search and decision versions of
these problems in the exact case when $\gamma = 1$.
  Both of these efficient reductions follow from more general
dimension-preserving reductions whose running times depend on the search and
decision approximation factors. Indeed, the SVP result is actually slightly
stronger, as we obtain a reduction to $\gamma$-unique SVP for the same values
of $\gamma$, a potentially easier problem.
  We also derive some interesting corollaries. For example, we give an improved
hardness result for $\gamma$-unique SVP. We show NP-hardness (under randomized
reductions) for any $\gamma \leq 1+O(\log n/n)$, improving on Aggarwal and
Dubey, who showed hardness for $\gamma \leq 1 + 1/n^{C^*}$ for some large
unspecified constant $C^*$. And, we obtain an efficient dimension-preserving
reduction from $\sqrt{n}$-CVP to $\gamma$-unique SVP (or $\gamma$-GapSVP),
again for any $\gamma \leq 1 + O(\log n/n)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04143</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04143</id><created>2015-12-13</created><authors><author><keyname>Bell</keyname><forenames>Sean</forenames></author><author><keyname>Zitnick</keyname><forenames>C. Lawrence</forenames></author><author><keyname>Bala</keyname><forenames>Kavita</forenames></author><author><keyname>Girshick</keyname><forenames>Ross</forenames></author></authors><title>Inside-Outside Net: Detecting Objects in Context with Skip Pooling and
  Recurrent Neural Networks</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well known that contextual and multi-scale representations are
important for accurate visual recognition. In this paper we present the
Inside-Outside Net (ION), an object detector that exploits information both
inside and outside the region of interest. Contextual information outside the
region of interest is integrated using spatial recurrent neural networks.
Inside, we use skip pooling to extract information at multiple scales and
levels of abstraction. Through extensive experiments we evaluate the design
space and provide readers with an overview of what tricks of the trade are
important. ION improves state-of-the-art on PASCAL VOC 2012 object detection
from 73.9% to 76.4% mAP. On the new and more challenging MS COCO dataset, we
improve state-of-art-the from 19.7% to 33.1% mAP. In the 2015 MS COCO Detection
Challenge, our ION model won the Best Student Entry and finished 3rd place
overall. As intuition suggests, our detection results provide strong evidence
that context and multi-scale representations improve small object detection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04150</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04150</id><created>2015-12-13</created><authors><author><keyname>Zhou</keyname><forenames>Bolei</forenames></author><author><keyname>Khosla</keyname><forenames>Aditya</forenames></author><author><keyname>Lapedriza</keyname><forenames>Agata</forenames></author><author><keyname>Oliva</keyname><forenames>Aude</forenames></author><author><keyname>Torralba</keyname><forenames>Antonio</forenames></author></authors><title>Learning Deep Features for Discriminative Localization</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we revisit the global average pooling layer proposed in [13],
and shed light on how it explicitly enables the convolutional neural network to
have remarkable localization ability despite being trained on image-level
labels. While this technique was previously proposed as a means for
regularizing training, we find that it actually builds a generic localizable
deep representation that can be applied to a variety of tasks. Despite the
apparent simplicity of global average pooling, we are able to achieve 37.1%
top-5 error for object localization on ILSVRC 2014, which is remarkably close
to the 34.2% top-5 error achieved by a fully supervised CNN approach. We
demonstrate that our network is able to localize the discriminative image
regions on a variety of tasks despite not being trained for them
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04152</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04152</id><created>2015-12-13</created><authors><author><keyname>Abernethy</keyname><forenames>Jacob</forenames></author><author><keyname>Lee</keyname><forenames>Chansoo</forenames></author><author><keyname>Tewari</keyname><forenames>Ambuj</forenames></author></authors><title>Fighting Bandits with a New Kind of Smoothness</title><categories>cs.LG cs.GT stat.ML</categories><comments>In Proceedings of NIPS, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define a novel family of algorithms for the adversarial multi-armed bandit
problem, and provide a simple analysis technique based on convex smoothing. We
prove two main results. First, we show that regularization via the
\emph{Tsallis entropy}, which includes EXP3 as a special case, achieves the
$\Theta(\sqrt{TN})$ minimax regret. Second, we show that a wide class of
perturbation methods achieve a near-optimal regret as low as $O(\sqrt{TN \log
N})$ if the perturbation distribution has a bounded hazard rate. For example,
the Gumbel, Weibull, Frechet, Pareto, and Gamma distributions all satisfy this
key property.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04156</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04156</id><created>2015-12-13</created><authors><author><keyname>Ni</keyname><forenames>Minming</forenames></author><author><keyname>Pan</keyname><forenames>Jianping</forenames></author><author><keyname>Hu</keyname><forenames>Miao</forenames></author><author><keyname>Zhong</keyname><forenames>Zhangdui</forenames></author></authors><title>Recursion-based Analysis for Information Propagation in Vehicular Ad Hoc
  Networks</title><categories>cs.NI</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Effective inter-vehicle communication is fundamental to a decentralized
traffic information system based on Vehicular Ad Hoc Networks (VANETs). To
reflect the uncertainty of the information propagation, most of the existing
work was conducted by assuming the inter-vehicle distance follows some specific
probability models, e.g., the lognormal or exponential distribution, while
reducing the analysis complexity. Aimed at providing more generic results, a
recursive modeling framework is proposed for VANETs in this paper when the
vehicle spacing can be captured by a general i.i.d. distribution. With the
framework, the analytical expressions for a series of commonly discussed
metrics are derived respectively, including the mean, variance, probability
distribution of the propagation distance, and expectation for the number of
vehicles included in a propagation process, when the transmission failures are
mainly caused by MAC contentions. Moreover, a discussion is also made for
demonstrating the efficiency of the recursive analysis method when the impact
of channel fading is also considered. All the analytical results are verified
by extensive simulations. We believe that this work is able to potentially
reveal a more insightful understanding of information propagation in VANETs by
allowing to evaluate the effect of any vehicle headway distributions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04170</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04170</id><created>2015-12-13</created><authors><author><keyname>Deshpande</keyname><forenames>Amit</forenames></author><author><keyname>Harsha</keyname><forenames>Prahladh</forenames></author><author><keyname>Venkat</keyname><forenames>Rakesh</forenames></author></authors><title>Embedding approximately low-dimensional $\ell_2^2$ metrics into $\ell_1$</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Goemans showed that any $n$ points $x_1, \dotsc x_n$ in $d$-dimensions
satisfying $\ell_2^2$ triangle inequalities can be embedded into $\ell_{1}$,
with worst-case distortion at most $\sqrt{d}$. We extend this to the case when
the points are approximately low-dimensional, albeit with average distortion
guarantees. More precisely, we give an $\ell_{2}^{2}$-to-$\ell_{1}$ embedding
with average distortion at most the stable rank, $\mathrm{sr}(M)$, of the
matrix $M$ consisting of columns $\{x_i-x_j\}_{i&lt;j}$. Average distortion
embedding suffices for applications such as the Sparsest Cut problem. Our
embedding gives an approximation algorithm for the \sparsestcut problem on low
threshold-rank graphs, where earlier work was inspired by Lasserre SDP
hierarchy, and improves on a previous result of the first and third author
[Deshpande and Venkat, In Proc. 17th APPROX, 2014]. Our ideas give a new
perspective on $\ell_{2}^{2}$ metric, an alternate proof of Goemans' theorem,
and a simpler proof for average distortion $\sqrt{d}$. Furthermore, while the
seminal result of Arora, Rao and Vazirani giving a $O(\sqrt{\log n})$ guarantee
for Uniform Sparsest Cut can be seen to imply Goemans' theorem with average
distortion, our work opens up the possibility of proving such a result directly
via a Goemans'-like theorem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04177</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04177</id><created>2015-12-14</created><authors><author><keyname>DeDeo</keyname><forenames>Simon</forenames></author></authors><title>Conflict and Computation on Wikipedia: a Finite-State Machine Analysis
  of Editor Interactions</title><categories>cs.SI physics.soc-ph q-bio.PE</categories><comments>17 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  What is the boundary between a vigorous argument and a breakdown of
relations? What drives a group of individuals across it? Taking Wikipedia as a
test case, we use a hidden Markov model to approximate the computational
structure and social grammar of more than a decade of cooperation and conflict
among its editors. Across a wide range of pages, we discover a bursty war/peace
structure where the systems can become trapped---sometimes for months---in a
computational subspace associated with high levels of rapid-fire conflict.
Distinct patterns of behavior sustain the low-conflict subspace, including
tit-for-tat reversion. While a fraction of the transitions between these
subspaces are associated with top-down actions taken by administrators, the
effects are weak and of uncertain valence. Surprisingly, we find no statistical
signal that transitions are associated with the appearance of particularly
anti-social users, and only weak association with significant news events
outside the system. The majority of transitions between high and low conflict
states appear to be driven by decentralized processes with no clear locus of
control. Our results show how, in a modern sociotechnical system, memory of
conflict is delocalized, and conflict management is a bottom-up process. It
suggests that policy-makers may be limited in their ability to manage conflict,
and that bad actors and exogenous shocks are less effective in causing conflict
than is generally believed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04180</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04180</id><created>2015-12-14</created><authors><author><keyname>Wu</keyname><forenames>Hao-Hsiang</forenames></author><author><keyname>Kucukyavuz</keyname><forenames>Simge</forenames></author></authors><title>Maximizing Influence in Social Networks: A Two-Stage Stochastic
  Programming Approach That Exploits Submodularity</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the influence maximization problem arising in social networks. In
contrast to existing studies that involve greedy approximation algorithms with
a 63% performance guarantee, our work focuses on solving the problem optimally.
We propose a Benders decomposition algorithm to find the optimal solution to
the problem with a finite number of samples. We show that the submodularity of
the influence function can be exploited to develop optimality cuts that are
more effective than the standard optimality cuts available in the literature.
We prove that the submodular cuts are facet-defining for the influence
maximization problem under certain conditions. Furthermore, we give an
extension of this algorithm to solve general two-stage stochastic programs
where the second-stage value function is submodular. Finally, we report our
computational experiments, which show that our proposed algorithm outperforms
the greedy algorithm for problems with a moderate number of scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04188</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04188</id><created>2015-12-14</created><authors><author><keyname>Radhakrishnan</keyname><forenames>Jaikumar</forenames></author><author><keyname>Shannigrahi</keyname><forenames>Saswata</forenames></author><author><keyname>Venkat</keyname><forenames>Rakesh</forenames></author></authors><title>Hypergraph Two-Coloring in the Streaming Model</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider space-efficient algorithms for two-coloring $n$-uniform
hypergraphs $H=(V,E)$ in the streaming model, when the hyperedges arrive one at
a time. It is known that any such hypergraph with at most $0.7
\sqrt{\frac{n}{\ln n}} 2^n$ hyperedges has a two-coloring [Radhakrishnan &amp;
Srinivasan, RSA, 2000], which can be found deterministically in polynomial
time, if allowed full access to the input.
  1. Let $s^D(v, q, n)$ be the minimum space used by a deterministic one-pass
streaming algorithm that on receiving an $n$-uniform hypergraph $H$ on $v$
vertices and $q$ hyperedges $(q \leq 0.7 \sqrt{\frac{n}{\ln n}} 2^n)$, produces
a proper two-coloring of $H$. We show that $s^D(n^2, q, n) = \Omega(q/n)$. This
shows that no efficient deterministic streaming algorithm can match the
performance of the Radhakrishnan-Srinivasan algorithm.
  2. Let $s^R(v, q,n)$ be the minimum space used by a randomized one-pass
streaming algorithm that on receiving an $n$-uniform hypergraph $H$ on $v$
vertices and $q$ hyperedges with high probability produces a proper
two-coloring of $H$ (or declares failure). We show that $s^R(v,
\frac{1}{10}\sqrt{\frac{n}{\ln n}} 2^n, n) = O(v \log v)$ by giving an
efficient randomized streaming algorithm.
  3. We show that for any $4 \leq t \leq \frac{n^2}{2n-1}$,
$s^R\left(\frac{n^2}{t}, 2^{n-2}\exp(\frac{t}{8}), n\right) = O(n^2/t)$; in
particular, every $n$-uniform hypergraph with at most $\frac{n^2}{t}$ vertices
and $ 2^{n-1}\exp(\frac{t}{8})$ hyperedges is two-colorable. This shows that
every non-two-colorable hypergraph on $o(n^2)$ vertices has $\omega(n^22^n)$
hyperedges.
  The above results are inspired by the study of the number $q(n)$, the minimum
possible number of hyperedges in a $n$-uniform hypergraph that is not
two-colorable. It is known that $q(n) = \Omega(\sqrt{\frac{n}{\ln n}})$
[Radhakrishnan-Srinivasan] and $ q(n)= O(n^2 2^n)$ [Erdos, 1963].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04200</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04200</id><created>2015-12-14</created><authors><author><keyname>Kolay</keyname><forenames>Sudeshna</forenames></author><author><keyname>Panolan</keyname><forenames>Fahad</forenames></author><author><keyname>Raman</keyname><forenames>Venkatesh</forenames></author><author><keyname>Saurabh</keyname><forenames>Saket</forenames></author></authors><title>Parameterized Algorithms on Perfect Graphs for deletion to
  $(r,\ell)$-graphs</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For fixed integers $r,\ell \geq 0$, a graph $G$ is called an {\em
$(r,\ell)$-graph} if the vertex set $V(G)$ can be partitioned into $r$
independent sets and $\ell$ cliques. The class of $(r, \ell)$ graphs
generalizes $r$-colourable graphs (when $\ell =0)$ and hence not surprisingly,
determining whether a given graph is an $(r, \ell)$-graph is \NP-hard even when
$r \geq 3$ or $\ell \geq 3$ in general graphs.
  When $r$ and $\ell$ are part of the input, then the recognition problem is
NP-hard even if the input graph is a perfect graph (where the {\sc Chromatic
Number} problem is solvable in polynomial time). It is also known to be
fixed-parameter tractable (FPT) on perfect graphs when parameterized by $r$ and
$\ell$. I.e. there is an $f(r+\ell) \cdot n^{\Oh(1)}$ algorithm on perfect
graphs on $n$ vertices where $f$ is some (exponential) function of $r$ and
$\ell$.
  In this paper, we consider the parameterized complexity of the following
problem, which we call {\sc Vertex Partization}. Given a perfect graph $G$ and
positive integers $r,\ell,k$ decide whether there exists a set $S\subseteq
V(G)$ of size at most $k$ such that the deletion of $S$ from $G$ results in an
$(r,\ell)$-graph. We obtain the following results: \begin{enumerate} \item {\sc
Vertex Partization} on perfect graphs is FPT when parameterized by $k+r+\ell$.
\item The problem does not admit any polynomial sized kernel when parameterized
by $k+r+\ell$. In other words, in polynomial time, the input graph can not be
compressed to an equivalent instance of size polynomial in $k+r+\ell$. In fact,
our result holds even when $k=0$.
  \item When $r,\ell$ are universal constants, then {\sc Vertex Partization} on
perfect graphs, parameterized by $k$, has a polynomial sized kernel.
\end{enumerate}
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04202</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04202</id><created>2015-12-14</created><authors><author><keyname>Li</keyname><forenames>Xi-Lin</forenames></author></authors><title>Preconditioned Stochastic Gradient Descent</title><categories>stat.ML cs.LG</categories><comments>Matlab package reproducing all the reported results are available by
  contacting the author</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stochastic gradient descent (SGD) still is the workhorse for many practical
problems. However, it converges slow, and can be difficult to tune. It is
possible to precondition SGD to accelerate its convergence remarkably. But many
attempts in this direction either aim at solving specialized problems, or
result in significantly more complicated methods than SGD. This paper proposes
a new way to estimate a preconditioner by equalizing the amplitudes of
parameter changes and the amplitudes of associated gradient changes. Unlike the
Hessian inverse like preconditioners based on secant equation fitting as done
in deterministic quasi-Newton methods, which work the best when the Hessian is
positive definite, the new preconditioner works equally well for both convex
and non-convex optimizations. When stochastic gradient is used, it can
naturally damp the gradient noise to stabilize SGD. Efficient preconditioner
estimation methods are developed, and with reasonable simplifications, they are
applicable to large scaled problems. Experimental results demonstrate that
equipped with the new preconditioner, without any tuning effort, preconditioned
SGD can efficiently solve many challenging problems like the training of a deep
neural network or a recurrent neural network requiring extremely long term
memories.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04205</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04205</id><created>2015-12-14</created><authors><author><keyname>Erichson</keyname><forenames>N. Benjamin</forenames></author><author><keyname>Brunton</keyname><forenames>Steven L.</forenames></author><author><keyname>Kutz</keyname><forenames>J. Nathan</forenames></author></authors><title>Compressed Dynamic Mode Decomposition for Real-Time Object Detection</title><categories>cs.CV</categories><comments>Preprint submitted to Journal of Real-Time Image Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the method of compressive dynamic mode decomposition (cDMD) for
robustly performing real-time foreground/background separation in
high-definition video. The DMD method provides a regression technique for
least-square fitting of video snapshots to a linear dynamical system. The
method integrates two of the leading data analysis methods in use today:
Fourier transforms and Principal Components. DMD modes with temporal Fourier
frequencies near the origin (zero-modes) are interpreted as background
(low-rank) portions of the given video frames, and the terms with Fourier
frequencies bounded away from the origin are their foreground (sparse)
counterparts. When combined with compression techniques, the resulting cDMD can
process full HD video feeds in real-time on CPU computing platforms while still
maintaining competitive video decomposition quality, quantified by F-measure,
Recall and Precision. On a GPU architecture, the method is significantly faster
than real-time, allowing for further video processing to improve the separation
quality and/or enacting further computer vision processes such as object
recognition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04208</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04208</id><created>2015-12-14</created><authors><author><keyname>Wu</keyname><forenames>Chenxia</forenames></author><author><keyname>Zhang</keyname><forenames>Jiemi</forenames></author><author><keyname>Selman</keyname><forenames>Bart</forenames></author><author><keyname>Savarese</keyname><forenames>Silvio</forenames></author><author><keyname>Saxena</keyname><forenames>Ashutosh</forenames></author></authors><title>Watch-Bot: Unsupervised Learning for Reminding Humans of Forgotten
  Actions</title><categories>cs.RO cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a robotic system that watches a human using a Kinect v2 RGB-D
sensor, detects what he forgot to do while performing an activity, and if
necessary reminds the person using a laser pointer to point out the related
object. Our simple setup can be easily deployed on any assistive robot.
  Our approach is based on a learning algorithm trained in a purely
unsupervised setting, which does not require any human annotations. This makes
our approach scalable and applicable to variant scenarios. Our model learns the
action/object co-occurrence and action temporal relations in the activity, and
uses the learned rich relationships to infer the forgotten action and the
related object. We show that our approach not only improves the unsupervised
action segmentation and action cluster assignment performance, but also
effectively detects the forgotten actions on a challenging human activity RGB-D
video dataset. In robotic experiments, we show that our robot is able to remind
people of forgotten actions successfully.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04214</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04214</id><created>2015-12-14</created><authors><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author><author><keyname>Etzkowitz</keyname><forenames>Henry</forenames></author><author><keyname>Kushnir</keyname><forenames>Duncan</forenames></author></authors><title>The Globalization of Academic Entrepreneurship? The Recent Growth
  (2009-2014) in University Patenting Decomposed</title><categories>cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The contribution of academia to US patents has become increasingly global.
Following a pause, with a relatively flat rate, from 1998 to 2008, the
long-term trend of university patenting rising as a share of all patenting has
resumed, driven by the internationalization of academic entrepreneurship and
the persistence of US university technology transfer. We disaggregate this
recent growth in university patenting at the US Patent and Trademark
Organization (USPTO) in terms of nations and patent classes. Foreign patenting
in the US has almost doubled during the period 2009-2014, mainly due to
patenting by universities in Taiwan, Korea, China, and Japan. These nations
compete with the US in terms of patent portfolios, whereas most European
countries--with the exception of the UK--have more specific portfolios, mainly
in the bio-medical fields. In the case of China, Tsinghua University holds 63%
of the university patents in USPTO, followed by King Fahd University with 55.2%
of the national portfolio.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04219</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04219</id><created>2015-12-14</created><authors><author><keyname>Ruland</keyname><forenames>Thomas</forenames></author></authors><title>On the Relation between two Rotation Metrics</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In their work &quot;Global Optimization through Rotation Space Search&quot;, Richard
Hartley and Fredrik Kahl introduce a global optimization strategy for problems
in geometric computer vision, based on rotation space search using a
branch-and-bound algorithm. In its core, Lemma 2 of their publication is the
important foundation for a class of global optimization algorithms, which is
adopted over a wide range of problems in subsequent publications. This lemma
relates a metric on rotations represented by rotation matrices with a metric on
rotations in axis-angle representation. This work focuses on a proof for this
relationship, which is based on Rodrigues' Rotation Theorem for the composition
of rotations in axis-angle representation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04222</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04222</id><created>2015-12-14</created><authors><author><keyname>Charron-Bost</keyname><forenames>Bernadette</forenames></author><author><keyname>F&#xfc;gger</keyname><forenames>Matthias</forenames></author><author><keyname>Nowak</keyname><forenames>Thomas</forenames></author></authors><title>Amortized Averaging Algorithms for Approximate Consensus</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new class of distributed algorithms for the approximate
consensus problem in dynamic rooted networks, which we call amortized averaging
algorithms. They are deduced from ordinary averaging algorithms by adding a
value-gathering phase before each value update. This allows their decision time
to drop from being exponential in the number $n$ of processes to being linear
under the assumption that each process knows $n$. In particular, the amortized
midpoint algorithm, which achieves a linear decision time, works in completely
anonymous dynamic rooted networks where processes can exchange and store
continuous values, and under the assumption that the number of processes is
known to all processes. We then study the way amortized averaging algorithms
degrade when communication graphs are from time to time non rooted, or with a
wrong estimate of the number of processes. Finally, we analyze the amortized
midpoint algorithm under the additional constraint that processes can only
store and send quantized values, and get as a corollary that the 2-set
consensus problem is solvable in linear time in any rooted dynamic network
model when allowing all decision values to be in the range of initial values.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04226</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04226</id><created>2015-12-14</created><updated>2016-02-11</updated><authors><author><keyname>G&#xe4;rtner</keyname><forenames>Bernd</forenames></author><author><keyname>Lengler</keyname><forenames>Johannes</forenames></author><author><keyname>Szedlak</keyname><forenames>May</forenames></author></authors><title>Random Sampling with Removal</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Random sampling is a classical tool in constrained optimization. Under
favorable conditions, the optimal solution subject to a small subset of
randomly chosen constraints violates only a small subset of the remaining
constraints. Here we study the following variant that we call random sampling
with removal: suppose that after sampling the subset, we remove a fixed number
of constraints from the sample, according to an arbitrary rule. Is it still
true that the optimal solution of the reduced sample violates only a small
subset of the constraints?
  The question naturally comes up in situations where the solution subject to
the sampled constraints is used as an approximate solution to the original
problem. In this case, it makes sense to improve cost and volatility of the
sample solution by removing some of the constraints that appear most
restricting. At the same time, the approximation quality (measured in terms of
violated constraints) should remain high.
  We study random sampling with removal in a generalized, completely abstract
setting where we assign to each subset $R$ of the constraints an arbitrary set
$V(R)$ of constraints disjoint from $R$; in applications, $V(R)$ corresponds to
the constraints violated by the optimal solution subject to only the
constraints in R. Furthermore, our results are parametrized by the dimension
$\delta$.
  In this setting, we prove matching upper and lower bounds for the expected
number of constraints violated by a random sample, after the removal of $k$
elements. For a large range of values of $k$, the new upper bounds improve the
previously best bounds for LP-type problems, which moreover had only been known
in special cases. We show that this bound on special LP-type problems, can be
derived in the much more general setting of violator spaces, and with very
elementary proofs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04234</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04234</id><created>2015-12-14</created><authors><author><keyname>Frougny</keyname><forenames>Christiane</forenames></author><author><keyname>Pelantov&#xe1;</keyname><forenames>Edita</forenames></author></authors><title>Beta-representations of 0 and Pisot numbers</title><categories>math.NT cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $\beta &gt;1 $, $d$ a positive integer, and $$Z_{\beta,d}=\{z_{1}
z_{2}\cdots \mid \sum_{i\ge 1}z_i \beta^{-i}=0, \; z_i \in \{-d, \ldots,
d\}\}$$ be the set of infinite words having value 0 in base $\beta$ on the
alphabet $\{-d, \ldots, d\}$. Based on a recent result of Feng on spectra of
numbers, we prove that if the set $Z_{\beta,\lceil \beta \rceil -1}$ is
recognizable by a finite B\&quot;uchi automaton then $\beta$ is a Pisot number. As a
consequence of previous results, the set $Z_{\beta, d}$ is recognizable by a
finite B\&quot;uchi automaton for every positive integer $d$ if and only if
$Z_{\beta, d}$ is recognizable by a finite B\&quot;uchi automaton for one $d \ge
\lceil \beta \rceil -1$. These conditions are equivalent to the fact that
$\beta$ is a Pisot number. The bound $\lceil \beta \rceil -1$ cannot be further
reduced.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04243</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04243</id><created>2015-12-14</created><authors><author><keyname>Rebollo-Neira</keyname><forenames>Laura</forenames></author></authors><title>Trigonometric dictionary based codec for music compression with high
  quality recovery</title><categories>cs.SD cs.IT math.IT</categories><comments>Software implementing the codec is available on
  http://www.nonlinear-approx.info/examples/node03.html</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A codec for compression of music signals is proposed. The method belongs to
the class of transform lossy compression. It is conceived to be applied in the
high quality recovery range though. The transformation, endowing the codec with
its distinctive feature, relies on the ability to construct high quality sparse
approximation of music signals. This is achieved by a redundant trigonometric
dictionary and a dedicated pursuit strategy. The potential of the approach is
illustrated by comparison with the OGG Vorbis format, on a sample consisting of
clips of melodic music. The comparison evidences remarkable improvements in
compression performance for the identical quality of the decompressed signal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04250</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04250</id><created>2015-12-14</created><authors><author><keyname>Lord</keyname><forenames>Phillip</forenames></author><author><keyname>Warrendar</keyname><forenames>Jennifer</forenames></author></authors><title>A Highly Literate Approach to Ontology Building</title><categories>cs.DL</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Ontologies present an attractive technology for describing bio-medicine,
because they can be shared, and have rich computational properties. However,
they lack the rich expressivity of English and fit poorly with the current
scientific &quot;publish or perish&quot; model. While, there have been attempts to
combine free text and ontologies, most of these perform \textit{post-hoc}
annotation of text. In this paper, we introduce our new environment which
borrows from literate programming, to allow an author to co-develop both text
and ontological description. We are currently using this environment to
document the Karyotype Ontology which allows rich descriptions of the
chromosomal complement in humans. We explore some of the advantages and
difficulties of this form of ontology development.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04252</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04252</id><created>2015-12-14</created><authors><author><keyname>Walk</keyname><forenames>Philipp</forenames></author><author><keyname>Becker</keyname><forenames>Henning</forenames></author><author><keyname>Jung</keyname><forenames>Peter</forenames></author></authors><title>OFDM Channel Estimation via Phase Retrieval</title><categories>cs.IT math.IT</categories><comments>8 pages, 2 figures, presented on Asilomar 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pilot-aided channel estimation is nowadays a standard component in each
wireless receiver enabling coherent transmission of complex-valued
constellations, only affected by noise and interference. Whenever these
disturbances are sufficiently small and long data frames are used, high data
rates can be achieved and the resource overhead due to the pilots vanishes
asymptotically. On the other, it is expected that for the next generation of
mobile networks not only data rate is in the main focus but also low latency,
short and sporadic messages, massive connectivity, distributed &amp; adhoc
processing and robustness with respect to asynchronism. Therefore a review of
several well-established principles in communication has been started already.
A particular implication when using complex-valued pilots is that these values
have to be known at the receiver and therefore these resources can not be used
simultaneously for user data. For an OFDM-like multicarrier scheme this means
that pilot tones (usually placed equidistantly according to the Nyquist
theorem) are allocated with globally known amplitudes and phases to reconstruct
the channel impulse response. Phases are designed and allocated globally which
is in contrast to a distributed infrastructure. In this work we present
therefore a new phaseless pilot scheme where only pilot amplitudes need to be
known at the receiver, i.e., phases are available again and can be used for
various other purposes. The idea is based on a phase retrieval result for
symmetrized and zero-padded magnitude Fourier measurements obtained by two of
the authors. The phases on the pilot tones can now be used to carry additional
user-specific data or compensate for other signal characteristics, like the
PAPR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04255</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04255</id><created>2015-12-14</created><updated>2016-02-14</updated><authors><author><keyname>P&#xe9;rez</keyname><forenames>Guillermo A.</forenames></author></authors><title>The Fixed Initial Credit Problem for Energy Games with
  Partial-Observation is ACK-complete</title><categories>cs.LO cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study two-player games with asymmetric partial-observation
and an energy objective. Such games are played on a weighted graph by Eve,
choosing actions, and Adam, selecting a transition labelled with the given
action. Eve attempts to maintain the sum of the weights (of the transitions
taken) non-negative while Adam tries to do the opposite. Eve does not know the
exact state of the game, she is only given an equivalence class of states which
contains it. In contrast, Adam has full-observation. We show the fixed initial
credit problem for these games is ACK-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04277</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04277</id><created>2015-12-14</created><authors><author><keyname>Mihal&#xe1;k</keyname><forenames>Mat&#xfa;&#x161;</forenames></author><author><keyname>Penna</keyname><forenames>Paolo</forenames></author><author><keyname>Widmayer</keyname><forenames>Peter</forenames></author></authors><title>Bribeproof mechanisms for two-values domains</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Schummer (Journal of Economic Theory 2000) introduced the concept of
bribeproof mechanism which, in a context where monetary transfer between agents
is possible, requires that manipulations through bribes are ruled out.
Unfortunately, in many domains, the only bribeproof mechanisms are the trivial
ones which return a fixed outcome.
  This work presents one of the few constructions of non-trivial bribeproof
mechanisms for these quasi-linear environments. Though the suggested
construction applies to rather restricted domains, the results obtained are
tight: For several natural problems, the method yields the only possible
bribeproof mechanism and no such mechanism is possible on more general domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04280</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04280</id><created>2015-12-14</created><updated>2016-03-03</updated><authors><author><keyname>Lu</keyname><forenames>Liang</forenames></author><author><keyname>Renals</keyname><forenames>Steve</forenames></author></authors><title>Small-footprint Deep Neural Networks with Highway Connections for Speech
  Recognition</title><categories>cs.CL cs.LG cs.NE</categories><comments>5 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For speech recognition, deep neural networks (DNNs) have significantly
improved the recognition accuracy in most of benchmark datasets and application
domains. However, compared to the conventional Gaussian mixture models,
DNN-based acoustic models usually have much larger number of model parameters,
making it challenging for their applications in resource constrained platforms,
e.g., mobile devices. In this paper, we study the application of the recently
proposed highway network to train small-footprint DNNs, which are {\it thinner}
and {\it deeper}, and have significantly smaller number of model parameters
compared to conventional DNNs. We investigated this approach on the AMI meeting
speech transcription corpus which has around 70 hours of audio data. The
highway neural networks constantly outperformed their plain DNN counterparts,
and the number of model parameters can be reduced significantly without
sacrificing the recognition accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04295</identifier>
 <datestamp>2016-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04295</id><created>2015-12-14</created><updated>2016-01-19</updated><authors><author><keyname>Cavigelli</keyname><forenames>Lukas</forenames></author><author><keyname>Benini</keyname><forenames>Luca</forenames></author></authors><title>Origami: A 803 GOp/s/W Convolutional Network Accelerator</title><categories>cs.CV cs.AI cs.LG cs.NE</categories><comments>14 pages</comments><acm-class>B.7.1; I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An ever increasing number of computer vision and image/video processing
challenges are being approached using deep convolutional neural networks,
obtaining state-of-the-art results in object recognition and detection,
semantic segmentation, action recognition, optical flow and superresolution.
Hardware acceleration of these algorithms is essential to adopt these
improvements in embedded and mobile computer vision systems. We present a new
architecture, design and implementation as well as the first reported silicon
measurements of such an accelerator, outperforming previous work in terms of
power-, area- and I/O-efficiency. The manufactured device provides up to 196
GOp/s on 3.09 mm^2 of silicon in UMC 65nm technology and can achieve a power
efficiency of 803 GOp/s/W. The massively reduced bandwidth requirements make it
the first architecture scalable to TOp/s performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04297</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04297</id><created>2015-12-14</created><authors><author><keyname>Kurz</keyname><forenames>Sascha</forenames></author></authors><title>Improved upper bounds for partial spreads</title><categories>math.CO cs.DM</categories><comments>8 pages</comments><msc-class>51E23, 05B15, 05B40, 11T71, 94B25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A partial $(k-1)$-spread in $\operatorname{PG}(n-1,q)$ is a collection of
$(k-1)$-dimensional subspaces with trivial intersection such that each point is
covered exactly once. So far the maximum size of a partial $(k-1)$-spread in
$\operatorname{PG}(n-1,q)$ was know for the cases $n\equiv 0\pmod k$, $n\equiv
1\pmod k$ and $n\equiv 2\pmod k$ with the additional requirements $q=2$ and
$k=3$. We completely resolve the case $n\equiv 2\pmod k$ for the binary case
$q=2$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04302</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04302</id><created>2015-12-14</created><authors><author><keyname>Zhou</keyname><forenames>Tianqing</forenames></author><author><keyname>Huang</keyname><forenames>Yongming</forenames></author><author><keyname>Yang</keyname><forenames>Luxi</forenames></author></authors><title>Joint User Association and Interference Mitigation for D2D-Enabled
  Heterogeneous Cellular Networks</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The heterogeneous cellular networks (HCNs) with device-to-device (D2D)
communications have been a promising solution to cost-efficient delivery of
high data rates. A key challenge in such D2D-enabled HCNs is how to design an
effective association scheme with D2D model selection for load balancing.
Moreover, the offloaded users and D2D receivers (RXs) would suffer strong
interference from BSs, especially from high-power BSs. Evidently, a good
association scheme should integrate with interference mitigation. Thus, we
first propose an effective resource partitioning strategy that can mitigate the
interference received by offloaded users from high-power BSs and the one
received by D2D RXs from BSs. Based on this, we then design a user association
scheme for load balancing, which jointly considers user association and D2D
model selection to maximize network-wide utility. Considering that the
formulated problem is in a nonlinear and mixted-integer form and hard to
tackle, we adopt a dual decomposition method to develop an efficient
distributed algorithm. Simulation results show that the proposed scheme
provides a load balancing gain and a resource partitioning gain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04303</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04303</id><created>2015-12-14</created><authors><author><keyname>Fernandes</keyname><forenames>Cristina G.</forenames></author><author><keyname>Oshiro</keyname><forenames>Marcio T. I.</forenames></author></authors><title>Kinetic Clustering of Points on the Line</title><categories>cs.CG cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of clustering a set of points moving on the line consists of the
following: given positive integers n and k, the initial position and the
velocity of n points, find an optimal k-clustering of the points. We consider
two classical quality measures for the clustering: minimizing the sum of the
clusters diameters and minimizing the maximum diameter of a cluster. For the
former, we present polynomial-time algorithms under some assumptions and, for
the latter, a (2.71 + epsilon)-approximation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04310</identifier>
 <datestamp>2015-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04310</id><created>2015-12-14</created><updated>2015-12-15</updated><authors><author><keyname>Darwish</keyname><forenames>Kareem</forenames></author><author><keyname>Magdy</keyname><forenames>Walid</forenames></author></authors><title>Attitudes towards Refugees in Light of the Paris Attacks</title><categories>cs.SI</categories><comments>3 pages, 1 table, and 2 figures</comments><acm-class>J.4; K.4.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Paris attacks prompted a massive response on social media including
Twitter. This paper explores the immediate response of English speakers on
Twitter towards Middle Eastern refugees in Europe. We show that antagonism
towards refugees is mostly coming from the United States and is mostly
partisan.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04313</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04313</id><created>2015-12-14</created><authors><author><keyname>Charapitsa</keyname><forenames>S.</forenames></author><author><keyname>Dubovskaya</keyname><forenames>I.</forenames></author><author><keyname>Kimlenko</keyname><forenames>I.</forenames></author><author><keyname>Kovalenko</keyname><forenames>A.</forenames></author><author><keyname>Lobko</keyname><forenames>A.</forenames></author><author><keyname>Mazanik</keyname><forenames>A.</forenames></author><author><keyname>Polyak</keyname><forenames>N.</forenames></author><author><keyname>Savitskaya</keyname><forenames>T.</forenames></author><author><keyname>Sytova</keyname><forenames>S.</forenames></author><author><keyname>Timoschenko</keyname><forenames>A.</forenames></author></authors><title>Steps in creation of educational and research web-portal of nuclear
  knowledge BelNET</title><categories>cs.CY</categories><comments>11 pages, 6 figures</comments><msc-class>90B18 (Primary) 68M10 (Secondary)</msc-class><acm-class>K.3.2; J.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Belarusian State University is currently developing the educational and
research web portal of nuclear knowledge BelNET (Belarusian Nuclear Education
and Training Portal). In the future, this specialized electronic portal could
grow into a national portal of nuclear knowledge. The concept, structure and
taxonomy of BelNET portal are developed. The requirements and conditions for
its functioning are analyzed. The information model and architecture of the
portal, as well as algorithms and methods of software are realized. At present,
BelNET software implemented all the basic functions of this portal, including
the ability to remotely (via the Internet) open content editing, sorting,
filtering, etc. Filling the BelNET by knowledge is at the beginning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04334</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04334</id><created>2015-12-14</created><authors><author><keyname>Jia</keyname><forenames>Yunde</forenames></author><author><keyname>Xu</keyname><forenames>Bin</forenames></author><author><keyname>Shen</keyname><forenames>Jiajun</forenames></author><author><keyname>Pei</keyname><forenames>Mintao</forenames></author><author><keyname>Dong</keyname><forenames>Zhen</forenames></author><author><keyname>Hou</keyname><forenames>Jingyi</forenames></author><author><keyname>Yang</keyname><forenames>Min</forenames></author></authors><title>Telepresence Interaction by Touching Live Video Images</title><categories>cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a telepresence interaction framework and system based on
touch screen and telepresence robot technologies. The system is composed of a
telepresence robot and tele-interactive devices in a remote environment
(presence space), the touching live video image user interface (TIUI) used by
an operator (user) in an operation space, and wireless network connecting the
two spaces. A tele-interactive device refers to a real object with its
identification, actuator, and Wireless communication. A telepresence robot is
used as the embodiment of an operator to go around in the presence space to
actively capture live videos. The TIUI is our new user interface which allows
an operator simply uses a pad to access the system anywhere to not only
remotely operate the telepresence robot but also interact with a
tele-interactive device, just by directly touching its live video image as if
him/her to do it in the presence. The preliminary evaluation and demonstration
show the efficacy and promising of our framework and system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04343</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04343</id><created>2015-12-14</created><authors><author><keyname>Zasada</keyname><forenames>Stefan J.</forenames></author><author><keyname>Coveney</keyname><forenames>Peter V.</forenames></author></authors><title>A Distributed Multi-agent Market Place for HPC Compute Cycle Resource
  Trading</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computer simulation is finding a role in an increasing number of scientific
disciplines, concomitant with the rise in available computing power. Realizing
this inevitably requires access to computational power beyond the desktop,
making use of clusters, supercomputers, data repositories, networks and
distributed aggregations of these resources. Accessing one such resource
entails a number of usability and security problems; when multiple
geographically distributed resources are involved, the difficulty is
compounded.
  This presents the user with the problem of how to gain access to suitable
resources to run their workloads as they need them. In this paper we present
our solutions to this problem, a resource trading platform that allows users to
purchase access to resources within a distributed e-infrastructure. We present
the implementation of this Resource Allocation Market Place as a distributed
multi-agent system, and show how it provides a highly flexible, efficient tool
to schedule workflows across high performance computing resources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04349</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04349</id><created>2015-12-14</created><authors><author><keyname>Driemel</keyname><forenames>Anne</forenames></author><author><keyname>Krivo&#x161;ija</keyname><forenames>Amer</forenames></author><author><keyname>Sohler</keyname><forenames>Christian</forenames></author></authors><title>Clustering time series under the Fr\'echet distance</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Fr\'echet distance is a popular distance measure for curves. We study the
problem of clustering time series under the Fr\'echet distance. In particular,
we give $(1+\varepsilon)$-approximation algorithms for variations of the
following problem with parameters $k$ and $\ell$. Given $n$ univariate time
series $P$, each of complexity at most $m$, we find $k$ time series, not
necessarily from $P$, which we call \emph{cluster centers} and which each have
complexity at most $\ell$, such that (a) the maximum distance of an element of
$P$ to its nearest cluster center or (b) the sum of these distances is
minimized. Our algorithms have running time near-linear in the input size for
constant $\varepsilon$, $k$ and $\ell$. To the best of our knowledge, our
algorithms are the first clustering algorithms for the Fr\'echet distance which
achieve an approximation factor of $(1+\varepsilon)$ or better.
  Keywords: time series, longitudinal data, functional data, clustering,
Fr\'echet distance, dynamic time warping, approximation algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04354</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04354</id><created>2015-11-04</created><authors><author><keyname>Paris</keyname><forenames>St&#xe9;fane</forenames><affiliation>QGAR</affiliation></author></authors><title>A proposal project for a blind image quality assessment by learning
  distortions from the full reference image quality assessments</title><categories>cs.MM cs.CV</categories><comments>International Workshop on Quality of Multimedia Experience, 2012,
  Melbourne, Australia</comments><proxy>ccsd</proxy><doi>10.1109/QoMEX.2012.6263876</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This short paper presents a perspective plan to build a null reference image
quality assessment. Its main goal is to deliver both the objective score and
the distortion map for a given distorted image without the knowledge of its
reference image.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04358</identifier>
 <datestamp>2015-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04358</id><created>2015-12-14</created><updated>2015-12-16</updated><authors><author><keyname>Patkos</keyname><forenames>Theodore</forenames></author><author><keyname>Plexousakis</keyname><forenames>Dimitris</forenames></author><author><keyname>Chibani</keyname><forenames>Abdelghani</forenames></author><author><keyname>Amirat</keyname><forenames>Yacine</forenames></author></authors><title>An Event Calculus Production Rule System for Reasoning in Dynamic and
  Uncertain Domains</title><categories>cs.AI</categories><comments>Under consideration in Theory and Practice of Logic Programming
  (TPLP)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Action languages have emerged as an important field of Knowledge
Representation for reasoning about change and causality in dynamic domains.
This article presents Cerbere, a production system designed to perform online
causal, temporal and epistemic reasoning based on the Event Calculus. The
framework implements the declarative semantics of the underlying logic theories
in a forward-chaining rule-based reasoning system, coupling the high
expressiveness of its formalisms with the efficiency of rule-based systems. To
illustrate its applicability, we present both the modeling of benchmark
problems in the field, as well as its utilization in the challenging domain of
smart spaces. A hybrid framework that combines logic-based with probabilistic
reasoning has been developed, that aims to accommodate activity recognition and
monitoring tasks in smart spaces. Under consideration in Theory and Practice of
Logic Programming (TPLP)
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04362</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04362</id><created>2015-12-14</created><updated>2016-01-26</updated><authors><author><keyname>Wang</keyname><forenames>Qing</forenames></author><author><keyname>Zuniga</keyname><forenames>Marco</forenames></author><author><keyname>Giustiniano</keyname><forenames>Domenico</forenames></author></authors><title>A Low-cost Passive Visible Light Communication System</title><categories>cs.NI</categories><comments>7 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we propose a new communication system for illuminated areas,
indoors and outdoors. Light sources in our environments -- such as light bulbs
or even the sun -- are our signal emitters, but we do not modulate data using
the light source. We instead propose that the environment itself modulates the
light signals: if any mobile element 'wears' a pattern consisting of
distinctive reflecting surfaces, a receiver could decode the disturbed light
signals to read passive information. Mobility here is key to transfer the
information to the destination, since objects or people passing by will disturb
the surrounding light signals. Like barcode readers in our stores, light
sources in our environments could be leveraged to monitor logistics and
activities in cities and buildings. Achieving this vision requires a deep
understanding of a new type of communication channel. Many parameters can
affect the performance of passive communication signals based on visible light:
the specific patterns of the reflective surfaces, the surrounding light
intensity and the speed of the mobile objects, to name a few. In this position
paper, we present an initial experimental and analytical approach to tackle
some of the main challenges associated with realizing a passive communication
channel with visible light.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04364</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04364</id><created>2015-11-16</created><authors><author><keyname>Joswig</keyname><forenames>Michael</forenames></author><author><keyname>Mehner</keyname><forenames>Milan</forenames></author><author><keyname>Sechelmann</keyname><forenames>Stefan</forenames></author><author><keyname>Techter</keyname><forenames>Jan</forenames></author><author><keyname>Bobenko</keyname><forenames>Alexander I.</forenames></author></authors><title>DGD Gallery: Storage, sharing, and publication of digital research data</title><categories>cs.OH math.DG</categories><comments>19 pages, 8 figures, to appear in &quot;Advances in Discrete Differential
  Geometry&quot;, ed. A. I. Bobenko, Springer, 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a project, called the &quot;Discretization in Geometry and Dynamics
Gallery&quot;, or DGD Gallery for short, whose goal is to store geometric data and
to make it publicly available. The DGD Gallery offers an online web service for
the storage, sharing, and publication of digital research data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04371</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04371</id><created>2015-12-14</created><authors><author><keyname>Pandey</keyname><forenames>Pankaj</forenames></author></authors><title>'Context, Content, Process' Approach to Align Information Security
  Investments with Overall Organizational Strategy</title><categories>cs.CY cs.CR</categories><journal-ref>International Journal of Security, Privacy and Trust Management
  (IJSPTM) Vol 4, No 3/4, pages 25-38, November 2015</journal-ref><doi>10.5121/ijsptm.2015.4403</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Today business environment is highly dependent on complex technologies, and
information is considered an important asset. Organizations are therefore
required to protect their information infrastructure and follow an inclusive
risk management approach. One way to achieve this is by aligning the
information security investment decisions with respect to organizational
strategy. A large number of information security investment models have are in
the literature. These models are useful for optimal and cost-effective
investments in information security. However, it is extremely challenging for a
decision maker to select one or combination of several models to decide on
investments in information security controls. We propose a framework to
simplify the task of selecting information security investment model(s). The
proposed framework follows the 'Context, Content, Process' approach, and this
approach is useful in evaluation and prioritization of investments in
information security controls in alignment with the overall organizational
strategy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04375</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04375</id><created>2015-12-14</created><authors><author><keyname>Fitzsimons</keyname><forenames>Joseph F.</forenames></author><author><keyname>Hajdu&#x161;ek</keyname><forenames>Michal</forenames></author></authors><title>Post hoc verification of quantum computation</title><categories>quant-ph cs.CC cs.CR</categories><comments>4 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With recent progress on experimental quantum information processing, an
important question has arisen as to whether it is possible to verify arbitrary
computation performed on a quantum processor. A number of protocols have been
proposed to achieve this goal, however all are interactive in nature, requiring
that the computation be performed in an interactive manner with back and forth
communication between the verifier and one or more provers. Here we propose two
methods for verifying quantum computation in a non-interactive manner based on
recent progress in the understanding of the local Hamiltonian problem. Provided
that the provers compute certain witnesses for the computation, this allows the
result of a quantum computation to be verified after the fact, a property not
seen in current verification protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04376</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04376</id><created>2015-11-17</created><authors><author><keyname>Randrianantenaina</keyname><forenames>Itsikiantsoa</forenames></author><author><keyname>Elsawy</keyname><forenames>Hesham</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author></authors><title>Limits on the Capacity of In-Band Full Duplex Communication in Uplink
  Cellular Networks</title><categories>cs.NI cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Simultaneous co-channel transmission and reception, denoted as in-band full
duplex (FD) communication, has been promoted as an attractive solution to
improve the spectral efficiency of cellular networks. However, in addition to
the self-interference problem, cross-mode interference (i.e., between uplink
and downlink) imposes a major obstacle for the deployment of FD communication
in cellular networks. More specifically, the downlink to uplink interference
represents the performance bottleneck for FD operation due to the uplink
limited transmission power and venerable operation when compared to the
downlink counterpart. While the positive impact of FD communication to the
downlink performance has been proved in the literature, its effect on the
uplink transmission has been neglected. This paper focuses on the effect of
downlink interference on the uplink transmission in FD cellular networks in
order to see whether FD communication is beneficial for the uplink transmission
or not, and if yes for which type of network. To quantify the expected
performance gains, we derive a closed form expression of the maximum achievable
uplink capacity in FD cellular networks. In contrast to the downlink capacity
which always improves with FD communication, our results show that the uplink
performance may improve or degrade depending on the associated network
parameters. Particularly, we show that the intensity of base stations (BSs) has
a more prominent effect on the uplink performance than their transmission
power.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04380</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04380</id><created>2015-12-14</created><authors><author><keyname>Zorbas</keyname><forenames>Dimitrios</forenames></author><author><keyname>Razafindralambo</keyname><forenames>Tahiry</forenames></author></authors><title>Modeling the power consumption of a Wifibot and studying the role of
  communication cost in operation time</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile robots are becoming part of our every day living at home, work or
entertainment. Due to their limited power capabilities, the development of new
energy consumption models can lead to energy conservation and energy efficient
designs. In this paper, we carry out a number of experiments and we focus on
the motors power consumption of a specific robot called Wifibot. Based on the
experimentation results, we build models for different speed and acceleration
levels. We compare the motors power consumption to other robot running modes.
We, also, create a simple robot network scenario and we investigate whether
forwarding data through a closer node could lead to longer operation times. We
assess the effect energy capacity, traveling distance and data rate on the
operation time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04386</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04386</id><created>2015-12-14</created><authors><author><keyname>Mahajan</keyname><forenames>Meena</forenames></author><author><keyname>Tawari</keyname><forenames>Anuj</forenames></author></authors><title>Read-once polynomials: How many summands suffice?</title><categories>cs.CC</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An arithmetic read-once formula (ROF) is a formula (circuit of fan-out 1)
over $+, \times$ where each variable labels at most one leaf. Every multilinear
polynomial can be expressed as the sum of ROFs. In this work, we prove, for
certain multilinear polynomials, a tight lower bound on the number of summands
in such an expression.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04387</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04387</id><created>2015-12-14</created><authors><author><keyname>Perov</keyname><forenames>Yura N</forenames></author><author><keyname>Le</keyname><forenames>Tuan Anh</forenames></author><author><keyname>Wood</keyname><forenames>Frank</forenames></author></authors><title>Data-driven Sequential Monte Carlo in Probabilistic Programming</title><categories>cs.AI stat.AP stat.ML</categories><comments>Black Box Learning and Inference, NIPS 2015 Workshop</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most of Markov Chain Monte Carlo (MCMC) and sequential Monte Carlo (SMC)
algorithms in existing probabilistic programming systems suboptimally use only
model priors as proposal distributions. In this work, we describe an approach
for training a discriminative model, namely a neural network, in order to
approximate the optimal proposal by using posterior estimates from previous
runs of inference. We show an example that incorporates a data-driven proposal
for use in a non-parametric model in the Anglican probabilistic programming
system. Our results show that data-driven proposals can significantly improve
inference performance so that considerably fewer particles are necessary to
perform a good posterior estimation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04388</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04388</id><created>2015-12-14</created><authors><author><keyname>Fatemi</keyname><forenames>Mitra</forenames></author><author><keyname>Amini</keyname><forenames>Arash</forenames></author><author><keyname>Vetterli</keyname><forenames>Martin</forenames></author></authors><title>Sampling and Reconstruction of Shapes with Algebraic Boundaries</title><categories>cs.CG</categories><comments>12 pages, 14 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a sampling theory for a class of binary images with finite rate of
innovation (FRI). Every image in our model is the restriction of
$\mathds{1}_{\{p\leq0\}}$ to the image plane, where $\mathds{1}$ denotes the
indicator function and $p$ is some real bivariate polynomial. This particularly
means that the boundaries in the image form a subset of an algebraic curve with
the implicit polynomial $p$. We show that the image parameters --i.e., the
polynomial coefficients-- satisfy a set of linear annihilation equations with
the coefficients being the image moments. The inherent sensitivity of the
moments to noise makes the reconstruction process numerically unstable and
narrows the choice of the sampling kernels to polynomial reproducing kernels.
As a remedy to these problems, we replace conventional moments with more stable
\emph{generalized moments} that are adjusted to the given sampling kernel. The
benefits are threefold: (1) it relaxes the requirements on the sampling
kernels, (2) produces annihilation equations that are robust at numerical
precision, and (3) extends the results to images with unbounded boundaries. We
further reduce the sensitivity of the reconstruction process to noise by taking
into account the sign of the polynomial at certain points, and sequentially
enforcing measurement consistency. We consider various numerical experiments to
demonstrate the performance of our algorithm in reconstructing binary images,
including low to moderate noise levels and a range of realistic sampling
kernels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04389</identifier>
 <datestamp>2015-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04389</id><created>2015-12-14</created><updated>2015-12-15</updated><authors><author><keyname>Uramoto</keyname><forenames>Takeo</forenames></author></authors><title>Semi-galois Categories: The Classical Eilenberg Variety Theory</title><categories>math.CT cs.FL</categories><comments>Added some missing URL and links to the references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces and studies the class of semi-galois categories, i.e.
an extension of galois categories, in order to develop a transparent
duality-based framework for the classical Eilenberg variety theory. For this
purpose, the current paper is committed to building some fundamental components
of our intended framework, namely, (I) a duality theorem between profinite
monoids and semi-galois categories, our proof of which is a natural extension
of an elementary proof of the duality theorem between profinite groups and
galois categories; and (II) new proofs of the variety theorems of Straubing and
Chaubard et al. based on the duality theorem between Boolean comonoids,
profinite monoids and semi-galois categories. In the last section, we also
discuss a role of this reformulation of the EIlenberg variety theory for
solving classical problems in this theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04392</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04392</id><created>2015-12-14</created><updated>2015-12-28</updated><authors><author><keyname>Wang</keyname><forenames>Li-Li</forenames></author><author><keyname>Ngan</keyname><forenames>Henry Y. T.</forenames></author><author><keyname>Yung</keyname><forenames>Nelson H. C.</forenames></author></authors><title>Automatic Incident Classification for Big Traffic Data by Adaptive
  Boosting SVM</title><categories>cs.LG</categories><comments>27 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern cities experience heavy traffic flows and congestions regularly across
space and time. Monitoring traffic situations becomes an important challenge
for the Traffic Control and Surveillance Systems (TCSS). In advanced TCSS, it
is helpful to automatically detect and classify different traffic incidents
such as severity of congestion, abnormal driving pattern, abrupt or illegal
stop on road, etc. Although most TCSS are equipped with basic incident
detection algorithms, they are however crude to be really useful as an
automated tool for further classification. In literature, there is a lack of
research for Automated Incident Classification (AIC). Therefore, a novel AIC
method is proposed in this paper to tackle such challenges. In the proposed
method, traffic signals are firstly extracted from captured videos and
converted as spatial-temporal (ST) signals. Based on the characteristics of the
ST signals, a set of realistic simulation data are generated to construct an
extended big traffic database to cover a variety of traffic situations. Next, a
Mean-Shift filter is introduced to suppress the effect of noise and extract
significant features from the ST signals. The extracted features are then
associated with various types of traffic data: one normal type (inliers) and
multiple abnormal types (outliers). For the classification, an adaptive
boosting classifier is trained to detect outliers in traffic data
automatically. Further, a Support Vector Machine (SVM) based method is adopted
to train the model for identifying the categories of outliers. In short, this
hybrid approach is called an Adaptive Boosting Support Vector Machines (AB-SVM)
method. Experimental results show that the proposed AB-SVM method achieves a
satisfied result with more than 92% classification accuracy on average.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04393</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04393</id><created>2015-12-14</created><authors><author><keyname>Dowden</keyname><forenames>Chris</forenames></author></authors><title>Secure message transmission in the presence of a fully generalised
  adversary</title><categories>cs.CR</categories><comments>14 pages</comments><journal-ref>Journal of Mathematical Cryptology (2015) 9, 205-214</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the problem of secure message transmission in the presence of
a &quot;fully generalised&quot; adversary, who disrupts and listens to separate sets of
communication wires. We extend previous results by considering the case when
these sets may have arbitrary size, providing necessary and sufficient
conditions for both one-way and two-way communication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04407</identifier>
 <datestamp>2015-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04407</id><created>2015-12-14</created><updated>2015-12-15</updated><authors><author><keyname>Chandrasekaran</keyname><forenames>Arjun</forenames></author><author><keyname>Vijayakumar</keyname><forenames>Ashwin K</forenames></author><author><keyname>Antol</keyname><forenames>Stanislaw</forenames></author><author><keyname>Bansal</keyname><forenames>Mohit</forenames></author><author><keyname>Batra</keyname><forenames>Dhruv</forenames></author><author><keyname>Zitnick</keyname><forenames>C. Lawrence</forenames></author><author><keyname>Parikh</keyname><forenames>Devi</forenames></author></authors><title>We Are Humor Beings: Understanding and Predicting Visual Humor</title><categories>cs.CV cs.CL cs.LG</categories><comments>17 pages, 16 figures, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Humor is an integral part of human lives. Despite being tremendously
impactful, it is perhaps surprising that we do not yet have a detailed
understanding of humor. In this work, we explore the novel problem of studying
visual humor and designing computational models of humor using abstract scenes.
We collect and make publicly available two datasets of abstract scenes: one
that enables the study of humor at the scene-level and the other at the
object-level. We study the funny scenes in our dataset and explore different
types of humor depicted in these scenes. We model two tasks that we believe
demonstrate an understanding of visual humor -- predicting the funniness of a
scene and altering the funniness of a scene. We show that our models perform
well using automatic evaluation as well as human studies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04412</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04412</id><created>2015-12-14</created><authors><author><keyname>Dai</keyname><forenames>Jifeng</forenames></author><author><keyname>He</keyname><forenames>Kaiming</forenames></author><author><keyname>Sun</keyname><forenames>Jian</forenames></author></authors><title>Instance-aware Semantic Segmentation via Multi-task Network Cascades</title><categories>cs.CV</categories><comments>Tech report. 1st-place winner of MS COCO 2015 segmentation
  competition</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Semantic segmentation research has recently witnessed rapid progress, but
many leading methods are unable to identify object instances. In this paper, we
present Multi-task Network Cascades for instance-aware semantic segmentation.
Our model consists of three networks, respectively differentiating instances,
estimating masks, and categorizing objects. These networks form a cascaded
structure, and are designed to share their convolutional features. We develop
an algorithm for the nontrivial end-to-end training of this causal, cascaded
structure. Our solution is a clean, single-step training framework and can be
generalized to cascades that have more stages. We demonstrate state-of-the-art
instance-aware semantic segmentation accuracy on PASCAL VOC. Meanwhile, our
method takes only 360ms testing an image using VGG-16, which is two orders of
magnitude faster than previous systems for this challenging problem. As a by
product, our method also achieves compelling object detection results which
surpass the competitive Fast/Faster R-CNN systems.
  The method described in this paper is the foundation of our submissions to
the MS COCO 2015 segmentation competition, where we won the 1st place.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04418</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04418</id><created>2015-12-14</created><authors><author><keyname>Lee</keyname><forenames>Chia-Chen</forenames></author><author><keyname>Hwang</keyname><forenames>Wen-Liang</forenames></author></authors><title>Sparse Representation of a Blur Kernel for Blind Image Restoration</title><categories>cs.CV</categories><comments>11 pages, 37 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Blind image restoration is a non-convex problem which involves restoration of
images from an unknown blur kernel. The factors affecting the performance of
this restoration are how much prior information about an image and a blur
kernel are provided and what algorithm is used to perform the restoration task.
Prior information on images is often employed to restore the sharpness of the
edges of an image. By contrast, no consensus is still present regarding what
prior information to use in restoring from a blur kernel due to complex image
blurring processes. In this paper, we propose modelling of a blur kernel as a
sparse linear combinations of basic 2-D patterns. Our approach has a
competitive edge over the existing blur kernel modelling methods because our
method has the flexibility to customize the dictionary design, which makes it
well-adaptive to a variety of applications. As a demonstration, we construct a
dictionary formed by basic patterns derived from the Kronecker product of
Gaussian sequences. We also compare our results with those derived by other
state-of-the-art methods, in terms of peak signal to noise ratio (PSNR).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04419</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04419</id><created>2015-12-14</created><authors><author><keyname>Balkir</keyname><forenames>Esma</forenames></author><author><keyname>Kartsaklis</keyname><forenames>Dimitri</forenames></author><author><keyname>Sadrzadeh</keyname><forenames>Mehrnoosh</forenames></author></authors><title>Sentence Entailment in Compositional Distributional Semantics</title><categories>cs.CL cs.AI math.CT</categories><comments>8 pages, 1 figure, 2 tables, accepted for presentation in the
  International Symposium on Artificial Intelligence and Mathematics (ISAIM),
  2016</comments><acm-class>I.2.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributional semantic models provide vector representations for words by
gathering co-occurrence frequencies from corpora of text. Compositional
distributional models extend these representations from words to phrases and
sentences. In categorical compositional distributional semantics these
representations are built in such a manner that meanings of phrases and
sentences are functions of their grammatical structure and the meanings of the
words therein. These models have been applied to reasoning about phrase and
sentence level similarity. In this paper, we argue for and prove that these
models can also be used to reason about phrase and sentence level entailment.
We provide preliminary experimental results on a toy entailment dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04433</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04433</id><created>2015-12-14</created><authors><author><keyname>Oymak</keyname><forenames>Samet</forenames></author><author><keyname>Recht</keyname><forenames>Ben</forenames></author></authors><title>Near-Optimal Bounds for Binary Embeddings of Arbitrary Sets</title><categories>cs.LG cs.DS math.FA</categories><comments>20 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study embedding a subset $K$ of the unit sphere to the Hamming cube
$\{-1,+1\}^m$. We characterize the tradeoff between distortion and sample
complexity $m$ in terms of the Gaussian width $\omega(K)$ of the set. For
subspaces and several structured sets we show that Gaussian maps provide the
optimal tradeoff $m\sim \delta^{-2}\omega^2(K)$, in particular for $\delta$
distortion one needs $m\approx\delta^{-2}{d}$ where $d$ is the subspace
dimension. For general sets, we provide sharp characterizations which reduces
to $m\approx{\delta^{-4}}{\omega^2(K)}$ after simplification. We provide
improved results for local embedding of points that are in close proximity of
each other which is related to locality sensitive hashing. We also discuss
faster binary embedding where one takes advantage of an initial sketching
procedure based on Fast Johnson-Lindenstauss Transform. Finally, we list
several numerical observations and discuss open problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04455</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04455</id><created>2015-12-14</created><authors><author><keyname>Heess</keyname><forenames>Nicolas</forenames></author><author><keyname>Hunt</keyname><forenames>Jonathan J</forenames></author><author><keyname>Lillicrap</keyname><forenames>Timothy P</forenames></author><author><keyname>Silver</keyname><forenames>David</forenames></author></authors><title>Memory-based control with recurrent neural networks</title><categories>cs.LG</categories><comments>NIPS Deep Reinforcement Learning Workshop 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Partially observed control problems are a challenging aspect of reinforcement
learning. We extend two related, model-free algorithms for continuous control
-- deterministic policy gradient and stochastic value gradient -- to solve
partially observed domains using recurrent neural networks trained with
backpropagation through time.
  We demonstrate that this approach, coupled with long-short term memory is
able to solve a variety of physical control problems exhibiting an assortment
of memory requirements. These include the short-term integration of information
from noisy sensors and the identification of system parameters, as well as
long-term memory problems that require preserving information over many time
steps. We also demonstrate success on a combined exploration and memory problem
in the form of a simplified version of the well-known Morris water maze task.
Finally, we show that our approach can deal with high-dimensional observations
by learning directly from pixels.
  We find that recurrent deterministic and stochastic policies are able to
learn similarly good solutions to these tasks, including the water maze where
the agent must learn effective search strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04456</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04456</id><created>2015-12-14</created><authors><author><keyname>Asamoah</keyname><forenames>Daniel</forenames></author><author><keyname>Doran</keyname><forenames>Derek</forenames></author><author><keyname>Schiller</keyname><forenames>Shu</forenames></author></authors><title>Teaching the Foundations of Data Science: An Interdisciplinary Approach</title><categories>cs.CY</categories><comments>Presented at SIGDSA Business Analytics Conference 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The astronomical growth of data has necessitated the need for educating
well-qualified data scientists to derive deep insights from large and complex
data sets generated by organizations. In this paper, we present our
interdisciplinary approach and experiences in teaching a Data Science course,
the first of its kind offered at the Wright State University. Two faculty
members from the Management Information Systems (MIS) and Computer Science (CS)
departments designed and co-taught the course with perspectives from their
previous research and teaching experiences. Students in the class had mix
backgrounds with mainly MIS and CS majors. Students' learning outcomes and post
course survey responses suggested that the course delivered a broad overview of
data science as desired, and that students worked synergistically with those of
different majors in collaborative lab assignments and in a semester long
project. The interdisciplinary pedagogy helped build collaboration and create
satisfaction among learners.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04466</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04466</id><created>2015-12-14</created><authors><author><keyname>Zhai</keyname><forenames>Shuangfei</forenames></author><author><keyname>Zhang</keyname><forenames>Zhongfei</forenames></author></authors><title>Semisupervised Autoencoder for Sentiment Analysis</title><categories>cs.LG</categories><comments>To appear in AAAI 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the usage of autoencoders in modeling textual
data. Traditional autoencoders suffer from at least two aspects: scalability
with the high dimensionality of vocabulary size and dealing with
task-irrelevant words. We address this problem by introducing supervision via
the loss function of autoencoders. In particular, we first train a linear
classifier on the labeled data, then define a loss for the autoencoder with the
weights learned from the linear classifier. To reduce the bias brought by one
single classifier, we define a posterior probability distribution on the
weights of the classifier, and derive the marginalized loss of the autoencoder
with Laplace approximation. We show that our choice of loss function can be
rationalized from the perspective of Bregman Divergence, which justifies the
soundness of our model. We evaluate the effectiveness of our model on six
sentiment analysis datasets, and show that our model significantly outperforms
all the competing methods with respect to classification accuracy. We also show
that our model is able to take advantage of unlabeled dataset and get improved
performance. We further show that our model successfully learns highly
discriminative feature maps, which explains its superior performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04467</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04467</id><created>2015-11-20</created><authors><author><keyname>Guiochet</keyname><forenames>J&#xe9;r&#xe9;mie</forenames><affiliation>LAAS-TSF</affiliation></author><author><keyname>Hoang</keyname><forenames>Quynh Anh Do</forenames><affiliation>LAAS-TSF</affiliation></author><author><keyname>Kaaniche</keyname><forenames>Mohamed</forenames><affiliation>LAAS-TSF</affiliation></author></authors><title>A Model for Safety Case Confidence Assessment</title><categories>cs.AI</categories><proxy>ccsd</proxy><journal-ref>34th International Conference on Computer Safety, Reliability and
  Security, Sep 2015, Delft, Netherlands. Springer, Lecture Notes in Computer
  Science, Vol. 9337, Programming and Software Engineering, Springer, 2015,
  http://safecomp2015.tudelft.nl/</journal-ref><doi>10.1007/978-3-319-24255-2_23</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Building a safety case is a common approach to make expert judgement explicit
about safety of a system. The issue of confidence in such argumentation is
still an open research field. Providing quantitative estimation of confidence
is an interesting approach to manage complexity of arguments. This paper
explores the main current approaches, and proposes a new model for quantitative
confidence estimation based on Belief Theory for its definition, and on
Bayesian Belief Networks for its propagation in safety case networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04468</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04468</id><created>2015-11-23</created><authors><author><keyname>Bayati</keyname><forenames>Basil S.</forenames></author></authors><title>A Method to Calculate the Exit Time in Stochastic Simulations</title><categories>stat.CO cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel method is presented to compute the exit time for the stochastic
simulation algorithm. The method is based on the addition of a series of random
variables and is derived using the convolution theorem. The final distribution
is derived and approximated in the frequency domain. The distribution for the
final time is transformed back to the real domain and can be sampled from in a
simulation. The result is an approximation of the classical stochastic
simulation algorithm that requires fewer random variates. An analysis of the
error and speedup compared to the stochastic simulation algorithm is presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04469</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04469</id><created>2015-11-23</created><authors><author><keyname>Thoma</keyname><forenames>Martin</forenames></author></authors><title>\&quot;Uber die Klassifizierung von Knoten in dynamischen Netzwerken mit
  Inhalt</title><categories>cs.LG</categories><comments>in German. This term paper was handed in on 17.01.2014</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This paper explains the DYCOS-Algorithm as it was introduced in by Aggarwal
and Li in 2011. It operates on graphs whichs nodes are partially labeled and
automatically adds missing labels to nodes. To do so, the DYCOS algorithm makes
use of the structure of the graph as well as content which is assigned to the
node. Aggarwal and Li measured in an experimental analysis that DYCOS adds the
missing labels to a Graph with 19396 nodes of which 14814 are labeled and
another Graph with 806635 nodes of which 18999 are labeld on one core of an
Intel Xeon 2.5 GHz CPU with 32 G RAM within less than a minute. Additionally,
extensions of the DYCOS algorithm are proposed.
  -----
  In dieser Arbeit wird der DYCOS-Algorithmus, wie er 2011 von Aggarwal und Li
vorgestellt wurde, erkl\&quot;art. Er arbeitet auf Graphen, deren Knoten teilweise
mit Beschriftungen versehen sind und erg\&quot;anzt automatisch Beschriftungen f\&quot;ur
Knoten, die bisher noch keine Beschriftung haben. Dieser Vorgang wird
&quot;Klassifizierung&quot; genannt. Dazu verwendet er die Struktur des Graphen sowie
textuelle Informationen, die den Knoten zugeordnet sind. Die von Aggarwal und
Li beschriebene experimentelle Analyse ergab, dass er auch auf dynamischen
Graphen mit 19396 bzw. 806635 Knoten, von denen nur 14814 bzw. 18999
beschriftet waren, innerhalb von weniger als einer Minute auf einem Kern einer
Intel Xeon 2.5 GHz CPU mit 32 G RAM ausgef\&quot;uhrt werden kann. Zus\&quot;atzlich wird
die Ver\&quot;offentlichung von Aggarwal und Li kritisch er\&quot;ortert und und es
werden m\&quot;ogliche Erweiterungen des DYCOS-Algorithmus vorgeschlagen.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04476</identifier>
 <datestamp>2015-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04476</id><created>2015-12-14</created><updated>2015-12-15</updated><authors><author><keyname>Garimella</keyname><forenames>Venkata Rama Kiran</forenames></author><author><keyname>Alfayad</keyname><forenames>Abdulrahman</forenames></author><author><keyname>Weber</keyname><forenames>Ingmar</forenames></author></authors><title>Social Media Image Analysis for Public Health</title><categories>cs.SI cs.CY</categories><comments>Accepted at CHI 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several projects have shown the feasibility to use textual social media data
to track public health concerns, such as temporal influenza patterns or
geographical obesity patterns. In this paper, we look at whether geo-tagged
images from Instagram also provide a viable data source. Especially for
&quot;lifestyle&quot; diseases, such as obesity, drinking or smoking, images of social
gatherings could provide information that is not necessarily shared in, say,
tweets. In this study, we explore whether (i) tags provided by the users and
(ii) annotations obtained via automatic image tagging are indeed valuable for
studying public health. We find that both user-provided and machine-generated
tags provide information that can be used to infer a county's health
statistics. Whereas for most statistics user-provided tags are better features,
for predicting excessive drinking machine-generated tags such as &quot;liquid&quot; and
&quot;glass&quot; yield better models. This hints at the potential of using
machine-generated tags to study substance abuse.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04483</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04483</id><created>2015-12-14</created><authors><author><keyname>Zhai</keyname><forenames>Shuangfei</forenames></author><author><keyname>Zhang</keyname><forenames>Zhongfei</forenames></author></authors><title>Dropout Training of Matrix Factorization and Autoencoder for Link
  Prediction in Sparse Graphs</title><categories>cs.LG</categories><comments>Published in SDM 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Matrix factorization (MF) and Autoencoder (AE) are among the most successful
approaches of unsupervised learning. While MF based models have been
extensively exploited in the graph modeling and link prediction literature, the
AE family has not gained much attention. In this paper we investigate both MF
and AE's application to the link prediction problem in sparse graphs. We show
the connection between AE and MF from the perspective of multiview learning,
and further propose MF+AE: a model training MF and AE jointly with shared
parameters. We apply dropout to training both the MF and AE parts, and show
that it can significantly prevent overfitting by acting as an adaptive
regularization. We conduct experiments on six real world sparse graph datasets,
and show that MF+AE consistently outperforms the competing methods, especially
on datasets that demonstrate strong non-cohesive structures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04509</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04509</id><created>2015-12-14</created><updated>2015-12-19</updated><authors><author><keyname>Eswaran</keyname><forenames>K.</forenames></author><author><keyname>Rao</keyname><forenames>K. Damodhar</forenames></author></authors><title>On non-iterative training of a neural classifier</title><categories>cs.CV cs.LG cs.NE</categories><comments>18 pages, 5 figures</comments><msc-class>62M45</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently an algorithm, was discovered, which separates points in n-dimension
by planes in such a manner that no two points are left un-separated by at least
one plane{[}1-3{]}. By using this new algorithm we show that there are two ways
of classification by a neural network, for a large dimension feature space,
both of which are non-iterative and deterministic. To demonstrate the power of
both these methods we apply them exhaustively to the classical pattern
recognition problem: The Fisher-Anderson's, IRIS flower data set and present
the results.
  It is expected these methods will now be widely used for the training of
neural networks for Deep Learning not only because of their non-iterative and
deterministic nature but also because of their efficiency and speed and will
supersede other classification methods which are iterative in nature and rely
on error minimization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04510</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04510</id><created>2015-12-14</created><authors><author><keyname>Milovanov</keyname><forenames>Alexey</forenames></author></authors><title>Algorithmic statistics: normal objects and universal models</title><categories>cs.IT math.IT</categories><comments>19 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Kolmogorov suggested to measure quality of a statistical hypothesis $P$ for a
data $x$ by two parameters: Kolmogorov complexity $C(P)$ of the hypothesis and
the probability $P(x)$ of $x$ with respect to $P$.
  P. G\'acs, J. Tromp, P.M.B. Vit\'anyi discovered a small class of models that
are universal in the following sense. Each hypothesis $S_{ij}$ from that class
is identified by two integer parameters $i,j$ and for every data $x$ and for
each complexity level $\alpha$ there is a hypothesis $S_{ij}$ with $j\le i\le
l(x)$ of complexity at most $\alpha$ that has almost the best fit among all
hypotheses of complexity at most $\alpha$. The hypothesis $S_{ij}$ is
identified by $i$ and the leading $i-j$ bits of the binary representation of
the number of strings of complexity at most $i$. On the other hand, the initial
data $x$ might be completely irrelevant to the the number of strings of
complexity at most $i$. Thus $S_{ij}$ seems to have some information irrelevant
to the data, which undermines Kolmogorov's approach: the best hypotheses should
not have irrelevant information.
  To restrict the class of hypotheses for a data $x$ to those that have only
relevant information, Vereshchagin introduced a notion of a strong model for
$x$: those are models for $x$ whose total conditional complexity conditional to
$x$ is negligible. An object $x$ is called normal if for each complexity level
$\alpha$ at least one its best fitting model of that complexity is strong.
  In this paper we show that there are &quot;many types&quot; of normal strings. Our
second result states that there is a normal object $x$ such that all its best
fitting models $S_{ij}$ are not strong for $x$. Our last result states that
every best fit strong model for a normal object is again a normal object.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04514</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04514</id><created>2015-12-14</created><authors><author><keyname>Kourtellaris</keyname><forenames>Christos K.</forenames></author><author><keyname>Charalambous</keyname><forenames>Charalambos D.</forenames></author></authors><title>Information Structures of Capacity Achieving Distributions for Feedback
  Channels with Memory and Transmission Cost: Stochastic Optimal Control &amp;
  Variational Equalities-Part I</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Finite Transmission Feedback Information (FTFI) capacity is characterized
for any class of channel conditional distributions $\big\{{\bf P}_{B_i|B^{i-1},
A_i} :i=0, 1, \ldots, n\big\}$ and $\big\{ {\bf P}_{B_i|B_{i-M}^{i-1}, A_i}
:i=0, 1, \ldots, n\big\}$, where $M$ is the memory of the channel, $B^n =
\{B_j: j=0,1, \ldots, n\}$ are the channel outputs and $A^n=\{A_j: j=0,1,
\ldots, n\}$ are the channel inputs. The characterizations of FTFI capacity,
are obtained by first identifying the information structures of the optimal
channel input conditional distributions ${\cal P}_{[0,n]} =\big\{ {\bf
P}_{A_i|A^{i-1}, B^{i-1}}: i=1, \ldots, n\big\}$, which maximize directed
information $C_{A^n \rightarrow B^n}^{FB} = \sup_{ {\cal P}_{[0,n]} }
I(A^n\rightarrow B^n), \ \ \ I(A^n \rightarrow B^n) = \sum_{i=0}^n
I(A^i;B_i|B^{i-1}) . $ The main theorem states, for any channel with memory
$M$, the optimal channel input conditional distributions occur in the subset
${\cal Q}_{[0,n]}= \big\{ {\bf P}_{A_i|B_{i-M}^{i-1}}: i=1, \ldots, n\big\}$,
and the characterization of FTFI capacity is given by $C_{A^n \rightarrow
B^n}^{FB, M} = \sup_{ {\cal Q}_{[0,n]} } \sum_{i=0}^n I(A_i;
B_i|B_{i-M}^{i-1})$ . Similar conclusions are derived for problems with
transmission cost constraints.
  The methodology utilizes stochastic optimal control theory, to identify the
control process, the controlled process, and a variational equality of directed
information, to derive upper bounds on $I(A^n \rightarrow B^n)$, which are
achievable over specific subsets of channel input conditional distributions.
For channels with limited memory, this implies the transition probabilities of
the channel output process are also of limited memory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04515</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04515</id><created>2015-12-14</created><authors><author><keyname>Kopelowitz</keyname><forenames>Tsvi</forenames></author><author><keyname>Porat</keyname><forenames>Ely</forenames></author></authors><title>Breaking the Variance: Approximating the Hamming Distance in $\tilde
  O(1/\epsilon)$ Time Per Alignment</title><categories>cs.DS</categories><comments>Appeared in FOCS 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The algorithmic tasks of computing the Hamming distance between a given
pattern of length $m$ and each location in a text of length $n$ is one of the
most fundamental algorithmic tasks in string algorithms. Unfortunately, there
is evidence that for a text $T$ of size $n$ and a pattern $P$ of size $m$, one
cannot compute the exact Hamming distance for all locations in $T$ in time
which is less than $\tilde O(n\sqrt m)$. However, Karloff~\cite{karloff} showed
that if one is willing to suffer a $1\pm\epsilon$ approximation, then it is
possible to solve the problem with high probability, in $\tilde O(\frac n
{\epsilon^2})$ time.
  Due to related lower bounds for computing the Hamming distance of two strings
in the one-way communication complexity model, it is strongly believed that
obtaining an algorithm for solving the approximation version cannot be done
much faster as a function of $\frac 1 \epsilon$. We show here that this belief
is false by introducing a new $\tilde O(\frac{n}{\epsilon})$ time algorithm
that succeeds with high probability.
  The main idea behind our algorithm, which is common in sparse recovery
problems, is to reduce the variance of a specific randomized experiment by
(approximately) separating heavy hitters from non-heavy hitters. However, while
known sparse recovery techniques work very well on vectors, they do not seem to
apply here, where we are dealing with mismatches between pairs of characters.
We introduce two main algorithmic ingredients. The first is a new sparse
recovery method that applies for pair inputs (such as in our setting). The
second is a new construction of hash/projection functions, which allows to
count the number of projections that induce mismatches between two characters
exponentially faster than brute force. We expect that these algorithmic
techniques will be of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04564</identifier>
 <datestamp>2015-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04564</id><created>2015-12-14</created><authors><author><keyname>Nien</keyname><forenames>Hung</forenames></author><author><keyname>Fessler</keyname><forenames>Jeffrey A.</forenames></author></authors><title>Relaxed Linearized Algorithms for Faster X-Ray CT Image Reconstruction</title><categories>math.OC cs.LG stat.ML</categories><comments>Submitted to IEEE Transactions on Medical Imaging</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Statistical image reconstruction (SIR) methods are studied extensively for
X-ray computed tomography (CT) due to the potential of acquiring CT scans with
reduced X-ray dose while maintaining image quality. However, the longer
reconstruction time of SIR methods hinders their use in X-ray CT in practice.
To accelerate statistical methods, many optimization techniques have been
investigated. Over-relaxation is a common technique to speed up convergence of
iterative algorithms. For instance, using a relaxation parameter that is close
to two in alternating direction method of multipliers (ADMM) has been shown to
speed up convergence significantly. This paper proposes a relaxed linearized
augmented Lagrangian (AL) method that shows theoretical faster convergence rate
with over-relaxation and applies the proposed relaxed linearized AL method to
X-ray CT image reconstruction problems. Experimental results with both
simulated and real CT scan data show that the proposed relaxed algorithm (with
ordered-subsets [OS] acceleration) is about twice as fast as the existing
unrelaxed fast algorithms, with negligible computation and memory overhead.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04570</identifier>
 <datestamp>2015-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04570</id><created>2015-12-14</created><authors><author><keyname>Magdy</keyname><forenames>Walid</forenames></author><author><keyname>Darwish</keyname><forenames>Kareem</forenames></author><author><keyname>Abokhodair</keyname><forenames>Norah</forenames></author></authors><title>Quantifying Public Response towards Islam on Twitter after Paris Attacks</title><categories>cs.SI</categories><comments>9 pages, 5 figures</comments><acm-class>J.4; K.4.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Paris terrorist attacks occurred on November 13, 2015 prompted a massive
response on social media including Twitter, with millions of posted tweets in
the first few hours after the attacks. Most of the tweets were condemning the
attacks and showing support to Parisians. One of the trending debates related
to the attacks concerned possible association between terrorism and Islam and
Muslims in general. This created a global discussion between those attacking
and those defending Islam and Muslims. In this paper, we provide quantitative
and qualitative analysis of data collection we streamed from Twitter starting 7
hours after the Paris attacks and for 50 subsequent hours that are related to
blaming Islam and Muslims and to defending them. We collected a set of 8.36
million tweets in this epoch consisting of tweets in many different of
languages. We could identify a subset consisting of 900K tweets relating to
Islam and Muslims. Using sampling methods and crowd-sourcing annotation, we
managed to estimate the public response of these tweets. Our findings show that
the majority of the tweets were in fact defending Muslims and absolving them
from responsibility for the attacks. However, a considerable number of tweets
were blaming Muslims, with most of these tweets coming from western countries
such as the Netherlands, France, and the US.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04582</identifier>
 <datestamp>2015-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04582</id><created>2015-10-21</created><authors><author><keyname>Egger</keyname><forenames>Jan</forenames></author><author><keyname>Busse</keyname><forenames>Harald</forenames></author><author><keyname>Brandmaier</keyname><forenames>Philipp</forenames></author><author><keyname>Seider</keyname><forenames>Daniel</forenames></author><author><keyname>Gawlitza</keyname><forenames>Matthias</forenames></author><author><keyname>Strocka</keyname><forenames>Steffen</forenames></author><author><keyname>Voglreiter</keyname><forenames>Philip</forenames></author><author><keyname>Dokter</keyname><forenames>Mark</forenames></author><author><keyname>Hofmann</keyname><forenames>Michael</forenames></author><author><keyname>Kainz</keyname><forenames>Bernhard</forenames></author><author><keyname>Hann</keyname><forenames>Alexander</forenames></author><author><keyname>Chen</keyname><forenames>Xiaojun</forenames></author><author><keyname>Alhonnoro</keyname><forenames>Tuomas</forenames></author><author><keyname>Pollari</keyname><forenames>Mika</forenames></author><author><keyname>Schmalstieg</keyname><forenames>Dieter</forenames></author><author><keyname>Moche</keyname><forenames>Michael</forenames></author></authors><title>Interactive Volumetry Of Liver Ablation Zones</title><categories>cs.CV cs.GR cs.HC physics.med-ph</categories><comments>18 pages, 15 figures, 8 tables, 57 references</comments><journal-ref>Sci. Rep. 5, 15373; doi: 10.1038/srep15373 (2015)</journal-ref><doi>10.1038/srep15373</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Percutaneous radiofrequency ablation (RFA) is a minimally invasive technique
that destroys cancer cells by heat. The heat results from focusing energy in
the radiofrequency spectrum through a needle. Amongst others, this can enable
the treatment of patients who are not eligible for an open surgery. However,
the possibility of recurrent liver cancer due to incomplete ablation of the
tumor makes post-interventional monitoring via regular follow-up scans
mandatory. These scans have to be carefully inspected for any conspicuousness.
Within this study, the RF ablation zones from twelve post-interventional CT
acquisitions have been segmented semi-automatically to support the visual
inspection. An interactive, graph-based contouring approach, which prefers
spherically shaped regions, has been applied. For the quantitative and
qualitative analysis of the algorithm's results, manual slice-by-slice
segmentations produced by clinical experts have been used as the gold standard
(which have also been compared among each other). As evaluation metric for the
statistical validation, the Dice Similarity Coefficient (DSC) has been
calculated. The results show that the proposed tool provides lesion
segmentation with sufficient accuracy much faster than manual segmentation. The
visual feedback and interactivity make the proposed tool well suitable for the
clinical workflow.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04602</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04602</id><created>2015-12-14</created><updated>2016-02-01</updated><authors><author><keyname>Tan</keyname><forenames>Jethro</forenames></author><author><keyname>Pawe&#x142;czak</keyname><forenames>Przemys&#x142;aw</forenames></author><author><keyname>Parks</keyname><forenames>Aaron</forenames></author><author><keyname>Smith</keyname><forenames>Joshua R.</forenames></author></authors><title>Wisent: Robust Downstream Communication and Storage for Computational
  RFIDs</title><categories>cs.NI</categories><comments>Accepted for Publication to IEEE INFOCOM 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computational RFID (CRFID) devices are emerging platforms that can enable
perennial computation and sensing by eliminating the need for batteries.
Although much research has been devoted to improving upstream (CRFID to RFID
reader) communication rates, the opposite direction has so far been neglected,
presumably due to the difficulty of guaranteeing fast and error-free transfer
amidst frequent power interruptions of CRFID. With growing interest in the
market where CRFIDs are forever-embedded in many structures, it is necessary
for this void to be filled. Therefore, we propose Wisent-a robust downstream
communication protocol for CRFIDs that operates on top of the legacy UHF RFID
communication protocol: EPC C1G2. The novelty of Wisent is its ability to
adaptively change the frame length sent by the reader, based on the length
throttling mechanism, to minimize the transfer times at varying channel
conditions. We present an implementation of Wisent for the WISP 5 and an
off-the-shelf RFID reader. Our experiments show that Wisent allows transfer up
to 16 times faster than a baseline, non-adaptive shortest frame case, i.e.
single word length, at sub-meter distance. As a case study, we show how Wisent
enables wireless CRFID reprogramming, demonstrating the world's first
wirelessly reprogrammable (software defined) CRFID.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04605</identifier>
 <datestamp>2015-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04605</id><created>2015-12-14</created><authors><author><keyname>Rizoiu</keyname><forenames>Marian-Andrei</forenames></author><author><keyname>Velcin</keyname><forenames>Julien</forenames></author><author><keyname>Lallich</keyname><forenames>St&#xe9;phane</forenames></author></authors><title>Semantic-enriched Visual Vocabulary Construction in a Weakly Supervised
  Context</title><categories>cs.CV</categories><journal-ref>M.-A. Rizoiu, J. Velcin, and S. Lallich, &quot;Semantic-enriched Visual
  Vocabulary Construction in a Weakly Supervised Context,&quot; Intelligent Data
  Analysis, vol. 19, iss. 1, pp. 161-185, 2015</journal-ref><doi>10.3233/IDA-140702</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the prevalent learning tasks involving images is content-based image
classification. This is a difficult task especially because the low-level
features used to digitally describe images usually capture little information
about the semantics of the images. In this paper, we tackle this difficulty by
enriching the semantic content of the image representation by using external
knowledge. The underlying hypothesis of our work is that creating a more
semantically rich representation for images would yield higher machine learning
performances, without the need to modify the learning algorithms themselves.
The external semantic information is presented under the form of non-positional
image labels, therefore positioning our work in a weakly supervised context.
Two approaches are proposed: the first one leverages the labels into the visual
vocabulary construction algorithm, the result being dedicated visual
vocabularies. The second approach adds a filtering phase as a pre-processing of
the vocabulary construction. Known positive and known negative sets are
constructed and features that are unlikely to be associated with the objects
denoted by the labels are filtered. We apply our proposition to the task of
content-based image classification and we show that semantically enriching the
image representation yields higher classification performances than the
baseline representation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04629</identifier>
 <datestamp>2015-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04629</id><created>2015-12-14</created><authors><author><keyname>Bienz</keyname><forenames>Amanda</forenames></author><author><keyname>Gropp</keyname><forenames>Robert D. Falgout William</forenames></author><author><keyname>Olson</keyname><forenames>Luke N.</forenames></author><author><keyname>Schroder</keyname><forenames>Jacob B.</forenames></author></authors><title>Reducing Parallel Communication in Algebraic Multigrid through
  Sparsification</title><categories>cs.DC math.NA</categories><comments>27 pages, 19 figures, submitted to SISC, multigrid, algebraic
  multigrid, non-Galerkin multigrid, high performance computing</comments><msc-class>65F50</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Algebraic multigrid (AMG) is an $\mathcal{O}(n)$ solution process for many
large sparse linear systems. A hierarchy of progressively coarser grids is
constructed that utilize complementary relaxation and interpolation operators.
High-energy error is reduced by relaxation, while low-energy error is mapped to
coarse-grids and reduced there. However, large parallel communication costs
often limit parallel scalability. As the multigrid hierarchy is formed, each
coarse matrix is formed through a triple matrix product. The resulting
coarse-grids often have significantly more nonzeros per row than the original
fine-grid operator, thereby generating high parallel communication costs on
coarse-levels. In this paper, we introduce a method that systematically removes
entries in coarse-grid matrices after the hierarchy is formed, leading to an
improved communication costs. We sparsify by removing weakly connected or
unimportant entries in the matrix, leading to improved solve time. The main
trade-off is that if the heuristic identifying unimportant entries is used too
aggressively, then AMG convergence can suffer. To counteract this, the original
hierarchy is retained, allowing entries to be reintroduced into the solver
hierarchy if convergence is too slow. This enables a balance between
communication cost and convergence, as necessary. In this paper we present new
algorithms for reducing communication and present a number of computational
experiments in support.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04633</identifier>
 <datestamp>2015-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04633</id><created>2015-12-14</created><authors><author><keyname>Lofgren</keyname><forenames>Peter</forenames></author></authors><title>Efficient Algorithms for Personalized PageRank</title><categories>cs.DS cs.IR cs.SI</categories><comments>PhD Thesis (Stanford Computer Science). Based on joint work with Sid
  Banerjee and Ashish Goel</comments><acm-class>H.3.3; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present new, more efficient algorithms for estimating random walk scores
such as Personalized PageRank from a given source node to one or several target
nodes. These scores are useful for personalized search and recommendations on
networks including social networks, user-item networks, and the web. Past work
has proposed using Monte Carlo or using linear algebra to estimate scores from
a single source to every target, making them inefficient for a single pair. Our
contribution is a new bidirectional algorithm which combines linear algebra and
Monte Carlo to achieve significant speed improvements. On a diverse set of six
graphs, our algorithm is 70x faster than past state-of-the-art algorithms. We
also present theoretical analysis: while past algorithms require $\Omega(n)$
time to estimate a random walk score of typical size $\frac{1}{n}$ on an
$n$-node graph to a given constant accuracy, our algorithm requires only
$O(\sqrt{m})$ expected time for an average target, where $m$ is the number of
edges, and is provably accurate.
  In addition to our core bidirectional estimator for personalized PageRank, we
present an alternative algorithm for undirected graphs, a generalization to
arbitrary walk lengths and Markov Chains, an algorithm for personalized search
ranking, and an algorithm for sampling random paths from a given source to a
given set of targets. We expect our bidirectional methods can be extended in
other ways and will be useful subroutines in other graph analysis problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04636</identifier>
 <datestamp>2015-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04636</id><created>2015-12-14</created><authors><author><keyname>Boroomand</keyname><forenames>Ameneh</forenames></author><author><keyname>Shafiee</keyname><forenames>Mohammad Javad</forenames></author><author><keyname>Khalvati</keyname><forenames>Farzad</forenames></author><author><keyname>Haider</keyname><forenames>Masoom A.</forenames></author><author><keyname>Wong</keyname><forenames>Alexander</forenames></author></authors><title>Noise-Compensated, Bias-Corrected Diffusion Weighted Endorectal Magnetic
  Resonance Imaging via a Stochastically Fully-Connected Joint Conditional
  Random Field Model</title><categories>stat.ME cs.CV physics.med-ph stat.AP</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Diffusion weighted magnetic resonance imaging (DW-MRI) is a powerful tool in
imaging-based prostate cancer (PCa) screening and detection. Endorectal coils
are commonly used in DW-MRI to improve the signal-to-noise ratio (SNR) of the
acquisition, at the expense of significant intensity inhomogeneities (bias
field) that worsens as we move away from the endorectal coil. The presence of
bias field can have a significant negative impact on the accuracy of different
image analysis tasks, as well as the accuracy of PCa tumor localization, thus
leading to increased inter- and intra-observer variability. The previously
proposed bias field correction methods often suffer from undesired noise
amplification that can reduce the image quality of the resulting bias-corrected
DW-MRI data. Here, we propose a unified data reconstruction approach that
enables joint compensation of bias field as well as data noise in diffusion
weighted endorectal magnetic resonance (DW-EMR) imaging. The proposed
noise-compensated, bias-corrected (NCBC) data reconstruction method takes
advantage of a novel stochastically fully connected joint conditional random
field (SFC-JCRF) model to mitigate the effects of data noise and bias field in
the reconstructed DW-EMR prostate imaging data. The proposed NCBC
reconstruction method was tested on synthetic DW-EMR data, physical DW-EMR
phantom, as well as real DW-EMR imaging data. Both qualitative and quantitative
analysis illustrated that the proposed NCBC method can achieve improved image
quality when compared to other tested bias correction methods. As such, the
proposed NCBC method can have strong potential for improving the consistency of
image interpretations, thus leading to more accurate PCa diagnosis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04637</identifier>
 <datestamp>2015-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04637</id><created>2015-12-14</created><authors><author><keyname>Dubey</keyname><forenames>Pradeep</forenames></author><author><keyname>Sahi</keyname><forenames>Siddhartha</forenames></author><author><keyname>Shubik</keyname><forenames>Martin</forenames></author></authors><title>Graphical Exchange Mechanisms</title><categories>cs.GT math.CO</categories><comments>26 pages</comments><msc-class>91B64</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider an exchange mechanism which accepts diversified offers of various
commodities and redistributes everything it receives. We impose certain
conditions of fairness and convenience on such a mechanism and show that it
admits unique prices, which equalize the value of offers and returns for each
individual.
  We next define the complexity of a mechanism in terms of certain integers
$\tau_{ij},\pi_{ij}$ and $k_{i}$ that represent the time required to exchange
$i$ for $j$, the difficulty in determining the exchange ratio, and the
dimension of the message space. We show that there are a finite number of
minimally complex mechanisms, in each of which all trade is conducted through
markets for commodity pairs.
  Finally we consider minimal mechanisms with smallest worst-case complexities
$\tau=\max\tau_{ij}$ and $\pi=\max\pi_{ij}$. For $m&gt;3$ commodities, there are
precisely three such mechanisms, one of which has a distinguished commodity --
the money -- that serves as the sole medium of exchange. As $m\rightarrow
\infty$ the money mechanism is the only one with bounded $\left( \pi
,\tau\right) $.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04639</identifier>
 <datestamp>2015-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04639</id><created>2015-12-14</created><authors><author><keyname>Bukatin</keyname><forenames>Michael</forenames></author><author><keyname>Matthews</keyname><forenames>Steve</forenames></author></authors><title>Linear Models of Computation and Program Learning</title><categories>cs.LO cs.NE</categories><comments>13 pages; September 3, 2015 version; to appear in the Proceedings of
  GCAI 2015, Tbilisi, Georgia, Oct.16-18, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider two classes of computations which admit taking linear
combinations of execution runs: probabilistic sampling and generalized
animation. We argue that the task of program learning should be more tractable
for these architectures than for conventional deterministic programs. We look
at the recent advances in the &quot;sampling the samplers&quot; paradigm in higher-order
probabilistic programming. We also discuss connections between partial
inconsistency, non-monotonic inference, and vector semantics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04648</identifier>
 <datestamp>2015-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04648</id><created>2015-12-14</created><authors><author><keyname>Maria</keyname><forenames>Cl&#xe9;ment</forenames></author><author><keyname>Spreer</keyname><forenames>Jonathan</forenames></author></authors><title>Admissible colourings of 3-manifold triangulations for Turaev-Viro type
  invariants</title><categories>cs.CG math.CO math.GT</categories><comments>26 pages, 10 figures, 5 tables</comments><msc-class>57M27, 57Q15, 68W40, 52C45</msc-class><acm-class>G.2.1; F.2.2; I.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Turaev Viro invariants are amongst the most powerful tools to distinguish
3-manifolds: They are implemented in mathematical software, and allow practical
computations. The invariants can be computed purely combinatorially by
enumerating colourings on the edges of a triangulation T.
  These edge colourings can be interpreted as embeddings of surfaces in T. We
give a characterisation of how these embedded surfaces intersect with the
tetrahedra of T. This is done by characterising isotopy classes of simple
closed loops in the 3-punctured disk. As a direct result we obtain a new system
of coordinates for edge colourings which allows for simpler definitions of the
tetrahedron weights incorporated in the Turaev-Viro invariants.
  Moreover, building on a detailed analysis of the colourings, as well as
classical work due to Kirby and Melvin, Matveev, and others, we show that
considering a much smaller set of colourings suffices to compute Turaev-Viro
invariants in certain significant cases. This results in a substantial
improvement of running times to compute the invariants, reducing the number of
colourings to consider by a factor of $2^n$. In addition, we present an
algorithm to compute Turaev-Viro invariants of degree four -- a problem known
to be #P-hard -- which capitalises on the combinatorial structure of the input.
  The improved algorithms are shown to be optimal in the following sense: There
exist triangulations admitting all colourings the algorithms consider.
Furthermore, we demonstrate that our new algorithms to compute Turaev-Viro
invariants are able to distinguish the majority of $\mathbb{Z}$-homology
spheres with complexity up to $11$ in $O(2^n)$ operations in $\mathbb{Q}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04650</identifier>
 <datestamp>2015-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04650</id><created>2015-12-14</created><authors><author><keyname>Cheng</keyname><forenames>Yong</forenames></author><author><keyname>Shen</keyname><forenames>Shiqi</forenames></author><author><keyname>He</keyname><forenames>Zhongjun</forenames></author><author><keyname>He</keyname><forenames>Wei</forenames></author><author><keyname>Wu</keyname><forenames>Hua</forenames></author><author><keyname>Sun</keyname><forenames>Maosong</forenames></author><author><keyname>Liu</keyname><forenames>Yang</forenames></author></authors><title>Agreement-based Joint Training for Bidirectional Attention-based Neural
  Machine Translation</title><categories>cs.CL</categories><comments>9 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The attentional mechanism has proven to be effective in improving end-to-end
neural machine translation. However, due to the structural divergence between
natural languages, unidirectional attention-based models might only capture
partial aspects of attentional regularities. We propose agreement-based joint
training for bidirectional attention-based end-to-end neural machine
translation. Instead of training source-to-target and target-to-source
translation models independently, our approach encourages the two complementary
models to agree on word alignment matrices on the same training data.
Experiments on Chinese-English and English-French translation tasks show that
joint training significantly improves both alignment and translation quality
over independent training.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04652</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04652</id><created>2015-12-15</created><updated>2016-01-24</updated><authors><author><keyname>Montazeri</keyname><forenames>Mitra</forenames></author><author><keyname>Baghshah</keyname><forenames>Mahdieh Soleymani</forenames></author><author><keyname>Enhesari</keyname><forenames>Ahmad</forenames></author></authors><title>Hyper-Heuristic Algorithm for Finding Efficient Features in Diagnose of
  Lung Cancer Disease</title><categories>cs.AI</categories><comments>Published in the Journal of Basic and Applied Scientific Research,
  2013</comments><journal-ref>J. Basic Appl. Sci. Res, 2013. 3(10): p. 134-140</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Background: Lung cancer was known as primary cancers and the survival rate of
cancer is about 15%. Early detection of lung cancer is the leading factor in
survival rate. All symptoms (features) of lung cancer do not appear until the
cancer spreads to other areas. It needs an accurate early detection of lung
cancer, for increasing the survival rate. For accurate detection, it need
characterizes efficient features and delete redundancy features among all
features. Feature selection is the problem of selecting informative features
among all features. Materials and Methods: Lung cancer database consist of 32
patient records with 57 features. This database collected by Hong and Youngand
indexed in the University of California Irvine repository. Experimental
contents include the extracted from the clinical data and X-ray data, etc. The
data described 3 types of pathological lung cancers and all features are taking
an integer value 0-3. In our study, new method is proposed for identify
efficient features of lung cancer. It is based on Hyper-Heuristic. Results: We
obtained an accuracy of 80.63% using reduced 11 feature set. The proposed
method compare to the accuracy of 5 machine learning feature selections. The
accuracy of these 5 methods are 60.94, 57.81, 68.75, 60.94 and 68.75.
Conclusions: The proposed method has better performance with the highest level
of accuracy. Therefore, the proposed model is recommended for identifying an
efficient symptom of Disease. These finding are very important in health
research, particularly in allocation of medical resources for patients who
predicted as high-risks
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04653</identifier>
 <datestamp>2015-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04653</id><created>2015-12-15</created><authors><author><keyname>Han</keyname><forenames>Sok-Min</forenames></author><author><keyname>Pang</keyname><forenames>Un-Chol</forenames></author><author><keyname>Choe</keyname><forenames>Hyok-Chol</forenames></author><author><keyname>Hwang</keyname><forenames>Chol-Jun</forenames></author></authors><title>Using Pi-calculus to Model Dynamic Web Services Composition Based on the
  Authority Model</title><categories>cs.SE cs.NI</categories><comments>11 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are lots of research works on web service, composition, modeling,
verification and other problems. Theses research works are done on the basis of
formal methods, such as petri-net, pi-calculus, automata theory, and so on.
Pi-calculus is a natural vehicle to model mobility aspect in dynamic web
services composition (DWSC). However, it has recently been shown that
pi-calculus needs to be extended suitably to specify and verify DWSC. In this
paper, we considers the authority model for DWSC, extends pi-calculus in order
to model dynamic attributes of system, and proposes a automatic method for
modeling DWSC based on extended pi-calculus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04656</identifier>
 <datestamp>2015-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04656</id><created>2015-12-15</created><authors><author><keyname>Blech</keyname><forenames>Jan Olaf</forenames></author></authors><title>An Example for BeSpaceD and its Use for Decision Support in Industrial
  Automation</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe our formal methods-based spatial reasoning framework BeSpaceD and
its application in decision support for industrial automation. In particular we
are supporting analysis and decisions based on formal models for industrial
plant and mining operations. BeSpaceD is a framework for deciding geometric and
topological properties of spatio-temporal models. We present an example and
report on our ongoing experience with applications in different projects around
software and cyber-physical systems engineering. The example features
abstracted aspects of a production plant model. Using the example we motivate
the use of our framework in the context of an existing software platform
supporting monitoring, incident handling and maintenance of industrial
automation facilities in remote locations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04701</identifier>
 <datestamp>2015-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04701</id><created>2015-12-15</created><authors><author><keyname>Li</keyname><forenames>Weixin</forenames></author><author><keyname>Joo</keyname><forenames>Jungseock</forenames></author><author><keyname>Qi</keyname><forenames>Hang</forenames></author><author><keyname>Zhu</keyname><forenames>Song-Chun</forenames></author></authors><title>Joint Image-Text News Topic Detection and Tracking with And-Or Graph
  Representation</title><categories>cs.IR cs.CL cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we aim to develop a method for automatically detecting and
tracking topics in broadcast news. We present a hierarchical And-Or graph (AOG)
to jointly represent the latent structure of both texts and visuals. The AOG
embeds a context sensitive grammar that can describe the hierarchical
composition of news topics by semantic elements about people involved, related
places and what happened, and model contextual relationships between elements
in the hierarchy. We detect news topics through a cluster sampling process
which groups stories about closely related events. Swendsen-Wang Cuts (SWC), an
effective cluster sampling algorithm, is adopted for traversing the solution
space and obtaining optimal clustering solutions by maximizing a Bayesian
posterior probability. Topics are tracked to deal with the continuously updated
news streams. We generate topic trajectories to show how topics emerge, evolve
and disappear over time. The experimental results show that our method can
explicitly describe the textual and visual data in news videos and produce
meaningful topic trajectories. Our method achieves superior performance
compared to state-of-the-art methods on both a public dataset Reuters-21578 and
a self-collected dataset named UCLA Broadcast News Dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04719</identifier>
 <datestamp>2015-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04719</id><created>2015-12-15</created><authors><author><keyname>Fischer</keyname><forenames>Carsten</forenames></author><author><keyname>R&#xf6;glin</keyname><forenames>Heiko</forenames></author></authors><title>Probabilistic Analysis of the Dual Next-Fit Algorithm for Bin Covering</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the bin covering problem, the goal is to fill as many bins as possible up
to a certain minimal level with a given set of items of different sizes. Online
variants, in which the items arrive one after another and have to be packed
immediately on their arrival without knowledge about the future items, have
been studied extensively in the literature. We study the simplest possible
online algorithm Dual Next-Fit, which packs all arriving items into the same
bin until it is filled and then proceeds with the next bin in the same manner.
The competitive ratio of this and any other reasonable online algorithm is
$1/2$.
  We study Dual Next-Fit in a probabilistic setting where the item sizes are
chosen i.i.d.\ according to a discrete distribution and we prove that, for
every distribution, its expected competitive ratio is at least $1/2+\epsilon$
for a constant $\epsilon&gt;0$ independent of the distribution. We also prove an
upper bound of $2/3$ and better lower bounds for certain restricted classes of
distributions. Finally, we prove that the expected competitive ratio equals,
for a large class of distributions, the random-order ratio, which is the
expected competitive ratio when adversarially chosen items arrive in uniformly
random order.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04748</identifier>
 <datestamp>2015-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04748</id><created>2015-12-15</created><authors><author><keyname>Akbari</keyname><forenames>Saieed</forenames></author><author><keyname>Motiei</keyname><forenames>Mohammad</forenames></author><author><keyname>Mozaffari</keyname><forenames>Sahand</forenames></author><author><keyname>Yazdanbod</keyname><forenames>Sina</forenames></author></authors><title>Cubic Graphs with Total Domatic Number at Least Two</title><categories>math.CO cs.DM</categories><comments>6 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $G$ be a graph. A total dominating set of $G$ is a set $S$ of vertices of
$G$ such that every vertex is adjacent to at least one vertex in $S$. The total
domatic number of a graph is the maximum number of total dominating sets which
partition the vertex set of $G$. In this paper we would like to characterize
the cubic graphs with total domatic number at least two.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04751</identifier>
 <datestamp>2015-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04751</id><created>2015-12-15</created><authors><author><keyname>Giustolisi</keyname><forenames>Rosario</forenames></author></authors><title>Design and Analysis of Secure Exam Protocols</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Except for the traditional threat that candidates may want to cheat, exams
have historically not been seen as a serious security problem. That threat is
routinely thwarted by having invigilators ensure that candidates do not
misbehave during testing. However, as recent exam scandals confirm, also
invigilators and exam authorities may have interest in frauds, hence they may
pose security threats as well. Moreover, new security issues arise from the
recent use of computers, which can facilitate the exam experience for example
by allowing candidates to register from home. Thus, exams must be designed with
the care normally devoted to security protocols.
  This dissertation studies exam protocol security and provides an in-depth
understanding that can be also useful for the study of the security of similar
systems, such as personnel selections, project reviews, and conference
management systems. It introduces an unambiguous terminology that leads to the
specification of a taxonomy of various exam types, depending on the level of
computer assistance. It then establishes a theoretical framework for the formal
analysis of exams. The framework defines several authentication, privacy, and
verifiability requirements that modern exams should meet, and enables the
security of exam protocols to be studied. Using the framework, we formally
analyse traditional, computer-assisted, and Internet-based exam protocols. We
find some security issues and propose modifications to partially achieve the
desired requirements.
  This dissertation also designs three novel exam protocols that guarantee a
wide set of security requirements.
  Finally, this dissertation looks at exams as carried out through modern
browsers. We advance a formal analysis of requirements that are not only
logically conditioned on the technology but also on user actions. We conclude
with best-practice recommendations to browser vendors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04754</identifier>
 <datestamp>2015-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04754</id><created>2015-12-15</created><authors><author><keyname>Kamilov</keyname><forenames>Ulugbek S.</forenames></author><author><keyname>Mansour</keyname><forenames>Hassan</forenames></author></authors><title>Learning optimal nonlinearities for iterative thresholding algorithms</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Iterative shrinkage/thresholding algorithm (ISTA) is a well-studied method
for finding sparse solutions to ill-posed inverse problems. In this letter, we
present a data-driven scheme for learning optimal thresholding functions for
ISTA. The proposed scheme is obtained by relating iterations of ISTA to layers
of a simple deep neural network (DNN) and developing a corresponding error
backpropagation algorithm that allows to fine-tune the thresholding functions.
Simulations on sparse statistical signals illustrate potential gains in
estimation quality due to the proposed data adaptive ISTA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04776</identifier>
 <datestamp>2015-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04776</id><created>2015-12-15</created><authors><author><keyname>Tabourier</keyname><forenames>Lionel</forenames></author><author><keyname>Libert</keyname><forenames>Anne-Sophie</forenames></author><author><keyname>Lambiotte</keyname><forenames>Renaud</forenames></author></authors><title>Predicting links in ego-networks using temporal information</title><categories>cs.SI physics.soc-ph</categories><comments>submitted to EPJ Data Science</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Link prediction appears as a central problem of network science, as it calls
for unfolding the mechanisms that govern the micro-dynamics of the network. In
this work, we are interested in ego-networks, that is the mere information of
interactions of a node to its neighbors, in the context of social
relationships. As the structural information is very poor, we rely on another
source of information to predict links among egos' neighbors: the timing of
interactions. We define several features to capture different kinds of temporal
information and apply machine learning methods to combine these various
features and improve the quality of the prediction. We demonstrate the
efficiency of this temporal approach on a cellphone interaction dataset,
pointing out features which prove themselves to perform well in this context,
in particular the temporal profile of interactions and elapsed time between
contacts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04778</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04778</id><created>2015-12-15</created><authors><author><keyname>Shi</keyname><forenames>Yuanming</forenames></author><author><keyname>Zhang</keyname><forenames>Jun</forenames></author><author><keyname>Letaief</keyname><forenames>Khaled B.</forenames></author></authors><title>Robust Group Sparse Beamforming for Multicast Green Cloud-RAN with
  Imperfect CSI</title><categories>cs.IT math.IT</categories><comments>This paper has been accepted by IEEE Transactions on Signal
  Processing 2015</comments><doi>10.1109/TSP.2015.2442957</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the network power minimization problem for the
multicast cloud radio access network (Cloud-RAN) with imperfect channel state
information (CSI). The key observation is that network power minimization can
be achieved by adaptively selecting active remote radio heads (RRHs) via
controlling the group-sparsity structure of the beamforming vector. However,
this yields a non-convex combinatorial optimization problem, for which we
propose a three-stage robust group sparse beamforming algorithm. In the first
stage, a quadratic variational formulation of the weighted mixed l1/l2-norm is
proposed to induce the group-sparsity structure in the aggregated beamforming
vector, which indicates those RRHs that can be switched off. A perturbed
alternating optimization algorithm is then proposed to solve the resultant
non-convex group-sparsity inducing optimization problem by exploiting its
convex substructures. In the second stage, we propose a PhaseLift technique
based algorithm to solve the feasibility problem with a given active RRH set,
which helps determine the active RRHs. Finally, the semidefinite relaxation
(SDR) technique is adopted to determine the robust multicast beamformers.
Simulation results will demonstrate the convergence of the perturbed
alternating optimization algorithm, as well as, the effectiveness of the
proposed algorithm to minimize the network power consumption for multicast
Cloud-RAN.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04782</identifier>
 <datestamp>2015-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04782</id><created>2015-12-15</created><authors><author><keyname>Escribano-Barreno</keyname><forenames>Julio</forenames></author><author><keyname>Garc&#xed;a-Valls</keyname><forenames>Marisol</forenames></author></authors><title>Supporting the monitoring of the verification process of critical
  systems'software</title><categories>cs.SE</categories><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Critical software systems face stringent requirements in safety, security,
and reliability due to the circumstances surrounding their operation. Safety
and security have progressively gained importance over the years due to the
integration of hardware with software-intensive deployments that introduce
additional sources of errors. It is, then, necessary to follow high-quality
exhaustive software development processes that besides the needed development
activities to increase safety and security also integrate techniques to
increase the reliability of the software development process itself. In
practice, the use of automated techniques for the verification of the
verification process is, however, not sufficiently wide spread. This is mainly
due to the high cost of the required techniques and to their degree of
complexity when adjusting to the different norms and regulations. This work
presents an approach for comprehensive management of the verification
processes; the approach allows engineers to monitor and control the project
status regarding the applicable standards. This approach has been validated
through its implementation in a tool and its application to real projects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04784</identifier>
 <datestamp>2015-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04784</id><created>2015-12-15</created><authors><author><keyname>Shi</keyname><forenames>Yuanming</forenames></author><author><keyname>Cheng</keyname><forenames>Jinkun</forenames></author><author><keyname>Zhang</keyname><forenames>Jun</forenames></author><author><keyname>Bai</keyname><forenames>Bo</forenames></author><author><keyname>Chen</keyname><forenames>Wei</forenames></author><author><keyname>Letaief</keyname><forenames>Khaled B.</forenames></author></authors><title>Smoothed Lp-Minimization for Green Cloud-RAN with User Admission Control</title><categories>cs.IT math.IT</categories><comments>This paper has been accepted by IEEE J. Select. Areas Commun.,
  Special Issue on Energy-Efficient Techniques for 5G Wireless Commun. Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The cloud radio access network (Cloud-RAN) has recently been proposed as one
cost-effective and energy-efficient technique for 5G wireless networks. By
moving the signal processing functionality to a single baseband unit (BBU)
pool, centralized signal processing and resource allocation are enabled in
Cloud-RAN, thereby providing the promise of improving the energy efficiency via
effective network adaptation and interference management. In this paper, we
propose a holistic sparse optimization framework to design green Cloud-RAN by
taking into consideration the power consumption of the fronthaul links,
multicast services, as well as user admission control. Specifically, we first
identify the sparsity structures in the solutions of both the network power
minimization and user admission control problems. However, finding the optimal
sparsity structures turns out to be NP-hard, with the coupled challenges of the
l0-norm based objective functions and the nonconvex quadratic QoS constraints
due to multicast beamforming. In contrast to the previous works on convex but
non-smooth sparsity inducing approaches, e.g., the group sparse beamforming
algorithm based on the mixed l1/l2-norm relaxation [1], we adopt the nonconvex
but smoothed lp-minimization (0 &lt; p &lt;= 1) approach to promote sparsity in the
multicast setting, thereby enabling efficient algorithm design based on the
principle of the majorization-minimization (MM) algorithm and the semidefinite
relaxation (SDR) technique. In particular, an iterative reweighted-l2 algorithm
is developed, which will converge to a Karush-Kuhn-Tucker (KKT) point of the
relaxed smoothed lp-minimization problem from the SDR technique. We illustrate
the effectiveness of the proposed algorithms with extensive simulations for
network power minimization and user admission control in multicast Cloud-RAN.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04785</identifier>
 <datestamp>2015-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04785</id><created>2015-12-15</created><authors><author><keyname>Vo</keyname><forenames>Phong D.</forenames></author><author><keyname>Ginsca</keyname><forenames>Alexandru</forenames></author><author><keyname>Borgne</keyname><forenames>Herv&#xe9; Le</forenames></author><author><keyname>Popescu</keyname><forenames>Adrian</forenames></author></authors><title>On Deep Representation Learning from Noisy Web Images</title><categories>cs.CV cs.MM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The keep-growing content of Web images may be the next important data source
to scale up deep neural networks, which recently obtained a great success in
the ImageNet classification challenge and related tasks. This prospect,
however, has not been validated on convolutional networks (convnet) -- one of
best performing deep models -- because of their supervised regime. While
unsupervised alternatives are not so good as convnet in generalizing the
learned model to new domains, we use convnet to leverage semi-supervised
representation learning. Our approach is to use massive amounts of unlabeled
and noisy Web images to train convnets as general feature detectors despite
challenges coming from data such as high level of mislabeled data, outliers,
and data biases. Extensive experiments are conducted at several data scales,
different network architectures, and data reranking techniques. The learned
representations are evaluated on nine public datasets of various topics. The
best results obtained by our convnets, trained on 3.14 million Web images,
outperform AlexNet trained on 1.2 million clean images of ILSVRC 2012 and is
closing the gap with VGG-16. These prominent results suggest a budget solution
to use deep learning in practice and motivate more research in semi-supervised
representation learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04788</identifier>
 <datestamp>2015-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04788</id><created>2015-12-15</created><authors><author><keyname>Abdon</keyname><forenames>Miriam</forenames></author><author><keyname>Rolland</keyname><forenames>Robert</forenames></author></authors><title>Hamming distances from a function to all codewords of a Generalized
  Reed-Muller code of order one</title><categories>cs.IT math.IT</categories><msc-class>11T71, 94B05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For any finite field ${\mathbb F}_q$ with $q$ elements, we study the set
${\mathcal F}_{(q,m)}$ of functions from ${\mathbb F}_q^m$ into ${\mathbb
F}^q$. We introduce a transformation that allows us to determine a linear
system of $q^{m+1}$ equations and $q^{m+1}$ unknowns, which has for solution
the Hamming distances of a function in ${\mathcal F}_{(q,m)}$ to all the affine
functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04792</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04792</id><created>2015-12-15</created><updated>2016-01-25</updated><authors><author><keyname>Xiao</keyname><forenames>Han</forenames></author><author><keyname>Huang</keyname><forenames>Minlie</forenames></author><author><keyname>Zhu</keyname><forenames>Xiaoyan</forenames></author></authors><title>From One Point to A Manifold: Knowledge Graph Embedding For Precise Link
  Prediction</title><categories>cs.AI cs.LG</categories><comments>arXiv admin note: text overlap with arXiv:1509.05488</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Knowledge graph embedding aims at offering a numerical knowledge
representation paradigm by transforming the entities and relations into
continuous vector space. However, existing methods could not characterize the
knowledge graph in a fine degree to make a precise prediction. There are two
reasons: being an ill-posed algebraic system and applying an overstrict
geometric form. As precise prediction is critical, we propose an manifold-based
embedding principle (\textbf{ManifoldE}) which could be treated as a well-posed
algebraic system that expands the position of golden triples from one point in
current models to a manifold in ours. Extensive experiments show that the
proposed models achieve substantial improvements against the state-of-the-art
baselines especially for the precise prediction task, and yet maintain high
efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04794</identifier>
 <datestamp>2015-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04794</id><created>2015-12-15</created><authors><author><keyname>Shao</keyname><forenames>Shuo</forenames></author><author><keyname>Liu</keyname><forenames>Tie</forenames></author><author><keyname>Tian</keyname><forenames>Chao</forenames></author></authors><title>Multilevel Diversity Coding with Regeneration: Separate Coding Achieves
  the MBR Point</title><categories>cs.IT math.IT</categories><comments>7 pages 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of multilevel diversity coding with regeneration is considered in
this work. Two new outer bounds on the optimal tradeoffs between the normalized
storage capacity and repair bandwidth are established, by which the optimality
of separate coding at the minimum-bandwidth-regeneration (MBR) point follows
immediately. This resolves a question left open in a previous work by Tian and
Liu.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04808</identifier>
 <datestamp>2015-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04808</id><created>2015-12-15</created><authors><author><keyname>Weichwald</keyname><forenames>Sebastian</forenames></author><author><keyname>Sch&#xf6;lkopf</keyname><forenames>Bernhard</forenames></author><author><keyname>Ball</keyname><forenames>Tonio</forenames></author><author><keyname>Grosse-Wentrup</keyname><forenames>Moritz</forenames></author></authors><title>Causal and anti-causal learning in pattern recognition for neuroimaging</title><categories>stat.ML cs.LG q-bio.NC stat.ME</categories><comments>accepted manuscript</comments><journal-ref>Pattern Recognition in Neuroimaging, 2014 International Workshop
  on, 1-4, 2014</journal-ref><doi>10.1109/PRNI.2014.6858551</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pattern recognition in neuroimaging distinguishes between two types of
models: encoding- and decoding models. This distinction is based on the insight
that brain state features, that are found to be relevant in an experimental
paradigm, carry a different meaning in encoding- than in decoding models. In
this paper, we argue that this distinction is not sufficient: Relevant features
in encoding- and decoding models carry a different meaning depending on whether
they represent causal- or anti-causal relations. We provide a theoretical
justification for this argument and conclude that causal inference is essential
for interpretation in neuroimaging.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04812</identifier>
 <datestamp>2015-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04812</id><created>2015-12-15</created><authors><author><keyname>Marculescu</keyname><forenames>Bogdan</forenames></author><author><keyname>Poulding</keyname><forenames>Simon</forenames></author><author><keyname>Feldt</keyname><forenames>Robert</forenames></author><author><keyname>Petersen</keyname><forenames>Kai</forenames></author><author><keyname>Torkar</keyname><forenames>Richard</forenames></author></authors><title>Tester Interactivity makes a Difference in Search-Based Software
  Testing: A Controlled Experiment</title><categories>cs.SE</categories><comments>27 pages, 15 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Context: Search-based software testing promises to provide users with the
ability to generate high-quality test cases, and hence increase product
quality, with a minimal increase in the time and effort required. One result
that emerged out of a previous study to investigate the application of
search-based software testing (SBST) in an industrial setting was the
development of the Interactive Search-Based Software Testing (ISBST) system.
ISBST allows users to interact with the underlying SBST system, guiding the
search and assessing the results. An industrial evaluation indicated that the
ISBST system could find test cases that are not created by testers employing
manual techniques. The validity of the evaluation was threatened, however, by
the low number of participants.
  Objective: This paper presents a follow-up study, to provide a more rigorous
evaluation of the ISBST system.
  Method: To assess the ISBST system a two-way crossover controlled experiment
was conducted with 58 students taking a Verification and Validation course. The
NASA Task Load Index (NASA-TLX) is used to assess the workload experienced by
the participants in the experiment.
  Results: The experimental results validated the hypothesis that the ISBST
system generates test cases that are not found by the same participants
employing manual testing techniques. A follow-up laboratory experiment also
investigates the importance of interaction in obtaining the results. In
addition to this main result, the subjective workload was assessed for each
participant by means of the NASA-TLX tool. The evaluation showed that, while
the ISBST system required more effort from the participants, they achieved the
same performance.
  Conclusions: The paper provides evidence that the ISBST system develops test
cases that are not found by manual techniques, and that interaction plays an
important role in achieving that result.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04817</identifier>
 <datestamp>2015-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04817</id><created>2015-12-15</created><authors><author><keyname>Hay</keyname><forenames>Michael</forenames></author><author><keyname>Machanavajjhala</keyname><forenames>Ashwin</forenames></author><author><keyname>Miklau</keyname><forenames>Gerome</forenames></author><author><keyname>Chen</keyname><forenames>Yan</forenames></author><author><keyname>Zhang</keyname><forenames>Dan</forenames></author></authors><title>Principled Evaluation of Differentially Private Algorithms using DPBench</title><categories>cs.DB cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Differential privacy has become the dominant standard in the research
community for strong privacy protection. There has been a flood of research
into query answering algorithms that meet this standard. Algorithms are
becoming increasingly complex, and in particular, the performance of many
emerging algorithms is {\em data dependent}, meaning the distribution of the
noise added to query answers may change depending on the input data.
Theoretical analysis typically only considers the worst case, making empirical
study of average case performance increasingly important.
  In this paper we propose a set of evaluation principles which we argue are
essential for sound evaluation. Based on these principles we propose DPBench, a
novel evaluation framework for standardized evaluation of privacy algorithms.
We then apply our benchmark to evaluate algorithms for answering 1- and
2-dimensional range queries. The result is a thorough empirical study of 15
published algorithms on a total of 27 datasets that offers new insights into
algorithm behavior---in particular the influence of dataset scale and
shape---and a more complete characterization of the state of the art. Our
methodology is able to resolve inconsistencies in prior empirical studies and
place algorithm performance in context through comparison to simple baselines.
Finally, we pose open research questions which we hope will guide future
algorithm design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04828</identifier>
 <datestamp>2015-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04828</id><created>2015-12-15</created><authors><author><keyname>Nunes</keyname><forenames>Ivan Oliveira</forenames></author><author><keyname>de Melo</keyname><forenames>Pedro O. S. Vaz</forenames></author><author><keyname>Loureiro</keyname><forenames>Antonio A. F.</forenames></author></authors><title>Group Mobility: Detection, Tracking and Characterization</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the era of mobile computing, understanding human mobility patterns is
crucial in order to better design protocols and applications. Many studies
focus on different aspects of human mobility such as people's points of
interests, routes, traffic, individual mobility patterns, among others. In this
work, we propose to look at human mobility through a social perspective, i.e.,
analyze the impact of social groups in mobility patterns. We use the MIT
Reality Mining proximity trace to detect, track and investigate group's
evolution throughout time. Our results show that group meetings happen in a
periodical fashion and present daily and weekly periodicity. We analyze how
groups' dynamics change over day hours and find that group meetings lasting
longer are those with less changes in members composition and with members
having stronger social bonds with each other. Our findings can be used to
propose meeting prediction algorithms, opportunistic routing and information
diffusion protocols, taking advantage of those revealed properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04829</identifier>
 <datestamp>2015-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04829</id><created>2015-12-15</created><authors><author><keyname>Kouw</keyname><forenames>Wouter M.</forenames></author><author><keyname>Krijthe</keyname><forenames>Jesse H.</forenames></author><author><keyname>Loog</keyname><forenames>Marco</forenames></author><author><keyname>van der Maaten</keyname><forenames>Laurens J. P.</forenames></author></authors><title>Feature-Level Domain Adaptation</title><categories>stat.ML cs.LG</categories><comments>34 pages, 10 figures, 14 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Domain adaptation is the supervised learning setting in which the training
and test data originate from different domains: the so-called source and target
domains. In this paper, we propose and study a domain adaption approach, called
feature-level domain adaptation (flda), that models the dependence between two
domains by means of a feature-level transfer distribution. The domain adapted
classifier is trained by minimizing the expected loss under this transfer
distribution. Our empirical evaluation of flda focuses on problems with binary
and count features in which the domain adaptation can be naturally modeled via
a dropout distribution, which allows the final classifier to adapt to the
importance of specific features in the target data. Our experimental evaluation
suggests that under certain conditions, flda converges to the classifier
trained on the target distribution. Experiments with our domain adaptation
approach on several real-world problems show that flda performs on par with
state-of-the-art techniques in domain adaptation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04830</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04830</id><created>2015-12-15</created><updated>2016-02-25</updated><authors><author><keyname>Montalban</keyname><forenames>Jon</forenames></author><author><keyname>Barrueco</keyname><forenames>Jon</forenames></author><author><keyname>Angueira</keyname><forenames>Pablo</forenames></author><author><keyname>Prothero</keyname><forenames>Jerrold D.</forenames></author></authors><title>Extending the Shannon Upper Bound Using Spiral Modulation</title><categories>cs.IT math.IT</categories><comments>18 pages, 4 figures, 18 pages with supplementary informationt The
  paper has been withdrawn by the authors due to fundamental doubts raised by
  reviewers</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Shannon upper bound places a limit on the error-free information
transmission rate (capacity) of a noisy channel. It has stood for over sixty
years, and underlies both theoretical and practical work in the
telecommunications industry. This upper bound arises from the Shannon-Hartley
law, which has two parameters: the available bandwidth and the signal-to-noise
power ratio. However, aside from these explicit parameters, the Shannon-Hartley
law also rests on certain assumptions. One of these is that the channel is
linear: recent work has shown that nonlinear channels are not limited by the
Shannon upper bound. A second assumption, arising from the mathematical tools
used in its proof, is that signals are periodic. Surprisingly, the capacity
limit associated with non-periodic signals has not previously been examined.
Here we show, both theoretically and by construction, that the use of
non-periodic signals, based on complex spirals, allows the Shannon upper bound
to be extended.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04832</identifier>
 <datestamp>2015-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04832</id><created>2015-12-15</created><authors><author><keyname>Kor</keyname><forenames>Liah</forenames><affiliation>GANG, LIAFA</affiliation></author><author><keyname>Korman</keyname><forenames>Amos</forenames><affiliation>GANG, LIAFA</affiliation></author><author><keyname>Peleg</keyname><forenames>David</forenames></author></authors><title>Tight Bounds for Distributed Minimum-Weight Spanning Tree Verification</title><categories>cs.DC</categories><proxy>ccsd</proxy><journal-ref>Theory of Computing Systems, Springer Verlag, 2013,
  \&amp;lt;10.1007/s00224-013-9479-7\&amp;gt;</journal-ref><doi>10.1007/s00224-013-9479-7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces the notion of distributed verification without
preprocessing. It focuses on the Minimum-weight Spanning Tree (MST)
verification problem and establishes tight upper and lower bounds for the time
and message complexities of this problem. Specifically, we provide an MST
verification algorithm that achieves simultaneously O(m) messages and O($\sqrt$
n+D) time, where m is the number of edges in the given graph G, n is the number
of nodes, and D is G's diameter. On the other hand, we show that any MST
verification algorithm must send {\Omega}(m) messages and incur
{\Omega}($\sqrt$ n + D) time in worst case. Our upper bound result appears to
indicate that the verification of an MST may be easier than its construction,
since for MST construction, both lower bounds of {\Omega}(m) messages and
{\Omega}($\sqrt$ n+D time hold, but at the moment there is no known distributed
algorithm that constructs an MST and achieves simultaneously O(m) messages and
O($\sqrt$ n + D) time. Specifically, the best known time-optimal algorithm
(using O($\sqrt$ n + D) time) requires O(m + n 3/2) messages, and the best
known message-optimal algorithm (using O(m) messages) requires O(n) time. On
the other hand, our lower bound results indicate that the verification of an
MST is not significantly easier than its construction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04833</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04833</id><created>2015-12-15</created><updated>2015-12-25</updated><authors><author><keyname>Liu</keyname><forenames>Ting</forenames></author><author><keyname>Wen</keyname><forenames>Chao-Kai</forenames></author><author><keyname>Jin</keyname><forenames>Shi</forenames></author><author><keyname>You</keyname><forenames>Xiaohu</forenames></author></authors><title>Generalized Turbo Signal Recovery for Nonlinear Measurements and
  Orthogonal Sensing Matrices</title><categories>cs.IT math.IT</categories><comments>5 pages, 2 figures, fix some typo</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this study, we propose a generalized turbo signal recovery algorithm to
estimate a signal from quantized measurements, in which the sensing matrix is a
row-orthogonal matrix, such as the partial discrete Fourier transform matrix.
The state evolution of the proposed algorithm is derived and is shown to be
consistent with that obtained with the replica method. Numerical experiments
illustrate the excellent agreement of the proposed algorithm with theoretical
state evolution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04844</identifier>
 <datestamp>2015-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04844</id><created>2015-12-15</created><authors><author><keyname>Cano</keyname><forenames>Julio</forenames></author><author><keyname>Garc&#xed;a-Valls</keyname><forenames>Marisol</forenames></author></authors><title>On the free, safe, and timely execution of component-based systems</title><categories>cs.SE</categories><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Traditional real-time systems are reluctant to integrate dynamic behavior
since it challenges predictability and timeliness. Current efforts are starting
to address the inclusion of a controllable level of dynamicity in real-time
systems to increase the degree of freedom or flexibility in the execution of
real-time systems. This is mainly achieved by imposing a set of bounds and
limitations to the allowed system structure and operations during a transition.
The ultimate goal is to time-bound the duration and result of the system state
transitions. One of the main obstacles for run-time transitions is the
difficulty in characterizing the different operations and phases of a run-time
system transition to guarantee a time bound for each phase. In this paper, an
infrastructure is described to ensure the timely execution of state transitions
that can be safely incorporated and performed at run-time in a component-based
real-time system, preserving its temporal properties. The infrastructure allows
to perform different management operations to modify components at run-time
ensuring the overall schedulability of the system. The infrastructure is
validated by its implementation in a specific component-based framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04848</identifier>
 <datestamp>2015-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04848</id><created>2015-12-15</created><authors><author><keyname>Dick</keyname><forenames>Travis</forenames></author><author><keyname>Li</keyname><forenames>Mu</forenames></author><author><keyname>Pillutla</keyname><forenames>Venkata Krishna</forenames></author><author><keyname>White</keyname><forenames>Colin</forenames></author><author><keyname>Balcan</keyname><forenames>Maria Florina</forenames></author><author><keyname>Smola</keyname><forenames>Alex</forenames></author></authors><title>Data Driven Resource Allocation for Distributed Learning</title><categories>cs.LG cs.DS stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In distributed machine learning, data is dispatched to multiple machines for
processing. Motivated by the fact that similar data points are often belonging
to the same or similar classes, and more generally, classification rules of
high accuracy tend to be &quot;locally simple but globally complex&quot;, we propose data
dependent dispatching that takes advantage of such structures. Our main
technical contribution is to provide algorithms with provable guarantees for
data-dependent dispatching, that partition the data in a way that satisfies
important conditions for accurate distributed learning, including fault
tolerance and balancedness. We show the effectiveness of our method over the
widely used random partitioning scheme in several real world image and
advertising datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04856</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04856</id><created>2015-12-15</created><updated>2015-12-27</updated><authors><author><keyname>Afshani</keyname><forenames>Peyman</forenames></author><author><keyname>Sheehy</keyname><forenames>Donald R.</forenames></author><author><keyname>Stein</keyname><forenames>Yannik</forenames></author></authors><title>Approximating the Simplicial Depth</title><categories>cs.CG</categories><comments>25 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $P$ be a set of $n$ points in $d$-dimensions. The simplicial depth,
$\sigma_P(q)$ of a point $q$ is the number of $d$-simplices with vertices in
$P$ that contain $q$ in their convex hulls. The simplicial depth is a notion of
data depth with many applications in robust statistics and computational
geometry. Computing the simplicial depth of a point is known to be a
challenging problem. The trivial solution requires $O(n^{d+1})$ time whereas it
is generally believed that one cannot do better than $O(n^{d-1})$. In this
paper, we consider approximation algorithms for computing the simplicial depth
of a point. For $d=2$, we present a new data structure that can approximate the
simplicial depth in polylogarithmic time, using polylogarithmic query time. In
3D, we can approximate the simplicial depth of a given point in near-linear
time, which is clearly optimal up to polylogarithmic factors. For higher
dimensions, we consider two approximation algorithms with different worst-case
scenarios. By combining these approaches, we compute a
$(1+\varepsilon)$-approximation of the simplicial depth in time
$\tilde{O}(n^{d/2 + 1})$ ignoring polylogarithmic factor. All of these
algorithms are Monte Carlo algorithms. Furthermore, we present a simple
strategy to compute the simplicial depth exactly in $O(n^d \log n)$ time, which
provides the first improvement over the trivial $O(n^{d+1})$ time algorithm for
$d&gt;4$. Finally, we show that computing the simplicial depth exactly is
#P-complete and W[1]-hard if the dimension is part of the input.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04857</identifier>
 <datestamp>2015-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04857</id><created>2015-12-15</created><authors><author><keyname>Ralinovski</keyname><forenames>Kiril</forenames></author><author><keyname>Goldenbaum</keyname><forenames>Mario</forenames></author><author><keyname>Sta&#x144;czak</keyname><forenames>S&#x142;awomir</forenames></author></authors><title>Energy-Efficient Classification for Anomaly Detection: The Wireless
  Channel as a Helper</title><categories>cs.IT cs.LG math.IT</categories><comments>submitted for possible conference publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Anomaly detection has various applications including condition monitoring and
fault diagnosis. The objective is to sense the environment, learn the normal
system state, and then periodically classify whether the instantaneous state
deviates from the normal one or not. A flexible and cost-effective way of
monitoring a system state is to use a wireless sensor network. In the
traditional approach, the sensors encode their observations and transmit them
to a fusion center by means of some interference avoiding channel access
method. The fusion center then decodes all the data and classifies the
corresponding system state. As this approach can be highly inefficient in terms
of energy consumption, in this paper we propose a transmission scheme that
exploits interference for carrying out the anomaly detection directly in the
air. In other words, the wireless channel helps the fusion center to retrieve
the sought classification outcome immediately from the channel output. To
achieve this, the chosen learning model is linear support vector machines.
After discussing the proposed scheme and proving its reliability, we present
numerical examples demonstrating that the scheme reduces the energy consumption
for anomaly detection by up to 53% compared to a strategy that uses time
division multiple-access.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04860</identifier>
 <datestamp>2015-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04860</id><created>2015-12-15</created><authors><author><keyname>Bellemare</keyname><forenames>Marc G.</forenames></author><author><keyname>Ostrovski</keyname><forenames>Georg</forenames></author><author><keyname>Guez</keyname><forenames>Arthur</forenames></author><author><keyname>Thomas</keyname><forenames>Philip S.</forenames></author><author><keyname>Munos</keyname><forenames>R&#xe9;mi</forenames></author></authors><title>Increasing the Action Gap: New Operators for Reinforcement Learning</title><categories>cs.AI cs.LG</categories><journal-ref>Bellemare, Marc G., Ostrovski, G., Guez, A., Thomas, Philip S.,
  and Munos, Remi. Increasing the Action Gap: New Operators for Reinforcement
  Learning. Proceedings of the AAAI Conference on Artificial Intelligence, 2016</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces new optimality-preserving operators on Q-functions. We
first describe an operator for tabular representations, the consistent Bellman
operator, which incorporates a notion of local policy consistency. We show that
this local consistency leads to an increase in the action gap at each state;
increasing this gap, we argue, mitigates the undesirable effects of
approximation and estimation errors on the induced greedy policies. This
operator can also be applied to discretized continuous space and time problems,
and we provide empirical results evidencing superior performance in this
context. Extending the idea of a locally consistent operator, we then derive
sufficient conditions for an operator to preserve optimality, leading to a
family of operators which includes our consistent Bellman operator. As
corollaries we provide a proof of optimality for Baird's advantage learning
algorithm and derive other gap-increasing operators with interesting
properties. We conclude with an empirical study on 60 Atari 2600 games
illustrating the strong potential of these new operators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04866</identifier>
 <datestamp>2015-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04866</id><created>2015-12-15</created><authors><author><keyname>Bekos</keyname><forenames>Michael A.</forenames></author><author><keyname>Kaufmann</keyname><forenames>Michael</forenames></author><author><keyname>Krug</keyname><forenames>Robert</forenames></author></authors><title>On the Total Number of Bends for Planar Octilinear Drawings</title><categories>cs.CG cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An octilinear drawing of a planar graph is one in which each edge is drawn as
a sequence of horizontal, vertical and diagonal at 45 degrees line-segments.
For such drawings to be readable, special care is needed in order to keep the
number of bends small. As the problem of finding planar octilinear drawings of
minimum number of bends is NP-hard, in this paper we focus on upper and lower
bounds. From a recent result of Keszegh et al. on the slope number of planar
graphs, we can derive an upper bound of 4n-10 bends for 8-planar graphs with n
vertices. We considerably improve this general bound and corresponding previous
ones for triconnected 4-, 5- and 6-planar graphs. We also derive non-trivial
lower bounds for these three classes of graphs by a technique inspired by the
network flow formulation of Tamassia.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04887</identifier>
 <datestamp>2015-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04887</id><created>2015-12-15</created><authors><author><keyname>Philippe</keyname><forenames>Matthew</forenames></author><author><keyname>Millerioux</keyname><forenames>Gilles</forenames></author><author><keyname>Jungers</keyname><forenames>Rapha&#xeb;l M.</forenames></author></authors><title>Deciding the boundedness and dead-beat stability of constrained
  switching systems</title><categories>math.DS cs.SY</categories><comments>Article has been submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study computational questions related with the stability of discrete-time
linear switching systems with switching sequences constrained by an automaton.
We first present a decidable sufficient condition for their boundedness when
the maximal exponential growth rate equals one. The condition generalizes the
notion of the irreducibility of a matrix set, which is a well known sufficient
condition for boundedness in the arbitrary switching (i.e. unconstrained) case.
Second, we provide a polynomial time algorithm for deciding the dead-beat
stability of a system, i.e. that all trajectories vanish to the origin in
finite time. The algorithm generalizes one proposed by Gurvits for arbitrary
switching systems, and is illustrated with a real-world case study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04891</identifier>
 <datestamp>2015-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04891</id><created>2015-12-15</created><updated>2015-12-22</updated><authors><author><keyname>Cao</keyname><forenames>Chao</forenames></author><author><keyname>Wan</keyname><forenames>Weiwei</forenames></author><author><keyname>Pan</keyname><forenames>Jia</forenames></author><author><keyname>Harada</keyname><forenames>Kensuke</forenames></author></authors><title>Analyzing the Utility of a Support Pin in Sequential Robotic
  Manipulation</title><categories>cs.RO</categories><comments>14 pages, 20 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pick-and-place regrasp is an important manipulation skill for a robot. It
helps a robot accomplish tasks that cannot be achieved within a single grasp,
due to constraints such as kinematics or collisions between the robot and the
environment. Previous work on pick-and-place regrasp only leveraged flat
surfaces for intermediate placements, and thus is limited in the capability to
reorient an object.
  In this paper, we extend the reorientation capability of a pick-and-place
regrasp by adding a vertical pin on the working surface and using it as the
intermediate location for regrasping. In particular, our method automatically
computes the stable placements of an object leaning against a vertical pin,
finds several force-closure grasps, generates a graph of regrasp actions, and
searches for the regrasp sequence. To compare the regrasping performance with
and without using pins, we evaluate the success rate and the length of regrasp
sequences while performing tasks on various models. Experiments on
reorientation and assembly tasks validate the benefit of using support pins for
regrasping.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04894</identifier>
 <datestamp>2015-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04894</id><created>2015-12-15</created><updated>2015-12-23</updated><authors><author><keyname>Thramboulidis</keyname><forenames>Kleanthis</forenames></author><author><keyname>Christoulakis</keyname><forenames>Foivos</forenames></author></authors><title>UML4IoT - A UML profile to exploit IoT in cyber-physical manufacturing
  systems</title><categories>cs.SE</categories><comments>11 pages, two column format, 10 figures, 2 Tables, Fig. 5 updated
  with the correct diagram. Improved performance measurements after tuning the
  garbage collector</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Internet of Things is changing the world. The manufacturing industry has
already identified that the IoT brings great opportunities to retain its
leading position in economy and society. However, the adoption of this new
technology changes the development process of the manufacturing system and
raises many challenges. In this paper the modern manufacturing system is
considered as a composition of cyber-physical, cyber and human components and
IoT is used as a glue for their integration as far as it regards their cyber
interfaces. The key idea is a UML profile for the IoT with an alternative to
apply the approach also at the source code level specification of the component
in case that a UML design specification is not available. The proposed
approach, namely UML4IoT, fully automates the generation process of the
IoT-compliant layer that is required for the cyber-physical component to be
integrated in the modern IoT manufacturing environment. A prototype
implementation of the myLiqueur laboratory system has been developed to
demonstrate the applicability and effectiveness of the UML4IoT approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04898</identifier>
 <datestamp>2015-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04898</id><created>2015-12-15</created><updated>2015-12-15</updated><authors><author><keyname>Meiklejohn</keyname><forenames>Christopher</forenames></author></authors><title>Declarative, Secure, Convergent Edge Computation</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Eventual consistency is a more natural model than strong consistency for a
distributed system, since it is closer to the underlying physical reality.
Therefore, we propose that it is important to find a programming model that is
both congenial to developers and supports eventual consistency. In particular,
we consider that a crucial test for such a model is that it should support edge
computation in a both natural and secure way. We present a preliminary work
report with an initial solution, called Lasp, which resembles a concurrent
functional language while naturally supporting an eventually consistent
coordination-free distribution model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04906</identifier>
 <datestamp>2015-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04906</id><created>2015-12-15</created><authors><author><keyname>Chen</keyname><forenames>Welin</forenames></author><author><keyname>Grangier</keyname><forenames>David</forenames></author><author><keyname>Auli</keyname><forenames>Michael</forenames></author></authors><title>Strategies for Training Large Vocabulary Neural Language Models</title><categories>cs.CL cs.LG</categories><comments>12 pages; journal paper; under review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Training neural network language models over large vocabularies is still
computationally very costly compared to count-based models such as Kneser-Ney.
At the same time, neural language models are gaining popularity for many
applications such as speech recognition and machine translation whose success
depends on scalability. We present a systematic comparison of strategies to
represent and train large vocabularies, including softmax, hierarchical
softmax, target sampling, noise contrastive estimation and self normalization.
We further extend self normalization to be a proper estimator of likelihood and
introduce an efficient variant of softmax. We evaluate each method on three
popular benchmarks, examining performance on rare words, the speed/accuracy
trade-off and complementarity to Kneser-Ney.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04912</identifier>
 <datestamp>2015-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04912</id><created>2015-12-15</created><authors><author><keyname>Kooti</keyname><forenames>Farshad</forenames></author><author><keyname>Lerman</keyname><forenames>Kristina</forenames></author><author><keyname>Aiello</keyname><forenames>Luca Maria</forenames></author><author><keyname>Grbovic</keyname><forenames>Mihajlo</forenames></author><author><keyname>Djuric</keyname><forenames>Nemanja</forenames></author><author><keyname>Radosavljevic</keyname><forenames>Vladan</forenames></author></authors><title>Portrait of an Online Shopper: Understanding and Predicting Consumer
  Behavior</title><categories>cs.SI cs.CY</categories><comments>10 pages, 11 figures, Proceedings of the 9th ACM International
  Conference on Web Search and Data Mining (WSDM 2016), San Fransisco, USA</comments><journal-ref>Proceedings of the 9th ACM International Conference on Web Search
  and Data Mining (WSDM 2016), San Fransisco, USA,</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consumer spending accounts for a large fraction of the US economic activity.
Increasingly, consumer activity is moving to the web, where digital traces of
shopping and purchases provide valuable data about consumer behavior. We
analyze these data extracted from emails and combine them with demographic
information to characterize, model, and predict consumer behavior. Breaking
down purchasing by age and gender, we find that the amount of money spent on
online purchases grows sharply with age, peaking in late 30s. Men are more
frequent online purchasers and spend more money when compared to women. Linking
online shopping to income, we find that shoppers from more affluent areas
purchase more expensive items and buy them more frequently, resulting in
significantly more money spent on online purchases. We also look at dynamics of
purchasing behavior and observe daily and weekly cycles in purchasing behavior,
similarly to other online activities.
  More specifically, we observe temporal patterns in purchasing behavior
suggesting shoppers have finite budgets: the more expensive an item, the longer
the shopper waits since the last purchase to buy it. We also observe that
shoppers who email each other purchase more similar items than socially
unconnected shoppers, and this effect is particularly evident among women.
Finally, we build a model to predict when shoppers will make a purchase and how
much will spend on it. We find that temporal features improve prediction
accuracy over competitive baselines. A better understanding of consumer
behavior can help improve marketing efforts and make online shopping more
pleasant and efficient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04927</identifier>
 <datestamp>2015-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04927</id><created>2015-12-15</created><authors><author><keyname>Liu</keyname><forenames>Wei</forenames></author><author><keyname>Sun</keyname><forenames>Ruoyu</forenames></author><author><keyname>Luo</keyname><forenames>Zhi-Quan</forenames></author><author><keyname>Li</keyname><forenames>Jiandong</forenames></author></authors><title>Globally Optimal Joint Uplink Base Station Association and Beamforming</title><categories>cs.IT math.IT</categories><comments>21 pages, 4 figures. Submitted to IEEE TSP. Part of the paper has
  appeared at ICASSP 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The joint base station (BS) association and beamforming problem has been
studied extensively in recent years, yet the computational complexity for even
the simplest SISO case has not been fully characterized. In this paper, we
consider the problems for an uplink SISO/SIMO cellular network under the
max-min fairness criterion. We first prove that the problems for both the SISO
and SIMO scenarios are polynomial time solvable. Secondly, we present a fixed
point based binary search (BS-FP) algorithm for both SISO and SIMO scenarios
whereby a QoS (Quality of Service) constrained subproblem is solved at each
step by a fixed point method. Thirdly, we propose a normalized fixed point
(NFP) iterative algorithm to directly solve the original problem and prove its
geometric convergence to global optima. Although it is not known whether the
NFP algorithm is a polynomial time algorithm, empirically it converges to the
global optima orders of magnitude faster than the polynomial time algorithms,
making it suitable for applications in huge-scale networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04932</identifier>
 <datestamp>2016-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04932</id><created>2015-12-15</created><updated>2016-02-23</updated><authors><author><keyname>Braun</keyname><forenames>G&#xe1;bor</forenames></author><author><keyname>Pokutta</keyname><forenames>Sebastian</forenames></author><author><keyname>Roy</keyname><forenames>Aurko</forenames></author></authors><title>Strong reductions for extended formulations</title><categories>cs.CC</categories><comments>correct approx factor for sparsest cut, typos, update references</comments><msc-class>68Q17, 90C05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We generalize the reduction mechanism for linear programming problems and
semidefinite programming problems from [arXiv:1410.8816] in two ways 1)
relaxing the requirement of affineness and 2) extending to fractional
optimization problems. As applications we provide several new LP-hardness and
SDP-hardness results, e.g., for the SparsestCut problem, the BalancedSeparator
problem, the MaxCut problem and the Matching problem on 3-regular graphs. We
also provide a new, very strong Lasserre integrality gap result for the
IndependentSet problem, which is strictly greater than the best known LP
approximation, showing that the Lasserre hierarchy does not always provide the
tightest SDP relaxation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04935</identifier>
 <datestamp>2016-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04935</id><created>2015-12-15</created><authors><author><keyname>Zhou</keyname><forenames>Sheng</forenames></author><author><keyname>Zhao</keyname><forenames>Tao</forenames></author><author><keyname>Niu</keyname><forenames>Zhisheng</forenames></author><author><keyname>Zhou</keyname><forenames>Shidong</forenames></author></authors><title>Software-Defined Hyper-Cellular Architecture for Green and Elastic
  Wireless Access</title><categories>cs.NI</categories><comments>Accepted by IEEE Communications Magazine</comments><doi>10.1109/MCOM.2016.7378420</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To meet the surging demand of increasing mobile Internet traffic from diverse
applications while maintaining moderate energy cost, the radio access network
(RAN) of cellular systems needs to take a green path into the future, and the
key lies in providing elastic service to dynamic traffic demands. To achieve
this, it is time to rethink RAN architectures and expect breakthroughs. In this
article, we review the state-of-art literature which aims to renovate RANs from
the perspectives of control-traffic decoupled air interface, cloud-based RANs,
and software-defined RANs. We then propose a software-defined hyper-cellular
architecture (SDHCA) that identifies a feasible way of integrating the above
three trends to enable green and elastic wireless access. We further present
key enabling technologies to realize SDHCA, including separation of the air
interface, green base station operations, and base station functions
virtualization, followed by our hardware testbed for SDHCA. Besides, we
summarize several future research issues worth investigating.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04958</identifier>
 <datestamp>2015-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04958</id><created>2015-12-15</created><authors><author><keyname>Hussein</keyname><forenames>Sarfaraz</forenames></author><author><keyname>Green</keyname><forenames>Aileen</forenames></author><author><keyname>Watane</keyname><forenames>Arjun</forenames></author><author><keyname>Papadakis</keyname><forenames>Georgios</forenames></author><author><keyname>Osman</keyname><forenames>Medhat</forenames></author><author><keyname>Bagci</keyname><forenames>Ulas</forenames></author></authors><title>Context Driven Label Fusion for segmentation of Subcutaneous and
  Visceral Fat in CT Volumes</title><categories>cs.CV</categories><comments>ISBI 2016 submission, 5 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantification of adipose tissue (fat) from computed tomography (CT) scans is
conducted mostly through manual or semi-automated image segmentation algorithms
with limited efficacy. In this work, we propose a completely unsupervised and
automatic method to identify adipose tissue, and then separate Subcutaneous
Adipose Tissue (SAT) from Visceral Adipose Tissue (VAT) at the abdominal
region. We offer a three-phase pipeline consisting of (1) Initial boundary
estimation using gradient points, (2) boundary refinement using Geometric
Median Absolute Deviation and Appearance based Local Outlier Scores (3) Context
driven label fusion using Conditional Random Fields (CRF) to obtain the final
boundary between SAT and VAT. We evaluate the proposed method on 151 abdominal
CT scans and obtain state-of-the-art 94% and 91% dice similarity scores for SAT
and VAT segmentation, as well as significant reduction in fat quantification
error measure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04960</identifier>
 <datestamp>2015-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04960</id><created>2015-12-15</created><authors><author><keyname>Cotter</keyname><forenames>Andrew</forenames></author><author><keyname>Gupta</keyname><forenames>Maya</forenames></author><author><keyname>Pfeifer</keyname><forenames>Jan</forenames></author></authors><title>A Light Touch for Heavily Constrained SGD</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Projected stochastic gradient descent (SGD) is often the default choice for
large-scale optimization in machine learning, but requires a projection after
each update. For heavily-constrained objectives, we propose an efficient
extension of SGD that stays close to the feasible region while only applying
constraints probabilistically at each iteration. Theoretical analysis shows a
good trade-off between per-iteration work and the number of iterations needed,
indicating compelling advantages on problems with a large number of constraints
onto which projecting is expensive. In MATLAB experiments, our algorithm
successfully handles a large-scale real-world video ranking problem with tens
of thousands of linear inequality constraints that was too large for projected
SGD and stochastic Frank-Wolfe.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04964</identifier>
 <datestamp>2015-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04964</id><created>2015-12-15</created><authors><author><keyname>Kanovich</keyname><forenames>Max</forenames></author></authors><title>Horn Linear Logic and Minsky Machines</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Here we give a detailed proof for the crucial point in our Minsky machine
simulation - that any linear logic derivation for a specific Horn sequent can
be transformed into a Minsky computation leading from an initial configuration
to the halting configuration.
  Among other things, the presentation advantage of the 3-step program is that
the non-trivial tricky points are distributed between the independent parts
each of which we justify following its own intrinsic methodology (to say
nothing of the induction used in the opposite directions):
  (1) From LL to HLL - we use purely proof-theoretic arguments.
  (2) From HLL to Horn programs - we translate trees (HLL derivations) into
another trees (Horn programs)of the same shape, almost.
  (3) From Horn programs to Minsky computations - we use purely computational
arguments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04965</identifier>
 <datestamp>2015-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04965</id><created>2015-12-15</created><authors><author><keyname>Grassl</keyname><forenames>Markus</forenames></author><author><keyname>Langenberg</keyname><forenames>Brandon</forenames></author><author><keyname>Roetteler</keyname><forenames>Martin</forenames></author><author><keyname>Steinwandt</keyname><forenames>Rainer</forenames></author></authors><title>Applying Grover's algorithm to AES: quantum resource estimates</title><categories>quant-ph cs.CR cs.ET</categories><comments>13 pages, 3 figures, 5 tables; to appear in: Proceedings of the 7th
  International Conference on Post-Quantum Cryptography (PQCrypto 2016)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present quantum circuits to implement an exhaustive key search for the
Advanced Encryption Standard (AES) and analyze the quantum resources required
to carry out such an attack. We consider the overall circuit size, the number
of qubits, and the circuit depth as measures for the cost of the presented
quantum algorithms. Throughout, we focus on Clifford$+T$ gates as the
underlying fault-tolerant logical quantum gate set. In particular, for all
three variants of AES (key size 128, 192, and 256 bit) that are standardized in
FIPS-PUB 197, we establish precise bounds for the number of qubits and the
number of elementary logical quantum gates that are needed to implement
Grover's quantum algorithm to extract the key from a small number of AES
plaintext-ciphertext pairs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04973</identifier>
 <datestamp>2015-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04973</id><created>2015-12-15</created><authors><author><keyname>Nakashole</keyname><forenames>Ndapandula</forenames></author></authors><title>An Operator for Entity Extraction in MapReduce</title><categories>cs.DB cs.CL</categories><comments>7 pages</comments><msc-class>68T50</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dictionary-based entity extraction involves finding mentions of dictionary
entities in text. Text mentions are often noisy, containing spurious or missing
words. Efficient algorithms for detecting approximate entity mentions follow
one of two general techniques. The first approach is to build an index on the
entities and perform index lookups of document substrings. The second approach
recognizes that the number of substrings generated from documents can explode
to large numbers, to get around this, they use a filter to prune many such
substrings which do not match any dictionary entity and then only verify the
remaining substrings if they are entity mentions of dictionary entities, by
means of a text join. The choice between the index-based approach and the
filter &amp; verification-based approach is a case-to-case decision as the best
approach depends on the characteristics of the input entity dictionary, for
example frequency of entity mentions. Choosing the right approach for the
setting can make a substantial difference in execution time. Making this choice
is however non-trivial as there are parameters within each of the approaches
that make the space of possible approaches very large. In this paper, we
present a cost-based operator for making the choice among execution plans for
entity extraction. Since we need to deal with large dictionaries and even
larger large datasets, our operator is developed for implementations of
MapReduce distributed algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04975</identifier>
 <datestamp>2015-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04975</id><created>2015-11-22</created><authors><author><keyname>Zamanipour</keyname><forenames>Makan</forenames></author><author><keyname>Mohammadi</keyname><forenames>Mohammadali</forenames></author></authors><title>MIMO CDMA-based Optical SATCOMs: A New Solution</title><categories>cs.NI cs.IT math.IT</categories><comments>IEEE PCITC 2015 (15-17 Oct, India)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new scheme for MIMO CDMA-based optical satellite communications (OSATCOMs)
is presented. Three independent problems are described for up-link and down-
link in terms of two distinguished optimization problems. At first, in up-link,
Pulse-width optimization is proposed to reduce dispersions over fibers as the
terrestrial part. This is performed for return-to-zero (RZ) modulation that is
supposed to be used as an example in here. This is carried out by solving the
first optimization problem, while minimizing the probability of overlapping for
the Gaussian pulses that are used to produce RZ. Some constraints are assumed
such as a threshold for the peak-to-average power ratio (PAPR). In down-link,
the second and the third problems are discussed as follows, jointly as a
closed-form solution. Solving the second optimization problem, an objective
function is obtained, namely the MIMO CDMA-based satellite weight-matrix as a
conventional adaptive beam-former. The Satellite link is stablished over flat
un-correlated Nakagami-m/Suzuki fading channels as the second problem. On the
other hand, the mentioned optimization problem is robustly solved as the third
important problem, while considering inter-cell interferences in the multi-cell
scenario. Robust solution is performed due to the partial knowledge of each
cell from the others in which the link capacity is maximized. Analytical
results are conducted to investigate the merit of system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04976</identifier>
 <datestamp>2015-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04976</id><created>2015-12-15</created><authors><author><keyname>Krasuski</keyname><forenames>Adam</forenames></author></authors><title>Conditions for Normative Decision Making at the Fire Ground</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss the changes in an attitude to decision making at the fire ground.
The changes are driven by the recent technological shift. The emerging new
approaches in sensing and data processing (under common umbrella of
Cyber-Physical Systems) allow for leveling off the gap, between humans and
machines, in perception of the fire ground. Furthermore, results from
descriptive decision theory question the rationality of human choices. This
creates the need for searching and testing new approaches for decision making
during emergency. We propose the framework that addresses this need. The
primary feature of the framework are possibilities for incorporation of
normative and prescriptive approaches to decision making. The framework also
allows for comparison of the performance of decisions, between human and
machine.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04987</identifier>
 <datestamp>2015-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04987</id><created>2015-12-15</created><authors><author><keyname>Chen</keyname><forenames>Tianran</forenames></author><author><keyname>Mehta</keyname><forenames>Dhagash</forenames></author></authors><title>On the Network Topology Dependent Solution Count of the Algebraic Load
  Flow Equations</title><categories>math.OC cs.SY</categories><comments>8 figures, 18 pages</comments><report-no>ADP-15-50/T952</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A large amount of research activity in power systems areas has focused on
developing computational methods to solve load flow equations where a key
question is the maximum number of isolated solutions.Though several concrete
upper bounds exist, recent studies have hinted that much sharper upper bounds
that depend the topology of underlying power networks may exist. This paper
establishes such a topology dependent solution bound which is actually the best
possible bound in the sense that it is always attainable. We also develop a
geometric construction called adjacency polytope which accurately captures the
topology of the underlying power network and is immensely useful in the
computation of the solution bound. Finally we highlight the significant
implications of the development of such solution bound in solving load flow
equations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.04999</identifier>
 <datestamp>2015-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.04999</id><created>2015-12-15</created><authors><author><keyname>Cunhua</keyname><forenames>Pan</forenames></author><author><keyname>Wei</keyname><forenames>Xu</forenames></author><author><keyname>Jiangzhou</keyname><forenames>Wang</forenames></author><author><keyname>Hong</keyname><forenames>Ren</forenames></author><author><keyname>Wence</keyname><forenames>Zhang</forenames></author><author><keyname>Nuo</keyname><forenames>Huang</forenames></author><author><keyname>Ming</keyname><forenames>Chen</forenames></author></authors><title>Pricing-based Distributed Energy-Efficient Beamforming for MISO
  Interference Channels</title><categories>cs.IT math.IT</categories><comments>40 pages</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  In this paper, we consider the problem of maximizing the weighted sum energy
efficiency (WS-EE) for multi-input single-output (MISO) interference channels
(ICs) which is well acknowledged as general models of heterogeneous networks
(HetNets), multicell networks, etc. To address this problem, we develop an
efficient distributed beamforming algorithm based on a pricing mechanism.
Specifically, we carefully introduce a price metric for distributed beamforming
design which fortunately allows efficient closed-form solutions to the per-user
beam-vector optimization problem. The convergence of the distributed
pricing-based beamforming design is theoretically proven. Furthermore, we
present an implementation strategy of the proposed distributed algorithm with
limited information exchange. Numerical results show that our algorithm
converges much faster than existing algorithms, while yielding comparable,
sometimes even better performance in terms of the WS-EE. Finally, by taking the
backhaul power consumption into account, it is interesting to show that the
proposed algorithm with limited information exchange achieves better WS-EE than
the full information exchange based algorithm in some special cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05002</identifier>
 <datestamp>2015-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05002</id><created>2015-12-15</created><authors><author><keyname>Gaumont</keyname><forenames>No&#xe9;</forenames></author><author><keyname>Viard</keyname><forenames>Tiphaine</forenames></author><author><keyname>Fournier-S'niehotta</keyname><forenames>Rapha&#xeb;l</forenames></author><author><keyname>Wang</keyname><forenames>Qinna</forenames></author><author><keyname>Latapy</keyname><forenames>Matthieu</forenames></author></authors><title>Analysis of the temporal and structural features of threads in a
  mailing-list</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A link stream is a collection of triplets $(t,u,v)$ indicating that an
interaction occurred between $u$ and $v$ at time $t$. Link streams model many
real-world situations like email exchanges between individuals, connections
between devices, and others. Much work is currently devoted to the
generalization of classical graph and network concepts to link streams. In this
paper, we generalize the existing notions of intra-community density and
inter-community density. We focus on emails exchanges in the Debian
mailing-list, and show that threads of emails, like communities in graphs, are
dense subsets loosely connected from a link stream perspective.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05004</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05004</id><created>2015-12-15</created><updated>2016-02-03</updated><authors><author><keyname>Murdock</keyname><forenames>Jaimie</forenames></author><author><keyname>Zeng</keyname><forenames>Jiaan</forenames></author><author><keyname>Allen</keyname><forenames>Colin</forenames></author></authors><title>Towards Cultural-Scale Models of Full Text</title><categories>cs.DL cs.CL cs.IR</categories><comments>HTRC ACS Technical Report; 10 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This technical report consists of two components: an administrative report
for the HathiTrust Research Center (HTRC) Advanced Collaborative Support (ACS)
program and a research report on the variance of topic models trained over
random samples of books in the Hathi Trust. Cultural-scale models of full text
documents are prone to over-interpretation by researchers making
unintentionally strong socio-linguistic claims without recognizing that even
large digital libraries are merely samples of all the books ever produced. In
this study, we test the sensitivity of the topic models to the sampling process
by taking random samples of books in the Hathi Trust Digital Library within
different Library of Congress Classification (LCC) areas. For each
classification area, we train several topic models over the entire class with
different random seeds, generating a set of spanning models. Then, we train
topic models on random samples of books from the classification area,
generating a set of sample models. Finally, we align topics from the sample
models to the spanning models and measure the alignment distance and topic
overlap. We find that sample models with a large sample size typically have an
alignment distance that falls in the range of the alignment distance between
spanning models. Unsurprisingly, as sample size increases, alignment distance
decreases. We also find that the topic overlap increases as sample size
increases. However, the decomposition of these measures by sample size differs
by field and by number of topics. We speculate that these measures could be
used to find classes which have a common &quot;canon&quot; discussed among all books in
the area, as shown by high topic overlap and low alignment distance even in
small sample sizes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05006</identifier>
 <datestamp>2015-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05006</id><created>2015-12-15</created><authors><author><keyname>Mansinghka</keyname><forenames>Vikash</forenames></author><author><keyname>Tibbetts</keyname><forenames>Richard</forenames></author><author><keyname>Baxter</keyname><forenames>Jay</forenames></author><author><keyname>Shafto</keyname><forenames>Pat</forenames></author><author><keyname>Eaves</keyname><forenames>Baxter</forenames></author></authors><title>BayesDB: A probabilistic programming system for querying the probable
  implications of data</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Is it possible to make statistical inference broadly accessible to
non-statisticians without sacrificing mathematical rigor or inference quality?
This paper describes BayesDB, a probabilistic programming platform that aims to
enable users to query the probable implications of their data as directly as
SQL databases enable them to query the data itself. This paper focuses on four
aspects of BayesDB: (i) BQL, an SQL-like query language for Bayesian data
analysis, that answers queries by averaging over an implicit space of
probabilistic models; (ii) techniques for implementing BQL using a broad class
of multivariate probabilistic models; (iii) a semi-parametric Bayesian
model-builder that auomatically builds ensembles of factorial mixture models to
serve as baselines; and (iv) MML, a &quot;meta-modeling&quot; language for imposing
qualitative constraints on the model-builder and combining baseline models with
custom algorithmic and statistical models that can be implemented in external
software. BayesDB is illustrated using three applications: cleaning and
exploring a public database of Earth satellites; assessing the evidence for
temporal dependence between macroeconomic indicators; and analyzing a salary
survey.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05008</identifier>
 <datestamp>2015-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05008</id><created>2015-12-15</created><authors><author><keyname>Hao</keyname><forenames>Yingshuai</forenames></author><author><keyname>Wang</keyname><forenames>Meng</forenames></author><author><keyname>Chow</keyname><forenames>Joe</forenames></author></authors><title>Likelihood of Cyber Data Injection Attacks to Power Systems</title><categories>cs.SY</categories><comments>To appear in the proceeding of IEEE GlobalSIP 2015. 4 pages plus the
  5th page for references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cyber data attacks are the worst-case interacting bad data to power system
state estimation and cannot be detected by existing bad data detectors. In this
paper, we for the first time analyze the likelihood of cyber data attacks by
characterizing the actions of a malicious intruder. We propose to use Markov
decision process to model an intruder's strategy, where the objective is to
maximize the cumulative reward across time. Linear programming method is
employed to find the optimal attack policy from the intruder's perspective.
Numerical experiments are conducted to study the intruder's attack strategy in
test power systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05010</identifier>
 <datestamp>2015-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05010</id><created>2015-12-15</created><authors><author><keyname>Kirov</keyname><forenames>Slav</forenames></author><author><keyname>Slep&#x10d;ev</keyname><forenames>Dejan</forenames></author></authors><title>Multiple penalized principal curves: analysis and computation</title><categories>math.AP cs.CV stat.ML</categories><msc-class>49M25, 65D10, 62G99, 65D18, 65K10, 49Q20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of finding the one-dimensional structure in a given data
set. In other words we consider ways to approximate a given measure (data) by
curves. We consider an objective functional whose minimizers are a
regularization of principal curves and introduce a new functional which allows
for multiple curves. We prove the existence of minimizers and establish their
basic properties. We develop an efficient algorithm for obtaining (near)
minimizers of the functional. While both of the functionals used are nonconvex,
we argue that enlarging the configuration space to allow for multiple curves
leads to a simpler energy landscape with fewer undesirable (high-energy) local
minima. Furthermore we note that the approach proposed is able to find the
one-dimensional structure even for data with considerable amount of noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05013</identifier>
 <datestamp>2015-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05013</id><created>2015-12-15</created><authors><author><keyname>Schleussner</keyname><forenames>Carl-Friedrich</forenames></author><author><keyname>Donges</keyname><forenames>Jonathan F.</forenames></author><author><keyname>Engemann</keyname><forenames>Denis A.</forenames></author><author><keyname>Levermann</keyname><forenames>Anders</forenames></author></authors><title>Co-evolutionary behaviour selection in adaptive social networks predicts
  clustered marginalization of minorities</title><categories>physics.soc-ph cs.SI nlin.AO</categories><comments>15 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Human behaviour is largely shaped by local social interactions and depends on
the structure of connections between individuals in social networks. These two
dimensions of behaviour selection are commonly studied in isolation by
different disciplines and are often treated as independent processes. To the
contrary, empirical findings on spread of behaviour in social networks suggest
that local interactions between individuals and network evolution are
interdependent. Empirical evidence, however, remains inconclusive as social
network studies often suffer from limited sample sizes or are prohibitive on
ethical grounds. Here we introduce a co-evolutionary adaptive network model of
social behaviour selection that provides insights into generative mechanisms by
resolving both these aspects through computer simulations. We considered four
complementary models and evaluated them with regard to emulating empirical
behaviour dynamics in social networks. For this purpose we modelled the
prevalence of smoking and and the network structure in response to changing
societal and normative support for smoking. Our simulations corroborate
empirical findings: we found a reduced prevalence of smoking. Remaining smokers
were suffering from reduced influence in the network and were preferentially
connected to other smokers. The analysis of partial models suggests that the
feedback loop between local social interactions and network structure is
indispensable for capturing empirically observed patterns of behavioural
change. Importantly, these patterns have also been reported for other examples
than smoking, i.e., spread of obesity, happiness and social exclusion. Our
results thus suggest a generative mechanism for the co-evolution of individual
behaviour and social network structure. We conclude by discussing implications
for sustainability transitions, medicine and neuroscience.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05015</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05015</id><created>2015-12-15</created><updated>2016-01-25</updated><authors><author><keyname>Miller</keyname><forenames>Christopher W.</forenames></author><author><keyname>Yang</keyname><forenames>Insoon</forenames></author></authors><title>Optimal Control of Conditional Value-at-Risk in Continuous Time</title><categories>math.OC cs.SY q-fin.PM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider continuous-time stochastic optimal control problems featuring
Conditional Value-at-Risk (CVaR) in the objective. The major difficulty in
these problems arises from time-inconsistency, which prevents us from directly
using dynamic programming. To resolve this challenge, we convert to an
equivalent bilevel optimization problem in which the inner optimization problem
is standard stochastic control. Furthermore, we provide conditions under which
the outer objective function is convex and differentiable. We compute the outer
objective's value via a Hamilton-Jacobi-Bellman equation and its gradient via
the viscosity solution of a linear parabolic equation, which allows us to
perform gradient descent. The significance of this result is that we provide an
efficient dynamic programming-based algorithm for optimal control of CVaR
without lifting the state-space. To broaden the applicability of the proposed
algorithm, we provide convergent approximation schemes in cases where our key
assumptions do not hold and characterize relevant suboptimality bounds. In
addition, we extend our method to a more general class of risk metrics, which
includes mean-variance and median-deviation. We also demonstrate a concrete
application to portfolio optimization under CVaR constraints. Our results
contribute an efficient framework for solving time-inconsistent CVaR-based
dynamic optimization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05020</identifier>
 <datestamp>2015-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05020</id><created>2015-12-15</created><updated>2015-12-17</updated><authors><author><keyname>Jara-Figueroa</keyname><forenames>C.</forenames></author><author><keyname>Yu</keyname><forenames>Amy Z.</forenames></author><author><keyname>Hidalgo</keyname><forenames>Cesar A.</forenames></author></authors><title>Estimating technological breaks in the size and composition of human
  collective memory from biographical data</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ability of humans to accumulate knowledge and information across
generations is a defining feature of our species. This ability depends on
factors that range from the psychological biases that predispose us to learn
from skillful, accomplished, and prestigious people, to the development of
technologies for recording and communicating information: from clay tablets to
the Internet. In this paper we present empirical evidence documenting how
communication technologies have shaped human collective memory. We show that
changes in communication technologies, including the introduction of printing
and the maturity of shorter forms of printed media, such as newspapers,
journals, and pamphlets, were accompanied by sharp changes (or breaks) in the
per-capita number of memorable biographies from a time period that are present
in current online and offline sources. Moreover, we find that changes in
technology, such as the introduction of printing, film, radio, and television,
coincide with sharp shifts in the occupations of the individuals present in
these biographical records. These two empirical facts provide evidence in
support of theories arguing that human collective memory is shaped by the
technologies we use to record and communicate information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05023</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05023</id><created>2015-12-15</created><updated>2016-01-29</updated><authors><author><keyname>Sivaraman</keyname><forenames>Anirudh</forenames></author><author><keyname>Budiu</keyname><forenames>Mihai</forenames></author><author><keyname>Cheung</keyname><forenames>Alvin</forenames></author><author><keyname>Kim</keyname><forenames>Changhoon</forenames></author><author><keyname>Licking</keyname><forenames>Steve</forenames></author><author><keyname>Varghese</keyname><forenames>George</forenames></author><author><keyname>Balakrishnan</keyname><forenames>Hari</forenames></author><author><keyname>Alizadeh</keyname><forenames>Mohammad</forenames></author><author><keyname>McKeown</keyname><forenames>Nick</forenames></author></authors><title>Packet Transactions: High-level Programming for Line-Rate Switches</title><categories>cs.NI</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many algorithms for congestion control, scheduling, network measurement,
active queue management, security, and load balancing require custom processing
of packets as they traverse the data plane of a network switch. To run at line
rate, these data-plane algorithms must be in hardware. With today's switch
hardware, algorithms cannot be changed, nor new algorithms installed, after a
switch has been built.
  This paper shows how to program data-plane algorithms in a high-level
language and compile those programs into low-level microcode that can run on
emerging programmable line-rate switching chipsets. The key challenge is that
these algorithms create and modify algorithmic state. The key idea to achieve
line-rate programmability for stateful algorithms is the notion of a packet
transaction : a sequential code block that is atomic and isolated from other
such code blocks. We have developed this idea in Domino, a C-like imperative
language to express data-plane algorithms. We show with many examples that
Domino provides a convenient and natural way to express sophisticated
data-plane algorithms, and show that these algorithms can be run at line rate
with modest estimated die-area overhead.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05027</identifier>
 <datestamp>2015-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05027</id><created>2015-12-15</created><authors><author><keyname>Feng</keyname><forenames>Yuan</forenames></author><author><keyname>Song</keyname><forenames>Lei</forenames></author><author><keyname>Zhang</keyname><forenames>Lijun</forenames></author></authors><title>Distribution-based Bisimulation and Bisimulation Metric in Probabilistic
  Automata</title><categories>cs.FL</categories><comments>27 pages, 8 figures. arXiv admin note: substantial text overlap with
  arXiv:1311.3396</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Probabilistic automata were introduced by Rabin in 1963 as language
acceptors. Two automata are equivalent if and only if they accept each word
with the same probability. On the other side, in the process algebra community,
probabilistic automata were re-proposed by Segala in 1995 which are more
general than Rabin's automata. Bisimulations have been proposed for Segala's
automata to characterize the equivalence between them. So far the two notions
of equivalences and their characteristics have been studied mostly
independently. In this paper, we consider Segala's automata, and propose a
novel notion of distribution-based bisimulation by joining the existing
equivalence and bisimilarities. We demonstrate the utility of our definition by
studying distribution-based bisimulation metrics, which gives rise to a robust
notion of equivalence for Rabin's automata. We compare our notions of
bisimulation to some existing distribution-based bisimulations and discuss
their compositionality and relations to trace equivalence. Finally, we show the
decidability and complexity of all relations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05028</identifier>
 <datestamp>2015-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05028</id><created>2015-12-15</created><authors><author><keyname>Belazzougui</keyname><forenames>Djamal</forenames></author></authors><title>Optimal Las Vegas reduction from one-way set reconciliation to error
  correction</title><categories>cs.DS</categories><comments>14 pages. Under submission to a journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Suppose we have two players $A$ and $C$, where player $A$ has a string
$s[0..u-1]$ and player $C$ has a string $t[0..u-1]$ and none of the two players
knows the other's string. Assume that $s$ and $t$ are both over an integer
alphabet $[\sigma]$, where the first string contains $n$ non-zero entries. We
would wish to answer to the following basic question. Assuming that $s$ and $t$
differ in at most $k$ positions, how many bits does player $A$ need to send to
player $C$ so that he can recover $s$ with certainty? Further, how much time
does player $A$ need to spend to compute the sent bits and how much time does
player $C$ need to recover the string $s$?
  This problem has a certain number of applications, for example in databases,
where each of the two parties possesses a set of $n$ key-value pairs, where
keys are from the universe $[u]$ and values are from $[\sigma]$ and usually
$n\ll u$. In this paper, we show a time and message-size optimal Las Vegas
reduction from this problem to the problem of systematic error correction of
$k$ errors for strings of length $\Theta(n)$ over an alphabet of size
$2^{\Theta(\log\sigma+\log (u/n))}$. The additional running time incurred by
the reduction is linear randomized for player $A$ and linear deterministic for
player $B$, but the correction works with certainty. When using the popular
Reed-Solomon codes, the reduction gives a protocol that transmits $O(k(\log
u+\log\sigma))$ bits and runs in time $O(n\cdot\mathrm{polylog}(n)(\log
u+\log\sigma))$ for all values of $k$. The time is randomized for player $A$
(encoding time) and deterministic for player $C$ (decoding time). The space is
optimal whenever $k\leq (u\sigma)^{1-\Omega(1)}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05030</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05030</id><created>2015-12-15</created><updated>2016-01-23</updated><authors><author><keyname>Faruqui</keyname><forenames>Manaal</forenames></author><author><keyname>McDonald</keyname><forenames>Ryan</forenames></author><author><keyname>Soricut</keyname><forenames>Radu</forenames></author></authors><title>Morpho-syntactic Lexicon Generation Using Graph-based Semi-supervised
  Learning</title><categories>cs.CL</categories><comments>Transactions of the Association for Computational Linguistics (TACL)
  2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Morpho-syntactic lexicons provide information about the morphological and
syntactic roles of words in a language. Such lexicons are not available for all
languages and even when available, their coverage can be limited. We present a
graph-based semi-supervised learning method that uses the morphological,
syntactic and semantic relations between words to automatically construct wide
coverage lexicons from small seed sets. Our method is language-independent, and
we show that we can expand a 1000 word seed lexicon to more than 100 times its
size with high quality for 11 languages. In addition, the automatically created
lexicons provide features that improve performance in two downstream tasks:
morphological tagging and dependency parsing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05031</identifier>
 <datestamp>2015-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05031</id><created>2015-12-15</created><authors><author><keyname>Yu</keyname><forenames>Yi</forenames></author><author><keyname>Zhao</keyname><forenames>Haiquan</forenames></author><author><keyname>Chen</keyname><forenames>Badong</forenames></author></authors><title>Set-membership versions of improved normalized subband adaptive filter
  algorithm for highly noisy system</title><categories>cs.SY</categories><comments>14 pages,8 figures,2 tables,submitted to Journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to improve the performances of the recently-presented improved
normalized subband adaptive filter (INSAF) algorithm for highly noisy system,
this paper proposes a set-membership version of the INSAF algorithm (SM-INSAF)
by exploiting the concept of the set-membership filtering. Apart from obtaining
lower steady-state error under the same convergence rate, the proposed
algorithm significantly reduces the overall computational complexity. In
addition, to further reduce the steady-state error of the SM-INSAF, its smooth
variant is developed by using smooth subband output errors to update the step
sizes, called the SSM-INSAF algorithm. Simulation results in low
signal-noise-ratio (SNR) environments, demonstrate the superiority of the
proposed algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05047</identifier>
 <datestamp>2015-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05047</id><created>2015-12-15</created><authors><author><keyname>Pak</keyname><forenames>UnSun</forenames></author><author><keyname>Sin</keyname><forenames>YongNam</forenames></author><author><keyname>Ryang</keyname><forenames>GyongIl</forenames></author></authors><title>An Approach for Design Parameter Optimization of the Triangle Cloud
  Control System</title><categories>cs.SY</categories><comments>7 pages, 2 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we have proposed the optimization approach of design parameter
of generalized cloud control system by hybrid chaos optimization approach. The
approach determined the off-line parameters of the cloud control system by the
chaos approach and on-line by gradient approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05055</identifier>
 <datestamp>2015-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05055</id><created>2015-12-16</created><authors><author><keyname>Chen</keyname><forenames>Yu</forenames></author><author><keyname>Zou</keyname><forenames>Xiufen</forenames></author></authors><title>Inferring Gene Regulatory Network Using An Evolutionary Multi-Objective
  Method</title><categories>cs.CE cs.NE q-bio.QM</categories><comments>8pages</comments><msc-class>92B99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inference of gene regulatory networks (GRNs) based on experimental data is a
challenging task in bioinformatics. In this paper, we present a bi-objective
minimization model (BoMM) for inference of GRNs, where one objective is the
fitting error of derivatives, and the other is the number of connections in the
network. To solve the BoMM efficiently, we propose a multi-objective
evolutionary algorithm (MOEA), and utilize the separable parameter estimation
method (SPEM) decoupling the ordinary differential equation (ODE) system. Then,
the Akaike Information Criterion (AIC) is employed to select one inference
result from the obtained Pareto set. Taking the S-system as the investigated
GRN model, our method can properly identify the topologies and parameter values
of benchmark systems. There is no need to preset problem-dependent parameter
values to obtain appropriate results, and thus, our method could be applicable
to inference of various GRNs models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05057</identifier>
 <datestamp>2015-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05057</id><created>2015-12-16</created><authors><author><keyname>Enduri</keyname><forenames>Murali Krishna</forenames></author><author><keyname>Reddy</keyname><forenames>I. Vinod</forenames></author><author><keyname>Jolad</keyname><forenames>Shivakumar</forenames></author></authors><title>Does diversity of papers affect their citations? Evidence from American
  Physical Society Journals</title><categories>cs.DL cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we study the correlation between interdisciplinarity of papers
within physical sciences and their citations by using meta data of articles
published in American Physical Society's Physical Review journals between 1985
to 2012. We use the Weitzman diversity index to measure the diversity of papers
and authors, exploiting the hierarchical structure of PACS (Physics and
Astronomy Classification Scheme) codes. We find that the fraction of authors
with high diversity is increasing with time, where as the fraction of least
diversity are decreasing, and moderate diversity authors have higher tendency
to switch over to other diversity groups. The diversity index of papers is
correlated with the citations they received in a given time period from their
publication year. Papers with lower and higher end of diversity index receive
lesser citations than the moderate diversity papers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05059</identifier>
 <datestamp>2015-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05059</id><created>2015-12-16</created><authors><author><keyname>Ghashami</keyname><forenames>Mina</forenames></author><author><keyname>Perry</keyname><forenames>Daniel</forenames></author><author><keyname>Phillips</keyname><forenames>Jeff M.</forenames></author></authors><title>Streaming Kernel Principal Component Analysis</title><categories>cs.DS cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Kernel principal component analysis (KPCA) provides a concise set of basis
vectors which capture non-linear structures within large data sets, and is a
central tool in data analysis and learning. To allow for non-linear relations,
typically a full $n \times n$ kernel matrix is constructed over $n$ data
points, but this requires too much space and time for large values of $n$.
Techniques such as the Nystr\&quot;om method and random feature maps can help
towards this goal, but they do not explicitly maintain the basis vectors in a
stream and take more space than desired. We propose a new approach for
streaming KPCA which maintains a small set of basis elements in a stream,
requiring space only logarithmic in $n$, and also improves the dependence on
the error parameter. Our technique combines together random feature maps with
recent advances in matrix sketching, it has guaranteed spectral norm error
bounds with respect to the original kernel matrix, and it compares favorably in
practice to state-of-the-art approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05060</identifier>
 <datestamp>2015-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05060</id><created>2015-12-16</created><authors><author><keyname>Singh</keyname><forenames>Jagpreet</forenames></author><author><keyname>Arora</keyname><forenames>Satish</forenames></author></authors><title>Relay Divide &amp; Rule Technique to Solve Energy Hole Problem for Wireless
  Sensor Network</title><categories>cs.NI</categories><comments>There are almost 9 pages of paper with 7 figures and 1 table that's
  based on prolong the network lifetime</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In remote sensor system, sensor hubs have the restricted battery power, so to
use the vitality in a more productive manner a few creator's created a few
strategies, yet at the same time there is have to decrease the vitality
utilization of hubs. In this paper, we presented another method called as
'Partition and Rule strategy' to unravel the scope dividing so as to open the
system field into subfield and next maintain a strategic distance from the
vitality gap issue with the assistance of static bunching. Essentially, in gap
and run plan system range is isolated into three locales to be specific
internal, center and external to conquer the issue of vitality utilization. We
execute this work in NS-2 and our recreation results demonstrate that our
system is far superior than old procedures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05064</identifier>
 <datestamp>2015-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05064</id><created>2015-12-16</created><authors><author><keyname>Traversa</keyname><forenames>Fabio L.</forenames></author><author><keyname>Di Ventra</keyname><forenames>Massimiliano</forenames></author></authors><title>Polynomial-time solution of prime factorization and NP-hard problems
  with digital memcomputing machines</title><categories>cs.ET cond-mat.mes-hall cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show how to solve the prime factorization and the NP-hard version of the
subset-sum problem on digital memcomputing machines using polynomial resources.
We first introduce the complexity classes for these machines, make a connection
with dynamical systems theory, and propose a practical scheme to solve the
above problems based on the novel concept of self-organizable logic gates and
circuits. These are logic gates and circuits that can solve boolean problems by
self-organizing into their solution. They can be fabricated either with circuit
elements with memory (such as memristors) and/or standard MOS technology. Using
tools of functional analysis, we prove that these machines have a global
attractor and the only equilibrium points are the solution(s) of the above
problems. In addition, any initial condition converges exponentially fast to
the solution(s), and the equilibrium convergence rate scales at most
polynomially with input size. Since digital memcomputing machines map integers
into integers they are robust against noise, and hence scalable. Therefore,
they are a realistic alternative to Turing machines to efficiently solve very
complex problems. We finally discuss the implications of our results to the
NP=P question.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05066</identifier>
 <datestamp>2015-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05066</id><created>2015-12-16</created><authors><author><keyname>Inoue</keyname><forenames>Hiroyasu</forenames></author></authors><title>Analyses of Aggregate Fluctuation of Firm Network Based on
  Self-Organized Criticality Model and Control Theory</title><categories>q-fin.GN cs.SY physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper examined the size difference of avalanches among industrial
sectors triggered by demand by using the production-inventory model and the
observed data. Also, we investigated how each industrial sector can be affected
in terms of network topology by using the control theory. We obtained the
following results. (1) The size of avalanches are diverse depending on sectors
where demands are given. (2) The simulated avalanche size for the policies
actually conducted correspond well to the ex-post evaluations of the policies.
(3) The expectation to get involved into avalanches are diverse depending on
sectors. (4) Service sectors and small firms are difficult to be indirectly
affected by fiscal policies. On the other hand, construction, manufacturing,
and wholesale sectors are well affected by fiscal policies. (5) If we need to
clip a network without losing the effect of the fiscal policy, we can clip the
network by the descending order of firms' capital size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05068</identifier>
 <datestamp>2015-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05068</id><created>2015-12-16</created><authors><author><keyname>Joung</keyname><forenames>Jingon</forenames></author><author><keyname>Kurniawan</keyname><forenames>Ernest</forenames></author><author><keyname>Sun</keyname><forenames>Sumei</forenames></author></authors><title>Principal Component Analysis (PCA)-based Massive-MIMO Channel Feedback</title><categories>cs.IT math.IT</categories><comments>10 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Channel-state-information (CSI) feedback methods are considered, especially
for massive or very large-scale multiple-input multiple-output (MIMO) systems.
To extract essential information from the CSI without redundancy that arises
from the highly correlated antennas, a receiver transforms (sparsifies) a
correlated CSI vector to an uncorrelated sparse CSI vector by using a
Karhunen-Loeve transform (KLT) matrix that consists of the eigen vectors of
covariance matrix of CSI vector and feeds back the essential components of the
sparse CSI, i.e., a principal component analysis method. A transmitter then
recovers the original CSI through the inverse transformation of the feedback
vector. Herein, to obtain the covariance matrix at transceiver, we derive
analytically the covariance matrix of spatially correlated Rayleigh fading
channels based on its statistics including transmit antennas' and receive
antennas' correlation matrices, channel variance, and channel delay profile.
With the knowledge of the channel statistics, the transceiver can readily
obtain the covariance matrix and KLT matrix. Compression feedback error and
bit-error-rate performance of the proposed method are analyzed. Numerical
results verify that the proposed method is promising, which reduces
significantly the feedback overhead of the massive-MIMO systems with marginal
performance degradation from full-CSI feedback (e.g., feedback amount reduction
by 80%, i.e., 1/5 of original CSI, with spectral efficiency reduction by only
2%). Furthermore, we show numerically that, for a given limited feedback
amount, we can find the optimal number of transmit antennas to achieve the
largest spectral efficiency, which is a new design framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05073</identifier>
 <datestamp>2015-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05073</id><created>2015-12-16</created><authors><author><keyname>Basu</keyname><forenames>Ayanendranath</forenames></author><author><keyname>Bose</keyname><forenames>Smarajit</forenames></author><author><keyname>Pal</keyname><forenames>Amita</forenames></author><author><keyname>Mukherjee</keyname><forenames>Anish</forenames></author><author><keyname>Das</keyname><forenames>Debasmita</forenames></author></authors><title>A Novel Minimum Divergence Approach to Robust Speaker Identification</title><categories>stat.ML cs.SD stat.AP</categories><comments>22 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, a novel solution to the speaker identification problem is
proposed through minimization of statistical divergences between the
probability distribution (g). of feature vectors from the test utterance and
the probability distributions of the feature vector corresponding to the
speaker classes. This approach is made more robust to the presence of outliers,
through the use of suitably modified versions of the standard divergence
measures. The relevant solutions to the minimum distance methods are referred
to as the minimum rescaled modified distance estimators (MRMDEs). Three
measures were considered - the likelihood disparity, the Hellinger distance and
Pearson's chi-square distance. The proposed approach is motivated by the
observation that, in the case of the likelihood disparity, when the empirical
distribution function is used to estimate g, it becomes equivalent to maximum
likelihood classification with Gaussian Mixture Models (GMMs) for speaker
classes, a highly effective approach used, for example, by Reynolds [22] based
on Mel Frequency Cepstral Coefficients (MFCCs) as features. Significant
improvement in classification accuracy is observed under this approach on the
benchmark speech corpus NTIMIT and a new bilingual speech corpus NISIS, with
MFCC features, both in isolation and in combination with delta MFCC features.
Moreover, the ubiquitous principal component transformation, by itself and in
conjunction with the principle of classifier combination, is found to further
enhance the performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05075</identifier>
 <datestamp>2015-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05075</id><created>2015-12-16</created><authors><author><keyname>Niyato</keyname><forenames>Dusit</forenames></author><author><keyname>Hoang</keyname><forenames>Dinh Thai</forenames></author><author><keyname>Luong</keyname><forenames>Nguyen Cong</forenames></author><author><keyname>Wang</keyname><forenames>Ping</forenames></author><author><keyname>Kim</keyname><forenames>Dong In</forenames></author><author><keyname>Han</keyname><forenames>Zhu</forenames></author></authors><title>Smart Data Pricing Models for Internet-of-Things (IoT): A Bundling
  Strategy Approach</title><categories>cs.GT</categories><comments>17 pages, 6 figures, 1 table, IEEE Network Magazine, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Internet of things (IoT) has emerged as a new paradigm for the future
Internet. In IoT, enormous devices are connected to the Internet and thereby
being a huge data source for numerous applications. In this article, we focus
on addressing data management in IoT through using a smart data pricing (SDP)
approach. With SDP, data can be managed flexibly and efficiently through
intelligent and adaptive incentive mechanisms. Moreover, it is a major source
of revenue for providers and partners. We propose a new pricing scheme for IoT
service providers to determine the sensing data buying price and IoT service
subscription fee offered to sensor owners and service users, respectively.
Additionally, we adopt the bundling strategy that allows multiple providers to
form a coalition and bid their services as a bundle, attracting more users and
achieving higher revenue. Finally, we outline some important open research
issues for SDP and IoT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05077</identifier>
 <datestamp>2015-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05077</id><created>2015-12-16</created><authors><author><keyname>Pak</keyname><forenames>UnSun</forenames></author><author><keyname>Kim</keyname><forenames>YongNam</forenames></author><author><keyname>Ryang</keyname><forenames>GyongIl</forenames></author></authors><title>A Method of Advanced Chaos Optimal Search Algorithm</title><categories>math.OC cs.SY</categories><comments>5 pages, 5 figures,1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the advanced parallel chaos optimal search algorithm is
proposed and the effectiveness of the proposed algorithm is verified through
the experiment for find out the minimum of several benchmark functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05088</identifier>
 <datestamp>2016-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05088</id><created>2015-12-16</created><updated>2015-12-31</updated><authors><author><keyname>Truong</keyname><forenames>Lan V.</forenames></author><author><keyname>Fong</keyname><forenames>Silas L.</forenames></author><author><keyname>Tan</keyname><forenames>Vincent Y. F.</forenames></author></authors><title>On Gaussian Channels with Feedback under Expected Power Constraints and
  with Non-Vanishing Error Probabilities</title><categories>cs.IT math.IT</categories><comments>Submitted to the IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider single- and multi-user Gaussian channels with
feedback under expected power constraints and with non-vanishing error
probabilities. In the first of two contributions, we study asymptotic
expansions for the additive white Gaussian noise (AWGN) channel with feedback
under the average error probability formalism. By drawing ideas from Schalkwijk
and Kailath for the direct part and the meta-converse for the converse part, we
establish the $\varepsilon$-capacity and show that it depends on $\varepsilon$
in general and so the strong converse fails to hold. Furthermore, we provide
bounds on the second-order term in the asymptotic expansion. We show that the
second-order term is bounded between $-\ln\ln n$ and a term that is
proportional to $+\sqrt{n\ln n}$ where $n$ is the blocklength. The lower bound
on the second-order term shows that feedback does provide an improvement in the
maximal achievable rate over the case where no feedback is available. In our
second contribution, we establish the $\varepsilon$-capacity region for the
AWGN multiple access channel (MAC) with feedback under the expected power
constraint by combining ideas from hypothesis testing, information spectrum
analysis, Ozarow's coding scheme, and power control.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05103</identifier>
 <datestamp>2015-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05103</id><created>2015-12-16</created><authors><author><keyname>Moosavi-Dezfooli</keyname><forenames>Seyed-Mohsen</forenames></author><author><keyname>Pignolet</keyname><forenames>Yvonne-Anne</forenames></author><author><keyname>Dzung</keyname><forenames>Dacfey</forenames></author></authors><title>Simultaneous Acoustic Localization of Multiple Smartphones with
  Euclidean Distance Matrices</title><categories>cs.NI cs.SD</categories><acm-class>C.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present an acoustic localization system for multiple
devices. In contrast to systems which localise a device relative to one or
several anchor points, we focus on the joint localisation of several devices
relative to each other. We present a prototype of our system on off-the-shelf
smartphones. No user interaction is required, the phones emit acoustic pulses
according to a precomputed schedule. Using the elapsed time between two times
of arrivals (ETOA) method with sample counting, distances between the devices
are estimated. These, possibly incomplete, distances are the input to an
efficient and robust multi-dimensional scaling algorithm returning a position
for each phone. We evaluated our system in real-world scenarios, achieving
error margins of 15 cm in an office environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1512.05110</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1512.05110</id><created>2015-12-16</created><updated>2015-12-21</updated><authors><author><keyname>Domingo-Ferrer</keyname><forenames>J.</forenames></author><author><keyname>Soria-Comas</keyname><forenames>J.</forenames></author></authors><title>From t-closeness to differential privacy and vice versa in data
  anonymization</title><categories>cs.CR</categories><journal-ref>Knowledge-Based Systems, Vol. 74, pp. 151-158, 2015</journal-ref><doi>10.1016/j.knosys.2014.11.011</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  k-Anonymity and {\epsilon}-differential privacy are two mainstream privacy
models, the former introduced to anonymize data sets and the latter to limit
the knowledge gain that results from including one individual in the data set.
Whereas basic k-anonymity only protects against identity disclosure,
t-closeness was presented as an extension of k-anonymity that also protects
against attribute disclosure. We show here that, if not quite equivalent,
t-closeness and {\epsilon}-differential privacy are strongly related to one
another when it comes to anonymizing data sets. Specifically, k-anonymity for
the quasi-identifiers combined with {\epsilon}-differential privacy for the
confidential attributes yields stochastic t-closeness (an extension of
t-closeness), with t a function of k and {\epsilon}. Conversely, t-closeness
can yield {\epsilon}- differential privacy when t = exp({\epsilon}/2) and the
assumptions made by t-closeness about the prior and posterior views of the data
hold
</abstract></arXiv>
</metadata>
</record>
<resumptionToken cursor="88000" completeListSize="102538">1122234|89001</resumptionToken>
</ListRecords>
</OAI-PMH>
