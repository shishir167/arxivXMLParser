<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
<responseDate>2016-03-09T00:58:00Z</responseDate>
<request verb="ListRecords" resumptionToken="1122234|29001">http://export.arxiv.org/oai2</request>
<ListRecords>
<record>
<header>
 <identifier>oai:arXiv.org:1202.4743</identifier>
 <datestamp>2012-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.4743</id><created>2012-02-21</created><authors><author><keyname>You</keyname><forenames>Wonsang</forenames></author><author><keyname>Sabirin</keyname><forenames>M. S. Houari</forenames></author><author><keyname>Kim</keyname><forenames>Munchurl</forenames></author></authors><title>Real-time detection and tracking of multiple objects with partial
  decoding in H.264/AVC bitstream domain</title><categories>cs.MM cs.CV</categories><comments>SPIE Real-Time Image and Video Processing Conference 2009</comments><journal-ref>Proceedings of SPIE 2009, Volume: 7244, Publisher: SPIE, Pages:
  72440D-72440D-12</journal-ref><doi>10.1117/12.805596</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we show that we can apply probabilistic spatiotemporal
macroblock filtering (PSMF) and partial decoding processes to effectively
detect and track multiple objects in real time in H.264|AVC bitstreams with
stationary background. Our contribution is that our method cannot only show
fast processing time but also handle multiple moving objects that are
articulated, changing in size or internally have monotonous color, even though
they contain a chaotic set of non-homogeneous motion vectors inside. In
addition, our partial decoding process for H.264|AVC bitstreams enables to
improve the accuracy of object trajectories and overcome long occlusion by
using extracted color information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.4798</identifier>
 <datestamp>2012-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.4798</id><created>2012-02-21</created><updated>2012-02-23</updated><authors><author><keyname>Etessami</keyname><forenames>Kousha</forenames></author><author><keyname>Stewart</keyname><forenames>Alistair</forenames></author><author><keyname>Yannakakis</keyname><forenames>Mihalis</forenames></author></authors><title>Polynomial Time Algorithms for Branching Markov Decision Processes and
  Probabilistic Min(Max) Polynomial Bellman Equations</title><categories>cs.CC cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that one can approximate the least fixed point solution for a
multivariate system of monotone probabilistic max(min) polynomial equations,
referred to as maxPPSs (and minPPSs, respectively), in time polynomial in both
the encoding size of the system of equations and in log(1/epsilon), where
epsilon &gt; 0 is the desired additive error bound of the solution. (The model of
computation is the standard Turing machine model.) We establish this result
using a generalization of Newton's method which applies to maxPPSs and minPPSs,
even though the underlying functions are only piecewise-differentiable. This
generalizes our recent work which provided a P-time algorithm for purely
probabilistic PPSs.
  These equations form the Bellman optimality equations for several important
classes of infinite-state Markov Decision Processes (MDPs). Thus, as a
corollary, we obtain the first polynomial time algorithms for computing to
within arbitrary desired precision the optimal value vector for several classes
of infinite-state MDPs which arise as extensions of classic, and heavily
studied, purely stochastic processes. These include both the problem of
maximizing and mininizing the termination (extinction) probability of
multi-type branching MDPs, stochastic context-free MDPs, and 1-exit Recursive
MDPs.
  Furthermore, we also show that we can compute in P-time an epsilon-optimal
policy for both maximizing and minimizing branching, context-free, and
1-exit-Recursive MDPs, for any given desired epsilon &gt; 0. This is despite the
fact that actually computing optimal strategies is Sqrt-Sum-hard and
PosSLP-hard in this setting.
  We also derive, as an easy consequence of these results, an FNP upper bound
on the complexity of computing the value (within arbitrary desired precision)
of branching simple stochastic games (BSSGs).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.4805</identifier>
 <datestamp>2012-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.4805</id><created>2012-02-21</created><authors><author><keyname>Pfeiffer</keyname><forenames>Joseph J.</forenames><suffix>III</suffix></author><author><keyname>La Fond</keyname><forenames>Timothy</forenames></author><author><keyname>Moreno</keyname><forenames>Sebastian</forenames></author><author><keyname>Neville</keyname><forenames>Jennifer</forenames></author></authors><title>Fast Generation of Large Scale Social Networks with Clustering</title><categories>cs.SI physics.soc-ph</categories><comments>11 pages</comments><acm-class>G.2.2; G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A key challenge within the social network literature is the problem of
network generation - that is, how can we create synthetic networks that match
characteristics traditionally found in most real world networks? Important
characteristics that are present in social networks include a power law degree
distribution, small diameter and large amounts of clustering; however, most
current network generators, such as the Chung Lu and Kronecker models, largely
ignore the clustering present in a graph and choose to focus on preserving
other network statistics, such as the power law distribution. Models such as
the exponential random graph model have a transitivity parameter, but are
computationally difficult to learn, making scaling to large real world networks
intractable. In this work, we propose an extension to the Chung Lu ran- dom
graph model, the Transitive Chung Lu (TCL) model, which incorporates the notion
of a random transitive edge. That is, with some probability it will choose to
connect to a node exactly two hops away, having been introduced to a 'friend of
a friend'. In all other cases it will follow the standard Chung Lu model,
selecting a 'random surfer' from anywhere in the graph according to the given
invariant distribution. We prove TCL's expected degree distribution is equal to
the degree distribution of the original graph, while being able to capture the
clustering present in the network. The single parameter required by our model
can be learned in seconds on graphs with millions of edges, while networks can
be generated in time that is linear in the number of edges. We demonstrate the
performance TCL on four real- world social networks, including an email dataset
with hundreds of thousands of nodes and millions of edges, showing TCL
generates graphs that match the degree distribution, clustering coefficients
and hop plots of the original networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.4815</identifier>
 <datestamp>2012-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.4815</id><created>2012-02-21</created><updated>2012-02-23</updated><authors><author><keyname>Yadav</keyname><forenames>Surjeet Kumar</forenames></author><author><keyname>Bharadwaj</keyname><forenames>Brijesh</forenames></author><author><keyname>Pal</keyname><forenames>Saurabh</forenames></author></authors><title>Data Mining Applications: A comparative Study for Predicting Student's
  performance</title><categories>cs.IR cs.DB</categories><comments>7 pages, 2 figures. arXiv admin note: text overlap with
  arXiv:1201.3417 and arXiv:1201.3418</comments><journal-ref>International Journal of Innovative Technology and Creative
  Engineering, Vol.1 No.12 (2011) 13-19</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Knowledge Discovery and Data Mining (KDD) is a multidisciplinary area
focusing upon methodologies for extracting useful knowledge from data and there
are several useful KDD tools to extracting the knowledge. This knowledge can be
used to increase the quality of education. But educational institution does not
use any knowledge discovery process approach on these data. Data mining can be
used for decision making in educational system. A decision tree classifier is
one of the most widely used supervised learning methods used for data
exploration based on divide &amp; conquer technique. This paper discusses use of
decision trees in educational data mining. Decision tree algorithms are applied
on students' past performance data to generate the model and this model can be
used to predict the students' performance. It helps earlier in identifying the
dropouts and students who need special attention and allow the teacher to
provide appropriate advising/counseling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.4818</identifier>
 <datestamp>2012-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.4818</id><created>2012-02-22</created><authors><author><keyname>Shaikh</keyname><forenames>Sanober</forenames></author><author><keyname>rao</keyname><forenames>Madhuri</forenames></author></authors><title>Association Rule Mining Based On Trade List</title><categories>cs.DB</categories><comments>15 pages</comments><journal-ref>http://www.airccj.org/ijdkp/ijdkp2011.html</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper a new mining algorithm is defined based on frequent item set.
Apriori Algorithm scans the database every time when it finds the frequent item
set so it is very time consuming and at each step it generates candidate item
set. So for large databases it takes lots of space to store candidate item set
.In undirected item set graph, it is improvement on apriori but it takes time
and space for tree generation. The defined algorithm scans the database at the
start only once and then from that scanned data base it generates the Trade
List. It contains the information of whole database. By considering minimum
support it finds the frequent item set and by considering the minimum
confidence it generates the association rule. If database and minimum support
is changed, the new algorithm finds the new frequent items by scanning Trade
List. That is why it's executing efficiency is improved distinctly compared to
traditional algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.4821</identifier>
 <datestamp>2012-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.4821</id><created>2012-02-22</created><updated>2012-07-02</updated><authors><author><keyname>Shin</keyname><forenames>Chan-Su</forenames></author></authors><title>A Note on Minimum-Sum Coverage by Aligned Disks</title><categories>cs.CG</categories><comments>7 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider a facility location problem to find a minimum-sum
coverage of n points by disks centered at a fixed line. The cost of a disk with
radius r has a form of a non-decreasing function f(r) = r^a for any a &gt;= 1. The
goal is to find a set of disks under Lp metric such that the disks are centered
on the x-axis, their union covers n points, and the sum of the cost of the
disks is minimized. Alt et al. [1] presented an algorithm in O(n^4 log n) time
for any a &gt; 1 under any Lp metric. We present a faster algorithm for this
problem in O(n^2 log n) time for any a &gt; 1 and any Lp metric.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.4824</identifier>
 <datestamp>2012-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.4824</id><created>2012-02-22</created><authors><author><keyname>Borchmann</keyname><forenames>Daniel</forenames></author></authors><title>A General Form of Attribute Exploration</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a general form of attribute exploration, a knowledge completion
algorithm from Formal Concept Analysis. The aim of our presentation is not only
to extend the applicability of attribute exploration by a general description.
It may also allow to view different existing variants of attribute exploration
as instances of a general form, which may simplify theoretical considerations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.4828</identifier>
 <datestamp>2012-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.4828</id><created>2012-02-22</created><authors><author><keyname>Autexier</keyname><forenames>Serge</forenames><affiliation>German Research Center for Artificial Intelligence</affiliation></author><author><keyname>Dietrich</keyname><forenames>Dominik</forenames><affiliation>German Research Center for Artificial Intelligence</affiliation></author><author><keyname>Schiller</keyname><forenames>Marvin</forenames><affiliation>Brunel University, London, UK</affiliation></author></authors><title>Towards an Intelligent Tutor for Mathematical Proofs</title><categories>cs.AI cs.LO cs.MS cs.SC</categories><comments>In Proceedings THedu'11, arXiv:1202.4535</comments><proxy>EPTCS</proxy><acm-class>K.3.0;I.2.3</acm-class><journal-ref>EPTCS 79, 2012, pp. 1-28</journal-ref><doi>10.4204/EPTCS.79.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computer-supported learning is an increasingly important form of study since
it allows for independent learning and individualized instruction. In this
paper, we discuss a novel approach to developing an intelligent tutoring system
for teaching textbook-style mathematical proofs. We characterize the
particularities of the domain and discuss common ITS design models. Our
approach is motivated by phenomena found in a corpus of tutorial dialogs that
were collected in a Wizard-of-Oz experiment. We show how an intelligent tutor
for textbook-style mathematical proofs can be built on top of an adapted
assertion-level proof assistant by reusing representations and proof search
strategies originally developed for automated and interactive theorem proving.
The resulting prototype was successfully evaluated on a corpus of tutorial
dialogs and yields good results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.4829</identifier>
 <datestamp>2012-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.4829</id><created>2012-02-22</created><authors><author><keyname>Back</keyname><forenames>Ralph-Johan</forenames><affiliation>&#xc5;bo Akademi University</affiliation></author><author><keyname>Eriksson</keyname><forenames>Johannes</forenames><affiliation>&#xc5;bo Akademi University</affiliation></author></authors><title>An Exercise in Invariant-based Programming with Interactive and
  Automatic Theorem Prover Support</title><categories>cs.LO cs.PL</categories><comments>In Proceedings THedu'11, arXiv:1202.4535</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 79, 2012, pp. 29-48</journal-ref><doi>10.4204/EPTCS.79.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Invariant-Based Programming (IBP) is a diagram-based correct-by-construction
programming methodology in which the program is structured around the
invariants, which are additionally formulated before the actual code. Socos is
a program construction and verification environment built specifically to
support IBP. The front-end to Socos is a graphical diagram editor, allowing the
programmer to construct invariant-based programs and check their correctness.
The back-end component of Socos, the program checker, computes the verification
conditions of the program and tries to prove them automatically. It uses the
theorem prover PVS and the SMT solver Yices to discharge as many of the
verification conditions as possible without user interaction. In this paper, we
first describe the Socos environment from a user and systems level perspective;
we then exemplify the IBP workflow by building a verified implementation of
heapsort in Socos. The case study highlights the role of both automatic and
interactive theorem proving in three sequential stages of the IBP workflow:
developing the background theory, formulating the program specification and
invariants, and proving the correctness of the final implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.4830</identifier>
 <datestamp>2012-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.4830</id><created>2012-02-22</created><authors><author><keyname>Botana</keyname><forenames>Francisco</forenames></author><author><keyname>Ab&#xe1;nades</keyname><forenames>Miguel A.</forenames></author></authors><title>Automatic Deduction in Dynamic Geometry using Sage</title><categories>cs.MS cs.SC</categories><comments>In Proceedings THedu'11, arXiv:1202.4535</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 79, 2012, pp. 49-62</journal-ref><doi>10.4204/EPTCS.79.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a symbolic tool that provides robust algebraic methods to handle
automatic deduction tasks for a dynamic geometry construction. The main
prototype has been developed as two different worksheets for the open source
computer algebra system Sage, corresponding to two different ways of coding a
geometric construction. In one worksheet, diagrams constructed with the open
source dynamic geometry system GeoGebra are accepted. In this worksheet,
Groebner bases are used to either compute the equation of a geometric locus in
the case of a locus construction or to determine the truth of a general
geometric statement included in the GeoGebra construction as a boolean
variable. In the second worksheet, locus constructions coded using the common
file format for dynamic geometry developed by the Intergeo project are accepted
for computation. The prototype and several examples are provided for testing.
Moreover, a third Sage worksheet is presented in which a novel algorithm to
eliminate extraneous parts in symbolically computed loci has been implemented.
The algorithm, based on a recent work on the Groebner cover of parametric
systems, identifies degenerate components and extraneous adherence points in
loci, both natural byproducts of general polynomial algebraic methods. Detailed
examples are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.4831</identifier>
 <datestamp>2012-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.4831</id><created>2012-02-22</created><authors><author><keyname>Mari&#x107;</keyname><forenames>Filip</forenames><affiliation>Faculty of Mathematics, University of Belgrade, Serbia</affiliation></author><author><keyname>Petrovi&#x107;</keyname><forenames>Ivan</forenames><affiliation>Faculty of Mathematics, University of Belgrade, Serbia</affiliation></author><author><keyname>Petrovi&#x107;</keyname><forenames>Danijela</forenames><affiliation>Faculty of Mathematics, University of Belgrade, Serbia</affiliation></author><author><keyname>Jani&#x10d;i&#x107;</keyname><forenames>Predrag</forenames><affiliation>Faculty of Mathematics, University of Belgrade, Serbia</affiliation></author></authors><title>Formalization and Implementation of Algebraic Methods in Geometry</title><categories>cs.SC cs.LO cs.MS</categories><comments>In Proceedings THedu'11, arXiv:1202.4535</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 79, 2012, pp. 63-81</journal-ref><doi>10.4204/EPTCS.79.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe our ongoing project of formalization of algebraic methods for
geometry theorem proving (Wu's method and the Groebner bases method), their
implementation and integration in educational tools. The project includes
formal verification of the algebraic methods within Isabelle/HOL proof
assistant and development of a new, open-source Java implementation of the
algebraic methods. The project should fill-in some gaps still existing in this
area (e.g., the lack of formal links between algebraic methods and synthetic
geometry and the lack of self-contained implementations of algebraic methods
suitable for integration with dynamic geometry tools) and should enable new
applications of theorem proving in education.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.4832</identifier>
 <datestamp>2012-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.4832</id><created>2012-02-22</created><authors><author><keyname>Neuper</keyname><forenames>Walther</forenames></author></authors><title>Automated Generation of User Guidance by Combining Computation and
  Deduction</title><categories>cs.LO cs.HC cs.PL</categories><comments>In Proceedings THedu'11, arXiv:1202.4535</comments><proxy>EPTCS</proxy><acm-class>D.3.2; F.4.1; I.2.3; I.2.2; I.2.3; I.2.5</acm-class><journal-ref>EPTCS 79, 2012, pp. 82-101</journal-ref><doi>10.4204/EPTCS.79.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Herewith, a fairly old concept is published for the first time and named
&quot;Lucas Interpretation&quot;. This has been implemented in a prototype, which has
been proved useful in educational practice and has gained academic relevance
with an emerging generation of educational mathematics assistants (EMA) based
on Computer Theorem Proving (CTP).
  Automated Theorem Proving (ATP), i.e. deduction, is the most reliable
technology used to check user input. However ATP is inherently weak in
automatically generating solutions for arbitrary problems in applied
mathematics. This weakness is crucial for EMAs: when ATP checks user input as
incorrect and the learner gets stuck then the system should be able to suggest
possible next steps.
  The key idea of Lucas Interpretation is to compute the steps of a calculation
following a program written in a novel CTP-based programming language, i.e.
computation provides the next steps. User guidance is generated by combining
deduction and computation: the latter is performed by a specific language
interpreter, which works like a debugger and hands over control to the learner
at breakpoints, i.e. tactics generating the steps of calculation. The
interpreter also builds up logical contexts providing ATP with the data
required for checking user input, thus combining computation and deduction.
  The paper describes the concepts underlying Lucas Interpretation so that open
questions can adequately be addressed, and prerequisites for further work are
provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.4833</identifier>
 <datestamp>2012-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.4833</id><created>2012-02-22</created><authors><author><keyname>Santos</keyname><forenames>Vanda</forenames><affiliation>CISUC/ESTGV - IPV</affiliation></author><author><keyname>Quaresma</keyname><forenames>Pedro</forenames><affiliation>CISUC/Department of Mathematics, University of Coimbra</affiliation></author></authors><title>Integrating DGSs and GATPs in an Adaptative and Collaborative
  Blended-Learning Web-Environment</title><categories>cs.CG cs.MS</categories><comments>In Proceedings THedu'11, arXiv:1202.4535</comments><proxy>EPTCS</proxy><acm-class>F.2.2; K.3</acm-class><journal-ref>EPTCS 79, 2012, pp. 111-123</journal-ref><doi>10.4204/EPTCS.79.7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The area of geometry with its very strong and appealing visual contents and
its also strong and appealing connection between the visual content and its
formal specification, is an area where computational tools can enhance, in a
significant way, the learning environments.
  The dynamic geometry software systems (DGSs) can be used to explore the
visual contents of geometry. This already mature tools allows an easy
construction of geometric figures build from free objects and elementary
constructions. The geometric automated theorem provers (GATPs) allows formal
deductive reasoning about geometric constructions, extending the reasoning via
concrete instances in a given model to formal deductive reasoning in a
geometric theory.
  An adaptative and collaborative blended-learning environment where the DGS
and GATP features could be fully explored would be, in our opinion a very rich
and challenging learning environment for teachers and students.
  In this text we will describe the Web Geometry Laboratory a Web environment
incorporating a DGS and a repository of geometric problems, that can be used in
a synchronous and asynchronous fashion and with some adaptative and
collaborative features.
  As future work we want to enhance the adaptative and collaborative aspects of
the environment and also to incorporate a GATP, constructing a dynamic and
individualised learning environment for geometry.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.4834</identifier>
 <datestamp>2012-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.4834</id><created>2012-02-22</created><authors><author><keyname>Schreiner</keyname><forenames>Wolfgang</forenames></author></authors><title>Computer-Assisted Program Reasoning Based on a Relational Semantics of
  Programs</title><categories>cs.LO cs.MS cs.PL cs.SC</categories><comments>In Proceedings THedu'11, arXiv:1202.4535</comments><proxy>EPTCS</proxy><acm-class>F.3.1;F3.2;K3.2</acm-class><journal-ref>EPTCS 79, 2012, pp. 124-142</journal-ref><doi>10.4204/EPTCS.79.8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an approach to program reasoning which inserts between a program
and its verification conditions an additional layer, the denotation of the
program expressed in a declarative form. The program is first translated into
its denotation from which subsequently the verification conditions are
generated. However, even before (and independently of) any verification
attempt, one may investigate the denotation itself to get insight into the
&quot;semantic essence&quot; of the program, in particular to see whether the denotation
indeed gives reason to believe that the program has the expected behavior.
Errors in the program and in the meta-information may thus be detected and
fixed prior to actually performing the formal verification. More concretely,
following the relational approach to program semantics, we model the effect of
a program as a binary relation on program states. A formal calculus is devised
to derive from a program a logic formula that describes this relation and is
subject for inspection and manipulation. We have implemented this idea in a
comprehensive form in the RISC ProgramExplorer, a new program reasoning
environment for educational purposes which encompasses the previously developed
RISC ProofNavigator as an interactive proving assistant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.4835</identifier>
 <datestamp>2012-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.4835</id><created>2012-02-22</created><authors><author><keyname>Wenzel</keyname><forenames>Makarius</forenames></author><author><keyname>Wolff</keyname><forenames>Burkhart</forenames></author></authors><title>Isabelle/PIDE as Platform for Educational Tools</title><categories>cs.LO cs.AI cs.MS</categories><comments>In Proceedings THedu'11, arXiv:1202.4535</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 79, 2012, pp. 143-153</journal-ref><doi>10.4204/EPTCS.79.9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Isabelle/PIDE platform addresses the question whether proof assistants of
the LCF family are suitable as technological basis for educational tools. The
traditionally strong logical foundations of systems like HOL, Coq, or Isabelle
have so far been counter-balanced by somewhat inaccessible interaction via the
TTY (or minor variations like the well-known Proof General / Emacs interface).
Thus the fundamental question of math education tools with fully-formal
background theories has often been answered negatively due to accidental
weaknesses of existing proof engines.
  The idea of &quot;PIDE&quot; (which means &quot;Prover IDE&quot;) is to integrate existing
provers like Isabelle into a larger environment, that facilitates access by
end-users and other tools. We use Scala to expose the proof engine in ML to the
JVM world, where many user-interfaces, editor frameworks, and educational tools
already exist. This shall ultimately lead to combined mathematical assistants,
where the logical engine is in the background, without obstructing the view on
applications of formal methods, formalized mathematics, and math education in
particular.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.4836</identifier>
 <datestamp>2012-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.4836</id><created>2012-02-22</created><authors><author><keyname>Badhera</keyname><forenames>Usha</forenames></author><author><keyname>Purohit</keyname><forenames>G. N.</forenames></author><author><keyname>Taruna</keyname><forenames>S.</forenames></author></authors><title>Fault Based Techniques for Testing Boolean Expressions: A Survey</title><categories>cs.SE</categories><comments>10 pages, 4 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Boolean expressions are major focus of specifications and they are very much
prone to introduction of faults, this survey presents various fault based
testing techniques. It identifies that the techniques differ in their fault
detection capabilities and generation of test suite. The various techniques
like Cause effect graph, meaningful impact strategy, Branch Operator Strategy
(BOR), BOR+MI, MUMCUT, Modified Condition/ Decision Coverage (MCDC) has been
considered. This survey describes the basic algorithms and fault categories
used by these strategies for evaluating their performance. Finally, it contains
short summaries of the papers that use Boolean expressions used to specify the
requirements for detecting faults. These techniques have been empirically
evaluated by various researchers on a simplified safety related real time
control system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.4837</identifier>
 <datestamp>2012-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.4837</id><created>2012-02-22</created><authors><author><keyname>Saludes</keyname><forenames>Jordi</forenames><affiliation>UPC</affiliation></author><author><keyname>Xamb&#xf3;</keyname><forenames>Sebastian</forenames><affiliation>UPC</affiliation></author></authors><title>The GF Mathematics Library</title><categories>cs.MS cs.CL</categories><comments>In Proceedings THedu'11, arXiv:1202.4535</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 79, 2012, pp. 102-110</journal-ref><doi>10.4204/EPTCS.79.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is devoted to present the Mathematics Grammar Library, a system
for multilingual mathematical text processing. We explain the context in which
it originated, its current design and functionality and the current development
goals. We also present two prototype services and comment on possible future
applications in the area of artificial mathematics assistants.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.4842</identifier>
 <datestamp>2013-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.4842</id><created>2012-02-22</created><authors><author><keyname>Aubry</keyname><forenames>Yves</forenames><affiliation>IML, IMATH</affiliation></author><author><keyname>Godin</keyname><forenames>Jean-Christophe</forenames><affiliation>IMATH</affiliation></author><author><keyname>Togni</keyname><forenames>Olivier</forenames><affiliation>Le2i</affiliation></author></authors><title>Vectorial solutions to list multicoloring problems on graphs</title><categories>math.CO cs.DM</categories><comments>10 pages</comments><proxy>ccsd</proxy><journal-ref>Advances and Applications in Discrete Mathematics Volume 9,
  Num\'ero 2 (2012) pp 65 --81</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a graph $G$ with a given list assignment $L$ on the vertices, we give an
algebraical description of the set of all weights $w$ such that $G$ is
$(L,w)$-colorable, called permissible weights. Moreover, for a graph $G$ with a
given list $L$ and a given permissible weight $w$, we describe the set of all
$(L,w)$-colorings of $G$. By the way, we solve the {\sl channel assignment
problem}. Furthermore, we describe the set of solutions to the {\sl on call
problem}: when $w$ is not a permissible weight, we find all the nearest
permissible weights $w'$. Finally, we give a solution to the non-recoloring
problem keeping a given subcoloring.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.4856</identifier>
 <datestamp>2012-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.4856</id><created>2012-02-22</created><authors><author><keyname>Zeng</keyname><forenames>Yong</forenames></author><author><keyname>Gunawan</keyname><forenames>Erry</forenames></author><author><keyname>Guan</keyname><forenames>Yong Liang</forenames></author></authors><title>Improved Linear Precoding over Block Diagonalization in Multi-cell
  Cooperative Networks</title><categories>cs.IT math.IT</categories><comments>21 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In downlink multiuser multiple-input multiple-output (MIMO) systems, block
diagonalization (BD) is a practical linear precoding scheme which achieves the
same degrees of freedom (DoF) as the optimal linear/nonlinear precoding
schemes. However, its sum-rate performance is rather poor in the practical SNR
regime due to the transmit power boost problem. In this paper, we propose an
improved linear precoding scheme over BD with a so-called
&quot;effective-SNR-enhancement&quot; technique. The transmit covariance matrices are
obtained by firstly solving a power minimization problem subject to the minimum
rate constraint achieved by BD, and then properly scaling the solution to
satisfy the power constraints. It is proved that such approach equivalently
enhances the system SNR, and hence compensates the transmit power boost problem
associated with BD. The power minimization problem is in general non-convex. We
therefore propose an efficient algorithm that solves the problem heuristically.
Simulation results show significant sum rate gains over the optimal BD and the
existing minimum mean square error (MMSE) based precoding schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.4865</identifier>
 <datestamp>2012-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.4865</id><created>2012-02-22</created><authors><author><keyname>Boano</keyname><forenames>Carlo Alberto</forenames></author><author><keyname>R&#xf6;mer</keyname><forenames>Kay</forenames></author><author><keyname>Z&#xfa;&#xf1;iga</keyname><forenames>Marco Antonio</forenames></author><author><keyname>Voigt</keyname><forenames>Thiemo</forenames></author></authors><title>Jam-X: Wireless Agreement under Interference</title><categories>cs.NI</categories><acm-class>C.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless low-power transceivers used in sensor networks such as IEEE 802.15.4
typically operate in unlicensed frequency bands that are subject to external
interference from devices transmitting at much higher power. Communication
protocols should therefore be designed to be robust against such interference.
A critical building block of many protocols at all layers is agreement on a
piece of information among a set of nodes. At the MAC layer, nodes may need to
agree on a new time slot or frequency channel; at the application layer nodes
may need to agree on handing over a leader role from one node to another.
Message loss caused by interference may break agreement in two different ways:
none of the nodes use the new information (time slot, channel, leader) and
stick with the previous assignment, or - even worse - some nodes use the new
information and some do not. This may lead to reduced performance or failures.
In this paper we investigate the problem of agreement under interference and
point out the limitations of the traditional message-based n-way handshake
approach. We propose novel protocols that use jamming instead of message
transmissions and show that they outperform the n-way handshake in terms of
agreement probability, energy consumption, and time-to-completion both in the
unicast case (two neighboring nodes agree) as well as in the broadcast case
(any number of neighboring nodes agree).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.4871</identifier>
 <datestamp>2012-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.4871</id><created>2012-02-22</created><authors><author><keyname>Rakesh</keyname><forenames>S.</forenames></author><author><keyname>Kaller</keyname><forenames>Ajitkumar A</forenames></author><author><keyname>Shadakshari</keyname><forenames>B. C.</forenames></author><author><keyname>Annappa</keyname><forenames>B.</forenames></author></authors><title>Multilevel Image Encryption</title><categories>cs.CR cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the fast evolution of digital data exchange and increased usage of multi
media images, it is essential to protect the confidential image data from
unauthorized access. In natural images the values and position of the
neighbouring pixels are strongly correlated. The method proposed in this paper,
breaks this correlation increasing entropy of the position and entropy of pixel
values using block shuffling and encryption by chaotic sequence respectively.
The plain-image is initially row wise shuffled and first level of encryption is
performed using addition modulo operation. The image is divided into blocks and
then block based shuffling is performed using Arnold Cat transformation,
further the blocks are uniformly scrambled across the image. Finally the
shuffled image undergoes second level of encryption by bitwise XOR operation,
and then the image as a whole is shuffled column wise to produce the ciphered
image for transmission. The experimental results show that the proposed
algorithm can successfully encrypt or decrypt the image with the secret keys,
and the analysis of the algorithm also demonstrates that the encrypted image
has good information entropy and low correlation coefficients.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.4880</identifier>
 <datestamp>2012-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.4880</id><created>2012-02-22</created><authors><author><keyname>Gallo</keyname><forenames>Massimo</forenames></author><author><keyname>Kauffmann</keyname><forenames>Bruno</forenames></author><author><keyname>Muscariello</keyname><forenames>Luca</forenames></author><author><keyname>Simonian</keyname><forenames>Alain</forenames></author><author><keyname>Tanguy</keyname><forenames>Christian</forenames></author></authors><title>Performance Evaluation of the Random Replacement Policy for Networks of
  Caches</title><categories>cs.PF cs.NI</categories><comments>14 pages, 11 figures, accepted in a poster version at ACM SIGMETRICS
  2012</comments><acm-class>C.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The overall performance of content distribution networks as well as recently
proposed information-centric networks rely on both memory and bandwidth
capacities. In this framework, the hit ratio is the key performance indicator
which captures the bandwidth / memory tradeo? for a given global
performance.This paper focuses on the estimation of the hit ratio in a network
of caches that employ the Random replacement policy. Assuming that requests are
independent and identically distributed, general expressions of miss
probabilities for a single Random cache are provided as well as exact results
for specif?c popularity distributions. Moreover, for any Zipf popularity
distribution with exponent ? &gt; 1, we obtain asymptotic equivalents for the miss
probability in the case of large cache size. We extend the analysis to networks
of Random caches, when the topology is either a line or a homogeneous tree. In
that case, approximations for miss probabilities across the network are derived
by assuming that miss events at any node occur independently in time; the
obtained results are compared to the same network using the Least-Recently-Used
discipline, already addressed in the literature. We further analyze the case of
a mixed tandem cache network where the two nodes employ either Random or
Least-Recently-Used policies. In all scenarios, asymptotic formulas and
approximations are extensively compared to simulations and shown to perform
very well. Finally, our results enable us to propose recommendations for cache
replacement disciplines in a network dedicated to content distribution. These
results also hold for a cache using the First-In-First-Out policy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.4883</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.4883</id><created>2012-02-22</created><updated>2012-12-12</updated><authors><author><keyname>Yamakami</keyname><forenames>Tomoyuki</forenames></author><author><keyname>Kato</keyname><forenames>Yuichi</forenames></author></authors><title>The Dissecting Power of Regular Languages</title><categories>cs.FL cs.CC</categories><comments>A4, 10pt, 9 pages, 2 figures</comments><journal-ref>Information Processing Letters, Vol.113, pp.116-122, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A recent study on structural properties of regular and context-free languages
has greatly promoted our basic understandings of the complex behaviors of those
languages. We continue the study to examine how regular languages behave when
they need to cut numerous infinite languages. A particular interest rests on a
situation in which a regular language needs to &quot;dissect&quot; a given infinite
language into two subsets of infinite size. Every context-free language is
dissected by carefully chosen regular languages (or it is REG-dissectible). In
a larger picture, we show that constantly-growing languages and semi-linear
languages are REG-dissectible. Under certain natural conditions, complements
and finite intersections of semi-linear languages also become REG-dissectible.
Restricted to bounded languages, the intersections of finitely many
context-free languages and, more surprisingly, the entire Boolean hierarchy
over bounded context-free languages are REG-dissectible. As an immediate
application of the REG-dissectibility, we show another structural property, in
which an appropriate bounded context-free language can &quot;separate with infinite
margins&quot; two given nested infinite bounded context-free languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.4905</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.4905</id><created>2012-02-22</created><updated>2012-03-01</updated><authors><author><keyname>Asperti</keyname><forenames>Andrea</forenames><affiliation>University of Bologna</affiliation></author><author><keyname>Ricciotti</keyname><forenames>Wilmer</forenames><affiliation>University of Bologna</affiliation></author><author><keyname>Coen</keyname><forenames>Claudio Sacerdoti</forenames><affiliation>University of Bologna</affiliation></author><author><keyname>Tassi</keyname><forenames>Enrico</forenames><affiliation>INRIA - Microsoft Research</affiliation></author></authors><title>A Bi-Directional Refinement Algorithm for the Calculus of (Co)Inductive
  Constructions</title><categories>cs.LO cs.AI</categories><proxy>LMCS</proxy><acm-class>D.3.1, F.3.0</acm-class><journal-ref>Logical Methods in Computer Science, Volume 8, Issue 1 (March 2,
  2012) lmcs:1044</journal-ref><doi>10.2168/LMCS-8(1:18)2012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper describes the refinement algorithm for the Calculus of
(Co)Inductive Constructions (CIC) implemented in the interactive theorem prover
Matita. The refinement algorithm is in charge of giving a meaning to the terms,
types and proof terms directly written by the user or generated by using
tactics, decision procedures or general automation. The terms are written in an
&quot;external syntax&quot; meant to be user friendly that allows omission of
information, untyped binders and a certain liberal use of user defined
sub-typing. The refiner modifies the terms to obtain related well typed terms
in the internal syntax understood by the kernel of the ITP. In particular, it
acts as a type inference algorithm when all the binders are untyped. The
proposed algorithm is bi-directional: given a term in external syntax and a
type expected for the term, it propagates as much typing information as
possible towards the leaves of the term. Traditional mono-directional
algorithms, instead, proceed in a bottom-up way by inferring the type of a
sub-term and comparing (unifying) it with the type expected by its context only
at the end. We propose some novel bi-directional rules for CIC that are
particularly effective. Among the benefits of bi-directionality we have better
error message reporting and better inference of dependent types. Moreover,
thanks to bi-directionality, the coercion system for sub-typing is more
effective and type inference generates simpler unification problems that are
more likely to be solved by the inherently incomplete higher order unification
algorithms implemented. Finally we introduce in the external syntax the notion
of vector of placeholders that enables to omit at once an arbitrary number of
arguments. Vectors of placeholders allow a trivial implementation of implicit
arguments and greatly simplify the implementation of primitive and simple
tactics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.4910</identifier>
 <datestamp>2014-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.4910</id><created>2012-02-22</created><updated>2014-11-06</updated><authors><author><keyname>Hsu</keyname><forenames>Justin</forenames></author><author><keyname>Khanna</keyname><forenames>Sanjeev</forenames></author><author><keyname>Roth</keyname><forenames>Aaron</forenames></author></authors><title>Distributed Private Heavy Hitters</title><categories>cs.DS cs.CR cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we give efficient algorithms and lower bounds for solving the
heavy hitters problem while preserving differential privacy in the fully
distributed local model. In this model, there are n parties, each of which
possesses a single element from a universe of size N. The heavy hitters problem
is to find the identity of the most common element shared amongst the n
parties. In the local model, there is no trusted database administrator, and so
the algorithm must interact with each of the $n$ parties separately, using a
differentially private protocol. We give tight information-theoretic upper and
lower bounds on the accuracy to which this problem can be solved in the local
model (giving a separation between the local model and the more common
centralized model of privacy), as well as computationally efficient algorithms
even in the case where the data universe N may be exponentially large.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.4912</identifier>
 <datestamp>2012-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.4912</id><created>2012-02-22</created><authors><author><keyname>Millo</keyname><forenames>Jean-Vivien</forenames><affiliation>INRIA Sophia Antipolis</affiliation></author><author><keyname>De Simone</keyname><forenames>Robert</forenames><affiliation>INRIA Rocquencourt / INRIA Sophia Antipolis / Laboratoire I3S</affiliation></author></authors><title>Periodic scheduling of marked graphs using balanced binary words</title><categories>cs.FL</categories><comments>No. RR-7891 (2012)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This report presents an algorithm to statically schedule live and strongly
connected Marked Graphs (MG). The proposed algorithm computes the best
execution where the execution rate is maximal and place sizes are minimal. The
proposed algorithm provides transition schedules represented as binary words.
These words are chosen to be balanced. The contributions of this paper is the
proposed algorithm itself along with the characterization of the best execution
of any MG.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.4941</identifier>
 <datestamp>2012-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.4941</id><created>2012-02-22</created><authors><author><keyname>Shankar</keyname><forenames>G.</forenames></author></authors><title>Innovative SQA Service Maturity Model using CMMI and ITIL</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This Journal details a maturity model for SQA services which has been
developed during QMS implementation in the IT division of a large multinational
organization. The scope of the engagement was to establish a standard set of
processes based on CMMI\textregistered and ITIL\textregistered Framework across
four business verticals scattered in Europe, United States and Asia. The
services of Software Quality Analyst (SQA) from different vendors were
leveraged to facilitate implementation of processes which was referred to as
the Quality Management System (QMS). To co-ordinate and support QMS
implementation, a Software Quality Assurance Group (SQAG) was established at
the organizational level. Considering the large number of applications, the
business verticals proposed that process implementation should be owned and
managed by practitioners themselves so that the mass deployment of QMS can be
achieved at a faster rate with the same SQA capacity. This called for a need to
devise an innovative implementation solution before moving to a process
implementation model which proposed Project Managers implementing processes
themself. While there are process models and frameworks available in the market
for establishing processes in an organization, there is no model that
elaborates activities to be performed by the SQA for effective implementation
of processes. SQA service maturity model was proposed as a solution based on
CMMI\textregistered and developed to eventually proceed towards a 'Process
Implementation Model proposing Project Managers implementing processes
themself'.
  SQA Service Maturity Model is a Software Quality Assurance implementation
framework that enables organisations to increase Efficiencies in Software
Quality Assurance, reduce the Cost of Defects and ultimately Increasing Return
on Investment in IT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.4943</identifier>
 <datestamp>2012-08-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.4943</id><created>2012-02-22</created><authors><author><keyname>Kumar</keyname><forenames>Bheshaj</forenames></author><author><keyname>Thakur</keyname><forenames>Kavita</forenames></author><author><keyname>Sinha</keyname><forenames>G. R.</forenames></author></authors><title>A new hybrid jpeg image compression scheme using symbol reduction
  technique</title><categories>cs.MM cs.CV</categories><comments>11 pages,9 figures, SIP 2012 held on 3-4 January 2012,at
  Bangalore,India. arXiv admin note: text overlap with standard references on
  JPEG without attribution</comments><doi>10.5121/csit.2012.2101-10.5121-csit.2012.2141</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lossy JPEG compression is a widely used compression technique. Normally the
JPEG standard technique uses three process mapping reduces interpixel
redundancy, quantization, which is lossy process and entropy encoding, which is
considered lossless process. In this paper, a new technique has been proposed
by combining the JPEG algorithm and Symbol Reduction Huffman technique for
achieving more compression ratio. The symbols reduction technique reduces the
number of symbols by combining together to form a new symbol. As a result of
this technique the number of Huffman code to be generated also reduced. It is
simple fast and easy to implement. The result shows that the performance of
standard JPEG method can be improved by proposed method. This hybrid approach
achieves about 20% more compression ratio than the Standard JPEG.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.4945</identifier>
 <datestamp>2012-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.4945</id><created>2012-02-22</created><authors><author><keyname>Miracle</keyname><forenames>Sarah</forenames></author><author><keyname>Randall</keyname><forenames>Dana</forenames></author><author><keyname>Streib</keyname><forenames>Amanda Pascoe</forenames></author><author><keyname>Tetali</keyname><forenames>Prasad</forenames></author></authors><title>Algorithms for Sampling 3-Orientations of Planar Triangulations</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a planar triangulation, a 3-orientation is an orientation of the
internal edges so all internal vertices have out-degree three. Each
3-orientation gives rise to a unique edge coloring known as a Schnyder wood
that has proven powerful for various computing and combinatorics applications.
We consider natural Markov chains for sampling uniformly from the set of
3-orientations. First, we study a &quot;triangle-reversing&quot; chain on the space of
3-orientations of a fixed triangulation that reverses the orientation of the
edges around a triangle in each move. It was shown previously that this chain
connects the state space and we show that (i) when restricted to planar
triangulations of maximum degree six, the Markov chain is rapidly mixing, and
(ii) there exists a triangulation with high degree on which this Markov chain
mixes slowly. Next, we consider an &quot;edge-flipping&quot; chain on the larger state
space consisting of 3-orientations of all planar triangulations on a fixed
number of vertices. It was also shown previously that this chain connects the
state space and we prove that the chain is always rapidly mixing. The
triangle-reversing and edge-flipping Markov chains both arise in the context of
sampling other combinatorial structures, such as Eulerian orientations and
triangulations of planar point sets, so our results here may shed light on the
mixing rate of these related chains as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.4959</identifier>
 <datestamp>2012-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.4959</id><created>2012-02-22</created><authors><author><keyname>Aref</keyname><forenames>Vahid</forenames></author><author><keyname>Macris</keyname><forenames>Nicolas</forenames></author><author><keyname>Urbanke</keyname><forenames>Rudiger</forenames></author><author><keyname>Vuffray</keyname><forenames>Marc</forenames></author></authors><title>Lossy Source Coding via Spatially Coupled LDGM Ensembles</title><categories>cs.IT cond-mat.stat-mech math.IT</categories><comments>Submitted to ISIT 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a new encoding scheme for lossy source compression based on
spatially coupled low-density generator-matrix codes. We develop a
belief-propagation guided-decimation algorithm, and show that this algorithm
allows to approach the optimal distortion of spatially coupled ensembles.
Moreover, using the survey propagation formalism, we also observe that the
optimal distortions of the spatially coupled and individual code ensembles are
the same. Since regular low-density generator-matrix codes are known to achieve
the Shannon rate-distortion bound under optimal encoding as the degrees grow,
our results suggest that spatial coupling can be used to reach the
rate-distortion bound, under a {\it low complexity} belief-propagation
guided-decimation algorithm.
  This problem is analogous to the MAX-XORSAT problem in computer science.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.4961</identifier>
 <datestamp>2014-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.4961</id><created>2012-02-22</created><updated>2014-05-15</updated><authors><author><keyname>Kaser</keyname><forenames>Owen</forenames></author><author><keyname>Lemire</keyname><forenames>Daniel</forenames></author></authors><title>Strongly universal string hashing is fast</title><categories>cs.DB cs.DS</categories><comments>Software is available at
  http://code.google.com/p/variablelengthstringhashing/ and
  https://github.com/lemire/StronglyUniversalStringHashing</comments><journal-ref>Computer Journal (2014) 57 (11): 1624-1638</journal-ref><doi>10.1093/comjnl/bxt070</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We present fast strongly universal string hashing families: they can process
data at a rate of 0.2 CPU cycle per byte. Maybe surprisingly, we find that
these families---though they require a large buffer of random numbers---are
often faster than popular hash functions with weaker theoretical guarantees.
Moreover, conventional wisdom is that hash functions with fewer multiplications
are faster. Yet we find that they may fail to be faster due to operation
pipelining. We present experimental results on several processors including
low-powered processors. Our tests include hash functions designed for
processors with the Carry-Less Multiplication (CLMUL) instruction set. We also
prove, using accessible proofs, the strong universality of our families.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.4970</identifier>
 <datestamp>2015-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.4970</id><created>2012-02-22</created><updated>2015-03-26</updated><authors><author><keyname>Meka</keyname><forenames>Raghu</forenames></author></authors><title>A polynomial time approximation scheme for computing the supremum of
  Gaussian processes</title><categories>cs.DS math.FA math.PR</categories><comments>Published in at http://dx.doi.org/10.1214/13-AAP997 the Annals of
  Applied Probability (http://www.imstat.org/aap/) by the Institute of
  Mathematical Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AAP-AAP997</report-no><journal-ref>Annals of Applied Probability 2015, Vol. 25, No. 2, 465-476</journal-ref><doi>10.1214/13-AAP997</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a polynomial time approximation scheme (PTAS) for computing the
supremum of a Gaussian process. That is, given a finite set of vectors
$V\subseteq\mathbb{R}^d$, we compute a $(1+\varepsilon)$-factor approximation
to $\mathop {\mathbb{E}}_{X\leftarrow\mathcal{N}^d}[\sup_{v\in V}|\langle
v,X\rangle|]$ deterministically in time $\operatorname
{poly}(d)\cdot|V|^{O_{\varepsilon}(1)}$. Previously, only a constant factor
deterministic polynomial time approximation algorithm was known due to the work
of Ding, Lee and Peres [Ann. of Math. (2) 175 (2012) 1409-1471]. This answers
an open question of Lee (2010) and Ding [Ann. Probab. 42 (2014) 464-496]. The
study of supremum of Gaussian processes is of considerable importance in
probability with applications in functional analysis, convex geometry, and in
light of the recent breakthrough work of Ding, Lee and Peres [Ann. of Math. (2)
175 (2012) 1409-1471], to random walks on finite graphs. As such our result
could be of use elsewhere. In particular, combining with the work of Ding [Ann.
Probab. 42 (2014) 464-496], our result yields a PTAS for computing the cover
time of bounded-degree graphs. Previously, such algorithms were known only for
trees. Along the way, we also give an explicit oblivious estimator for
semi-norms in Gaussian space with optimal query complexity. Our algorithm and
its analysis are elementary in nature, using two classical comparison
inequalities, Slepian's lemma and Kanter's lemma.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.4971</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.4971</id><created>2012-02-22</created><updated>2012-03-17</updated><authors><author><keyname>Audoly</keyname><forenames>Basile</forenames></author><author><keyname>Clauvelin</keyname><forenames>Nicolas</forenames></author><author><keyname>Brun</keyname><forenames>Pierre-Thomas</forenames></author><author><keyname>Bergou</keyname><forenames>Mikl&#xf3;s</forenames></author><author><keyname>Grinspun</keyname><forenames>Eitan</forenames></author><author><keyname>Wardetzky</keyname><forenames>Max</forenames></author></authors><title>A discrete geometric approach for simulating the dynamics of thin
  viscous threads</title><categories>physics.flu-dyn cs.CG math.DG nlin.PS</categories><doi>10.1016/j.jcp.2013.06.034</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a numerical model for the dynamics of thin viscous threads based
on a discrete, Lagrangian formulation of the smooth equations. The model makes
use of a condensed set of coordinates, called the centerline/spin
representation: the kinematical constraints linking the centerline's tangent to
the orientation of the material frame is used to eliminate two out of three
degrees of freedom associated with rotations. Based on a description of twist
inspired from discrete differential geometry and from variational principles,
we build a full-fledged discrete viscous thread model, which includes in
particular a discrete representation of the internal viscous stress.
Consistency of the discrete model with the classical, smooth equations is
established formally in the limit of a vanishing discretization length. The
discrete models lends itself naturally to numerical implementation. Our
numerical method is validated against reference solutions for steady coiling.
The method makes it possible to simulate the unsteady behavior of thin viscous
jets in a robust and efficient way, including the combined effects of inertia,
stretching, bending, twisting, large rotations and surface tension.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.4974</identifier>
 <datestamp>2012-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.4974</id><created>2012-02-22</created><authors><author><keyname>Coupechoux</keyname><forenames>Emilie</forenames></author><author><keyname>Lelarge</keyname><forenames>Marc</forenames></author></authors><title>How Clustering Affects Epidemics in Random Networks</title><categories>math.PR cs.SI physics.soc-ph</categories><comments>30 pages</comments><msc-class>60C05, 05C80, 91D30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by the analysis of social networks, we study a model of random
networks that has both a given degree distribution and a tunable clustering
coefficient. We consider two types of growth processes on these graphs:
diffusion and symmetric threshold model. The diffusion process is inspired from
epidemic models. It is characterized by an infection probability, each neighbor
transmitting the epidemic independently. In the symmetric threshold process,
the interactions are still local but the propagation rule is governed by a
threshold (that might vary among the different nodes). An interesting example
of symmetric threshold process is the contagion process, which is inspired by a
simple coordination game played on the network. Both types of processes have
been used to model spread of new ideas, technologies, viruses or worms and
results have been obtained for random graphs with no clustering. In this paper,
we are able to analyze the impact of clustering on the growth processes. While
clustering inhibits the diffusion process, its impact for the contagion process
is more subtle and depends on the connectivity of the graph: in a low
connectivity regime, clustering also inhibits the contagion, while in a high
connectivity regime, clustering favors the appearance of global cascades but
reduces their size.
  For both diffusion and symmetric threshold models, we characterize conditions
under which global cascades are possible and compute their size explicitly, as
a function of the degree distribution and the clustering coefficient. Our
results are applied to regular or power-law graphs with exponential cutoff and
shed new light on the impact of clustering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.4997</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.4997</id><created>2012-02-22</created><authors><author><keyname>Ghosh</keyname><forenames>Arpita</forenames></author><author><keyname>McAfee</keyname><forenames>Preston</forenames></author></authors><title>Crowdsourcing with Endogenous Entry</title><categories>cs.GT</categories><comments>In Proceedings of the 21st International World Wide Web Conference:
  WWW 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the design of mechanisms to incentivize high quality in
crowdsourcing environments with strategic agents, when entry is an endogenous,
strategic choice. Modeling endogenous entry in crowdsourcing is important
because there is a nonzero cost to making a contribution of any quality which
can be avoided by not participating, and indeed many sites based on
crowdsourced content do not have adequate participation. We use a mechanism
with monotone, rank-based, rewards in a model where agents strategically make
participation and quality choices to capture a wide variety of crowdsourcing
environments, ranging from conventional crowdsourcing contests to crowdsourced
content as in online Q&amp;A forums.
  We first explicitly construct the unique mixed-strategy equilibrium for such
monotone rank-order mechanisms, and use these participation probabilities and
quality distribution to address the design of incentives for two kinds of
rewards that arise in crowdsourcing. We first show that for attention rewards
as in crowdsourced content, the entire equilibrium distribution improves when
the rewards for every rank but the last are as high as possible. In particular,
when producing the lowest quality content has low cost, the optimal mechanism
displays all but the worst contribution. We next investigate settings where
there is a total reward that can be arbitrarily distributed amongst all
participants, as in crowdsourcing contests. Unlike with exogenous entry, here
the expected number of participants can be increased by subsidizing entry,
which could potentially improve the expected quality of the best contribution.
However, we show that free entry is dominated by taxing entry- making all
entrants pay a small fee, which is rebated to the winner along with whatever
rewards were already assigned, can improve the quality of the best contribution
over a winner-take-all contest with no taxes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5003</identifier>
 <datestamp>2012-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5003</id><created>2012-02-22</created><authors><author><keyname>Schmidt</keyname><forenames>Jens M.</forenames></author></authors><title>A Planarity Test via Construction Sequences</title><categories>cs.DS cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optimal linear-time algorithms for testing the planarity of a graph are
well-known for over 35 years. However, these algorithms are quite involved and
recent publications still try to give simpler linear-time tests. We give a
simple reduction from planarity testing to the problem of computing a certain
construction of a 3-connected graph. The approach is different from previous
planarity tests; as key concept, we maintain a planar embedding that is
3-connected at each point in time. The algorithm runs in linear time and
computes a planar embedding if the input graph is planar and a
Kuratowski-subdivision otherwise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5012</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5012</id><created>2012-02-22</created><updated>2013-11-13</updated><authors><author><keyname>Padilla</keyname><forenames>Jennifer E.</forenames></author><author><keyname>Patitz</keyname><forenames>Matthew J.</forenames></author><author><keyname>Pena</keyname><forenames>Raul</forenames></author><author><keyname>Schweller</keyname><forenames>Robert T.</forenames></author><author><keyname>Seeman</keyname><forenames>Nadrian C.</forenames></author><author><keyname>Sheline</keyname><forenames>Robert</forenames></author><author><keyname>Summers</keyname><forenames>Scott M.</forenames></author><author><keyname>Zhong</keyname><forenames>Xingsi</forenames></author></authors><title>Asynchronous Signal Passing for Tile Self-Assembly: Fuel Efficient
  Computation and Efficient Assembly of Shapes</title><categories>cs.ET</categories><comments>This version contains the appendices omitted from the version
  appearing in the UCNC 2013 proceedings</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we demonstrate the power of a model of tile self-assembly based
on active glues which can dynamically change state. We formulate the
Signal-passing Tile Assembly Model (STAM), based on the model of Padilla, Liu,
and Seeman to be asynchronous, allowing any action of turning a glue on or off,
attaching a new tile, or breaking apart an assembly to happen in any order.
Within this highly generalized model we provide three new solutions to tile
self-assembly problems that have been addressed within the abstract Tile
Assembly Model and its variants, showing that signal passing tiles allow for
substantial improvement across multiple complexity metrics. Our first result
utilizes a recursive assembly process to achieve tile-type efficient assembly
of linear structures, using provably fewer tile types than what is possible in
standard tile assembly models. Our second system of signal-passing tiles
simulates any Turing machine with high fuel efficiency by using only a constant
number of tiles per computation step. Our third system assembles the discrete
Sierpinski triangle, demonstrating that this pattern can be strictly
self-assembled within the STAM. This result is of particular interest in that
it is known that this pattern cannot self-assemble within a number of well
studied tile self-assembly models. Notably, all of our constructions are at
temperature 1, further demonstrating that signal-passing confers the power to
bypass many restrictions found in standard tile assembly models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5014</identifier>
 <datestamp>2012-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5014</id><created>2012-02-22</created><authors><author><keyname>Suh</keyname><forenames>Changho</forenames></author><author><keyname>Wang</keyname><forenames>I-Hsiang</forenames></author><author><keyname>Tse</keyname><forenames>David</forenames></author></authors><title>Two-way Interference Channels</title><categories>cs.IT math.IT</categories><comments>submitted to the IEEE International Symposium on Information Theory
  2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider two-way interference channels (ICs) where forward and backward
channels are ICs but not necessarily the same. We first consider a scenario
where there are only two forward messages and feedback is offered through the
backward IC for aiding forward-message transmission. For a linear deterministic
model of this channel, we develop inner and outer bounds that match for a wide
range of channel parameters. We find that the backward IC can be more
efficiently used for feedback rather than if it were used for sending its own
independent backward messages. As a consequence, we show that feedback can
provide a net increase in capacity even if feedback cost is taken into
consideration. Moreover we extend this to a more general scenario with two
additional independent backward messages, from which we find that interaction
can provide an arbitrarily large gain in capacity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5025</identifier>
 <datestamp>2012-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5025</id><created>2012-02-22</created><authors><author><keyname>Kliemann</keyname><forenames>Lasse</forenames></author></authors><title>The Price of Anarchy for Network Formation in an Adversary Model</title><categories>cs.GT</categories><acm-class>G.2.2; C.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study network formation with n players and link cost \alpha &gt; 0. After the
network is built, an adversary randomly deletes one link according to a certain
probability distribution. Cost for player v incorporates the expected number of
players to which v will become disconnected. We show existence of equilibria
and a price of stability of 1+o(1) under moderate assumptions on the adversary
and n \geq 9.
  As the main result, we prove bounds on the price of anarchy for two special
adversaries: one removes a link chosen uniformly at random, while the other
removes a link that causes a maximum number of player pairs to be separated.
For unilateral link formation we show a bound of O(1) on the price of anarchy
for both adversaries, the constant being bounded by 10+o(1) and 8+o(1),
respectively. For bilateral link formation we show O(1+\sqrt{n/\alpha}) for one
adversary (if \alpha &gt; 1/2), and \Theta(n) for the other (if \alpha &gt; 2
considered constant and n \geq 9). The latter is the worst that can happen for
any adversary in this model (if \alpha = \Omega(1)). This points out
substantial differences between unilateral and bilateral link formation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5041</identifier>
 <datestamp>2012-04-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5041</id><created>2012-02-22</created><updated>2012-04-18</updated><authors><author><keyname>Marinazzo</keyname><forenames>Daniele</forenames></author><author><keyname>Pellicoro</keyname><forenames>Mario</forenames></author><author><keyname>Wu</keyname><forenames>Guorong</forenames></author><author><keyname>Angelini</keyname><forenames>Leonardo</forenames></author><author><keyname>Stramaglia</keyname><forenames>Sebastiano</forenames></author></authors><title>Information flow in a network model and the law of diminishing marginal
  returns</title><categories>physics.data-an cs.SI physics.soc-ph q-bio.NC</categories><comments>5 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze a simple dynamical network model which describes the limited
capacity of nodes to process the input information. For a suitable choice of
the parameters, the information flow pattern is characterized by exponential
distribution of the incoming information and a fat-tailed distribution of the
outgoing information, as a signature of the law of diminishing marginal
returns. The analysis of a real EEG data-set shows that similar phenomena may
be relevant for brain signals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5049</identifier>
 <datestamp>2012-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5049</id><created>2012-02-22</created><authors><author><keyname>Fung</keyname><forenames>Isaac</forenames></author><author><keyname>Georgiou</keyname><forenames>Konstantinos</forenames></author><author><keyname>Koenemann</keyname><forenames>Jochen</forenames></author><author><keyname>Sharpe</keyname><forenames>Malcolm</forenames></author></authors><title>Efficient Algorithms for Solving Hypergraphic Steiner Tree Relaxations
  in Quasi-Bipartite Instances</title><categories>cs.DM cs.DS</categories><comments>15 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the Steiner tree problem in quasi-bipartite graphs, where no two
Steiner vertices are connected by an edge. For this class of instances, we
present an efficient algorithm to exactly solve the so called directed
component relaxation (DCR), a specific form of hypergraphic LP relaxation that
was instrumental in the recent break-through result by Byrka et al. [BGRS10]
(STOC 2010). Our algorithm hinges on an efficiently computable map from extreme
points of the bidirected cut relaxation to feasible solutions of (DCR). As a
consequence, together with [BGRS10] we immediately obtain an efficient
73/60-approximation for quasi-bipartite Steiner tree instances. We also present
a particularly simple (BCR)-based random sampling algorithm that achieves a
performance guarantee slightly better than 77/60.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5074</identifier>
 <datestamp>2012-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5074</id><created>2012-02-22</created><updated>2012-03-04</updated><authors><author><keyname>Eppstein</keyname><forenames>David</forenames></author></authors><title>Solving Single-digit Sudoku Subproblems</title><categories>cs.DS</categories><comments>12 pages, 5 figures. To appear at the 6th International Conference on
  Fun with Algorithms (FUN 2012). This revision simplifies the algorithm
  description and adds more references to related prior work</comments><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that single-digit &quot;Nishio&quot; subproblems in nxn Sudoku puzzles may be
solved in time o(2^n), faster than previous solutions such as the pattern
overlay method. We also show that single-digit deduction in Sudoku is NP-hard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5094</identifier>
 <datestamp>2012-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5094</id><created>2012-02-23</created><authors><author><keyname>Kanrar</keyname><forenames>Soumen</forenames></author></authors><title>Analysis and implementation of the Large Scale Video-on-Demand System</title><categories>cs.NI cs.MM</categories><comments>9 pages, 8 figures</comments><journal-ref>International Journal of Applied Information Systems (IJAIS) -
  ISSN :2249-0868 Volume 1- No.4, 2012</journal-ref><doi>10.5120/ijais12-450168</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Next Generation Network (NGN) provides multimedia services over broadband
based networks, which supports high definition TV (HDTV), and DVD quality
video-on-demand content. The video services are thus seen as merging mainly
three areas such as computing, communication, and broadcasting. It has numerous
advantages and more exploration for the large-scale deployment of
video-on-demand system is still needed. This is due to its economic and design
constraints. It's need significant initial investments for full service
provision. This paper presents different estimation for the different
topologies and it require efficient planning for a VOD system network. The
methodology investigates the network bandwidth requirements of a VOD system
based on centralized servers, and distributed local proxies. Network traffic
models are developed to evaluate the VOD system's operational bandwidth
requirements for these two network architectures. This paper present an
efficient estimation of the of the bandwidth requirement for the different
architectures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5110</identifier>
 <datestamp>2012-11-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5110</id><created>2012-02-23</created><updated>2012-11-06</updated><authors><author><keyname>Li</keyname><forenames>Gang</forenames></author><author><keyname>Rao</keyname><forenames>Wei</forenames></author><author><keyname>Wang</keyname><forenames>Xiqin</forenames></author><author><keyname>Xia</keyname><forenames>Xiang-Gen</forenames></author></authors><title>ISAR Image Formation Using Sequential Minimization of L0 and L2 Norms</title><categories>cs.IT math.IT</categories><comments>This paper has been withdrawn by the authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A sparsity-driven algorithm of inverse synthetic aperture radar (ISAR)
imaging is proposed. Based on the parametric sparse representation of the
received ISAR signal, the problem of ISAR image formation is converted into the
joint estimation of the target rotation rate and the sparse power distribution
in the spatial domain. This goal is achieved by sequential minimization of L0
and L2 norms, which ensure the sparsest ISAR image and the minimum recovery
error, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5127</identifier>
 <datestamp>2012-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5127</id><created>2012-02-23</created><updated>2012-02-27</updated><authors><author><keyname>Bonichon</keyname><forenames>Nicolas</forenames><affiliation>LaBRI, INRIA Bordeaux - Sud-Ouest</affiliation></author><author><keyname>Gavoille</keyname><forenames>Cyril</forenames><affiliation>LaBRI, INRIA Bordeaux - Sud-Ouest, IUF</affiliation></author><author><keyname>Hanusse</keyname><forenames>Nicolas</forenames><affiliation>LaBRI, INRIA Bordeaux - Sud-Ouest</affiliation></author><author><keyname>Perkovic</keyname><forenames>Ljubomir</forenames><affiliation>CTI, SOC</affiliation></author></authors><title>The Stretch Factor of $L_1$- and $L_\infty$-Delaunay Triangulations</title><categories>cs.CG</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we determine the stretch factor of the $L_1$-Delaunay and
$L_\infty$-Delaunay triangulations, and we show that this stretch is
$\sqrt{4+2\sqrt{2}} \approx 2.61$. Between any two points $x,y$ of such
triangulations, we construct a path whose length is no more than
$\sqrt{4+2\sqrt{2}}$ times the Euclidean distance between $x$ and $y$, and this
bound is best possible. This definitively improves the 25-year old bound of
$\sqrt{10}$ by Chew (SoCG '86). To the best of our knowledge, this is the first
time the stretch factor of the well-studied $L_p$-Delaunay triangulations, for
any real $p\ge 1$, is determined exactly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5150</identifier>
 <datestamp>2014-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5150</id><created>2012-02-23</created><updated>2014-01-13</updated><authors><author><keyname>Stefanov</keyname><forenames>Emil</forenames></author><author><keyname>van Dijk</keyname><forenames>Marten</forenames></author><author><keyname>Shi</keyname><forenames>Elaine</forenames></author><author><keyname>Chan</keyname><forenames>T-H. Hubert</forenames></author><author><keyname>Fletcher</keyname><forenames>Christopher</forenames></author><author><keyname>Ren</keyname><forenames>Ling</forenames></author><author><keyname>Yu</keyname><forenames>Xiangyao</forenames></author><author><keyname>Devadas</keyname><forenames>Srinivas</forenames></author></authors><title>Path ORAM: An Extremely Simple Oblivious RAM Protocol</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present Path ORAM, an extremely simple Oblivious RAM protocol with a small
amount of client storage. Partly due to its simplicity, Path ORAM is the most
practical ORAM scheme known to date with small client storage. We formally
prove that Path ORAM has a O(log N) bandwidth cost for blocks of size B =
Omega(log^2 N) bits. For such block sizes, Path ORAM is asymptotically better
than the best known ORAM schemes with small client storage. Due to its
practicality, Path ORAM has been adopted in the design of secure processors
since its proposal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5154</identifier>
 <datestamp>2014-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5154</id><created>2012-02-23</created><updated>2014-12-19</updated><authors><author><keyname>F&#xe9;dou</keyname><forenames>Jean-Marc</forenames></author><author><keyname>Fici</keyname><forenames>Gabriele</forenames></author></authors><title>Vertical representation of $C^{\infty}$-words</title><categories>cs.DM cs.FL</categories><comments>Published in Theoretical Computer Science</comments><msc-class>68R15</msc-class><journal-ref>Theoretical Computer Science 565: 90-101, 2015</journal-ref><doi>10.1016/j.tcs.2014.11.011</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new framework for dealing with $C^{\infty}$-words, based on
their left and right frontiers. This allows us to give a compact representation
of them, and to describe the set of $C^{\infty}$-words through an infinite
directed acyclic graph $G$. This graph is defined by a map acting on the
frontiers of $C^{\infty}$-words. We show that this map can be defined
recursively and with no explicit references to $C^{\infty}$-words. We then show
that some important conjectures on $C^{\infty}$-words follow from analogous
statements on the structure of the graph $G$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5184</identifier>
 <datestamp>2014-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5184</id><created>2012-02-23</created><updated>2014-09-10</updated><authors><author><keyname>Rizzi</keyname><forenames>Romeo</forenames></author><author><keyname>Sikora</keyname><forenames>Florian</forenames></author></authors><title>Some results on more flexible versions of Graph Motif</title><categories>cs.CC</categories><doi>10.1007/s00224-014-9564-6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problems studied in this paper originate from Graph Motif, a problem
introduced in 2006 in the context of biological networks. Informally speaking,
it consists in deciding if a multiset of colors occurs in a connected subgraph
of a vertex-colored graph. Due to the high rate of noise in the biological
data, more flexible definitions of the problem have been outlined. We present
in this paper two inapproximability results for two different optimization
variants of Graph Motif: one where the size of the solution is maximized, the
other when the number of substitutions of colors to obtain the motif from the
solution is minimized. We also study a decision version of Graph Motif where
the connectivity constraint is replaced by the well known notion of graph
modularity. While the problem remains NP-complete, it allows algorithms in FPT
for biologically relevant parameterizations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5187</identifier>
 <datestamp>2013-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5187</id><created>2012-02-23</created><updated>2013-01-16</updated><authors><author><keyname>Rajashekar</keyname><forenames>Rakshith</forenames></author><author><keyname>Hari</keyname><forenames>K. V. S.</forenames></author></authors><title>Sphere Decoding for Spatial Modulation Systems with Arbitrary Nt</title><categories>cs.IT math.IT</categories><comments>This paper has been withdrawn by the authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, three Sphere Decoding (SD) algorithms were proposed for Spatial
Modulation (SM) scheme which focus on reducing the transmit-, receive-, and
both transmit and receive-search spaces at the receiver and were termed as
Receiver-centric SD (Rx-SD), Transmitter-centric SD (Tx-SD), and Combined SD
(C-SD) detectors, respectively. The Tx-SD detector was proposed for systems
with Nt \leq Nr, where Nt and Nr are the number of transmit and receive
antennas of the system. In this paper, we show that the existing Tx-SD detector
is not limited to systems with Nt \leq Nr but can be used with systems Nr &lt; Nt
\leq 2Nr - 1 as well. We refer to this detector as the Extended Tx-SD (E-Tx-SD)
detector. Further, we propose an E- Tx-SD based detection scheme for SM systems
with arbitrary Nt by exploiting the Inter-Channel Interference (ICI) free
property of the SM systems. We show with our simulation results that the
proposed detectors are ML-optimal and offer significantly reduced complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5194</identifier>
 <datestamp>2012-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5194</id><created>2012-02-19</created><authors><author><keyname>Mondal</keyname><forenames>Uttam Kr.</forenames></author><author><keyname>Mandal</keyname><forenames>J. K.</forenames></author></authors><title>Fabrication of Message Digest to Authenticate Audio Signals with
  Alternation of Coefficients of Harmonics in Multi-Stages (MDAC)</title><categories>cs.CR</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Providing security to audio songs for maintaining its intellectual property
right (IPR) is one of chanllenging fields in commercial world especially in
creative industry. In this paper, an effective approach has been incorporated
to fabricate authentication of audio song through application of message digest
method with alternation of coefficients of harmonics in multi-stages of higher
frequency domain without affecting its audible quality. Decomposing constituent
frequency components of song signal using Fourier transform with generating
secret code via applying message digest followed by alternating coefficients of
specific harmonics in multi-stages generates a secret code and this unique code
is utilized to detect the originality of the song. A comparative study has been
made with similar existing techniques and experimental results are also
supported with mathematical formula based on Microsoft WAVE (&quot;.wav&quot;) stereo
sound file.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5198</identifier>
 <datestamp>2012-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5198</id><created>2012-02-21</created><authors><author><keyname>Ghaffari</keyname><forenames>H. O.</forenames></author><author><keyname>Young</keyname><forenames>R. P.</forenames></author></authors><title>Network Theory, Cracking and Frictional Sliding</title><categories>physics.geo-ph cs.CE nlin.AO</categories><comments>2012 ARMA, This paper was prepared for presentation at the 46th US
  Rock Mechanics / Geomechanics Symposium held in Chicago, IL, USA, 24-27 June
  2012</comments><report-no>ARMA 12-268</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We have developed different network approaches to complex patterns of
frictional interfaces (contact areas developments). Here, we analyze the
dynamics of static friction. We found, under the correlation measure, the
fraction of triangles correlates with the detachment fronts. Also, for all
types of the loops (such as triangles), there is a universal power law between
nodes' degree and motifs where motifs frequency follow a power law. This shows
high energy localization is characterized by fast variation of the loops
fraction. Also, this proves that the congestion of loops occurs around hubs.
Furthermore, the motif distributions and modularity space of networks -in terms
of within-module degree and participation coefficient- show universal trends,
indicating an in common aspect of energy flow in shear ruptures. Moreover, we
confirmed that slow ruptures generally hold small localization, while regular
ruptures carry a high level of energy localization. We proposed that
assortativity, as an index to correlation of node's degree, can uncover
acoustic features of the interfaces. We showed that increasing assortativity
induces a nearly silent period of fault's activities. Also, we proposed that
slow ruptures resulted from within-module developments rather than
extra-modules of the networks. Our approach presents a completely new
perspective of the evolution of shear ruptures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5202</identifier>
 <datestamp>2012-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5202</id><created>2012-02-22</created><authors><author><keyname>Cai</keyname><forenames>Sheng</forenames></author><author><keyname>Ye</keyname><forenames>Jihang</forenames></author><author><keyname>Chen</keyname><forenames>Minghua</forenames></author><author><keyname>Yan</keyname><forenames>Jianxin</forenames></author><author><keyname>Jaggi</keyname><forenames>Sidharth</forenames></author></authors><title>Secure Compressed Reading in Smart Grids</title><categories>cs.IT cs.PF math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Smart Grids measure energy usage in real-time and tailor supply and delivery
accordingly, in order to improve power transmission and distribution. For the
grids to operate effectively, it is critical to collect readings from
massively-installed smart meters to control centers in an efficient and secure
manner. In this paper, we propose a secure compressed reading scheme to address
this critical issue. We observe that our collected real-world meter data
express strong temporal correlations, indicating they are sparse in certain
domains. We adopt Compressed Sensing technique to exploit this sparsity and
design an efficient meter data transmission scheme. Our scheme achieves
substantial efficiency offered by compressed sensing, without the need to know
beforehand in which domain the meter data are sparse. This is in contrast to
traditional compressed-sensing based scheme where such sparse-domain
information is required a priori. We then design specific dependable scheme to
work with our compressed sensing based data transmission scheme to make our
meter reading reliable and secure. We provide performance guarantee for the
correctness, efficiency, and security of our proposed scheme. Through analysis
and simulations, we demonstrate the effectiveness of our schemes and compare
their performance to prior arts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5216</identifier>
 <datestamp>2012-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5216</id><created>2012-02-23</created><authors><author><keyname>O'Callaghan</keyname><forenames>Derek</forenames></author><author><keyname>Harrigan</keyname><forenames>Martin</forenames></author><author><keyname>Carthy</keyname><forenames>Joe</forenames></author><author><keyname>Cunningham</keyname><forenames>P&#xe1;draig</forenames></author></authors><title>Identifying Discriminating Network Motifs in YouTube Spam</title><categories>cs.SI</categories><comments>8 pages, 5 figures. arXiv admin note: significant text overlap with
  arXiv:1201.3783</comments><acm-class>C.2.0; H.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Like other social media websites, YouTube is not immune from the attention of
spammers. In particular, evidence can be found of attempts to attract users to
malicious third-party websites. As this type of spam is often associated with
orchestrated campaigns, it has a discernible network signature, based on
networks derived from comments posted by users to videos. In this paper, we
examine examples of different YouTube spam campaigns of this nature, and use a
feature selection process to identify network motifs that are characteristic of
the corresponding campaign strategies. We demonstrate how these discriminating
motifs can be used as part of a network motif profiling process that tracks the
activity of spam user accounts over time, enabling the process to scale to
larger networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5230</identifier>
 <datestamp>2014-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5230</id><created>2012-02-23</created><updated>2012-10-18</updated><authors><author><keyname>Seshadhri</keyname><forenames>C.</forenames></author><author><keyname>Pinar</keyname><forenames>Ali</forenames></author><author><keyname>Kolda</keyname><forenames>Tamara G.</forenames></author></authors><title>Triadic Measures on Graphs: The Power of Wedge Sampling</title><categories>cs.SI cs.DM</categories><journal-ref>SDM13: Proceedings of the 2013 SIAM International Conference on
  Data Mining, pp. 10-18, May 2013</journal-ref><doi>10.1137/1.9781611972832.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graphs are used to model interactions in a variety of contexts, and there is
a growing need to quickly assess the structure of a graph. Some of the most
useful graph metrics, especially those measuring social cohesion, are based on
triangles. Despite the importance of these triadic measures, associated
algorithms can be extremely expensive. We propose a new method based on wedge
sampling. This versatile technique allows for the fast and accurate
approximation of all current variants of clustering coefficients and enables
rapid uniform sampling of the triangles of a graph. Our methods come with
provable and practical time-approximation tradeoffs for all computations. We
provide extensive results that show our methods are orders of magnitude faster
than the state-of-the-art, while providing nearly the accuracy of full
enumeration. Our results will enable more wide-scale adoption of triadic
measures for analysis of extremely large graphs, as demonstrated on several
real-world examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5233</identifier>
 <datestamp>2012-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5233</id><created>2012-02-23</created><updated>2012-11-30</updated><authors><author><keyname>Starikovskaya</keyname><forenames>Tatiana</forenames></author></authors><title>Computing Lempel-Ziv Factorization Online</title><categories>cs.DS</categories><doi>10.1007/978-3-642-32589-2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an algorithm which computes the Lempel-Ziv factorization of a word
$W$ of length $n$ on an alphabet $\Sigma$ of size $\sigma$ online in the
following sense: it reads $W$ starting from the left, and, after reading each
$r = O(\log_{\sigma} n)$ characters of $W$, updates the Lempel-Ziv
factorization. The algorithm requires $O(n \log \sigma)$ bits of space and O(n
\log^2 n) time. The basis of the algorithm is a sparse suffix tree combined
with wavelet trees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5249</identifier>
 <datestamp>2015-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5249</id><created>2012-02-23</created><updated>2014-05-07</updated><authors><author><keyname>Salzman</keyname><forenames>Oren</forenames></author><author><keyname>Hemmer</keyname><forenames>Michael</forenames></author><author><keyname>Halperin</keyname><forenames>Dan</forenames></author></authors><title>On the Power of Manifold Samples in Exploring Configuration Spaces and
  the Dimensionality of Narrow Passages</title><categories>cs.RO cs.CG</categories><comments>20 pages</comments><doi>10.1109/TASE.2014.2331983</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We extend our study of Motion Planning via Manifold Samples (MMS), a general
algorithmic framework that combines geometric methods for the exact and
complete analysis of low-dimensional configuration spaces with sampling-based
approaches that are appropriate for higher dimensions. The framework explores
the configuration space by taking samples that are entire low-dimensional
manifolds of the configuration space capturing its connectivity much better
than isolated point samples. The contributions of this paper are as follows:
(i) We present a recursive application of MMS in a six-dimensional
configuration space, enabling the coordination of two polygonal robots
translating and rotating amidst polygonal obstacles. In the adduced experiments
for the more demanding test cases MMS clearly outperforms PRM, with over
20-fold speedup in a coordination-tight setting. (ii) A probabilistic
completeness proof for the most prevalent case, namely MMS with samples that
are affine subspaces. (iii) A closer examination of the test cases reveals that
MMS has, in comparison to standard sampling-based algorithms, a significant
advantage in scenarios containing high-dimensional narrow passages. This
provokes a novel characterization of narrow passages which attempts to capture
their dimensionality, an attribute that had been (to a large extent) unattended
in previous definitions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5258</identifier>
 <datestamp>2013-08-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5258</id><created>2012-02-23</created><updated>2013-08-08</updated><authors><author><keyname>De</keyname><forenames>Anindya</forenames></author><author><keyname>Mossel</keyname><forenames>Elchanan</forenames></author></authors><title>Explicit Optimal Hardness via Gaussian stability results</title><categories>cs.CC</categories><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The results of Raghavendra (2008) show that assuming Khot's Unique Games
Conjecture (2002), for every constraint satisfaction problem there exists a
generic semi-definite program that achieves the optimal approximation factor.
This result is existential as it does not provide an explicit optimal rounding
procedure nor does it allow to calculate exactly the Unique Games hardness of
the problem.
  Obtaining an explicit optimal approximation scheme and the corresponding
approximation factor is a difficult challenge for each specific approximation
problem. An approach for determining the exact approximation factor and the
corresponding optimal rounding was established in the analysis of MAX-CUT (KKMO
2004) and the use of the Invariance Principle (MOO 2005). However, this
approach crucially relies on results explicitly proving optimal partitions in
Gaussian space. Until recently, Borell's result (Borell 1985) was the only
non-trivial Gaussian partition result known.
  In this paper we derive the first explicit optimal approximation algorithm
and the corresponding approximation factor using a new result on Gaussian
partitions due to Isaksson and Mossel (2012). This Gaussian result allows us to
determine exactly the Unique Games Hardness of MAX-3-EQUAL. In particular, our
results show that Zwick algorithm for this problem achieves the optimal
approximation factor and prove that the approximation achieved by the algorithm
is $\approx 0.796$ as conjectured by Zwick.
  We further use the previously known optimal Gaussian partitions results to
obtain a new Unique Games Hardness factor for MAX-k-CSP : Using the well known
fact that jointly normal pairwise independent random variables are fully
independent, we show that the the UGC hardness of Max-k-CSP is $\frac{\lceil
(k+1)/2 \rceil}{2^{k-1}}$, improving on results of Austrin and Mossel (2009).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5259</identifier>
 <datestamp>2012-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5259</id><created>2012-02-23</created><authors><author><keyname>Etezadi</keyname><forenames>Farrokh</forenames></author><author><keyname>Khisti</keyname><forenames>Ashish</forenames></author><author><keyname>Trott</keyname><forenames>Mitchell</forenames></author></authors><title>Sequential Coding of Markov Sources over Burst Erasure Channels</title><categories>cs.IT math.IT</categories><comments>22 pages, 12 figures, Submitted to IEEE Transaction on Information
  Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study sequential coding of Markov sources under an error propagation
constraint. An encoder sequentially compresses a sequence of vector-sources
that are spatially i.i.d. but temporally correlated according to a first-order
Markov process. The channel erases up to B packets in a single burst, but
reveals all other packets to the destination. The destination is required to
reproduce all the source-vectors instantaneously and in a lossless manner,
except those sequences that occur in an error propagation window of length B +
W following the start of the erasure burst. We define the rate-recovery
function R(B, W) - the minimum achievable compression rate per source sample in
this framework - and develop upper and lower bounds on this function. Our upper
bound is obtained using a random binning technique, whereas our lower bound is
obtained by drawing connections to multi-terminal source coding. Our upper and
lower bounds coincide, yielding R(B, W), in some special cases. More generally,
both the upper and lower bounds equal the rate for predictive coding plus a
term that decreases as 1/(W+1), thus establishing a scaling behaviour of the
rate-recovery function. For a special class of semi-deterministic Markov
sources we propose a new optimal coding scheme: prospicient coding. An
extension of this coding technique to Gaussian sources is also developed. For
the class of symmetric Markov sources and memoryless encoders, we establish the
optimality of random binning. When the destination is required to reproduce
each source sequence with a fixed delay and when W = 0 we also establish the
optimality of binning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5261</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5261</id><created>2012-02-23</created><updated>2013-07-30</updated><authors><author><keyname>Savic</keyname><forenames>Vladimir</forenames></author><author><keyname>Wymeersch</keyname><forenames>Henk</forenames></author><author><keyname>Zazo</keyname><forenames>Santiago</forenames></author></authors><title>Belief Consensus Algorithms for Fast Distributed Target Tracking in
  Wireless Sensor Networks</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In distributed target tracking for wireless sensor networks, agreement on the
target state can be achieved by the construction and maintenance of a
communication path, in order to exchange information regarding local likelihood
functions. Such an approach lacks robustness to failures and is not easily
applicable to ad-hoc networks. To address this, several methods have been
proposed that allow agreement on the global likelihood through fully
distributed belief consensus (BC) algorithms, operating on local likelihoods in
distributed particle filtering (DPF). However, a unified comparison of the
convergence speed and communication cost has not been performed. In this paper,
we provide such a comparison and propose a novel BC algorithm based on belief
propagation (BP). According to our study, DPF based on metropolis belief
consensus (MBC) is the fastest in loopy graphs, while DPF based on BP consensus
is the fastest in tree graphs. Moreover, we found that BC-based DPF methods
have lower communication overhead than data flooding when the network is
sufficiently sparse.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5282</identifier>
 <datestamp>2012-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5282</id><created>2012-02-23</created><updated>2012-06-02</updated><authors><author><keyname>Husain</keyname><forenames>Mohammad Iftekhar</forenames></author><author><keyname>Mandvekar</keyname><forenames>Lokesh</forenames></author><author><keyname>Qiao</keyname><forenames>Chunming</forenames></author><author><keyname>Sridhar</keyname><forenames>Ramalingam</forenames></author></authors><title>How to Bypass Verified Boot Security in Chromium OS</title><categories>cs.CR cs.OS</categories><comments>Update information about Chromium OS. Added new and advanced
  exploits. Added mitigation techniques and evaluation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Verified boot is an interesting feature of Chromium OS that supposedly can
detect any modification in the root file system (rootfs) by a dedicated
adversary. However, by exploiting a design flaw in verified boot, we show that
an adversary can replace the original rootfs by a malicious rootfs containing
exploits such as a spyware or keylogger and still pass the verified boot
process. The exploit is based on the fact that a dedicated adversary can
replace the rootfs and the corresponding verification information in the
bootloader. We experimentally demonstrate an attack using both the base and
developer version of Chromium OS in which the adversary installs a spyware in
the target system to send cached user data to the attacker machine in plain
text which are otherwise encrypted, and thus inaccessible. We also demonstrate
techniques to mitigate this vulnerability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5284</identifier>
 <datestamp>2012-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5284</id><created>2012-02-23</created><updated>2012-04-02</updated><authors><author><keyname>Ter-Sarkisov</keyname><forenames>Aram</forenames></author></authors><title>Elitism Levels Traverse Mechanism For The Derivation of Upper Bounds on
  Unimodal Functions</title><categories>cs.NE cs.AI</categories><comments>accepted to Congress on Evolutionary Computation (WCCI/CEC) 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article we present an Elitism Levels Traverse Mechanism that we
designed to find bounds on population-based Evolutionary algorithms solving
unimodal functions. We prove its efficiency theoretically and test it on OneMax
function deriving bounds c{\mu}n log n - O({\mu} n). This analysis can be
generalized to any similar algorithm using variants of tournament selection and
genetic operators that flip or swap only 1 bit in each string.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5289</identifier>
 <datestamp>2013-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5289</id><created>2012-02-23</created><updated>2013-07-30</updated><authors><author><keyname>Zielinska</keyname><forenames>Elzbieta</forenames></author><author><keyname>Mazurczyk</keyname><forenames>Wojciech</forenames></author><author><keyname>Szczypiorski</keyname><forenames>Krzysztof</forenames></author></authors><title>Development Trends in Steganography</title><categories>cs.MM</categories><comments>13 pages, 1 figure, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Steganography is a general term referring to all methods for the embedding of
additional secret content into some form of carrier, with the aim of
concealment of the introduced alterations. The choice of the carrier is nearly
unlimited, it may be an ancient piece of parchment, as well as a network
protocol header. Inspired by biological phenomena, adopted by man in the
ancient times, it has been developed over the ages. Present day steganographic
methods are far more sophisticated than their ancient predecessors, but the
main principles have remained unchanged. They typically rely on the utilization
of digital media files or network protocols as a carrier, in which secret data
is embedded. This paper presents the evolution of the hidden data carrier from
the ancient times till the present day and pinpoints the observed development
trends, with special emphasis on network steganography.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5298</identifier>
 <datestamp>2012-10-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5298</id><created>2012-02-23</created><updated>2012-10-30</updated><authors><author><keyname>Fonteneau</keyname><forenames>Raphael</forenames></author><author><keyname>Ernst</keyname><forenames>Damien</forenames></author><author><keyname>Boigelot</keyname><forenames>Bernard</forenames></author><author><keyname>Louveaux</keyname><forenames>Quentin</forenames></author></authors><title>Min Max Generalization for Two-stage Deterministic Batch Mode
  Reinforcement Learning: Relaxation Schemes</title><categories>cs.SY cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the minmax optimization problem introduced in [22] for computing
policies for batch mode reinforcement learning in a deterministic setting.
First, we show that this problem is NP-hard. In the two-stage case, we provide
two relaxation schemes. The first relaxation scheme works by dropping some
constraints in order to obtain a problem that is solvable in polynomial time.
The second relaxation scheme, based on a Lagrangian relaxation where all
constraints are dualized, leads to a conic quadratic programming problem. We
also theoretically prove and empirically illustrate that both relaxation
schemes provide better results than those given in [22].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5299</identifier>
 <datestamp>2012-07-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5299</id><created>2012-02-23</created><authors><author><keyname>Gao</keyname><forenames>Jianbo</forenames></author><author><keyname>Hu</keyname><forenames>Jing</forenames></author><author><keyname>Mao</keyname><forenames>Xiang</forenames></author><author><keyname>Perc</keyname><forenames>Matjaz</forenames></author></authors><title>Culturomics meets random fractal theory: Insights into long-range
  correlations of social and natural phenomena over the past two centuries</title><categories>physics.soc-ph cond-mat.stat-mech cs.DL cs.SI stat.AP</categories><comments>8 two-column pages, 5 figures; accepted for publication in Journal of
  the Royal Society Interface [data available at
  http://books.google.com/ngrams]</comments><journal-ref>J. R. Soc. Interface 9 (2012) 1956-1964</journal-ref><doi>10.1098/rsif.2011.0846</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Culturomics was recently introduced as the application of high-throughput
data collection and analysis to the study of human culture. Here we make use of
this data by investigating fluctuations in yearly usage frequencies of specific
words that describe social and natural phenomena, as derived from books that
were published over the course of the past two centuries. We show that the
determination of the Hurst parameter by means of fractal analysis provides
fundamental insights into the nature of long-range correlations contained in
the culturomic trajectories, and by doing so, offers new interpretations as to
what might be the main driving forces behind the examined phenomena. Quite
remarkably, we find that social and natural phenomena are governed by
fundamentally different processes. While natural phenomena have properties that
are typical for processes with persistent long-range correlations, social
phenomena are better described as nonstationary, on-off intermittent, or Levy
walk processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5302</identifier>
 <datestamp>2012-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5302</id><created>2012-02-23</created><authors><author><keyname>Bahi</keyname><forenames>Jacques M.</forenames><affiliation>Authors in alphabetic order</affiliation></author><author><keyname>Couchot</keyname><forenames>Jean-Fran&#xe7;ois</forenames><affiliation>Authors in alphabetic order</affiliation></author><author><keyname>Friot</keyname><forenames>Nicolas</forenames><affiliation>Authors in alphabetic order</affiliation></author><author><keyname>Guyeux</keyname><forenames>Christophe</forenames><affiliation>Authors in alphabetic order</affiliation></author></authors><title>Application of Steganography for Anonymity through the Internet</title><categories>cs.CR cs.IT math.IT</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a novel steganographic scheme based on chaotic iterations is
proposed. This research work takes place into the information hiding security
framework. The applications for anonymity and privacy through the Internet are
regarded too. To guarantee such an anonymity, it should be possible to set up a
secret communication channel into a web page, being both secure and robust. To
achieve this goal, we propose an information hiding scheme being stego-secure,
which is the highest level of security in a well defined and studied category
of attacks called &quot;watermark-only attack&quot;. This category of attacks is the best
context to study steganography-based anonymity through the Internet. The
steganalysis of our steganographic process is also studied in order to show it
security in a real test framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5332</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5332</id><created>2012-02-23</created><authors><author><keyname>Skataric</keyname><forenames>Maja</forenames></author><author><keyname>Sontag</keyname><forenames>Eduardo</forenames></author></authors><title>A Characterization of Scale Invariant Responses in Enzymatic Networks</title><categories>cs.SY cs.CE q-bio.MN</categories><doi>10.1371/journal.pcbi.1002748</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An ubiquitous property of biological sensory systems is adaptation: a step
increase in stimulus triggers an initial change in a biochemical or
physiological response, followed by a more gradual relaxation toward a basal,
pre-stimulus level. Adaptation helps maintain essential variables within
acceptable bounds and allows organisms to readjust themselves to an optimum and
non-saturating sensitivity range when faced with a prolonged change in their
environment. Recently, it was shown theoretically and experimentally that many
adapting systems, both at the organism and single-cell level, enjoy a
remarkable additional feature: scale invariance, meaning that the initial,
transient behavior remains (approximately) the same even when the background
signal level is scaled. In this work, we set out to investigate under what
conditions a broadly used model of biochemical enzymatic networks will exhibit
scale-invariant behavior. An exhaustive computational study led us to discover
a new property of surprising simplicity and generality, uniform linearizations
with fast output (ULFO), whose validity we show is both necessary and
sufficient for scale invariance of enzymatic networks. Based on this study, we
go on to develop a mathematical explanation of how ULFO results in scale
invariance. Our work provides a surprisingly consistent, simple, and general
framework for understanding this phenomenon, and results in concrete
experimental predictions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5337</identifier>
 <datestamp>2013-08-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5337</id><created>2012-02-23</created><updated>2012-09-29</updated><authors><author><keyname>Lov&#xe1;sz</keyname><forenames>L&#xe1;szl&#xf3;</forenames></author><author><keyname>Vesztergombi</keyname><forenames>Katalin</forenames></author></authors><title>Nondeterministic graph property testing</title><categories>math.CO cs.DM</categories><comments>Version 2: 11 pages; we allow orientation in the certificate,
  describe new applications</comments><msc-class>05C85 (primary), 68R10, 05C82 (secondary)</msc-class><journal-ref>Combinatorics, Probability and Computing, volume 22 (2013), issue
  05, pp. 749-762</journal-ref><doi>10.1017/S0963548313000205</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A property of finite graphs is called nondeterministically testable if it has
a &quot;certificate&quot; such that once the certificate is specified, its correctness
can be verified by random local testing. In this paper we study certificates
that consist of one or more unary and/or binary relations on the nodes, in the
case of dense graphs. Using the theory of graph limits, we prove that
nondeterministically testable properties are also deterministically testable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5349</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5349</id><created>2012-02-23</created><updated>2012-10-13</updated><authors><author><keyname>Zlatanov</keyname><forenames>Nikola</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author><author><keyname>Popovski</keyname><forenames>Petar</forenames></author></authors><title>Buffer-Aided Relaying with Adaptive Link Selection</title><categories>cs.IT math.IT</categories><comments>IEEE Journal on Selected Areas in Communications; Special Issue on
  Theories and Methods for Advanced Wireless Relays</comments><doi>10.1109/JSAC.2013.130816</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider a simple network consisting of a source, a
half-duplex decode-and-forward relay, and a destination. We propose a new
relaying protocol employing adaptive link selection, i.e., in any given time
slot, based on the channel state information of the source-relay and the
relay-destination link a decision is made whether the source or the relay
transmits. In order to avoid data loss at the relay, adaptive link selection
requires the relay to be equipped with a buffer such that data can be queued
until the relay-destination link is selected for transmission. We study both
delay constrained and delay unconstrained transmission. For the delay
unconstrained case, we characterize the optimal link selection policy, derive
the corresponding throughput, and develop an optimal power allocation scheme.
For the delay constrained case, we propose to starve the buffer of the relay by
choosing the decision threshold of the link selection policy smaller than the
optimal one and derive a corresponding upper bound on the average delay.
Furthermore, we propose a modified link selection protocol which avoids buffer
overflow by limiting the queue size. Our analytical and numerical results show
that buffer-aided relaying with adaptive link selection achieves significant
throughput gains compared to conventional relaying protocols with and without
buffers where the relay employs a fixed schedule for reception and
transmission.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5358</identifier>
 <datestamp>2012-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5358</id><created>2012-02-23</created><authors><author><keyname>Xiao</keyname><forenames>Yonghui</forenames></author><author><keyname>Xiong</keyname><forenames>Li</forenames></author><author><keyname>Fan</keyname><forenames>Liyue</forenames></author><author><keyname>Goryczka</keyname><forenames>Slawomir</forenames></author></authors><title>DPCube: Differentially Private Histogram Release through
  Multidimensional Partitioning</title><categories>cs.DB</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Differential privacy is a strong notion for protecting individual privacy in
privacy preserving data analysis or publishing. In this paper, we study the
problem of differentially private histogram release for random workloads. We
study two multidimensional partitioning strategies including: 1) a baseline
cell-based partitioning strategy for releasing an equi-width cell histogram,
and 2) an innovative 2-phase kd-tree based partitioning strategy for releasing
a v-optimal histogram. We formally analyze the utility of the released
histograms and quantify the errors for answering linear queries such as
counting queries. We formally characterize the property of the input data that
will guarantee the optimality of the algorithm. Finally, we implement and
experimentally evaluate several applications using the released histograms,
including counting queries, classification, and blocking for record linkage and
show the benefit of our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5360</identifier>
 <datestamp>2012-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5360</id><created>2012-02-23</created><authors><author><keyname>Yang</keyname><forenames>Fei</forenames></author><author><keyname>Cao</keyname><forenames>Yong</forenames></author><author><keyname>Tian</keyname><forenames>Jie</forenames></author></authors><title>Efficient and Effective Volume Visualization with Enhanced Isosurface
  Rendering</title><categories>cs.GR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compared with full volume rendering, isosurface rendering has several well
recognized advantages in efficiency and accuracy. However, standard isosurface
rendering has some limitations in effectiveness. First, it uses a monotone
colored approach and can only visualize the geometry features of an isosurface.
The lack of the capability to illustrate the material property and the internal
structures behind an isosurface has been a big limitation of this method in
applications. Another limitation of isosurface rendering is the difficulty to
reveal physically meaningful structures, which are hidden in one or multiple
isosurfaces. As such, the application requirements of extract and recombine
structures of interest can not be implemented effectively with isosurface
rendering. In this work, we develop an enhanced isosurface rendering technique
to improve the effectiveness while maintaining the performance efficiency of
the standard isosurface rendering. First, an isosurface color enhancement
method is proposed to illustrate the neighborhood density and to reveal some of
the internal structures. Second, we extend the structure extraction capability
of isosurface rendering by enabling explicit scene exploration within a
3D-view, using surface peeling, voxel-selecting, isosurface segmentation, and
multi-surface-structure visualization. Our experiments show that the color
enhancement not only improves the visual fidelity of the rendering, but also
reveals the internal structures without significant increase of the
computational cost. Explicit scene exploration is also demonstrated as a
powerful tool in some application scenarios, such as displaying multiple
abdominal organs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5398</identifier>
 <datestamp>2012-04-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5398</id><created>2012-02-24</created><updated>2012-04-25</updated><authors><author><keyname>Lee</keyname><forenames>Juyong</forenames></author><author><keyname>Gross</keyname><forenames>Steven P.</forenames></author><author><keyname>Lee</keyname><forenames>Jooyoung</forenames></author></authors><title>Mod-CSA: Modularity optimization by conformational space annealing</title><categories>physics.comp-ph cs.SI physics.data-an physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new modularity optimization method, Mod-CSA, based on stochastic
global optimization algorithm, conformational space annealing (CSA). Our method
outperforms simulated annealing in terms of both efficiency and accuracy,
finding higher modularity partitions with less computational resources
required. The high modularity values found by our method are higher than, or
equal to, the largest values previously reported. In addition, the method can
be combined with other heuristic methods, and implemented in parallel fashion,
allowing it to be applicable to large graphs with more than 10000 nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5413</identifier>
 <datestamp>2012-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5413</id><created>2012-02-24</created><authors><author><keyname>Yu</keyname><forenames>Jiun-Hung</forenames></author></authors><title>On the Joint Error-and-Erasure Decoding for Irreducible Polynomial
  Remainder Codes</title><categories>cs.IT math.IT math.RA</categories><comments>Submitted (on 03/Feb/2012) to 2012 IEEE International Symposium on
  Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A general class of polynomial remainder codes is considered. Such codes are
very flexible in rate and length and include Reed-Solomon codes as a special
case.
  As an extension of previous work, two joint error-and-erasure decoding
approaches are proposed. In particular, both the decoding approaches by means
of a fixed transform are treated in a way compatible with the error-only
decoding. In the end, a collection of gcd-based decoding algorithm is obtained,
some of which appear to be new even when specialized to Reed-Solomon codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5414</identifier>
 <datestamp>2012-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5414</id><created>2012-02-24</created><authors><author><keyname>Reisert</keyname><forenames>Marco</forenames></author><author><keyname>Skibbe</keyname><forenames>Henrik</forenames></author></authors><title>Left-Invariant Diffusion on the Motion Group in terms of the Irreducible
  Representations of SO(3)</title><categories>math.AP cs.CV cs.NA math.RT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we study the formulation of convection/diffusion equations on
the 3D motion group SE(3) in terms of the irreducible representations of SO(3).
Therefore, the left-invariant vector-fields on SE(3) are expressed as linear
operators, that are differential forms in the translation coordinate and
algebraic in the rotation. In the context of 3D image processing this approach
avoids the explicit discretization of SO(3) or $S_2$, respectively. This is
particular important for SO(3), where a direct discretization is infeasible due
to the enormous memory consumption. We show two applications of the framework:
one in the context of diffusion-weighted magnetic resonance imaging and one in
the context of object detection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5447</identifier>
 <datestamp>2012-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5447</id><created>2012-02-24</created><authors><author><keyname>Li</keyname><forenames>Zhongkui</forenames></author><author><keyname>Liu</keyname><forenames>Xiangdong</forenames></author><author><keyname>Fu</keyname><forenames>Mengyin</forenames></author><author><keyname>Xie</keyname><forenames>Lihua</forenames></author></authors><title>Global $H_\infty$ Consensus of Multi-Agent Systems with Lipschitz
  Nonlinear Dynamics</title><categories>cs.SY math.OC</categories><comments>15 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the global consensus problems of a class of nonlinear
multi-agent systems with Lipschitz nonlinearity and directed communication
graphs, by using a distributed consensus protocol based on the relative states
of neighboring agents. A two-step algorithm is presented to construct a
protocol, under which a Lipschitz multi-agent system without disturbances can
reach global consensus for a strongly connected directed communication graph.
Another algorithm is then given to design a protocol which can achieve global
consensus with a guaranteed $H_\infty$ performance for a Lipschitz multiagent
system subject to external disturbances. The case with a leader-follower
communication graph is also discussed. Finally, the effectiveness of the
theoretical results is demonstrated through a network of single-link
manipulators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5449</identifier>
 <datestamp>2012-05-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5449</id><created>2012-02-24</created><updated>2012-05-04</updated><authors><author><keyname>Fearnley</keyname><forenames>John</forenames></author><author><keyname>Peled</keyname><forenames>Doron</forenames></author><author><keyname>Schewe</keyname><forenames>Sven</forenames></author></authors><title>Synthesis of Succinct Systems</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Synthesis of correct by design systems from specification has recently
attracted much attention. The theoretical results imply that this problem is
highly intractable, e.g., synthesizing a system is 2EXPTIME-complete for an LTL
specification, and EXPTIME-complete for a CTL specification. However, an
argument against it is that the temporal specification is highly compact, and
the complexity reflects the large size of the system constructed. In that
respect, the complexity should, perhaps, be specified relative to the size of
the minimal satisfying system. A careful observation reveals that the size of
the system is presented in such arguments as the size of its state space. This
view is a bit nonstandard, in the sense that the state space can be
exponentially larger than the size of a reasonable implementation such as a
circuit or a program. Although this alternative measure of the size of the
synthesized system is more intuitive (e.g., this is the standard way model
checking problems are measured), research on synthesis has so far stayed with
measuring the system in terms of the explicit state space. This raises the
question of whether or not there always exists a small system. In this paper,
we show that this is the case if, and only if, PSPACE = EXPTIME.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5469</identifier>
 <datestamp>2012-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5469</id><created>2012-02-23</created><authors><author><keyname>Zubiaga</keyname><forenames>Arkaitz</forenames></author></authors><title>Enhancing Navigation on Wikipedia with Social Tags</title><categories>cs.IR cs.DL cs.HC cs.SI</categories><comments>Wikimania 2009, 5th International Conference of the Wikimedia
  Community</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Social tagging has become an interesting approach to improve search and
navigation over the actual Web, since it aggregates the tags added by different
users to the same resource in a collaborative way. This way, it results in a
list of weighted tags describing its resource. Combined to a classical
taxonomic classification system such as that by Wikipedia, social tags can
enhance document navigation and search. On the one hand, social tags suggest
alternative navigation ways, including pivot-browsing, popularity-driven
navigation, and filtering. On the other hand, it provides new metadata,
sometimes uncovered by documents' content, that can substantially improve
document search. In this work, the inclusion of an interface to add
user-defined tags describing Wikipedia articles is proposed, as a way to
improve article navigation and retrieval. As a result, a prototype on applying
tags over Wikipedia is proposed in order to evaluate its effectiveness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5470</identifier>
 <datestamp>2013-04-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5470</id><created>2012-02-24</created><updated>2013-04-17</updated><authors><author><keyname>He</keyname><forenames>Zhaoshui</forenames></author><author><keyname>Xie</keyname><forenames>Shengli</forenames></author><author><keyname>Cichocki</keyname><forenames>Andrzej</forenames></author></authors><title>Convergence analysis of the FOCUSS algorithm</title><categories>cs.IT math.IT</categories><comments>This paper has been withdrawn by the author due to a crucial error</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  FOCal Underdetermined System Solver (FOCUSS) is a powerful tool for sparse
representation and underdetermined inverse problems, which is extremely easy to
implement. In this paper, we give a comprehensive convergence analysis on the
FOCUSS algorithm towards establishing a systematic convergence theory by
providing three primary contributions as follows. First, we give a rigorous
derivation for this algorithm exploiting the auxiliary function. Then, we prove
its convergence. Third, we systematically study its convergence rate with
respect to the sparsity parameter p and demonstrate its convergence rate by
numerical experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5471</identifier>
 <datestamp>2012-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5471</id><created>2012-02-24</created><authors><author><keyname>Wu</keyname><forenames>Jiasong</forenames></author><author><keyname>Zhang</keyname><forenames>Xu</forenames></author><author><keyname>Wang</keyname><forenames>Xiaoqing</forenames></author><author><keyname>Senhadji</keyname><forenames>Lotfi</forenames></author><author><keyname>Shu</keyname><forenames>Huazhong</forenames></author></authors><title>L1-norm minimization for quaternion signals</title><categories>cs.NA cs.DS cs.IT math.IT</categories><comments>4 pages,2 figures</comments><acm-class>G.1.3</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The l1-norm minimization problem plays an important role in the compressed
sensing (CS) theory. We present in this letter an algorithm for solving the
problem of l1-norm minimization for quaternion signals by converting it to
second-order cone programming. An application example of the proposed algorithm
is also given for practical guidelines of perfect recovery of quaternion
signals. The proposed algorithm may find its potential application when CS
theory meets the quaternion signal processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5474</identifier>
 <datestamp>2013-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5474</id><created>2012-02-24</created><updated>2013-07-05</updated><authors><author><keyname>Cao</keyname><forenames>Pan</forenames></author><author><keyname>Jorswieck</keyname><forenames>Eduard A.</forenames></author><author><keyname>Shi</keyname><forenames>Shuying</forenames></author></authors><title>Pareto Boundary of the Rate Region for Single-Stream MIMO Interference
  Channels: Linear Transceiver Design</title><categories>cs.IT math.IT</categories><comments>16 pages, 9 figures. Accepted for publication in IEEE Tans. Signal
  Process. June. 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a multiple-input multiple-output (MIMO) interference channel
(IC), where a single data stream per user is transmitted and each receiver
treats interference as noise. The paper focuses on the open problem of
computing the outermost boundary (so-called Pareto boundary-PB) of the
achievable rate region under linear transceiver design. The Pareto boundary
consists of the strict PB and non-strict PB. For the two user case, we compute
the non-strict PB and the two ending points of the strict PB exactly. For the
strict PB, we formulate the problem to maximize one rate while the other rate
is fixed such that a strict PB point is reached. To solve this non-convex
optimization problem which results from the hard-coupled two transmit
beamformers, we propose an alternating optimization algorithm. Furthermore, we
extend the algorithm to the multi-user scenario and show convergence. Numerical
simulations illustrate that the proposed algorithm computes a sequence of
well-distributed operating points that serve as a reasonable and complete inner
bound of the strict PB compared with existing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5477</identifier>
 <datestamp>2012-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5477</id><created>2012-02-23</created><authors><author><keyname>Zubiaga</keyname><forenames>Arkaitz</forenames></author><author><keyname>Mart&#xed;nez</keyname><forenames>Raquel</forenames></author><author><keyname>Fresno</keyname><forenames>V&#xed;ctor</forenames></author></authors><title>Analyzing Tag Distributions in Folksonomies for Resource Classification</title><categories>cs.DL cs.IR</categories><journal-ref>KSEM 2011, 5th International Conference on Knowledge Science,
  Engineering and Management</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Recent research has shown the usefulness of social tags as a data source to
feed resource classification. Little is known about the effect of settings on
folksonomies created on social tagging systems. In this work, we consider the
settings of social tagging systems to further understand tag distributions in
folksonomies. We analyze in depth the tag distributions on three large-scale
social tagging datasets, and analyze the effect on a resource classification
task. To this end, we study the appropriateness of applying weighting schemes
based on the well-known TF-IDF for resource classification. We show the great
importance of settings as to altering tag distributions. Among those settings,
tag suggestions produce very different folksonomies, which condition the
success of the employed weighting schemes. Our findings and analyses are
relevant for researchers studying tag-based resource classification, user
behavior in social networks, the structure of folksonomies and tag
distributions, as well as for developers of social tagging systems in search of
an appropriate setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5480</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5480</id><created>2012-02-24</created><authors><author><keyname>Hasham</keyname><forenames>Khawar</forenames></author><author><keyname>Peris</keyname><forenames>Antonio Delgado</forenames></author><author><keyname>Anjum</keyname><forenames>Ashiq</forenames></author><author><keyname>Evans</keyname><forenames>Dave</forenames></author><author><keyname>Hufnagel</keyname><forenames>Dirk</forenames></author><author><keyname>Huedo</keyname><forenames>Eduardo</forenames></author><author><keyname>Hern&#xe1;ndez</keyname><forenames>Jos&#xe9; M.</forenames></author><author><keyname>McClatchey</keyname><forenames>Richard</forenames></author><author><keyname>Gowdy</keyname><forenames>Stephen</forenames></author><author><keyname>Metson</keyname><forenames>Simon</forenames></author></authors><title>CMS Workflow Execution using Intelligent Job Scheduling and Data Access
  Strategies</title><categories>cs.SE cs.DC</categories><comments>12 pages, 12 figures</comments><journal-ref>IEEE Transactions in Nuclear Science 58 (3) pp. 1221-1232. ISSN
  0018-9499 2011</journal-ref><doi>10.1109/TNS.2011.2146276</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Complex scientific workflows can process large amounts of data using
thousands of tasks. The turnaround times of these workflows are often affected
by various latencies such as the resource discovery, scheduling and data access
latencies for the individual workflow processes or actors. Minimizing these
latencies will improve the overall execution time of a workflow and thus lead
to a more efficient and robust processing environment. In this paper, we
propose a pilot job based infrastructure that has intelligent data reuse and
job execution strategies to minimize the scheduling, queuing, execution and
data access latencies. The results have shown that significant improvements in
the overall turnaround time of a workflow can be achieved with this approach.
The proposed approach has been evaluated, first using the CMS Tier0 data
processing workflow, and then simulating the workflows to evaluate its
effectiveness in a controlled environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5482</identifier>
 <datestamp>2012-11-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5482</id><created>2012-02-24</created><updated>2012-11-13</updated><authors><author><keyname>Rahmouni</keyname><forenames>Hanene Boussi</forenames></author><author><keyname>Munir</keyname><forenames>Kamran</forenames></author><author><keyname>Odeh</keyname><forenames>Mohammed</forenames></author><author><keyname>McClatchey</keyname><forenames>Richard</forenames></author></authors><title>Risk-Driven Compliant Access Controls for Clouds</title><categories>cs.DC</categories><comments>9 pages, 3 figures. International Arab Conference on Information
  Technology (ACIT 2011) / Riyadh, Saudi Arabia. December 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is widespread agreement that cloud computing have proven cost cutting
and agility benefits. However, security and regulatory compliance issues are
continuing to challenge the wide acceptance of such technology both from social
and commercial stakeholders. An important facture behind this is the fact that
clouds and in particular public clouds are usually deployed and used within
broad geographical or even international domains. This implies that the
exchange of private and other protected data within the cloud environment would
be governed by multiple jurisdictions. These jurisdictions have a great degree
of harmonisation; however, they present possible conflicts that are hard to
negotiate at run time. So far, important efforts were played in order to deal
with regulatory compliance management for large distributed systems. However,
measurable solutions are required for the context of cloud. In this position
paper, we are suggesting an approach that starts with a conceptual model of
explicit regulatory requirements for exchanging private data on a
multijurisdictional environment and build on it in order to define metrics for
non-compliance or, in other terms, risks to compliance. These metrics will be
integrated within usual data access-control policies and will be checked at
policy analysis time before a decision to allow/deny the data access is made.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5483</identifier>
 <datestamp>2012-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5483</id><created>2012-02-24</created><authors><author><keyname>Khan</keyname><forenames>Zaheer</forenames></author><author><keyname>Ludlow</keyname><forenames>David</forenames></author><author><keyname>McClatchey</keyname><forenames>Richard</forenames></author><author><keyname>Anjum</keyname><forenames>Ashiq</forenames></author></authors><title>An Architecture for Integrated Intelligence in Urban Management using
  Cloud Computing</title><categories>cs.DC</categories><comments>6 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the emergence of new methodologies and technologies it has now become
possible to manage large amounts of environmental sensing data and apply new
integrated computing models to acquire information intelligence. This paper
advocates the application of cloud capacity to support the information,
communication and decision making needs of a wide variety of stakeholders in
the complex business of the management of urban and regional development. The
complexity lies in the interactions and impacts embodied in the concept of the
urban-ecosystem at various governance levels. This highlights the need for more
effective integrated environmental management systems. This paper offers a
user-orientated approach based on requirements for an effective management of
the urban-ecosystem and the potential contributions that can be supported by
the cloud computing community. Furthermore, the commonality of the influence of
the drivers of change at the urban level offers the opportunity for the cloud
computing community to develop generic solutions that can serve the needs of
hundreds of cities from Europe and indeed globally.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5509</identifier>
 <datestamp>2012-04-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5509</id><created>2012-02-24</created><updated>2012-04-02</updated><authors><author><keyname>Beal</keyname><forenames>Jacob</forenames></author><author><keyname>Dulman</keyname><forenames>Stefan</forenames></author><author><keyname>Usbeck</keyname><forenames>Kyle</forenames></author><author><keyname>Viroli</keyname><forenames>Mirko</forenames></author><author><keyname>Correll</keyname><forenames>Nikolaus</forenames></author></authors><title>Organizing the Aggregate: Languages for Spatial Computing</title><categories>cs.PL cs.DC cs.MA</categories><comments>60 pages; Review chapter to appear as a chapter in book &quot;Formal and
  Practical Aspects of Domain-Specific Languages: Recent Developments&quot;</comments><acm-class>D.3; D.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As the number of computing devices embedded into engineered systems continues
to rise, there is a widening gap between the needs of the user to control
aggregates of devices and the complex technology of individual devices. Spatial
computing attempts to bridge this gap for systems with local communication by
exploiting the connection between physical locality and device connectivity. A
large number of spatial computing domain specific languages (DSLs) have emerged
across diverse domains, from biology and reconfigurable computing, to sensor
networks and agent-based systems. In this chapter, we develop a framework for
analyzing and comparing spatial computing DSLs, survey the current state of the
art, and provide a roadmap for future spatial computing DSL investigation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5512</identifier>
 <datestamp>2012-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5512</id><created>2012-02-24</created><authors><author><keyname>Osman</keyname><forenames>Asif</forenames></author><author><keyname>Anjum</keyname><forenames>Ashiq</forenames></author><author><keyname>Batool</keyname><forenames>Naheed</forenames></author><author><keyname>McClatchey</keyname><forenames>Richard</forenames></author></authors><title>A Fault Tolerant, Dynamic and Low Latency BDII Architecture for Grids</title><categories>cs.DC</categories><comments>18 pages; 10 figures; 4 tables</comments><journal-ref>International Journal of Grid and Distributed Computing Vol. 3,
  No. 4, December, 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The current BDII model relies on information gathering from agents that run
on each core node of a Grid. This information is then published into a Grid
wide information resource known as Top BDII. The Top level BDIIs are updated
typically in cycles of a few minutes each. A new BDDI architecture is proposed
and described in this paper based on the hypothesis that only a few attribute
values change in each BDDI information cycle and consequently it may not be
necessary to update each parameter in a cycle. It has been demonstrated that
significant performance gains can be achieved by exchanging only the
information about records that changed during a cycle. Our investigations have
led us to implement a low latency and fault tolerant BDII system that involves
only minimal data transfer and facilitates secure transactions in a Grid
environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5514</identifier>
 <datestamp>2015-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5514</id><created>2012-02-24</created><updated>2015-02-24</updated><authors><author><keyname>Ndour</keyname><forenames>Cheikh</forenames><affiliation>Universit&#xe9; Gaston Berger, Saint-Louis, S&#xe9;n&#xe9;gal</affiliation><affiliation>Universit&#xe9; de Pau et des Pays de l 'Adour, Pau, France</affiliation><affiliation>Universit&#xe9; de Bordeaux, Bordeaux, France</affiliation></author><author><keyname>Diop</keyname><forenames>Aliou</forenames><affiliation>Universit&#xe9; Gaston Berger, Saint-Louis, S&#xe9;n&#xe9;gal</affiliation></author><author><keyname>Dossou-Gb&#xe9;t&#xe9;</keyname><forenames>Simplice</forenames><affiliation>Universit&#xe9; de Pau et des Pays de l 'Adour, Pau, France</affiliation></author></authors><title>Classification approach based on association rules mining for unbalanced
  data</title><categories>stat.ML cs.LG</categories><comments>29 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with the binary classification task when the target class
has the lower probability of occurrence. In such situation, it is not possible
to build a powerful classifier by using standard methods such as logistic
regression, classification tree, discriminant analysis, etc. To overcome this
short-coming of these methods which yield classifiers with low sensibility, we
tackled the classification problem here through an approach based on the
association rules learning. This approach has the advantage of allowing the
identification of the patterns that are well correlated with the target class.
Association rules learning is a well known method in the area of data-mining.
It is used when dealing with large database for unsupervised discovery of local
patterns that expresses hidden relationships between input variables. In
considering association rules from a supervised learning point of view, a
relevant set of weak classifiers is obtained from which one derives a
classifier that performs well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5516</identifier>
 <datestamp>2012-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5516</id><created>2012-02-24</created><authors><author><keyname>Anjum</keyname><forenames>Ashiq</forenames></author><author><keyname>Bloodsworth</keyname><forenames>Peter</forenames></author><author><keyname>Habib</keyname><forenames>Irfan</forenames></author><author><keyname>Lansdale</keyname><forenames>Tom</forenames></author><author><keyname>McClatchey</keyname><forenames>Richard</forenames></author><author><keyname>Mehmood</keyname><forenames>Yasir</forenames></author><author><keyname>Consortium</keyname><forenames>the neuGRID</forenames></author></authors><title>Reusable Services from the neuGRID Project for Grid-Based Health
  Applications</title><categories>cs.SE</categories><comments>6 pages; 3 figures.Proceedings the 7th HealthGrid Int. Conference
  (HG'09). Berlin, Germany. June 2009</comments><journal-ref>Studies in Health Technology &amp; Informatics Vol 147, pp 283-288
  ISBN 978-1-60750-027-8 IOS Press. 2009</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  By abstracting Grid middleware specific considerations from clinical research
applications, re-usable services should be developed that will provide generic
functionality aimed specifically at medical applications. In the scope of the
neuGRID project, generic services are being designed and developed which will
be applied to satisfy the requirements of neuroscientists. These services will
bring together sources of data and computing elements into a single view as far
as applications are concerned, making it possible to cope with centralised,
distributed or hybrid data and provide native support for common medical file
formats. Services will include querying, provenance, portal, anonymization and
pipeline services together with a 'glueing' service for connection to Grid
services. Thus lower-level services will hide the peculiarities of any specific
Grid technology from upper layers, provide application independence and will
enable the selection of 'fit-for-purpose' infrastructures. This paper outlines
the design strategy being followed in neuGRID using the glueing and pipeline
services as examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5517</identifier>
 <datestamp>2012-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5517</id><created>2012-02-24</created><authors><author><keyname>Anjum</keyname><forenames>Ashiq</forenames></author><author><keyname>Bloodsworth</keyname><forenames>Peter</forenames></author><author><keyname>Branson</keyname><forenames>Andrew</forenames></author><author><keyname>Habib</keyname><forenames>Irfan</forenames></author><author><keyname>McClatchey</keyname><forenames>Richard</forenames></author><author><keyname>Solomonides</keyname><forenames>Tony</forenames></author><author><keyname>Consortium</keyname><forenames>the neuGRID</forenames></author></authors><title>Research Traceability using Provenance Services for Biomedical Analysis</title><categories>cs.DB cs.SE</categories><comments>9 pages; 5 figures. Proceedings of the 8th HealthGrid Int. Conference
  (HG'10). Paris, France. June 2010</comments><journal-ref>Studies in Health Technology &amp; Informatics Vol 159, pp 88-99 ISBN
  978-1-60750-582-2 IOS Press. 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We outline the approach being developed in the neuGRID project to use
provenance management techniques for the purposes of capturing and preserving
the provenance data that emerges in the specification and execution of
workflows in biomedical analyses. In the neuGRID project a provenance service
has been designed and implemented that is intended to capture, store, retrieve
and reconstruct the workflow information needed to facilitate users in
conducting user analyses. We describe the architecture of the neuGRID
provenance service and discuss how the CRISTAL system from CERN is being
adapted to address the requirements of the project and then consider how a
generalised approach for provenance management could emerge for more generic
application to the (Health)Grid community.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5519</identifier>
 <datestamp>2012-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5519</id><created>2012-02-24</created><authors><author><keyname>Kiani</keyname><forenames>Saad Liaquat</forenames></author><author><keyname>Anjum</keyname><forenames>Ashiq</forenames></author><author><keyname>Antonopoulos</keyname><forenames>Nick</forenames></author><author><keyname>Knappmeyer</keyname><forenames>Michael</forenames></author><author><keyname>Baker</keyname><forenames>Nigel</forenames></author><author><keyname>McClatchey</keyname><forenames>Richard</forenames></author></authors><title>Context-Aware Service Utilisation in the Clouds and Energy Conservation</title><categories>cs.DC</categories><comments>27 pages; 17 figures; 2 tables. Under review at the Journal of
  Ambient Intelligence and Humanized Computing. 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ubiquitous computing environments are characterised by smart, interconnected
artefacts embedded in our physical world that are projected to provide useful
services to human inhabitants unobtrusively. Mobile devices are becoming the
primary tools of human interaction with these embedded artefacts and
utilisation of services available in smart computing environments such as
clouds. Advancements in capabilities of mobile devices allow a number of user
and environment related context consumers to be hosted on these devices.
Without a coordinating component, these context consumers and providers are a
potential burden on device resources; specifically the effect of uncoordinated
computation and communication with cloud-enabled services can negatively impact
the battery life. Therefore energy conservation is a major concern in realising
the collaboration and utilisation of mobile device based context-aware
applications and cloud based services. This paper presents the concept of a
context-brokering component to aid in coordination and communication of context
information between mobile devices and services deployed in a cloud
infrastructure. A prototype context broker is experimentally analysed for
effects on energy conservation when accessing and coordinating with cloud
services on a smart device, with results signifying reduction in energy
consumption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5523</identifier>
 <datestamp>2015-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5523</id><created>2012-02-24</created><updated>2015-01-09</updated><authors><author><keyname>Giscard</keyname><forenames>P. -L.</forenames></author><author><keyname>Thwaite</keyname><forenames>S. J.</forenames></author><author><keyname>Jaksch</keyname><forenames>D.</forenames></author></authors><title>Walk-Sums, Continued Fractions and Unique Factorisation on Digraphs</title><categories>cs.DM math.RT</categories><comments>Updated with links between nesting and loop-erasing. Still under
  review (!)</comments><msc-class>05C38, 05C20, 05C22, 05C25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the series of all walks between any two vertices of any
(possibly weighted) directed graph $\mathcal{G}$ is given by a universal
continued fraction of finite depth and breadth involving the simple paths and
simple cycles of $\mathcal{G}$. A simple path is a walk forbidden to visit any
vertex more than once. We obtain an explicit formula giving this continued
fraction. Our results are based on an equivalent to the fundamental theorem of
arithmetic: we demonstrate that arbitrary walks on $\mathcal{G}$ factorize
uniquely into nesting products of simple paths and simple cycles, where nesting
is a product operation between walks that we define. We show that the simple
paths and simple cycles are the prime elements of the set of all walks on
$\mathcal{G}$ equipped with the nesting product. We give an algorithm producing
the prime factorization of individual walks, and obtain a recursive formula
producing the prime factorization of sets of walks. Our results have already
found applications in machine learning, matrix computations and quantum
mechanics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5528</identifier>
 <datestamp>2012-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5528</id><created>2012-02-24</created><authors><author><keyname>Sadr</keyname><forenames>Sanam</forenames></author><author><keyname>Adve</keyname><forenames>Raviraj</forenames></author></authors><title>Hierarchical Resource Allocation in Femtocell Networks using Graph
  Algorithms</title><categories>cs.IT cs.NI math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a hierarchical approach to resource allocation in
open-access femtocell networks. The major challenge in femtocell networks is
interference management which in our system, based on the Long Term Evolution
(LTE) standard, translates to which user should be allocated which physical
resource block (or fraction thereof) from which femtocell access point (FAP).
The globally optimal solution requires integer programming and is
mathematically intractable. We propose a hierarchical three-stage solution:
first, the load of each FAP is estimated considering the number of users
connected to the FAP, their average channel gain and required data rates.
Second, based on each FAP's load, the physical resource blocks (PRBs) are
allocated to FAPs in a manner that minimizes the interference by coloring the
modified interference graph. Finally, the resource allocation is performed at
each FAP considering users' instantaneous channel gain. The two major
advantages of this suboptimal approach are the significantly reduced
computation complexity and the fact that the proposed algorithm only uses
information that is already likely to be available at the nodes executing the
relevant optimization step. The performance of the proposed solution is
evaluated in networks based on the LTE standard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5529</identifier>
 <datestamp>2012-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5529</id><created>2012-02-24</created><authors><author><keyname>Bloch</keyname><forenames>Matthieu R.</forenames></author><author><keyname>Kliewer</keyname><forenames>Joerg</forenames></author></authors><title>On Secure Communication with Constrained Randomization</title><categories>cs.IT math.IT</categories><comments>Submitted to ISIT 2012 (slightly extended version with all proof
  details)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate how constraints on the randomization in the
encoding process affect the secrecy rates achievable over wiretap channels. In
particular, we characterize the secrecy capacity with a rate-limited local
source of randomness and a less capable eavesdropper's channel, which shows
that limited rate incurs a secrecy rate penalty but does not preclude secrecy.
We also discuss a more practical aspect of rate-limited randomization in the
context of cooperative jamming. Finally, we show that secure communication is
possible with a non-uniform source for randomness; this suggests the
possibility of designing robust coding schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5539</identifier>
 <datestamp>2012-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5539</id><created>2012-02-24</created><authors><author><keyname>Wang</keyname><forenames>Yin</forenames></author><author><keyname>Dybvig</keyname><forenames>R. Kent</forenames></author></authors><title>Register Allocation By Model Transformer Semantics</title><categories>cs.PL</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Register allocation has long been formulated as a graph coloring problem,
coloring the conflict graph with physical registers. Such a formulation does
not fully capture the goal of the allocation, which is to minimize the traffic
between registers and memory. Linear scan has been proposed as an alternative
to graph coloring, but in essence, it can be viewed as a greedy algorithm for
graph coloring: coloring the vertices not in the order of their degrees, but in
the order of their occurence in the program. Thus it suffers from almost the
same constraints as graph coloring. In this article, I propose a new method of
register allocation based on the ideas of model transformer semantics (MTS) and
static cache replacement (SCR). Model transformer semantics captures the
semantics of registers and the stack. Static cache replacement relaxes the
assumptions made by graph coloring and linear scan, aiming directly at reducing
register-memory traffic. The method explores a much larger solution space than
that of graph coloring and linear scan, thus providing more opportunities of
optimization. It seamlessly performs live range splitting, an optimization
found in extensions to graph coloring and linear scan. Also, it simplifies the
compiler, and its semantics-based approach provides possibilities of
simplifying the formal verification of compilers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5544</identifier>
 <datestamp>2012-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5544</id><created>2012-02-24</created><authors><author><keyname>Huynh</keyname><forenames>Vu Anh</forenames></author><author><keyname>Karaman</keyname><forenames>Sertac</forenames></author><author><keyname>Frazzoli</keyname><forenames>Emilio</forenames></author></authors><title>An Incremental Sampling-based Algorithm for Stochastic Optimal Control</title><categories>cs.RO cs.SY math.DS math.OC math.PR</categories><comments>Part of the results have been submitted to the IEEE International
  Conference on Robotics and Automation (ICRA 2012). Minnesota, USA, May 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider a class of continuous-time, continuous-space
stochastic optimal control problems. Building upon recent advances in Markov
chain approximation methods and sampling-based algorithms for deterministic
path planning, we propose a novel algorithm called the incremental Markov
Decision Process (iMDP) to compute incrementally control policies that
approximate arbitrarily well an optimal policy in terms of the expected cost.
The main idea behind the algorithm is to generate a sequence of finite
discretizations of the original problem through random sampling of the state
space. At each iteration, the discretized problem is a Markov Decision Process
that serves as an incrementally refined model of the original problem. We show
that with probability one, (i) the sequence of the optimal value functions for
each of the discretized problems converges uniformly to the optimal value
function of the original stochastic optimal control problem, and (ii) the
original optimal value function can be computed efficiently in an incremental
manner using asynchronous value iterations. Thus, the proposed algorithm
provides an anytime approach to the computation of optimal control policies of
the continuous problem. The effectiveness of the proposed approach is
demonstrated on motion planning and control problems in cluttered environments
in the presence of process noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5548</identifier>
 <datestamp>2012-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5548</id><created>2012-02-24</created><authors><author><keyname>Erde</keyname><forenames>Joshua</forenames></author></authors><title>Knight's Tours in Higher Dimensions</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we are concerned with knight's tours on high-dimensional
boards. Our main aim is to show that on the $d$-dimensional board $[n]^d$, with
$n$ even, there is always a knight's tour provided that $n$ is sufficiently
large. In fact, we give an exact classification of the grids $[n_1] \times ...
\times [n_d]$ in which there is a knight's tour. This answers questions of
DeMaio, DeMaio and Mathew, and Watkins.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5569</identifier>
 <datestamp>2012-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5569</id><created>2012-02-24</created><authors><author><keyname>Abdullah</keyname><forenames>Mohammed</forenames></author></authors><title>The Cover Time of Random Walks on Graphs</title><categories>math.PR cs.DM</categories><comments>179 pages, PhD thesis</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A simple random walk on a graph is a sequence of movements from one vertex to
another where at each step an edge is chosen uniformly at random from the set
of edges incident on the current vertex, and then transitioned to next vertex.
Central to this thesis is the cover time of the walk, that is, the expectation
of the number of steps required to visit every vertex, maximised over all
starting vertices. In our first contribution, we establish a relation between
the cover times of a pair of graphs, and the cover time of their Cartesian
product. This extends previous work on special cases of the Cartesian product,
in particular, the square of a graph. We show that when one of the factors is
in some sense larger than the other, its cover time dominates, and can become
within a logarithmic factor of the cover time of the product as a whole. Our
main theorem effectively gives conditions for when this holds. The techniques
and lemmas we introduce may be of independent interest. In our second
contribution, we determine the precise asymptotic value of the cover time of a
random graph with given degree sequence. This is a graph picked uniformly at
random from all simple graphs with that degree sequence. We also show that with
high probability, a structural property of the graph called conductance, is
bounded below by a constant. This is of independent interest. Finally, we
explore random walks with weighted random edge choices. We present a weighting
scheme that has a smaller worst case cover time than a simple random walk. We
give an upper bound for a random graph of given degree sequence weighted
according to our scheme. We demonstrate that the speed-up (that is, the ratio
of cover times) over a simple random walk can be unbounded
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5597</identifier>
 <datestamp>2012-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5597</id><created>2012-02-24</created><updated>2012-04-30</updated><authors><author><keyname>Azimi</keyname><forenames>Javad</forenames></author><author><keyname>Jalali</keyname><forenames>Ali</forenames></author><author><keyname>Fern</keyname><forenames>Xiaoli</forenames></author></authors><title>Hybrid Batch Bayesian Optimization</title><categories>cs.AI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bayesian Optimization aims at optimizing an unknown non-convex/concave
function that is costly to evaluate. We are interested in application scenarios
where concurrent function evaluations are possible. Under such a setting, BO
could choose to either sequentially evaluate the function, one input at a time
and wait for the output of the function before making the next selection, or
evaluate the function at a batch of multiple inputs at once. These two
different settings are commonly referred to as the sequential and batch
settings of Bayesian Optimization. In general, the sequential setting leads to
better optimization performance as each function evaluation is selected with
more information, whereas the batch setting has an advantage in terms of the
total experimental time (the number of iterations). In this work, our goal is
to combine the strength of both settings. Specifically, we systematically
analyze Bayesian optimization using Gaussian process as the posterior estimator
and provide a hybrid algorithm that, based on the current state, dynamically
switches between a sequential policy and a batch policy with variable batch
sizes. We provide theoretical justification for our algorithm and present
experimental results on eight benchmark BO problems. The results show that our
method achieves substantial speedup (up to %78) compared to a pure sequential
policy, without suffering any significant performance loss.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5598</identifier>
 <datestamp>2012-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5598</id><created>2012-02-24</created><updated>2012-04-13</updated><authors><author><keyname>Jalali</keyname><forenames>Ali</forenames></author><author><keyname>Srebro</keyname><forenames>Nathan</forenames></author></authors><title>Clustering using Max-norm Constrained Optimization</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We suggest using the max-norm as a convex surrogate constraint for
clustering. We show how this yields a better exact cluster recovery guarantee
than previously suggested nuclear-norm relaxation, and study the effectiveness
of our method, and other related convex relaxations, compared to other
clustering approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5599</identifier>
 <datestamp>2014-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5599</id><created>2012-02-24</created><updated>2014-11-28</updated><authors><author><keyname>Mao</keyname><forenames>Wei</forenames></author><author><keyname>Thill</keyname><forenames>Matthew</forenames></author><author><keyname>Hassibi</keyname><forenames>Babak</forenames></author></authors><title>On the Ingleton-Violations in Finite Groups</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given $n$ discrete random variables, its entropy vector is the $2^n-1$
dimensional vector obtained from the joint entropies of all non-empty subsets
of the random variables. It is well known that there is a one-to-one
correspondence between such an entropy vector and a certain
group-characterizable vector obtained from a finite group and $n$ of its
subgroups [3]. This correspondence may be useful for characterizing the space
of entropic vectors and for designing network codes. If one restricts attention
to abelian groups then not all entropy vectors can be obtained. This is an
explanation for the fact shown by Dougherty et al [4] that linear network codes
cannot achieve capacity in general network coding problems. All abelian
group-characterizable vectors, and by fiat all entropy vectors generated by
linear network codes, satisfy a linear inequality called the Ingleton
inequality. It is therefore of interest to identify groups that violate the
Ingleton inequality. In this paper, we study the problem of finding nonabelian
finite groups that yield characterizable vectors which violate the Ingleton
inequality. Using a refined computer search, we find the symmetric group $S_5$
to be the smallest group that violates the Ingleton inequality. Careful study
of the structure of this group, and its subgroups, reveals that it belongs to
the Ingleton-violating family $PGL(2,q)$ with a prime power $q \geq 5$, i.e.,
the projective group of $2\times 2$ nonsingular matrices with entries in
$\mathbb{F}_q$. We further interpret this family using the theory of group
actions. We also extend the construction to more general groups such as
$PGL(n,q)$ and $GL(n,q)$. The families of groups identified here are therefore
good candidates for constructing network codes more powerful than linear
network codes, and we discuss some considerations for constructing such group
network codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5600</identifier>
 <datestamp>2012-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5600</id><created>2012-02-24</created><authors><author><keyname>Broz</keyname><forenames>Frank</forenames></author><author><keyname>Nehaniv</keyname><forenames>Chrystopher L.</forenames></author><author><keyname>Kose-Bagci</keyname><forenames>Hatice</forenames></author><author><keyname>Dautenhahn</keyname><forenames>Kerstin</forenames></author></authors><title>Interaction Histories and Short Term Memory: Enactive Development of
  Turn-taking Behaviors in a Childlike Humanoid Robot</title><categories>cs.AI nlin.AO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, an enactive architecture is described that allows a humanoid
robot to learn to compose simple actions into turn-taking behaviors while
playing interaction games with a human partner. The robot's action choices are
reinforced by social feedback from the human in the form of visual attention
and measures of behavioral synchronization. We demonstrate that the system can
acquire and switch between behaviors learned through interaction based on
social feedback from the human partner. The role of reinforcement based on a
short term memory of the interaction is experimentally investigated. Results
indicate that feedback based only on the immediate state is insufficient to
learn certain turn-taking behaviors. Therefore some history of the interaction
must be considered in the acquisition of turn-taking, which can be efficiently
handled through the use of short term memory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5609</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5609</id><created>2012-02-25</created><authors><author><keyname>Basha</keyname><forenames>N Md Jubair</forenames></author><author><keyname>Moiz</keyname><forenames>Salman Abdul</forenames></author></authors><title>A Framework Studio for Component Reusability</title><categories>cs.SE</categories><comments>11 pages</comments><journal-ref>Natarajan Meghanathan, et al. (Eds): ITCS, SIP, JSE-2012, CS &amp; IT
  04, pp. 325-335, 2012</journal-ref><doi>10.5121/csit.2012.2130</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The deployment of a software product requires considerable amount of time and
effort. In order to increase the productivity of the software products,
reusability strategies were proposed in the literature. However effective reuse
is still a challenging issue. This paper presents a framework studio for
effective components reusability which provides the selection of components
from framework studio and generation of source code based on stakeholders
needs. The framework studio is implemented using swings which are integrated
onto the Net Beans IDE which help in faster generation of the source code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5610</identifier>
 <datestamp>2012-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5610</id><created>2012-02-25</created><authors><author><keyname>Har-Peled</keyname><forenames>Sariel</forenames></author><author><keyname>Raichel</keyname><forenames>Benjamin</forenames></author></authors><title>Fr\'echet Distance Revisited and Extended</title><categories>cs.CG</categories><comments>28 pages, 3 figures, in SoCG 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given two simplicial complexes in R^d, and start and end vertices in each
complex, we show how to compute curves (in each complex) between these
vertices, such that the Fr\'echet distance between these curves is minimized.
As a polygonal curve is a complex, this generalizes the regular notion of weak
Fr\'echet distance between curves. We also generalize the algorithm to handle
an input of k simplicial complexes.
  Using this new algorithm we can solve a slew of new problems, from computing
a mean curve for a given collection of curves, to various motion planning
problems. Additionally, we show that for the mean curve problem, when the k
input curves are c-packed, one can (1+epsilon)-approximate the mean curve in
near linear time, for fixed k and epsilon.
  Additionally, we present an algorithm for computing the strong Fr\'echet
distance between two curves, which is simpler than previous algorithms, and
avoids using parametric search.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5618</identifier>
 <datestamp>2012-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5618</id><created>2012-02-25</created><authors><author><keyname>Bold</keyname><forenames>Katherine A.</forenames></author><author><keyname>Rajendran</keyname><forenames>Karthikeyan</forenames></author><author><keyname>R&#xe1;th</keyname><forenames>Bal&#xe1;zs</forenames></author><author><keyname>Kevrekidis</keyname><forenames>Ioannis G.</forenames></author></authors><title>An equation-free approach to coarse-graining the dynamics of networks</title><categories>cs.SI nlin.AO physics.comp-ph physics.soc-ph</categories><comments>28 pages, 8 figures</comments><msc-class>68U20, 37E25, 65Z05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose and illustrate an approach to coarse-graining the dynamics of
evolving networks (networks whose connectivity changes dynamically). The
approach is based on the equation-free framework: short bursts of detailed
network evolution simulations are coupled with lifting and restriction
operators that translate between actual network realizations and their
(appropriately chosen) coarse observables. This framework is used here to
accelerate temporal simulations (through coarse projective integration), and to
implement coarsegrained fixed point algorithms (through matrix-free
Newton-Krylov GMRES). The approach is illustrated through a simple network
evolution example, for which analytical approximations to the coarse-grained
dynamics can be independently obtained, so as to validate the computational
results. The scope and applicability of the approach, as well as the issue of
selection of good coarse observables are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5619</identifier>
 <datestamp>2012-10-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5619</id><created>2012-02-25</created><updated>2012-10-15</updated><authors><author><keyname>Alamdari</keyname><forenames>Soroush</forenames></author><author><keyname>Fata</keyname><forenames>Elaheh</forenames></author><author><keyname>Smith</keyname><forenames>Stephen L.</forenames></author></authors><title>Persistent Monitoring in Discrete Environments: Minimizing the Maximum
  Weighted Latency Between Observations</title><categories>cs.DS</categories><comments>17 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the problem of planning a path for a robot to
monitor a known set of features of interest in an environment. We represent the
environment as a graph with vertex weights and edge lengths. The vertices
represent regions of interest, edge lengths give travel times between regions,
and the vertex weights give the importance of each region. As the robot
repeatedly performs a closed walk on the graph, we define the weighted latency
of a vertex to be the maximum time between visits to that vertex, weighted by
the importance (vertex weight) of that vertex. Our goal is to find a closed
walk that minimizes the maximum weighted latency of any vertex. We show that
there does not exist a polynomial time algorithm for the problem. We then
provide two approximation algorithms; an $O(\log n)$-approximation algorithm
and an $O(\log \rho_G)$-approximation algorithm, where $\rho_G$ is the ratio
between the maximum and minimum vertex weights. We provide simulation results
which demonstrate that our algorithms can be applied to problems consisting of
thousands of vertices, and a case study for patrolling a city for crime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5657</identifier>
 <datestamp>2012-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5657</id><created>2012-02-25</created><authors><author><keyname>Saha</keyname><forenames>Suman</forenames></author><author><keyname>Das</keyname><forenames>Saptarshi</forenames></author><author><keyname>Ghosh</keyname><forenames>Ratna</forenames></author><author><keyname>Goswami</keyname><forenames>Bhaswati</forenames></author><author><keyname>Balasubramanian</keyname><forenames>R.</forenames></author><author><keyname>Chandra</keyname><forenames>A. K.</forenames></author><author><keyname>Das</keyname><forenames>Shantanu</forenames></author><author><keyname>Gupta</keyname><forenames>Amitava</forenames></author></authors><title>Design of a Fractional Order Phase Shaper for Iso-damped Control of a
  PHWR under Step-back Condition</title><categories>math.OC cs.SY</categories><comments>11 pages, 10figures</comments><journal-ref>IEEE Transactions on Nuclear Science, vol. 57, no. 3, part. 3,
  art. no. 5485191, pp. 1602-1612, June 2010</journal-ref><doi>10.1109/TNS.2010.2047405</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Phase shaping using fractional order (FO) phase shapers has been proposed by
many contemporary researchers as a means of producing systems with iso-damped
closed loop response due to a stepped variation in input. Such systems, with
the closed loop damping remaining invariant to gain changes can be used to
produce dead-beat step response with only rise time varying with gain. This
technique is used to achieve an active step-back in a Pressurized Heavy Water
Reactor (PHWR) where it is desired to change the reactor power to a
pre-determined value within a short interval keeping the power undershoot as
low as possible. This paper puts forward an approach as an alternative for the
present day practice of a passive step-back mechanism where the control rods
are allowed to drop during a step-back action by gravity, with release of
electromagnetic clutches. The reactor under a step-back condition is identified
as a system using practical test data and a suitable Proportional plus Integral
plus Derivative (PID) controller is designed for it. Then the combined plant is
augmented with a phase shaper to achieve a dead-beat response in terms of power
drop. The fact that the identified static gain of the system depends on the
initial power level at which a step-back is initiated, makes this application
particularly suited for using a FO phase shaper. In this paper, a model of a
nuclear reactor is developed for a control rod drop scenario involving rapid
power reduction in a 500MWe Canadian Deuterium Uranium (CANDU) reactor using
AutoRegressive Exogenous (ARX) algorithm. The system identification and reduced
order modeling are developed from practical test data. For closed loop active
control of the identified reactor model, the fractional order phase shaper
along with a PID controller is shown to perform better than the present Reactor
Regulating System (RRS) due to its iso-damped nature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5665</identifier>
 <datestamp>2013-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5665</id><created>2012-02-25</created><updated>2012-06-16</updated><authors><author><keyname>Ghoshdastidar</keyname><forenames>Debarghya</forenames></author><author><keyname>Dukkipati</keyname><forenames>Ambedkar</forenames></author><author><keyname>Bhatnagar</keyname><forenames>Shalabh</forenames></author></authors><title>q-Gaussian based Smoothed Functional Algorithm for Stochastic
  Optimization</title><categories>cs.SY cs.IT math.IT</categories><comments>5 pages, 1 figure</comments><doi>10.1109/ISIT.2012.6283013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The q-Gaussian distribution results from maximizing certain generalizations
of Shannon entropy under some constraints. The importance of q-Gaussian
distributions stems from the fact that they exhibit power-law behavior, and
also generalize Gaussian distributions. In this paper, we propose a Smoothed
Functional (SF) scheme for gradient estimation using q-Gaussian distribution,
and also propose an algorithm for optimization based on the above scheme.
Convergence results of the algorithm are presented. Performance of the proposed
algorithm is shown by simulation results on a queuing model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5667</identifier>
 <datestamp>2012-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5667</id><created>2012-02-25</created><authors><author><keyname>Saha</keyname><forenames>Suman</forenames></author><author><keyname>Das</keyname><forenames>Saptarshi</forenames></author><author><keyname>Ghosh</keyname><forenames>Ratna</forenames></author><author><keyname>Goswami</keyname><forenames>Bhaswati</forenames></author><author><keyname>Gupta</keyname><forenames>Amitava</forenames></author><author><keyname>Balasubramanian</keyname><forenames>R.</forenames></author><author><keyname>Chandra</keyname><forenames>A. K.</forenames></author><author><keyname>Das</keyname><forenames>Shantanu</forenames></author></authors><title>Fractional Order Phase Shaper Design with Routh's Criterion for
  Iso-damped Control System</title><categories>cs.SY</categories><comments>4 pages, 4 figures; Proceedings of INDICON 2009 - An IEEE India
  Council Conference, art. no. 5409434, Dec. 2009, Gujarat</comments><doi>10.1109/INDCON.2009.5409434</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Phase curve of an open loop system is flat in nature if the derivative of
phase with respect to frequency is zero. With a flat phase curve, the
corresponding closed-loop system exhibits an iso-damped property i.e. maintains
constant overshoot with the change of gain and with other parametric
variations. In recent past application, fractional order (FO) phase shapers
have been proposed by contemporary researchers to achieve enhanced parametric
robustness. In this paper, a simple Routh tabulation based methodology is
proposed to design an appropriate FO phase shaper to achieve phase flattening
in a control loop, comprising a system, controlled by a classical PID
controller. The method is demonstrated using MATLAB simulation of a second
order DC motor plant and also a first order with time delay system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5670</identifier>
 <datestamp>2012-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5670</id><created>2012-02-25</created><authors><author><keyname>Gawrychowski</keyname><forenames>Pawel</forenames></author></authors><title>(Really) Tight bounds for dispatching binary methods</title><categories>cs.DS</categories><comments>submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider binary dispatching problem originating from object oriented
programming. We want to preprocess a hierarchy of classes and collection of
methods so that given a function call in the run-time we are able to retrieve
the most specialized implementation which can be invoked with the actual types
of the arguments. For the binary dispatching, where the methods take exactly
two arguments, logarithmic query time is possible, even if the structure is
allowed to take linear space. Unfortunately, known solutions achieving such
complexity require superlinear time for constructing the structure. Using a
different idea we are able to construct in (deterministic) linear time and
space a structure allowing dispatching binary methods in the same logarithmic
time. Then we show how to improve the query time to slightly sublogarithmic,
which is easily seen to be optimal as a consequence of some already known lower
bounds if we want to keep the size of the resulting structure close to linear.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5674</identifier>
 <datestamp>2012-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5674</id><created>2012-02-25</created><authors><author><keyname>Pan</keyname><forenames>Indranil</forenames></author><author><keyname>Das</keyname><forenames>Saptarshi</forenames></author><author><keyname>Gupta</keyname><forenames>Amitava</forenames></author></authors><title>Handling Packet Dropouts and Random Delays for Unstable Delayed
  Processes in NCS by Optimal Tuning of PI{\lambda}D{\mu} Controllers with
  Evolutionary Algorithms</title><categories>cs.SY</categories><comments>30 pages, 18 figures</comments><journal-ref>ISA Transactions, vol. 50, no. 4, pp. 557-572, Oct. 2011</journal-ref><doi>10.1016/j.isatra.2011.04.002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The issues of stochastically varying network delays and packet dropouts in
Networked Control System (NCS) applications have been simultaneously addressed
by time domain optimal tuning of fractional order (FO) PID controllers.
Different variants of evolutionary algorithms are used for the tuning process
and their performances are compared. Also the effectiveness of the fractional
order PI{\lambda}D{\mu} controllers over their integer order counterparts is
looked into. Two standard test bench plants with time delay and unstable poles
which are encountered in process control applications are tuned with the
proposed method to establish the validity of the tuning methodology. The
proposed tuning methodology is independent of the specific choice of plant and
is also applicable for less complicated systems. Thus it is useful in a wide
variety of scenarios. The paper also shows the superiority of FOPID controllers
over their conventional PID counterparts for NCS applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5675</identifier>
 <datestamp>2012-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5675</id><created>2012-02-25</created><updated>2012-08-20</updated><authors><author><keyname>Krauthgamer</keyname><forenames>Robert</forenames></author><author><keyname>Zondiner</keyname><forenames>Tamar</forenames></author></authors><title>Preserving Terminal Distances using Minors</title><categories>cs.DS math.MG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the following notion of compressing an undirected graph G with
edge-lengths and terminal vertices $R\subseteq V(G)$. A distance-preserving
minor is a minor G' (of G) with possibly different edge-lengths, such that
$R\subseteq V(G')$ and the shortest-path distance between every pair of
terminals is exactly the same in G and in G'. What is the smallest f*(k) such
that every graph G with k=|R| terminals admits a distance-preserving minor G'
with at most f*(k) vertices?
  Simple analysis shows that $f*(k)\leq O(k^4)$. Our main result proves that
$f*(k)\geq \Omega(k^2)$, significantly improving over the trivial $f*(k)\geq
k$. Our lower bound holds even for planar graphs G, in contrast to graphs G of
constant treewidth, for which we prove that O(k) vertices suffice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5677</identifier>
 <datestamp>2012-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5677</id><created>2012-02-25</created><authors><author><keyname>Das</keyname><forenames>Saptarshi</forenames></author><author><keyname>Saha</keyname><forenames>Suman</forenames></author><author><keyname>Das</keyname><forenames>Shantanu</forenames></author><author><keyname>Gupta</keyname><forenames>Amitava</forenames></author></authors><title>On the Selection of Tuning Methodology of FOPID Controllers for the
  Control of Higher Order Processes</title><categories>cs.SY</categories><comments>27 pages, 10 figures</comments><journal-ref>ISA Transactions, vol. 5, no. 3, pp. 376-388, July 2011</journal-ref><doi>10.1016/j.isatra.2011.02.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a comparative study is done on the time and frequency domain
tuning strategies for fractional order (FO) PID controllers to handle higher
order processes. A new fractional order template for reduced parameter modeling
of stable minimum/non-minimum phase higher order processes is introduced and
its advantage in frequency domain tuning of FOPID controllers is also
presented. The time domain optimal tuning of FOPID controllers have also been
carried out to handle these higher order processes by performing optimization
with various integral performance indices. The paper highlights on the
practical control system implementation issues like flexibility of online
autotuning, reduced control signal and actuator size, capability of measurement
noise filtration, load disturbance suppression, robustness against parameter
uncertainties etc. in light of the above tuning methodologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5680</identifier>
 <datestamp>2012-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5680</id><created>2012-02-25</created><authors><author><keyname>Das</keyname><forenames>Saptarshi</forenames></author><author><keyname>Pan</keyname><forenames>Indranil</forenames></author><author><keyname>Das</keyname><forenames>Shantanu</forenames></author><author><keyname>Gupta</keyname><forenames>Amitava</forenames></author></authors><title>A Novel Fractional Order Fuzzy PID Controller and Its Optimal Time
  Domain Tuning Based on Integral Performance Indices</title><categories>cs.SY</categories><comments>30 pages, 20 figures</comments><journal-ref>Engineering Applications of Artificial Intelligence, vol. 25, no.
  2, pp. 430-442, March 2012</journal-ref><doi>10.1016/j.engappai.2011.10.004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel fractional order (FO) fuzzy Proportional-Integral-Derivative (PID)
controller has been proposed in this paper which works on the closed loop error
and its fractional derivative as the input and has a fractional integrator in
its output. The fractional order differ-integrations in the proposed fuzzy
logic controller (FLC) are kept as design variables along with the input-output
scaling factors (SF) and are optimized with Genetic Algorithm (GA) while
minimizing several integral error indices along with the control signal as the
objective function. Simulations studies are carried out to control a delayed
nonlinear process and an open loop unstable process with time delay. The closed
loop performances and controller efforts in each case are compared with
conventional PID, fuzzy PID and PI{\lambda}D{\mu} controller subjected to
different integral performance indices. Simulation results show that the
proposed fractional order fuzzy PID controller outperforms the others in most
cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5683</identifier>
 <datestamp>2012-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5683</id><created>2012-02-25</created><authors><author><keyname>Das</keyname><forenames>Saptarshi</forenames></author><author><keyname>Pan</keyname><forenames>Indranil</forenames></author><author><keyname>Das</keyname><forenames>Shantanu</forenames></author><author><keyname>Gupta</keyname><forenames>Amitava</forenames></author></authors><title>Improved Model Reduction and Tuning of Fractional Order
  PI{\lambda}D{\mu} Controllers for Analytical Rule Extraction with Genetic
  Programming</title><categories>cs.SY cs.NE</categories><comments>41 pages, 29 figures</comments><journal-ref>ISA Transactions, vol. 51, no. 2, pp. 237-261, March 2012</journal-ref><doi>10.1016/j.isatra.2011.10.004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Genetic Algorithm (GA) has been used in this paper for a new approach of
sub-optimal model reduction in the Nyquist plane and optimal time domain tuning
of PID and fractional order (FO) PI{\lambda}D{\mu} controllers. Simulation
studies show that the Nyquist based new model reduction technique outperforms
the conventional H2 norm based reduced parameter modeling technique. With the
tuned controller parameters and reduced order model parameter data-set, optimum
tuning rules have been developed with a test-bench of higher order processes
via Genetic Programming (GP). The GP performs a symbolic regression on the
reduced process parameters to evolve a tuning rule which provides the best
analytical expression to map the data. The tuning rules are developed for a
minimum time domain integral performance index described by weighted sum of
error index and controller effort. From the reported Pareto optimal front of GP
based optimal rule extraction technique a trade-off can be made between the
complexity of the tuning formulae and the control performance. The efficacy of
the single-gene and multi-gene GP based tuning rules has been compared with
original GA based control performance for the PID and PI{\lambda}D{\mu}
controllers, handling four different class of representative higher order
processes. These rules are very useful for process control engineers as they
inherit the power of the GA based tuning methodology, but can be easily
calculated without the requirement for running the computationally intensive GA
every time. Three dimensional plots of the required variation in PID/FOPID
controller parameters with reduced process parameters have been shown as a
guideline for the operator. Parametric robustness of the reported GP based
tuning rules has also been shown with credible simulation examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5684</identifier>
 <datestamp>2012-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5684</id><created>2012-02-25</created><authors><author><keyname>Das</keyname><forenames>Saptarshi</forenames></author><author><keyname>Das</keyname><forenames>Shantanu</forenames></author><author><keyname>Gupta</keyname><forenames>Amitava</forenames></author></authors><title>Fractional Order Modeling of a PHWR Under Step-Back Condition and
  Control of Its Global Power with a Robust PI{\lambda}D{\mu} Controller</title><categories>cs.SY</categories><comments>10 pages, 11 figures</comments><journal-ref>IEEE Transactions on Nuclear Science, vol. 58, no. 5, part 2, art.
  no. 6025228, pp. 2431-2441, Oct. 2011</journal-ref><doi>10.1109/TNS.2011.2164422</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bulk reduction of reactor power within a small finite time interval under
abnormal conditions is referred to as step-back. In this paper, a 500MWe
Canadian Deuterium Uranium (CANDU) type Pressurized Heavy Water Reactor (PHWR)
is modeled using few variants of Least Square Estimator (LSE) from practical
test data under a control rod drop scenario in order to design a control system
to achieve a dead-beat response during a stepped reduction of its global power.
A new fractional order (FO) model reduction technique is attempted which
increases the parametric robustness of the control loop due to lesser modeling
error and ensures iso-damped closed loop response with a PI{\lambda}D{\mu} or
FOPID controller. Such a controller can, therefore, be used to achieve active
step-back under varying load conditions for which the system dynamics change
significantly. For closed loop active control of the reduced FO reactor models,
the PI{\lambda}D{\mu} controller is shown to perform better than the classical
integer order PID controllers and present operating Reactor Regulating System
(RRS) due to its robustness against shift in system parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5685</identifier>
 <datestamp>2013-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5685</id><created>2012-02-25</created><authors><author><keyname>Sivakumar</keyname><forenames>Lavanya</forenames></author><author><keyname>Dehmer</keyname><forenames>Matthias</forenames></author></authors><title>Information inequalities and Generalized Graph Entropies</title><categories>cs.IT math.IT</categories><comments>A preliminary version. To be submitted to a journal</comments><journal-ref>(2012) PLoS ONE 7(6): e38159</journal-ref><doi>10.1371/journal.pone.0038159</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we discuss the problem of establishing relations between
information measures assessed for network structures. Two types of entropy
based measures namely, the Shannon entropy and its generalization, the
R\'{e}nyi entropy have been considered for this study. Our main results involve
establishing formal relationship, in the form of implicit inequalities, between
these two kinds of measures when defined for graphs. Further, we also state and
prove inequalities connecting the classical partition-based graph entropies and
the functional-based entropy measures. In addition, several explicit
inequalities are derived for special classes of graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5686</identifier>
 <datestamp>2012-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5686</id><created>2012-02-25</created><authors><author><keyname>Das</keyname><forenames>Saptarshi</forenames></author><author><keyname>Pan</keyname><forenames>Indranil</forenames></author><author><keyname>Das</keyname><forenames>Shantanu</forenames></author><author><keyname>Gupta</keyname><forenames>Amitava</forenames></author></authors><title>Genetic Algorithm Based Improved Sub-Optimal Model Reduction in Nyquist
  Plane for Optimal Tuning Rule Extraction of PID and PI{\lambda}D{\mu}
  Controllers via Genetic Programming</title><categories>cs.SY</categories><comments>6 pages, 9 figures</comments><journal-ref>Proceedings of 2011 International Conference on Process
  Automation, Control and Computing, PACC 2011, art. no. 5978962, July 2011,
  Coimbatore</journal-ref><doi>10.1109/PACC.2011.5978962</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Genetic Algorithm (GA) has been used in this paper for a new Nyquist based
sub-optimal model reduction and optimal time domain tuning of PID and
fractional order (FO) PI{\lambda}D{\mu} controllers. Comparative studies show
that the new model reduction technique outperforms the conventional H2-norm
based reduced order modeling techniques. Optimum tuning rule has been developed
next with a test-bench of higher order processes via Genetic Programming (GP)
with minimum value of weighted integral error index and control signal. From
the Pareto optimal front which is a trade-off between the complexity of the
formulae and control performance, an efficient set of tuning rules has been
generated for time domain optimal PID and PI{\lambda}D{\mu} controllers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5689</identifier>
 <datestamp>2012-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5689</id><created>2012-02-25</created><authors><author><keyname>Majumder</keyname><forenames>Basudev</forenames></author><author><keyname>Das</keyname><forenames>Saptarshi</forenames></author><author><keyname>Pan</keyname><forenames>Indranil</forenames></author><author><keyname>Saha</keyname><forenames>Sayan</forenames></author><author><keyname>Das</keyname><forenames>Shantanu</forenames></author><author><keyname>Gupta</keyname><forenames>Amitava</forenames></author></authors><title>Estimation, Analysis and Smoothing of Self-Similar Network Induced
  Delays in Feedback Control of Nuclear Reactors</title><categories>cs.SY</categories><comments>6 pages, 6 figures</comments><journal-ref>Proceedings of 2011 International Conference on Process
  Automation, Control and Computing, PACC 2011, art. no. 5978960, July 2011,
  Coimbatore</journal-ref><doi>10.1109/PACC.2011.5978960</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper analyzes a nuclear reactor power signal that suffers from network
induced random delays in the shared data network while being fed-back to the
Reactor Regulating System (RRS). A detailed study is carried out to investigate
the self similarity of random delay dynamics due to the network traffic in
shared medium. The fractionality or selfsimilarity in the network induced delay
that corrupts the measured power signal coming from Self Powered Neutron
Detectors (SPND) is estimated and analyzed. As any fractional order randomness
is intrinsically different from conventional Gaussian kind of randomness, these
delay dynamics need to be handled efficiently, before reaching the controller
within the RRS. An attempt has been made to minimize the effect of the
randomness in the reactor power transient data with few classes of smoothing
filters. The performance measure of the smoothers with fractional order noise
consideration is also investigated into.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5690</identifier>
 <datestamp>2012-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5690</id><created>2012-02-25</created><authors><author><keyname>Mukherjee</keyname><forenames>Ayan</forenames></author><author><keyname>Pakhira</keyname><forenames>Anindya</forenames></author><author><keyname>Das</keyname><forenames>Saptarshi</forenames></author><author><keyname>Pan</keyname><forenames>Indranil</forenames></author><author><keyname>Gupta</keyname><forenames>Amitava</forenames></author></authors><title>Embedded Network Test-Bed for Validating Real-Time Control Algorithms to
  Ensure Optimal Time Domain Performance</title><categories>cs.SY</categories><comments>6 pages, 12 figures</comments><journal-ref>Proceedings of 2011 International Conference on Process
  Automation, Control and Computing, PACC 2011, art. no. 5979045, July 2011,
  Coimbatore</journal-ref><doi>10.1109/PACC.2011.5979045</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper presents a Stateflow based network test-bed to validate real-time
optimal control algorithms. Genetic Algorithm (GA) based time domain
performance index minimization is attempted for tuning of PI controller to
handle a balanced lag and delay type First Order Plus Time Delay (FOPTD)
process over network. The tuning performance is validated on a real-time
communication network with artificially simulated stochastic delay, packet loss
and out-of order packets characterizing the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5692</identifier>
 <datestamp>2012-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5692</id><created>2012-02-25</created><authors><author><keyname>Das</keyname><forenames>Saptarshi</forenames></author><author><keyname>Saha</keyname><forenames>Sayan</forenames></author><author><keyname>Mukherjee</keyname><forenames>Ayan</forenames></author><author><keyname>Pan</keyname><forenames>Indranil</forenames></author><author><keyname>Gupta</keyname><forenames>Amitava</forenames></author></authors><title>Adaptive Gain and Order Scheduling of Optimal Fractional Order
  PI{\lambda}D{\mu} Controllers with Radial Basis Function Neural-Network</title><categories>cs.SY</categories><comments>6 pages, 12 figures</comments><journal-ref>Proceedings of 2011 International Conference on Process
  Automation, Control and Computing, PACC 2011, art. no. 5979047, July 2011,
  Coimbatore</journal-ref><doi>10.1109/PACC.2011.5979047</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gain and order scheduling of fractional order (FO) PI{\lambda}D{\mu}
controllers are studied in this paper considering four different classes of
higher order processes. The mapping between the optimum PID/FOPID controller
parameters and the reduced order process models are done using Radial Basis
Function (RBF) type Artificial Neural Network (ANN). Simulation studies have
been done to show the effectiveness of the RBFNN for online scheduling of such
controllers with random change in set-point and process parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5693</identifier>
 <datestamp>2012-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5693</id><created>2012-02-25</created><authors><author><keyname>Das</keyname><forenames>Saptarshi</forenames></author><author><keyname>Majumder</keyname><forenames>Basudev</forenames></author><author><keyname>Pakhira</keyname><forenames>Anindya</forenames></author><author><keyname>Pan</keyname><forenames>Indranil</forenames></author><author><keyname>Das</keyname><forenames>Shantanu</forenames></author><author><keyname>Gupta</keyname><forenames>Amitava</forenames></author></authors><title>Optimizing Continued Fraction Expansion Based IIR Realization of
  Fractional Order Differ-Integrators with Genetic Algorithm</title><categories>cs.SY</categories><comments>6 pages, 8 figures</comments><journal-ref>Proceedings of 2011 International Conference on Process
  Automation, Control and Computing, PACC 2011, art. no. 5979043, July 2011,
  Coimbatore</journal-ref><doi>10.1109/PACC.2011.5979043</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rational approximation of fractional order (FO) differ-integrators via
Continued Fraction Expansion (CFE) is a well known technique. In this paper,
the nominal structures of various generating functions are optimized using
Genetic Algorithm (GA) to minimize the deviation in magnitude and phase
response between the original FO element and the rationalized discrete time
filter in Infinite Impulse Response (IIR) structure. The optimized filter based
realizations show better approximation of the FO elements in comparison with
the existing methods and is demonstrated by the frequency response of the IIR
filters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5695</identifier>
 <datestamp>2012-07-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5695</id><created>2012-02-25</created><updated>2012-07-05</updated><authors><author><keyname>Dahl</keyname><forenames>George E.</forenames></author><author><keyname>Adams</keyname><forenames>Ryan P.</forenames></author><author><keyname>Larochelle</keyname><forenames>Hugo</forenames></author></authors><title>Training Restricted Boltzmann Machines on Word Observations</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The restricted Boltzmann machine (RBM) is a flexible tool for modeling
complex data, however there have been significant computational difficulties in
using RBMs to model high-dimensional multinomial observations. In natural
language processing applications, words are naturally modeled by K-ary discrete
distributions, where K is determined by the vocabulary size and can easily be
in the hundreds of thousands. The conventional approach to training RBMs on
word observations is limited because it requires sampling the states of K-way
softmax visible units during block Gibbs updates, an operation that takes time
linear in K. In this work, we address this issue by employing a more general
class of Markov chain Monte Carlo operators on the visible units, yielding
updates with computational complexity independent of K. We demonstrate the
success of our approach by training RBMs on hundreds of millions of word
n-grams using larger vocabularies than previously feasible and using the
learned features to improve performance on chunking and sentiment
classification tasks, achieving state-of-the-art results on the latter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5710</identifier>
 <datestamp>2015-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5710</id><created>2012-02-25</created><updated>2015-01-22</updated><authors><author><keyname>Hegland</keyname><forenames>Markus</forenames><affiliation>Australian National University</affiliation></author><author><keyname>Leopardi</keyname><forenames>Paul</forenames><affiliation>Australian National University</affiliation></author></authors><title>Sparse grid quadrature on products of spheres</title><categories>math.NA cs.NA math.OC</categories><comments>34 pages, 6 figures. Accepted 7 January 2015 for publication in
  Numerical Algorithms. Revised at page proof stage to (1) update email
  address; (2) correct the accent on &quot;Wozniakowski&quot; on p. 7; (3) update
  reference 2; (4) correct references 3, 18 and 26</comments><msc-class>65D30 (Primary), 90C09 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We examine sparse grid quadrature on weighted tensor products (WTP) of
reproducing kernel Hilbert spaces on products of the unit sphere, in the case
of worst case quadrature error for rules with arbitrary quadrature weights. We
describe a dimension adaptive quadrature algorithm based on an algorithm of
Hegland (2003), and also formulate a version of Wasilkowski and Wozniakowski's
WTP algorithm (1999), here called the WW algorithm. We prove that the dimension
adaptive algorithm is optimal in the sense of Dantzig (1957) and therefore no
greater in cost than the WW algorithm. Both algorithms therefore have the
optimal asymptotic rate of convergence given by Theorem 3 of Wasilkowski and
Wozniakowski (1999). A numerical example shows that, even though the asymptotic
convergence rate is optimal, if the dimension weights decay slowly enough, and
the dimensionality of the problem is large enough, the initial convergence of
the dimension adaptive algorithm can be slow.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5713</identifier>
 <datestamp>2012-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5713</id><created>2012-02-25</created><authors><author><keyname>Potamias</keyname><forenames>Michalis</forenames></author></authors><title>The warm-start bias of Yelp ratings</title><categories>cs.SI</categories><comments>5 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Yelp ratings are often viewed as a reputation metric for local businesses. In
this paper we study how Yelp ratings evolve over time. Our main finding is that
on average the first ratings that businesses receive overestimate their
eventual reputation. In particular, the first review that a business receives
in our dataset averages 4.1 stars, while the 20th review averages just 3.69
stars. This significant warm-start bias which may be attributed to the limited
exposure of a business in its first steps may mask analysis performed on
ratings and reputational ramifications. Therefore, we study techniques to
identify and correct for this bias. Further, we perform a case study to explore
the effect of a Groupon deal on the merchant's subsequent ratings and show both
that previous research has overestimated Groupon's effect to merchants'
reputation and that average ratings anticorrelate with the number of reviews
received. Our analysis points to the importance of identifying and removing
biases from Yelp reviews.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5715</identifier>
 <datestamp>2012-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5715</id><created>2012-02-25</created><authors><author><keyname>Chen</keyname><forenames>Danny Z.</forenames></author><author><keyname>Wang</keyname><forenames>Haitao</forenames></author></authors><title>Computing L1 Shortest Paths among Polygonal Obstacles in the Plane</title><categories>cs.CG cs.DS</categories><comments>48 pages; 19 figures; partial results appeared in ESA 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a point $s$ and a set of $h$ pairwise disjoint polygonal obstacles of
totally $n$ vertices in the plane, we present a new algorithm for building an
$L_1$ shortest path map of size O(n) in $O(T)$ time and O(n) space such that
for any query point $t$, the length of the $L_1$ shortest obstacle-avoiding
path from $s$ to $t$ can be reported in $O(\log n)$ time and the actual
shortest path can be found in additional time proportional to the number of
edges of the path, where $T$ is the time for triangulating the free space. It
is currently known that $T=O(n+h\log^{1+\epsilon}h)$ for an arbitrarily small
constant $\epsilon&gt;0$. If the triangulation can be done optimally (i.e.,
$T=O(n+h\log h)$), then our algorithm is optimal. Previously, the best
algorithm computes such an $L_1$ shortest path map in $O(n\log n)$ time and
O(n) space. Our techniques can be extended to obtain improved results for other
related problems, e.g., computing the $L_1$ geodesic Voronoi diagram for a set
of point sites in a polygonal domain, finding shortest paths with fixed
orientations, finding approximate Euclidean shortest paths, etc.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5718</identifier>
 <datestamp>2012-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5718</id><created>2012-02-25</created><authors><author><keyname>Lai</keyname><forenames>Hsin-Hao</forenames></author><author><keyname>Lih</keyname><forenames>Ko-Wei</forenames></author></authors><title>Chordal Graphs are Fully Orientable</title><categories>math.CO cs.DM</categories><comments>11 pages, 1 figure, accepted by Ars Combinatoria (March 26, 2010)</comments><msc-class>05C99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Suppose that D is an acyclic orientation of a graph G. An arc of D is called
dependent if its reversal creates a directed cycle. Let m and M denote the
minimum and the maximum of the number of dependent arcs over all acyclic
orientations of G. We call G fully orientable if G has an acyclic orientation
with exactly d dependent arcs for every d satisfying m &lt;= d &lt;= M. A graph G is
called chordal if every cycle in G of length at least four has a chord. We show
that all chordal graphs are fully orientable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5722</identifier>
 <datestamp>2012-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5722</id><created>2012-02-25</created><authors><author><keyname>Mohan</keyname><forenames>Sibin</forenames></author><author><keyname>Bak</keyname><forenames>Stanley</forenames></author><author><keyname>Betti</keyname><forenames>Emiliano</forenames></author><author><keyname>Yun</keyname><forenames>Heechul</forenames></author><author><keyname>Sha</keyname><forenames>Lui</forenames></author><author><keyname>Caccamo</keyname><forenames>Marco</forenames></author></authors><title>S3A: Secure System Simplex Architecture for Enhanced Security of
  Cyber-Physical Systems</title><categories>cs.CR cs.SY</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Until recently, cyber-physical systems, especially those with safety-critical
properties that manage critical infrastructure (e.g. power generation plants,
water treatment facilities, etc.) were considered to be invulnerable against
software security breaches. The recently discovered 'W32.Stuxnet' worm has
drastically changed this perception by demonstrating that such systems are
susceptible to external attacks. Here we present an architecture that enhances
the security of safety-critical cyber-physical systems despite the presence of
such malware. Our architecture uses the property that control systems have
deterministic execution behavior, to detect an intrusion within 0.6 {\mu}s
while still guaranteeing the safety of the plant. We also show that even if an
attack is successful, the overall state of the physical system will still
remain safe. Even if the operating system's administrative privileges have been
compromised, our architecture will still be able to protect the physical system
from coming to harm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5749</identifier>
 <datestamp>2012-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5749</id><created>2012-02-26</created><authors><author><keyname>Kratsch</keyname><forenames>Stefan</forenames></author><author><keyname>Pilipczuk</keyname><forenames>Marcin</forenames></author><author><keyname>Pilipczuk</keyname><forenames>Micha&#x142;</forenames></author><author><keyname>Wahlstr&#xf6;m</keyname><forenames>Magnus</forenames></author></authors><title>Fixed-parameter tractability of multicut in directed acyclic graphs</title><categories>cs.DS cs.CC</categories><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The MULTICUT problem, given a graph G, a set of terminal pairs T={(s_i,t_i) |
1 &lt;= i &lt;= r} and an integer p, asks whether one can find a cutset consisting of
at most p non-terminal vertices that separates all the terminal pairs, i.e.,
after removing the cutset, t_i is not reachable from s_i for each 1 &lt;= i &lt;= r.
The fixed-parameter tractability of MULTICUT in undirected graphs,
parameterized by the size of the cutset only, has been recently proven by Marx
and Razgon (STOC'11) and, independently, by Bousquet et al. (STOC'11), after
resisting attacks as a long-standing open problem. In this paper we prove that
MULTICUT is fixed-parameter tractable on directed acyclic graphs, when
parameterized both by the size of the cutset and the number of terminal pairs.
We complement this result by showing that this is implausible for
parameterization by the size of the cutset only, as this version of the problem
remains W[1]-hard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5755</identifier>
 <datestamp>2013-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5755</id><created>2012-02-26</created><updated>2013-09-05</updated><authors><author><keyname>Kogan</keyname><forenames>Kirill</forenames></author><author><keyname>Lopez-Ortiz</keyname><forenames>Alejandro</forenames></author><author><keyname>Nikolenko</keyname><forenames>Sergey I.</forenames></author><author><keyname>Scalosub</keyname><forenames>Gabriel</forenames></author><author><keyname>Segal</keyname><forenames>Michael</forenames></author></authors><title>Balancing Work and Size with Bounded Buffers</title><categories>cs.NI cs.PF</categories><comments>22 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the fundamental problem of managing a bounded size queue buffer
where traffic consists of packets of varying size, where each packet requires
several rounds of processing before it can be transmitted from the queue
buffer. The goal in such an environment is to maximize the overall size of
packets that are successfully transmitted. This model is motivated by the
ever-growing ubiquity of network processors architectures, which must deal with
heterogeneously-sized traffic, with heterogeneous processing requirements. Our
work addresses the tension between two conflicting algorithmic approaches in
such settings: the tendency to favor packets with fewer processing
requirements, thus leading to fast contributions to the accumulated throughput,
as opposed to preferring packets of larger size, which imply a large increase
in throughput at each step. We present a model for studying such systems, and
present competitive algorithms whose performance depend on the maximum size a
packet may have, and maximum amount of processing a packet may require. We
further provide lower bounds on algorithms performance in such settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5762</identifier>
 <datestamp>2012-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5762</id><created>2012-02-26</created><authors><author><keyname>Beaulieu</keyname><forenames>Gabriel</forenames></author><author><keyname>Burke</keyname><forenames>Kyle</forenames></author><author><keyname>Duch&#xea;ne</keyname><forenames>Eric</forenames></author></authors><title>Impartial coloring games</title><categories>math.CO cs.CC cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Coloring games are combinatorial games where the players alternate painting
uncolored vertices of a graph one of $k &gt; 0$ colors. Each different ruleset
specifies that game's coloring constraints. This paper investigates six
impartial rulesets (five new), derived from previously-studied graph coloring
schemes, including proper map coloring, oriented coloring, 2-distance coloring,
weak coloring, and sequential coloring. For each, we study the outcome classes
for special cases and general computational complexity. In some cases we pay
special attention to the Grundy function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5797</identifier>
 <datestamp>2012-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5797</id><created>2012-02-26</created><updated>2012-03-01</updated><authors><author><keyname>Goertz</keyname><forenames>Inge Li</forenames></author><author><keyname>Nagarajan</keyname><forenames>Viswanath</forenames></author><author><keyname>Saket</keyname><forenames>Rishi</forenames></author></authors><title>Stochastic Vehicle Routing with Recourse</title><categories>cs.DS cs.CC</categories><comments>20 Pages, 1 figure Revision corrects the statement and proof of
  Theorem 1.2</comments><msc-class>68Q25, 68W05</msc-class><acm-class>F.2.2; G.2.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the classic Vehicle Routing Problem in the setting of stochastic
optimization with recourse. StochVRP is a two-stage optimization problem, where
demand is satisfied using two routes: fixed and recourse. The fixed route is
computed using only a demand distribution. Then after observing the demand
instantiations, a recourse route is computed -- but costs here become more
expensive by a factor lambda.
  We present an O(log^2 n log(n lambda))-approximation algorithm for this
stochastic routing problem, under arbitrary distributions. The main idea in
this result is relating StochVRP to a special case of submodular orienteering,
called knapsack rank-function orienteering. We also give a better approximation
ratio for knapsack rank-function orienteering than what follows from prior
work. Finally, we provide a Unique Games Conjecture based omega(1) hardness of
approximation for StochVRP, even on star-like metrics on which our algorithm
achieves a logarithmic approximation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5810</identifier>
 <datestamp>2013-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5810</id><created>2012-02-26</created><updated>2013-11-10</updated><authors><author><keyname>Blankertz</keyname><forenames>Raoul</forenames></author><author><keyname>Gathen</keyname><forenames>Joachim von zur</forenames></author><author><keyname>Ziegler</keyname><forenames>Konstantin</forenames></author></authors><title>Compositions and collisions at degree p^2</title><categories>math.AC cs.SC</categories><msc-class>12Y05 (Primary), 11T06, 05A15 (Secondary)</msc-class><acm-class>F.2.1</acm-class><journal-ref>Journal of Symbolic Computation 59 (2013) 113-145</journal-ref><doi>10.1016/j.jsc.2013.06.001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A univariate polynomial f over a field is decomposable if f = g o h = g(h)
for nonlinear polynomials g and h. In order to count the decomposables, one
wants to know, under a suitable normalization, the number of equal-degree
collisions of the form f = g o h = g^* o h^* with (g, h) = (g^*, h^*) and deg g
= deg g^*. Such collisions only occur in the wild case, where the field
characteristic p divides deg f. Reasonable bounds on the number of
decomposables over a finite field are known, but they are less sharp in the
wild case, in particular for degree p^2.
  We provide a classification of all polynomials of degree p^2 with a
collision. It yields the exact number of decomposable polynomials of degree p^2
over a finite field of characteristic p. We also present an efficient algorithm
that determines whether a given polynomial of degree p^2 has a collision or
not.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5820</identifier>
 <datestamp>2012-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5820</id><created>2012-02-26</created><authors><author><keyname>Zhang</keyname><forenames>Zi-Ke</forenames></author><author><keyname>Zhou</keyname><forenames>Tao</forenames></author><author><keyname>Zhang</keyname><forenames>Yi-Cheng</forenames></author></authors><title>Tag-Aware Recommender Systems: A State-of-the-art Survey</title><categories>cs.IR cs.SI</categories><comments>19 pages, 3 figures</comments><journal-ref>Journal of Computer Science and Technology 26 (2011) 767</journal-ref><doi>10.1007/s11390-011-0176-1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the past decade, Social Tagging Systems have attracted increasing
attention from both physical and computer science communities. Besides the
underlying structure and dynamics of tagging systems, many efforts have been
addressed to unify tagging information to reveal user behaviors and
preferences, extract the latent semantic relations among items, make
recommendations, and so on. Specifically, this article summarizes recent
progress about tag-aware recommender systems, emphasizing on the contributions
from three mainstream perspectives and approaches: network-based methods,
tensor-based methods, and the topic-based methods. Finally, we outline some
other tag-related works and future challenges of tag-aware recommendation
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5826</identifier>
 <datestamp>2012-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5826</id><created>2012-02-27</created><authors><author><keyname>Petrenko</keyname><forenames>Alexander K.</forenames></author><author><keyname>Schlingloff</keyname><forenames>Holger</forenames></author></authors><title>Proceedings 7th Workshop on Model-Based Testing</title><categories>cs.SE</categories><proxy>EPTCS</proxy><acm-class>D.2.4; D.2.5</acm-class><journal-ref>EPTCS 80, 2012</journal-ref><doi>10.4204/EPTCS.80</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains the proceedings of the Seventh Workshop on Model-Based
Testing (MBT 2012), which was held on 25 March, 2012 in Tallinn, Estonia, as a
satellite event of the European Joint Conferences on Theory and Practice of
Software, ETAPS 2012.
  The workshop is devoted to model-based testing of both software and hardware.
Model-based testing uses models describing the required behavior of the system
under consideration to guide such efforts as test selection and test results
evaluation. Testing validates the real system behavior against models and
checks that the implementation conforms to them, but is capable also to find
errors in the models themselves.
  The first MBT workshop was held in 2004, in Barcelona. At that time MBT
already had become a hot topic, but the MBT workshop was the first event
devoted mostly to this topic. Since that time the area has generated enormous
scientific interest, and today there are several specialized workshops and more
broad conferences on software and hardware design and quality assurance
covering model based testing. MBT has become one of the most powerful system
analysis tools, one of the latest hot topic related is applying MBT in security
analysis and testing. MBT workshop tries to keep up with current trends. In
2012 &quot;industrial paper&quot; category was added to the program and two industrial
papers were accepted by the program committee.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5830</identifier>
 <datestamp>2012-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5830</id><created>2012-02-27</created><authors><author><keyname>Lin</keyname><forenames>Pin-Hsun</forenames></author><author><keyname>Lai</keyname><forenames>Szu-Hsiang</forenames></author><author><keyname>Lin</keyname><forenames>Shih-Chun</forenames></author><author><keyname>Su</keyname><forenames>Hsuan-Jung</forenames></author></authors><title>On Secrecy Rate of the Generalized Artificial-Noise Assisted Secure
  Beamforming for Wiretap Channels</title><categories>cs.IT math.IT</categories><comments>28 pages, 7 figures, submitted to IEEE Transactions on Wireless
  Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider the secure transmission in fast Rayleigh fading
channels with full knowledge of the main channel and only the statistics of the
eavesdropper's channel state information at the transmitter. For the
multiple-input, single-output, single-antenna eavesdropper systems, we
generalize Goel and Negi's celebrated artificial-noise (AN) assisted
beamforming, which just selects the directions to transmit AN heuristically.
Our scheme may inject AN to the direction of the message, which outperforms
Goel and Negi's scheme where AN is only injected in the directions orthogonal
to the main channel. The ergodic secrecy rate of the proposed AN scheme can be
represented by a highly simplified power allocation problem. To attain it, we
prove that the optimal transmission scheme for the message bearing signal is a
beamformer, which is aligned to the direction of the legitimate channel. After
characterizing the optimal eigenvectors of the covariance matrices of signal
and AN, we also provide the necessary condition for transmitting AN in the main
channel to be optimal. Since the resulting secrecy rate is a non-convex power
allocation problem, we develop an algorithm to efficiently solve it. Simulation
results show that our generalized AN scheme outperforms Goel and Negi's,
especially when the quality of legitimate channel is much worse than that of
eavesdropper's. In particular, the regime with non-zero secrecy rate is
enlarged, which can significantly improve the connectivity of the secure
network when the proposed AN assisted beamforming is applied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5844</identifier>
 <datestamp>2012-04-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5844</id><created>2012-02-27</created><updated>2012-04-25</updated><authors><author><keyname>Meng</keyname><forenames>Deyu</forenames></author><author><keyname>Xu</keyname><forenames>Zongben</forenames></author></authors><title>Divide-and-Conquer Method for L1 Norm Matrix Factorization in the
  Presence of Outliers and Missing Data</title><categories>cs.NA cs.CV</categories><comments>19 pages, 2 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The low-rank matrix factorization as a L1 norm minimization problem has
recently attracted much attention due to its intrinsic robustness to the
presence of outliers and missing data. In this paper, we propose a new method,
called the divide-and-conquer method, for solving this problem. The main idea
is to break the original problem into a series of smallest possible
sub-problems, each involving only unique scalar parameter. Each of these
subproblems is proved to be convex and has closed-form solution. By recursively
optimizing these small problems in an analytical way, efficient algorithm,
entirely avoiding the time-consuming numerical optimization as an inner loop,
for solving the original problem can naturally be constructed. The
computational complexity of the proposed algorithm is approximately linear in
both data size and dimensionality, making it possible to handle large-scale L1
norm matrix factorization problems. The algorithm is also theoretically proved
to be convergent. Based on a series of experiment results, it is substantiated
that our method always achieves better results than the current
state-of-the-art methods on $L1$ matrix factorization calculation in both
computational time and accuracy, especially on large-scale applications such as
face recognition and structure from motion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5850</identifier>
 <datestamp>2012-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5850</id><created>2012-02-27</created><authors><author><keyname>Delzanno</keyname><forenames>Giorgio</forenames></author><author><keyname>Sangnier</keyname><forenames>Arnaud</forenames></author><author><keyname>Traverso</keyname><forenames>Riccardo</forenames></author><author><keyname>Zavattaro</keyname><forenames>Gianluigi</forenames></author></authors><title>The Cost of Parameterized Reachability in Mobile Ad Hoc Networks</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the impact of spontaneous movement in the complexity of
verification problems for an automata-based protocol model of networks with
selective broadcast communication. We first consider reachability of an error
state and show that parameterized verification is decidable with polynomial
complexity. We then move to richer queries and show how the complexity changes
when considering properties with negation or cardinality constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5856</identifier>
 <datestamp>2012-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5856</id><created>2012-02-27</created><updated>2012-09-03</updated><authors><author><keyname>Escala</keyname><forenames>Alex</forenames></author><author><keyname>Herranz</keyname><forenames>Javier</forenames></author><author><keyname>Libert</keyname><forenames>Benoit</forenames></author><author><keyname>Rafols</keyname><forenames>Carla</forenames></author></authors><title>Hierarchical Identity-Based Lossy Trapdoor Functions</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lossy trapdoor functions, introduced by Peikert and Waters (STOC'08), have
received a lot of attention in the last years, because of their wide range of
applications in theoretical cryptography. The notion has been recently extended
to the identity-based setting by Bellare et al. (Eurocrypt'12). We provide one
more step in this direction, by considering the notion of hierarchical
identity-based (lossy) trapdoor functions (HIB-TDFs). Hierarchical
identity-based cryptography has proved very useful both for practical
applications and to establish theoretical relations with other cryptographic
primitives.
  The notion of security for IB-TDFs put forward by Bellare et al. easily
extends to the hierarchical scenario, but an (H)IB-TDF secure in this sense is
not known to generically imply other related primitives with security against
adaptive-id adversaries, not even IND-ID-CPA secure encryption. Our first
contribution is to define a new security property for (H)IB-TDFs. We show that
functions satisfying this property imply secure cryptographic primitives in the
adaptive identity-based setting: these include encryption schemes with semantic
security under chosen-plaintext attacks, deterministic encryption schemes, and
(non-adaptive) hedged encryption schemes that maintain some security when
messages are encrypted using randomness of poor quality.
  Then, we describe the first pairing-based HIB-TDF realization. Our HIB-TDF
construction is based on techniques that differ from those of Bellare et al. in
that it uses a hierarchical predicate encryption scheme as a key ingredient.
The resulting HIB-TDF is proved to satisfy the new security definition, against
either selective or, for hierarchies of constant depth, adaptive adversaries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5857</identifier>
 <datestamp>2012-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5857</id><created>2012-02-27</created><authors><author><keyname>Hollanti</keyname><forenames>Camilla</forenames></author><author><keyname>Markin</keyname><forenames>Nadya</forenames></author></authors><title>Algebraic Fast-Decodable Relay Codes for Distributed Communications</title><categories>cs.IT math.IT math.RA</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, fast-decodable lattice code constructions are designed for the
nonorthogonal amplify-and-forward (NAF) multiple-input multiple-output (MIMO)
channel. The constructions are based on different types of algebraic
structures, e.g. quaternion division algebras. When satisfying certain
properties, these algebras provide us with codes whose structure naturally
reduces the decoding complexity. The complexity can be further reduced by
shortening the block length, i.e., by considering rectangular codes called less
than minimum delay (LMD) codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5885</identifier>
 <datestamp>2012-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5885</id><created>2012-02-27</created><updated>2012-04-05</updated><authors><author><keyname>Karpinski</keyname><forenames>Marek</forenames></author><author><keyname>Rucinski</keyname><forenames>Andrzej</forenames></author><author><keyname>Szymanska</keyname><forenames>Edyta</forenames></author></authors><title>Approximate Counting of Matchings in Sparse Hypergraphs</title><categories>cs.DS cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we give a fully polynomial randomized approximation scheme
(FPRAS) for the number of all matchings in hypergraphs belonging to a class of
sparse, uniform hypergraphs. Our method is based on a generalization of the
canonical path method to the case of uniform hypergraphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5888</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5888</id><created>2012-02-27</created><updated>2012-10-31</updated><authors><author><keyname>Ganesan</keyname><forenames>Ashwin</forenames></author></authors><title>An efficient algorithm for the diameter of Cayley graphs generated by
  transposition trees</title><categories>cs.DM cs.DS math.CO</categories><comments>A journal version that includes parts of arXiv:1111.3114</comments><journal-ref>IAENG International Journal of Applied Mathematics, vol. 42, no.
  4, pp. 214-223, November 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A problem of practical and theoretical interest is to determine or estimate
the diameter of various families of Cayley networks. The previously known
estimate for the diameter of Cayley graphs generated by transposition trees is
an upper bound given in the oft-cited paper of Akers and Krishnamurthy (1989).
In this work, we first assess the performance of their upper bound. We show
that for every $n$, there exists a tree on $n$ vertices, such that the
difference between the upper bound and the true diameter value is at least
$n-4$.
  Evaluating their upper bound takes time $\Omega(n!)$. In this paper, we
provide an algorithm that obtains an estimate of the diameter, but which
requires only time $O(n^2)$; furthermore, the value obtained by our algorithm
is less than or equal to the previously known diameter upper bound. Such an
improvement to polynomial time, while still performing at least as well as the
previous bound, is possible because our algorithm works directly with the
transposition tree on $n$ vertices and does not require examining any of the
permutations. We also provide a tree for which the value computed by our
algorithm is not necessarily unique, which is an important result because such
examples are quite rare. For all families of trees we have investigated so far,
each of the possible values computed by our algorithm happens to also be an
upper bound on the diameter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5895</identifier>
 <datestamp>2013-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5895</id><created>2012-02-27</created><updated>2013-01-21</updated><authors><author><keyname>Barbour</keyname><forenames>A. D.</forenames></author><author><keyname>Reinert</keyname><forenames>G.</forenames></author></authors><title>Asymptotic behaviour of gossip processes and small world networks</title><categories>math.PR cs.SI physics.soc-ph</categories><comments>30 pages</comments><msc-class>92H30, 60K35, 60J85</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Both small world models of random networks with occasional long range
connections and gossip processes with occasional long range transmission of
information have similar characteristic behaviour. The long range elements
appreciably reduce the effective distances, measured in space or in time,
between pairs of typical points. In this paper, we show that their common
behaviour can be interpreted as a product of the locally branching nature of
the models. In particular, it is shown that both typical distances between
points and the proportion of space that can be reached within a given distance
or time can be approximated by formulae involving the limit random variable of
the branching process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5909</identifier>
 <datestamp>2013-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5909</id><created>2012-02-27</created><updated>2012-12-27</updated><authors><author><keyname>Aldecoa</keyname><forenames>Rodrigo</forenames></author><author><keyname>Mar&#xed;n</keyname><forenames>Ignacio</forenames></author></authors><title>Closed benchmarks for network community structure characterization</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>18 pages, 5 figures. Available at
  http://pre.aps.org/abstract/PRE/v85/i2/e026109</comments><journal-ref>Phys. Rev. E 85, 026109 (2012)</journal-ref><doi>10.1103/PhysRevE.85.026109</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Characterizing the community structure of complex networks is a key challenge
in many scientific fields. Very diverse algorithms and methods have been
proposed to this end, many working reasonably well in specific situations.
However, no consensus has emerged on which of these methods is the best to use
in practice. In part, this is due to the fact that testing their performance
requires the generation of a comprehensive, standard set of synthetic
benchmarks, a goal not yet fully achieved. Here, we present a type of benchmark
that we call &quot;closed&quot;, in which an initial network of known community structure
is progressively converted into a second network whose communities are also
known. This approach differs from all previously published ones, in which
networks evolve toward randomness. The use of this type of benchmark allows us
to monitor the transformation of the community structure of a network.
Moreover, we can predict the optimal behavior of the variation of information,
a measure of the quality of the partitions obtained, at any moment of the
process. This enables us in many cases to determine the best partition among
those suggested by different algorithms. Also, since any network can be used as
a starting point, extensive studies and comparisons can be performed using a
heterogeneous set of structures, including random ones. These properties make
our benchmarks a general standard for comparing community detection algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5913</identifier>
 <datestamp>2012-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5913</id><created>2012-02-27</created><authors><author><keyname>Stoop</keyname><forenames>Ruedi</forenames></author><author><keyname>N&#xfc;esch</keyname><forenames>Patrick</forenames></author><author><keyname>Stoop</keyname><forenames>Ralph Lukas</forenames></author><author><keyname>Bunimovich</keyname><forenames>Leonid</forenames></author></authors><title>Fly out-smarts man</title><categories>q-bio.PE cs.CL physics.bio-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Precopulatory courtship is a high-cost, non-well understood animal world
mystery. Drosophila's (=D.'s) precopulatory courtship not only shows marked
structural similarities with mammalian courtship, but also with human spoken
language. This suggests the study of purpose, modalities and in particular of
the power of this language and to compare it to human language. Following a
mathematical symbolic dynamics approach, we translate courtship videos of D.'s
body language into a formal language. This approach made it possible to show
that D. may use its body language to express individual information -
information that may be important for evolutionary optimization, on top of the
sexual group membership. Here, we use Chomsky's hierarchical language
classification to characterize the power of D.'s body language, and then
compare it with the power of languages spoken by humans. We find that from a
formal language point of view, D.'s body language is at least as powerful as
the languages spoken by humans. From this we conclude that human intellect
cannot be the direct consequence of the formal grammar complexity of human
language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5919</identifier>
 <datestamp>2012-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5919</id><created>2012-02-27</created><authors><author><keyname>Stapel</keyname><forenames>Kai</forenames></author><author><keyname>Schneider</keyname><forenames>Kurt</forenames></author></authors><title>FLOW-Methode - Methodenbeschreibung zur Anwendung von FLOW</title><categories>cs.SE</categories><comments>49 pages in German. Method developed in research project InfoFLOW
  (http://www.se.uni-hannover.de/pages/en:projekte_flow)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Information of many kinds is flowing in software projects and organizations.
Requirements have to flow from the customer to the developers. Testers need to
know the requirements as well. Boundary conditions and design decisions have to
be at the right place at the right time. Information flow analysis with FLOW
facilitates modeling of mode and route of the flow of information and
experience independent of the development methodology. Experience often acts as
a control factor, because experienced developers can process and route
information more efficiently. Therefore, experience needs to be at the right
place at the right time, too. However, most valuable experiences never get
documented. Since information and experience is flowing in agile as well as in
traditional environments, the FLOW method does not distinguish between agile
and traditional, but only between how the flows are shaped.
  ----
  In Softwareprojekten flie{\ss}en vielerlei Informationen. Anforderungen
m\&quot;ussen vom Kunden zu den Entwicklern gelangen. Auch Tester m\&quot;ussen die
Anforderungen kennen. Randbedingungen und Entwurfsentscheidungen m\&quot;ussen zur
rechten Zeit am rechten Ort sein. Die Informationsflussanalyse mit FLOW
erm\&quot;oglicht es, unabh\&quot;angig von der Entwicklungsmethode zu modellieren, wie
und auf welchem Wege Informationen und Erfahrungen flie{\ss}en. Erfahrungen
spielen dabei oft die Rolle von Steuergr\&quot;o{\ss}en, denn erfahrene Mitarbeiter
k\&quot;onnen Informationen kompetenter bearbeiten und weiterleiten. Auch die
Erfahrungen m\&quot;ussen in geeigneter Form zur rechten Zeit am rechten Ort sein.
Viele Erfahrungen werden aber nie dokumentiert. Da Informationen und
Erfahrungen sowohl in agilen als auch in traditionellen Umgebungen flie{\ss}en
m\&quot;ussen, wird in FLOW ein Modell aufgebaut, das nicht nach agil, traditionell
oder anderen Bezeichnungen unterscheidet, sondern einzig danach, wie die
Fl\&quot;usse gestaltet sind.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5921</identifier>
 <datestamp>2012-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5921</id><created>2012-02-27</created><updated>2012-02-28</updated><authors><author><keyname>Constantinescu</keyname><forenames>Nicolae</forenames></author></authors><title>Estimators in Cryptography</title><categories>cs.CR</categories><comments>7 pages</comments><journal-ref>Ann. Univ. Tibiscus Comp. Sci. Series VII/2 (2009), 23-29</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the main problems in cryptography is to give criteria to provide good
comparators of cipher systems. The security of a cipher system must include the
security of the algorithm, the security of the key generator and management
module (see [BM94], [CM97],[Mau92a]) and the security of the cryptographic key
agreement protocol (see [Mau93a],[MC94],[Mau93b],[Mau92b]). This paper gives
show the necessary mathematical background to estimate the most important
cryptographic measures of the key generators and of the unconditionally key
agreement protocols. These cryptographic measures are the Shannon entropy (for
the key generator module) and Renyi entropy of order alpha for the key
agreement protocol.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5938</identifier>
 <datestamp>2012-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5938</id><created>2012-02-27</created><authors><author><keyname>Siddique</keyname><forenames>Qasim</forenames></author></authors><title>Intelligent Car System</title><categories>cs.RO</categories><comments>12 pages</comments><journal-ref>Ann. Univ. Tibiscus Comp. Sci. Series VII/2 (2009), 113-124</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In modern life the road safety has becomes the core issue. One single move of
a driver can cause horrifying accident. The main goal of intelligent car system
is to make communication with other cars on the road. The system is able to
control to speed, direction and the distance between the cars the intelligent
car system is able to recognize traffic light and is able to take decision
according to it. This paper presents a framework of the intelligent car system.
I validate several aspect of our system using simulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5941</identifier>
 <datestamp>2012-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5941</id><created>2012-02-27</created><authors><author><keyname>Reddy</keyname><forenames>P. Chenna</forenames></author></authors><title>TCP over IEEE 802.11</title><categories>cs.NI</categories><comments>11 pages</comments><journal-ref>Ann. Univ. Tibiscus Comp. Sci. Series VII/2 (2009), 173-183</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  IEEE 802.11 is a widely used wireless LAN standard for medium access control.
TCP is a prominent transport protocol originally designed for wired networks.
TCP treats packet loss as congestion and reduces the data rate. In wireless
networks packets are lost not only due to congestion but also due to various
other reasons. Hence there is need for making TCP adaptable to wireless
networks. Various parameters of TCP and IEEE 802.11 can be set to appropriate
values to achieve optimum performance results. In this paper optimum values for
various parameters of IEEE 802.11 are determined. Network simulator NS2 is used
for simulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5944</identifier>
 <datestamp>2012-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5944</id><created>2012-02-27</created><updated>2012-02-28</updated><authors><author><keyname>Zamo&#x15f;teanu</keyname><forenames>Alina Oana</forenames></author></authors><title>Computer applications in clinical psychology</title><categories>cs.OH</categories><comments>14 pages</comments><journal-ref>Ann. Univ. Tibiscus Comp. Sci. Series VII/2 (2009), 159-172</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The computer-assisted analysis is not currently a novelty, but a necessity in
all areas of psychology. A number of studies that examine the limits of the
computer assisted and analyzed interpretations, also its advantages. A series
of studies aim to assess how the computer assisting programs are able to
establish a diagnosis referring to the presence of certain mental disorders. We
will present the results of one computer application in clinical psychology
regarding the assessment of Theory of Mind capacity by animation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5945</identifier>
 <datestamp>2012-06-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5945</id><created>2012-02-27</created><updated>2012-06-12</updated><authors><author><keyname>Devroye</keyname><forenames>Luc</forenames></author><author><keyname>Morin</keyname><forenames>Pat</forenames></author></authors><title>A Note on Interference in Random Point Sets</title><categories>cs.CG cs.NI</categories><comments>Updated for journal submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The (maximum receiver-centric) interference of a geometric graph (von
Rickenbach etal (2005)) is studied. It is shown that, with high probability,
the following results hold for a set, V, of n points independently and
uniformly distributed in the unit d-cube, for constant dimension d: (1) there
exists a connected graph with vertex set V that has interference O((log
n)^{1/3}); (2) no connected graph with vertex set V has interference o((log
n)^{1/4}); and (3) the minimum spanning tree of $V$ has interference
Theta((\log n)^{1/2}).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5953</identifier>
 <datestamp>2012-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5953</id><created>2012-02-27</created><authors><author><keyname>Shukla</keyname><forenames>Ripunjai Kumar</forenames></author><author><keyname>Chakraborty</keyname><forenames>Soubhik</forenames></author></authors><title>On an Ethical Use of Neural Networks: A Case Study on a North Indian
  Raga</title><categories>cs.NE cs.SD</categories><comments>16 pages</comments><journal-ref>Ann. Univ. Tibiscus Comp. Sci. Series VII/2 (2009), 41-56</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper gives an artificial neural network (ANN) approach to time series
modeling, the data being instance versus notes (characterized by pitch)
depicting the structure of a North Indian raga, namely, Bageshree. Respecting
the sentiments of the artists' community, the paper argues why it is more
ethical to model a structure than try and &quot;manufacture&quot; an artist by training
the neural network to copy performances of artists. Indian Classical Music
centers on the ragas, where emotion and devotion are both important and neither
can be substituted by such &quot;calculated artistry&quot; which the ANN generated copies
are ultimately up to.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5954</identifier>
 <datestamp>2014-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5954</id><created>2012-02-27</created><authors><author><keyname>Antonopoulos</keyname><forenames>Angelos</forenames></author><author><keyname>Verikoukis</keyname><forenames>Christos</forenames></author></authors><title>Game Theoretic Network Coding-aided MAC for Data Dissemination towards
  Energy Efficiency</title><categories>cs.GT cs.NI</categories><comments>accepted at IEEE CoCoNet (Cooperative and Cognitive Mobile Networks)
  Workshop 2012, co-located with IEEE ICC 2012 in Ottawa, Canada</comments><doi>10.1109/ICC.2012.6364734</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose game theoretic Medium Access Control (MAC)
strategies for data dissemination scenarios. In particular, we use energy-based
utility functions that inherently imply power-awareness, while we consider
network coding techniques to eliminate the necessity of exchanging
acknowledgement control packets. Simulation results show that our proposed
strategies enhance the energy efficiency of the system and reduce the
dissemination completion time compared to an optimized standard protocol.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5959</identifier>
 <datestamp>2012-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5959</id><created>2012-02-27</created><updated>2012-02-28</updated><authors><author><keyname>Biernacki</keyname><forenames>Dariusz</forenames></author><author><keyname>Lenglet</keyname><forenames>Serguei</forenames></author></authors><title>Normal Form Bisimulations for Delimited-Control Operators</title><categories>cs.PL</categories><acm-class>D.3.1; D.3.3; F.3.2; F.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define a notion of normal form bisimilarity for the untyped call-by-value
lambda calculus extended with the delimited-control operators shift and reset.
Normal form bisimilarities are simple, easy-to-use behavioral equivalences
which relate terms without having to test them within all contexts (like
contextual equivalence), or by applying them to function arguments (like
applicative bisimilarity). We prove that the normal form bisimilarity for shift
and reset is sound but not complete w.r.t. contextual equivalence and we define
up-to techniques that aim at simplifying bisimulation proofs. Finally, we
illustrate the simplicity of the techniques we develop by proving several
equivalences on terms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5961</identifier>
 <datestamp>2013-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5961</id><created>2012-02-27</created><authors><author><keyname>Bulian</keyname><forenames>Jannis</forenames></author><author><keyname>Hodkinson</keyname><forenames>Ian</forenames></author></authors><title>Bare canonicity of representable cylindric and polyadic algebras</title><categories>math.LO cs.LO</categories><msc-class>03G15 (Primary) 03C05, 06B15, 06E15, 06E25 (Secondary)</msc-class><journal-ref>Annals of Pure and Applied Logic 164 (2013), pp. 884-906</journal-ref><doi>10.1016/j.apal.2013.04.002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that for finite n at least 3, every first-order axiomatisation of the
varieties of representable n-dimensional cylindric algebras, diagonal-free
cylindric algebras, polyadic algebras, and polyadic equality algebras contains
an infinite number of non-canonical formulas. We also show that the class of
structures for each of these varieties is non-elementary. The proofs employ
algebras derived from random graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5964</identifier>
 <datestamp>2012-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5964</id><created>2012-02-27</created><authors><author><keyname>Khan</keyname><forenames>Muhammad Taimoor</forenames></author><author><keyname>Usman</keyname><forenames>Anila</forenames></author></authors><title>Technique detection software for Sparse Matrices</title><categories>cs.MS</categories><comments>10 pages</comments><journal-ref>Ann. Univ. Tibiscus Comp. Sci. Series VII/2 (2009), 57-66</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse storage formats are techniques for storing and processing the sparse
matrix data efficiently. The performance of these storage formats depend upon
the distribution of non-zeros, within the matrix in different dimensions. In
order to have better results we need a technique that suits best the
organization of data in a particular matrix. So the decision of selecting a
better technique is the main step towards improving the system's results
otherwise the efficiency can be decreased. The purpose of this research is to
help identify the best storage format in case of reduced storage size and high
processing efficiency for a sparse matrix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5967</identifier>
 <datestamp>2012-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5967</id><created>2012-02-27</created><authors><author><keyname>Gunduz</keyname><forenames>Deniz</forenames></author><author><keyname>Erkip</keyname><forenames>Elza</forenames></author><author><keyname>Goldsmith</keyname><forenames>Andrea</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Joint Source-Channel Cooperative Transmission over Relay-Broadcast
  Networks</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory, 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reliable transmission of a discrete memoryless source over a multiple-relay
relay-broadcast network is considered. Motivated by sensor network
applications, it is assumed that the relays and the destinations all have
access to side information correlated with the underlying source signal. Joint
source-channel cooperative transmission is studied in which the relays help the
transmission of the source signal to the destinations by using both their
overheard signals, as in the classical channel cooperation scenario, as well as
the available correlated side information. Decode-and-forward (DF) based
cooperative transmission is considered in a network of multiple relay terminals
and two different achievability schemes are proposed: i) a regular encoding and
sliding-window decoding scheme without explicit source binning at the encoder,
and ii) a semi-regular encoding and backward decoding scheme with binning based
on the side information statistics. It is shown that both of these schemes lead
to the same source-channel code rate, which is shown to be the &quot;source-channel
capacity&quot; in the case of i) a physically degraded relay network in which the
side information signals are also degraded in the same order as the channel;
and ii) a relay-broadcast network in which all the terminals want to
reconstruct the source reliably, while at most one of them can act as a relay.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.5985</identifier>
 <datestamp>2012-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.5985</id><created>2012-02-27</created><authors><author><keyname>Giot</keyname><forenames>Romain</forenames><affiliation>GREYC</affiliation></author><author><keyname>El-Abed</keyname><forenames>Mohamad</forenames><affiliation>GREYC</affiliation></author><author><keyname>Rosenberger</keyname><forenames>Christophe</forenames><affiliation>GREYC</affiliation></author></authors><title>Fast computation of the performance evaluation of biometric systems:
  application to multibiometric</title><categories>cs.DS</categories><comments>Future Generation Computer Systems (2012)</comments><proxy>ccsd</proxy><doi>10.1016/j.future.2012.02.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The performance evaluation of biometric systems is a crucial step when
designing and evaluating such systems. The evaluation process uses the Equal
Error Rate (EER) metric proposed by the International Organization for
Standardization (ISO/IEC). The EER metric is a powerful metric which allows
easily comparing and evaluating biometric systems. However, the computation
time of the EER is, most of the time, very intensive. In this paper, we propose
a fast method which computes an approximated value of the EER. We illustrate
the benefit of the proposed method on two applications: the computing of non
parametric confidence intervals and the use of genetic algorithms to compute
the parameters of fusion functions. Experimental results show the superiority
of the proposed EER approximation method in term of computing time, and the
interest of its use to reduce the learning of parameters with genetic
algorithms. The proposed method opens new perspectives for the development of
secure multibiometrics systems by speeding up their computation time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6001</identifier>
 <datestamp>2012-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6001</id><created>2012-02-27</created><updated>2012-02-27</updated><authors><author><keyname>Yun</keyname><forenames>Hyokun</forenames></author><author><keyname>Vishwanathan</keyname><forenames>S. V. N.</forenames></author></authors><title>Efficiently Sampling Multiplicative Attribute Graphs Using a
  Ball-Dropping Process</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a novel and efficient sampling algorithm for the Multiplicative
Attribute Graph Model (MAGM - Kim and Leskovec (2010)}). Our algorithm is
\emph{strictly} more efficient than the algorithm proposed by Yun and
Vishwanathan (2012), in the sense that our method extends the \emph{best} time
complexity guarantee of their algorithm to a larger fraction of parameter
space. Both in theory and in empirical evaluation on sparse graphs, our new
algorithm outperforms the previous one. To design our algorithm, we first
define a stochastic \emph{ball-dropping process} (BDP). Although a special case
of this process was introduced as an efficient approximate sampling algorithm
for the Kronecker Product Graph Model (KPGM - Leskovec et al. (2010)}), neither
\emph{why} such an approximation works nor \emph{what} is the actual
distribution this process is sampling from has been addressed so far to the
best of our knowledge. Our rigorous treatment of the BDP enables us to clarify
the rational behind a BDP approximation of KPGM, and design an efficient
sampling algorithm for the MAGM.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6009</identifier>
 <datestamp>2012-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6009</id><created>2012-02-27</created><authors><author><keyname>Domingo-Ferrer</keyname><forenames>Josep</forenames></author></authors><title>Marginality: a numerical mapping for enhanced treatment of nominal and
  hierarchical attributes</title><categories>cs.AI</categories><comments>12 pages</comments><msc-class>62-07 Data Analysis</msc-class><acm-class>H.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The purpose of statistical disclosure control (SDC) of microdata, a.k.a. data
anonymization or privacy-preserving data mining, is to publish data sets
containing the answers of individual respondents in such a way that the
respondents corresponding to the released records cannot be re-identified and
the released data are analytically useful. SDC methods are either based on
masking the original data, generating synthetic versions of them or creating
hybrid versions by combining original and synthetic data. The choice of SDC
methods for categorical data, especially nominal data, is much smaller than the
choice of methods for numerical data. We mitigate this problem by introducing a
numerical mapping for hierarchical nominal data which allows computing means,
variances and covariances on them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6033</identifier>
 <datestamp>2013-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6033</id><created>2012-02-27</created><updated>2013-10-13</updated><authors><author><keyname>Borgs</keyname><forenames>Christian</forenames></author><author><keyname>Brautbar</keyname><forenames>Michael</forenames></author><author><keyname>Chayes</keyname><forenames>Jennifer</forenames></author><author><keyname>Khanna</keyname><forenames>Sanjeev</forenames></author><author><keyname>Lucier</keyname><forenames>Brendan</forenames></author></authors><title>The Power of Local Information in Social Networks</title><categories>cs.SI cs.DM cs.DS physics.soc-ph</categories><comments>An extended abstract of this work appeared in the 8th Workshop on
  Internet &amp; Network Economics (WINE 2012)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the power of \textit{local information algorithms} for optimization
problems on social networks. We focus on sequential algorithms for which the
network topology is initially unknown and is revealed only within a local
neighborhood of vertices that have been irrevocably added to the output set.
The distinguishing feature of this setting is that locality is necessitated by
constraints on the network information visible to the algorithm, rather than
being desirable for reasons of efficiency or parallelizability. In this sense,
changes to the level of network visibility can have a significant impact on
algorithm design.
  We study a range of problems under this model of algorithms with local
information. We first consider the case in which the underlying graph is a
preferential attachment network. We show that one can find the node of maximum
degree in the network in a polylogarithmic number of steps, using an
opportunistic algorithm that repeatedly queries the visible node of maximum
degree. This addresses an open question of Bollob{\'a}s and Riordan. In
contrast, local information algorithms require a linear number of queries to
solve the problem on arbitrary networks.
  Motivated by problems faced by recruiters in online networks, we also
consider network coverage problems such as finding a minimum dominating set.
For this optimization problem we show that, if each node added to the output
set reveals sufficient information about the set's neighborhood, then it is
possible to design randomized algorithms for general networks that nearly match
the best approximations possible even with full access to the graph structure.
We show that this level of visibility is necessary.
  We conclude that a network provider's decision of how much structure to make
visible to its users can have a significant effect on a user's ability to
interact strategically with the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6035</identifier>
 <datestamp>2012-04-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6035</id><created>2012-02-27</created><updated>2012-04-16</updated><authors><author><keyname>Ruozzi</keyname><forenames>Nicholas</forenames></author></authors><title>The Bethe Partition Function of Log-supermodular Graphical Models</title><categories>cs.DM math-ph math.CO math.MP</categories><comments>Typo, bug fixes, and improved exposition</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sudderth, Wainwright, and Willsky have conjectured that the Bethe
approximation corresponding to any fixed point of the belief propagation
algorithm over an attractive, pairwise binary graphical model provides a lower
bound on the true partition function. In this work, we resolve this conjecture
in the affirmative by demonstrating that, for any graphical model with binary
variables whose potential functions (not necessarily pairwise) are all
log-supermodular, the Bethe partition function always lower bounds the true
partition function. The proof of this result follows from a new variant of the
&quot;four functions&quot; theorem that may be of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6037</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6037</id><created>2012-02-09</created><updated>2012-04-10</updated><authors><author><keyname>Wagner</keyname><forenames>Noam</forenames></author><author><keyname>Eldar</keyname><forenames>Yonina C.</forenames></author><author><keyname>Friedman</keyname><forenames>Zvi</forenames></author></authors><title>Compressed Beamforming in Ultrasound Imaging</title><categories>cs.IT cs.CV math.IT</categories><comments>14 pages, 11 figures</comments><doi>10.1109/TSP.2012.2200891</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Emerging sonography techniques often require increasing the number of
transducer elements involved in the imaging process. Consequently, larger
amounts of data must be acquired and processed. The significant growth in the
amounts of data affects both machinery size and power consumption. Within the
classical sampling framework, state of the art systems reduce processing rates
by exploiting the bandpass bandwidth of the detected signals. It has been
recently shown, that a much more significant sample-rate reduction may be
obtained, by treating ultrasound signals within the Finite Rate of Innovation
framework. These ideas follow the spirit of Xampling, which combines classic
methods from sampling theory with recent developments in Compressed Sensing.
Applying such low-rate sampling schemes to individual transducer elements,
which detect energy reflected from biological tissues, is limited by the noisy
nature of the signals. This often results in erroneous parameter extraction,
bringing forward the need to enhance the SNR of the low-rate samples. In our
work, we achieve SNR enhancement, by beamforming the sub-Nyquist samples
obtained from multiple elements. We refer to this process as &quot;compressed
beamforming&quot;. Applying it to cardiac ultrasound data, we successfully image
macroscopic perturbations, while achieving a nearly eight-fold reduction in
sample-rate, compared to standard techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6038</identifier>
 <datestamp>2012-02-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6038</id><created>2012-02-27</created><authors><author><keyname>Baghaie</keyname><forenames>Marjan</forenames></author><author><keyname>Hochbaum</keyname><forenames>Dorit S.</forenames></author><author><keyname>Krishnamachari</keyname><forenames>Bhaskar</forenames></author></authors><title>Multiflow Transmission in Delay Constrained Cooperative Wireless
  Networks</title><categories>cs.NI cs.CC</categories><comments>9 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the problem of energy-efficient transmission in
multi-flow multihop cooperative wireless networks. Although the performance
gains of cooperative approaches are well known, the combinatorial nature of
these schemes makes it difficult to design efficient polynomial-time algorithms
for joint routing, scheduling and power control. This becomes more so when
there is more than one flow in the network. It has been conjectured by many
authors, in the literature, that the multiflow problem in cooperative networks
is an NP-hard problem. In this paper, we formulate the problem, as a
combinatorial optimization problem, for a general setting of $k$-flows, and
formally prove that the problem is not only NP-hard but it is
$o(n^{1/7-\epsilon})$ inapproxmiable. To our knowledge*, these results provide
the first such inapproxmiablity proof in the context of multiflow cooperative
wireless networks. We further prove that for a special case of k = 1 the
solution is a simple path, and devise a polynomial time algorithm for jointly
optimizing routing, scheduling and power control. We then use this algorithm to
establish analytical upper and lower bounds for the optimal performance for the
general case of $k$ flows. Furthermore, we propose a polynomial time heuristic
for calculating the solution for the general case and evaluate the performance
of this heuristic under different channel conditions and against the analytical
upper and lower bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6042</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6042</id><created>2012-02-27</created><updated>2013-02-19</updated><authors><author><keyname>Xu</keyname><forenames>Kevin S.</forenames></author><author><keyname>Kliger</keyname><forenames>Mark</forenames></author><author><keyname>Hero</keyname><forenames>Alfred O.</forenames><suffix>III</suffix></author></authors><title>A Regularized Graph Layout Framework for Dynamic Network Visualization</title><categories>cs.SI cs.DM stat.CO</categories><comments>To appear in Data Mining and Knowledge Discovery, supporting material
  (animations and MATLAB toolbox) available at
  http://tbayes.eecs.umich.edu/xukevin/visualization_dmkd_2012</comments><acm-class>G.2.2; H.3.4; H.5</acm-class><journal-ref>Data Mining and Knowledge Discovery 27 (2013) 84-116</journal-ref><doi>10.1007/s10618-012-0286-6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many real-world networks, including social and information networks, are
dynamic structures that evolve over time. Such dynamic networks are typically
visualized using a sequence of static graph layouts. In addition to providing a
visual representation of the network structure at each time step, the sequence
should preserve the mental map between layouts of consecutive time steps to
allow a human to interpret the temporal evolution of the network. In this
paper, we propose a framework for dynamic network visualization in the on-line
setting where only present and past graph snapshots are available to create the
present layout. The proposed framework creates regularized graph layouts by
augmenting the cost function of a static graph layout algorithm with a grouping
penalty, which discourages nodes from deviating too far from other nodes
belonging to the same group, and a temporal penalty, which discourages large
node movements between consecutive time steps. The penalties increase the
stability of the layout sequence, thus preserving the mental map. We introduce
two dynamic layout algorithms within the proposed framework, namely dynamic
multidimensional scaling (DMDS) and dynamic graph Laplacian layout (DGLL). We
apply these algorithms on several data sets to illustrate the importance of
both grouping and temporal regularization for producing interpretable
visualizations of dynamic networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6043</identifier>
 <datestamp>2012-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6043</id><created>2012-02-27</created><updated>2012-12-18</updated><authors><author><keyname>Mamino</keyname><forenames>Marcello</forenames></author></authors><title>On the computational complexity of a game of cops and robbers</title><categories>cs.CC</categories><comments>15 pages, 2 figures. Final accepted version, to be published in
  Theoretical Computer Science</comments><msc-class>91A24, 91A43, 68Q17</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the computational complexity of a perfect-information two-player
game proposed by Aigner and Fromme. The game takes place on an undirected graph
where n simultaneously moving cops attempt to capture a single robber, all
moving at the same speed. The players are allowed to pick their starting
positions at the first move. The question of the computational complexity of
deciding this game was raised in the '90s by Goldstein and Reingold. We prove
that the game is hard for PSPACE.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6049</identifier>
 <datestamp>2012-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6049</id><created>2012-02-27</created><authors><author><keyname>Pasqualetti</keyname><forenames>Fabio</forenames></author><author><keyname>D&#xf6;rfler</keyname><forenames>Florian</forenames></author><author><keyname>Bullo</keyname><forenames>Francesco</forenames></author></authors><title>Attack Detection and Identification in Cyber-Physical Systems -- Part
  II: Centralized and Distributed Monitor Design</title><categories>math.OC cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cyber-physical systems integrate computation, communication, and physical
capabilities to interact with the physical world and humans. Besides failures
of components, cyber-physical systems are prone to malicious attacks so that
specific analysis tools and monitoring mechanisms need to be developed to
enforce system security and reliability. This paper builds upon the results
presented in our companion paper [1] and proposes centralized and distributed
monitors for attack detection and identification. First, we design optimal
centralized attack detection and identification monitors. Optimality refers to
the ability of detecting (respectively identifying) every detectable
(respectively identifiable) attack. Second, we design an optimal distributed
attack detection filter based upon a waveform relaxation technique. Third, we
show that the attack identification problem is computationally hard, and we
design a sub-optimal distributed attack identification procedure with
performance guarantees. Finally, we illustrate the robustness of our monitors
to system noise and unmodeled dynamics through a simulation study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6071</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6071</id><created>2012-02-27</created><updated>2014-08-11</updated><authors><author><keyname>Guruswami</keyname><forenames>Venkatesan</forenames></author><author><keyname>Sinop</keyname><forenames>Ali Kemal</forenames></author><author><keyname>Zhou</keyname><forenames>Yuan</forenames></author></authors><title>Constant Factor Lasserre Integrality Gaps for Graph Partitioning
  Problems</title><categories>cs.CC cs.DM</categories><comments>15 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Partitioning the vertices of a graph into two roughly equal parts while
minimizing the number of edges crossing the cut is a fundamental problem
(called Balanced Separator) that arises in many settings. For this problem, and
variants such as the Uniform Sparsest Cut problem where the goal is to minimize
the fraction of pairs on opposite sides of the cut that are connected by an
edge, there are large gaps between the known approximation algorithms and
non-approximability results. While no constant factor approximation algorithms
are known, even APX-hardness is not known either.
  In this work we prove that for balanced separator and uniform sparsest cut,
semidefinite programs from the Lasserre hierarchy (which are the most powerful
relaxations studied in the literature) have an integrality gap bounded away
from $1$, even for $\Omega(n)$ levels of the hierarchy. This complements recent
algorithmic results in Guruswami and Sinop (2011) which used the Lasserre
hierarchy to give an approximation scheme for these problems (with runtime
depending on the spectrum of the graph). Along the way, we make an observation
that simplifies the task of lifting &quot;polynomial constraints&quot; (such as the
global balance constraint in balanced separator) to higher levels of the
Lasserre hierarchy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6078</identifier>
 <datestamp>2012-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6078</id><created>2012-02-27</created><authors><author><keyname>Daume</keyname><forenames>Hal</forenames><suffix>III</suffix></author><author><keyname>Phillips</keyname><forenames>Jeff M.</forenames></author><author><keyname>Saha</keyname><forenames>Avishek</forenames></author><author><keyname>Venkatasubramanian</keyname><forenames>Suresh</forenames></author></authors><title>Protocols for Learning Classifiers on Distributed Data</title><categories>stat.ML cs.LG</categories><comments>19 pages, 12 figures, accepted at AISTATS 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of learning classifiers for labeled data that has
been distributed across several nodes. Our goal is to find a single classifier,
with small approximation error, across all datasets while minimizing the
communication between nodes. This setting models real-world communication
bottlenecks in the processing of massive distributed datasets. We present
several very general sampling-based solutions as well as some two-way protocols
which have a provable exponential speed-up over any one-way protocol. We focus
on core problems for noiseless data distributed across two or more nodes. The
techniques we introduce are reminiscent of active learning, but rather than
actively probing labels, nodes actively communicate with each other, each node
simultaneously learning the important data from another node.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6079</identifier>
 <datestamp>2012-04-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6079</id><created>2012-02-27</created><updated>2012-04-17</updated><authors><author><keyname>Kissinger</keyname><forenames>Aleks</forenames></author></authors><title>Synthesising Graphical Theories</title><categories>cs.AI math.CT quant-ph</categories><comments>10 pages, 22 figures. Shortened and one theorem added</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, diagrammatic languages have been shown to be a powerful and
expressive tool for reasoning about physical, logical, and semantic processes
represented as morphisms in a monoidal category. In particular, categorical
quantum mechanics, or &quot;Quantum Picturalism&quot;, aims to turn concrete features of
quantum theory into abstract structural properties, expressed in the form of
diagrammatic identities. One way we search for these properties is to start
with a concrete model (e.g. a set of linear maps or finite relations) and start
composing generators into diagrams and looking for graphical identities.
  Naively, we could automate this procedure by enumerating all diagrams up to a
given size and check for equalities, but this is intractable in practice
because it produces far too many equations. Luckily, many of these identities
are not primitive, but rather derivable from simpler ones. In 2010, Johansson,
Dixon, and Bundy developed a technique called conjecture synthesis for
automatically generating conjectured term equations to feed into an inductive
theorem prover. In this extended abstract, we adapt this technique to
diagrammatic theories, expressed as graph rewrite systems, and demonstrate its
application by synthesising a graphical theory for studying entangled quantum
states.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6086</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6086</id><created>2012-02-27</created><updated>2013-06-29</updated><authors><author><keyname>Guruswami</keyname><forenames>Venkatesan</forenames></author><author><keyname>Narayanan</keyname><forenames>Srivatsan</forenames></author></authors><title>Combinatorial limitations of average-radius list-decoding</title><categories>cs.IT cs.CC math.CO math.IT</categories><comments>28 pages. Extended abstract in RANDOM 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study certain combinatorial aspects of list-decoding, motivated by the
exponential gap between the known upper bound (of $O(1/\gamma)$) and lower
bound (of $\Omega_p(\log (1/\gamma))$) for the list-size needed to decode up to
radius $p$ with rate $\gamma$ away from capacity, i.e., $1-\h(p)-\gamma$ (here
$p\in (0,1/2)$ and $\gamma &gt; 0$). Our main result is the following:
  We prove that in any binary code $C \subseteq \{0,1\}^n$ of rate
$1-\h(p)-\gamma$, there must exist a set $\mathcal{L} \subset C$ of
$\Omega_p(1/\sqrt{\gamma})$ codewords such that the average distance of the
points in $\mathcal{L}$ from their centroid is at most $pn$. In other words,
there must exist $\Omega_p(1/\sqrt{\gamma})$ codewords with low &quot;average
radius.&quot; The standard notion of list-decoding corresponds to working with the
maximum distance of a collection of codewords from a center instead of average
distance. The average-radius form is in itself quite natural and is implied by
the classical Johnson bound.
  The remaining results concern the standard notion of list-decoding, and help
clarify the combinatorial landscape of list-decoding:
  1. We give a short simple proof, over all fixed alphabets, of the
above-mentioned $\Omega_p(\log (\gamma))$ lower bound. Earlier, this bound
followed from a complicated, more general result of Blinovsky.
  2. We show that one {\em cannot} improve the $\Omega_p(\log (1/\gamma))$
lower bound via techniques based on identifying the zero-rate regime for list
decoding of constant-weight codes.
  3. We show a &quot;reverse connection&quot; showing that constant-weight codes for list
decoding imply general codes for list decoding with higher rate.
  4. We give simple second moment based proofs of tight (up to constant
factors) lower bounds on the list-size needed for list decoding random codes
and random linear codes from errors as well as erasures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6091</identifier>
 <datestamp>2013-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6091</id><created>2012-02-27</created><updated>2012-03-21</updated><authors><author><keyname>Ruan</keyname><forenames>Liangzhong</forenames></author><author><keyname>Lau</keyname><forenames>Vincent K. N.</forenames></author></authors><title>Interference Alignment for Partially Connected MIMO Cellular Networks</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Signal Processing, accepted</comments><doi>10.1109/TSP.2012.2192432</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose an iterative interference alignment (IA) algorithm
for MIMO cellular networks with partial connectivity, which is induced by
heterogeneous path losses and spatial correlation. Such systems impose several
key technical challenges in the IA algorithm design, namely the overlapping
between the direct and interfering links due to the MIMO cellular topology as
well as how to exploit the partial connectivity. We shall address these
challenges and propose a three stage IA algorithm. As illustration, we analyze
the achievable degree of freedom (DoF) of the proposed algorithm for a
symmetric partially connected MIMO cellular network. We show that there is
significant DoF gain compared with conventional IA algorithms due to partial
connectivity. The derived DoF bound is also backward compatible with that
achieved on fully connected K-pair MIMO interference channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6094</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6094</id><created>2012-02-27</created><updated>2012-03-16</updated><authors><author><keyname>Vaidya</keyname><forenames>Nitin</forenames></author><author><keyname>Tseng</keyname><forenames>Lewis</forenames></author><author><keyname>Liang</keyname><forenames>Guanfeng</forenames></author></authors><title>Iterative Approximate Byzantine Consensus in Arbitrary Directed Graphs -
  Part II: Synchronous and Asynchronous Systems</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This report contains two related sets of results with different assumptions
on synchrony. The first part is about iterative algorithms in synchronous
systems. Following our previous work on synchronous iterative approximate
Byzantine consensus (IABC) algorithms, we provide a more intuitive tight
necessary and sufficient condition for the existence of such algorithms in
synchronous networks1. We believe this condition and the previous results also
hold in partially asynchronous algorithmic model.
  In the second part of the report, we explore the problem in asynchronous
networks. While the traditional Byzantine consensus is not solvable in
asynchronous systems, approximate Byzantine consensus can be solved using
iterative algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6095</identifier>
 <datestamp>2015-06-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6095</id><created>2012-02-27</created><updated>2015-05-30</updated><authors><author><keyname>Jian</keyname><forenames>Yung-Yih</forenames></author><author><keyname>Pfister</keyname><forenames>Henry D.</forenames></author><author><keyname>Narayanan</keyname><forenames>Krishna R.</forenames></author></authors><title>Approaching Capacity at High-Rates with Iterative Hard-Decision Decoding</title><categories>cs.IT math.IT</categories><comments>37 pages, this is the journal version of previous paper and has been
  submitted to IT</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A variety of low-density parity-check (LDPC) ensembles have now been observed
to approach capacity with message-passing decoding. However, all of them use
soft (i.e., non-binary) messages and a posteriori probability (APP) decoding of
their component codes. In this paper, we show that one can approach capacity at
high rates using iterative hard-decision decoding (HDD) of generalized product
codes. Specifically, a class of spatially-coupled GLDPC codes with BCH
component codes is considered, and it is observed that, in the high-rate
regime, they can approach capacity under the proposed iterative HDD. These
codes can be seen as generalized product codes and are closely related to
braided block codes. An iterative HDD algorithm is proposed that enables one to
analyze the performance of these codes via density evolution (DE).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6101</identifier>
 <datestamp>2012-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6101</id><created>2012-02-27</created><authors><author><keyname>Ram</keyname><forenames>Parikshit</forenames></author><author><keyname>Gray</keyname><forenames>Alexander G.</forenames></author></authors><title>Maximum Inner-Product Search using Tree Data-structures</title><categories>cs.CG cs.DS cs.IR</categories><comments>Under submission in KDD 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of {\em efficiently} finding the best match for a query in a
given set with respect to the Euclidean distance or the cosine similarity has
been extensively studied in literature. However, a closely related problem of
efficiently finding the best match with respect to the inner product has never
been explored in the general setting to the best of our knowledge. In this
paper we consider this general problem and contrast it with the existing
best-match algorithms. First, we propose a general branch-and-bound algorithm
using a tree data structure. Subsequently, we present a dual-tree algorithm for
the case where there are multiple queries. Finally we present a new data
structure for increasing the efficiency of the dual-tree algorithm. These
branch-and-bound algorithms involve novel bounds suited for the purpose of
best-matching with inner products. We evaluate our proposed algorithms on a
variety of data sets from various applications, and exhibit up to five orders
of magnitude improvement in query time over the naive search technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6103</identifier>
 <datestamp>2012-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6103</id><created>2012-02-27</created><updated>2012-07-17</updated><authors><author><keyname>Giannakis</keyname><forenames>Dimitrios</forenames></author><author><keyname>Majda</keyname><forenames>Andrew J.</forenames></author></authors><title>Nonlinear Laplacian spectral analysis: Capturing intermittent and
  low-frequency spatiotemporal patterns in high-dimensional data</title><categories>physics.data-an cs.LG</categories><comments>39 pages, 8 figures, invited paper under review in Statistical
  Analysis and Data Mining</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a technique for spatiotemporal data analysis called nonlinear
Laplacian spectral analysis (NLSA), which generalizes singular spectrum
analysis (SSA) to take into account the nonlinear manifold structure of complex
data sets. The key principle underlying NLSA is that the functions used to
represent temporal patterns should exhibit a degree of smoothness on the
nonlinear data manifold M; a constraint absent from classical SSA. NLSA
enforces such a notion of smoothness by requiring that temporal patterns belong
in low-dimensional Hilbert spaces V_l spanned by the leading l Laplace-Beltrami
eigenfunctions on M. These eigenfunctions can be evaluated efficiently in high
ambient-space dimensions using sparse graph-theoretic algorithms. Moreover,
they provide orthonormal bases to expand a family of linear maps, whose
singular value decomposition leads to sets of spatiotemporal patterns at
progressively finer resolution on the data manifold. The Riemannian measure of
M and an adaptive graph kernel width enhances the capability of NLSA to detect
important nonlinear processes, including intermittency and rare events. The
minimum dimension of V_l required to capture these features while avoiding
overfitting is estimated here using spectral entropy criteria.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6104</identifier>
 <datestamp>2012-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6104</id><created>2012-02-27</created><authors><author><keyname>yahia</keyname><forenames>Nesrine Ben</forenames></author><author><keyname>Bellamine</keyname><forenames>Narj&#xe8;s</forenames></author><author><keyname>Gh&#xe9;zala</keyname><forenames>Henda Ben</forenames></author></authors><title>On the Convergence of Collaboration and Knowledge Management</title><categories>cs.HC</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Collaboration technology typically focuses on collaboration and group
processes (cooperation, communication, coordination and coproduction).
Knowledge Management (KM) technology typically focuses on content (creation,
storage, sharing and use of data, information and knowledge). Yet, to achieve
their common goals, teams and organizations need both KM and collaboration
technology to make that more effective and efficient. This paper is interested
in knowledge management and collaboration regarding their convergence and their
integration. First, it contributes to a better understanding of the knowledge
management and collaboration concepts. Second, it focuses on KM and
collaboration convergence by presenting the different interpretation of this
convergence. Third, this paper proposes a generic framework of collaborative
knowledge management.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6106</identifier>
 <datestamp>2012-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6106</id><created>2012-02-27</created><updated>2012-04-13</updated><authors><author><keyname>Kurihara</keyname><forenames>Kazutaka</forenames></author><author><keyname>Tsukada</keyname><forenames>Koji</forenames></author></authors><title>SpeechJammer: A System Utilizing Artificial Speech Disturbance with
  Delayed Auditory Feedback</title><categories>cs.HC</categories><acm-class>H.5.m</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we report on a system, &quot;SpeechJammer&quot;, which can be used to
disturb people's speech. In general, human speech is jammed by giving back to
the speakers their own utterances at a delay of a few hundred milliseconds.
This effect can disturb people without any physical discomfort, and disappears
immediately by stop speaking. Furthermore, this effect does not involve anyone
but the speaker. We utilize this phenomenon and implemented two prototype
versions by combining a direction-sensitive microphone and a
direction-sensitive speaker, enabling the speech of a specific person to be
disturbed. We discuss practical application scenarios of the system, such as
facilitating and controlling discussions. Finally, we argue what system
parameters should be examined in detail in future formal studies based on the
lessons learned from our preliminary study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6109</identifier>
 <datestamp>2012-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6109</id><created>2012-02-27</created><authors><author><keyname>Fraser</keyname><forenames>Maia</forenames></author></authors><title>Local Routing in Graphs Embedded on Surfaces of Arbitrary Genus</title><categories>cs.CG math.AT</categories><comments>18 pages, 3 figures</comments><msc-class>05C85, 05C10, 05C40, 05C38, 57M99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a local routing algorithm which guarantees delivery in all
connected graphs embedded on a known surface of genus $g$. The algorithm
transports $O(g\log n)$ memory and finishes in time $O(g^2n^2)$, where $n$ is
the size of the graph. It requires access to a homology basis for the surface.
This algorithm, GFR, may be viewed as a suitable generalization of Face Routing
(FR), the well-known algorithm for plane graphs, which we previously showed
does {\it not} guarantee delivery in graphs embedded on positive genus
surfaces. The problem for such surfaces is the potential presence of
homologically non-trivial closed walks which may be traversed by the right-hand
rule. We use an interesting mathematical property of homology bases (proven in
Lemma \ref{lem:connectFaceBdr}) to show that such walks will not impede GFR. FR
is at the base of most routing algorithms used in modern (2D) ad hoc networks:
these algorithms all involve additional local techniques to deal with
edge-crossings so FR may be applied. GFR should be viewed in the same light, as
a base algorithm which could for example be tailored to sensor networks on
surfaces in 3D. Currently there are no known efficient local, logarithmic
memory algorithms for 3D ad hoc networks. From a theoretical point of view our
work suggests that the efficiency advantages from which FR benefits are related
to the codimension one nature of an embedded graph in a surface rather than the
flatness of that surface (planarity).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6110</identifier>
 <datestamp>2012-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6110</id><created>2012-02-27</created><authors><author><keyname>Cassandras</keyname><forenames>Christos. G.</forenames></author><author><keyname>Lin</keyname><forenames>Xuchao</forenames></author><author><keyname>Ding</keyname><forenames>Xu Chu</forenames></author></authors><title>An Optimal Control Approach to the Persistent Monitoring Problem</title><categories>cs.SY math.OC</categories><comments>Technical Report accompanying CDC 2012 submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an optimal control framework for persistent monitoring problems
where the objective is to control the movement of mobile nodes to minimize an
uncertainty metric in a given mission space. For multi agent in a
one-dimensional mission space, we show that the optimal solution is obtained in
terms of a sequence of switching locations and waiting time on these switching
points, thus reducing it to a parametric optimization problem. Using
Infinitesimal Perturbation Analysis (IPA) we obtain a complete solution through
a gradient-based algorithm. We also discuss a receding horizon controller which
is capable of obtaining a near-optimal solution on-the-fly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6118</identifier>
 <datestamp>2012-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6118</id><created>2012-02-28</created><authors><author><keyname>Schieferdecker</keyname><forenames>Ina</forenames><affiliation>Fraunhofer FOKUS</affiliation></author><author><keyname>Grossmann</keyname><forenames>Juergen</forenames><affiliation>Fraunhofer FOKUS</affiliation></author><author><keyname>Schneider</keyname><forenames>Martin</forenames><affiliation>Fraunhofer FOKUS</affiliation></author></authors><title>Model-Based Security Testing</title><categories>cs.SE</categories><comments>In Proceedings MBT 2012, arXiv:1202.5826</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 80, 2012, pp. 1-12</journal-ref><doi>10.4204/EPTCS.80.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Security testing aims at validating software system requirements related to
security properties like confidentiality, integrity, authentication,
authorization, availability, and non-repudiation. Although security testing
techniques are available for many years, there has been little approaches that
allow for specification of test cases at a higher level of abstraction, for
enabling guidance on test identification and specification as well as for
automated test generation.
  Model-based security testing (MBST) is a relatively new field and especially
dedicated to the systematic and efficient specification and documentation of
security test objectives, security test cases and test suites, as well as to
their automated or semi-automated generation. In particular, the combination of
security modelling and test generation approaches is still a challenge in
research and of high interest for industrial applications. MBST includes e.g.
security functional testing, model-based fuzzing, risk- and threat-oriented
testing, and the usage of security test patterns. This paper provides a survey
on MBST techniques and the related models as well as samples of new methods and
tools that are under development in the European ITEA2-project DIAMONDS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6119</identifier>
 <datestamp>2012-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6119</id><created>2012-02-28</created><authors><author><keyname>Blech</keyname><forenames>Jan Olaf</forenames><affiliation>fortiss GmbH</affiliation></author><author><keyname>Mou</keyname><forenames>Dongyue</forenames><affiliation>fortiss GmbH</affiliation></author><author><keyname>Ratiu</keyname><forenames>Daniel</forenames><affiliation>fortiss GmbH</affiliation></author></authors><title>Reusing Test-Cases on Different Levels of Abstraction in a Model Based
  Development Tool</title><categories>cs.SE</categories><comments>In Proceedings MBT 2012, arXiv:1202.5826</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 80, 2012, pp. 13-27</journal-ref><doi>10.4204/EPTCS.80.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Seamless model based development aims to use models during all phases of the
development process of a system. During the development process in a
component-based approach, components of a system are described at qualitatively
differing abstraction levels: during requirements engineering component models
are rather abstract high-level and underspecified, while during implementation
the component models are rather concrete and fully specified in order to enable
code generation. An important issue that arises is assuring that the concrete
models correspond to abstract models. In this paper, we propose a method to
assure that concrete models for system components refine more abstract models
for the same components. In particular we advocate a framework for reusing
testcases at different abstraction levels. Our approach, even if it cannot
completely prove the refinement, can be used to ensure confidence in the
development process. In particular we are targeting the refinement of
requirements which are represented as very abstract models. Besides a formal
model of our approach, we discuss our experiences with the development of an
Adaptive Cruise Control (ACC) system in a model driven development process.
This uses extensions which we implemented for our model-based development tool
and which are briefly presented in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6120</identifier>
 <datestamp>2012-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6120</id><created>2012-02-28</created><authors><author><keyname>Cristi&#xe1;</keyname><forenames>Maximiliano</forenames><affiliation>CIFASIS and UNR</affiliation></author><author><keyname>Frydman</keyname><forenames>Claudia</forenames><affiliation>LSIS-CIFASIS</affiliation></author></authors><title>Applying SMT Solvers to the Test Template Framework</title><categories>cs.SE</categories><comments>In Proceedings MBT 2012, arXiv:1202.5826</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 80, 2012, pp. 28-42</journal-ref><doi>10.4204/EPTCS.80.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Test Template Framework (TTF) is a model-based testing method for the Z
notation. In the TTF, test cases are generated from test specifications, which
are predicates written in Z. In turn, the Z notation is based on first-order
logic with equality and Zermelo-Fraenkel set theory. In this way, a test case
is a witness satisfying a formula in that theory. Satisfiability Modulo Theory
(SMT) solvers are software tools that decide the satisfiability of arbitrary
formulas in a large number of built-in logical theories and their combination.
In this paper, we present the first results of applying two SMT solvers, Yices
and CVC3, as the engines to find test cases from TTF's test specifications. In
doing so, shallow embeddings of a significant portion of the Z notation into
the input languages of Yices and CVC3 are provided, given that they do not
directly support Zermelo-Fraenkel set theory as defined in Z. Finally, the
results of applying these embeddings to a number of test specifications of
eight cases studies are analysed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6121</identifier>
 <datestamp>2012-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6121</id><created>2012-02-28</created><authors><author><keyname>Richter</keyname><forenames>Dirk</forenames><affiliation>Martin-Luther-University of Halle-Wittenberg</affiliation></author><author><keyname>Berg</keyname><forenames>Christian</forenames><affiliation>Martin-Luther-University of Halle-Wittenberg</affiliation></author></authors><title>Exact Gap Computation for Code Coverage Metrics in ISO-C</title><categories>cs.SE</categories><comments>In Proceedings MBT 2012, arXiv:1202.5826</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 80, 2012, pp. 43-57</journal-ref><doi>10.4204/EPTCS.80.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Test generation and test data selection are difficult tasks for model based
testing. Tests for a program can be meld to a test suite. A lot of research is
done to quantify the quality and improve a test suite. Code coverage metrics
estimate the quality of a test suite. This quality is fine, if the code
coverage value is high or 100%. Unfortunately it might be impossible to achieve
100% code coverage because of dead code for example. There is a gap between the
feasible and theoretical maximal possible code coverage value. Our review of
the research indicates, none of current research is concerned with exact gap
computation. This paper presents a framework to compute such gaps exactly in an
ISO-C compatible semantic and similar languages. We describe an efficient
approximation of the gap in all the other cases. Thus, a tester can decide if
more tests might be able or necessary to achieve better coverage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6122</identifier>
 <datestamp>2012-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6122</id><created>2012-02-28</created><authors><author><keyname>Kanstr&#xe9;n</keyname><forenames>Teemu</forenames><affiliation>VTT</affiliation></author><author><keyname>Puolitaival</keyname><forenames>Olli-Pekka</forenames><affiliation>F-Secure</affiliation></author></authors><title>Using Built-In Domain-Specific Modeling Support to Guide Model-Based
  Test Generation</title><categories>cs.SE</categories><comments>In Proceedings MBT 2012, arXiv:1202.5826</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 80, 2012, pp. 58-72</journal-ref><doi>10.4204/EPTCS.80.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a model-based testing approach to support automated test
generation with domain-specific concepts. This includes a language expert who
is an expert at building test models and domain experts who are experts in the
domain of the system under test. First, we provide a framework to support the
language expert in building test models using a full (Java) programming
language with the help of simple but powerful modeling elements of the
framework. Second, based on the model built with this framework, the toolset
automatically forms a domain-specific modeling language that can be used to
further constrain and guide test generation from these models by a domain
expert. This makes it possible to generate a large set of test cases covering
the full model, chosen (constrained) parts of the model, or manually define
specific test cases on top of the model while using concepts familiar to the
domain experts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6123</identifier>
 <datestamp>2012-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6123</id><created>2012-02-28</created><authors><author><keyname>Aichernig</keyname><forenames>Bernhard K.</forenames><affiliation>Graz University of Technology</affiliation></author><author><keyname>J&#xf6;bstl</keyname><forenames>Elisabeth</forenames><affiliation>Graz University of Technology</affiliation></author></authors><title>Towards Symbolic Model-Based Mutation Testing: Combining Reachability
  and Refinement Checking</title><categories>cs.SE</categories><comments>In Proceedings MBT 2012, arXiv:1202.5826</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 80, 2012, pp. 88-102</journal-ref><doi>10.4204/EPTCS.80.7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Model-based mutation testing uses altered test models to derive test cases
that are able to reveal whether a modelled fault has been implemented. This
requires conformance checking between the original and the mutated model. This
paper presents an approach for symbolic conformance checking of action systems,
which are well-suited to specify reactive systems. We also consider
nondeterminism in our models. Hence, we do not check for equivalence, but for
refinement. We encode the transition relation as well as the conformance
relation as a constraint satisfaction problem and use a constraint solver in
our reachability and refinement checking algorithms. Explicit conformance
checking techniques often face state space explosion. First experimental
evaluations show that our approach has potential to outperform explicit
conformance checkers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6124</identifier>
 <datestamp>2012-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6124</id><created>2012-02-28</created><authors><author><keyname>Stokkink</keyname><forenames>Gerjan</forenames><affiliation>University of Twente</affiliation></author><author><keyname>Timmer</keyname><forenames>Mark</forenames><affiliation>University of Twente</affiliation></author><author><keyname>Stoelinga</keyname><forenames>Mari&#xeb;lle</forenames><affiliation>University of Twente</affiliation></author></authors><title>Talking quiescence: a rigorous theory that supports parallel
  composition, action hiding and determinisation</title><categories>cs.SE</categories><comments>In Proceedings MBT 2012, arXiv:1202.5826</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 80, 2012, pp. 73-87</journal-ref><doi>10.4204/EPTCS.80.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The notion of quiescence - the absence of outputs - is vital in both
behavioural modelling and testing theory. Although the need for quiescence was
already recognised in the 90s, it has only been treated as a second-class
citizen thus far. This paper moves quiescence into the foreground and
introduces the notion of quiescent transition systems (QTSs): an extension of
regular input-output transition systems (IOTSs) in which quiescence is
represented explicitly, via quiescent transitions. Four carefully crafted rules
on the use of quiescent transitions ensure that our QTSs naturally capture
quiescent behaviour.
  We present the building blocks for a comprehensive theory on QTSs supporting
parallel composition, action hiding and determinisation. In particular, we
prove that these operations preserve all the aforementioned rules.
Additionally, we provide a way to transform existing IOTSs into QTSs, allowing
even IOTSs as input that already contain some quiescent transitions. As an
important application, we show how our QTS framework simplifies the fundamental
model-based testing theory formalised around ioco.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6125</identifier>
 <datestamp>2012-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6125</id><created>2012-02-28</created><authors><author><keyname>Polivaev</keyname><forenames>Dimitry</forenames><affiliation>Giesecke and Devrient GmbH</affiliation></author></authors><title>Rule-based Test Generation with Mind Maps</title><categories>cs.SE</categories><comments>In Proceedings MBT 2012, arXiv:1202.5826</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 80, 2012, pp. 103-114</journal-ref><doi>10.4204/EPTCS.80.8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces basic concepts of rule based test generation with mind
maps, and reports experiences learned from industrial application of this
technique in the domain of smart card testing by Giesecke &amp; Devrient GmbH over
the last years. It describes the formalization of test selection criteria used
by our test generator, our test generation architecture and test generation
framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6126</identifier>
 <datestamp>2012-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6126</id><created>2012-02-28</created><authors><author><keyname>Ahman</keyname><forenames>Danel</forenames><affiliation>University of Cambridge</affiliation></author><author><keyname>K&#xe4;&#xe4;ramees</keyname><forenames>Marko</forenames><affiliation>Tallinn University of Technology</affiliation></author></authors><title>Constraint-Based Heuristic On-line Test Generation from
  Non-deterministic I/O EFSMs</title><categories>cs.SE</categories><comments>In Proceedings MBT 2012, arXiv:1202.5826</comments><proxy>EPTCS</proxy><acm-class>D.2.5</acm-class><journal-ref>EPTCS 80, 2012, pp. 115-129</journal-ref><doi>10.4204/EPTCS.80.9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We are investigating on-line model-based test generation from
non-deterministic output-observable Input/Output Extended Finite State Machine
(I/O EFSM) models of Systems Under Test (SUTs). We propose a novel
constraint-based heuristic approach (Heuristic Reactive Planning Tester (xRPT))
for on-line conformance testing non-deterministic SUTs. An indicative feature
of xRPT is the capability of making reasonable decisions for achieving the test
goals in the on-line testing process by using the results of off-line bounded
static reachability analysis based on the SUT model and test goal
specification. We present xRPT in detail and make performance comparison with
other existing search strategies and approaches on examples with varying
complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6127</identifier>
 <datestamp>2012-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6127</id><created>2012-02-28</created><authors><author><keyname>Gerlits</keyname><forenames>Yevgeny</forenames><affiliation>ISP RAS</affiliation></author><author><keyname>Khoroshilov</keyname><forenames>Alexey</forenames><affiliation>ISP RAS</affiliation></author></authors><title>Model-Based Testing of Safety Critical Real-Time Control Logic Software</title><categories>cs.SE</categories><comments>In Proceedings MBT 2012, arXiv:1202.5826</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 80, 2012, pp. 130-144</journal-ref><doi>10.4204/EPTCS.80.10</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper presents the experience of the authors in model based testing of
safety critical real-time control logic software. It describes specifics of the
corresponding industrial settings and discusses technical details of usage of
UniTESK model based testing technology in these settings. Finally, we discuss
possible future directions of safety critical software development processes
and a place of model based testing techniques in it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6129</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6129</id><created>2012-02-28</created><authors><author><keyname>Hou</keyname><forenames>Jianfeng</forenames></author></authors><title>Acyclic edge coloring of sparse graphs</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A proper edge coloring of a graph $G$ is called acyclic if there is no
bichromatic cycle in $G$. The acyclic chromatic index of $G$, denoted by
$\chi'_a(G)$, is the least number of colors $k$ such that $G$ has an acyclic
edge $k$-coloring. The maximum average degree of a graph $G$, denoted by
$\mad(G)$, is the maximum of the average degree of all subgraphs of $G$. In
this paper, it is proved that if $\mad(G)&lt;4$, then
$\chi'_a(G)\leq{\Delta(G)+2}$; if $\mad(G)&lt;3$, then
$\chi'_a(G)\leq{\Delta(G)+1}$. This implies that every triangle-free planar
graph $G$ is acyclically edge $(\Delta(G)+2)$-colorable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6134</identifier>
 <datestamp>2013-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6134</id><created>2012-02-28</created><updated>2013-01-14</updated><authors><author><keyname>Zhan</keyname><forenames>Jianfeng</forenames></author><author><keyname>Zhang</keyname><forenames>Lixin</forenames></author><author><keyname>Sun</keyname><forenames>Ninghui</forenames></author><author><keyname>Wang</keyname><forenames>Lei</forenames></author><author><keyname>Jia</keyname><forenames>Zhen</forenames></author><author><keyname>Luo</keyname><forenames>Chunjie</forenames></author></authors><title>High Volume Computing: Identifying and Characterizing Throughput
  Oriented Workloads in Data Centers</title><categories>cs.DC</categories><comments>10 pages</comments><journal-ref>Workshop on Large-Scale Parallel Processing in conjunction with
  26th IEEE International Parallel and Distributed Processing Symposium, 2012,
  Shanghai, China</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For the first time, this paper systematically identifies three categories of
throughput oriented workloads in data centers: services, data processing
applications, and interactive real-time applications, whose targets are to
increase the volume of throughput in terms of processed requests or data, or
supported maximum number of simultaneous subscribers, respectively, and we coin
a new term high volume computing (in short HVC) to describe those workloads and
data center computer systems designed for them. We characterize and compare HVC
with other computing paradigms, e.g., high throughput computing,
warehouse-scale computing, and cloud computing, in terms of levels, workloads,
metrics, coupling degree, data scales, and number of jobs or service instances.
We also preliminarily report our ongoing work on the metrics and benchmarks for
HVC systems, which is the foundation of designing innovative data center
computer systems for HVC workloads.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6136</identifier>
 <datestamp>2012-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6136</id><created>2012-02-28</created><authors><author><keyname>Hong</keyname><forenames>Dohy</forenames></author></authors><title>D-iteration: evaluation of the update algorithm</title><categories>cs.DM math.NA</categories><comments>5 pages</comments><acm-class>G.1.3; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this paper is to analyse the gain of the update algorithm
associated to the recently proposed D-iteration: the D-iteration is a fluid
diffusion based new iterative method. It exploits a simple intuitive
decomposition of the product matrix-vector as elementary operations of fluid
diffusion (forward scheme) associated to a new algebraic representation. We
show through experimentations on real datasets how much this approach can
improve the computation efficiency in presence of the graph evolution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6141</identifier>
 <datestamp>2013-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6141</id><created>2012-02-28</created><updated>2013-06-04</updated><authors><author><keyname>Wang</keyname><forenames>Zhiyong</forenames></author><author><keyname>Yin</keyname><forenames>Huarui</forenames></author><author><keyname>Zhang</keyname><forenames>Wenyi</forenames></author><author><keyname>Wei</keyname><forenames>Guo</forenames></author></authors><title>Monobit Digital Receivers for QPSK: Design, Analysis and Performance</title><categories>cs.IT math.IT</categories><comments>38 pages 10figures, accepted by TCOM</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Future communication system requires large bandwidth to achieve high data
rate up to multigigabit/ sec, which makes analog-to-digital (ADC) become a key
bottleneck for the implementation of digital receivers due to its high
complexity and large power consumption. Therefore, monobit receivers for BPSK
have been proposed to address this problem. In this work, QPSK modulation is
considered for higher data rate. First, the optimal receiver based on monobit
ADC with Nyquist sampling is derived, and its corresponding performance in the
form of deflection ratio is calculated. Then a suboptimal but more practical
monobit receiver is obtained, along with iterative demodulation and small
sample removal. The effect of the imbalances between the In-phase (I) and
Quadrature-phase (Q) branches, including the amplitude and phase imbalances, is
carefully investigated too. To combat the performance loss caused by IQ
imbalances, monobit receivers based on double training sequences are proposed.
Numerical simulations show that the low-complexity suboptimal receiver suffers
only 3dB signal to noise ratio (SNR) loss in AWGN channels and 1dB SNR loss in
multipath static channels compared with the matched filter based monobit
receiver with full channel state information (CSI). The impact of the phase
difference between the transmitter and receiver is presented. It is observed
that the performance degradation caused by the amplitude imbalance is
negligible. Receivers based on double training sequences can efficiently
compensate the performance loss in AWGN channel. Thanks to the diversity
offered by the multipath, the effect of imbalances on monobit receivers in
fading channels is slight. I
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6144</identifier>
 <datestamp>2012-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6144</id><created>2012-02-28</created><updated>2012-03-09</updated><authors><author><keyname>Pasqualetti</keyname><forenames>Fabio</forenames></author><author><keyname>D&#xf6;rfler</keyname><forenames>Florian</forenames></author><author><keyname>Bullo</keyname><forenames>Francesco</forenames></author></authors><title>Attack Detection and Identification in Cyber-Physical Systems -- Part I:
  Models and Fundamental Limitations</title><categories>math.OC cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cyber-physical systems integrate computation, communication, and physical
capabilities to interact with the physical world and humans. Besides failures
of components, cyber-physical systems are prone to malignant attacks, and
specific analysis tools as well as monitoring mechanisms need to be developed
to enforce system security and reliability. This paper proposes a unified
framework to analyze the resilience of cyber-physical systems against attacks
cast by an omniscient adversary. We model cyber-physical systems as linear
descriptor systems, and attacks as exogenous unknown inputs. Despite its
simplicity, our model captures various real-world cyber-physical systems, and
it includes and generalizes many prototypical attacks, including stealth,
(dynamic) false-data injection and replay attacks. First, we characterize
fundamental limitations of static, dynamic, and active monitors for attack
detection and identification. Second, we provide constructive algebraic
conditions to cast undetectable and unidentifiable attacks. Third, by using the
system interconnection structure, we describe graph-theoretic conditions for
the existence of undetectable and unidentifiable attacks. Finally, we validate
our findings through some illustrative examples with different cyber-physical
systems, such as a municipal water supply network and two electrical power
grids.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6148</identifier>
 <datestamp>2012-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6148</id><created>2012-02-28</created><authors><author><keyname>Baumgartner</keyname><forenames>Peter</forenames></author><author><keyname>Thorstensen</keyname><forenames>Evgenij</forenames></author></authors><title>Instance Based Methods --- A Brief Overview</title><categories>cs.LO</categories><comments>Final publication availible at http://www.springerlink.com</comments><journal-ref>KI - K\&quot;unstliche Intelligenz 24(1):35--42, 2010</journal-ref><doi>10.1007/s13218-010-0002-x</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Instance-based methods are a specific class of methods for automated proof
search in first-order logic. This article provides an overview of the major
methods in the area and discusses their properties and relations to the more
established resolution methods. It also discusses some recent trends on
refinements and applications.
  This overview is rather brief and informal, but we provide a comprehensive
literature list to follow-up on the details.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6153</identifier>
 <datestamp>2013-05-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6153</id><created>2012-02-28</created><authors><author><keyname>Hutter</keyname><forenames>Marcus</forenames></author></authors><title>One Decade of Universal Artificial Intelligence</title><categories>cs.AI</categories><comments>20 LaTeX pages</comments><journal-ref>In Theoretical Foundations of Artificial General Intelligence,
  Vol.4 (2012) pages 67--88</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The first decade of this century has seen the nascency of the first
mathematical theory of general artificial intelligence. This theory of
Universal Artificial Intelligence (UAI) has made significant contributions to
many theoretical, philosophical, and practical AI questions. In a series of
papers culminating in book (Hutter, 2005), an exciting sound and complete
mathematical model for a super intelligent agent (AIXI) has been developed and
rigorously analyzed. While nowadays most AI researchers avoid discussing
intelligence, the award-winning PhD thesis (Legg, 2008) provided the
philosophical embedding and investigated the UAI-based universal measure of
rational intelligence, which is formal, objective and non-anthropocentric.
Recently, effective approximations of AIXI have been derived and experimentally
investigated in JAIR paper (Veness et al. 2011). This practical breakthrough
has resulted in some impressive applications, finally muting earlier critique
that UAI is only a theory. For the first time, without providing any domain
knowledge, the same agent is able to self-adapt to a diverse range of
interactive environments. For instance, AIXI is able to learn from scratch to
play TicTacToe, Pacman, Kuhn Poker, and other games by trial and error, without
even providing the rules of the games.
  These achievements give new hope that the grand goal of Artificial General
Intelligence is not elusive.
  This article provides an informal overview of UAI in context. It attempts to
gently introduce a very theoretical, formal, and mathematical subject, and
discusses philosophical and technical ingredients, traits of intelligence, some
social questions, and the past and future of UAI.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6157</identifier>
 <datestamp>2012-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6157</id><created>2012-02-28</created><authors><author><keyname>Rose</keyname><forenames>Luca</forenames></author><author><keyname>Perlaza</keyname><forenames>Samir M.</forenames></author><author><keyname>Debbah</keyname><forenames>M&#xe9;rouane</forenames></author><author><keyname>Martret</keyname><forenames>Christophe J. Le</forenames></author></authors><title>Distributed Power Allocation with SINR Constraints Using Trial and Error
  Learning</title><categories>cs.GT cs.AI cs.LG</categories><comments>6 pages, 3 figures, accepted at WCNC 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we address the problem of global transmit power minimization
in a self-congiguring network where radio devices are subject to operate at a
minimum signal to interference plus noise ratio (SINR) level. We model the
network as a parallel Gaussian interference channel and we introduce a fully
decentralized algorithm (based on trial and error) able to statistically
achieve a congiguration where the performance demands are met. Contrary to
existing solutions, our algorithm requires only local information and can learn
stable and efficient working points by using only one bit feedback. We model
the network under two different game theoretical frameworks: normal form and
satisfaction form. We show that the converging points correspond to equilibrium
points, namely Nash and satisfaction equilibrium. Similarly, we provide
sufficient conditions for the algorithm to converge in both formulations.
Moreover, we provide analytical results to estimate the algorithm's
performance, as a function of the network parameters. Finally, numerical
results are provided to validate our theoretical conclusions. Keywords:
Learning, power control, trial and error, Nash equilibrium, spectrum sharing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6158</identifier>
 <datestamp>2012-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6158</id><created>2012-02-28</created><authors><author><keyname>Hong</keyname><forenames>Dohy</forenames></author></authors><title>Optimized on-line computation of PageRank algorithm</title><categories>cs.DM cs.IR math.NA</categories><comments>7 pages</comments><acm-class>G.2.2; F.2.2; H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present new ideas to accelerate the computation of the
eigenvector of the transition matrix associated to the PageRank algorithm. New
ideas are based on the decomposition of the matrix-vector product that can be
seen as a fluid diffusion model, associated to new algebraic equations. We show
through experiments on synthetic data and on real data-sets how much this
approach can improve the computation efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6163</identifier>
 <datestamp>2012-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6163</id><created>2012-02-28</created><authors><author><keyname>Murray</keyname><forenames>Lawrence</forenames></author></authors><title>GPU acceleration of the particle filter: the Metropolis resampler</title><categories>stat.CO cs.DC</categories><comments>Originally presented at Distributed Machine Learning and Sparse
  Representation with Massive Data Sets (DMMD 2011)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider deployment of the particle filter on modern massively parallel
hardware architectures, such as Graphics Processing Units (GPUs), with a focus
on the resampling stage. While standard multinomial and stratified resamplers
require a sum of importance weights computed collectively between threads, a
Metropolis resampler favourably requires only pair-wise ratios between weights,
computed independently by threads, and can be further tuned for performance by
adjusting its number of iterations. While achieving respectable results for the
stratified and multinomial resamplers, we demonstrate that a Metropolis
resampler can be faster where the variance in importance weights is modest, and
so is worth considering in a performance-critical context, such as particle
Markov chain Monte Carlo and real-time applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6164</identifier>
 <datestamp>2012-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6164</id><created>2012-02-28</created><authors><author><keyname>Zamosteanu</keyname><forenames>Alina Oana</forenames></author></authors><title>E-learning and use of computer in forensic field</title><categories>cs.CY</categories><comments>8 pages</comments><journal-ref>Ann. Univ. Tibiscus Comp. Sci. Series VIII / 1 (2010), 9-16</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the Romanian penitentiary establishments and those of other European
countries one talks about the formal and informal education. Since the
correspondence education system, important steps have been made towards the
e-learning didactics which are reflected by the modern teaching methods
(through ICT) used even by the penitentiary system. The Moodle platform, the
web site with certain specific, and the forum represent the means used as an
interface between the educator and the student, their benefits being clearly
demonstrated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6165</identifier>
 <datestamp>2012-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6165</id><created>2012-02-28</created><authors><author><keyname>Chiu</keyname><forenames>Eddy</forenames></author><author><keyname>Lau</keyname><forenames>Vincent K. N.</forenames></author><author><keyname>Zhang</keyname><forenames>Shunqing</forenames></author><author><keyname>Mok</keyname><forenames>Bao S. M.</forenames></author></authors><title>Precoder Design for Multi-antenna Partial Decode-and-Forward (PDF)
  Cooperative Systems with Statistical CSIT and MMSE-SIC Receivers</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cooperative communication is an important technology in next generation
wireless networks. Aside from conventional amplify-and-forward (AF) and
decode-and-forward (DF) protocols, the partial decode-and-forward (PDF)
protocol is an alternative relaying scheme that is especially promising for
scenarios in which the relay node cannot reliably decode the complete source
message. However, there are several important issues to be addressed regarding
the application of PDF protocols. In this paper, we propose a PDF protocol and
MIMO precoder designs at the source and relay nodes. The precoder designs are
adapted based on statistical channel state information for correlated MIMO
channels, and matched to practical minimum mean-square-error successive
interference cancelation (MMSE-SIC) receivers at the relay and destination
nodes. We show that under similar system settings, the proposed MIMO precoder
design with PDF protocol and MMSE-SIC receivers achieves substantial
performance enhancement compared with conventional baselines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6168</identifier>
 <datestamp>2012-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6168</id><created>2012-02-28</created><authors><author><keyname>Hong</keyname><forenames>Dohy</forenames></author></authors><title>D-iteration: Evaluation of the Asynchronous Distributed Computation</title><categories>math.NA cs.DC</categories><comments>8 pages</comments><acm-class>G.1.0; G.1.3; C.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this paper is to present a first evaluation of the potential of an
asynchronous distributed computation associated to the recently proposed
approach, D-iteration: the D-iteration is a fluid diffusion based iterative
method, which has the advantage of being natively distributive. It exploits a
simple intuitive decomposition of the matrix-vector product as elementary
operations of fluid diffusion associated to a new algebraic representation. We
show through experiments on real datasets how much this approach can improve
the computation efficiency when the parallelism is applied: with the proposed
solution, when the computation is distributed over $K$ virtual machines (PIDs),
the memory size to be handled by each virtual machine decreases linearly with
$K$ and the computation speed increases almost linearly with $K$ with a slope
becoming closer to one when the number $N$ of linear equations to be solved
increases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6174</identifier>
 <datestamp>2013-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6174</id><created>2012-02-28</created><updated>2013-05-13</updated><authors><author><keyname>Solovey</keyname><forenames>Kiril</forenames></author><author><keyname>Halperin</keyname><forenames>Dan</forenames></author></authors><title>k-Color Multi-Robot Motion Planning</title><categories>cs.RO</categories><comments>23</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a simple and natural extension of the multi-robot motion planning
problem where the robots are partitioned into groups (colors), such that in
each group the robots are interchangeable. Every robot is no longer required to
move to a specific target, but rather to some target placement that is assigned
to its group. We call this problem k-color multi-robot motion planning and
provide a sampling-based algorithm specifically designed for solving it. At the
heart of the algorithm is a novel technique where the k-color problem is
reduced to several discrete multi-robot motion planning problems. These
reductions amplify basic samples into massive collections of free placements
and paths for the robots. We demonstrate the performance of the algorithm by an
implementation for the case of disc robots and polygonal robots translating in
the plane. We show that the algorithm successfully and efficiently copes with a
variety of challenging scenarios, involving many robots, while a simplified
version of this algorithm, that can be viewed as an extension of a prevalent
sampling-based algorithm for the k-color case, fails even on simple scenarios.
Interestingly, our algorithm outperforms a well established implementation of
PRM for the standard multi-robot problem, in which each robot has a distinct
color.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6175</identifier>
 <datestamp>2012-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6175</id><created>2012-02-28</created><authors><author><keyname>Joda</keyname><forenames>Roghayeh</forenames></author><author><keyname>Lahouti</keyname><forenames>Farshad</forenames></author></authors><title>Delay-limited Source and Channel Coding of Quasi-Stationary Sources over
  Block Fading Channels: Design and Scaling Laws</title><categories>cs.IT math.IT</categories><comments>22 pages,5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, delay-limited transmission of quasi-stationary sources over
block fading channels are considered. Considering distortion outage probability
as the performance measure, two source and channel coding schemes with power
adaptive transmission are presented. The first one is optimized for fixed rate
transmission, and hence enjoys simplicity of implementation. The second one is
a high performance scheme, which also benefits from optimized rate adaptation
with respect to source and channel states. In high SNR regime, the performance
scaling laws in terms of outage distortion exponent and asymptotic outage
distortion gain are derived, where two schemes with fixed transmission power
and adaptive or optimized fixed rates are considered as benchmarks for
comparisons. Various analytical and numerical results are provided which
demonstrate a superior performance for source and channel optimized rate and
power adaptive scheme. It is also observed that from a distortion outage
perspective, the fixed rate adaptive power scheme substantially outperforms an
adaptive rate fixed power scheme for delay-limited transmission of
quasi-stationary sources over wireless block fading channels. The effect of the
characteristics of the quasi-stationary source on performance, and the
implication of the results for transmission of stationary sources are also
investigated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6177</identifier>
 <datestamp>2013-05-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6177</id><created>2012-02-28</created><authors><author><keyname>Hutter</keyname><forenames>Marcus</forenames></author></authors><title>Can Intelligence Explode?</title><categories>cs.AI physics.soc-ph</categories><comments>20 LaTeX pages</comments><journal-ref>Journal of Consciousness Studies, 19:1-2 (2012) 143-166</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The technological singularity refers to a hypothetical scenario in which
technological advances virtually explode. The most popular scenario is the
creation of super-intelligent algorithms that recursively create ever higher
intelligences. It took many decades for these ideas to spread from science
fiction to popular science magazines and finally to attract the attention of
serious philosophers. David Chalmers' (JCS 2010) article is the first
comprehensive philosophical analysis of the singularity in a respected
philosophy journal. The motivation of my article is to augment Chalmers' and to
discuss some issues not addressed by him, in particular what it could mean for
intelligence to explode. In this course, I will (have to) provide a more
careful treatment of what intelligence actually is, separate speed from
intelligence explosion, compare what super-intelligent participants and
classical human observers might experience and do, discuss immediate
implications for the diversity and value of life, consider possible bounds on
intelligence, and contemplate intelligences right at the singularity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6219</identifier>
 <datestamp>2013-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6219</id><created>2012-02-28</created><updated>2013-05-10</updated><authors><author><keyname>K&#xfc;hn</keyname><forenames>Daniela</forenames></author><author><keyname>Osthus</keyname><forenames>Deryk</forenames></author></authors><title>Hamilton decompositions of regular expanders: a proof of Kelly's
  conjecture for large tournaments</title><categories>math.CO cs.DM</categories><comments>new version includes a standalone version of the `robust
  decomposition lemma' for application in subsequent papers</comments><msc-class>05C45, 05C70, 05C85, 05C35, 05C38, 05C20</msc-class><journal-ref>Advances in Mathematics 237 (2013), 62-146</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A long-standing conjecture of Kelly states that every regular tournament on n
vertices can be decomposed into (n-1)/2 edge-disjoint Hamilton cycles. We prove
this conjecture for large n. In fact, we prove a far more general result, based
on our recent concept of robust expansion and a new method for decomposing
graphs. We show that every sufficiently large regular digraph G on n vertices
whose degree is linear in n and which is a robust outexpander has a
decomposition into edge-disjoint Hamilton cycles. This enables us to obtain
numerous further results, e.g. as a special case we confirm a conjecture of
Erdos on packing Hamilton cycles in random tournaments. As corollaries to the
main result, we also obtain several results on packing Hamilton cycles in
undirected graphs, giving e.g. the best known result on a conjecture of
Nash-Williams. We also apply our result to solve a problem on the domination
ratio of the Asymmetric Travelling Salesman problem, which was raised e.g. by
Glover and Punnen as well as Alon, Gutin and Krivelevich.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6221</identifier>
 <datestamp>2012-05-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6221</id><created>2012-02-28</created><updated>2012-05-24</updated><authors><author><keyname>Machart</keyname><forenames>Pierre</forenames><affiliation>LIF, LSIS</affiliation></author><author><keyname>Ralaivola</keyname><forenames>Liva</forenames><affiliation>LIF</affiliation></author></authors><title>Confusion Matrix Stability Bounds for Multiclass Classification</title><categories>cs.LG</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we provide new theoretical results on the generalization
properties of learning algorithms for multiclass classification problems. The
originality of our work is that we propose to use the confusion matrix of a
classifier as a measure of its quality; our contribution is in the line of work
which attempts to set up and study the statistical properties of new evaluation
measures such as, e.g. ROC curves. In the confusion-based learning framework we
propose, we claim that a targetted objective is to minimize the size of the
confusion matrix C, measured through its operator norm ||C||. We derive
generalization bounds on the (size of the) confusion matrix in an extended
framework of uniform stability, adapted to the case of matrix valued loss.
Pivotal to our study is a very recent matrix concentration inequality that
generalizes McDiarmid's inequality. As an illustration of the relevance of our
theoretical results, we show how two SVM learning procedures can be proved to
be confusion-friendly. To the best of our knowledge, the present paper is the
first that focuses on the confusion matrix from a theoretical point of view.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6228</identifier>
 <datestamp>2013-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6228</id><created>2012-02-28</created><updated>2013-10-22</updated><authors><author><keyname>Morvant</keyname><forenames>Emilie</forenames><affiliation>LIF</affiliation></author><author><keyname>Ko&#xe7;o</keyname><forenames>Sokol</forenames><affiliation>LIF</affiliation></author><author><keyname>Ralaivola</keyname><forenames>Liva</forenames><affiliation>LIF</affiliation></author></authors><title>PAC-Bayesian Generalization Bound on Confusion Matrix for Multi-Class
  Classification</title><categories>stat.ML cs.LG</categories><comments>Arxiv: http://arxiv.org/abs/1202.6228, Accepted at ICML 2012</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we propose a PAC-Bayes bound for the generalization risk of the
Gibbs classifier in the multi-class classification framework. The novelty of
our work is the critical use of the confusion matrix of a classifier as an
error measure; this puts our contribution in the line of work aiming at dealing
with performance measure that are richer than mere scalar criterion such as the
misclassification rate. Thanks to very recent and beautiful results on matrix
concentration inequalities, we derive two bounds showing that the true
confusion risk of the Gibbs classifier is upper-bounded by its empirical risk
plus a term depending on the number of training examples in each class. To the
best of our knowledge, this is the first PAC-Bayes bounds based on confusion
matrices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6235</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6235</id><created>2012-02-28</created><updated>2013-01-29</updated><authors><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author><author><keyname>Rotolo</keyname><forenames>Daniele</forenames></author><author><keyname>de Nooy</keyname><forenames>Wouter</forenames></author></authors><title>Innovation as a Nonlinear Process, the Scientometric Perspective, and
  the Specification of an &quot;Innovation Opportunities Explorer&quot;</title><categories>cs.DL physics.soc-ph</categories><comments>Technology Analysis and Strategic Management (forthcoming in 2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The process of innovation follows non-linear patterns across the domains of
science, technology, and the economy. Novel bibliometric mapping techniques can
be used to investigate and represent distinctive, but complementary
perspectives on the innovation process (e.g., &quot;demand&quot; and &quot;supply&quot;) as well as
the interactions among these perspectives. The perspectives can be represented
as &quot;continents&quot; of data related to varying extents over time. For example, the
different branches of Medical Subject Headings (MeSH) in the Medline database
provide sources of such perspectives (e.g., &quot;Diseases&quot; versus &quot;Drugs and
Chemicals&quot;). The multiple-perspective approach enables us to reconstruct facets
of the dynamics of innovation, in terms of selection mechanisms shaping
localizable trajectories and/or resulting in more globalized regimes. By
expanding the data with patents and scholarly publications, we demonstrate the
use of this multi-perspective approach in the case of RNA Interference (RNAi).
The possibility to develop an &quot;Innovation Opportunities Explorer&quot; is specified.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6256</identifier>
 <datestamp>2012-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6256</id><created>2012-02-28</created><authors><author><keyname>Gnanajeyaraman</keyname><forenames>R.</forenames></author><author><keyname>Seenivasan</keyname><forenames>G.</forenames></author></authors><title>Diagonalization Matrix Method of Solving the First Problem of Hidden
  Markov Model in Speech Recognition System</title><categories>cs.DS</categories><comments>10 pages</comments><journal-ref>Ann. Univ. Tibiscus Comp. Sci. Series VIII / 1 (2010), 17-26</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a computationally efficient method of solving evaluation
problem of Hidden Markov Model (HMM) with a given set of discrete observation
symbols, number of states and probability distribution matrices. The
observation probability for a given HMM model is evaluated using an approach in
which the probability evaluation is reduced to the problem of evaluating the
product of matrices with different powers and formed out of state transition
probabilities and observation probabilities. Finding powers of a matrix is done
by using the computationally efficient diagonalization method thereby reducing
the overall computational effort for evaluating the Evaluation problem of
HMM.The proposed method is compared with the existing direct method. It is
found that evaluating matrix power by diagnolisation method is more suitable
than that of the direct, method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6258</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6258</id><created>2012-02-28</created><updated>2013-03-11</updated><authors><author><keyname>Roux</keyname><forenames>Nicolas Le</forenames><affiliation>INRIA Paris - Rocquencourt, LIENS</affiliation></author><author><keyname>Schmidt</keyname><forenames>Mark</forenames><affiliation>INRIA Paris - Rocquencourt, LIENS</affiliation></author><author><keyname>Bach</keyname><forenames>Francis</forenames><affiliation>INRIA Paris - Rocquencourt, LIENS</affiliation></author></authors><title>A Stochastic Gradient Method with an Exponential Convergence Rate for
  Finite Training Sets</title><categories>math.OC cs.LG</categories><comments>The notable changes over the current version: - worked example of
  convergence rates showing SAG can be faster than first-order methods -
  pointing out that the storage cost is O(n) for linear models - the
  more-stable line-search - comparison to additional optimal SG methods -
  comparison to rates of coordinate descent methods in quadratic case</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new stochastic gradient method for optimizing the sum of a
finite set of smooth functions, where the sum is strongly convex. While
standard stochastic gradient methods converge at sublinear rates for this
problem, the proposed method incorporates a memory of previous gradient values
in order to achieve a linear convergence rate. In a machine learning context,
numerical experiments indicate that the new algorithm can dramatically
outperform standard algorithms, both in terms of optimizing the training error
and reducing the test error quickly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6260</identifier>
 <datestamp>2012-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6260</id><created>2012-02-28</created><updated>2012-12-01</updated><authors><author><keyname>Gutin</keyname><forenames>Gregory</forenames></author><author><keyname>Jones</keyname><forenames>Mark</forenames></author></authors><title>Note on Existence and Non-Existence of Large Subsets of Binary Vectors
  with Similar Distances</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider vectors from $\{0,1\}^n$. The weight of such a vector $v$ is the
sum of the coordinates of $v$. The distance ratio of a set $L$ of vectors is
${\rm dr}(L):=\max \{\rho(x,y):\ x,y \in L\}/ \min \{\rho(x,y):\ x,y \in L,\
x\neq y\},$ where $\rho(x,y)$ is the Hamming distance between $x$ and $y$. We
prove that (a) for every constant $\lambda&gt;1$ there are no positive constants
$\alpha$ and $C$ such that every set $K$ of at least $\lambda^p$ vectors with
weight $p$ contains a subset $K'$ with $|K'|\ge |K|^{\alpha}$ and ${\rm
dr}(K')\le C$, % even when $|K|\ge \lambda$,
  (b) For a set $K$ of vectors with weight $p$, and a constant $C&gt;2$, there
exists $K'\subseteq K$ such that ${\rm dr}(K')\le C$ and $|K'| \ge |K|^\alpha$,
where $\alpha = 1/ \lceil \log(p/2)/\log(C/2) \rceil$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6266</identifier>
 <datestamp>2012-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6266</id><created>2012-02-28</created><authors><author><keyname>Sadiqui</keyname><forenames>Ali</forenames></author><author><keyname>Chenfour</keyname><forenames>Noureddine</forenames></author></authors><title>Realisation d'un systeme de reconnaissance automatique de la parole
  arabe base sur CMU Sphinx</title><categories>cs.CL</categories><report-no>14 pages</report-no><journal-ref>Ann. Univ. Tibiscus Comp. Sci. Series VIII / 1 (2010), 27-40</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the continuation of the work completed by Satori and all.
[SCH07] by the realization of an automatic speech recognition system (ASR) for
Arabic language based SPHINX 4 system. The previous work was limited to the
recognition of the first ten digits, whereas the present work is a remarkable
projection consisting in continuous Arabic speech recognition with a rate of
recognition of surroundings 96%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6275</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6275</id><created>2012-02-28</created><updated>2012-04-07</updated><authors><author><keyname>Carayol</keyname><forenames>Arnaud</forenames></author><author><keyname>Esik</keyname><forenames>Zoltan</forenames></author></authors><title>The FC-rank of a context-free language</title><categories>cs.FL</categories><msc-class>68Q45</msc-class><acm-class>F.4.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that the finite condensation rank (FC-rank) of the lexicographic
ordering of a context-free language is strictly less than $\omega^\omega$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6278</identifier>
 <datestamp>2012-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6278</id><created>2012-02-28</created><authors><author><keyname>Gamal</keyname><forenames>Aly El</forenames></author><author><keyname>Annapureddy</keyname><forenames>V. Sreekanth</forenames></author><author><keyname>Veeravalli</keyname><forenames>Venugopal V.</forenames></author></authors><title>On Optimal Message Assignments for Interference Channels with CoMP
  Transmission</title><categories>cs.IT math.IT</categories><comments>In Proc. 46th Annual Conference on Information Sciences and Systems
  (CISS 2012), Princeton, NJ, Mar. 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The degrees of freedom (DoF) number of the fully connected K-user Gaussian
interference channel is known to be K/2. In [1], the DoF for the same channel
model was studied while allowing each message to be available at its own
transmitter as well as M-1 successive transmitters. In particular, it was shown
that the DoF gain through cooperation does not scale with the number of users K
for a fixed value of M, i.e., the per user DoF number is 1/2 . In this work, we
relax the cooperation constraint such that each message can be assigned to M
transmitters without imposing further constraints on their location. Under the
new constraint, we study properties for different message assignments in terms
of the gain in the per user DoF number over that achieved without cooperation.
In particular, we show that a local cooperation constraint that confines the
transmit set of each message within a o(K) radius cannot achieve a per user DoF
number that is greater than 1/2. Moreover, we show that the same conclusion
about the per user DoF number holds for any assignment of messages such that
each message cannot be available at more than two transmitters. Finally, for
the case where M &gt; 2, we do not know whether a per user DoF number that is
greater than 1/2 is achievable. However, we identify a candidate class of
message assignments that could potentially lead to a positive answer. [1] V. S.
Annapureddy, A. El Gamal, and V. V. Veervalli, &quot;Degrees of Freedom of
Interference Channels with CoMP Transmission and Reception,&quot; Submitted to IEEE
Trans. Inf. Theory, Sep. 2011
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6291</identifier>
 <datestamp>2012-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6291</id><created>2012-02-28</created><authors><author><keyname>Aroca</keyname><forenames>Jordi Arjona</forenames></author><author><keyname>Anta</keyname><forenames>Antonio Fern&#xe1;ndez</forenames></author></authors><title>Bisection (Band)Width of Product Networks with Application to Data
  Centers</title><categories>cs.NI cs.DC cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The bisection width of interconnection networks has always been important in
parallel computing, since it bounds the amount of information that can be moved
from one side of a network to another, i.e., the bisection bandwidth. Finding
its exact value has proven to be challenging for some network families. For
instance, the problem of finding the exact bisection width of the
multidimensional torus was posed by Leighton and has remained open for almost
20 years. In this paper we provide the exact value of the bisection width of
the torus, as well as of several d-dimensional classical parallel topologies
that can be obtained by the application of the Cartesian product of graphs. To
do so, we first provide two general results that allow to obtain upper and
lower bounds on the bisection width of a product graph as a function of some
properties of its factor graphs. We also apply these results to obtain bounds
for the bisection bandwidth of a d-dimensional BCube network, a recently
proposed topology for data centers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6299</identifier>
 <datestamp>2015-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6299</id><created>2012-02-28</created><authors><author><keyname>Goela</keyname><forenames>Naveen</forenames></author><author><keyname>Gastpar</keyname><forenames>Michael</forenames></author></authors><title>Reduced-Dimension Linear Transform Coding of Correlated Signals in
  Networks</title><categories>cs.IT math.IT</categories><comments>33 pages, 7 figures, To appear in IEEE Transactions on Signal
  Processing</comments><journal-ref>IEEE Transactions on Signal Processing, vol. 60, no. 6, pp.
  3174-3187, June 2012</journal-ref><doi>10.1109/TSP.2012.2188716</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A model, called the linear transform network (LTN), is proposed to analyze
the compression and estimation of correlated signals transmitted over directed
acyclic graphs (DAGs). An LTN is a DAG network with multiple source and
receiver nodes. Source nodes transmit subspace projections of random correlated
signals by applying reduced-dimension linear transforms. The subspace
projections are linearly processed by multiple relays and routed to intended
receivers. Each receiver applies a linear estimator to approximate a subset of
the sources with minimum mean squared error (MSE) distortion. The model is
extended to include noisy networks with power constraints on transmitters. A
key task is to compute all local compression matrices and linear estimators in
the network to minimize end-to-end distortion. The non-convex problem is solved
iteratively within an optimization framework using constrained quadratic
programs (QPs). The proposed algorithm recovers as special cases the regular
and distributed Karhunen-Loeve transforms (KLTs). Cut-set lower bounds on the
distortion region of multi-source, multi-receiver networks are given for linear
coding based on convex relaxations. Cut-set lower bounds are also given for any
coding strategy based on information theory. The distortion region and
compression-estimation tradeoffs are illustrated for different communication
demands (e.g. multiple unicast), and graph structures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6321</identifier>
 <datestamp>2012-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6321</id><created>2012-02-28</created><authors><author><keyname>Ullrich</keyname><forenames>Mario</forenames></author></authors><title>Rapid mixing of Swendsen-Wang and single-bond dynamics in two dimensions</title><categories>math.PR cs.DM math-ph math.MP</categories><comments>20 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that the spectral gap of the Swendsen-Wang dynamics for the
random-cluster model on arbitrary graphs with m edges is bounded above by 16 m
log m times the spectral gap of the single-bond (or heat-bath) dynamics. This
and the corresponding lower bound imply that rapid mixing of these two dynamics
is equivalent.
  Using the known lower bound on the spectral gap of the Swendsen-Wang dynamics
for the two dimensional square lattice $Z_L^2$ of side length L at high
temperatures and a result for the single-bond dynamics on dual graphs, we
obtain rapid mixing of both dynamics on $\Z_L^2$ at all non-critical
temperatures. In particular this implies, as far as we know, the first proof of
rapid mixing of a classical Markov chain for the Ising model on $\Z_L^2$ at all
temperatures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6344</identifier>
 <datestamp>2013-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6344</id><created>2012-02-28</created><updated>2013-07-01</updated><authors><author><keyname>D'Alfonso</keyname><forenames>Lisi</forenames></author><author><keyname>Jeronimo</keyname><forenames>Gabriela</forenames></author><author><keyname>Solern&#xf3;</keyname><forenames>Pablo</forenames></author></authors><title>Effective Differential L\&quot;uroth's Theorem</title><categories>math.AC cs.SC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper focuses on effectivity aspects of the L\&quot;uroth's theorem in
differential fields. Let $\mathcal{F}$ be an ordinary differential field of
characteristic 0 and $\mathcal{F}&lt;u&gt;$ be the field of differential rational
functions generated by a single indeterminate $u$. Let be given non constant
rational functions $v_1,...,v_n\in \mathcal{F}&lt;u&gt;$ generating a differential
subfield $\mathcal{G}\subseteq \mathcal{F}&lt;e u&gt;$. The differential L\&quot;uroth's
theorem proved by Ritt in 1932 states that there exists $v\in \mathcal G$ such
that $\mathcal{G}= \mathcal{F}&lt;v&gt;$. Here we prove that the total order and
degree of a generator $v$ are bounded by $\min_j \textrm{ord} (v_j)$ and
$(nd(e+1)+1)^{2e+1}$, respectively, where $e:=\max_j \textrm{ord} (v_j)$ and
$d:=\max_j \textrm{deg} (v_j)$. As a byproduct, our techniques enable us to
compute a L\&quot;uroth generator by dealing with a polynomial ideal in a polynomial
ring in finitely many variables.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6345</identifier>
 <datestamp>2012-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6345</id><created>2012-02-28</created><authors><author><keyname>Gallos</keyname><forenames>Lazaros K.</forenames></author><author><keyname>Barttfeld</keyname><forenames>Pablo</forenames></author><author><keyname>Havlin</keyname><forenames>Shlomo</forenames></author><author><keyname>Sigman</keyname><forenames>Mariano</forenames></author><author><keyname>Makse</keyname><forenames>Hern&#xe1;n A.</forenames></author></authors><title>Collective behavior and critical fluctuations in the spatial spreading
  of obesity, diabetes and cancer</title><categories>physics.soc-ph cs.SI</categories><comments>9 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Non-communicable diseases like diabetes, obesity and certain forms of cancer
have been increasing in many countries at alarming levels. A difficulty in the
conception of policies to reverse these trends is the identification of the
drivers behind the global epidemics. Here, we implement a spatial spreading
analysis to investigate whether diabetes, obesity and cancer show spatial
correlations revealing the effect of collective and global factors acting above
individual choices. We adapt a theoretical framework for critical physical
systems displaying collective behavior to decipher the laws of spatial
spreading of diseases. We find a regularity in the spatial fluctuations of
their prevalence revealed by a pattern of scale-free long-range correlations.
The fluctuations are anomalous, deviating in a fundamental way from the weaker
correlations found in the underlying population distribution. This collective
behavior indicates that the spreading dynamics of obesity, diabetes and some
forms of cancer like lung cancer are analogous to a critical point of
fluctuations, just as a physical system in a second-order phase transition.
According to this notion, individual interactions and habits may have
negligible influence in shaping the global patterns of spreading. Thus, obesity
turns out to be a global problem where local details are of little importance.
Interestingly, we find the same critical fluctuations in obesity and diabetes,
and in the activities of economic sectors associated with food production such
as supermarkets, food and beverage stores--- which cluster in a different
universality class than other generic sectors of the economy. These results
motivate future interventions to investigate the causality of this relation
providing guidance for the implementation of preventive health policies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6348</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6348</id><created>2012-02-28</created><updated>2013-12-17</updated><authors><author><keyname>Moustakas</keyname><forenames>Aris L.</forenames></author><author><keyname>Mertikopoulos</keyname><forenames>Panayotis</forenames></author><author><keyname>Bambos</keyname><forenames>Nicholas</forenames></author></authors><title>Power Optimization in Random Wireless Networks</title><categories>cs.IT cond-mat.stat-mech cs.SI math.IT</categories><comments>Submitted to IEEE Trans. Inform. Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a wireless network of transmitter-receiver pairs where the
transmitters adjust their powers to maintain a target SINR level in the
presence of interference. In this paper, we analyze the optimal power vector
that achieves this target in large, random networks obtained by &quot;erasing&quot; a
finite fraction of nodes from a regular lattice of transmitter-receiver pairs.
We show that this problem is equivalent to the so-called Anderson model of
electron motion in dirty metals which has been used extensively in the analysis
of diffusion in random environments. A standard approximation to this model is
the so-called coherent potential approximation (CPA) method which we apply to
evaluate the first and second order intra-sample statistics of the optimal
power vector in one- and two-dimensional systems. This approach is equivalent
to traditional techniques from random matrix theory and free probability, but
while generally accurate (and in agreement with numerical simulations), it
fails to fully describe the system: in particular, results obtained in this way
fail to predict when power control becomes infeasible. In this regard, we find
that the infinite system is always unstable beyond a certain value of the
target SINR, but any finite system only has a small probability of becoming
unstable. This instability probability is proportional to the tails of the
eigenvalue distribution of the system which are calculated to exponential
accuracy using methodologies developed within the Anderson model and its ties
with random walks in random media. Finally, using these techniques, we also
calculate the tails of the system's power distribution under power control and
the rate of convergence of the Foschini-Miljanic power control algorithm in the
presence of random erasures. Overall, in the paper we try to strike a balance
between intuitive arguments and formal proofs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6350</identifier>
 <datestamp>2012-07-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6350</id><created>2012-02-28</created><updated>2012-07-29</updated><authors><author><keyname>Lemvig</keyname><forenames>Jakob</forenames></author><author><keyname>Miller</keyname><forenames>Christopher</forenames></author><author><keyname>Okoudjou</keyname><forenames>Kasso A.</forenames></author></authors><title>Prime tight frames</title><categories>math.FA cs.IT math.IT</categories><msc-class>42C15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a class of finite tight frames called prime tight frames and
prove some of their elementary properties. In particular, we show that any
finite tight frame can be written as a union of prime tight frames. We then
characterize all prime harmonic tight frames and use this characterization to
suggest effective analysis and synthesis computation strategies for such
frames. Finally, we describe all prime frames constructed from the spectral
tetris method, and, as a byproduct, we obtain a characterization of when the
spectral tetris construction works for redundancies below two.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6352</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6352</id><created>2012-02-28</created><updated>2012-03-05</updated><authors><author><keyname>Baaz</keyname><forenames>Matthias</forenames><affiliation>Department of Discrete Mathematics and Geometry, TU Vienna</affiliation></author><author><keyname>Ciabattoni</keyname><forenames>Agata</forenames><affiliation>Department of Computer Languages, TU Vienna</affiliation></author><author><keyname>Ferm&#xfc;ller</keyname><forenames>Christian G</forenames><affiliation>Department of Computer Languages, TU Vienna</affiliation></author></authors><title>Theorem proving for prenex G\&quot;odel logic with Delta: checking validity
  and unsatisfiability</title><categories>cs.LO</categories><comments>23 pages, accepted for LMCS (Logical Methods in Computer Science)</comments><proxy>LMCS</proxy><acm-class>F.4.1</acm-class><journal-ref>Logical Methods in Computer Science, Volume 8, Issue 1 (March 6,
  2012) lmcs:833</journal-ref><doi>10.2168/LMCS-8(1:20)2012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  G\&quot;odel logic with the projection operator Delta (G_Delta) is an important
many-valued as well as intermediate logic. In contrast to classical logic, the
validity and the satisfiability problems of G_Delta are not directly dual to
each other. We nevertheless provide a uniform, computational treatment of both
problems for prenex formulas by describing appropriate translations into sets
of order clauses that can be subjected to chaining resolution. For validity a
version of Herbrand's Theorem allows us to show the soundness of standard
Skolemization. For satisfiability the translation involves a novel, extended
Skolemization method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6354</identifier>
 <datestamp>2012-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6354</id><created>2012-02-28</created><authors><author><keyname>Haslhofer</keyname><forenames>Bernhard</forenames></author><author><keyname>Sanderson</keyname><forenames>Robert</forenames></author><author><keyname>Simon</keyname><forenames>Rainer</forenames></author><author><keyname>van de Sompel</keyname><forenames>Herbert</forenames></author></authors><title>Open Annotations on Multimedia Web Resources</title><categories>cs.DL</categories><comments>20 pages</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Many Web portals allow users to associate additional information with
existing multimedia resources such as images, audio, and video. However, these
portals are usually closed systems and user-generated annotations are almost
always kept locked up and remain inaccessible to the Web of Data. We believe
that an important step to take is the integration of multimedia annotations and
the Linked Data principles. We present the current state of the Open Annotation
Model, explain our design rationale, and describe how the model can represent
user annotations on multimedia Web resources. Applying this model in Web
portals and devices, which support user annotations, should allow clients to
easily publish and consume, thus exchange annotations on multimedia Web
resources via common Web standards.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6384</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6384</id><created>2012-02-28</created><authors><author><keyname>Szlam</keyname><forenames>Arthur</forenames></author><author><keyname>Gregor</keyname><forenames>Karol</forenames></author><author><keyname>LeCun</keyname><forenames>Yann</forenames></author></authors><title>Fast approximations to structured sparse coding and applications to
  object classification</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a method for fast approximation of sparse coding. The input space
is subdivided by a binary decision tree, and we simultaneously learn a
dictionary and assignment of allowed dictionary elements for each leaf of the
tree. We store a lookup table with the assignments and the pseudoinverses for
each node, allowing for very fast inference. We give an algorithm for learning
the tree, the dictionary and the dictionary element assignment, and In the
process of describing this algorithm, we discuss the more general problem of
learning the groups in group structured sparse modelling. We show that our
method creates good sparse representations by using it in the object
recognition framework of \cite{lazebnik06,yang-cvpr-09}. Implementing our own
fast version of the SIFT descriptor the whole system runs at 20 frames per
second on $321 \times 481$ sized images on a laptop with a quad-core cpu, while
sacrificing very little accuracy on the Caltech 101 and 15 scenes benchmarks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6386</identifier>
 <datestamp>2012-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6386</id><created>2012-02-28</created><authors><author><keyname>Mohan</keyname><forenames>Shiwali</forenames></author><author><keyname>Laird</keyname><forenames>John E.</forenames></author></authors><title>Relational Reinforcement Learning in Infinite Mario</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Relational representations in reinforcement learning allow for the use of
structural information like the presence of objects and relationships between
them in the description of value functions. Through this paper, we show that
such representations allow for the inclusion of background knowledge that
qualitatively describes a state and can be used to design agents that
demonstrate learning behavior in domains with large state and actions spaces
such as computer games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6388</identifier>
 <datestamp>2012-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6388</id><created>2012-02-28</created><authors><author><keyname>Fontanari</keyname><forenames>Jose F.</forenames></author><author><keyname>Bonniot-Cabanac</keyname><forenames>Marie-Claude</forenames></author><author><keyname>Cabanac</keyname><forenames>Michel</forenames></author><author><keyname>Perlovsky</keyname><forenames>Leonid I.</forenames></author></authors><title>A structural model of emotions of cognitive dissonances</title><categories>q-bio.NC cs.HC physics.soc-ph</categories><journal-ref>Neural Networks 32 (2012) 57-64</journal-ref><doi>10.1016/j.neunet.2012.04.007</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cognitive dissonance is the stress that comes from holding two conflicting
thoughts simultaneously in the mind, usually arising when people are asked to
choose between two detrimental or two beneficial options. In view of the
well-established role of emotions in decision making, here we investigate
whether the conventional structural models used to represent the relationships
among basic emotions, such as the Circumplex model of affect, can describe the
emotions of cognitive dissonance as well. We presented a questionnaire to 34
anonymous participants, where each question described a decision to be made
among two conflicting motivations and asked the participants to rate
analogically the pleasantness and the intensity of the experienced emotion. We
found that the results were compatible with the predictions of the Circumplex
model for basic emotions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6389</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6389</id><created>2012-02-28</created><authors><author><keyname>Bajovic</keyname><forenames>Dragana</forenames></author><author><keyname>Xavier</keyname><forenames>Joao</forenames></author><author><keyname>Moura</keyname><forenames>Jose M. F.</forenames></author><author><keyname>Sinopoli</keyname><forenames>Bruno</forenames></author></authors><title>Consensus and Products of Random Stochastic Matrices: Exact Rate for
  Convergence in Probability</title><categories>math.PR cs.IT cs.SI math.IT</categories><doi>10.1109/TSP.2013.2248003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed consensus and other linear systems with system stochastic
matrices $W_k$ emerge in various settings, like opinion formation in social
networks, rendezvous of robots, and distributed inference in sensor networks.
The matrices $W_k$ are often random, due to, e.g., random packet dropouts in
wireless sensor networks. Key in analyzing the performance of such systems is
studying convergence of matrix products $W_kW_{k-1}... W_1$. In this paper, we
find the exact exponential rate $I$ for the convergence in probability of the
product of such matrices when time $k$ grows large, under the assumption that
the $W_k$'s are symmetric and independent identically distributed in time.
Further, for commonly used random models like with gossip and link failure, we
show that the rate $I$ is found by solving a min-cut problem and, hence, easily
computable. Finally, we apply our results to optimally allocate the sensors'
transmission power in consensus+innovations distributed detection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6395</identifier>
 <datestamp>2012-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6395</id><created>2012-02-28</created><authors><author><keyname>Fenner</keyname><forenames>Stephen A.</forenames></author></authors><title>Functions that preserve p-randomness</title><categories>cs.CC</categories><comments>24 pages, 2 figures. An extended abstract of this paper appeared in
  Proceedings of the 18th International Symposium on Fundamentals of
  Computation Theory (FCT), volume 6914 of Lecture Notes in Computer Science,
  Springer-Verlag, pages 336-347, 2011</comments><acm-class>F.2.1; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that polynomial-time randomness (p-randomness) is preserved under a
variety of familiar operations, including addition and multiplication by a
nonzero polynomial-time computable real number. These results follow from a
general theorem: If $I$ is an open interval in the reals, $f$ is a function
mapping $I$ into the reals, and $r$ in $I$ is p-random, then $f(r)$ is p-random
provided
  1. $f$ is p-computable on the dyadic rational points in $I$, and
  2. $f$ varies sufficiently at $r$, i.e., there exists a real constant $C &gt; 0$
such that either (a) $(f(x) - f(r))/(x-r) &gt; C$ for all $x$ in $I$ with $x \ne
r$, or (b) $(f(x) - f(r))(x-r) &lt; -C$ for all $x$ in $I$ with $x \ne r$.
  Our theorem implies in particular that any analytic function about a
p-computable point whose power series has uniformly p-computable coefficients
preserves p-randomness in its open interval of absolute convergence. Such
functions include all the familiar functions from first-year calculus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6404</identifier>
 <datestamp>2014-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6404</id><created>2012-02-28</created><updated>2012-09-13</updated><authors><author><keyname>Agrell</keyname><forenames>Erik</forenames></author><author><keyname>Alvarado</keyname><forenames>Alex</forenames></author></authors><title>Signal Shaping for BICM at Low SNR</title><categories>cs.IT math.IT</categories><journal-ref>IEEE Trans. Inf. Theory, vol. 59, no. 4, pp. 2396-2410, Apr. 2013</journal-ref><doi>10.1109/TIT.2012.2231900</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The mutual information of bit-interleaved coded modulation (BICM) systems,
sometimes called the BICM capacity, is investigated at low signal-to-noise
ratio (SNR), i.e., in the wideband regime. A new linear transform that depends
on bits' probabilities is introduced. This transform is used to prove the
asymptotical equivalence between certain BICM systems with uniform and
nonuniform input distributions. Using known results for BICM systems with a
uniform input distribution, we completely characterize the combinations of
input alphabet, input distribution, and binary labeling that achieve the
Shannon limit -1.59 dB. The main conclusion is that a BICM system achieves the
Shannon limit at low SNR if and only if it can be represented as a zero-mean
linear projection of a hypercube, which is the same condition as for uniform
input distributions. Hence, probabilistic shaping offers no extra degrees of
freedom to optimize the low-SNR mutual information of BICM systems, in addition
to what is provided by geometrical shaping. These analytical conclusions are
confirmed by numerical results, which also show that for a fixed input
alphabet, probabilistic shaping of BICM can improve the mutual information in
the low and medium SNR range over any coded modulation system with a uniform
input distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6409</identifier>
 <datestamp>2014-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6409</id><created>2012-02-28</created><authors><author><keyname>Pinheiro</keyname><forenames>Jerry Anderson</forenames></author><author><keyname>Firer</keyname><forenames>Marcelo</forenames></author></authors><title>Classification of poset-block spaces admitting MacWilliams-type identity</title><categories>cs.IT math.IT</categories><comments>8 pages, 1 figure. Submitted to IEEE Transactions on Information
  Theory</comments><doi>10.1109/TIT.2012.2210192</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this work we prove that a poset-block space admits a MacWilliams-type
identity if and only if the poset is hierarchical and at any level of the
poset, all the blocks have the same dimension. When the poset-block admits the
MacWilliams-type identity we explicit the relation between the weight
enumerators of a code and its dual.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6423</identifier>
 <datestamp>2013-10-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6423</id><created>2012-02-28</created><updated>2013-03-20</updated><authors><author><keyname>Bash</keyname><forenames>Boulat A.</forenames></author><author><keyname>Goeckel</keyname><forenames>Dennis</forenames></author><author><keyname>Towsley</keyname><forenames>Don</forenames></author></authors><title>Limits of Reliable Communication with Low Probability of Detection on
  AWGN Channels</title><categories>cs.IT cs.NI math.IT</categories><comments>Major revision in v2. Context, esp. the relationship to steganography
  updated. Also, added discussion on secret key length. Results are unchanged
  from previous version. Minor revision in v3. Major revision in v4, Clarified
  derivations (adding appendix), also context, esp. relationship to previous
  work in communication updated. Results are unchanged from previous revisions</comments><report-no>UM-CS-2012-003</report-no><journal-ref>IEEE Journal on Selected Areas in Communications 31.9 (2013)
  1921-1930</journal-ref><doi>10.1109/JSAC.2013.130923</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a square root limit on the amount of information transmitted
reliably and with low probability of detection (LPD) over additive white
Gaussian noise (AWGN) channels. Specifically, if the transmitter has AWGN
channels to an intended receiver and a warden, both with non-zero noise power,
we prove that $o(\sqrt{n})$ bits can be sent from the transmitter to the
receiver in $n$ channel uses while lower-bounding $\alpha+\beta\geq1-\epsilon$
for any $\epsilon&gt;0$, where $\alpha$ and $\beta$ respectively denote the
warden's probabilities of a false alarm when the sender is not transmitting and
a missed detection when the sender is transmitting. Moreover, in most practical
scenarios, a lower bound on the noise power on the channel between the
transmitter and the warden is known and $O(\sqrt{n})$ bits can be sent in $n$
LPD channel uses. Conversely, attempting to transmit more than $O(\sqrt{n})$
bits either results in detection by the warden with probability one or a
non-zero probability of decoding error at the receiver as $n\rightarrow\infty$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6429</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6429</id><created>2012-02-28</created><updated>2013-03-12</updated><authors><author><keyname>Needell</keyname><forenames>Deanna</forenames></author><author><keyname>Ward</keyname><forenames>Rachel</forenames></author></authors><title>Stable image reconstruction using total variation minimization</title><categories>cs.CV cs.IT math.IT math.NA</categories><comments>25 pages</comments><msc-class>41A46, 68Q25, 68W20, 90C27</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article presents near-optimal guarantees for accurate and robust image
recovery from under-sampled noisy measurements using total variation
minimization. In particular, we show that from O(slog(N)) nonadaptive linear
measurements, an image can be reconstructed to within the best s-term
approximation of its gradient up to a logarithmic factor, and this factor can
be removed by taking slightly more measurements. Along the way, we prove a
strengthened Sobolev inequality for functions lying in the null space of
suitably incoherent matrices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6436</identifier>
 <datestamp>2012-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6436</id><created>2012-02-28</created><authors><author><keyname>Rehman</keyname><forenames>Obaid Ur</forenames></author><author><keyname>Petersen</keyname><forenames>Ian R.</forenames></author><author><keyname>Fidan</keyname><forenames>Bar&#x131;s</forenames></author></authors><title>A Mean Value Theorem Approach to Robust Control Design for Uncertain
  Nonlinear Systems</title><categories>cs.SY</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a scheme to design a tracking controller for a class of
uncertain nonlinear systems using a robust feedback linearization approach. The
scheme is composed of two steps. In the first step, a linearized uncertainty
model for the corresponding uncertain nonlinear system is developed using a
robust feedback linearization approach. In this step, the standard feedback
linearization approach is used to linearize the nominal nonlinear dynamics of
the uncertain nonlinear system. The remaining nonlinear uncertainties are then
linearized at an arbitrary point using the mean value theorem. This approach
gives a multi-input multi-output (MIMO) linear uncertain system model with a
structured uncertainty representation. In the second step, a minimax linear
quadratic regulation (LQR) controller is designed for MIMO linearized uncertain
system model. In order to demonstrate the effectiveness of the proposed method,
it is applied to a velocity and altitude tracking control problem for an
air-breathing hypersonic flight vehicle.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6438</identifier>
 <datestamp>2012-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6438</id><created>2012-02-28</created><authors><author><keyname>Kino</keyname><forenames>Fumika</forenames></author><author><keyname>Uno</keyname><forenames>Yushi</forenames></author></authors><title>Solving Tantrix via Integer Programming</title><categories>cs.DM</categories><comments>14 pages + 1 page appendix, 15 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tantrix is a puzzle to make a loop by connecting lines drawn on hexagonal
tiles, and the objective of this research is to solve it by a computer. For
this purpose, we give a problem setting of solving Tantrix as arranging tiles
in an appropriate shape and making a loop at the same time within a given
hexagonal lattice board. We then formulate it as an integer program by
expressing the rules of Tantrix as its constraints, and solve it by a
mathematical programming solver to have a solution. As a result, we establish a
formulation that solves Tantrix of moderate sizes, and even when the solutions
are invalid only by elementary constraints, we achieved it by introducing
additional constraints and an artificial objective function to avoid flaws in
invalid solutions. By this approach we are successful in solving Tantrix of
size up to 50.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6444</identifier>
 <datestamp>2013-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6444</id><created>2012-02-29</created><updated>2012-10-14</updated><authors><author><keyname>Villagra</keyname><forenames>Marcos</forenames></author><author><keyname>Nakanishi</keyname><forenames>Masaki</forenames></author><author><keyname>Yamashita</keyname><forenames>Shigeru</forenames></author><author><keyname>Nakashima</keyname><forenames>Yasuhiko</forenames></author></authors><title>Tensor Rank and Strong Quantum Nondeterminism in Multiparty
  Communication</title><categories>cs.CC quant-ph</categories><comments>In v3 corrected some lesser typos. Extended abstract in Proc. of
  TAMC'12, LNCS 7287, pp. 400-411, 2012</comments><msc-class>68Q05, 68Q12, 94A05</msc-class><acm-class>F.1.2; G.0</acm-class><journal-ref>IEICE Transactions on Information and Systems Vol. E96.D (2013)
  No. 1 pp. 1-8</journal-ref><doi>10.1587/transinf.E96.D.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study quantum nondeterminism in multiparty communication.
There are three (possibly) different types of nondeterminism in quantum
computation: i) strong, ii) weak with classical proofs, and iii) weak with
quantum proofs. Here we focus on the first one. A strong quantum
nondeterministic protocol accepts a correct input with positive probability,
and rejects an incorrect input with probability 1. In this work we relate
strong quantum nondeterministic multiparty communication complexity to the rank
of the communication tensor in the Number-On-Forehead and Number-In-Hand
models. In particular, by extending the definition proposed by de Wolf to {\it
nondeterministic tensor-rank} ($nrank$), we show that for any boolean function
$f$ when there is no prior shared entanglement between the players, 1) in the
Number-On-Forehead model, the cost is upper-bounded by the logarithm of
$nrank(f)$; 2) in the Number-In-Hand model, the cost is lower-bounded by the
logarithm of $nrank(f)$. Furthermore, we show that when the number of players
is $o(\log\log n)$ we have that $NQP\nsubseteq BQP$ for Number-On-Forehead
communication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6445</identifier>
 <datestamp>2012-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6445</id><created>2012-02-29</created><authors><author><keyname>Ganesh</keyname><forenames>Arvind</forenames></author><author><keyname>Min</keyname><forenames>Kerui</forenames></author><author><keyname>Wright</keyname><forenames>John</forenames></author><author><keyname>Ma</keyname><forenames>Yi</forenames></author></authors><title>Principal Component Pursuit with Reduced Linear Measurements</title><categories>cs.IT math.IT</categories><comments>32 pages, preliminary version submitted to ISIT'12</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the problem of decomposing a superposition of a
low-rank matrix and a sparse matrix when a relatively few linear measurements
are available. This problem arises in many data processing tasks such as
aligning multiple images or rectifying regular texture, where the goal is to
recover a low-rank matrix with a large fraction of corrupted entries in the
presence of nonlinear domain transformation. We consider a natural convex
heuristic to this problem which is a variant to the recently proposed Principal
Component Pursuit. We prove that under suitable conditions, this convex program
guarantees to recover the correct low-rank and sparse components despite
reduced measurements. Our analysis covers both random and deterministic
measurement models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6447</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6447</id><created>2012-02-29</created><updated>2012-03-15</updated><authors><author><keyname>Zhu</keyname><forenames>Mingzhi</forenames></author><author><keyname>Ge</keyname><forenames>Gennian</forenames></author></authors><title>Quaternary Constant-Composition Codes with Weight Four and Distances
  Five or Six</title><categories>cs.IT math.CO math.IT</categories><comments>23 pages, 3 tables</comments><msc-class>94B25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The sizes of optimal constant-composition codes of weight three have been
determined by Chee, Ge and Ling with four cases in doubt. Group divisible codes
played an important role in their constructions. In this paper, we study the
problem of constructing optimal quaternary constant-composition codes with
Hamming weight four and minimum distances five or six through group divisible
codes and Room square approaches. The problem is solved leaving only five
lengths undetermined. Previously, the results on the sizes of such quaternary
constant-composition codes were scarce.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6456</identifier>
 <datestamp>2012-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6456</id><created>2012-02-29</created><authors><author><keyname>King</keyname><forenames>Valerie</forenames></author><author><keyname>Saia</keyname><forenames>Jared</forenames></author><author><keyname>Young</keyname><forenames>Maxwell</forenames></author></authors><title>Resource-Competitive Communication</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider the general scenario where Alice wishes to transmit a message m to
Bob. These two players share a communication channel; however, there exists an
adversary, Carol, who aims to prevent the transmission of m by blocking this
channel. There are costs to send, receive or block m on the channel, and we
ask: How much do Alice and Bob need to spend relative to the adversary Carol in
order to guarantee transmission of m?
  We show that in a time-slotted network with constant costs to send, receive
and block m in a slot, if Carol spends a total of B slots trying to block m,
then both Alice and Bob must be active for only O(B^{\varphi - 1} +
1)=O(B^{.62}+1) slots in expectation to transmit m, where \varphi = (1 +
\sqrt{5})/2 is the golden ratio. Surprisingly, this result holds even if (1) B
is unknown to either player; (2) Carol knows the algorithms of both players,
but not their random bits; and (3) Carol can attack using total knowledge of
past actions of both players.
  In the spirit of competitive analysis, approximation guarantees, and
game-theoretic treatments, our approach represents another notion of relative
performance that we call resource competitiveness. This new metric measures the
worst-case performance of an algorithm relative to any adversarial strategy and
pertains to scenarios where all network devices are resource-constrained. Here,
we apply the resource-competitive results above to two concrete problems.
First, we consider jamming attacks in WSNs and address the fundamental task of
propagating m from a single device to all others in the presence of faults.
Second, we examine how to mitigate application-level DDoS attacks in a wired
client-server scenario.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6472</identifier>
 <datestamp>2012-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6472</id><created>2012-02-29</created><authors><author><keyname>Shi</keyname><forenames>Xiaomu</forenames><affiliation>LIAMA</affiliation></author><author><keyname>Monin</keyname><forenames>Jean-Fran&#xe7;ois</forenames><affiliation>LIAMA, UJF</affiliation></author><author><keyname>Tuong</keyname><forenames>Frederic</forenames><affiliation>LIAMA</affiliation></author><author><keyname>Blanqui</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames><affiliation>LIAMA</affiliation></author></authors><title>First steps towards the certification of an ARM simulator using Compcert</title><categories>cs.LO cs.SE</categories><comments>First International Conference on Certified Programs and Proofs 7086
  (2011)</comments><proxy>ccsd</proxy><doi>10.1007/978-3-642-25379-9_25</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The simulation of Systems-on-Chip (SoC) is nowadays a hot topic because,
beyond providing many debugging facilities, it allows the development of
dedicated software before the hardware is available. Low-consumption CPUs such
as ARM play a central role in SoC. However, the effectiveness of simulation
depends on the faithfulness of the simulator. To this effect, we propose here
to prove significant parts of such a simulator, SimSoC. Basically, on one hand,
we develop a Coq formal model of the ARM architecture while on the other hand,
we consider a version of the simulator including components written in
Compcert-C. Then we prove that the simulation of ARM operations, according to
Compcert-C formal semantics, conforms to the expected formal model of ARM. Size
issues are partly dealt with using automatic generation of significant parts of
the Coq model and of SimSoC from the official textual definition of ARM.
However, this is still a long-term project. We report here the current stage of
our efforts and discuss in particular the use of Compcert-C in this framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6473</identifier>
 <datestamp>2012-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6473</id><created>2012-02-29</created><authors><author><keyname>Blanqui</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames><affiliation>LIAMA</affiliation></author><author><keyname>Koprowski</keyname><forenames>Adam</forenames></author></authors><title>CoLoR: a Coq library on well-founded rewrite relations and its
  application to the automated verification of termination certificates</title><categories>cs.LO</categories><proxy>ccsd</proxy><journal-ref>Mathematical Structures in Computer Science 21, 4 (2011) 827-859</journal-ref><doi>10.1017/S0960129511000120</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Termination is an important property of programs; notably required for
programs formulated in proof assistants. It is a very active subject of
research in the Turing-complete formalism of term rewriting systems, where many
methods and tools have been developed over the years to address this problem.
Ensuring reliability of those tools is therefore an important issue. In this
paper we present a library formalizing important results of the theory of
well-founded (rewrite) relations in the proof assistant Coq. We also present
its application to the automated verification of termination certificates, as
produced by termination tools.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6481</identifier>
 <datestamp>2012-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6481</id><created>2012-02-29</created><authors><author><keyname>Sharon</keyname><forenames>Eran</forenames></author><author><keyname>Alrod</keyname><forenames>Idan</forenames></author></authors><title>Coding Scheme for Optimizing Random I/O Performance</title><categories>cs.IT math.IT</categories><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Flash memories intended for SSD and mobile applications need to provide high
random I/O performance. This requires using efficient schemes for reading small
chunks of data (e.g. 0.5KB - 4KB) from random addresses. Furthermore, in order
to be cost efficient, it is desirable to use high density Multi-Level Cell
(MLC) memories, such as the ones based on 3 or 4 bit per cell technologies.
Unfortunately, these two requirements are contradicting, as reading an MLC
memory, whose data is coded conventionally, requires multiple sensing
operations, resulting in slow reading and degraded random I/O performance. This
paper describes a novel coding scheme that optimizes random read throughput, by
allowing reading small data chunks from an MLC memory using a single sensing
operation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6501</identifier>
 <datestamp>2012-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6501</id><created>2012-02-29</created><updated>2012-04-06</updated><authors><author><keyname>Lee</keyname><forenames>Seunghyun</forenames></author><author><keyname>Huang</keyname><forenames>Kaibin</forenames></author></authors><title>Coverage and Economy of Cellular Networks with Many Base Stations</title><categories>cs.NI</categories><comments>3 pages, 3 figures, to appear in IEEE Communications Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The performance of a cellular network can be significantly improved by
employing many base stations (BSs), which shortens transmission distances.
However, there exist no known results on quantifying the performance gains from
deploying many BSs. To address this issue, we adopt a stochastic-geometry model
of the downlink cellular network and analyze the mobile outage probability.
Specifically, given Poisson distributed BSs, the outage probability is shown to
diminish inversely with the increasing ratio between the BS and mobile
densities. Furthermore, we analyze the optimal tradeoff between the performance
gain from increasing the BS density and the resultant network cost accounting
for energy consumption, BS hardware and backhaul cables. The optimal BS density
is proved to be proportional to the square root of the mobile density and the
inverse of the square root of the cost factors considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6504</identifier>
 <datestamp>2013-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6504</id><created>2012-02-29</created><updated>2013-01-12</updated><authors><author><keyname>Muandet</keyname><forenames>Krikamol</forenames></author><author><keyname>Fukumizu</keyname><forenames>Kenji</forenames></author><author><keyname>Dinuzzo</keyname><forenames>Francesco</forenames></author><author><keyname>Sch&#xf6;lkopf</keyname><forenames>Bernhard</forenames></author></authors><title>Learning from Distributions via Support Measure Machines</title><categories>stat.ML cs.LG</categories><comments>Advances in Neural Information Processing Systems 25</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a kernel-based discriminative learning framework on
probability measures. Rather than relying on large collections of vectorial
training examples, our framework learns using a collection of probability
distributions that have been constructed to meaningfully represent training
data. By representing these probability distributions as mean embeddings in the
reproducing kernel Hilbert space (RKHS), we are able to apply many standard
kernel-based learning techniques in straightforward fashion. To accomplish
this, we construct a generalization of the support vector machine (SVM) called
a support measure machine (SMM). Our analyses of SMMs provides several insights
into their relationship to traditional SVMs. Based on such insights, we propose
a flexible SVM (Flex-SVM) that places different kernel functions on each
training example. Experimental results on both synthetic and real-world data
demonstrate the effectiveness of our proposed framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6513</identifier>
 <datestamp>2012-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6513</id><created>2012-02-29</created><authors><author><keyname>Slonina</keyname><forenames>Mariusz</forenames></author><author><keyname>Gozdziewski</keyname><forenames>Krzysztof</forenames></author><author><keyname>Migaszewski</keyname><forenames>Cezary</forenames></author></authors><title>Mechanic: a new numerical MPI framework for the dynamical astronomy</title><categories>astro-ph.IM astro-ph.EP cs.DC</categories><comments>4 pages, 7 figures, in GREAT-ESF Workshop 'Orbital Couples: &quot;Pas de
  Deux&quot; in the Solar System and the Milky Way', Paris, IMCCE proceedings, in
  press</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop the Mechanic package, which is a new numerical framework for
dynamical astronomy. The aim of our software is to help in massive numerical
simulations by efficient task management and unified data storage. The code is
built on top of the Message Passing Interface (MPI) and Hierarchical Data
Format (HDF5) standards and uses the Task Farm approach to manage numerical
tasks. It relies on the core-module approach. The numerical problem implemented
in the user-supplied module is separated from the host code (core). The core is
designed to handle basic setup, data storage and communication between nodes in
a computing pool. It has been tested on large CPU-clusters, as well as desktop
computers. The Mechanic may be used in computing dynamical maps, data
optimization or numerical integration. The code and sample modules are freely
available at http://git.astri.umk.pl/projects/mechanic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6517</identifier>
 <datestamp>2012-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6517</id><created>2012-02-29</created><authors><author><keyname>Ciesla</keyname><forenames>Michal</forenames></author><author><keyname>Koziol</keyname><forenames>Przemyslaw</forenames></author></authors><title>Eye Pupil Location Using Webcam</title><categories>cs.HC cs.CV</categories><comments>11 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Three different algorithms used for eye pupil location were described and
tested. Algorithm efficiency comparison was based on human faces images taken
from the BioID database. Moreover all the eye localisation methods were
implemented in a dedicated application supporting eye movement based computer
control. In this case human face images were acquired by a webcam and processed
in a real-time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6522</identifier>
 <datestamp>2015-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6522</id><created>2012-02-29</created><updated>2015-01-07</updated><authors><author><keyname>Schaeffer</keyname><forenames>Nathana&#xeb;l</forenames></author></authors><title>Efficient Spherical Harmonic Transforms aimed at pseudo-spectral
  numerical simulations</title><categories>physics.comp-ph cs.MS cs.NA cs.PF</categories><comments>8 pages</comments><proxy>Ccsd</proxy><journal-ref>Geochemistry, Geophysics, Geosystems, American Geophysical Union
  (AGU), 2013, 14 (3), pp.751-758</journal-ref><doi>10.1002/ggge.20071</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we report on very efficient algorithms for the spherical
harmonic transform (SHT). Explicitly vectorized variations of the algorithm
based on the Gauss-Legendre quadrature are discussed and implemented in the
SHTns library which includes scalar and vector transforms. The main
breakthrough is to achieve very efficient on-the-fly computations of the
Legendre associated functions, even for very high resolutions, by taking
advantage of the specific properties of the SHT and the advanced capabilities
of current and future computers. This allows us to simultaneously and
significantly reduce memory usage and computation time of the SHT. We measure
the performance and accuracy of our algorithms. Even though the complexity of
the algorithms implemented in SHTns are in $O(N^3)$ (where N is the maximum
harmonic degree of the transform), they perform much better than any third
party implementation, including lower complexity algorithms, even for
truncations as high as N=1023. SHTns is available at
https://bitbucket.org/nschaeff/shtns as open source software.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6530</identifier>
 <datestamp>2012-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6530</id><created>2012-02-29</created><authors><author><keyname>Liang</keyname><forenames>Min</forenames></author><author><keyname>Yang</keyname><forenames>Li</forenames></author></authors><title>On Quantum Turing Machine Halting Deterministically</title><categories>quant-ph cs.CC</categories><comments>13 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define a subclass of quantum Turing machine (QTM) named SR-QTM, which
halts deterministically and has deterministic tape head position. A quantum
state transition diagram (QSTD) is proposed to describe SR-QTM. With the help
of QSTD, we construct a SR-QTM which is universal for all near-trivial
transformations. This means there exists a QTM which is universal for the above
subclass. Finally we prove that SR-QTM is computational equivalent with
ordinary QTM in the bounded error setting. It can be seen that, because SR-QTM
has the same time steps for different branches of computation, the halting
scheme problem will not exist when considering SR-QTM as a model of quantum
computing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6548</identifier>
 <datestamp>2012-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6548</id><created>2012-02-29</created><updated>2012-03-01</updated><authors><author><keyname>Albanese</keyname><forenames>Davide</forenames></author><author><keyname>Visintainer</keyname><forenames>Roberto</forenames></author><author><keyname>Merler</keyname><forenames>Stefano</forenames></author><author><keyname>Riccadonna</keyname><forenames>Samantha</forenames></author><author><keyname>Jurman</keyname><forenames>Giuseppe</forenames></author><author><keyname>Furlanello</keyname><forenames>Cesare</forenames></author></authors><title>mlpy: Machine Learning Python</title><categories>cs.MS cs.LG stat.ML</categories><comments>Corrected a few typos; rephrased two sentences in the Overview
  section</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  mlpy is a Python Open Source Machine Learning library built on top of
NumPy/SciPy and the GNU Scientific Libraries. mlpy provides a wide range of
state-of-the-art machine learning methods for supervised and unsupervised
problems and it is aimed at finding a reasonable compromise among modularity,
maintainability, reproducibility, usability and efficiency. mlpy is
multiplatform, it works with Python 2 and 3 and it is distributed under GPL3 at
the website http://mlpy.fbk.eu.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6555</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6555</id><created>2012-02-29</created><authors><author><keyname>Haghighatshoar</keyname><forenames>Saeid</forenames></author><author><keyname>Abbe</keyname><forenames>Emmanuel</forenames></author><author><keyname>Telatar</keyname><forenames>Emre</forenames></author></authors><title>Adaptive sensing using deterministic partial Hadamard matrices</title><categories>cs.IT math.IT</categories><comments>8 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the construction of deterministic matrices preserving
the entropy of random vectors with a given probability distribution. In
particular, it is shown that for random vectors having i.i.d. discrete
components, this is achieved by selecting a subset of rows of a Hadamard matrix
such that (i) the selection is deterministic (ii) the fraction of selected rows
is vanishing. In contrast, it is shown that for random vectors with i.i.d.
continuous components, no partial Hadamard matrix of reduced dimension allows
to preserve the entropy. These results are in agreement with the results of
Wu-Verdu on almost lossless analog compression. This paper is however motivated
by the complexity attribute of Hadamard matrices, which allows the use of
efficient and stable reconstruction algorithms. The proof technique is based on
a polar code martingale argument and on a new entropy power inequality for
integer-valued random variables.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6562</identifier>
 <datestamp>2013-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6562</id><created>2012-02-29</created><updated>2013-05-11</updated><authors><author><keyname>Meng</keyname><forenames>Deyu</forenames></author><author><keyname>Leung</keyname><forenames>Yee</forenames></author><author><keyname>Zhao</keyname><forenames>Qian</forenames></author><author><keyname>Xu</keyname><forenames>Zongben</forenames></author></authors><title>Dictionary learning under global sparsity constraint</title><categories>cs.DS</categories><comments>27 pages, 9 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new method is proposed in this paper to learn overcomplete dictionary from
training data samples. Differing from the current methods that enforce similar
sparsity constraint on each of the input samples, the proposed method attempts
to impose global sparsity constraint on the entire data set. This enables the
proposed method to fittingly assign the atoms of the dictionary to represent
various samples and optimally adapt to the complicated structures underlying
the entire data set. By virtue of the sparse coding and sparse PCA techniques,
a simple algorithm is designed for the implementation of the method. The
efficiency and the convergence of the proposed algorithm are also theoretically
analyzed. Based on the experimental results implemented on a series of signal
and image data sets, it is apparent that our method performs better than the
current dictionary learning methods in original dictionary recovering, input
data reconstructing, and salient data structure revealing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6575</identifier>
 <datestamp>2012-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6575</id><created>2012-02-29</created><updated>2012-06-29</updated><authors><author><keyname>Tr&#xe4;ff</keyname><forenames>Jesper Larsson</forenames></author></authors><title>Simplified, stable parallel merging</title><categories>cs.DC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This note makes an observation that significantly simplifies a number of
previous parallel, two-way merge algorithms based on binary search and
sequential merge in parallel. First, it is shown that the additional merge step
of distinguished elements as found in previous algorithms is not necessary,
thus simplifying the implementation and reducing constant factors. Second, by
fixating the requirements to the binary search, the merge algorithm becomes
stable, provided that the sequential merge subroutine is stable. The stable,
parallel merge algorithm can easily be used to implement a stable, parallel
merge sort.
  For ordered sequences with $n$ and $m$ elements, $m\leq n$, the simplified
merge algorithm runs in $O(n/p+\log n)$ operations using $p$ processing
elements. It can be implemented on an EREW PRAM, but since it requires only a
single synchronization step, it is also a candidate for implementation on other
parallel, shared-memory computers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6581</identifier>
 <datestamp>2015-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6581</id><created>2012-02-29</created><updated>2015-01-31</updated><authors><author><keyname>Viglietta</keyname><forenames>Giovanni</forenames></author></authors><title>Lemmings is PSPACE-complete</title><categories>cs.GT cs.CC</categories><comments>26 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lemmings is a computer puzzle game developed by DMA Design and published by
Psygnosis in 1991, in which the player has to guide a tribe of lemming
creatures to safety through a hazardous landscape, by assigning them specific
skills that modify their behavior in different ways. In this paper we study the
optimization problem of saving the highest number of lemmings in a given
landscape with a given number of available skills.
  We prove that the game is PSPACE-complete, even if there is only one lemming
to save, and only Builder and Basher skills are available. We thereby settle an
open problem posed by Cormode in 2004, and again by Forisek in 2010. However we
also prove that, if we restrict the game to levels in which the available
Builder skills are only polynomially many (and there is any number of other
skills), then the game is solvable in NP. Similarly, if the available Basher,
Miner, and Digger skills are polynomially many, the game is solvable in NP.
  Furthermore, we show that saving the maximum number of lemmings is APX-hard,
even when only one type of skill is available, whatever this skill is. This
contrasts with the membership in P of the decision problem restricted to levels
with no &quot;deadly areas&quot; (such as water or traps) and only Climber and Floater
skills, as previously established by Cormode.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6583</identifier>
 <datestamp>2012-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6583</id><created>2012-02-29</created><authors><author><keyname>Quesada</keyname><forenames>Luis</forenames></author><author><keyname>Berzal</keyname><forenames>Fernando</forenames></author><author><keyname>Cortijo</keyname><forenames>Francisco J.</forenames></author></authors><title>A Lexical Analysis Tool with Ambiguity Support</title><categories>cs.CL cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lexical ambiguities naturally arise in languages. We present Lamb, a lexical
analyzer that produces a lexical analysis graph describing all the possible
sequences of tokens that can be found within the input string. Parsers can
process such lexical analysis graphs and discard any sequence of tokens that
does not produce a valid syntactic sentence, therefore performing, together
with Lamb, a context-sensitive lexical analysis in lexically-ambiguous language
specifications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6586</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6586</id><created>2012-02-29</created><authors><author><keyname>Quesada</keyname><forenames>Luis</forenames></author><author><keyname>Le&#xf3;n</keyname><forenames>Alejandro J.</forenames></author></authors><title>Filling-Based Techniques Applied to Object Projection Feature Estimation</title><categories>cs.CV</categories><comments>arXiv admin note: substantial text overlap with arXiv:1111.3969</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  3D motion tracking is a critical task in many computer vision applications.
Unsupervised markerless 3D motion tracking systems determine the most relevant
object in the screen and then track it by continuously estimating its
projection features (center and area) from the edge image and a point inside
the relevant object projection (namely, inner point), until the tracking fails.
Existing object projection feature estimation techniques are based on
ray-casting from the inner point. These techniques present three main
drawbacks: when the inner point is surrounded by edges, rays may not reach
other relevant areas; as a consequence of that issue, the estimated features
may greatly vary depending on the position of the inner point relative to the
object projection; and finally, increasing the number of rays being casted and
the ray-casting iterations (which would make the results more accurate and
stable) increases the processing time to the point the tracking cannot be
performed on the fly. In this paper, we analyze an intuitive filling-based
object projection feature estimation technique that solves the aforementioned
problems but is too sensitive to edge miscalculations. Then, we propose a less
computing-intensive modification to that technique that would not be affected
by the existing techniques issues and would be no more sensitive to edge
miscalculations than ray-casting-based techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6591</identifier>
 <datestamp>2012-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6591</id><created>2012-02-27</created><authors><author><keyname>Akinwale</keyname><forenames>A. T.</forenames></author><author><keyname>Ibharalu</keyname><forenames>F. T.</forenames></author></authors><title>Password Authentication Scheme with Secured Login Interface</title><categories>cs.CR</categories><comments>9 pages</comments><journal-ref>Ann. Univ. Tibiscus Comp. Sci. Series VII/2 (2009), 77-85</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel solution to the age long problem of password
security at input level. In our solution, each of the various characters from
which a password could be composed is encoded with a random single digit
integer and presented to the user via an input interface form. A legitimate
user entering his password only needs to carefully study the sequence of code
that describe his password, and then enter these code in place of his actual
password characters. This approach does not require the input code to be hidden
from anyone or converted to placeholder characters for security reasons. Our
solution engine regenerates new code for each character each time the carriage
return key is struck, producing a hardened password that is convincingly more
secure than conventional password entry system against both online and offline
attackers. Using empirical data and a prototype implementation of our scheme,
we give evidence that our approach is viable in practice, in terms of ease of
use, improved security, and performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6593</identifier>
 <datestamp>2012-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6593</id><created>2012-02-29</created><authors><author><keyname>Quesada</keyname><forenames>Luis</forenames></author><author><keyname>Berzal</keyname><forenames>Fernando</forenames></author><author><keyname>Cubero</keyname><forenames>Juan-Carlos</forenames></author></authors><title>A Model-Driven Parser Generator, from Abstract Syntax Trees to Abstract
  Syntax Graphs</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Model-based parser generators decouple language specification from language
processing. The model-driven approach avoids the limitations that conventional
parser generators impose on the language designer. Conventional tools require
the designed language grammar to conform to the specific kind of grammar
supported by the particular parser generator (being LL and LR parser generators
the most common). Model-driven parser generators, like ModelCC, do not require
a grammar specification, since that grammar can be automatically derived from
the language model and, if needed, adapted to conform to the requirements of
the given kind of parser, all of this without interfering with the conceptual
design of the language and its associated applications. Moreover, model-driven
tools such as ModelCC are able to automatically resolve references between
language elements, hence producing abstract syntax graphs instead of abstract
syntax trees as the result of the parsing process. Such graphs are not confined
to directed acyclic graphs and they can contain cycles, since ModelCC supports
anaphoric, cataphoric, and recursive references.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6596</identifier>
 <datestamp>2012-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6596</id><created>2012-02-29</created><authors><author><keyname>Luo</keyname><forenames>Shuangyu</forenames></author><author><keyname>Li</keyname><forenames>Jiangyuan</forenames></author><author><keyname>Petropulu</keyname><forenames>Athina</forenames></author></authors><title>Physical Layer Security with Uncoordinated Helpers Implementing
  Cooperative Jamming</title><categories>cs.IT cs.CR math.IT</categories><comments>4 pages, 4 figures, conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A wireless communication network is considered, consisting of a source
(Alice), a destination (Bob) and an eavesdropper (Eve), each equipped with a
single antenna. The communication is assisted by multiple helpers, each
equipped with two antennas, which implement cooperative jamming, i.e.,
transmitting noise to confound Eve. The optimal structure of the jamming noise
that maximizes the secrecy rate is derived. A nulling noise scenario is also
considered, in which each helper transmits noise that nulls out at Bob. Each
helper only requires knowledge of its own link to Bob to determine the noise
locally. For the optimally structured noise, global information of all the
links is required. Although analysis shows that under the two-antenna per
helper scenario the nulling solution is sub-optimal in terms of the achievable
secrecy rate, simulations show that the performance difference is rather small,
with the inexpensive and easy to implement nulling scheme performing near
optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6597</identifier>
 <datestamp>2012-09-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6597</id><created>2012-02-29</created><updated>2012-09-18</updated><authors><author><keyname>Luo</keyname><forenames>Shuangyu</forenames></author><author><keyname>Li</keyname><forenames>Jiangyuan</forenames></author><author><keyname>Petropulu</keyname><forenames>Athina</forenames></author></authors><title>Outage Constrained Secrecy Rate Maximization Using Cooperative Jamming</title><categories>cs.IT cs.CR math.IT</categories><comments>4 pages, 3 figures, conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a Gaussian MISO wiretap channel, where a multi-antenna source
communicates with a single-antenna destination in the presence of a
single-antenna eavesdropper. The communication is assisted by multi-antenna
helpers that act as jammers to the eavesdropper. Each helper independently
transmits noise which lies in the null space of the channel to the destination,
thus creates no interference to the destination. Under the assumption that
there is eavesdropper channel uncertainty, we derive the optimal covariance
matrix for the source signal so that the secrecy rate is maximized subject to
probability of outage and power constraints. Assuming that the eavesdropper
channels follow zero-mean Gaussian model with known covariances, we derive the
outage probability in a closed form. Simulation results in support of the
analysis are provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6598</identifier>
 <datestamp>2012-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6598</id><created>2012-02-29</created><authors><author><keyname>Fu</keyname><forenames>Bin</forenames></author><author><keyname>Li</keyname><forenames>Wenfeng</forenames></author><author><keyname>Peng</keyname><forenames>Zhiyong</forenames></author></authors><title>Sublinear Time Approximate Sum via Uniform Random Sampling</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the approximation for computing the sum $a_1+...+a_n$ with an
input of a list of nonnegative elements $a_1,..., a_n$. If all elements are in
the range $[0,1]$, there is a randomized algorithm that can compute an
$(1+\epsilon)$-approximation for the sum problem in time ${O({n(\log\log
n)\over\sum_{i=1}^n a_i})}$, where $\epsilon$ is a constant in $(0,1)$. Our
randomized algorithm is based on the uniform random sampling, which selects one
element with equal probability from the input list each time. We also prove a
lower bound $\Omega({n\over \sum_{i=1}^n a_i})$, which almost matches the upper
bound, for this problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6600</identifier>
 <datestamp>2012-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6600</id><created>2012-02-29</created><authors><author><keyname>Despi</keyname><forenames>Ioan</forenames></author><author><keyname>Luca</keyname><forenames>Lucian</forenames></author></authors><title>On the Role of Service Concept in IT</title><categories>cs.SE</categories><comments>10 pages</comments><journal-ref>Ann. Univ. Tibiscus Comp. Sci. Series VII/2 (2009), 31-40</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hard times affecting world-wide economy have strong consequences and are
challenging IT departments from all sorts of enterprises. Expensive software
projects are replaced by component-based agile systems and paradigms like SOA,
REST, cloud computing are the new buzz-words. Behind the canvas, the service
concept plays a central role, which we try to reveal
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6601</identifier>
 <datestamp>2012-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6601</id><created>2012-02-29</created><updated>2012-03-09</updated><authors><author><keyname>Shuai</keyname><forenames>Xin</forenames></author><author><keyname>Ding</keyname><forenames>Ying</forenames></author><author><keyname>Busemeyer</keyname><forenames>Jerome</forenames></author></authors><title>Multiple spreaders affect the indirect influence on Twitter</title><categories>cs.SI physics.soc-ph</categories><comments>2 pages, 2 figures; www2012 poster</comments><acm-class>J.4; H.1.2</acm-class><journal-ref>www2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most studies on social influence have focused on direct influence, while
another interesting question can be raised as whether indirect influence exists
between two users who're not directly connected in the network and what affects
such influence. In addition, the theory of \emph{complex contagion} tells us
that more spreaders will enhance the indirect influence between two users. Our
observation of intensity of indirect influence, propagated by $n$ parallel
spreaders and quantified by retweeting probability on Twitter, shows that
complex contagion is validated globally but is violated locally. In other
words, the retweeting probability increases non-monotonically with some local
drops.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6609</identifier>
 <datestamp>2012-04-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6609</id><created>2012-02-29</created><updated>2012-04-18</updated><authors><author><keyname>M&#xe9;tral</keyname><forenames>Claudine</forenames></author><author><keyname>Ghoula</keyname><forenames>Nizar</forenames></author><author><keyname>Falquet</keyname><forenames>Gilles</forenames></author></authors><title>Towards an Integrated Visualization Of Semantically Enriched 3D City
  Models: An Ontology of 3D Visualization Techniques</title><categories>cs.AI cs.GR cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  3D city models - which represent in 3 dimensions the geometric elements of a
city - are increasingly used for an intended wide range of applications. Such
uses are made possible by using semantically enriched 3D city models and by
presenting such enriched 3D city models in a way that allows decision-making
processes to be carried out from the best choices among sets of objectives, and
across issues and scales. In order to help in such a decision-making process we
have defined a framework to find the best visualization technique(s) for a set
of potentially heterogeneous data that have to be visualized within the same 3D
city model, in order to perform a given task in a specific context. We have
chosen an ontology-based approach. This approach and the specification and use
of the resulting ontology of 3D visualization techniques are described in this
paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6614</identifier>
 <datestamp>2015-04-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6614</id><created>2012-02-29</created><updated>2015-04-02</updated><authors><author><keyname>Markov</keyname><forenames>Igor L.</forenames></author><author><keyname>Saeedi</keyname><forenames>Mehdi</forenames></author></authors><title>Constant-Optimized Quantum Circuits for Modular Multiplication and
  Exponentiation</title><categories>cs.ET quant-ph</categories><comments>29 pages, 9 tables, 19 figures. Minor change: fixed two typos in the
  abstract and body</comments><journal-ref>Quantum Information and Computation, Vol. 12, No. 5&amp;6, pp.
  0361-0394, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reversible circuits for modular multiplication $Cx$%$M$ with $x&lt;M$ arise as
components of modular exponentiation in Shor's quantum number-factoring
algorithm. However, existing generic constructions focus on asymptotic gate
count and circuit depth rather than actual values, producing fairly large
circuits not optimized for specific $C$ and $M$ values. In this work, we
develop such optimizations in a bottom-up fashion, starting with most
convenient $C$ values. When zero-initialized ancilla registers are available,
we reduce the search for compact circuits to a shortest-path problem. Some of
our modular-multiplication circuits are asymptotically smaller than previous
constructions, but worst-case bounds and average sizes remain $\Theta(n^2)$. In
the context of modular exponentiation, we offer several constant-factor
improvements, as well as an improvement by a constant additive term that is
significant for few-qubit circuits arising in ongoing laboratory experiments
with Shor's algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6623</identifier>
 <datestamp>2012-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6623</id><created>2012-02-29</created><authors><author><keyname>Hassan</keyname><forenames>Qusay F.</forenames></author></authors><title>Aspects of SOA: An Entry Point for Starters</title><categories>cs.SE</categories><comments>18 pages</comments><journal-ref>Ann. Univ. Tibiscus Comp. Sci. Series VII/2 (2009), 125-142</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Because Service-Oriented Architecture (SOA) is one of the hottest topics that
is currently gaining momentum, and the number of its adopters (both business
and IT executives) is increasing in a tremendous manner, it is really a must to
enlist important aspects related to it in order to allow these adopters to
better understand the role that it can play in both software and business
markets. These aspects varies from the definition of SOA and key components of
it, different forms of support given by elite software vendors to it, its
evolution history, the relationship between it and web services, the future
expectations about its uses and benefits in different organizations, the
relationship between SOA and Enterprise Application Integration (EAI), and
various applications that can use it to overcome limitations related to other
traditional methods. Moreover, challenges that face SOA in software market
should be addressed and discussed in order to be able to see the big picture
and to look for better solutions for them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6641</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6641</id><created>2012-02-29</created><updated>2012-03-06</updated><authors><author><keyname>Hemaspaandra</keyname><forenames>Edith</forenames></author><author><keyname>Hemaspaandra</keyname><forenames>Lane A.</forenames></author><author><keyname>Menton</keyname><forenames>Curtis</forenames></author></authors><title>Search versus Decision for Election Manipulation Problems</title><categories>cs.GT cs.CC cs.MA</categories><report-no>URCS-TR-2012-971</report-no><acm-class>I.2.11; F.1.3; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most theoretical definitions about the complexity of manipulating elections
focus on the decision problem of recognizing which instances can be
successfully manipulated, rather than the search problem of finding the
successful manipulative actions. Since the latter is a far more natural goal
for manipulators, that definitional focus may be misguided if these two
complexities can differ. Our main result is that they probably do differ: If
integer factoring is hard, then for election manipulation, election bribery,
and some types of election control, there are election systems for which
recognizing which instances can be successfully manipulated is in polynomial
time but producing the successful manipulations cannot be done in polynomial
time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6642</identifier>
 <datestamp>2012-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6642</id><created>2012-02-29</created><authors><author><keyname>Cygan</keyname><forenames>Marek</forenames></author></authors><title>Deterministic parameterized connected vertex cover</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the Connected Vertex Cover problem we are given an undirected graph G
together with an integer k and we are to find a subset of vertices X of size at
most k, such that X contains at least one end-point of each edge and moreover X
induces a connected subgraph. For this problem we present a deterministic
algorithm running in O(2^k n^O(1)) time and polynomial space, improving over
previously best O(2.4882^k n^O(1)) deterministic algorithm and O(2^k n^O(1))
randomized algorithm. Furthermore, when usage of exponential space is allowed,
we present an O(2^k k(n+m)) time algorithm that solves a more general variant
with arbitrary real weights.
  Finally, we show that in O(2k k(n + m)) time and O(2^k k) space one can count
the number of connected vertex covers of size at most k, which can not be
improved to O((2 - eps)^k nO(1)) for any eps &gt; 0 under the Strong Exponential
Time Hypothesis, as shown by Cygan et al. [CCC'12].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6645</identifier>
 <datestamp>2012-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6645</id><created>2012-02-29</created><authors><author><keyname>Schweitzer</keyname><forenames>Pascal</forenames></author></authors><title>On Zero Divisors with Small Support in Group Rings of Torsion-Free
  Groups</title><categories>math.RA cs.DM math.GR</categories><comments>20 pages, 8 figures</comments><msc-class>16S34, 20C07 (Primary) 68R05 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Kaplanski's Zero Divisor Conjecture envisions that for a torsion-free group G
and an integral domain R, the group ring R[G] does not contain non-trivial zero
divisors. We define the length of an element a in R[G] as the minimal
non-negative integer k for which there are ring elements r_1,...,r_k in R and
group elements g_1,...,g_k in G such that a = r_1 g_1+...+r_k g_k. We
investigate the conjecture when R is the field of rational numbers. By a
reduction to the finite field with two elements, we show that if ab = 0 for
non-trivial elements in the group ring of a torsion-free group over the
rationals, then the lengths of a and b cannot be among certain combinations.
More precisely, we show for various pairs of integers (i,j) that if one of the
lengths is at most i then the other length must exceed j. Using combinatorial
arguments we show this for the pairs (3,6) and (4,4). With a computer-assisted
approach we strengthen this to show the statement holds for the pairs (3,16)
and (4,7). As part of our method, we describe a combinatorial structure, which
we call matched rectangles, and show that for these a canonical labeling can be
computed in quadratic time. Each matched rectangle gives rise to a presentation
of a group. These associated groups are universal in the sense that there is no
counterexample to the conjecture among them if and only if the conjecture is
true over the rationals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6649</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6649</id><created>2012-02-29</created><authors><author><keyname>Hemaspaandra</keyname><forenames>Edith</forenames></author><author><keyname>Hemaspaandra</keyname><forenames>Lane A.</forenames></author><author><keyname>Rothe</keyname><forenames>Joerg</forenames></author></authors><title>Controlling Candidate-Sequential Elections</title><categories>cs.GT cs.CC cs.MA</categories><comments>6 pages</comments><report-no>URCS-TR-2012-975</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  All previous work on &quot;candidate-control&quot; manipulation of elections has been
in the model of full-information, simultaneous voting. This is a problem, since
in quite a few real-world settings---from TV singing/dancing talent shows to
university faculty-hiring processes---candidates are introduced, and appraised
by the voters, in sequence. We provide a natural model for sequential candidate
evaluation, a framework for evaluating the computational complexity of
controlling the outcome within that framework, and some initial results on the
range such complexity can take on. We hope our work will lead to further
examination of temporally involved candidate control.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6654</identifier>
 <datestamp>2012-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6654</id><created>2012-02-29</created><authors><author><keyname>Orhan</keyname><forenames>Oner</forenames></author><author><keyname>Erkip</keyname><forenames>Elza</forenames></author></authors><title>Optimal Transmission Policies for Energy Harvesting Two-hop Networks</title><categories>cs.IT math.IT</categories><comments>6 pages, 5 figures, submitted to CISS 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a two-hop communication system with energy harvesting nodes is
considered. Unlike battery powered wireless nodes, both the source and the
relay are able to harvest energy from environment during communication,
therefore, both data and energy causality over the two hops need to be
considered. Assuming both nodes know the harvested energies in advance,
properties of optimal transmission policies to maximize the delivered data by a
given deadline are identified. Using these properties, optimal power allocation
and transmission schedule for the case in which both nodes harvest two energy
packets is developed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6655</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6655</id><created>2012-02-29</created><updated>2012-09-28</updated><authors><author><keyname>Hemaspaandra</keyname><forenames>Edith</forenames></author><author><keyname>Hemaspaandra</keyname><forenames>Lane A.</forenames></author><author><keyname>Rothe</keyname><forenames>Joerg</forenames></author></authors><title>The Complexity of Online Manipulation of Sequential Elections</title><categories>cs.GT cs.CC cs.MA</categories><comments>24 pages</comments><report-no>URCS-TR-2012-974</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most work on manipulation assumes that all preferences are known to the
manipulators. However, in many settings elections are open and sequential, and
manipulators may know the already cast votes but may not know the future votes.
We introduce a framework, in which manipulators can see the past votes but not
the future ones, to model online coalitional manipulation of sequential
elections, and we show that in this setting manipulation can be extremely
complex even for election systems with simple winner problems. Yet we also show
that for some of the most important election systems such manipulation is
simple in certain settings. This suggests that when using sequential voting,
one should pay great attention to the details of the setting in choosing one's
voting rule. Among the highlights of our classifications are: We show that,
depending on the size of the manipulative coalition, the online manipulation
problem can be complete for each level of the polynomial hierarchy or even for
PSPACE. We obtain the most dramatic contrast to date between the
nonunique-winner and unique-winner models: Online weighted manipulation for
plurality is in P in the nonunique-winner model, yet is coNP-hard (constructive
case) and NP-hard (destructive case) in the unique-winner model. And we obtain
what to the best of our knowledge are the first P^NP[1]-completeness and
P^NP-completeness results in the field of computational social choice, in
particular proving such completeness for, respectively, the complexity of
3-candidate and 4-candidate (and unlimited-candidate) online weighted coalition
manipulation of veto elections.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6658</identifier>
 <datestamp>2012-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6658</id><created>2012-02-29</created><authors><author><keyname>Vaze</keyname><forenames>Chinmay S.</forenames></author><author><keyname>Varanasi</keyname><forenames>Mahesh K.</forenames></author></authors><title>Independent signaling achieves the capacity region of the Gaussian
  interference channel with common information to within one bit</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Trans. on Information Theory, Feb. 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The interference channel with common information (IC-CI) consists of two
transmit-receive pairs that communicate over a common noisy medium. Each
transmitter has an individual message for its paired receiver, and
additionally, both transmitters have a common message to deliver to both
receivers. In this paper, through explicit inner and outer bounds on the
capacity region, we establish the capacity region of the Gaussian IC-CI to
within a bounded gap of one bit, independently of the values of all channel
parameters. Using this constant-gap characterization, the generalized degrees
of freedom (GDoF) region is determined. It is shown that the introduction of
the common message leads to an increase in the GDoF over that achievable over
the Gaussian interference channel without a common message, and hence to an
unbounded improvement in the achievable rate. A surprising feature of the
capacity-within-one-bit result is that most of the available benefit (i.e., to
within one bit of capacity) due to the common message is achieved through a
simple and explicit coding scheme that involves independent signaling at the
two transmitters so that, in effect, this scheme forgoes the opportunity for
transmitter cooperation that is inherently available due to shared knowledge of
the common message at both transmitters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6666</identifier>
 <datestamp>2012-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6666</id><created>2012-02-29</created><authors><author><keyname>Meyer</keyname><forenames>Francois G.</forenames></author><author><keyname>Shen</keyname><forenames>Xilin</forenames></author></authors><title>Perturbation of the Eigenvectors of the Graph Laplacian: Application to
  Image Denoising</title><categories>physics.data-an cs.CV stat.ML</categories><msc-class>62H35</msc-class><acm-class>G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The original contributions of this paper are twofold: a new understanding of
the influence of noise on the eigenvectors of the graph Laplacian of a set of
image patches, and an algorithm to estimate a denoised set of patches from a
noisy image. The algorithm relies on the following two observations: (1) the
low-index eigenvectors of the diffusion, or graph Laplacian, operators are very
robust to random perturbations of the weights and random changes in the
connections of the patch-graph; and (2) patches extracted from smooth regions
of the image are organized along smooth low-dimensional structures in the
patch-set, and therefore can be reconstructed with few eigenvectors.
Experiments demonstrate that our denoising algorithm outperforms the denoising
gold-standards.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6668</identifier>
 <datestamp>2013-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6668</id><created>2012-02-29</created><updated>2013-05-03</updated><authors><author><keyname>Bauwens</keyname><forenames>Bruno</forenames></author><author><keyname>Shen</keyname><forenames>Alexander</forenames></author></authors><title>Complexity of complexity and strings with maximal plain and prefix
  Kolmogorov complexity</title><categories>cs.CC</categories><comments>13 pages, 1 figure</comments><msc-class>68Q30</msc-class><acm-class>F.1.3</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Peter Gacs showed (Gacs 1974) that for every n there exists a bit string x of
length n whose plain complexity C(x) has almost maximal conditional complexity
relative to x, i.e., C(C(x)|x) &gt; log n - log^(2) n - O(1). (Here log^(2) i =
log log i.) Following Elena Kalinina (Kalinina 2011), we provide a simple
game-based proof of this result; modifying her argument, we get a better (and
tight) bound log n - O(1). We also show the same bound for prefix-free
complexity.
  Robert Solovay showed (Solovay 1975) that infinitely many strings x have
maximal plain complexity but not maximal prefix complexity (among the strings
of the same length): for some c there exist infinitely many x such that |x| -
C(x) &lt; c and |x| + K(|x|) - K(x) &gt; log^(2) |x| - c log^(3) |x|. In fact, the
results of Solovay and Gacs are closely related. Using the result above, we
provide a short proof for Solovay's result. We also generalize it by showing
that for some c and for all n there are strings x of length n with n - C (x) &lt;
c and n + K(n) - K(x) &gt; K(K(n)|n) - 3 K(K(K(n)|n)|n) - c. We also prove a close
upper bound K(K(n)|n) + O(1).
  Finally, we provide a direct game proof for Joseph Miller's generalization
(Miller 2006) of the same Solovay's theorem: if a co-enumerable set (a set with
c.e. complement) contains for every length a string of this length, then it
contains infinitely many strings x such that |x| + K(|x|) - K(x) &gt; log^(2) |x|
+ O(log^(3) |x|).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6669</identifier>
 <datestamp>2012-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6669</id><created>2012-02-29</created><updated>2012-04-10</updated><authors><author><keyname>Firouzbakht</keyname><forenames>Koorosh</forenames></author><author><keyname>Noubir</keyname><forenames>Guevara</forenames></author><author><keyname>Salehi</keyname><forenames>Masoud</forenames></author></authors><title>On the Capacity of Rate-Adaptive Packetized Wireless Communication Links
  under Jamming</title><categories>cs.IT cs.GT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We formulate the interaction between the communicating nodes and an adversary
within a game-theoretic context. We show that earlier information-theoretic
capacity results for a jammed channel correspond to a pure Nash Equilibrium
(NE). However, when both players are allowed to randomize their actions (i.e.,
coding rate and jamming power) new mixed Nash equilibria appear with surprising
properties. We show the existence of a threshold ($J_{TH}$) such that if the
jammer average power exceeds $J_{TH}$, the channel capacity at the NE is the
same as if the jammer was using its maximum allowable power, $J_{Max}$, all the
time. This indicates that randomization significantly advantages powerful
jammers. We also show how the NE strategies can be derived, and we provide very
simple (e.g., semi-uniform) approximations to the optimal communication and
jamming strategies. Such strategies are very simple to implement in current
hardware and software.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6677</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6677</id><created>2012-02-29</created><authors><author><keyname>Deutsch</keyname><forenames>Alin</forenames></author><author><keyname>Hull</keyname><forenames>Richard</forenames></author><author><keyname>Vyas</keyname><forenames>Avinash</forenames></author><author><keyname>Zhao</keyname><forenames>Kevin Keliang</forenames></author></authors><title>Trajectory and Policy Aware Sender Anonymity in Location Based Services</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider Location-based Service (LBS) settings, where a LBS provider logs
the requests sent by mobile device users over a period of time and later wants
to publish/share these logs. Log sharing can be extremely valuable for
advertising, data mining research and network management, but it poses a
serious threat to the privacy of LBS users. Sender anonymity solutions prevent
a malicious attacker from inferring the interests of LBS users by associating
them with their service requests after gaining access to the anonymized logs.
With the fast-increasing adoption of smartphones and the concern that historic
user trajectories are becoming more accessible, it becomes necessary for any
sender anonymity solution to protect against attackers that are
trajectory-aware (i.e. have access to historic user trajectories) as well as
policy-aware (i.e they know the log anonymization policy). We call such
attackers TP-aware.
  This paper introduces a first privacy guarantee against TP-aware attackers,
called TP-aware sender k-anonymity. It turns out that there are many possible
TP-aware anonymizations for the same LBS log, each with a different utility to
the consumer of the anonymized log. The problem of finding the optimal TP-aware
anonymization is investigated. We show that trajectory-awareness renders the
problem computationally harder than the trajectory-unaware variants found in
the literature (NP-complete in the size of the log, versus PTIME). We describe
a PTIME l-approximation algorithm for trajectories of length l and empirically
show that it scales to large LBS logs (up to 2 million users).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6680</identifier>
 <datestamp>2012-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6680</id><created>2012-02-29</created><authors><author><keyname>Diakonikolas</keyname><forenames>Ilias</forenames></author><author><keyname>Jaiswal</keyname><forenames>Ragesh</forenames></author><author><keyname>Servedio</keyname><forenames>Rocco A.</forenames></author><author><keyname>Tan</keyname><forenames>Li-Yang</forenames></author><author><keyname>Wan</keyname><forenames>Andrew</forenames></author></authors><title>On the Distribution of the Fourier Spectrum of Halfspaces</title><categories>cs.CC cs.DM math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bourgain showed that any noise stable Boolean function $f$ can be
well-approximated by a junta. In this note we give an exponential sharpening of
the parameters of Bourgain's result under the additional assumption that $f$ is
a halfspace.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1202.6685</identifier>
 <datestamp>2012-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1202.6685</id><created>2012-02-29</created><authors><author><keyname>Mas</keyname><forenames>Massimiliano Dal</forenames></author></authors><title>Faceted Semantic Search for Personalized Social Search</title><categories>cs.IR</categories><comments>12 pages, 1 figures; 1 table; for details see:
  http://www.maxdalmas.com</comments><msc-class>03B65, 68P05, 68P10, 68Q55, 68T30</msc-class><acm-class>H.3.1; I.2.3; I.2.4; I.2.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Actual social networks (like Facebook, Twitter, Linkedin, ...) need to deal
with vagueness on ontological indeterminacy. In this paper is analyzed the
prototyping of a faceted semantic search for personalized social search using
the &quot;joint meaning&quot; in a community environment. User researches in a
&quot;collaborative&quot; environment defined by folksonomies can be supported by the
most common features on the faceted semantic search. A solution for the
context-aware personalized search is based on &quot;joint meaning&quot; understood as a
joint construal of the creators of the contents and the user of the contents
using the faced taxonomy with the Semantic Web. A proof-of concept prototype
shows how the proposed methodological approach can also be applied to existing
presentation components, built with different languages and/or component
technologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0024</identifier>
 <datestamp>2012-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0024</id><created>2012-02-29</created><authors><author><keyname>Hariri</keyname><forenames>Babak Bagheri</forenames></author><author><keyname>Calvanese</keyname><forenames>Diego</forenames></author><author><keyname>De Giacomo</keyname><forenames>Giuseppe</forenames></author><author><keyname>Deutsch</keyname><forenames>Alin</forenames></author><author><keyname>Montali</keyname><forenames>Marco</forenames></author></authors><title>Verification of Relational Data-Centric Dynamic Systems with External
  Services</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data-centric dynamic systems are systems where both the process controlling
the dynamics and the manipulation of data are equally central. In this paper we
study verification of (first-order) mu-calculus variants over relational
data-centric dynamic systems, where data are represented by a full-fledged
relational database, and the process is described in terms of atomic actions
that evolve the database. The execution of such actions may involve calls to
external services, providing fresh data inserted into the system. As a result
such systems are typically infinite-state. We show that verification is
undecidable in general, and we isolate notable cases, where decidability is
achieved. Specifically we start by considering service calls that return values
deterministically (depending only on passed parameters). We show that in a
mu-calculus variant that preserves knowledge of objects appeared along a run we
get decidability under the assumption that the fresh data introduced along a
run are bounded, though they might not be bounded in the overall system. In
fact we tie such a result to a notion related to weak acyclicity studied in
data exchange. Then, we move to nondeterministic services where the assumption
of data bounded run would result in a bound on the service calls that can be
invoked during the execution and hence would be too restrictive. So we
investigate decidability under the assumption that knowledge of objects is
preserved only if they are continuously present. We show that if infinitely
many values occur in a run but do not accumulate in the same state, then we get
again decidability. We give syntactic conditions to avoid this accumulation
through the novel notion of &quot;generate-recall acyclicity&quot;, which takes into
consideration that every service call activation generates new values that
cannot be accumulated indefinitely.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0029</identifier>
 <datestamp>2012-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0029</id><created>2012-02-29</created><updated>2012-11-23</updated><authors><author><keyname>Zhou</keyname><forenames>Di</forenames></author><author><keyname>D'Agostino</keyname><forenames>Gregorio</forenames></author><author><keyname>Scala</keyname><forenames>Antonio</forenames></author><author><keyname>Stanley</keyname><forenames>H. Eugene</forenames></author></authors><title>Assortativity Decreases the Robustness of Interdependent Networks</title><categories>physics.soc-ph cs.SI physics.data-an</categories><doi>10.1103/PhysRevE.86.066103</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It was recently recognized that interdependencies among different networks
can play a crucial role in triggering cascading failures and hence system-wide
disasters. A recent model shows how pairs of interdependent networks can
exhibit an abrupt percolation transition as failures accumulate. We report on
the effects of topology on failure propagation for a model system consisting of
two interdependent networks. We find that the internal node correlations in
each of the two interdependent networks significantly changes the critical
density of failures that triggers the total disruption of the two-network
system. Specifically, we find that the assortativity (i.e. the likelihood of
nodes with similar degree to be connected) within a single network decreases
the robustness of the entire system. The results of this study on the influence
of assortativity may provide insights into ways of improving the robustness of
network architecture, and thus enhances the level of protection of critical
infrastructures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0030</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0030</id><created>2012-02-29</created><authors><author><keyname>Ramesh</keyname><forenames>Chithrupa</forenames></author><author><keyname>Sandberg</keyname><forenames>Henrik</forenames></author><author><keyname>Johansson</keyname><forenames>Karl H.</forenames></author></authors><title>Design of State-based Schedulers for a Network of Control Loops</title><categories>cs.SY</categories><comments>17 pages, technical report</comments><doi>10.1109/TAC.2013.2251791</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a closed-loop system, which has a contention-based multiple access
network on its sensor link, the Medium Access Controller (MAC) may discard some
packets when the traffic on the link is high. We use a local state-based
scheduler to select a few critical data packets to send to the MAC. In this
paper, we analyze the impact of such a scheduler on the closed-loop system in
the presence of traffic, and show that there is a dual effect with state-based
scheduling. In general, this makes the optimal scheduler and controller hard to
find. However, by removing past controls from the scheduling criterion, we find
that certainty equivalence holds. This condition is related to the classical
result of Bar-Shalom and Tse, and it leads to the design of a scheduler with a
certainty equivalent controller. This design, however, does not result in an
equivalent system to the original problem, in the sense of Witsenhausen.
Computing the estimate is difficult, but can be simplified by introducing a
symmetry constraint on the scheduler. Based on these findings, we propose a
dual predictor architecture for the closed-loop system, which ensures
separation between scheduler, observer and controller. We present an example of
this architecture, which illustrates a network-aware event-triggering
mechanism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0038</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0038</id><created>2012-02-29</created><authors><author><keyname>Dewar</keyname><forenames>Michael</forenames></author><author><keyname>Wiggins</keyname><forenames>Chris</forenames></author><author><keyname>Wood</keyname><forenames>Frank</forenames></author></authors><title>Inference in Hidden Markov Models with Explicit State Duration
  Distributions</title><categories>stat.ML cs.LG</categories><doi>10.1109/LSP.2012.2184795</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this letter we borrow from the inference techniques developed for
unbounded state-cardinality (nonparametric) variants of the HMM and use them to
develop a tuning-parameter free, black-box inference procedure for
Explicit-state-duration hidden Markov models (EDHMM). EDHMMs are HMMs that have
latent states consisting of both discrete state-indicator and discrete
state-duration random variables. In contrast to the implicit geometric state
duration distribution possessed by the standard HMM, EDHMMs allow the direct
parameterisation and estimation of per-state duration distributions. As most
duration distributions are defined over the positive integers, truncation or
other approximations are usually required to perform EDHMM inference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0044</identifier>
 <datestamp>2012-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0044</id><created>2012-02-29</created><authors><author><keyname>Li</keyname><forenames>Junshan</forenames></author></authors><title>Connectivity in one-dimensional ad hoc networks with an access point</title><categories>cs.DM</categories><comments>6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the connectivity in one-dimensional ad hoc wireless
networks with an fixed access point. In recent years, various closed
expressions for the probability of connectivity on one-dimensional networks
(interval graphs) have been derived by many researchers. We will provide some
numerical validation for them by means of extensive simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0050</identifier>
 <datestamp>2012-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0050</id><created>2012-02-29</created><authors><author><keyname>Piliouras</keyname><forenames>Georgios</forenames></author><author><keyname>Valla</keyname><forenames>Tomas</forenames></author><author><keyname>Vegh</keyname><forenames>Laszlo A.</forenames></author></authors><title>LP-based Covering Games with Low Price of Anarchy</title><categories>cs.GT cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new class of vertex cover and set cover games. The price of
anarchy bounds match the best known constant factor approximation guarantees
for the centralized optimization problems for linear and also for submodular
costs -- in contrast to all previously studied covering games, where the price
of anarchy cannot be bounded by a constant (e.g. [6, 7, 11, 5, 2]). In
particular, we describe a vertex cover game with a price of anarchy of 2. The
rules of the games capture the structure of the linear programming relaxations
of the underlying optimization problems, and our bounds are established by
analyzing these relaxations. Furthermore, for linear costs we exhibit linear
time best response dynamics that converge to these almost optimal Nash
equilibria. These dynamics mimic the classical greedy approximation algorithm
of Bar-Yehuda and Even [3].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0055</identifier>
 <datestamp>2012-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0055</id><created>2012-02-29</created><authors><author><keyname>Halim</keyname><forenames>Felix</forenames></author><author><keyname>Idreos</keyname><forenames>Stratos</forenames></author><author><keyname>Karras</keyname><forenames>Panagiotis</forenames></author><author><keyname>Yap</keyname><forenames>Roland H. C.</forenames></author></authors><title>Stochastic Database Cracking: Towards Robust Adaptive Indexing in
  Main-Memory Column-Stores</title><categories>cs.DB</categories><comments>VLDB2012</comments><proxy>Ahmet Sacan</proxy><journal-ref>Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 6, pp.
  502-513 (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern business applications and scientific databases call for inherently
dynamic data storage environments. Such environments are characterized by two
challenging features: (a) they have little idle system time to devote on
physical design; and (b) there is little, if any, a priori workload knowledge,
while the query and data workload keeps changing dynamically. In such
environments, traditional approaches to index building and maintenance cannot
apply. Database cracking has been proposed as a solution that allows on-the-fly
physical data reorganization, as a collateral effect of query processing.
Cracking aims to continuously and automatically adapt indexes to the workload
at hand, without human intervention. Indexes are built incrementally,
adaptively, and on demand. Nevertheless, as we show, existing adaptive indexing
methods fail to deliver workload-robustness; they perform much better with
random workloads than with others. This frailty derives from the inelasticity
with which these approaches interpret each query as a hint on how data should
be stored. Current cracking schemes blindly reorganize the data within each
query's range, even if that results into successive expensive operations with
minimal indexing benefit. In this paper, we introduce stochastic cracking, a
significantly more resilient approach to adaptive indexing. Stochastic cracking
also uses each query as a hint on how to reorganize data, but not blindly so;
it gains resilience and avoids performance bottlenecks by deliberately applying
certain arbitrary choices in its decision-making. Thereby, we bring adaptive
indexing forward to a mature formulation that confers the workload-robustness
previous approaches lacked. Our extensive experimental study verifies that
stochastic cracking maintains the desired properties of original database
cracking while at the same time it performs well with diverse realistic
workloads.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0056</identifier>
 <datestamp>2012-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0056</id><created>2012-02-29</created><authors><author><keyname>Giannikis</keyname><forenames>Georgios</forenames></author><author><keyname>Alonso</keyname><forenames>Gustavo</forenames></author><author><keyname>Kossmann</keyname><forenames>Donald</forenames></author></authors><title>SharedDB: Killing One Thousand Queries With One Stone</title><categories>cs.DB</categories><comments>VLDB2012</comments><proxy>Ahmet Sacan</proxy><journal-ref>Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 6, pp.
  526-537 (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional database systems are built around the query-at-a-time model. This
approach tries to optimize performance in a best-effort way. Unfortunately,
best effort is not good enough for many modern applications. These applications
require response time guarantees in high load situations. This paper describes
the design of a new database architecture that is based on batching queries and
shared computation across possibly hundreds of concurrent queries and updates.
Performance experiments with the TPC-W benchmark show that the performance of
our implementation, SharedDB, is indeed robust across a wide range of dynamic
workloads.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0057</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0057</id><created>2012-02-29</created><authors><author><keyname>Selke</keyname><forenames>Joachim</forenames></author><author><keyname>Lofi</keyname><forenames>Christoph</forenames></author><author><keyname>Balke</keyname><forenames>Wolf-Tilo</forenames></author></authors><title>Pushing the Boundaries of Crowd-enabled Databases with Query-driven
  Schema Expansion</title><categories>cs.DB</categories><comments>VLDB2012</comments><proxy>Ahmet Sacan</proxy><journal-ref>Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 6, pp.
  538-549 (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  By incorporating human workers into the query execution process crowd-enabled
databases facilitate intelligent, social capabilities like completing missing
data at query time or performing cognitive operators. But despite all their
flexibility, crowd-enabled databases still maintain rigid schemas. In this
paper, we extend crowd-enabled databases by flexible query-driven schema
expansion, allowing the addition of new attributes to the database at query
time. However, the number of crowd-sourced mini-tasks to fill in missing values
may often be prohibitively large and the resulting data quality is doubtful.
Instead of simple crowd-sourcing to obtain all values individually, we leverage
the user-generated data found in the Social Web: By exploiting user ratings we
build perceptual spaces, i.e., highly-compressed representations of opinions,
impressions, and perceptions of large numbers of users. Using few training
samples obtained by expert crowd sourcing, we then can extract all missing data
automatically from the perceptual space with high quality and at low costs.
Extensive experiments show that our approach can boost both performance and
quality of crowd-enabled databases, while also providing the flexibility to
expand schemas in a query-driven fashion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0058</identifier>
 <datestamp>2012-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0058</id><created>2012-02-29</created><authors><author><keyname>Zhao</keyname><forenames>Bo</forenames></author><author><keyname>Rubinstein</keyname><forenames>Benjamin I. P.</forenames></author><author><keyname>Gemmell</keyname><forenames>Jim</forenames></author><author><keyname>Han</keyname><forenames>Jiawei</forenames></author></authors><title>A Bayesian Approach to Discovering Truth from Conflicting Sources for
  Data Integration</title><categories>cs.DB cs.LG</categories><comments>VLDB2012</comments><proxy>Ahmet Sacan</proxy><journal-ref>Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 6, pp.
  550-561 (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In practical data integration systems, it is common for the data sources
being integrated to provide conflicting information about the same entity.
Consequently, a major challenge for data integration is to derive the most
complete and accurate integrated records from diverse and sometimes conflicting
sources. We term this challenge the truth finding problem. We observe that some
sources are generally more reliable than others, and therefore a good model of
source quality is the key to solving the truth finding problem. In this work,
we propose a probabilistic graphical model that can automatically infer true
records and source quality without any supervision. In contrast to previous
methods, our principled approach leverages a generative process of two types of
errors (false positive and false negative) by modeling two different aspects of
source quality. In so doing, ours is also the first approach designed to merge
multi-valued attribute types. Our method is scalable, due to an efficient
sampling-based inference algorithm that needs very few iterations in practice
and enjoys linear time complexity, with an even faster incremental variant.
Experiments on two real world datasets show that our new method outperforms
existing state-of-the-art approaches to the truth finding problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0059</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0059</id><created>2012-02-29</created><authors><author><keyname>Upadhyaya</keyname><forenames>Prasang</forenames></author><author><keyname>Balazinska</keyname><forenames>Magdalena</forenames></author><author><keyname>Suciu</keyname><forenames>Dan</forenames></author></authors><title>How to Price Shared Optimizations in the Cloud</title><categories>cs.DB</categories><comments>VLDB2012</comments><proxy>Ahmet Sacan</proxy><journal-ref>Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 6, pp.
  562-573 (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data-management-as-a-service systems are increasingly being used in
collaborative settings, where multiple users access common datasets. Cloud
providers have the choice to implement various optimizations, such as indexing
or materialized views, to accelerate queries over these datasets. Each
optimization carries a cost and may benefit multiple users. This creates a
major challenge: how to select which optimizations to perform and how to share
their cost among users. The problem is especially challenging when users are
selfish and will only report their true values for different optimizations if
doing so maximizes their utility. In this paper, we present a new approach for
selecting and pricing shared optimizations by using Mechanism Design. We first
show how to apply the Shapley Value Mechanism to the simple case of selecting
and pricing additive optimizations, assuming an offline game where all users
access the service for the same time-period. Second, we extend the approach to
online scenarios where users come and go. Finally, we consider the case of
substitutive optimizations. We show analytically that our mechanisms induce
truth- fulness and recover the optimization costs. We also show experimentally
that our mechanisms yield higher utility than the state-of-the-art approach
based on regret accumulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0060</identifier>
 <datestamp>2012-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0060</id><created>2012-02-29</created><authors><author><keyname>Angel</keyname><forenames>Albert</forenames></author><author><keyname>Koudas</keyname><forenames>Nick</forenames></author><author><keyname>Sarkas</keyname><forenames>Nikos</forenames></author><author><keyname>Srivastava</keyname><forenames>Divesh</forenames></author></authors><title>Dense Subgraph Maintenance under Streaming Edge Weight Updates for
  Real-time Story Identification</title><categories>cs.DB</categories><comments>VLDB2012</comments><proxy>Ahmet Sacan</proxy><journal-ref>Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 6, pp.
  574-585 (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent years have witnessed an unprecedented proliferation of social media.
People around the globe author, every day, millions of blog posts, social
network status updates, etc. This rich stream of information can be used to
identify, on an ongoing basis, emerging stories, and events that capture
popular attention. Stories can be identified via groups of tightly-coupled
real-world entities, namely the people, locations, products, etc., that are
involved in the story. The sheer scale, and rapid evolution of the data
involved necessitate highly efficient techniques for identifying important
stories at every point of time. The main challenge in real-time story
identification is the maintenance of dense subgraphs (corresponding to groups
of tightly-coupled entities) under streaming edge weight updates (resulting
from a stream of user-generated content). This is the first work to study the
efficient maintenance of dense subgraphs under such streaming edge weight
updates. For a wide range of definitions of density, we derive theoretical
results regarding the magnitude of change that a single edge weight update can
cause. Based on these, we propose a novel algorithm, DYNDENS, which outperforms
adaptations of existing techniques to this setting, and yields meaningful
results. Our approach is validated by a thorough experimental evaluation on
large-scale real and synthetic datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0061</identifier>
 <datestamp>2012-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0061</id><created>2012-02-29</created><authors><author><keyname>Elghandour</keyname><forenames>Iman</forenames></author><author><keyname>Aboulnaga</keyname><forenames>Ashraf</forenames></author></authors><title>ReStore: Reusing Results of MapReduce Jobs</title><categories>cs.DB</categories><comments>VLDB2012</comments><proxy>Ahmet Sacan</proxy><journal-ref>Proceedings of the VLDB Endowment (PVLDB), Vol. 5, No. 6, pp.
  586-597 (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Analyzing large scale data has emerged as an important activity for many
organizations in the past few years. This large scale data analysis is
facilitated by the MapReduce programming and execution model and its
implementations, most notably Hadoop. Users of MapReduce often have analysis
tasks that are too complex to express as individual MapReduce jobs. Instead,
they use high-level query languages such as Pig, Hive, or Jaql to express their
complex tasks. The compilers of these languages translate queries into
workflows of MapReduce jobs. Each job in these workflows reads its input from
the distributed file system used by the MapReduce system and produces output
that is stored in this distributed file system and read as input by the next
job in the workflow. The current practice is to delete these intermediate
results from the distributed file system at the end of executing the workflow.
One way to improve the performance of workflows of MapReduce jobs is to keep
these intermediate results and reuse them for future workflows submitted to the
system. In this paper, we present ReStore, a system that manages the storage
and reuse of such intermediate results. ReStore can reuse the output of whole
MapReduce jobs that are part of a workflow, and it can also create additional
reuse opportunities by materializing and storing the output of query execution
operators that are executed within a MapReduce job. We have implemented ReStore
as an extension to the Pig dataflow system on top of Hadoop, and we
experimentally demonstrate significant speedups on queries from the PigMix
benchmark.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0076</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0076</id><created>2012-02-29</created><authors><author><keyname>Quesada</keyname><forenames>Luis</forenames></author></authors><title>Using Barriers to Reduce the Sensitivity to Edge Miscalculations of
  Casting-Based Object Projection Feature Estimation</title><categories>cs.CV</categories><comments>arXiv admin note: substantial text overlap with arXiv:1202.6586v1 and
  arXiv:1111.3969</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  3D motion tracking is a critical task in many computer vision applications.
Unsupervised markerless 3D motion tracking systems determine the most relevant
object in the screen and then track it by continuously estimating its
projection features (center and area) from the edge image and a point inside
the relevant object projection (namely, inner point), until the tracking fails.
Existing reliable object projection feature estimation techniques are based on
ray-casting or grid-filling from the inner point. These techniques assume the
edge image to be accurate. However, in real case scenarios, edge
miscalculations may arise from low contrast between the target object and its
surroundings or motion blur caused by low frame rates or fast moving target
objects. In this paper, we propose a barrier extension to casting-based
techniques that mitigates the effect of edge miscalculations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0077</identifier>
 <datestamp>2012-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0077</id><created>2012-02-29</created><authors><author><keyname>Barany</keyname><forenames>Vince</forenames></author><author><keyname>Cate</keyname><forenames>Balder ten</forenames></author><author><keyname>Otto</keyname><forenames>Martin</forenames></author></authors><title>Queries with Guarded Negation (full version)</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A well-established and fundamental insight in database theory is that
negation (also known as complementation) tends to make queries difficult to
process and difficult to reason about. Many basic problems are decidable and
admit practical algorithms in the case of unions of conjunctive queries, but
become difficult or even undecidable when queries are allowed to contain
negation. Inspired by recent results in finite model theory, we consider a
restricted form of negation, guarded negation. We introduce a fragment of SQL,
called GN-SQL, as well as a fragment of Datalog with stratified negation,
called GN-Datalog, that allow only guarded negation, and we show that these
query languages are computationally well behaved, in terms of testing query
containment, query evaluation, open-world query answering, and boundedness.
GN-SQL and GN-Datalog subsume a number of well known query languages and
constraint languages, such as unions of conjunctive queries, monadic Datalog,
and frontier-guarded tgds. In addition, an analysis of standard benchmark
workloads shows that most usage of negation in SQL in practice is guarded
negation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0088</identifier>
 <datestamp>2012-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0088</id><created>2012-02-29</created><updated>2012-03-17</updated><authors><author><keyname>Panigrahy</keyname><forenames>Rina</forenames></author><author><keyname>Zhang</keyname><forenames>Li</forenames></author></authors><title>The Mind Grows Circuits</title><categories>cs.AI cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is a vast supply of prior art that study models for mental processes.
Some studies in psychology and philosophy approach it from an inner perspective
in terms of experiences and percepts. Others such as neurobiology or
connectionist-machines approach it externally by viewing the mind as complex
circuit of neurons where each neuron is a primitive binary circuit. In this
paper, we also model the mind as a place where a circuit grows, starting as a
collection of primitive components at birth and then builds up incrementally in
a bottom up fashion. A new node is formed by a simple composition of prior
nodes when we undergo a repeated experience that can be described by that
composition. Unlike neural networks, however, these circuits take &quot;concepts&quot; or
&quot;percepts&quot; as inputs and outputs. Thus the growing circuits can be likened to a
growing collection of lambda expressions that are built on top of one another
in an attempt to compress the sensory input as a heuristic to bound its
Kolmogorov Complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0096</identifier>
 <datestamp>2012-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0096</id><created>2012-03-01</created><authors><author><keyname>Sircar</keyname><forenames>Pradip</forenames></author></authors><title>Joint Estimation of Angle and Delay of Radio Wave Arrival under
  Multiplicative Noise Environment</title><categories>cs.CE math.ST stat.TH</categories><comments>3 pages, 1 table, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel technique for joint estimation of angle and delay of radio
wave arrival in a multipath mobile communication channel using knowledge of the
transmitted pulse shape function. Employing an array of sensors to sample the
radio received signal, and subsequent array signal processing can provide the
characterization of a high-rank channel in terms of the multipath angles of
arrival and time delays. Although several works have been reported in the
literature for estimation of the high-rank channel parameters, we are not aware
of any work that deals with the problem of estimation in a fading channel,
which essentially leads to a multiplicative noise environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0100</identifier>
 <datestamp>2012-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0100</id><created>2012-03-01</created><updated>2012-04-07</updated><authors><author><keyname>Ianovski</keyname><forenames>Egor</forenames></author></authors><title>Cake Cutting Mechanisms</title><categories>cs.GT</categories><comments>Honours dissertation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We examine the history of cake cutting mechanisms and discuss the efficiency
of their allocations. In the case of piecewise uniform preferences, we define a
game that in the presence of strategic agents has equilibria that are not
dominated by the allocations of any mechanism. We identify that the equilibria
of this game coincide with the allocations of an existing cake cutting
mechanism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0103</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0103</id><created>2012-03-01</created><updated>2015-07-25</updated><authors><author><keyname>Japaridze</keyname><forenames>Giorgi</forenames><affiliation>Shandong University and Villanova University</affiliation></author></authors><title>On the system CL12 of computability logic</title><categories>cs.LO math.LO</categories><comments>arXiv admin note: substantial text overlap with arXiv:1003.0425 and
  arXiv:1003.4719</comments><proxy>LMCS</proxy><journal-ref>LMCS 11 (3:1) 2015</journal-ref><doi>10.2168/LMCS-11(3:1)2015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computability logic (see http://www.csc.villanova.edu/~japaridz/CL/) is a
long-term project for redeveloping logic on the basis of a constructive game
semantics, with games seen as abstract models of interactive computational
problems. Among the fragments of this logic successfully axiomatized so far is
CL12 --- a conservative extension of classical first-order logic, whose
language augments that of classical logic with the so called choice sorts of
quantifiers and connectives. This system has already found fruitful
applications as a logical basis for constructive and complexity-oriented
versions of Peano arithmetic, such as arithmetics for polynomial time
computability, polynomial space computability, and beyond. The present paper
introduces a third, indispensable complexity measure for interactive
computations termed amplitude complexity, and establishes the adequacy of CL12
with respect to A-amplitude, S-space and T-time computability under certain
minimal conditions on the triples (A,S,T) of function classes. This result very
substantially broadens the potential application areas of CL12. The paper is
self-contained, and targets readers with no prior familiarity with the subject.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0113</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0113</id><created>2012-03-01</created><updated>2015-03-14</updated><authors><author><keyname>Lin</keyname><forenames>Tianrong</forenames></author></authors><title>On equivalence and emptiness problems of multi-letter (measure many)
  quantum finite automata</title><categories>cs.CC cs.FL</categories><acm-class>F.4.1</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper, we study some decision problems both for {\it multi-letter
quantum finite automata} and {\it measure many multi-letter quantum finite
automata}. We first show that given a $k_1$-letter quantum finite automaton
$\mathcal{A}_1$ and a $k_2$-letter quantum finite automaton $\mathcal{A}_2$
over the same input alphabet $\Sigma$, they are equivalent if and only if they
are $(n_1^2+n_2^2-1)|\Sigma|^{k-1}+k$-equivalent where $n_i$, $i=1,2$, are the
number of states in $\mathcal{A}_i$ respectively, and $k=\max\{k_1,k_2\}$. By
applying a method, due to the author, used to deal with the equivalence problem
of {\it measure many one-way quantum finite automata}, we also show that a
$k_1$-letter measure many quantum finite automaton $\mathcal{A}_1$ and a
$k_2$-letter measure many quantum finite automaton $\mathcal{A}_2$ are
equivalent if and only if they are $(n_1^2+n_2^2-1)|\Sigma|^{k-1}+k$-equivalent
where $n_i$, $i=1,2$, are the number of states in $\mathcal{A}_i$ respectively,
and $k=\max\{k_1,k_2\}$.
  Next, we study the emptiness problem of those two kinds of quantum finite
automata. We show that whether the language recognized by a $k$-letter quantum
finite automaton with non-strict cut-point is empty is undecidable, but we
leave open the emptiness of language reorganized by a $k$-letter quantum finite
automaton with strict cutpoint. We also show that whether the languages
recognized by a $k$-letter measure many quantum finite automaton with both
nonstrict and strict cutpoints are undecidable. And the direct consequences of
the above outcomes are summarized in the paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0115</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0115</id><created>2012-03-01</created><updated>2012-03-03</updated><authors><author><keyname>Wilken</keyname><forenames>Gunnar</forenames><affiliation>Okinawa Institute of Science and Technology, Mathematical Biology Unit, Japan</affiliation></author><author><keyname>Weiermann</keyname><forenames>Andreas</forenames><affiliation>Ghent University</affiliation></author></authors><title>Derivation Lengths Classification of G\&quot;odel's T Extending Howard's
  Assignment</title><categories>cs.LO math.LO</categories><proxy>LMCS</proxy><acm-class>F.4.1; F.1.3</acm-class><journal-ref>Logical Methods in Computer Science, Volume 8, Issue 1 (March 6,
  2012) lmcs:1073</journal-ref><doi>10.2168/LMCS-8(1:19)2012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let T be Goedel's system of primitive recursive functionals of finite type in
the lambda formulation. We define by constructive means using recursion on
nested multisets a multivalued function I from the set of terms of T into the
set of natural numbers such that if a term a reduces to a term b and if a
natural number I(a) is assigned to a then a natural number I(b) can be assigned
to b such that I(a) is greater than I(b). The construction of I is based on
Howard's 1970 ordinal assignment for T and Weiermann's 1996 treatment of T in
the combinatory logic version. As a corollary we obtain an optimal derivation
length classification for the lambda formulation of T and its fragments.
Compared with Weiermann's 1996 exposition this article yields solutions to
several non-trivial problems arising from dealing with lambda terms instead of
combinatory logic terms. It is expected that the methods developed here can be
applied to other higher order rewrite systems resulting in new powerful
termination orderings since T is a paradigm for such systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0120</identifier>
 <datestamp>2012-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0120</id><created>2012-03-01</created><authors><author><keyname>Pal</keyname><forenames>Mita</forenames></author><author><keyname>Chakraborty</keyname><forenames>Soubhik</forenames></author><author><keyname>Mahanti</keyname><forenames>N. C.</forenames></author></authors><title>How does the Shift-insertion sort behave when the sorting elements
  follow a Normal distribution?</title><categories>cs.DS</categories><comments>6 pages</comments><journal-ref>Ann. Univ. Tibiscus Comp. Sci. Series VIII/2 (2010), 93-98</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The present paper examines the behavior of Shift-insertion sort (insertion
sort with shifting) for normal distribution inputs and is in continuation of
our earlier work on this new algorithm for discrete distribution inputs,
namely, negative binomial. Shift insertion sort is found more sensitive for
main effects but not for all interaction effects compared to conventional
insertion sort.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0135</identifier>
 <datestamp>2012-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0135</id><created>2012-03-01</created><authors><author><keyname>Dayama</keyname><forenames>Pankaj</forenames></author><author><keyname>Karnik</keyname><forenames>Aditya</forenames></author><author><keyname>Narahari</keyname><forenames>Y.</forenames></author></authors><title>Optimal Mix of Incentive Strategies for Product Marketing on Social
  Networks</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of devising incentive strategies for viral marketing
of a product. In particular, we assume that the seller can influence
penetration of the product by offering two incentive programs: a) direct
incentives to potential buyers (influence) and b) referral rewards for
customers who influence potential buyers to make the purchase (exploit
connections). The problem is to determine the optimal timing of these programs
over a finite time horizon. In contrast to algorithmic perspective popular in
the literature, we take a mean-field approach and formulate the problem as a
continuous-time deterministic optimal control problem. We show that the optimal
strategy for the seller has a simple structure and can take both forms, namely,
influence-and-exploit and exploit-and-influence. We also show that in some
cases it may optimal for the seller to deploy incentive programs mostly for low
degree nodes. We support our theoretical results through numerical studies and
provide practical insights by analyzing various scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0145</identifier>
 <datestamp>2012-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0145</id><created>2012-03-01</created><updated>2012-09-27</updated><authors><author><keyname>Graben</keyname><forenames>Peter beim</forenames></author></authors><title>The Horse Raced Past: Gardenpath Processing in Dynamical Systems</title><categories>cs.CL</categories><comments>The paper has been withdrawn by the author due to some
  inconsistencies in the argumentation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  I pinpoint an interesting similarity between a recent account to rational
parsing and the treatment of sequential decisions problems in a dynamical
systems approach. I argue that expectation-driven search heuristics aiming at
fast computation resembles a high-risk decision strategy in favor of large
transition velocities. Hale's rational parser, combining generalized
left-corner parsing with informed $\mathrm{A}^*$ search to resolve processing
conflicts, explains gardenpath effects in natural sentence processing by
misleading estimates of future processing costs that are to be minimized. On
the other hand, minimizing the duration of cognitive computations in
time-continuous dynamical systems can be described by combining vector space
representations of cognitive states by means of filler/role decompositions and
subsequent tensor product representations with the paradigm of stable
heteroclinic sequences. Maximizing transition velocities according to a
high-risk decision strategy could account for a fast race even between states
that are apparently remote in representation space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0146</identifier>
 <datestamp>2012-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0146</id><created>2012-03-01</created><authors><author><keyname>Bass</keyname><forenames>Richard F.</forenames></author><author><keyname>Gr&#xf6;chenig</keyname><forenames>Karlheinz</forenames></author></authors><title>Relevant Sampling of Band-limited Functions</title><categories>math.PR cs.IT math.IT</categories><msc-class>94A20, 42C15, 60E15, 62M30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the random sampling of band-limited functions of several variables.
If a bandlimited function with bandwidth one has its essential support on a
cube of volume $R^d$, then $\cO (R^d \log R^d)$ random samples suffice to
approximate the function up to a given error with high probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0160</identifier>
 <datestamp>2012-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0160</id><created>2012-03-01</created><updated>2012-03-02</updated><authors><author><keyname>Bu</keyname><forenames>Yingyi</forenames></author><author><keyname>Borkar</keyname><forenames>Vinayak</forenames></author><author><keyname>Carey</keyname><forenames>Michael J.</forenames></author><author><keyname>Rosen</keyname><forenames>Joshua</forenames></author><author><keyname>Polyzotis</keyname><forenames>Neoklis</forenames></author><author><keyname>Condie</keyname><forenames>Tyson</forenames></author><author><keyname>Weimer</keyname><forenames>Markus</forenames></author><author><keyname>Ramakrishnan</keyname><forenames>Raghu</forenames></author></authors><title>Scaling Datalog for Machine Learning on Big Data</title><categories>cs.DB cs.LG cs.PF</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present the case for a declarative foundation for
data-intensive machine learning systems. Instead of creating a new system for
each specific flavor of machine learning task, or hardcoding new optimizations,
we argue for the use of recursive queries to program a variety of machine
learning systems. By taking this approach, database query optimization
techniques can be utilized to identify effective execution plans, and the
resulting runtime plans can be executed on a single unified data-parallel query
processing engine. As a proof of concept, we consider two programming
models--Pregel and Iterative Map-Reduce-Update---from the machine learning
domain, and show how they can be captured in Datalog, tuned for a specific
task, and then compiled into an optimized physical plan. Experiments performed
on a large computing cluster with real data demonstrate that this declarative
approach can provide very good performance while offering both increased
generality and programming ease.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0197</identifier>
 <datestamp>2012-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0197</id><created>2012-03-01</created><updated>2012-03-06</updated><authors><author><keyname>Raghavendra</keyname><forenames>G. S.</forenames></author><author><keyname>Kumar</keyname><forenames>N. Prasanna</forenames></author></authors><title>Statistical Approach for Selecting Elite Ants</title><categories>cs.NE</categories><comments>22 pages</comments><journal-ref>Ann. Univ. Tibiscus Comp. Sci. Series IX/2 (2011), 69-90</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Applications of ACO algorithms to obtain better solutions for combinatorial
optimization problems have become very popular in recent years. In ACO
algorithms, group of agents repeatedly perform well defined actions and
collaborate with other ants in order to accomplish the defined task. In this
paper, we introduce new mechanisms for selecting the Elite ants dynamically
based on simple statistical tools. We also investigate the performance of newly
proposed mechanisms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0200</identifier>
 <datestamp>2012-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0200</id><created>2012-03-01</created><updated>2012-03-06</updated><authors><author><keyname>Rajini</keyname><forenames>S. Nirmala Sugirtha</forenames></author><author><keyname>Bhuvaneswari</keyname><forenames>T.</forenames></author></authors><title>An Interface using SOA Framework For Mediclaim Provider</title><categories>cs.SE</categories><comments>8 pages</comments><journal-ref>Ann. Univ. Tibiscus Comp. Sci. Series IX/2 (2011), 9-16</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  SOA brought new opportunities for the long expected agility, reuse and the
adaptive capability of information technology to the ever changing business
requirements and environments. The purpose of this paper is to describe the
implementation of Medical Insurance Claim Process Model using SOA. We adopt
Service Oriented Architecture (SOA) to reduce the complexity among systems and
solve data consistency problems among services. We choose n-tier and
Service-Oriented Architecture (SOA) as our system environment. This model can
also establish a potentially new innovative market branch for the insurance
industry.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0202</identifier>
 <datestamp>2012-03-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0202</id><created>2012-03-01</created><updated>2012-03-22</updated><authors><author><keyname>Kissinger</keyname><forenames>Aleks</forenames></author></authors><title>Pictures of Processes: Automated Graph Rewriting for Monoidal Categories
  and Applications to Quantum Computing</title><categories>math.CT cs.AI quant-ph</categories><comments>PhD Thesis. Passed examination. Minor corrections made and one
  theorem added at the end of Chapter 5. 182 pages, ~300 figures. See full text
  for unabridged abstract</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work is about diagrammatic languages, how they can be represented, and
what they in turn can be used to represent. More specifically, it focuses on
representations and applications of string diagrams. String diagrams are used
to represent a collection of processes, depicted as &quot;boxes&quot; with multiple
(typed) inputs and outputs, depicted as &quot;wires&quot;. If we allow plugging input and
output wires together, we can intuitively represent complex compositions of
processes, formalised as morphisms in a monoidal category.
  [...] The first major contribution of this dissertation is the introduction
of a discretised version of a string diagram called a string graph. String
graphs form a partial adhesive category, so they can be manipulated using
double-pushout graph rewriting. Furthermore, we show how string graphs modulo a
rewrite system can be used to construct free symmetric traced and compact
closed categories on a monoidal signature.
  The second contribution is in the application of graphical languages to
quantum information theory. We use a mixture of diagrammatic and algebraic
techniques to prove a new classification result for strongly complementary
observables. [...] We also introduce a graphical language for multipartite
entanglement and illustrate a simple graphical axiom that distinguishes the two
maximally-entangled tripartite qubit states: GHZ and W. [...]
  The third contribution is a description of two software tools developed in
part by the author to implement much of the theoretical content described here.
The first tool is Quantomatic, a desktop application for building string graphs
and graphical theories, as well as performing automated graph rewriting
visually. The second is QuantoCoSy, which performs fully automated,
model-driven theory creation using a procedure called conjecture synthesis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0203</identifier>
 <datestamp>2012-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0203</id><created>2012-02-29</created><authors><author><keyname>Dulac-Arnold</keyname><forenames>Gabriel</forenames></author><author><keyname>Denoyer</keyname><forenames>Ludovic</forenames></author><author><keyname>Preux</keyname><forenames>Philippe</forenames></author><author><keyname>Gallinari</keyname><forenames>Patrick</forenames></author></authors><title>Fast Reinforcement Learning with Large Action Sets using
  Error-Correcting Output Codes for MDP Factorization</title><categories>cs.LG stat.ML</categories><msc-class>68T05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of Reinforcement Learning in real-world scenarios is strongly limited
by issues of scale. Most RL learning algorithms are unable to deal with
problems composed of hundreds or sometimes even dozens of possible actions, and
therefore cannot be applied to many real-world problems. We consider the RL
problem in the supervised classification framework where the optimal policy is
obtained through a multiclass classifier, the set of classes being the set of
actions of the problem. We introduce error-correcting output codes (ECOCs) in
this setting and propose two new methods for reducing complexity when using
rollouts-based approaches. The first method consists in using an ECOC-based
classifier as the multiclass classifier, reducing the learning complexity from
O(A2) to O(Alog(A)). We then propose a novel method that profits from the
ECOC's coding dictionary to split the initial MDP into O(log(A)) seperate
two-action MDPs. This second method reduces learning complexity even further,
from O(A2) to O(log(A)), thus rendering problems with large action sets
tractable. We finish by experimentally demonstrating the advantages of our
approach on a set of benchmark problems, both in speed and performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0220</identifier>
 <datestamp>2012-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0220</id><created>2012-03-01</created><authors><author><keyname>Gabbay</keyname><forenames>Dov M.</forenames></author></authors><title>The Equational Approach to CF2 Semantics</title><categories>cs.AI cs.LO</categories><comments>36 pages, version dated 15 February 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a family of new equational semantics for argumentation networks
which can handle odd and even loops in a uniform manner. We offer one version
of equational semantics which is equivalent to CF2 semantics, and a better
version which gives the same results as traditional Dung semantics for even
loops but can still handle odd loops.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0222</identifier>
 <datestamp>2012-11-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0222</id><created>2012-03-01</created><updated>2012-08-11</updated><authors><author><keyname>Lemmen</keyname><forenames>Carsten</forenames></author><author><keyname>Wirtz</keyname><forenames>Kai W.</forenames></author></authors><title>On the sensitivity of the simulated European Neolithic transition to
  climate extremes</title><categories>q-bio.PE cs.MA math.DS physics.geo-ph</categories><comments>Revised version submitted to the Journal of Archaeological Science,
  special issue on The World Reshaped: impacts of the Neolithic transition. 10
  pages, 4 figures, 1 table + supplementary material</comments><doi>10.1016/j.jas.2012.10.023</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Was the spread of agropastoralism from the Fertile Crescent throughout Europe
influenced by extreme climate events, or was it independent of climate? We here
generate idealized climate events using palaeoclimate records. In a
mathematical model of regional sociocultural development, these events disturb
the subsistence base of simulated forager and farmer societies. We evaluate the
regional simulated transition timings and durations against a published large
set of radiocarbon dates for western Eurasia; the model is able to
realistically hindcast much of the inhomogeneous space-time evolution of
regional Neolithic transitions. Our study shows that the consideration of
climate events improves the simulation of typical lags between cultural
complexes, but that the overall difference to a model without climate events is
not significant. Climate events may not have been as important for early
sociocultural dynamics as endogenous factors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0224</identifier>
 <datestamp>2012-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0224</id><created>2012-03-01</created><updated>2012-03-05</updated><authors><author><keyname>Dinitz</keyname><forenames>Michael</forenames></author><author><keyname>Kortsarz</keyname><forenames>Guy</forenames></author><author><keyname>Raz</keyname><forenames>Ran</forenames></author></authors><title>Label Cover instances with large girth and the hardness of approximating
  basic k-spanner</title><categories>cs.DS cs.CC</categories><comments>16 pages, revised to add a reference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the well-known Label Cover problem under the additional requirement
that problem instances have large girth. We show that if the girth is some $k$,
the problem is roughly $2^{\log^{1-\epsilon} n/k}$ hard to approximate for all
constant $\epsilon &gt; 0$. A similar theorem was claimed by Elkin and Peleg
[ICALP 2000], but their proof was later found to have a fundamental error. We
use the new proof to show inapproximability for the basic $k$-spanner problem,
which is both the simplest problem in graph spanners and one of the few for
which super-logarithmic hardness was not known. Assuming $NP \not\subseteq
BPTIME(2^{polylog(n)})$, we show that for every $k \geq 3$ and every constant
$\epsilon &gt; 0$ it is hard to approximate the basic $k$-spanner problem within a
factor better than $2^{(\log^{1-\epsilon} n) / k}$ (for large enough $n$). A
similar hardness for basic $k$-spanner was claimed by Elkin and Peleg [ICALP
2000], but the error in their analysis of Label Cover made this proof fail as
well. Thus for the problem of Label Cover with large girth we give the first
non-trivial lower bound. For the basic $k$-spanner problem we improve the
previous best lower bound of $\Omega(\log n)/k$ by Kortsarz [Algorithmica
1998]. Our main technique is subsampling the edges of 2-query PCPs, which
allows us to reduce the degree of a PCP to be essentially equal to the
soundness desired. This turns out to be enough to essentially guarantee large
girth.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0231</identifier>
 <datestamp>2012-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0231</id><created>2012-03-01</created><authors><author><keyname>Bhattasali</keyname><forenames>Tapalina</forenames></author><author><keyname>Chaki</keyname><forenames>Rituparna</forenames></author><author><keyname>Sanyal</keyname><forenames>Sugata</forenames></author></authors><title>Sleep Deprivation Attack Detection in Wireless Sensor Network</title><categories>cs.NI</categories><comments>7 pages,4 figures, IJCA Journal February 2012</comments><journal-ref>International Journal of Computer Applications 40(15):19-25,
  February 2012. Published by Foundation of Computer Science, New York, USA</journal-ref><doi>10.5120/5056-7374 10.5120/5056-7374 10.5120/5056-7374 10.5120/5056-7374</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deployment of sensor network in hostile environment makes it mainly
vulnerable to battery drainage attacks because it is impossible to recharge or
replace the battery power of sensor nodes. Among different types of security
threats, low power sensor nodes are immensely affected by the attacks which
cause random drainage of the energy level of sensors, leading to death of the
nodes. The most dangerous type of attack in this category is sleep deprivation,
where target of the intruder is to maximize the power consumption of sensor
nodes, so that their lifetime is minimized. Most of the existing works on sleep
deprivation attack detection involve a lot of overhead, leading to poor
throughput. The need of the day is to design a model for detecting intrusions
accurately in an energy efficient manner. This paper proposes a hierarchical
framework based on distributed collaborative mechanism for detecting sleep
deprivation torture in wireless sensor network efficiently. Proposed model uses
anomaly detection technique in two steps to reduce the probability of false
intrusion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0240</identifier>
 <datestamp>2012-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0240</id><created>2012-03-01</created><authors><author><keyname>Bhattasali</keyname><forenames>Tapalina</forenames></author><author><keyname>Chaki</keyname><forenames>Rituparna</forenames></author></authors><title>A Survey of Recent Intrusion Detection Systems for Wireless Sensor
  Network</title><categories>cs.NI</categories><comments>10 pages,5 figures,proceedings of CNSA 2011 conference held in
  Chennai</comments><journal-ref>Advances in Network Security and Applications: Conference
  Proceedings of Fourth International Conference on Network Security and
  Applications (CNSA 2011), Chennai, July 15-17, 2011, pp. 268-280, ISBN:
  978-3-642-22539-0</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Security of Wireless sensor network (WSN) becomes a very important issue with
the rapid development of WSN that is vulnerable to a wide range of attacks due
to deployment in the hostile environment and having limited resources.
Intrusion detection system is one of the major and efficient defensive methods
against attacks in WSN. A particularly devastating attack is the sleep
deprivation attack, where a malicious node forces legitimate nodes to waste
their energy by resisting the sensor nodes from going into low power sleep
mode. The goal of this attack is to maximize the power consumption of the
target node, thereby decreasing its battery life. Existing works on sleep
deprivation attack have mainly focused on mitigation using MAC based protocols,
such as S-MAC, T-MAC, B-MAC, etc. In this article, a brief review of some of
the recent intrusion detection systems in wireless sensor network environment
is presented. Finally, we propose a framework of cluster based layered
countermeasure that can efficiently mitigate sleep deprivation attack in WSN.
Simulation results on MATLAB exhibit the effectiveness of the proposed model in
detecting sleep-deprivation attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0251</identifier>
 <datestamp>2012-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0251</id><created>2012-03-01</created><authors><author><keyname>Hill</keyname><forenames>Theodore P.</forenames></author><author><keyname>Dall'Aglio</keyname><forenames>Marco</forenames></author></authors><title>Bayesian Posteriors Without Bayes' Theorem</title><categories>math.ST cs.IT math.IT math.PR stat.TH</categories><comments>6 pages, no figures</comments><msc-class>62F15, 94A17, 60A05, 62B10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The classical Bayesian posterior arises naturally as the unique solution of
several different optimization problems, without the necessity of interpreting
data as conditional probabilities and then using Bayes' Theorem. For example,
the classical Bayesian posterior is the unique posterior that minimizes the
loss of Shannon information in combining the prior and the likelihood
distributions. These results, direct corollaries of recent results about
conflations of probability distributions, reinforce the use of Bayesian
posteriors, and may help partially reconcile some of the differences between
classical and Bayesian statistics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0259</identifier>
 <datestamp>2012-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0259</id><created>2012-03-01</created><authors><author><keyname>Alexeev</keyname><forenames>Boris</forenames></author><author><keyname>Jacokes</keyname><forenames>M. Brian</forenames></author></authors><title>A rearrangement step with potential uses in priority queues</title><categories>cs.DS</categories><comments>3 pages, 1 figure</comments><msc-class>68P05</msc-class><acm-class>E.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Link-based data structures, such as linked lists and binary search trees,
have many well-known rearrangement steps allowing for efficient implementations
of insertion, deletion, and other operations. We describe a rearrangement
primitive designed for link-based, heap-ordered priority queues in the
comparison model, such as those similar to Fibonacci heaps or binomial heaps.
  In its most basic form, the primitive rearranges a collection of heap-ordered
perfect binary trees. Doing so offers a data structure control on the number of
trees involved in such a collection, in particular keeping this number
logarithmic in the number of elements. The rearrangement step is free from an
amortized complexity standpoint (using an appropriate potential function).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0265</identifier>
 <datestamp>2012-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0265</id><created>2012-02-29</created><authors><author><keyname>Chitra</keyname><forenames>S.</forenames></author><author><keyname>Bhattacharjee</keyname><forenames>J. B.</forenames></author><author><keyname>Thilakavathi</keyname><forenames>B.</forenames></author></authors><title>Image Fusion and Re-Modified SPIHT for Fused Image</title><categories>cs.CV</categories><comments>16 pages</comments><journal-ref>Ann. Univ. Tibiscus Comp. Sci. Series VII/2 (2009), 143-158</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the Discrete Wavelet based fusion techniques for
combining perceptually important image features. SPIHT (Set Partitioning in
Hierarchical Trees) algorithm is an efficient method for lossy and lossless
coding of fused image. This paper presents some modifications on the SPIHT
algorithm. It is based on the idea of insignificant correlation of wavelet
coefficient among the medium and high frequency sub bands. In RE-MSPIHT
algorithm, wavelet coefficients are scaled prior to SPIHT coding based on the
sub band importance, with the goal of minimizing the MSE.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0289</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0289</id><created>2012-03-01</created><updated>2015-09-27</updated><authors><author><keyname>Dani</keyname><forenames>Varsha</forenames></author><author><keyname>King</keyname><forenames>Valerie</forenames></author><author><keyname>Movahedi</keyname><forenames>Mahnush</forenames></author><author><keyname>Saia</keyname><forenames>Jared</forenames></author><author><keyname>Zamani</keyname><forenames>Mahdi</forenames></author></authors><title>Secure Multi-Party Computation in Large Networks</title><categories>cs.DS</categories><comments>51 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe scalable protocols for solving the secure multi-party computation
(MPC) problem among a large number of parties. We consider both the synchronous
and the asynchronous communication models. In the synchronous setting, our
protocol is secure against a static malicious adversary corrupting less than a
$1/3$ fraction of the parties. In the asynchronous setting, we allow the
adversary to corrupt less than a $1/8$ fraction of parties. For any
deterministic function that can be computed by an arithmetic circuit with $m$
gates, both of our protocols require each party to send a number of field
elements and perform an amount of computation that is $\tilde{O}(m/n + \sqrt
n)$. We also show that our protocols provide perfect and universally-composable
security.
  To achieve our asynchronous MPC result, we define the \emph{threshold
counting problem} and present a distributed protocol to solve it in the
asynchronous setting. This protocol is load balanced, with computation,
communication and latency complexity of $O(\log{n})$, and can also be used for
designing other load-balanced applications in the asynchronous communication
model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0290</identifier>
 <datestamp>2012-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0290</id><created>2012-03-01</created><updated>2012-03-26</updated><authors><author><keyname>Kaipa</keyname><forenames>K. V.</forenames></author><author><keyname>Pillai</keyname><forenames>H.</forenames></author></authors><title>Weight spectrum of codes associated with the Grassmannian G(3,7)</title><categories>cs.IT math.IT</categories><comments>concluding remarks added</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider the problem of determining the weight spectrum of
q-ary codes C(3,m) associated with Grassmann varieties G(3,m). For m=6 this was
done by Nogin. We derive a formula for the weight of a codeword of C(3,m), in
terms of certain varieties associated with alternating trilinear forms on
(F_q)^m. The classification of such forms under the action of the general
linear group GL(m,F_q) is the other component that is required to calculate the
spectrum of C(3,m). For m=7, we explicitly determine the varieties mentioned
above. The classification problem for alternating 3-forms on (F_q)^7 was solved
by Cohen and Helminck, which we then use to determine the spectrum of C(3,7).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0298</identifier>
 <datestamp>2012-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0298</id><created>2012-03-01</created><updated>2012-03-06</updated><authors><author><keyname>Aruna</keyname><forenames>S.</forenames></author><author><keyname>Rajagopalan</keyname><forenames>S. P.</forenames></author><author><keyname>Nandakishore</keyname><forenames>L. V.</forenames></author></authors><title>Application of Gist SVM in Cancer Detection</title><categories>cs.LG</categories><comments>10 pages</comments><journal-ref>Ann. Univ. Tibiscus Comp. Sci. Series IX/2 (2011), 39-48</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the application of GIST SVM in disease prediction
(detection of cancer). Pattern classification problems can be effectively
solved by Support vector machines. Here we propose a classifier which can
differentiate patients having benign and malignant cancer cells. To improve the
accuracy of classification, we propose to determine the optimal size of the
training set and perform feature selection. To find the optimal size of the
training set, different sizes of training sets are experimented and the one
with highest classification rate is selected. The optimal features are selected
through their F-Scores.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0321</identifier>
 <datestamp>2012-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0321</id><created>2012-03-01</created><authors><author><keyname>Drost</keyname><forenames>Niels</forenames></author><author><keyname>Maassen</keyname><forenames>Jason</forenames></author><author><keyname>van Meersbergen</keyname><forenames>Maarten A. J.</forenames></author><author><keyname>Bal</keyname><forenames>Henri E.</forenames></author><author><keyname>Pelupessy</keyname><forenames>F. Inti</forenames></author><author><keyname>Zwart</keyname><forenames>Simon Portegies</forenames></author><author><keyname>Kliphuis</keyname><forenames>Michael</forenames></author><author><keyname>Dijkstra</keyname><forenames>Henk A.</forenames></author><author><keyname>Seinstra</keyname><forenames>Frank J.</forenames></author></authors><title>High-Performance Distributed Multi-Model / Multi-Kernel Simulations: A
  Case-Study in Jungle Computing</title><categories>cs.DC astro-ph.SR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  High-performance scientific applications require more and more compute power.
The concurrent use of multiple distributed compute resources is vital for
making scientific progress. The resulting distributed system, a so-called
Jungle Computing System, is both highly heterogeneous and hierarchical,
potentially consisting of grids, clouds, stand-alone machines, clusters,
desktop grids, mobile devices, and supercomputers, possibly with accelerators
such as GPUs.
  One striking example of applications that can benefit greatly of Jungle
Computing Systems are Multi-Model / Multi-Kernel simulations. In these
simulations, multiple models, possibly implemented using different techniques
and programming models, are coupled into a single simulation of a physical
system. Examples include the domain of computational astrophysics and climate
modeling.
  In this paper we investigate the use of Jungle Computing Systems for such
Multi-Model / Multi-Kernel simulations. We make use of the software developed
in the Ibis project, which addresses many of the problems faced when running
applications on Jungle Computing Systems. We create a prototype Jungle-aware
version of AMUSE, an astrophysical simulation framework. We show preliminary
experiments with the resulting system, using clusters, grids, stand-alone
machines, and GPUs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0332</identifier>
 <datestamp>2012-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0332</id><created>2012-03-01</created><authors><author><keyname>Durao</keyname><forenames>Frederico</forenames></author><author><keyname>Dolog</keyname><forenames>Peter</forenames></author></authors><title>A Personalized Tag-Based Recommendation in Social Web Systems</title><categories>cs.SI cs.IR</categories><journal-ref>Original language English, Journal CEUR Workshop Proceedings,
  Publication date 2009, Volume 485, Pages 40-49, ISSN 1613-0073</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Tagging activity has been recently identified as a potential source of
knowledge about personal interests, preferences, goals, and other attributes
known from user models. Tags themselves can be therefore used for finding
personalized recommendations of items. In this paper, we present a tag-based
recommender system which suggests similar Web pages based on the similarity of
their tags from a Web 2.0 tagging application. The proposed approach extends
the basic similarity calculus with external factors such as tag popularity, tag
representativeness and the affinity between user and tag. In order to study and
evaluate the recommender system, we have conducted an experiment involving 38
people from 12 countries using data from Del.icio.us, a social bookmarking web
system on which users can share their personal bookmarks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0333</identifier>
 <datestamp>2012-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0333</id><created>2012-03-01</created><authors><author><keyname>Esposito</keyname><forenames>Alfredo</forenames></author></authors><title>Debunking some myths about biometric authentication</title><categories>cs.CR</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Biometric authentication systems are presented as the best way to reach high
security levels in controlling access to IT systems or sensitive
infrastructures. But several issues are often not taken properly into account.
In order for the implementation of those systems to be successful, the hidden
risks and the related liabilities have to be carefully analyzed before
biometrics can be used on a large scale for sensitive applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0353</identifier>
 <datestamp>2012-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0353</id><created>2012-03-01</created><authors><author><keyname>Roth</keyname><forenames>Aaron</forenames></author><author><keyname>Schoenebeck</keyname><forenames>Grant</forenames></author></authors><title>Conducting Truthful Surveys, Cheaply</title><categories>cs.GT cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of conducting a survey with the goal of obtaining an
unbiased estimator of some population statistic when individuals have unknown
costs (drawn from a known prior) for participating in the survey. Individuals
must be compensated for their participation and are strategic agents, and so
the payment scheme must incentivize truthful behavior. We derive optimal
truthful mechanisms for this problem for the two goals of minimizing the
variance of the estimator given a fixed budget, and minimizing the expected
cost of the survey given a fixed variance goal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0369</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0369</id><created>2012-03-01</created><authors><author><keyname>Chowdhury</keyname><forenames>Abhijit</forenames><affiliation>NSHM College of Management &amp; Technology, Durgapur West Bengal, INDIA</affiliation></author><author><keyname>Sinha</keyname><forenames>Angshu Kumar</forenames><affiliation>NSHM College of Management &amp; Technology, Durgapur West Bengal, INDIA</affiliation></author><author><keyname>Dutta</keyname><forenames>Saurabh</forenames><affiliation>Dr. B.C Roy Engineering College West Bengal, INDIA</affiliation></author></authors><title>Introduction of a Triple Prime Symmetric Key Block Cipher</title><categories>cs.CR</categories><msc-class>94A60</msc-class><journal-ref>International Journal of Computer Applications (0975 - 8887)
  Volume 39 - No.7, February 2012</journal-ref><doi>10.5120/4831-7089</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes to put forward an innovative algorithm for symmetric key
block cipher named as &quot;Triple Prime Symmetric Key Block Cipher with Variable
Key-Spaces (TPSKBCVK)&quot; that employs triple prime integers as private key-spaces
of varying lengths to encrypt data files. Principles of modular arithmetic have
been elegantly used in the proposed idea of the cipher. Depending on
observations of the results of implementation of the proposed cipher on a set
of real data files of several types, all results are registered and analyzed.
The strength of the underlying design of the cipher and the liberty of using a
long key-space expectedly makes it reasonably non-susceptible against possible
cryptanalytic intrusions. As a future scope of the work, it is intended to
formulate and employ an improved scheme that will use a carrier media (image or
multimedia data file) for a secure transmission of the private keys.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0400</identifier>
 <datestamp>2012-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0400</id><created>2012-03-02</created><authors><author><keyname>Monfort</keyname><forenames>Valerie</forenames></author><author><keyname>Cherif</keyname><forenames>Sihem</forenames></author></authors><title>Bridging the Gap between Technical Heterogeneity of Context-Aware
  Platforms: Experimenting a Service Based Connectivity between Adaptable
  Android, WComp and OpenORB</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many companies include in their Information Systems (IS) several
communicating heterogeneous middleware according to their technical needs. The
need is the same when IS requires using context aware platforms for different
aims. Moreover, users may be mobile and want to receive and send services with
their PDA that more often supports Android based Human Man Interface. In this
paper, we show how we extend Android to make it adaptable and open. We also
present how we communicate between different heterogeneous context aware
platforms as WComp and OpenORB by using Android and Web Services. We introduce
a concrete case study to explain our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0411</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0411</id><created>2012-03-02</created><authors><author><keyname>Hemaspaandra</keyname><forenames>Edith</forenames></author><author><keyname>Hemaspaandra</keyname><forenames>Lane A.</forenames></author><author><keyname>Rothe</keyname><forenames>Joerg</forenames></author></authors><title>Online Voter Control in Sequential Elections</title><categories>cs.GT cs.CC cs.MA</categories><comments>14 pages</comments><report-no>URCS-TR-2012-976</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Previous work on voter control, which refers to situations where a chair
seeks to change the outcome of an election by deleting, adding, or partitioning
voters, takes for granted that the chair knows all the voters' preferences and
that all votes are cast simultaneously. However, elections are often held
sequentially and the chair thus knows only the previously cast votes and not
the future ones, yet needs to decide instantaneously which control action to
take. We introduce a framework that models \emph{online voter control in
sequential elections}. We show that the related problems can be much harder
than in the standard (non-online) case: For certain election systems, even with
efficient winner problems, online control by deleting, adding, or partitioning
voters is PSPACE-complete, even if there are only two candidates. In addition,
we obtain completeness for coNP in the deleting/adding cases with a bounded
deletion/addition limit, and for NP in the partition cases with only one
candidate. Finally, we show that for plurality, online control by deleting or
adding voters is in P, and for partitioning voters is coNP-hard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0415</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0415</id><created>2012-03-02</created><authors><author><keyname>Blech</keyname><forenames>Jan Olaf</forenames></author></authors><title>On Compositional Reasoning for Guaranteeing Probabilistic Properties</title><categories>cs.SE cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a framework to formally describe probabilistic system behavior and
symbolically reason about it. In particular we aim at reasoning about possible
failures and fault tolerance. We regard systems which are composed of different
units: sensors, computational parts and actuators. Considering worst-case
failure behavior of system components, our framework is most suited to derive
reliability guarantees for composed systems. The behavior of system components
is modeled using monad like constructs that serve as an abstract representation
for system behavior. We introduce rules to reason about these representations
and derive results like guaranteed upper bounds for system failure. Our
approach is characterized by the fact that we do not just map a certain
component to a failure probability, but regard distributions of error behavior
and their evolvement over system runs. This serves as basis for deriving
probabilities of events, in particular failure probabilities. The work
presented in this paper slightly extends a complete framework and a case study
which has been previously published. One focus of this report is a more
detailed explanation of definitions and a more comprehensive description of
examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0429</identifier>
 <datestamp>2012-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0429</id><created>2012-03-02</created><authors><author><keyname>Dimitrakos</keyname><forenames>Theo</forenames></author><author><keyname>Brossard</keyname><forenames>David</forenames></author><author><keyname>de Leusse</keyname><forenames>Pierre</forenames></author></authors><title>Securing business operations in an SOA</title><categories>cs.DC</categories><journal-ref>BT Technology Journal, vol.27, no.2, 2009</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Service-oriented infrastructures pose new challenges in a number of areas,
notably with regard to security and dependability. BT has developed a
combination of innovative security solutions and governance frameworks that can
address these challenges. They include advances in identity federation;
distributed usage and access management; context-aware secure messaging,
routing and transformation; and (security) policy governance for
service-oriented architectures. This paper discusses these developments and the
steps being taken to validate their functionality and performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0432</identifier>
 <datestamp>2012-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0432</id><created>2012-03-02</created><authors><author><keyname>de Leusse</keyname><forenames>Pierre</forenames></author><author><keyname>Zielinski</keyname><forenames>Krzysztof</forenames></author></authors><title>Toward Governance of Cross-Cloud Application Deployment</title><categories>cs.DC</categories><journal-ref>ServiceWave 2011, Second Optimising Cloud Services Workshop</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, the authors introduce the main ideas around the governance
of cross-Cloud application deployment and their related concepts. It is argued
that, due to the increasing complexity and nature of the Cloud market, an
intermediary specialized in brokering the deployment of different components of
a same application onto different Cloud products could both facilitate said
deployment and in some cases improve its quality in terms of cost, security &amp;
reliability and QoS. In order to fulfill these objectives, the authors propose
a high level architecture that relies on their previous work on governance of
policy &amp; rule driven distributed systems. This architecture aims at supplying
five main functions of 1) translation of Service Level Agreements (SLAs) and
pricing into a common shared DSL, 2) correlation of analytical data (e.g.
monitoring, metering), 3) combination of Cloud products, 4) information from
third parties regarding different aspects of Quality of Service (QoS) and 5)
cross-Cloud application deployment specification and governance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0434</identifier>
 <datestamp>2012-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0434</id><created>2012-03-02</created><authors><author><keyname>Chakravarthy</keyname><forenames>Katuru SM Kalyana</forenames></author></authors><title>&quot;openness of search engine&quot;: A critical flaw in search systems; a case
  study on google, yahoo and bing</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is no doubt that Search Engines are playing a great role in Internet
usage. But all the top search engines Google, Yahoo and Bing are having a
critical flaw called &quot;Openness of a Search Engine&quot;. An Internet user should be
allowed to get the search results only when requested through Search engine's
web page but the user must not be allowed to get the search results when
requested through any web page that does not belong to the Search Engine. Only
results of a search engine should be available to the Internet user but not the
Search Engine. This paper explains the critical flaw called &quot;Openness of Search
Engine&quot; with a case study on top 3 search engines 'Google', 'Yahoo' and 'Bing'.
This paper conducts an attack based test using J2EE framework and proves that
'Google' passed the test and it strongly protects its Critical Search System,
where 'Yahoo' and 'Bing' are failed to protect their Search Engines. But
previously 'Google' also had other high severity issues with the Openness of
search engine; this paper reveals those issues also. Finally this paper appeals
strongly to the all top Search Engines to fix their critical flaws of &quot;Openness
of Search Engine&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0435</identifier>
 <datestamp>2012-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0435</id><created>2012-03-02</created><authors><author><keyname>de Leusse</keyname><forenames>Pierre</forenames></author><author><keyname>Kwolek</keyname><forenames>Bartosz</forenames></author><author><keyname>Zielinski</keyname><forenames>Krzysztof</forenames></author></authors><title>A common interface for multi-rule-engine distributed systems</title><categories>cs.DC</categories><journal-ref>RuleML-2010 Challenge: 4th international rule challenge : Web rule
  symposium : Washington, DC, USA, October, 21-23, 2010, eds. Monica Palmirani,
  [et al.]</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The rule technological landscape is becoming ever more complex, with an
extended number of specifications and products. It is therefore becoming
increasingly difficult to integrate rule-driven components and manage
interoperability in multi-rule engine environments. The described work presents
the possibility to provide a common interface for rule-driven components in a
distributed system. The authors' approach leverages on a set of discovery
protocol, rule interchange and user interface to alleviate the environment's
complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0436</identifier>
 <datestamp>2013-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0436</id><created>2012-03-02</created><updated>2013-10-14</updated><authors><author><keyname>Arthan</keyname><forenames>Rob</forenames></author><author><keyname>Oliva</keyname><forenames>Paulo</forenames></author></authors><title>(Dual) Hoops Have Unique Halving</title><categories>cs.AI math.LO</categories><comments>17 pages, 5 figures, published as a chapter in the Bill McCune
  Memorial Festschrift</comments><msc-class>03G25, 03B50</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Continuous logic extends the multi-valued Lukasiewicz logic by adding a
halving operator on propositions. This extension is designed to give a more
satisfactory model theory for continuous structures. The semantics of these
logics can be given using specialisations of algebraic structures known as
hoops. As part of an investigation into the metatheory of propositional
continuous logic, we were indebted to Prover9 for finding a proof of an
important algebraic law.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0439</identifier>
 <datestamp>2012-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0439</id><created>2012-03-02</created><authors><author><keyname>de Leusse</keyname><forenames>Pierre</forenames></author><author><keyname>Periorellis</keyname><forenames>Panos</forenames></author><author><keyname>Dimitrakos</keyname><forenames>Theo</forenames></author><author><keyname>Nair</keyname><forenames>Srijith K.</forenames></author></authors><title>Self Managed Security Cell, a security model for the Internet of Things
  and Services</title><categories>cs.CR</categories><journal-ref>The First International Conference on Advances in Future Internet,
  AFIN 2009, IEEE Computer Society, June 18-23, 2009, Athens/Vouliagmeni,
  Greece, Best paper award</journal-ref><doi>10.1109/AFIN.2009.15</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Internet of Things and Services is a rapidly growing concept that
illustrates that the ever increasing amount of physical items of our daily life
which become addressable through a network could be made more easily manageable
and usable through the use of Services. This surge of exposed resources along
with the level of privacy and value of the information they hold, together with
the increase of their usage make for an augmentation in the number of the
security threats and violation attempts that existing security systems do not
appear robust enough to address. In this paper, the authors underline this
increase in risk and identify the requirements for resources to be more
resilient in this type of environment while keeping an important level of
flexibility. In addition, the authors propose an architectural model of Self
Managed Security Cell, which leverages on current knowledge in large scale
security systems, information management and autonomous systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0440</identifier>
 <datestamp>2012-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0440</id><created>2012-03-02</created><authors><author><keyname>de Leusse</keyname><forenames>Pierre</forenames></author><author><keyname>Dimitrakos</keyname><forenames>Theo</forenames></author></authors><title>SOA-based security governance middleware</title><categories>cs.CR</categories><journal-ref>SECURWARE 2010: the fourth international conference on Emerging
  security information, systems and technologies : 18-25 July 2010, Venice,
  Italy, eds. Reijo Savola et al., IEEE, 2010, ISBN-13 978- 0-7695-4095-5</journal-ref><doi>10.1109/SECURWARE.2010.17</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Business requirements for rapid operational efficiency, customer
responsiveness as well as rapid adaptability are actively driving the need for
ever increasing communication and integration apabilities of software assets.
In this context, security, although acknowledged as being a necessity, is often
perceived as a hindrance. Indeed, dynamic environments require flexible and
understandable security that can be customized, adapted and reconfigured
dynamically to face changing requirements. In this paper, the authors propose
SOA based security governance middleware that handles security requirements on
behalf of a resource exposed through it. The middleware aims at providing
different security settings through the use of managed compositions of security
services called profiles. The main added value of this work compared to
existing handlers or centralized approaches lies in its enhanced flexibility
and transparency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0442</identifier>
 <datestamp>2012-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0442</id><created>2012-03-02</created><authors><author><keyname>shen</keyname><forenames>Liyong</forenames></author><author><keyname>Cheng</keyname><forenames>Jin-san</forenames></author><author><keyname>Jia</keyname><forenames>Xiaohong</forenames></author></authors><title>Homeomorphic approximation of the intersection curve of two rational
  surfaces</title><categories>cs.CG math.GT</categories><comments>18 pages,15 figures</comments><acm-class>I.3.5; G.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an approach of computing the intersection curve $\mathcal{C}$ of
two rational parametric surface $\S_1(u,s)$ and $\S_2(v,t)$, one being
projectable and hence can easily be implicitized. Plugging the parametric
surface to the implicit surface yields a plane algebraic curve $G(v,t)=0$. By
analyzing the topology graph $\G$ of $G(v,t)=0$ and the singular points on the
intersection curve $\mathcal{C}$ we associate a space topology graph to
$\mathcal{C}$, which is homeomorphic to $\mathcal{C}$ and therefore leads us to
an approximation for $\mathcal{C}$ in a given precision.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0443</identifier>
 <datestamp>2012-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0443</id><created>2012-03-02</created><authors><author><keyname>de Leusse</keyname><forenames>Pierre</forenames></author><author><keyname>Periorellis</keyname><forenames>Panos</forenames></author><author><keyname>Watson</keyname><forenames>Paul</forenames></author><author><keyname>Maierhofer</keyname><forenames>Andreas</forenames></author></authors><title>Secure &amp; Rapid Composition of Infrastructure Services in the Cloud</title><categories>cs.DC</categories><journal-ref>SENSORCOMM '08. Second International Conference on , vol., no.,
  pp.770-775, 25-31 Aug. 2008</journal-ref><doi>10.1109/SENSORCOMM.2008.130</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A fundamental ambition of grid and distributed systems is to be capable of
sustaining evolution and allowing for adaptability ((F. Losavio et al., 2002),
(S. Radhakrishnan, 2005)). Furthermore, as the complexity and sophistication of
theses structures increases, so does the need for adaptability of each
component. One of the primary benefits of service oriented architecture (SOA)
is the ability to compose applications, processes or more complex services from
other services which increases the capacity for adaptation. This document
proposes a novel infrastructure composition model that aims at increasing the
adaptability of the capabilities exposed through it by dynamically managing
their non functional requirements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0453</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0453</id><created>2012-03-02</created><updated>2013-01-16</updated><authors><author><keyname>Liu</keyname><forenames>Song</forenames></author><author><keyname>Yamada</keyname><forenames>Makoto</forenames></author><author><keyname>Collier</keyname><forenames>Nigel</forenames></author><author><keyname>Sugiyama</keyname><forenames>Masashi</forenames></author></authors><title>Change-Point Detection in Time-Series Data by Relative Density-Ratio
  Estimation</title><categories>stat.ML cs.LG stat.ME</categories><doi>10.1016/j.neunet.2013.01.012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The objective of change-point detection is to discover abrupt property
changes lying behind time-series data. In this paper, we present a novel
statistical change-point detection algorithm based on non-parametric divergence
estimation between time-series samples from two retrospective segments. Our
method uses the relative Pearson divergence as a divergence measure, and it is
accurately and efficiently estimated by a method of direct density-ratio
estimation. Through experiments on artificial and real-world datasets including
human-activity sensing, speech, and Twitter messages, we demonstrate the
usefulness of the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0473</identifier>
 <datestamp>2015-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0473</id><created>2012-03-02</created><authors><author><keyname>Cain</keyname><forenames>Alan J.</forenames></author><author><keyname>Maltcev</keyname><forenames>Victor</forenames></author></authors><title>Finitely presented monoids with linear Dehn function need not have
  regular cross-sections</title><categories>cs.FL math.GR</categories><comments>13 pages; 1 table</comments><msc-class>03D40 (Primary) 20M05, 68Q42 (Secondary)</msc-class><journal-ref>Semigroup Forum, 88, no. 2 (2014), pp. 300--315</journal-ref><doi>10.1007/s00233-013-9531-6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper shows that a finitely presented monoid with linear Dehn function
need not have a regular cross-section, strengthening the previously-known
result that such a monoid need not be presented by a finite complete string
rewriting system, and contrasting the fact that finitely presented groups with
linear Dehn function always have regular cross-sections.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0474</identifier>
 <datestamp>2012-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0474</id><created>2012-03-02</created><authors><author><keyname>Morier-Genoud</keyname><forenames>Sophie</forenames></author><author><keyname>Ovsienko</keyname><forenames>Valentin</forenames></author></authors><title>Orthogonal Designs and a Cubic Binary Function</title><categories>cs.IT math.CO math.IT</categories><msc-class>94C30, 5B30, 11E25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Orthogonal designs are fundamental mathematical notions used in the
construction of space time block codes for wireless transmissions. Designs have
two important parameters, the rate and the decoding delay; the main problem of
the theory is to construct designs maximizing the rate and minimizing the
decoding delay. All known constructions of CODs are inductive or algorithmic.
In this paper, we present an explicit construction of optimal CODs. We do not
apply recurrent procedures and do calculate the matrix elements directly. Our
formula is based on a cubic function in two binary n-vectors. In our previous
work (Comm. Math. Phys., 2010, and J. Pure and Appl. Algebra, 2011), we used
this function to define a series of non-associative algebras generalizing the
classical algebra of octonions and to obtain sum of squares identities of
Hurwitz-Radon type.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0478</identifier>
 <datestamp>2012-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0478</id><created>2012-03-02</created><authors><author><keyname>Shen</keyname><forenames>Liyong</forenames></author><author><keyname>Yuan</keyname><forenames>Chunming</forenames></author><author><keyname>Gao</keyname><forenames>Xiao-Shan</forenames></author></authors><title>Certified Approximation of Parametric Space Curves with Cubic B-spline
  Curves</title><categories>cs.CG math.GT</categories><comments>26 pages, 18 figures</comments><acm-class>I.3.5; G.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Approximating complex curves with simple parametric curves is widely used in
CAGD, CG, and CNC. This paper presents an algorithm to compute a certified
approximation to a given parametric space curve with cubic B-spline curves. By
certified, we mean that the approximation can approximate the given curve to
any given precision and preserve the geometric features of the given curve such
as the topology, singular points, etc. The approximated curve is divided into
segments called quasi-cubic B\'{e}zier curve segments which have properties
similar to a cubic rational B\'{e}zier curve. And the approximate curve is
naturally constructed as the associated cubic rational B\'{e}zier curve of the
control tetrahedron of a quasi-cubic curve. A novel optimization method is
proposed to select proper weights in the cubic rational B\'{e}zier curve to
approximate the given curve. The error of the approximation is controlled by
the size of its tetrahedron, which converges to zero by subdividing the curve
segments. As an application, approximate implicit equations of the approximated
curves can be computed. Experiments show that the method can approximate space
curves of high degrees with high precision and very few cubic B\'{e}zier curve
segments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0488</identifier>
 <datestamp>2012-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0488</id><created>2012-03-02</created><authors><author><keyname>Kong</keyname><forenames>Shu</forenames></author><author><keyname>Wang</keyname><forenames>Donghui</forenames></author></authors><title>Multi-Level Feature Descriptor for Robust Texture Classification via
  Locality-Constrained Collaborative Strategy</title><categories>cs.CV cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a simple but highly efficient ensemble for robust
texture classification, which can effectively deal with translation, scale and
changes of significant viewpoint problems. The proposed method first inherits
the spirit of spatial pyramid matching model (SPM), which is popular for
encoding spatial distribution of local features, but in a flexible way,
partitioning the original image into different levels and incorporating
different overlapping patterns of each level. This flexible setup helps capture
the informative features and produces sufficient local feature codes by some
well-chosen aggregation statistics or pooling operations within each
partitioned region, even when only a few sample images are available for
training. Then each texture image is represented by several orderless feature
codes and thereby all the training data form a reliable feature pond. Finally,
to take full advantage of this feature pond, we develop a collaborative
representation-based strategy with locality constraint (LC-CRC) for the final
classification, and experimental results on three well-known public texture
datasets demonstrate the proposed approach is very competitive and even
outperforms several state-of-the-art methods. Particularly, when only a few
samples of each category are available for training, our approach still
achieves very high classification performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0494</identifier>
 <datestamp>2012-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0494</id><created>2012-03-02</created><authors><author><keyname>Kim</keyname><forenames>Minseong</forenames></author></authors><title>Inconsistency of the Zermelo-Fraenkel set theory with the axiom of
  choice and its effects on the computational complexity</title><categories>cs.LO cs.CC</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper exposes a contradiction in the Zermelo-Fraenkel set theory with
the axiom of choice (ZFC). While Godel's incompleteness theorems state that a
consistent system cannot prove its consistency, they do not eliminate proofs
using a stronger system or methods that are outside the scope of the system.
The paper shows that the cardinalities of infinite sets are uncontrollable and
contradictory. The paper then states that Peano arithmetic, or first-order
arithmetic, is inconsistent if all of the axioms and axiom schema assumed in
the ZFC system are taken as being true, showing that ZFC is inconsistent. The
paper then exposes some consequences that are in the scope of the computational
complexity theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0500</identifier>
 <datestamp>2012-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0500</id><created>2012-03-02</created><authors><author><keyname>Sparavigna</keyname><forenames>A. C.</forenames></author><author><keyname>Marazzato</keyname><forenames>R.</forenames></author></authors><title>Georeferenced lives</title><categories>cs.CY</categories><comments>GIS, Satellite Maps, Google Earth, KML, XML, Acme Mapper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To give a georeference means to give a reference as existing in the physical
space of Earth. This procedure is widely used for the location of
archaeological, historical and other sites when geographic information systems
(GIS) are used. Here we are proposing to georeference the lives of famous
people (in the paper, Newton and Schiaparelli) for teaching purposes, to
increase the appeal of some scientific disciplines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0502</identifier>
 <datestamp>2012-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0502</id><created>2012-03-02</created><updated>2012-10-23</updated><authors><author><keyname>Bauer</keyname><forenames>Frank</forenames></author><author><keyname>Lizier</keyname><forenames>Joseph T.</forenames></author></authors><title>Identifying influential spreaders and efficiently estimating infection
  numbers in epidemic models: a walk counting approach</title><categories>physics.bio-ph cs.SI physics.soc-ph</categories><comments>6 pages</comments><report-no>MPI MIS Preprint 1/2012</report-no><journal-ref>Europhysics Letters 99, 68007 (2012)</journal-ref><doi>10.1209/0295-5075/99/68007</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new method to efficiently approximate the number of infections
resulting from a given initially-infected node in a network of susceptible
individuals. Our approach is based on counting the number of possible infection
walks of various lengths to each other node in the network. We analytically
study the properties of our method, in particular demonstrating different forms
for SIS and SIR disease spreading (e.g. under the SIR model our method counts
self-avoiding walks). In comparison to existing methods to infer the spreading
efficiency of different nodes in the network (based on degree, k-shell
decomposition analysis and different centrality measures), our method directly
considers the spreading process and, as such, is unique in providing estimation
of actual numbers of infections. Crucially, in simulating infections on various
real-world networks with the SIR model, we show that our walks-based method
improves the inference of effectiveness of nodes over a wide range of infection
rates compared to existing methods. We also analyse the trade-off between
estimate accuracy and computational cost, showing that the better accuracy here
can still be obtained at a comparable computational cost to other methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0503</identifier>
 <datestamp>2013-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0503</id><created>2012-03-02</created><authors><author><keyname>Abdalla</keyname><forenames>Haidara</forenames></author><author><keyname>Ageyev</keyname><forenames>Dmitry</forenames></author></authors><title>Application of Multi-layer Graphs In the Design of MPLS Networks</title><categories>cs.NI</categories><comments>2 pages</comments><journal-ref>Proceedings of the XIth International Conference on Modern
  Problems of Radio Engineering, Telecommunications and Computer Science
  TCSET'2012, 2012, pp. 336-337</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  It is suggested to use multi-layer graphs as a mathematical model in the
design of MPLS networks. The application of this model makes it possible to
design multi-service telecommunication systems simultaneously at several levels
and to reduce the problem to the search of the minimum weight graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0504</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0504</id><created>2012-03-02</created><authors><author><keyname>Bachwerk</keyname><forenames>Martin</forenames></author><author><keyname>Vogel</keyname><forenames>Carl</forenames></author></authors><title>Modelling Social Structures and Hierarchies in Language Evolution</title><categories>cs.CL cs.AI cs.MA</categories><comments>14 pages, 3 figures, 1 table. In proceedings of AI-2010, The
  Thirtieth SGAI International Conference on Innovative Techniques and
  Applications of Artificial Intelligence, Cambridge, England, UK, 14-16
  December 2010</comments><journal-ref>Research and Development in Intelligent Systems XXVII, 2011, pp.
  49-62</journal-ref><doi>10.1007/978-0-85729-130-1_4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Language evolution might have preferred certain prior social configurations
over others. Experiments conducted with models of different social structures
(varying subgroup interactions and the role of a dominant interlocutor) suggest
that having isolated agent groups rather than an interconnected agent is more
advantageous for the emergence of a social communication system. Distinctive
groups that are closely connected by communication yield systems less like
natural language than fully isolated groups inhabiting the same world.
Furthermore, the addition of a dominant male who is asymmetrically favoured as
a hearer, and equally likely to be a speaker has no positive influence on the
disjoint groups.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0511</identifier>
 <datestamp>2012-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0511</id><created>2012-03-02</created><authors><author><keyname>Ageyev</keyname><forenames>Dmitry</forenames></author><author><keyname>Abdalla</keyname><forenames>Haidara</forenames></author></authors><title>Multiservice Telecommunication Systems Parametrical Synthesis by using
  of Multilayer Graph Mathematical Model</title><categories>cs.NI</categories><comments>9 pages, 2 figures, 14 references. In russian</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This study is devoted to the problem of parametric synthesis of multi-service
telecommunication sys-tems. The main characteristics of telecommunication
systems, which are brought to account in an article, are multilayer structure
formed by the overlayed networks and presence flows with self-similarity
effect. For accounting these features of modern telecommunications systems is
proposed to use a multi-layered graph for describing the system structure, and
self-similar processes model for modeling flows in a network. Solution of
parametric synthesis problem is reduced to a nonlinear programming problem
which is solved by using gradient descent method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0512</identifier>
 <datestamp>2012-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0512</id><created>2012-03-02</created><authors><author><keyname>Bachwerk</keyname><forenames>Martin</forenames></author><author><keyname>Vogel</keyname><forenames>Carl</forenames></author></authors><title>Establishing linguistic conventions in task-oriented primeval dialogue</title><categories>cs.CL cs.AI cs.MA</categories><comments>8 pages, 5 figures. In proceedings of the COST 2102 International
  Conference, Budapest, Hungary, September 7-10, 2010, Revised Selected Papers</comments><journal-ref>Analysis of Verbal and Nonverbal Communication and Enactment. The
  Processing Issues. Lecture Notes in Computer Science, Volume 6800/2011, 2011,
  pp. 48-55</journal-ref><doi>10.1007/978-3-642-25775-9_4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we claim that language is likely to have emerged as a
mechanism for coordinating the solution of complex tasks. To confirm this
thesis, computer simulations are performed based on the coordination task
presented by Garrod &amp; Anderson (1987). The role of success in task-oriented
dialogue is analytically evaluated with the help of performance measurements
and a thorough lexical analysis of the emergent communication system.
Simulation results confirm a strong effect of success mattering on both
reliability and dispersion of linguistic conventions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0516</identifier>
 <datestamp>2012-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0516</id><created>2012-03-02</created><authors><author><keyname>Ageyev</keyname><forenames>Dmitry</forenames></author><author><keyname>Ignatenko</keyname><forenames>Artem</forenames></author></authors><title>Describing and Modeling of Video-on-Demand Service with the Usage of
  Multi-Layer Graph</title><categories>cs.NI</categories><comments>2 pages, 1 figure, 4 references</comments><journal-ref>Proceedings of the XI th International Conference on Modern
  Problems of Radio Engineering, Telecommunications and Computer Science
  TCSET'2012, 2012, pp. 340 - 341</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Considered in this paper is the method of describingc of telecommunications
systems providing VoD service using multi-layer graph. The paper describes the
relations between the structural elements at each hierarchical level of the
multi-layer graph. Transfer of video is a resource consuming task, and it
requires an optimal configuration of the studied system. The usage of the
multi-layer graph makes it possible to consider the telecommunication system as
a whole and avoid falling in the local optimums when solving optimization
problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0518</identifier>
 <datestamp>2012-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0518</id><created>2012-03-02</created><authors><author><keyname>Urbano</keyname><forenames>Juli&#xe1;n</forenames></author><author><keyname>Mart&#xed;n</keyname><forenames>Diego</forenames></author><author><keyname>Marrero</keyname><forenames>M&#xf3;nica</forenames></author><author><keyname>Morato</keyname><forenames>Jorge</forenames></author></authors><title>Overview of EIREX 2011: Crowdsourcing</title><categories>cs.IR</categories><comments>9 pages, 4 figures, 5 tables</comments><acm-class>K.3.2; H.3.4</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The second Information Retrieval Education through EXperimentation track
(EIREX 2011) was run at the University Carlos III of Madrid, during the 2011
spring semester. EIREX 2011 is the second in a series of experiments designed
to foster new Information Retrieval (IR) education methodologies and resources,
with the specific goal of teaching undergraduate IR courses from an
experimental perspective. For an introduction to the motivation behind the
EIREX experiments, see the first sections of [Urbano et al., 2011a]. For
information on other editions of EIREX and related data, see the website at
http://ir.kr.inf.uc3m.es/eirex/. The EIREX series have the following goals: a)
to help students get a view of the Information Retrieval process as they would
find it in a real-world scenario, either industrial or academic; b) to make
students realize the importance of laboratory experiments in Computer Science
and have them initiated in their execution and analysis; c) to create a public
repository of resources to teach Information Retrieval courses; d) to seek the
collaboration and active participation of other Universities in this endeavor.
This overview paper summarizes the results of the EIREX 2011 track, focusing on
the creation of the test collection and the analysis to assess its reliability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0532</identifier>
 <datestamp>2012-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0532</id><created>2012-03-02</created><authors><author><keyname>Waltman</keyname><forenames>Ludo</forenames></author><author><keyname>van Eck</keyname><forenames>Nees Jan</forenames></author></authors><title>A new methodology for constructing a publication-level classification
  system of science</title><categories>cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Classifying journals or publications into research areas is an essential
element of many bibliometric analyses. Classification usually takes place at
the level of journals, where the Web of Science subject categories are the most
popular classification system. However, journal-level classification systems
have two important limitations: They offer only a limited amount of detail, and
they have difficulties with multidisciplinary journals. To avoid these
limitations, we introduce a new methodology for constructing classification
systems at the level of individual publications. In the proposed methodology,
publications are clustered into research areas based on citation relations. The
methodology is able to deal with very large numbers of publications. We present
an application in which a classification system is produced that includes
almost ten million publications. Based on an extensive analysis of this
classification system, we discuss the strengths and the limitations of the
proposed methodology. Important strengths are the transparency and relative
simplicity of the methodology and its fairly modest computing and memory
requirements. The main limitation of the methodology is its exclusive reliance
on direct citation relations between publications. The accuracy of the
methodology can probably be increased by also taking into account other types
of relations, for instance based on bibliographic coupling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0535</identifier>
 <datestamp>2014-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0535</id><created>2012-03-02</created><updated>2014-11-01</updated><authors><author><keyname>De Meo</keyname><forenames>Pasquale</forenames></author><author><keyname>Ferrara</keyname><forenames>Emilio</forenames></author><author><keyname>Fiumara</keyname><forenames>Giacomo</forenames></author><author><keyname>Provetti</keyname><forenames>Alessandro</forenames></author></authors><title>On Facebook, most ties are weak</title><categories>cs.SI cs.CY physics.soc-ph</categories><comments>Accepted version of the manuscript before ACM editorial work. Check
  http://cacm.acm.org/magazines/2014/11/179820-on-facebook-most-ties-are-weak/
  for the final version</comments><journal-ref>Communications of the ACM, Vol. 57 No. 11, Pages 78-84, 2014</journal-ref><doi>10.1145/2629438</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pervasive socio-technical networks bring new conceptual and technological
challenges to developers and users alike. A central research theme is
evaluation of the intensity of relations linking users and how they facilitate
communication and the spread of information. These aspects of human
relationships have been studied extensively in the social sciences under the
framework of the &quot;strength of weak ties&quot; theory proposed by Mark Granovetter.13
Some research has considered whether that theory can be extended to online
social networks like Facebook, suggesting interaction data can be used to
predict the strength of ties. The approaches being used require handling
user-generated data that is often not publicly available due to privacy
concerns. Here, we propose an alternative definition of weak and strong ties
that requires knowledge of only the topology of the social network (such as who
is a friend of whom on Facebook), relying on the fact that online social
networks, or OSNs, tend to fragment into communities. We thus suggest
classifying as weak ties those edges linking individuals belonging to different
communities and strong ties as those connecting users in the same community. We
tested this definition on a large network representing part of the Facebook
social graph and studied how weak and strong ties affect the
information-diffusion process. Our findings suggest individuals in OSNs
self-organize to create well-connected communities, while weak ties yield
cohesion and optimize the coverage of information spread.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0536</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0536</id><created>2012-03-02</created><authors><author><keyname>Goussevskaia</keyname><forenames>Olga</forenames></author><author><keyname>Halld&#xf3;rsson</keyname><forenames>Magn&#xfa;s M.</forenames></author><author><keyname>Wattenhofer</keyname><forenames>Roger</forenames></author></authors><title>Algorithms for Wireless Capacity</title><categories>cs.NI cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we address two basic questions in wireless communication:
First, how long does it take to schedule an arbitrary set of communication
requests? Second, given a set of communication requests, how many of them can
be scheduled concurrently? Our results are derived in an interference model
with geometric path loss and consist of efficient algorithms that find a
constant approximation for the second problem and a logarithmic approximation
for the first problem. In addition, we analyze some important properties of the
interference model and show that it is robust to various factors that can
influence the signal attenuation. More specifically, we prove that as long as
such influences on the signal attenuation are constant, they affect the
capacity only by a constant factor.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0541</identifier>
 <datestamp>2012-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0541</id><created>2012-03-02</created><authors><author><keyname>Nandi</keyname><forenames>Mrinal</forenames></author><author><keyname>Nayak</keyname><forenames>Amiya</forenames></author><author><keyname>Roy</keyname><forenames>Bimal</forenames></author><author><keyname>Sarkar</keyname><forenames>Santanu</forenames></author></authors><title>Hypothesis Testing and Decision Theoretic Approach for Fault Detection
  in Wireless Sensor Networks</title><categories>cs.NI stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sensor networks aim at monitoring their surroundings for event detection and
object tracking. But due to failure or death of sensors, false signal can be
transmitted. In this paper, we consider the problem of fault detection in
wireless sensor network (WSN), in particular, addressing both the noise-related
measurement error and sensor fault simultaneously in fault detection. We assume
that the sensors are placed at the center of a square (or hexagonal) cell in
region of interest (ROI) and, if the event occurs, it occurs at a particular
cell of the ROI. We propose fault detection schemes that take into account
error probabilities into the optimal event detection process. We develop the
schemes under the consideration of Neyman-Pearson test and Bayes test.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0543</identifier>
 <datestamp>2013-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0543</id><created>2012-03-02</created><updated>2013-12-03</updated><authors><author><keyname>Kallitsis</keyname><forenames>Michalis</forenames></author><author><keyname>Stoev</keyname><forenames>Stilian</forenames></author><author><keyname>Michailidis</keyname><forenames>George</forenames></author></authors><title>Efficient Approximation Algorithms for Optimal Large-scale Network
  Monitoring</title><categories>cs.DS cs.NI</categories><comments>Paper withdrawn since the official journal paper is now available.
  arXiv admin note: substantial text overlap with arXiv:1108.3048</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The growing amount of applications that generate vast amount of data in short
time scales render the problem of partial monitoring, coupled with prediction,
a rather fundamental one. We study the aforementioned canonical problem under
the context of large-scale monitoring of communication networks. We consider
the problem of selecting the &quot;best&quot; subset of links so as to optimally predict
the quantity of interest at the remaining ones. This is a well know NP-hard
problem, and algorithms seeking the exact solution are prohibitively expensive.
We present a number of approximation algorithms that: 1) their computational
complexity gains a significant improvement over existing greedy algorithms; 2)
exploit the geometry of principal component analysis, which also helps us
establish theoretical bounds on the prediction error; 3) are amenable for
randomized implementation and execution in parallel or distributed fashion, a
process that often yields the exact solution. The new algorithms are
demonstrated and evaluated using real-world network data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0550</identifier>
 <datestamp>2014-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0550</id><created>2012-03-02</created><updated>2014-04-08</updated><authors><author><keyname>Cortes</keyname><forenames>Corinna</forenames></author><author><keyname>Mohri</keyname><forenames>Mehryar</forenames></author><author><keyname>Rostamizadeh</keyname><forenames>Afshin</forenames></author></authors><title>Algorithms for Learning Kernels Based on Centered Alignment</title><categories>cs.LG cs.AI</categories><journal-ref>Journal of Machine Learning Research 13 (2012) 795-828</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents new and effective algorithms for learning kernels. In
particular, as shown by our empirical results, these algorithms consistently
outperform the so-called uniform combination solution that has proven to be
difficult to improve upon in the past, as well as other algorithms for learning
kernels based on convex combinations of base kernels in both classification and
regression. Our algorithms are based on the notion of centered alignment which
is used as a similarity measure between kernels or kernel matrices. We present
a number of novel algorithmic, theoretical, and empirical results for learning
kernels based on our notion of centered alignment. In particular, we describe
efficient algorithms for learning a maximum alignment kernel by showing that
the problem can be reduced to a simple QP and discuss a one-stage algorithm for
learning both a kernel and a hypothesis based on that kernel using an
alignment-based regularization. Our theoretical results include a novel
concentration bound for centered alignment between kernel matrices, the proof
of the existence of effective predictors for kernels with high alignment, both
for classification and for regression, and the proof of stability-based
generalization bounds for a broad family of algorithms for learning kernels
based on centered alignment. We also report the results of experiments with our
centered alignment-based algorithms in both classification and regression.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0586</identifier>
 <datestamp>2012-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0586</id><created>2012-03-02</created><authors><author><keyname>Celaya</keyname><forenames>Marcel</forenames></author><author><keyname>Ruskey</keyname><forenames>Frank</forenames></author></authors><title>An Undecidable Nested Recurrence Relation</title><categories>math.CO cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Roughly speaking, a recurrence relation is nested if it contains a
subexpression of the form ... A(...A(...)...). Many nested recurrence relations
occur in the literature, and determining their behavior seems to be quite
difficult and highly dependent on their initial conditions. A nested recurrence
relation A(n) is said to be undecidable if the following problem is
undecidable: given a finite set of initial conditions for A(n), is the
recurrence relation calculable? Here calculable means that for every n &gt;= 0,
either A(n) is an initial condition or the calculation of A(n) involves only
invocations of A on arguments in {0,1,...,n-1}. We show that the recurrence
relation A(n) = A(n-4-A(A(n-4)))+4A(A(n-4)) +A(2A(n-4-A(n-2))+A(n-2)). is
undecidable by showing how it can be used, together with carefully chosen
initial conditions, to simulate Post 2-tag systems, a known Turing complete
problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0587</identifier>
 <datestamp>2012-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0587</id><created>2012-03-02</created><authors><author><keyname>Brik</keyname><forenames>Alex</forenames></author><author><keyname>Remmel</keyname><forenames>Jeffrey B.</forenames></author></authors><title>Expressing Preferences using Preference Set Constraint Atoms</title><categories>cs.LO</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces an extension of Answer Set Programming called
Preference Set Constraint Programming which is a convenient and general
formalism to reason with preferences. PSC programming extends Set Constraint
Programming introduced by Marek and Remmel (Marek and Remmel 2004) by
introducing two types of preference set constraint atoms, measure preference
set constraint atoms and pre-ordered preference set constraint atoms, which are
extensions of set constraint atoms. We show that the question of whether a PSC
program has a preferred stable model is CoNP-complete. We give examples of the
uses of the preference set constraint atoms and show that Answer Set
Optimization (Brewka, Niemel\&quot;a, and Truszczynski 2003) and General Preference
(Son and Pontelli 2006) can be expressed using preference set constraint atoms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0594</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0594</id><created>2012-03-02</created><updated>2013-04-03</updated><authors><author><keyname>Feldman</keyname><forenames>Vitaly</forenames></author></authors><title>Learning DNF Expressions from Fourier Spectrum</title><categories>cs.LG cs.CC cs.DS</categories><comments>Appears in Conference on Learning Theory (COLT) 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since its introduction by Valiant in 1984, PAC learning of DNF expressions
remains one of the central problems in learning theory. We consider this
problem in the setting where the underlying distribution is uniform, or more
generally, a product distribution. Kalai, Samorodnitsky and Teng (2009) showed
that in this setting a DNF expression can be efficiently approximated from its
&quot;heavy&quot; low-degree Fourier coefficients alone. This is in contrast to previous
approaches where boosting was used and thus Fourier coefficients of the target
function modified by various distributions were needed. This property is
crucial for learning of DNF expressions over smoothed product distributions, a
learning model introduced by Kalai et al. (2009) and inspired by the seminal
smoothed analysis model of Spielman and Teng (2001).
  We introduce a new approach to learning (or approximating) a polynomial
threshold functions which is based on creating a function with range [-1,1]
that approximately agrees with the unknown function on low-degree Fourier
coefficients. We then describe conditions under which this is sufficient for
learning polynomial threshold functions. Our approach yields a new, simple
algorithm for approximating any polynomial-size DNF expression from its &quot;heavy&quot;
low-degree Fourier coefficients alone. Our algorithm greatly simplifies the
proof of learnability of DNF expressions over smoothed product distributions.
We also describe an application of our algorithm to learning monotone DNF
expressions over product distributions. Building on the work of Servedio
(2001), we give an algorithm that runs in time $\poly((s \cdot
\log{(s/\eps)})^{\log{(s/\eps)}}, n)$, where $s$ is the size of the target DNF
expression and $\eps$ is the accuracy. This improves on $\poly((s \cdot
\log{(ns/\eps)})^{\log{(s/\eps)} \cdot \log{(1/\eps)}}, n)$ bound of Servedio
(2001).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0617</identifier>
 <datestamp>2012-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0617</id><created>2012-03-03</created><updated>2012-11-08</updated><authors><author><keyname>Xiao</keyname><forenames>Yonghui</forenames></author><author><keyname>Xiong</keyname><forenames>Li</forenames></author></authors><title>Bayesian inference under differential privacy</title><categories>cs.DB</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bayesian inference is an important technique throughout statistics. The
essence of Beyesian inference is to derive the posterior belief updated from
prior belief by the learned information, which is a set of differentially
private answers under differential privacy. Although Bayesian inference can be
used in a variety of applications, it becomes theoretically hard to solve when
the number of differentially private answers is large. To facilitate Bayesian
inference under differential privacy, this paper proposes a systematic
mechanism. The key step of the mechanism is the implementation of Bayesian
updating with the best linear unbiased estimator derived by Gauss-Markov
theorem. In addition, we also apply the proposed inference mechanism into an
online queryanswering system, the novelty of which is that the utility for
users is guaranteed by Bayesian inference in the form of credible interval and
confidence level. Theoretical and experimental analysis are shown to
demonstrate the efficiency and effectiveness of both inference mechanism and
online query-answering system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0631</identifier>
 <datestamp>2012-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0631</id><created>2012-03-03</created><updated>2012-05-28</updated><authors><author><keyname>Chistikov</keyname><forenames>Dmitry V.</forenames></author></authors><title>Checking Tests for Read-Once Functions over Arbitrary Bases</title><categories>cs.DM cs.CC cs.LG</categories><comments>Accepted to the 7th International Computer Science Symposium in
  Russia (CSR 2012), revised version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Boolean function is called read-once over a basis B if it can be expressed
by a formula over B where no variable appears more than once. A checking test
for a read-once function f over B depending on all its variables is a set of
input vectors distinguishing f from all other read-once functions of the same
variables. We show that every read-once function f over B has a checking test
containing O(n^l) vectors, where n is the number of relevant variables of f and
l is the largest arity of functions in B. For some functions, this bound cannot
be improved by more than a constant factor. The employed technique involves
reconstructing f from its l-variable projections and provides a stronger form
of Kuznetsov's classic theorem on read-once representations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0640</identifier>
 <datestamp>2012-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0640</id><created>2012-03-03</created><authors><author><keyname>Siddique</keyname><forenames>Qasim</forenames></author></authors><title>Kerberos Authentication in Wireless Sensor Networks</title><categories>cs.CR cs.NI</categories><comments>14 pages</comments><journal-ref>Ann. Univ. Tibiscus Comp. Sci. Series VIII / 1 (2010), 67-80</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We proposed an authentication mechanism in the wireless sensor network.
Sensor network uses the Kerberos authentication scheme for the authentication
of bases station in the network. Kerberos provides a centralized authentication
server whose function is to authenticate user by providing him the ticket to
grant request to the base station. In this paper we have provided architecture
for the authentication of base station in the wireless sensor network based on
the Kerberos server authentication scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0648</identifier>
 <datestamp>2012-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0648</id><created>2012-03-03</created><authors><author><keyname>Levin</keyname><forenames>Mark Sh.</forenames></author></authors><title>Towards Electronic Shopping of Composite Product</title><categories>cs.SE cs.AI math.OC</categories><comments>10 pages, 20 figures, 17 tables</comments><msc-class>90B50, 90c27, 90C29, 90C59, 90C90, 91B08, 05C69</msc-class><acm-class>H.4; D.2; J.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the paper, frameworks for electronic shopping of composite (modular)
products are described: (a) multicriteria selection (product is considered as a
whole system, it is a traditional approach), (b) combinatorial synthesis
(composition) of the product from its components, (c) aggregation of the
product from several selected products/prototypes. The following product model
is examined: (i) general tree-like structure, (ii) set of system
parts/components (leaf nodes), (iii) design alternatives (DAs) for each
component, (iv) ordinal priorities for DAs, and (v) estimates of compatibility
between DAs for different components. The combinatorial synthesis is realized
as morphological design of a composite (modular) product or an extended
composite product (e.g., product and support services as financial
instruments). Here the solving process is based on Hierarchical Morphological
Multicriteria Design (HMMD): (i) multicriteria selection of alternatives for
system parts, (ii) composing the selected alternatives into a resultant
combination (while taking into account ordinal quality of the alternatives
above and their compatibility). The aggregation framework is based on
consideration of aggregation procedures, for example: (i) addition procedure:
design of a products substructure or an extended substructure ('kernel') and
addition of elements, and (ii) design procedure: design of the composite
solution based on all elements of product superstructure. Applied numerical
examples (e.g., composite product, extended composite product, product repair
plan, and product trajectory) illustrate the proposed approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0651</identifier>
 <datestamp>2012-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0651</id><created>2012-03-03</created><authors><author><keyname>Rizvandi</keyname><forenames>Nikzad Babaii</forenames></author><author><keyname>Zomaya</keyname><forenames>Albert Y.</forenames></author><author><keyname>Boloori</keyname><forenames>Ali Javadzadeh</forenames></author><author><keyname>Taheri</keyname><forenames>Javid</forenames></author></authors><title>On Modeling Dependency between MapReduce Configuration Parameters and
  Total Execution Time</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose an analytical method to model the dependency
between configuration parameters and total execution time of Map-Reduce
applications. Our approach has three key phases: profiling, modeling, and
prediction. In profiling, an application is run several times with different
sets of MapReduce configuration parameters to profile the execution time of the
application on a given platform. Then in modeling, the relation between these
parameters and total execution time is modeled by multivariate linear
regression. Among the possible configuration parameters, two main parameters
have been used in this study: the number of Mappers, and the number of
Reducers. For evaluation, two standard applications (WordCount, and Exim
Mainlog parsing) are utilized to evaluate our technique on a 4-node MapReduce
platform.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0652</identifier>
 <datestamp>2012-04-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0652</id><created>2012-03-03</created><authors><author><keyname>G&#xf3;mez</keyname><forenames>Vicen&#xe7;</forenames></author><author><keyname>Kappen</keyname><forenames>Hilbert J.</forenames></author><author><keyname>Litvak</keyname><forenames>Nelly</forenames></author><author><keyname>Kaltenbrunner</keyname><forenames>Andreas</forenames></author></authors><title>A likelihood-based framework for the analysis of discussion threads</title><categories>cs.SI physics.soc-ph</categories><comments>31 pages, 12 figures, journal</comments><acm-class>G.3; H.5.4</acm-class><doi>10.1007/s11280-012-0162-8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online discussion threads are conversational cascades in the form of posted
messages that can be generally found in social systems that comprise
many-to-many interaction such as blogs, news aggregators or bulletin board
systems. We propose a framework based on generative models of growing trees to
analyse the structure and evolution of discussion threads. We consider the
growth of a discussion to be determined by an interplay between popularity,
novelty and a trend (or bias) to reply to the thread originator. The relevance
of these features is estimated using a full likelihood approach and allows to
characterize the habits and communication patterns of a given platform and/or
community.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0653</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0653</id><created>2012-03-03</created><updated>2012-12-17</updated><authors><author><keyname>Manin</keyname><forenames>Yuri I.</forenames></author><author><keyname>Marcolli</keyname><forenames>Matilde</forenames></author></authors><title>Kolmogorov complexity and the asymptotic bound for error-correcting
  codes</title><categories>cs.IT math.IT</categories><comments>Typos corrected, dedication added. 21 pages</comments><msc-class>94B65, 82B26</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The set of all error--correcting block codes over a fixed alphabet with $q$
letters determines a recursively enumerable set of rational points in the unit
square with coordinates $(R,\delta)$:= (relative transmission rate, relative
minimal distance). Limit points of this set form a closed subset, defined by
$R\le \alpha_q(\delta)$, where $\alpha_q(\delta)$ is a continuous decreasing
function called asymptotic bound. Its existence was proved by the first--named
author in 1981 ([Man1]), but no approaches to the computation of this function
are known, and in [Man5] it was even suggested that this function might be
uncomputable in the sense of constructive analysis.
  In this note we show that the asymptotic bound becomes computable with the
assistance of an oracle producing codes in the order of their growing
Kolmogorov complexity. Moreover, a natural partition function involving
complexity allows us to interpret the asymptotic bound as a curve dividing two
different thermodynamic phases of codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0656</identifier>
 <datestamp>2012-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0656</id><created>2012-03-03</created><authors><author><keyname>Maalel</keyname><forenames>Ahmed</forenames></author><author><keyname>Hadj-Mabrouk</keyname><forenames>Habib</forenames></author></authors><title>Contribution of Case Based Reasoning (CBR) in the Exploitation of Return
  of Experience. Application to Accident Scenarii in Railroad Transport</title><categories>cs.AI cs.HC</categories><comments>Paper Award, 8 pages, 10 figures,3rd International Conference on
  Information Systems and Economic Intelligence, 18-20 february 2010, Sousse,
  Tunisia. http://siie2010.loria.fr/</comments><journal-ref>Int. Conf. Info. Sys. Eco. Intelligence. (2010) 1-8</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The study is from a base of accident scenarii in rail transport (feedback) in
order to develop a tool to share build and sustain knowledge and safety and
secondly to exploit the knowledge stored to prevent the reproduction of
accidents / incidents. This tool should ultimately lead to the proposal of
prevention and protection measures to minimize the risk level of a new
transport system and thus to improve safety. The approach to achieving this
goal largely depends on the use of artificial intelligence techniques and
rarely the use of a method of automatic learning in order to develop a
feasibility model of a software tool based on case based reasoning (CBR) to
exploit stored knowledge in order to create know-how that can help stimulate
domain experts in the task of analysis, evaluation and certification of a new
system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0657</identifier>
 <datestamp>2012-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0657</id><created>2012-03-03</created><authors><author><keyname>Attia</keyname><forenames>Osama</forenames></author><author><keyname>ElBatt</keyname><forenames>Tamer</forenames></author></authors><title>On the Role of Vehicular Mobility in Cooperative Content Caching</title><categories>cs.NI</categories><comments>5 pages, conference, WCNC 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we analyze the performance of cooperative content caching in
vehicular ad hoc networks (VANETs). In particular, we characterize, using
analysis and simulations, the behavior of the probability of outage (i.e. not
finding a requested data chunk at a neighbor) under freeway vehicular mobility.
First, we introduce a formal definition for the probability of outage in the
context of cooperative content caching. Second, we characterize, analytically,
the outage probability under vehicular and random mobility scenarios. Next, we
verify the analytical results using simulations and compare the performance
under a number of plausible mobility scenarios. This provides key insights into
the problem and the involved trade-offs and enable us to assess the potential
opportunity offered by the, somewhat structured, vehicular mobility that can be
exploited by cooperative content caching schemes. The presented numerical
results exhibit complete agreement between the analytical and simulation
studies. Finally, we observe that vehicular mobility creates opportunities for
enhanced outage performance under practically relevant scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0665</identifier>
 <datestamp>2012-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0665</id><created>2012-03-03</created><authors><author><keyname>Hahanov</keyname><forenames>Vladimir</forenames></author><author><keyname>Gharibi</keyname><forenames>Wajeb</forenames></author><author><keyname>Litvinova</keyname><forenames>Eugenia</forenames></author><author><keyname>Chumachenko</keyname><forenames>Svetlana</forenames></author></authors><title>Verification and Diagnosis Infrastructure of SoC HDL-model</title><categories>cs.OH</categories><comments>7 pages; International Conference on VLSI, MEMS and NEMS, VMN-2012,
  24th - 25th January 2012, Noidar, Uttar, Paradesh, India</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article describes technology for diagnosing SoC HDL-models, based on
transactional graph. Diagnosis method is focused to considerable decrease the
time of fault detection and memory for storage of diagnosis matrix by means of
forming ternary relations in the form of test, monitor, and functional
component. The following problems are solved: creation of digital system model
in the form of transaction graph and multi-tree of fault detection tables, as
well as ternary matrices for activating functional components in tests,
relative to the selected set of monitors; development of a method for analyzing
the activation matrix to detect the faults with given depth and synthesizing
logic functions for subsequent embedded hardware fault diagnosing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0666</identifier>
 <datestamp>2012-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0666</id><created>2012-03-03</created><authors><author><keyname>Chiang</keyname><forenames>Chun-Ying</forenames></author><author><keyname>Huang</keyname><forenames>Liang-Hao</forenames></author><author><keyname>Yeh</keyname><forenames>Hong-Gwa</forenames></author></authors><title>Target set selection problem for honeycomb networks</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $G$ be a graph with a threshold function $\theta:V(G)\rightarrow
\mathbb{N}$ such that $1\leq \theta(v)\leq d_G(v)$ for every vertex $v$ of $G$,
where $d_G(v)$ is the degree of $v$ in $G$. Suppose we are given a target set
$S\subseteq V(G)$. The paper considers the following repetitive process on $G$.
At time step 0 the vertices of $S$ are colored black and the other vertices are
colored white. After that, at each time step $t&gt;0$, the colors of white
vertices (if any) are updated according to the following rule. All white
vertices $v$ that have at least $\theta(v)$ black neighbors at the time step
$t-1$ are colored black, and the colors of the other vertices do not change.
The process runs until no more white vertices can update colors from white to
black. The following optimization problem is called Target Set Selection:
Finding a target set $S$ of smallest possible size such that all vertices in
$G$ are black at the end of the process. Such an $S$ is called an {\em optimal
target set} for $G$ under the threshold function $\theta$. We are interested in
finding an optimal target set for the well-known class of honeycomb networks
under an important threshold function called {\em strict majority threshold},
where $\theta(v)=\lceil (d_G(v)+1)/2\rceil$ for each vertex $v$ in $G$. In a
graph $G$, a {\em feedback vertex set} is a subset $S\subseteq V(G)$ such that
the subgraph induced by $V(G)\setminus S$ is cycle-free. In this paper we give
exact value on the size of the optimal target set for various kinds of
honeycomb networks under strict majority threshold, and as a by-product we also
provide a minimum feedback vertex set for different kinds regular graphs in the
class of honeycomb networks
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0670</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0670</id><created>2012-03-03</created><updated>2012-03-23</updated><authors><author><keyname>Accattoli</keyname><forenames>Beniamino</forenames><affiliation>LIPN</affiliation></author><author><keyname>Kesner</keyname><forenames>Delia</forenames><affiliation>PPS, Universite Paris-Diderot and CNRS</affiliation></author></authors><title>Preservation of Strong Normalisation modulo permutations for the
  structural lambda-calculus</title><categories>cs.LO</categories><proxy>LMCS</proxy><acm-class>F.3.2, D.1.1, F.4.1</acm-class><journal-ref>Logical Methods in Computer Science, Volume 8, Issue 1 (March 27,
  2012) lmcs:847</journal-ref><doi>10.2168/LMCS-8(1:28)2012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inspired by a recent graphical formalism for lambda-calculus based on linear
logic technology, we introduce an untyped structural lambda-calculus, called
lambda j, which combines actions at a distance with exponential rules
decomposing the substitution by means of weakening, contraction and
derelicition. First, we prove some fundamental properties of lambda j such as
confluence and preservation of beta-strong normalisation. Second, we add a
strong bisimulation to lambda j by means of an equational theory which captures
in particular Regnier's sigma-equivalence. We then complete this bisimulation
with two more equations for (de)composition of substitutions and we prove that
the resulting calculus still preserves beta-strong normalization. Finally, we
discuss some consequences of our results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0681</identifier>
 <datestamp>2012-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0681</id><created>2012-03-03</created><authors><author><keyname>Abdulla</keyname><forenames>Mohammed Fadle</forenames></author></authors><title>Manual and Fast C Code Optimization</title><categories>cs.PL</categories><comments>16 pages</comments><journal-ref>Ann. Univ. Tibiscus Comp. Sci. Series VIII / 1 (2010), 93-108</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Developing an application with high performance through the code optimization
places a greater responsibility on the programmers. While most of the existing
compilers attempt to automatically optimize the program code, manual techniques
remain the predominant method for performing optimization. Deciding where to
try to optimize code is difficult, especially for large complex applications.
For manual optimization, the programmers can use his experiences in writing the
code, and then he can use a software profiler in order to collect and analyze
the performance data from the code. In this work, we have gathered the most
experiences which can be applied to improve the style of writing programs in C
language as well as we present an implementation of the manual optimization of
the codes using the Intel VTune profiler. The paper includes two case studies
to illustrate our optimization on the Heap Sort and Factorial functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0683</identifier>
 <datestamp>2012-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0683</id><created>2012-03-03</created><updated>2012-09-05</updated><authors><author><keyname>Anandkumar</keyname><forenames>Animashree</forenames></author><author><keyname>Hsu</keyname><forenames>Daniel</forenames></author><author><keyname>Kakade</keyname><forenames>Sham M.</forenames></author></authors><title>A Method of Moments for Mixture Models and Hidden Markov Models</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mixture models are a fundamental tool in applied statistics and machine
learning for treating data taken from multiple subpopulations. The current
practice for estimating the parameters of such models relies on local search
heuristics (e.g., the EM algorithm) which are prone to failure, and existing
consistent methods are unfavorable due to their high computational and sample
complexity which typically scale exponentially with the number of mixture
components. This work develops an efficient method of moments approach to
parameter estimation for a broad class of high-dimensional mixture models with
many components, including multi-view mixtures of Gaussians (such as mixtures
of axis-aligned Gaussians) and hidden Markov models. The new method leads to
rigorous unsupervised learning results for mixture models that were not
achieved by previous works; and, because of its simplicity, it offers a viable
alternative to EM for practical deployment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0690</identifier>
 <datestamp>2012-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0690</id><created>2012-03-03</created><authors><author><keyname>Eger</keyname><forenames>Steffen</forenames></author></authors><title>Asymptotic normality of integer compositions inside a rectangle</title><categories>math.CO cs.DM math.PR</categories><comments>7 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Among all restricted integer compositions with at most $m$ parts, each of
which has size at most $l$, choose one uniformly at random. Which integer does
this composition represent? In the current note, we show that underlying
distribution is, for large $m$ and $l$, approximately normal with mean value
$\frac{ml}{2}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0692</identifier>
 <datestamp>2012-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0692</id><created>2012-03-03</created><authors><author><keyname>Bahi</keyname><forenames>Jacques M.</forenames></author><author><keyname>Friot</keyname><forenames>Nicolas</forenames></author><author><keyname>Guyeux</keyname><forenames>Christophe</forenames></author></authors><title>Lyapunov exponent evaluation of a digital watermarking scheme proven to
  be secure</title><categories>cs.CR cs.MM math.DS math.GN</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In our previous researches, a new digital watermarking scheme based on
chaotic iterations has been introduced. This scheme was both stego-secure and
topologically secure. The stego-security is to face an attacker in the
&quot;watermark only attack&quot; category, whereas the topological security concerns
other categories of attacks. Its Lyapunov exponent is evaluated here, to
quantify the chaos generated by this scheme.
  Keywords : Lyapunov exponent; Information hiding; Security; Chaotic
iterations; Digital Watermarking.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0695</identifier>
 <datestamp>2012-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0695</id><created>2012-03-03</created><authors><author><keyname>Nokleby</keyname><forenames>Matthew</forenames></author><author><keyname>Aazhang</keyname><forenames>Behnaam</forenames></author></authors><title>Cooperative Compute-and-Forward</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We examine the benefits of user cooperation under compute-and-forward. Much
like in network coding, receivers in a compute-and-forward network recover
finite-field linear combinations of transmitters' messages. Recovery is enabled
by linear codes: transmitters map messages to a linear codebook, and receivers
attempt to decode the incoming superposition of signals to an integer
combination of codewords. However, the achievable computation rates are low if
channel gains do not correspond to a suitable linear combination. In response
to this challenge, we propose a cooperative approach to compute-and-forward. We
devise a lattice-coding approach to block Markov encoding with which we
construct a decode-and-forward style computation strategy. Transmitters
broadcast lattice codewords, decode each other's messages, and then
cooperatively transmit resolution information to aid receivers in decoding the
integer combinations. Using our strategy, we show that cooperation offers a
significant improvement both in the achievable computation rate and in the
diversity-multiplexing tradeoff.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0696</identifier>
 <datestamp>2012-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0696</id><created>2012-03-03</created><authors><author><keyname>Celik</keyname><forenames>G&#xfc;ner D.</forenames></author><author><keyname>Le</keyname><forenames>Long B.</forenames></author><author><keyname>Modiano</keyname><forenames>Eytan</forenames></author></authors><title>Dynamic Server Allocation over Time Varying Channels with Switchover
  Delay</title><categories>math.OC cs.IT math.IT</categories><comments>38 Pages, 18 figures. arXiv admin note: substantial text overlap with
  arXiv:1008.2347</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a dynamic server allocation problem over parallel queues with
randomly varying connectivity and server switchover delay between the queues.
At each time slot the server decides either to stay with the current queue or
switch to another queue based on the current connectivity and the queue length
information. Switchover delay occurs in many telecommunications applications
and is a new modeling component of this problem that has not been previously
addressed. We show that the simultaneous presence of randomly varying
connectivity and switchover delay changes the system stability region and the
structure of optimal policies. In the first part of the paper, we consider a
system of two parallel queues, and develop a novel approach to explicitly
characterize the stability region of the system using state-action frequencies
which are stationary solutions to a Markov Decision Process (MDP) formulation.
We then develop a frame-based dynamic control (FBDC) policy, based on the
state-action frequencies, and show that it is throughput-optimal asymptotically
in the frame length. The FBDC policy is applicable to a broad class of network
control systems and provides a new framework for developing throughput-optimal
network control policies using state-action frequencies. Furthermore, we
develop simple Myopic policies that provably achieve more than 90% of the
stability region. In the second part of the paper, we extend our results to
systems with an arbitrary but finite number of queues.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0697</identifier>
 <datestamp>2012-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0697</id><created>2012-03-03</created><updated>2012-06-30</updated><authors><author><keyname>Anandkumar</keyname><forenames>A.</forenames></author><author><keyname>Hsu</keyname><forenames>D.</forenames></author><author><keyname>Huang</keyname><forenames>F.</forenames></author><author><keyname>Kakade</keyname><forenames>S. M.</forenames></author></authors><title>Learning High-Dimensional Mixtures of Graphical Models</title><categories>stat.ML cs.AI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider unsupervised estimation of mixtures of discrete graphical models,
where the class variable corresponding to the mixture components is hidden and
each mixture component over the observed variables can have a potentially
different Markov graph structure and parameters. We propose a novel approach
for estimating the mixture components, and our output is a tree-mixture model
which serves as a good approximation to the underlying graphical model mixture.
Our method is efficient when the union graph, which is the union of the Markov
graphs of the mixture components, has sparse vertex separators between any pair
of observed variables. This includes tree mixtures and mixtures of bounded
degree graphs. For such models, we prove that our method correctly recovers the
union graph structure and the tree structures corresponding to
maximum-likelihood tree approximations of the mixture components. The sample
and computational complexities of our method scale as $\poly(p, r)$, for an
$r$-component mixture of $p$-variate graphical models. We further extend our
results to the case when the union graph has sparse local separators between
any pair of observed variables, such as mixtures of locally tree-like graphs,
and the mixture components are in the regime of correlation decay.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0699</identifier>
 <datestamp>2012-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0699</id><created>2012-03-03</created><authors><author><keyname>Halpern</keyname><forenames>Joseph Y.</forenames></author><author><keyname>Kets</keyname><forenames>Willemien</forenames></author></authors><title>Ambiguous Language and Differences in Beliefs</title><categories>cs.AI cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Standard models of multi-agent modal logic do not capture the fact that
information is often ambiguous, and may be interpreted in different ways by
different agents. We propose a framework that can model this, and consider
different semantics that capture different assumptions about the agents'
beliefs regarding whether or not there is ambiguity. We consider the impact of
ambiguity on a seminal result in economics: Aumann's result saying that agents
with a common prior cannot agree to disagree. This result is known not to hold
if agents do not have a common prior; we show that it also does not hold in the
presence of ambiguity. We then consider the tradeoff between assuming a common
interpretation (i.e., no ambiguity) and a common prior (i.e., shared initial
beliefs).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0714</identifier>
 <datestamp>2013-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0714</id><created>2012-03-04</created><authors><author><keyname>Mouzoune</keyname><forenames>Abdessamad</forenames></author></authors><title>Towards an intelligence based conceptual framework for e-maintenance</title><categories>cs.MA</categories><comments>16 pages,1 figure</comments><doi>10.1109/SITA.2013.6560789</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since the time when concept of e-maintenance was introduced, most of the
works insisted on the relevance of the underlying Information and Communication
Technologies infrastructure. Through a review of current e-maintenance
conceptual approaches and realizations, this paper aims to reconsider the
predominance of ICT within e-maintenance projects and literature. The review
brings to light the importance of intelligence as a fundamental dimension of
e-maintenance that is to be led in a holistic predefined manner rather than
isolated efforts within ICT driven approaches. As a contribution towards an
intelligence based e-maintenance conceptual framework, a proposal is outlined
in this paper to model e-maintenance system as an intelligent system. The
proposed frame is based on CogAff architecture for intelligent agents. Within
the proposed frame, more importance was reserved to the environment that the
system is to be continuously aware of: Plant Environment, Internal and External
Enterprise Environment and Human Environment. In addition to the abilities
required for internal coherent behavior of the system, requirements for
maintenance activities support are also mapped within the same frame according
to corresponding levels of management. A case study was detailed in this paper
sustaining the applicability of the proposal in relation to the classification
of existing e-maintenance platforms. However, more work is needed to enhance
exhaustiveness of the frame to serve as a comparison tool of existing
e-maintenance systems. At the conceptual level, our future work is to use the
proposed frame in an e-maintenance project.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0728</identifier>
 <datestamp>2012-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0728</id><created>2012-03-04</created><authors><author><keyname>Alahmadi</keyname><forenames>A.</forenames></author><author><keyname>Aldred</keyname><forenames>R. E. L.</forenames></author><author><keyname>Cruz</keyname><forenames>R. dela</forenames></author><author><keyname>Sol&#xe9;</keyname><forenames>P.</forenames></author><author><keyname>Thomassen</keyname><forenames>C.</forenames></author></authors><title>The maximum number of minimal codewords in an $[n,k]-$code</title><categories>cs.IT math.CO math.IT</categories><comments>6 pp. Submitted</comments><msc-class>94A10 (Primary) 05C38, 05B35 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Upper and lower bounds are derived for the quantity in the title, which is
tabulated for modest values of $n$ and $k.$ An application to graphs with many
cycles is given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0730</identifier>
 <datestamp>2014-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0730</id><created>2012-03-04</created><updated>2014-08-21</updated><authors><author><keyname>Yassaee</keyname><forenames>Mohammad Hossein</forenames></author><author><keyname>Aref</keyname><forenames>Mohammad Reza</forenames></author><author><keyname>Gohari</keyname><forenames>Amin</forenames></author></authors><title>Achievability proof via output statistics of random binning</title><categories>cs.IT math.IT</categories><comments>IEEE Transactions on Information Theory, accepted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a new and ubiquitous framework for establishing
achievability results in \emph{network information theory} (NIT) problems. The
framework uses random binning arguments and is based on a duality between
channel and source coding problems. {Further,} the framework uses pmf
approximation arguments instead of counting and typicality. This allows for
proving coordination and \emph{strong} secrecy problems where certain
statistical conditions on the distribution of random variables need to be
satisfied. These statistical conditions include independence between messages
and eavesdropper's observations in secrecy problems and closeness to a certain
distribution (usually, i.i.d. distribution) in coordination problems. One
important feature of the framework is to enable one {to} add an eavesdropper
and obtain a result on the secrecy rates &quot;for free.&quot;
  We make a case for generality of the framework by studying examples in the
variety of settings containing channel coding, lossy source coding, joint
source-channel coding, coordination, strong secrecy, feedback and relaying. In
particular, by investigating the framework for the lossy source coding problem
over broadcast channel, it is shown that the new framework provides a simple
alternative scheme to \emph{hybrid} coding scheme. Also, new results on secrecy
rate region (under strong secrecy criterion) of wiretap broadcast channel and
wiretap relay channel are derived. In a set of accompanied papers, we have
shown the usefulness of the framework to establish achievability results for
coordination problems including interactive channel simulation, coordination
via relay and channel simulation via another channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0731</identifier>
 <datestamp>2012-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0731</id><created>2012-03-04</created><authors><author><keyname>Haddadpour</keyname><forenames>Farzin</forenames></author><author><keyname>Yassaee</keyname><forenames>Mohammad Hossein</forenames></author><author><keyname>Gohari</keyname><forenames>Amin</forenames></author><author><keyname>Aref</keyname><forenames>Mohammad Reza</forenames></author></authors><title>Coordination via a relay</title><categories>cs.IT math.IT</categories><comments>Submitted to ISIT 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the problem of coordinating two nodes which can only
exchange information via a relay at limited rates. The nodes are allowed to do
a two-round interactive two-way communication with the relay, after which they
should be able to generate i.i.d. copies of two random variables with a given
joint distribution within a vanishing total variation distance. We prove inner
and outer bounds on the coordination capacity region for this problem. Our
inner bound is proved using the technique of &quot;output statistics of random
binning&quot; that has recently been developed by Yassaee, et al.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0732</identifier>
 <datestamp>2012-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0732</id><created>2012-03-04</created><authors><author><keyname>Sen</keyname><forenames>Jaydip</forenames></author></authors><title>Secure and Energy-Efficient Data Aggregation in Wireless Sensor Networks</title><categories>cs.CR cs.NI</categories><comments>7 pages, 3 figures. In Proceedings of the 2nd National Conference on
  Computational Intelligence and Signal Processing (CISP 2012), March 2- 3,
  Guwahati, India.(IEEE Technically Sponsored)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data aggregation in intermediate nodes (called aggregator nodes) is an
effective approach for optimizing consumption of scarce resources like
bandwidth and energy in Wireless Sensor Networks (WSNs). However, in-network
processing poses a problem for the privacy of the sensor data since individual
data of sensor nodes need to be known to the aggregator node before the
aggregation process can be carried out. In applications of WSNs,
privacy-preserving data aggregation has become an important requirement due to
sensitive nature of the sensor data. Researchers have proposed a number of
protocols and schemes for this purpose. He et al. (INFOCOM 2007) have proposed
a protocol - called CPDA - for carrying out additive data aggregation in a
privacy-preserving manner for application in WSNs. The scheme has been quite
popular and well-known. In spite of the popularity of this protocol, it has
been found that the protocol is vulnerable to attack and it is also not
energy-efficient. In this paper, we first present a brief state of the art
survey on the current privacy-preserving data aggregation protocols for WSNS.
Then we describe the CPDA protocol and identify its security vulnerability.
Finally, we demonstrate how the protocol can be made secure and energy
efficient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0740</identifier>
 <datestamp>2012-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0740</id><created>2012-03-04</created><authors><author><keyname>Li</keyname><forenames>Bo</forenames></author><author><keyname>Pei</keyname><forenames>Yijian</forenames></author><author><keyname>Shen</keyname><forenames>Bin</forenames></author><author><keyname>Wu</keyname><forenames>Hao</forenames></author><author><keyname>He</keyname><forenames>Min</forenames></author><author><keyname>Yang</keyname><forenames>Jundong</forenames></author></authors><title>Resource Availability-Aware Advance Reservation for Parallel Jobs with
  Deadlines</title><categories>cs.DC</categories><comments>25 pages,7 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Advance reservation is important to guarantee the quality of services of jobs
by allowing exclusive access to resources over a defined time interval on
resources. It is a challenge for the scheduler to organize available resources
efficiently and to allocate them for parallel AR jobs with deadline constraint
appropriately. This paper provides a slot-based data structure to organize
available resources of multiprocessor systems in a way that enables efficient
search and update operations, and formulates a suite of scheduling policies to
allocate resources for dynamically arriving AR requests. The performance of the
scheduling algorithms were investigated by simulations with different job sizes
and durations, system loads and scheduling flexibilities. Simulation results
show that job sizes and durations, system load and the flexibility of
scheduling will impact the performance metrics of all the scheduling
algorithms, and the PE-Worst-Fit algorithm becomes the best algorithm for the
scheduler with the highest acceptance rate of AR requests, and the jobs with
the First-Fit algorithm experience the lowest average slowdown. The data
structure and scheduling policies can be used to organize and allocate
resources for parallel AR jobs with deadline constraint in large-scale
computing systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0744</identifier>
 <datestamp>2012-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0744</id><created>2012-03-04</created><authors><author><keyname>Kong</keyname><forenames>Shu</forenames></author><author><keyname>Wang</keyname><forenames>Donghui</forenames></author></authors><title>A Report on Multilinear PCA Plus Multilinear LDA to Deal with Tensorial
  Data: Visual Classification as An Example</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In practical applications, we often have to deal with high order data, such
as a grayscale image and a video sequence are intrinsically 2nd-order tensor
and 3rd-order tensor, respectively. For doing clustering or classification of
these high order data, it is a conventional way to vectorize these data before
hand, as PCA or FDA does, which often induce the curse of dimensionality
problem. For this reason, experts have developed many methods to deal with the
tensorial data, such as multilinear PCA, multilinear LDA, and so on. In this
paper, we still address the problem of high order data representation and
recognition, and propose to study the result of merging multilinear PCA and
multilinear LDA into one scenario, we name it \textbf{GDA} for the abbreviation
of Generalized Discriminant Analysis. To evaluate GDA, we perform a series of
experiments, and the experimental results demonstrate our GDA outperforms a
selection of competing methods such (2D)$^2$PCA, (2D)$^2$LDA, and MDA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0747</identifier>
 <datestamp>2014-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0747</id><created>2012-03-04</created><updated>2012-06-19</updated><authors><author><keyname>Quartulli</keyname><forenames>Marco</forenames></author><author><keyname>Olaizola</keyname><forenames>Igor G.</forenames></author></authors><title>A review of EO image information mining</title><categories>cs.IR</categories><journal-ref>Quartulli, Marco, and Igor G Olaizola. &quot;A review of EO image
  information mining.&quot; ISPRS Journal of Photogrammetry and Remote Sensing 75:
  p11-28. 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze the state of the art of content-based retrieval in Earth
observation image archives focusing on complete systems showing promise for
operational implementation. The different paradigms at the basis of the main
system families are introduced. The approaches taken are analyzed, focusing in
particular on the phases after primitive feature extraction. The solutions
envisaged for the issues related to feature simplification and synthesis,
indexing, semantic labeling are reviewed. The methodologies for query
specification and execution are analyzed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0780</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0780</id><created>2012-03-04</created><updated>2012-03-13</updated><authors><author><keyname>Castagna</keyname><forenames>Giuseppe</forenames><affiliation>CNRS, PPS, Univ Paris Diderot, Sorbonne Paris Cit&#xe9;, Paris, France</affiliation></author><author><keyname>Dezani-Ciancaglini</keyname><forenames>Mariangiola</forenames><affiliation>Dipartimento di Informatica, Universit`a degli Studi di Torino, Torino, Italy</affiliation></author><author><keyname>Padovani</keyname><forenames>Luca</forenames><affiliation>Dipartimento di Informatica, Universit`a degli Studi di Torino, Torino, Italy</affiliation></author></authors><title>On Global Types and Multi-Party Session</title><categories>cs.PL</categories><proxy>LMCS</proxy><acm-class>F.1.2, F.3.3, H.3.5, H.5.3</acm-class><journal-ref>Logical Methods in Computer Science, Volume 8, Issue 1 (March 15,
  2012) lmcs:773</journal-ref><doi>10.2168/LMCS-8(1:24)2012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Global types are formal specifications that describe communication protocols
in terms of their global interactions. We present a new, streamlined language
of global types equipped with a trace-based semantics and whose features and
restrictions are semantically justified. The multi-party sessions obtained
projecting our global types enjoy a liveness property in addition to the
traditional progress and are shown to be sound and complete with respect to the
set of traces of the originating global type. Our notion of completeness is
less demanding than the classical ones, allowing a multi-party session to leave
out redundant traces from an underspecified global type. In addition to the
technical content, we discuss some limitations of our language of global types
and provide an extensive comparison with related specification languages
adopted in different communities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0781</identifier>
 <datestamp>2012-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0781</id><created>2012-03-04</created><updated>2012-03-22</updated><authors><author><keyname>Katsuki</keyname><forenames>Takayuki</forenames></author><author><keyname>Inoue</keyname><forenames>Masato</forenames></author></authors><title>Posterior Mean Super-Resolution with a Compound Gaussian Markov Random
  Field Prior</title><categories>cs.CV</categories><comments>5 pages, 20 figures, 1 tables, accepted to ICASSP2012 (corrected
  2012/3/23)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This manuscript proposes a posterior mean (PM) super-resolution (SR) method
with a compound Gaussian Markov random field (MRF) prior. SR is a technique to
estimate a spatially high-resolution image from observed multiple
low-resolution images. A compound Gaussian MRF model provides a preferable
prior for natural images that preserves edges. PM is the optimal estimator for
the objective function of peak signal-to-noise ratio (PSNR). This estimator is
numerically determined by using variational Bayes (VB). We then solve the
conjugate prior problem on VB and the exponential-order calculation cost
problem of a compound Gaussian MRF prior with simple Taylor approximations. In
experiments, the proposed method roughly overcomes existing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0786</identifier>
 <datestamp>2012-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0786</id><created>2012-03-04</created><authors><author><keyname>Mahoney</keyname><forenames>Michael W.</forenames></author></authors><title>Approximate Computation and Implicit Regularization for Very Large-scale
  Data Analysis</title><categories>cs.DS stat.ML</categories><comments>To appear in the Proceedings of the 2012 ACM Symposium on Principles
  of Database Systems (PODS 2012)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Database theory and database practice are typically the domain of computer
scientists who adopt what may be termed an algorithmic perspective on their
data. This perspective is very different than the more statistical perspective
adopted by statisticians, scientific computers, machine learners, and other who
work on what may be broadly termed statistical data analysis. In this article,
I will address fundamental aspects of this algorithmic-statistical disconnect,
with an eye to bridging the gap between these two very different approaches. A
concept that lies at the heart of this disconnect is that of statistical
regularization, a notion that has to do with how robust is the output of an
algorithm to the noise properties of the input data. Although it is nearly
completely absent from computer science, which historically has taken the input
data as given and modeled algorithms discretely, regularization in one form or
another is central to nearly every application domain that applies algorithms
to noisy data. By using several case studies, I will illustrate, both
theoretically and empirically, the nonobvious fact that approximate
computation, in and of itself, can implicitly lead to statistical
regularization. This and other recent work suggests that, by exploiting in a
more principled way the statistical properties implicit in worst-case
algorithms, one can in many cases satisfy the bicriteria of having algorithms
that are scalable to very large-scale databases and that also have good
inferential or predictive properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0787</identifier>
 <datestamp>2012-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0787</id><created>2012-03-04</created><authors><author><keyname>Mahran</keyname><forenames>Ahmed M.</forenames></author></authors><title>A handy systematic method for data hazards detection in an instruction
  set of a pipelined microprocessor</title><categories>cs.AR</categories><comments>7 pages, 10 figures, 9 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is intended in this document to introduce a handy systematic method for
enumerating all possible data dependency cases that could occur between any two
instructions that might happen to be processed at the same time at different
stages of the pipeline. Given instructions of the instruction set, specific
information about operands of each instruction and when an instruction reads or
writes data, the method could be used to enumerate all possible data hazard
cases and to determine whether forwarding or stalling is suitable for resolving
each case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0788</identifier>
 <datestamp>2012-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0788</id><created>2012-03-04</created><authors><author><keyname>Suchecki</keyname><forenames>Krzysztof</forenames></author><author><keyname>Salah</keyname><forenames>Alkim Almila Akdag</forenames></author><author><keyname>Gao</keyname><forenames>Cheng</forenames></author><author><keyname>Scharnhorst</keyname><forenames>Andrea</forenames></author></authors><title>Evolution of Wikipedia's Category Structure</title><categories>physics.soc-ph cs.DL cs.SI</categories><comments>Preprint of an article submitted for consideration in Advances in
  Complex Systems (2012) http://www.worldscinet.com/acs/, 19 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wikipedia, as a social phenomenon of collaborative knowledge creating, has
been studied extensively from various points of views. The category system of
Wikipedia, introduced in 2004, has attracted relatively little attention. In
this study, we focus on the documentation of knowledge, and the transformation
of this documentation with time. We take Wikipedia as a proxy for knowledge in
general and its category system as an aspect of the structure of this
knowledge. We investigate the evolution of the category structure of the
English Wikipedia from its birth in 2004 to 2008. We treat the category system
as if it is a hierarchical Knowledge Organization System, capturing the changes
in the distributions of the top categories. We investigate how the clustering
of articles, defined by the category system, matches the direct link network
between the articles and show how it changes over time. We find the Wikipedia
category network mostly stable, but with occasional reorganization. We show
that the clustering matches the link structure quite well, except short periods
preceding the reorganizations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0833</identifier>
 <datestamp>2012-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0833</id><created>2012-03-05</created><updated>2012-03-12</updated><authors><author><keyname>Lokshtanov</keyname><forenames>Daniel</forenames></author><author><keyname>Narayanaswamy</keyname><forenames>N. S.</forenames></author><author><keyname>Raman</keyname><forenames>Venkatesh</forenames></author><author><keyname>Ramanujan</keyname><forenames>M. S.</forenames></author><author><keyname>Saurabh</keyname><forenames>Saket</forenames></author></authors><title>Faster Parameterized Algorithms using Linear Programming</title><categories>cs.DS cs.CC cs.DM</categories><comments>A preliminary version of this paper appears in the proceedings of
  STACS 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the parameterized complexity of Vertex Cover parameterized by
the difference between the size of the optimal solution and the value of the
linear programming (LP) relaxation of the problem. By carefully analyzing the
change in the LP value in the branching steps, we argue that combining
previously known preprocessing rules with the most straightforward branching
algorithm yields an $O^*((2.618)^k)$ algorithm for the problem. Here $k$ is the
excess of the vertex cover size over the LP optimum, and we write $O^*(f(k))$
for a time complexity of the form $O(f(k)n^{O(1)})$, where $f (k)$ grows
exponentially with $k$. We proceed to show that a more sophisticated branching
algorithm achieves a runtime of $O^*(2.3146^k)$.
  Following this, using known and new reductions, we give $O^*(2.3146^k)$
algorithms for the parameterized versions of Above Guarantee Vertex Cover, Odd
Cycle Transversal, Split Vertex Deletion and Almost 2-SAT, and an
$O^*(1.5214^k)$ algorithm for Ko\&quot;nig Vertex Deletion, Vertex Cover Param by
OCT and Vertex Cover Param by KVD. These algorithms significantly improve the
best known bounds for these problems. The most notable improvement is the new
bound for Odd Cycle Transversal - this is the first algorithm which beats the
dependence on $k$ of the seminal $O^*(3^k)$ algorithm of Reed, Smith and Vetta.
Finally, using our algorithm, we obtain a kernel for the standard
parameterization of Vertex Cover with at most $2k - c \log k$ vertices. Our
kernel is simpler than previously known kernels achieving the same size bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0835</identifier>
 <datestamp>2012-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0835</id><created>2012-03-05</created><authors><author><keyname>de Haan</keyname><forenames>Ronald</forenames></author></authors><title>Functional Logic Programming with Generalized Circular Coinduction</title><categories>cs.PL cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a method to adapt functional logic programming to deal with
reasoning on coinductively interpreted programs as well as on inductively
interpreted programs. In order to do so, we consider a class of objects
interesting for this coinductive interpretation, namely regular terms. We show
how the usual data structures can be adapted to capture these objects. We adapt
the operational semantics of Curry to interpret programs coinductively. We
illustrate this method with several examples that show the working of our
method and several cases in which it could be useful. Finally, we suggest how
the declarative semantics can be adapted suitably.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0840</identifier>
 <datestamp>2012-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0840</id><created>2012-03-05</created><authors><author><keyname>Dong</keyname><forenames>Guanghua</forenames></author><author><keyname>Wang</keyname><forenames>Ning</forenames></author><author><keyname>Huang</keyname><forenames>Yuanqiu</forenames></author><author><keyname>Ren</keyname><forenames>Han</forenames></author><author><keyname>Liu</keyname><forenames>Yanpei</forenames></author></authors><title>Vertex Splitting and Upper Embeddable Graphs</title><categories>math.CO cs.DM</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The weak minor G of a graph G is the graph obtained from G by a sequence of
edge-contraction operations on G. A weak-minor-closed family of upper
embeddable graphs is a set G of upper embeddable graphs that for each graph G
in G, every weak minor of G is also in G. Up to now, there are few results
providing the necessary and sufficient conditions for characterizing upper
embeddability of graphs. In this paper, we studied the relation between the
vertex splitting operation and the upper embeddability of graphs; provided not
only a necessary and sufficient condition for characterizing upper
embeddability of graphs, but also a way to construct weak-minor-closed family
of upper embeddable graphs from the bouquet of circles; extended a result in J:
Graph Theory obtained by L. Nebesk{\P}y. In addition, the algorithm complex of
determining the upper embeddability of a graph can be reduced much by the
results obtained in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0841</identifier>
 <datestamp>2012-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0841</id><created>2012-03-05</created><authors><author><keyname>Didehvar</keyname><forenames>Farzad</forenames></author><author><keyname>Mansouri</keyname><forenames>Mohsen</forenames></author><author><keyname>Taheri</keyname><forenames>Zahra</forenames></author></authors><title>How much could we cover a set by c.e sets?</title><categories>cs.FL cs.CC</categories><comments>5 pages</comments><msc-class>68Q45</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  &quot;How much c.e. sets could cover a given set?&quot; in this paper we are going to
answer this question. Also, in this approach some old concepts come into a new
arrangement. The major goal of this article is to introduce an appropriate
definition for this purpose. Introduction In Computability Theory (Recursion
Theory) in the first step we wish to recognize the sets which could be
enumerated by Turing machines (equivalently, algorithms) and in the next step
we will compare these sets by some reasonable order (Like Turing degree). Also
sometimes with some extra information (Oracles) a class of non c.e. sets show
the same behavior as c.e. sets (Post hierarchy and related theorems). Here we
try another approach: &quot;Let A be an arbitrary set and we wish to recognize how
much this set might be covered by a c.e. set?&quot; Although in some sense this
approach could be seen in some definitions of Recursion Theory, but at the best
of our knowledge it didn't considered as an approach yet, even though it is
able to shed a light on some subjects of Computability of sets. Defining this
approach is not quite straightforward and there are some obstacles to define
them. To overcome these difficulties we modify the definitions. We have an
alternative problem here when we consider recursive sets and not c.e. sets. In
this case, the problem would be: &quot;Let A be an arbitrary set and we wish to know
that how much this set might be covered by a recursive Set?&quot; Here, we try the
first definition and the first problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0856</identifier>
 <datestamp>2012-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0856</id><created>2012-03-05</created><authors><author><keyname>Kong</keyname><forenames>Shu</forenames></author><author><keyname>Wang</keyname><forenames>Donghui</forenames></author></authors><title>Online Discriminative Dictionary Learning for Image Classification Based
  on Block-Coordinate Descent Method</title><categories>cs.CV</categories><comments>This paper was completed in Dec 2010, and submitted (unsuccessfully)
  to ICCV2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Previous researches have demonstrated that the framework of dictionary
learning with sparse coding, in which signals are decomposed as linear
combinations of a few atoms of a learned dictionary, is well adept to
reconstruction issues. This framework has also been used for discrimination
tasks such as image classification. To achieve better performances of
classification, experts develop several methods to learn a discriminative
dictionary in a supervised manner. However, another issue is that when the data
become extremely large in scale, these methods will be no longer effective as
they are all batch-oriented approaches. For this reason, we propose a novel
online algorithm for discriminative dictionary learning, dubbed \textbf{ODDL}
in this paper. First, we introduce a linear classifier into the conventional
dictionary learning formulation and derive a discriminative dictionary learning
problem. Then, we exploit an online algorithm to solve the derived problem.
Unlike the most existing approaches which update dictionary and classifier
alternately via iteratively solving sub-problems, our approach directly
explores them jointly. Meanwhile, it can largely shorten the runtime for
training and is also particularly suitable for large-scale classification
issues. To evaluate the performance of the proposed ODDL approach in image
recognition, we conduct some experiments on three well-known benchmarks, and
the experimental results demonstrate ODDL is fairly promising for image
classification tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0871</identifier>
 <datestamp>2013-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0871</id><created>2012-03-05</created><updated>2013-05-21</updated><authors><author><keyname>Galliani</keyname><forenames>Pietro</forenames></author></authors><title>Transition Semantics - The Dynamics of Dependence Logic</title><categories>math.LO cs.LO</categories><msc-class>03B60, 03B70, 03B80</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We examine the relationship between Dependence Logic and game logics. A
variant of Dynamic Game Logic, called Transition Logic, is developed, and we
show that its relationship with Dependence Logic is comparable to the one
between First-Order Logic and Dynamic Game Logic discussed by van Benthem. This
suggests a new perspective on the interpretation of Dependence Logic formulas,
in terms of assertions about reachability in games of im- perfect information
against Nature. We then capitalize on this intuition by developing expressively
equivalent variants of Dependence Logic in which this interpretation is taken
to the foreground.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0876</identifier>
 <datestamp>2012-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0876</id><created>2012-03-05</created><authors><author><keyname>Basu</keyname><forenames>Subhadip</forenames></author><author><keyname>Das</keyname><forenames>Nibaran</forenames></author><author><keyname>Sarkar</keyname><forenames>Ram</forenames></author><author><keyname>Kundu</keyname><forenames>Mahantapas</forenames></author><author><keyname>Nasipuri</keyname><forenames>Mita</forenames></author><author><keyname>Basu</keyname><forenames>Dipak Kumar</forenames></author></authors><title>An MLP based Approach for Recognition of Handwritten `Bangla' Numerals</title><categories>cs.CV cs.AI</categories><journal-ref>Proc. 2nd Indian International Conference on Artificial
  Intelligence, pp. 407-417, Dec. 2005, Pune</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The work presented here involves the design of a Multi Layer Perceptron (MLP)
based pattern classifier for recognition of handwritten Bangla digits using a
76 element feature vector. Bangla is the second most popular script and
language in the Indian subcontinent and the fifth most popular language in the
world. The feature set developed for representing handwritten Bangla numerals
here includes 24 shadow features, 16 centroid features and 36 longest-run
features. On experimentation with a database of 6000 samples, the technique
yields an average recognition rate of 96.67% evaluated after three-fold cross
validation of results. It is useful for applications related to OCR of
handwritten Bangla Digit and can also be extended to include OCR of handwritten
characters of Bangla alphabet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0882</identifier>
 <datestamp>2012-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0882</id><created>2012-03-05</created><authors><author><keyname>Basu</keyname><forenames>Subhadip</forenames></author><author><keyname>Das</keyname><forenames>Nibaran</forenames></author><author><keyname>Sarkar</keyname><forenames>Ram</forenames></author><author><keyname>Kundu</keyname><forenames>Mahantapas</forenames></author><author><keyname>Nasipuri</keyname><forenames>Mita</forenames></author><author><keyname>Basu</keyname><forenames>Dipak Kumar</forenames></author></authors><title>Handwritten Bangla Alphabet Recognition using an MLP Based Classifier</title><categories>cs.CV cs.AI</categories><journal-ref>Proc. of the 2nd National Conf. on Computer Processing of Bangla,
  pp. 285-291, Feb-2005, Dhaka</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The work presented here involves the design of a Multi Layer Perceptron (MLP)
based classifier for recognition of handwritten Bangla alphabet using a 76
element feature set Bangla is the second most popular script and language in
the Indian subcontinent and the fifth most popular language in the world. The
feature set developed for representing handwritten characters of Bangla
alphabet includes 24 shadow features, 16 centroid features and 36 longest-run
features. Recognition performances of the MLP designed to work with this
feature set are experimentally observed as 86.46% and 75.05% on the samples of
the training and the test sets respectively. The work has useful application in
the development of a complete OCR system for handwritten Bangla text.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0889</identifier>
 <datestamp>2012-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0889</id><created>2012-03-05</created><authors><author><keyname>Ltaief</keyname><forenames>Hatem</forenames></author><author><keyname>Yokota</keyname><forenames>Rio</forenames></author></authors><title>Data-Driven Execution of Fast Multipole Methods</title><categories>cs.NA</categories><msc-class>70F10</msc-class><acm-class>D.1.2; D.1.3; G.1.0; G.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fast multipole methods have O(N) complexity, are compute bound, and require
very little synchronization, which makes them a favorable algorithm on
next-generation supercomputers. Their most common application is to accelerate
N-body problems, but they can also be used to solve boundary integral
equations. When the particle distribution is irregular and the tree structure
is adaptive, load-balancing becomes a non-trivial question. A common strategy
for load-balancing FMMs is to use the work load from the previous step as
weights to statically repartition the next step. The authors discuss in the
paper another approach based on data-driven execution to efficiently tackle
this challenging load-balancing problem. The core idea consists of breaking the
most time-consuming stages of the FMMs into smaller tasks. The algorithm can
then be represented as a Directed Acyclic Graph (DAG) where nodes represent
tasks, and edges represent dependencies among them. The execution of the
algorithm is performed by asynchronously scheduling the tasks using the QUARK
runtime environment, in a way such that data dependencies are not violated for
numerical correctness purposes. This asynchronous scheduling results in an
out-of-order execution. The performance results of the data-driven FMM
execution outperform the previous strategy and show linear speedup on a
quad-socket quad-core Intel Xeon system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0905</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0905</id><created>2012-03-05</created><updated>2014-01-10</updated><authors><author><keyname>Ronda</keyname><forenames>Jos&#xe9; I.</forenames></author><author><keyname>Vald&#xe9;s</keyname><forenames>Antonio</forenames></author><author><keyname>Gallego</keyname><forenames>Guillermo</forenames></author></authors><title>Autocalibration with the Minimum Number of Cameras with Known Pixel
  Shape</title><categories>cs.CV</categories><comments>19 pages, 14 figures, 7 tables, J. Math. Imaging Vis</comments><doi>10.1007/s10851-014-0492-5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In 3D reconstruction, the recovery of the calibration parameters of the
cameras is paramount since it provides metric information about the observed
scene, e.g., measures of angles and ratios of distances. Autocalibration
enables the estimation of the camera parameters without using a calibration
device, but by enforcing simple constraints on the camera parameters. In the
absence of information about the internal camera parameters such as the focal
length and the principal point, the knowledge of the camera pixel shape is
usually the only available constraint. Given a projective reconstruction of a
rigid scene, we address the problem of the autocalibration of a minimal set of
cameras with known pixel shape and otherwise arbitrarily varying intrinsic and
extrinsic parameters. We propose an algorithm that only requires 5 cameras (the
theoretical minimum), thus halving the number of cameras required by previous
algorithms based on the same constraint. To this purpose, we introduce as our
basic geometric tool the six-line conic variety (SLCV), consisting in the set
of planes intersecting six given lines of 3D space in points of a conic. We
show that the set of solutions of the Euclidean upgrading problem for three
cameras with known pixel shape can be parameterized in a computationally
efficient way. This parameterization is then used to solve autocalibration from
five or more cameras, reducing the three-dimensional search space to a
two-dimensional one. We provide experiments with real images showing the good
performance of the technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0912</identifier>
 <datestamp>2012-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0912</id><created>2012-03-05</created><authors><author><keyname>Soare</keyname><forenames>Ionica</forenames></author><author><keyname>Antohe</keyname><forenames>Carmen</forenames></author></authors><title>Modeling the geographical studies with GeoGebra-software</title><categories>cs.CG</categories><comments>8 pages</comments><journal-ref>Ann. Univ. Tibiscus Comp. Sci. Series VIII / 1 (2010), 173-180</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of mathematical modeling in geography is one of the most
important strategies in order to establish the evolution and the prevision of
geographical phenomena. Models must have a simplified structure, to reflect
essential components and must be selective, structured, and suggestive and
approximate the reality. Models could be static or dynamic, developed in a
theoretical, symbolic, conceptual or mental way, mathematically modeled. The
present paper is focused on the virtual model which uses GeoGebra software,
free and available at www.geogebra.org, in order to establish new methods of
geographical analysis in a dynamic, didactic way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0920</identifier>
 <datestamp>2013-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0920</id><created>2012-03-05</created><updated>2013-01-21</updated><authors><author><keyname>Bortolussi</keyname><forenames>Luca</forenames></author><author><keyname>Hillston</keyname><forenames>Jane</forenames></author></authors><title>Fluid Model Checking</title><categories>cs.LO cs.PF</categories><acm-class>D.2.4; D.2.8; I.6.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we investigate a potential use of fluid approximation
techniques in the context of stochastic model checking of CSL formulae. We
focus on properties describing the behaviour of a single agent in a (large)
population of agents, exploiting a limit result known also as fast simulation.
In particular, we will approximate the behaviour of a single agent with a
time-inhomogeneous CTMC which depends on the environment and on the other
agents only through the solution of the fluid differential equation. We will
prove the asymptotic correctness of our approach in terms of satisfiability of
CSL formulae and of reachability probabilities. We will also present a
procedure to model check time-inhomogeneous CTMC against CSL formulae.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0924</identifier>
 <datestamp>2012-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0924</id><created>2012-03-05</created><updated>2012-06-01</updated><authors><author><keyname>B&#xf6;cherer</keyname><forenames>Georg</forenames></author><author><keyname>Altenbach</keyname><forenames>Fabian</forenames></author><author><keyname>Alvarado</keyname><forenames>Alex</forenames></author><author><keyname>Corroy</keyname><forenames>Steven</forenames></author><author><keyname>Mathar</keyname><forenames>Rudolf</forenames></author></authors><title>An Efficient Algorithm to Calculate BICM Capacity</title><categories>cs.IT math.IT</categories><comments>5 pages, to be presented at ISIT 2012. Compared to version v1,
  several parts were clarified</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bit-interleaved coded modulation (BICM) is a practical approach for reliable
communication over the AWGN channel in the bandwidth limited regime. For a
signal point constellation with 2^m points, BICM labels the signal points with
bit strings of length m and then treats these m bits separately both at the
transmitter and the receiver. BICM capacity is defined as the maximum of a
certain achievable rate. Maximization has to be done over the probability mass
functions (pmf) of the bits. This is a non-convex optimization problem. So far,
the optimal bit pmfs were determined via exhaustive search, which is of
exponential complexity in m. In this work, an algorithm called bit-alternating
convex concave method (Bacm) is developed. This algorithm calculates BICM
capacity with a complexity that scales approximately as m^3. The algorithm
iteratively applies convex optimization techniques. Bacm is used to calculate
BICM capacity of 4,8,16,32, and 64-PAM in AWGN. For PAM constellations with
more than 8 points, the presented values are the first results known in the
literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0960</identifier>
 <datestamp>2012-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0960</id><created>2012-03-05</created><authors><author><keyname>Suthisopapan</keyname><forenames>Puripong</forenames></author><author><keyname>Kasai</keyname><forenames>Kenta</forenames></author><author><keyname>Meesomboon</keyname><forenames>Anupap</forenames></author><author><keyname>Imtawil</keyname><forenames>Virasit</forenames></author></authors><title>Near Capacity Approaching for Large MIMO Systems by Non-Binary LDPC
  Codes with MMSE Detection</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we have investigated the application of non-binary LDPC codes
to spatial multiplexing MIMO systems with a large number of low power antennas.
We demonstrate that such large MIMO systems incorporating with low-complexity
MMSE detector and non-binary LDPC codes can achieve low probability of bit
error at near MIMO capacity. The new proposed non-binary LDPC coded system also
performs better than other coded large MIMO systems known in the present
literature. For instance, non-binary LDPC coded BPSK-MIMO system with 600
transmit/receive antennas performs within 3.4 dB from the capacity while the
best known turbo coded system operates about 9.4 dB away from the capacity.
Based on the simulation results provided in this paper, the proposed non-binary
LDPC coded large MIMO system is capable of supporting ultra high spectral
efficiency at low bit error rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0964</identifier>
 <datestamp>2012-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0964</id><created>2012-02-12</created><authors><author><keyname>Abreu</keyname><forenames>Fernando Brito e</forenames></author></authors><title>The cloud paradigm: Are you tuned for the lyrics?</title><categories>cs.DC cs.SE</categories><comments>Position paper to introduce a keynote, proceedings of WAMPS'2011 - VI
  Annual MPS.BR Workshop, pp. 20-25, Campinas, Brazil, October 2011</comments><acm-class>A.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Major players, business angels and opinion-makers are broadcasting beguiled
lyrics on the most recent IT hype: your software should ascend to the clouds.
There are many clouds and the stake is high. Distractedly, many of us became
assiduous users of the cloud, but perhaps due to the legacy systems and legacy
knowledge, IT professionals, mainly those many that work in business
information systems for the long tail, are not as much plunged into producing
cloud-based systems for their clients.
  This keynote will delve into several aspects of this cloud paradigm, from
more generic concerns regarding security and value for money, to more specific
worries that reach software engineers in general. Do we need a different
software development process? Are development techniques and tools mature
enough? What about the role of open-source in the cloud? How do we assess the
quality in cloud-based development? Please stay tuned for more!
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.0970</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.0970</id><created>2012-03-05</created><updated>2013-05-20</updated><authors><author><keyname>Wang</keyname><forenames>Yuyang</forenames></author><author><keyname>Khardon</keyname><forenames>Roni</forenames></author><author><keyname>Protopapas</keyname><forenames>Pavlos</forenames></author></authors><title>Infinite Shift-invariant Grouped Multi-task Learning for Gaussian
  Processes</title><categories>cs.LG astro-ph.IM stat.ML</categories><comments>This is an extended version of our ECML 2010 paper entitled
  &quot;Shift-invariant Grouped Multi-task Learning for Gaussian Processes&quot;; ECML
  PKDD'10 Proceedings of the 2010 European conference on Machine learning and
  knowledge discovery in databases: Part III</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-task learning leverages shared information among data sets to improve
the learning performance of individual tasks. The paper applies this framework
for data where each task is a phase-shifted periodic time series. In
particular, we develop a novel Bayesian nonparametric model capturing a mixture
of Gaussian processes where each task is a sum of a group-specific function and
a component capturing individual variation, in addition to each task being
phase shifted. We develop an efficient \textsc{em} algorithm to learn the
parameters of the model. As a special case we obtain the Gaussian mixture model
and \textsc{em} algorithm for phased-shifted periodic time series. Furthermore,
we extend the proposed model by using a Dirichlet Process prior and thereby
leading to an infinite mixture model that is capable of doing automatic model
selection. A Variational Bayesian approach is developed for inference in this
model. Experiments in regression, classification and class discovery
demonstrate the performance of the proposed models using both synthetic data
and real-world time series data from astrophysics. Our methods are particularly
useful when the time series are sparsely and non-synchronously sampled.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1005</identifier>
 <datestamp>2013-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1005</id><created>2012-03-05</created><updated>2013-02-04</updated><authors><author><keyname>Elhamifar</keyname><forenames>Ehsan</forenames></author><author><keyname>Vidal</keyname><forenames>Rene</forenames></author></authors><title>Sparse Subspace Clustering: Algorithm, Theory, and Applications</title><categories>cs.CV cs.IR cs.IT cs.LG math.IT math.OC stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many real-world problems, we are dealing with collections of
high-dimensional data, such as images, videos, text and web documents, DNA
microarray data, and more. Often, high-dimensional data lie close to
low-dimensional structures corresponding to several classes or categories the
data belongs to. In this paper, we propose and study an algorithm, called
Sparse Subspace Clustering (SSC), to cluster data points that lie in a union of
low-dimensional subspaces. The key idea is that, among infinitely many possible
representations of a data point in terms of other points, a sparse
representation corresponds to selecting a few points from the same subspace.
This motivates solving a sparse optimization program whose solution is used in
a spectral clustering framework to infer the clustering of data into subspaces.
Since solving the sparse optimization program is in general NP-hard, we
consider a convex relaxation and show that, under appropriate conditions on the
arrangement of subspaces and the distribution of data, the proposed
minimization program succeeds in recovering the desired sparse representations.
The proposed algorithm can be solved efficiently and can handle data points
near the intersections of subspaces. Another key advantage of the proposed
algorithm with respect to the state of the art is that it can deal with data
nuisances, such as noise, sparse outlying entries, and missing entries,
directly by incorporating the model of the data into the sparse optimization
program. We demonstrate the effectiveness of the proposed algorithm through
experiments on synthetic data as well as the two real-world problems of motion
segmentation and face clustering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1006</identifier>
 <datestamp>2012-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1006</id><created>2012-03-05</created><updated>2012-04-07</updated><authors><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author><author><keyname>Rotolo</keyname><forenames>Daniele</forenames></author><author><keyname>Rafols</keyname><forenames>Ismael</forenames></author></authors><title>Bibliometric Perspectives on Medical Innovation using the Medical
  Subject Headings (MeSH) of PubMed</title><categories>cs.DL</categories><comments>forthcoming in the Journal of the American Society for Information
  Science and Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiple perspectives on the nonlinear processes of medical innovations can
be distinguished and combined using the Medical Subject Headings (MeSH) of the
Medline database. Focusing on three main branches-&quot;diseases,&quot; &quot;drugs and
chemicals,&quot; and &quot;techniques and equipment&quot;-we use base maps and overlay
techniques to investigate the translations and interactions and thus to gain a
bibliometric perspective on the dynamics of medical innovations. To this end,
we first analyze the Medline database, the MeSH index tree, and the various
options for a static mapping from different perspectives and at different
levels of aggregation. Following a specific innovation (RNA interference) over
time, the notion of a trajectory which leaves a signature in the database is
elaborated. Can the detailed index terms describing the dynamics of research be
used to predict the diffusion dynamics of research results? Possibilities are
specified for further integration between the Medline database, on the one
hand, and the Science Citation Index and Scopus (containing citation
information), on the other.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1007</identifier>
 <datestamp>2012-07-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1007</id><created>2012-03-05</created><updated>2012-07-03</updated><authors><author><keyname>Ross</keyname><forenames>Stephane</forenames></author><author><keyname>Bagnell</keyname><forenames>J. Andrew</forenames></author></authors><title>Agnostic System Identification for Model-Based Reinforcement Learning</title><categories>cs.LG cs.AI cs.SY stat.ML</categories><comments>8 pages, published in ICML 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A fundamental problem in control is to learn a model of a system from
observations that is useful for controller synthesis. To provide good
performance guarantees, existing methods must assume that the real system is in
the class of models considered during learning. We present an iterative method
with strong guarantees even in the agnostic case where the system is not in the
class. In particular, we show that any no-regret online learning algorithm can
be used to obtain a near-optimal policy, provided some model achieves low
training error and access to a good exploration distribution. Our approach
applies to both discrete and continuous domains. We demonstrate its efficacy
and scalability on a challenging helicopter domain from the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1017</identifier>
 <datestamp>2012-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1017</id><created>2012-03-05</created><authors><author><keyname>Diochnos</keyname><forenames>Dimitrios I.</forenames></author><author><keyname>Emiris</keyname><forenames>Ioannis Z.</forenames></author><author><keyname>Tsigaridas</keyname><forenames>Elias P.</forenames></author></authors><title>On the asymptotic and practical complexity of solving bivariate systems
  over the reals</title><categories>cs.SC cs.DS cs.MS cs.NA math.AG math.NA</categories><comments>17 pages, 4 algorithms, 1 table, and 1 figure with 2 sub-figures</comments><journal-ref>J. Symb. Comput. 44(7): 818-835 (2009)</journal-ref><doi>10.1016/j.jsc.2008.04.009</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is concerned with exact real solving of well-constrained,
bivariate polynomial systems. The main problem is to isolate all common real
roots in rational rectangles, and to determine their intersection
multiplicities. We present three algorithms and analyze their asymptotic bit
complexity, obtaining a bound of $\sOB(N^{14})$ for the purely projection-based
method, and $\sOB(N^{12})$ for two subresultant-based methods: this notation
ignores polylogarithmic factors, where $N$ bounds the degree and the bitsize of
the polynomials. The previous record bound was $\sOB(N^{14})$.
  Our main tool is signed subresultant sequences. We exploit recent advances on
the complexity of univariate root isolation, and extend them to sign evaluation
of bivariate polynomials over two algebraic numbers, and real root counting for
polynomials over an extension field. Our algorithms apply to the problem of
simultaneous inequalities; they also compute the topology of real plane
algebraic curves in $\sOB(N^{12})$, whereas the previous bound was
$\sOB(N^{14})$.
  All algorithms have been implemented in MAPLE, in conjunction with numeric
filtering. We compare them against FGB/RS, system solvers from SYNAPS, and
MAPLE libraries INSULATE and TOP, which compute curve topology. Our software is
among the most robust, and its runtimes are comparable, or within a small
constant factor, with respect to the C/C++ libraries.
  Key words: real solving, polynomial systems, complexity, MAPLE software
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1021</identifier>
 <datestamp>2012-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1021</id><created>2012-03-05</created><authors><author><keyname>Maalel</keyname><forenames>Ahmed</forenames></author><author><keyname>mabrouk</keyname><forenames>Habib Hadj</forenames></author><author><keyname>Mejri</keyname><forenames>Lassad</forenames></author><author><keyname>Ghezela</keyname><forenames>Henda Hajjami Ben</forenames></author></authors><title>Development of an Ontology to Assist the Modeling of Accident Scenarii
  &quot;Application on Railroad Transport &quot;</title><categories>cs.AI</categories><comments>7 pages, 9 figures, Journal of Computing (ISSN 2151-9617); Journal of
  Computing, Volume 3, Issue 7, July 2011</comments><journal-ref>J. of Computing. 3. 7. (2011) 1-8</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a world where communication and information sharing are at the heart of
our business, the terminology needs are most pressing. It has become imperative
to identify the terms used and defined in a consensual and coherent way while
preserving linguistic diversity. To streamline and strengthen the process of
acquisition, representation and exploitation of scenarii of train accidents, it
is necessary to harmonize and standardize the terminology used by players in
the security field. The research aims to significantly improve analytical
activities and operations of the various safety studies, by tracking the error
in system, hardware, software and human. This paper presents the contribution
of ontology to modeling scenarii for rail accidents through a knowledge model
based on a generic ontology and domain ontology. After a detailed presentation
of the state of the art material, this article presents the first results of
the developed model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1023</identifier>
 <datestamp>2012-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1023</id><created>2012-03-05</created><authors><author><keyname>Stoutemyer</keyname><forenames>David R.</forenames></author></authors><title>Can the Eureqa symbolic regression program, computer algebra and
  numerical analysis help each other?</title><categories>cs.MS cs.SC</categories><comments>21 pages, 3 figures, a Mathematica notebook attachment</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Eureqa symbolic regression program has recently received extensive press
praise. A representative quote is
  &quot;There are very clever 'thinking machines' in existence today, such as
Watson, the IBM computer that conquered Jeopardy! last year. But next to
Eureqa, Watson is merely a glorified search engine.&quot;
  The program was designed to work with noisy experimental data. However, if
the data is generated from an expression for which there exists more concise
equivalent expressions, sometimes some of the Eureqa results are one or more of
those more concise equivalents. If not, perhaps one or more of the returned
Eureqa results might be a sufficiently accurate approximation that is more
concise than the given expression. Moreover, when there is no known closed form
expression, the data points can be generated by numerical methods, enabling
Eureqa to find expressions that concisely fit those data points with sufficient
accuracy. In contrast to typical regression software, the user does not have to
explicitly or implicitly provide a specific expression or class of expressions
containiing unknown constants for the software to determine.
  Is Eureqa useful enough in these regards to provide an additional tool for
experimental mathematics, computer algebra users and numerical analysis? Yes if
used carefully. Can computer algebra and numerical methods help Eureqa?
Definitely.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1034</identifier>
 <datestamp>2012-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1034</id><created>2012-03-05</created><authors><author><keyname>Skowron</keyname><forenames>J.</forenames><affiliation>Department of Astronomy, Ohio State University</affiliation></author><author><keyname>Gould</keyname><forenames>A.</forenames><affiliation>Department of Astronomy, Ohio State University</affiliation></author></authors><title>General Complex Polynomial Root Solver and Its Further Optimization for
  Binary Microlenses</title><categories>astro-ph.EP cs.MS math.NA</categories><comments>29 pages, 4 figures, 1 table, 3 appendices. Open-source codes
  described in this paper are available in the ancillary files directory (anc/)
  of this submission, and on the author's web page:
  http://www.astrouw.edu.pl/~jskowron/cmplx_roots_sg/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new algorithm to solve polynomial equations, and publish its
code, which is 1.6-3 times faster than the ZROOTS subroutine that is
commercially available from Numerical Recipes, depending on application. The
largest improvement, when compared to naive solvers, comes from a fail-safe
procedure that permits us to skip the majority of the calculations in the great
majority of cases, without risking catastrophic failure in the few cases that
these are actually required. Second, we identify a discriminant that enables a
rational choice between Laguerre's Method and Newton's Method (or a new
intermediate method) on a case-by-case basis. We briefly review the history of
root solving and demonstrate that &quot;Newton's Method&quot; was discovered neither by
Newton (1671) nor by Raphson (1690), but only by Simpson (1740). Some of the
arguments leading to this conclusion were first given by the British historian
of science Nick Kollerstrom in 1992, but these do not appear to have penetrated
the astronomical community. Finally, we argue that Numerical Recipes should
voluntarily surrender its copyright protection for non-profit applications,
despite the fact that, in this particular case, such protection was the major
stimulant for developing our improved algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1042</identifier>
 <datestamp>2012-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1042</id><created>2012-03-05</created><authors><author><keyname>Gupta</keyname><forenames>Sandeep</forenames></author><author><keyname>Ravishankar</keyname><forenames>Chinya</forenames></author></authors><title>Lower bounds for Arrangement-based Range-Free Localization in Sensor
  Networks</title><categories>cs.CG cs.CC</categories><acm-class>F.2.2; G.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Colander are location aware entities that collaborate to determine
approximate location of mobile or static objects when beacons from an object
are received by all colanders that are within its distance $R$. This model,
referred to as arrangement-based localization, does not require distance
estimation between entities, which has been shown to be highly erroneous in
practice. Colander are applicable in localization in sensor networks and
tracking of mobile objects.
  A set $S \subset {\mathbb R}^2$ is an $(R,\epsilon)$-colander if by placing
receivers at the points of $S$, a wireless device with transmission radius $R$
can be localized to within a circle of radius $\epsilon$. We present tight
upper and lower bounds on the size of $(R,\epsilon)$-colanders. We measure the
expected size of colanders that will form $(R, \epsilon)$-colanders if they
distributed uniformly over the plane.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1069</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1069</id><created>2012-03-05</created><updated>2012-03-10</updated><authors><author><keyname>Borri</keyname><forenames>Alessandro</forenames></author><author><keyname>Pola</keyname><forenames>Giordano</forenames></author><author><keyname>Di Benedetto</keyname><forenames>Maria Domenica</forenames></author></authors><title>A Symbolic Approach to the Design of Nonlinear Networked Control Systems</title><categories>cs.SY</categories><comments>To appear in HSCC'12, April 17--19, 2012, Beijing, China</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Networked control systems (NCS) are spatially distributed systems where
communication among plants, sensors, actuators and controllers occurs in a
shared communication network. NCS have been studied for the last ten years and
important research results have been obtained. These results are in the area of
stability and stabilizability. However, while important, these results must be
complemented in different areas to be able to design effective NCS. In this
paper we approach the control design of NCS using symbolic (finite) models.
Symbolic models are abstract descriptions of continuous systems where one
symbol corresponds to an &quot;aggregate&quot; of continuous states. We consider a fairly
general multiple-loop network architecture where plants communicate with
digital controllers through a shared, non-ideal, communication network
characterized by variable sampling and transmission intervals, variable
communication delays, quantization errors, packet losses and limited bandwidth.
We first derive a procedure to obtain symbolic models that are proven to
approximate NCS in the sense of alternating approximate bisimulation. We then
use these symbolic models to design symbolic controllers that realize
specifications expressed in terms of automata on infinite strings. An example
is provided where we address the control design of a pair of nonlinear control
systems sharing a common communication network. The closed-loop NCS obtained is
validated through the OMNeT++ network simulation framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1080</identifier>
 <datestamp>2012-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1080</id><created>2012-03-05</created><updated>2012-05-03</updated><authors><author><keyname>Chen</keyname><forenames>Shiteng</forenames></author><author><keyname>Verbin</keyname><forenames>Elad</forenames></author><author><keyname>Yu</keyname><forenames>Wei</forenames></author></authors><title>Data Structure Lower Bounds on Random Access to Grammar-Compressed
  Strings</title><categories>cs.CC cs.DS</categories><comments>submitted to ICALP 2012, with strengthened results included</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we investigate the problem of building a static data structure
that represents a string s using space close to its compressed size, and allows
fast access to individual characters of s. This type of structures was
investigated by the recent paper of Bille et al. Let n be the size of a
context-free grammar that derives a unique string s of length L. (Note that L
might be exponential in n.) Bille et al. showed a data structure that uses
space O(n) and allows to query for the i-th character of s using running time
O(log L). Their data structure works on a word RAM with a word size of logL
bits. Here we prove that for such data structures, if the space is poly(n),
then the query time must be at least (log L)^{1-\epsilon}/log S where S is the
space used, for any constant eps&gt;0. As a function of n, our lower bound is
\Omega(n^{1/2-\epsilon}). Our proof holds in the cell-probe model with a word
size of log L bits, so in particular it holds in the word RAM model. We show
that no lower bound significantly better than n^{1/2-\epsilon} can be achieved
in the cell-probe model, since there is a data structure in the cell-probe
model that uses O(n) space and achieves O(\sqrt{n log n}) query time. The &quot;bad&quot;
setting of parameters occurs roughly when L=2^{\sqrt{n}}. We also prove a lower
bound for the case of not-as-compressible strings, where, say,
L=n^{1+\epsilon}. For this case, we prove that if the space is n polylog(n),
then the query time must be at least \Omega(log n/loglog n).
  The proof works by reduction to communication complexity, namely to the LSD
problem, recently employed by Patrascu and others. We prove lower bounds also
for the case of LZ-compression and Burrows-Wheeler (BWT) compression. All of
our lower bounds hold even when the strings are over an alphabet of size 2 and
hold even for randomized data structures with 2-sided error.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1084</identifier>
 <datestamp>2012-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1084</id><created>2012-03-05</created><authors><author><keyname>Hartke</keyname><forenames>Stephen G.</forenames></author><author><keyname>Stolee</keyname><forenames>Derrick</forenames></author></authors><title>Uniquely K_r-Saturated Graphs</title><categories>math.CO cs.DM</categories><comments>35 pages, 23 figures</comments><msc-class>05C75</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A graph G is uniquely K_r-saturated if it contains no clique with r vertices
and if for all edges e in the complement, G + e has a unique clique with r
vertices. Previously, few examples of uniquely K_r-saturated graphs were known,
and little was known about their properties. We search for these graphs by
adapting orbital branching, a technique originally developed for symmetric
integer linear programs. We find several new uniquely K_r-saturated graphs with
4 \leq r \leq 7, as well as two new infinite families based on Cayley graphs
for Z_n with a small number of generators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1095</identifier>
 <datestamp>2012-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1095</id><created>2012-03-05</created><authors><author><keyname>Schrijvers</keyname><forenames>Tom</forenames></author><author><keyname>Tack</keyname><forenames>Guido</forenames></author><author><keyname>Wuille</keyname><forenames>Pieter</forenames></author><author><keyname>Samulowitz</keyname><forenames>Horst</forenames></author><author><keyname>Stuckey</keyname><forenames>Peter J.</forenames></author></authors><title>Search Combinators</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ability to model search in a constraint solver can be an essential asset
for solving combinatorial problems. However, existing infrastructure for
defining search heuristics is often inadequate. Either modeling capabilities
are extremely limited or users are faced with a general-purpose programming
language whose features are not tailored towards writing search heuristics. As
a result, major improvements in performance may remain unexplored.
  This article introduces search combinators, a lightweight and
solver-independent method that bridges the gap between a conceptually simple
modeling language for search (high-level, functional and naturally
compositional) and an efficient implementation (low-level, imperative and
highly non-modular). By allowing the user to define application-tailored search
strategies from a small set of primitives, search combinators effectively
provide a rich domain-specific language (DSL) for modeling search to the user.
Remarkably, this DSL comes at a low implementation cost to the developer of a
constraint solver.
  The article discusses two modular implementation approaches and shows, by
empirical evaluation, that search combinators can be implemented without
overhead compared to a native, direct implementation in a constraint solver.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1097</identifier>
 <datestamp>2012-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1097</id><created>2012-03-05</created><authors><author><keyname>Bhorkar</keyname><forenames>Abhijeet</forenames></author><author><keyname>Naghshwar</keyname><forenames>Mohammad</forenames></author><author><keyname>Javidi</keyname><forenames>Tara</forenames></author></authors><title>Opportunistic Routing with Congestion Diversity in Wireless Ad-hoc
  Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of routing packets across a multi-hop network
consisting of multiple sources of traffic and wireless links while ensuring
bounded expected delay. Each packet transmission can be overheard by a random
subset of receiver nodes among which the next relay is selected
opportunistically.
  The main challenge in the design of minimum-delay routing policies is
balancing the trade-off between routing the packets along the shortest paths to
the destination and distributing traffic according to the maximum backpressure.
Combining important aspects of shortest path and backpressure routing, this
paper provides a systematic development of a distributed opportunistic routing
policy with congestion diversity ({D-ORCD}).
  {D-ORCD} uses a measure of draining time to opportunistically identify and
route packets along the paths with an expected low overall congestion. {D-ORCD}
is proved to ensure a bounded expected delay for all networks and under any
admissible traffic. Furthermore, this paper proposes a practical implementation
which empirically optimizes critical algorithm parameters and their effects on
delay as well as protocol overhead. Realistic Qualnet simulations for
802.11-based networks demonstrate a significant improvement in the average
delay over comparative solutions in the literature. %Finally, various practical
modifications to {D-ORCD} are proposed and their performance are evaluated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1105</identifier>
 <datestamp>2012-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1105</id><created>2012-03-06</created><authors><author><keyname>Xu</keyname><forenames>Xiao-Ke</forenames></author><author><keyname>Wang</keyname><forenames>Jian-Bo</forenames></author><author><keyname>Wu</keyname><forenames>Ye</forenames></author><author><keyname>Small</keyname><forenames>Michael</forenames></author></authors><title>Pairwise interaction pattern in the weighted communication network</title><categories>physics.soc-ph cs.SI</categories><comments>7 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although recent studies show that both topological structures and human
dynamics can strongly affect information spreading on social networks, the
complicated interplay of the two significant factors has not yet been clearly
described. In this work, we find a strong pairwise interaction based on
analyzing the weighted network generated by the short message communication
dataset within a Chinese tele-communication provider. The pairwise interaction
bridges the network topological structure and human interaction dynamics, which
can promote local information spreading between pairs of communication partners
and in contrast can also suppress global information (e.g., rumor) cascade and
spreading. In addition, the pairwise interaction is the basic pattern of group
conversations and it can greatly reduce the waiting time of communication
events between a pair of intimate friends. Our findings are also helpful for
communication operators to design novel tariff strategies and optimize their
communication services.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1107</identifier>
 <datestamp>2012-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1107</id><created>2012-03-06</created><authors><author><keyname>Ladley</keyname><forenames>Dan</forenames><affiliation>University of Leicester</affiliation></author><author><keyname>Wilkinson</keyname><forenames>Ian</forenames><affiliation>University of Sydney and University of Southern Denmark</affiliation></author><author><keyname>Young</keyname><forenames>Louise</forenames><affiliation>University of Western Sydney and University of Southern Denmark</affiliation></author></authors><title>The Evolution of Cooperation in Business</title><categories>cs.GT</categories><comments>35 pages, 2 tables, 2 figures under review</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The development of cooperative relations within and between firms plays an
important role in the successful implementation of business strategy. How to
produce such relations is less well understood. We build on work in relational
contract theory and the evolution of cooperation to examine the conditions
under which group based incentives outperform individual based incentives and
how they produce more cooperative behavior. Group interactions are modeled as
iterated games in which individuals learn optimal strategies under individual
and group based reward mechanisms. The space of possible games is examined and
it is found that, when individual and group interests are not aligned, group
evaluation and reward systems lead to higher group performance and,
counter-intuitively, higher individual performance. Such groups include
individuals who, quite differently to free-riders, sacrifice their own
performance for the good of the group. We discuss the implications of these
results for the design of incentive systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1122</identifier>
 <datestamp>2015-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1122</id><created>2012-03-06</created><updated>2014-05-02</updated><authors><author><keyname>Guha</keyname><forenames>Ashwin</forenames></author><author><keyname>Dukkipati</keyname><forenames>Ambedkar</forenames></author></authors><title>An Algorithmic Characterization of Polynomial Functions over $Z_{p^n}$</title><categories>cs.SC math.RA</categories><journal-ref>Algorithmica: Volume 71, Issue 1 (2015), Page 201-218</journal-ref><doi>10.1007/s00453-013-9799-7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider polynomial representability of functions defined
over $Z_{p^n}$, where $p$ is a prime and $n$ is a positive integer. Our aim is
to provide an algorithmic characterization that (i) answers the decision
problem: to determine whether a given function over $Z_{p^n}$ is polynomially
representable or not, and (ii) finds the polynomial if it is polynomially
representable. The previous characterizations given by Kempner (1921) and
Carlitz (1964) are existential in nature and only lead to an exhaustive search
method, i.e., algorithm with complexity exponential in size of the input. Our
characterization leads to an algorithm whose running time is linear in size of
input. We also extend our result to the multivariate case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1150</identifier>
 <datestamp>2012-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1150</id><created>2012-03-06</created><authors><author><keyname>Yuasa</keyname><forenames>Tomoyuki</forenames></author><author><keyname>Shirayama</keyname><forenames>Susumu</forenames></author></authors><title>A New Analysis Method for Simulations Using Node Categorizations</title><categories>cs.SI physics.soc-ph</categories><comments>9 pages, 8 figures. This paper will be published in Social Network
  Analysis and Mining(www.springerlink.com)</comments><doi>10.1007/s13278-012-0048-x</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most research concerning the influence of network structure on phenomena
taking place on the network focus on relationships between global statistics of
the network structure and characteristic properties of those phenomena, even
though local structure has a significant effect on the dynamics of some
phenomena. In the present paper, we propose a new analysis method for phenomena
on networks based on a categorization of nodes. First, local statistics such as
the average path length and the clustering coefficient for a node are
calculated and assigned to the respective node. Then, the nodes are categorized
using the self-organizing map (SOM) algorithm. Characteristic properties of the
phenomena of interest are visualized for each category of nodes. The validity
of our method is demonstrated using the results of two simulation models. The
proposed method is useful as a research tool to understand the behavior of
networks, in particular, for the large-scale networks that existing
visualization techniques cannot work well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1153</identifier>
 <datestamp>2012-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1153</id><created>2012-03-06</created><authors><author><keyname>Jain</keyname><forenames>Rahul</forenames></author><author><keyname>Shi</keyname><forenames>Yaoyun</forenames></author><author><keyname>Wei</keyname><forenames>Zhaohui</forenames></author><author><keyname>Zhang</keyname><forenames>Shengyu</forenames></author></authors><title>Correlation/Communication complexity of generating bipartite states</title><categories>cs.CC quant-ph</categories><comments>12 pages, no figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the correlation complexity (or equivalently, the communication
complexity) of generating a bipartite quantum state $\rho$. When $\rho$ is a
pure state, we completely characterize the complexity for approximately
generating $\rho$ by a corresponding approximate rank, closing a gap left in
Ambainis, Schulman, Ta-Shma, Vazirani and Wigderson (SIAM Journal on Computing,
32(6):1570-1585, 2003). When $\rho$ is a classical distribution $P(x,y)$, we
tightly characterize the complexity of generating $P$ by the psd-rank, a
measure recently proposed by Fiorini, Massar, Pokutta, Tiwary and de Wolf (STOC
2012). We also present a characterization of the complexity of generating a
general quantum state $\rho$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1175</identifier>
 <datestamp>2012-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1175</id><created>2012-03-06</created><authors><author><keyname>Sen</keyname><forenames>Jaydip</forenames></author></authors><title>Secure and Privacy-Preserving Data Aggregation Protocols for Wireless
  Sensor Networks</title><categories>cs.CR cs.NI</categories><comments>32 pages, 7 figures, 3 tables</comments><journal-ref>Book Chapter published in the book: &quot;Cryptography and Security in
  Computing&quot; published by INTECH OPEN, Croatia, February 2012. ISBN:
  978-953-51-0179-6</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This chapter discusses the need of security and privacy protection mechanisms
in aggregation protocols used in wireless sensor networks (WSN). It presents a
comprehensive state of the art discussion on the various privacy protection
mechanisms used in WSNs and particularly focuses on the CPDA protocols proposed
by He et al. (INFOCOM 2007). It identifies a security vulnerability in the CPDA
protocol and proposes a mechanism to plug that vulnerability. To demonstrate
the need of security in aggregation process, the chapter further presents
various threats in WSN aggregation mechanisms. A large number of existing
protocols for secure aggregation in WSN are discussed briefly and a protocol is
proposed for secure aggregation which can detect false data injected by
malicious nodes in a WSN. The performance of the protocol is also presented.
The chapter concludes while highlighting some future directions of research in
secure data aggregation in WSNs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1177</identifier>
 <datestamp>2012-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1177</id><created>2012-03-06</created><authors><author><keyname>Wongpiromsarn</keyname><forenames>Tichakorn</forenames></author><author><keyname>Frazzoli</keyname><forenames>Emilio</forenames></author></authors><title>Control of Probabilistic Systems under Dynamic, Partially Known
  Environments with Temporal Logic Specifications</title><categories>cs.LO cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the synthesis of control policies for probabilistic systems,
modeled by Markov decision processes, operating in partially known environments
with temporal logic specifications. The environment is modeled by a set of
Markov chains. Each Markov chain describes the behavior of the environment in
each mode. The mode of the environment, however, is not known to the system.
Two control objectives are considered: maximizing the expected probability and
maximizing the worst-case probability that the system satisfies a given
specification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1179</identifier>
 <datestamp>2013-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1179</id><created>2012-03-06</created><updated>2012-08-02</updated><authors><author><keyname>Shen</keyname><forenames>Chuansheng</forenames></author><author><keyname>Chen</keyname><forenames>Hanshuang</forenames></author><author><keyname>Hou</keyname><forenames>Zhonghuai</forenames></author></authors><title>An efficient strategy to suppress epidemic explosion in heterogeneous
  metapopulation networks</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>14 pages, 6 figures</comments><journal-ref>Physical Review E 86, 036114 (2012)</journal-ref><doi>10.1103/PhysRevE.86.036114</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an efficient strategy to suppress epidemic explosion in
heterogeneous metapopulation networks, wherein each node represents a
subpopulation with any number of individuals and is assigned a curing rate that
is proportional to $k^{\alpha}$ with $k$ the node degree and $\alpha$ an
adjustable parameter. We have performed stochastic simulations of the dynamical
reaction-diffusion processes associated with the
susceptible-infected-susceptible model in scale-free networks. We found that
the epidemic threshold reaches a maximum when the exponent $\alpha$ is tuned to
be $\alpha_{opt}\simeq 1.3$. This nontrivial phenomenon is robust to the change
of the network size and the average degree. In addition, we have carried out a
mean field analysis to further validate our scheme, which also demonstrates
that epidemic explosion follows different routes for $\alpha$ larger or less
than $\alpha_{opt}$. Our work suggests that in order to effectively suppress
epidemic spreading on heterogeneous complex networks, subpopulations with
higher degrees should be allocated more resources than just being linearly
dependent on the degree $k$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1180</identifier>
 <datestamp>2012-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1180</id><created>2012-03-06</created><authors><author><keyname>Wongpiromsarn</keyname><forenames>Tichakorn</forenames></author><author><keyname>Ulusoy</keyname><forenames>Alphan</forenames></author><author><keyname>Belta</keyname><forenames>Calin</forenames></author><author><keyname>Frazzoli</keyname><forenames>Emilio</forenames></author><author><keyname>Rus</keyname><forenames>Daniela</forenames></author></authors><title>Incremental Temporal Logic Synthesis of Control Policies for Robots
  Interacting with Dynamic Agents</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the synthesis of control policies from temporal logic
specifications for robots that interact with multiple dynamic environment
agents. Each environment agent is modeled by a Markov chain whereas the robot
is modeled by a finite transition system (in the deterministic case) or Markov
decision process (in the stochastic case). Existing results in probabilistic
verification are adapted to solve the synthesis problem. To partially address
the state explosion issue, we propose an incremental approach where only a
small subset of environment agents is incorporated in the synthesis procedure
initially and more agents are successively added until we hit the constraints
on computational resources. Our algorithm runs in an anytime fashion where the
probability that the robot satisfies its specification increases as the
algorithm progresses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1185</identifier>
 <datestamp>2012-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1185</id><created>2012-03-06</created><authors><author><keyname>Banerjee</keyname><forenames>Abhik</forenames></author><author><keyname>Agarwal</keyname><forenames>Rachit</forenames></author><author><keyname>Gauthier</keyname><forenames>Vincent</forenames></author><author><keyname>Yeo</keyname><forenames>Chai Kiat</forenames></author><author><keyname>Afifi</keyname><forenames>Hossam</forenames></author><author><keyname>Lee</keyname><forenames>Bu Sung</forenames></author></authors><title>A Self-Organization Framework for Wireless Ad Hoc Networks as Small
  Worlds</title><categories>cs.NI</categories><comments>Submitted to IEEE Transactions on Vehicular Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by the benefits of small world networks, we propose a
self-organization framework for wireless ad hoc networks. We investigate the
use of directional beamforming for creating long-range short cuts between
nodes. Using simulation results for randomized beamforming as a guideline, we
identify crucial design issues for algorithm design. Our results show that,
while significant path length reduction is achievable, this is accompanied by
the problem of asymmetric paths between nodes. Subsequently, we propose a
distributed algorithm for small world creation that achieves path length
reduction while maintaining connectivity. We define a new centrality measure
that estimates the structural importance of nodes based on traffic flow in the
network, which is used to identify the optimum nodes for beamforming. We show,
using simulations, that this leads to significant reduction in path length
while maintaining connectivity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1203</identifier>
 <datestamp>2013-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1203</id><created>2012-03-06</created><updated>2012-10-29</updated><authors><author><keyname>Bucci</keyname><forenames>Michelangelo</forenames></author><author><keyname>De Luca</keyname><forenames>Alessandro</forenames></author><author><keyname>Fici</keyname><forenames>Gabriele</forenames></author></authors><title>Enumeration and Structure of Trapezoidal Words</title><categories>cs.FL math.CO</categories><comments>Accepted for publication in Theoretical Computer Science</comments><msc-class>68R15</msc-class><acm-class>F.4.3</acm-class><journal-ref>Theoretical Computer Science 468, 12-22 (2013)</journal-ref><doi>10.1016/j.tcs.2012.11.007</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Trapezoidal words are words having at most $n+1$ distinct factors of length
$n$ for every $n\ge 0$. They therefore encompass finite Sturmian words. We give
combinatorial characterizations of trapezoidal words and exhibit a formula for
their enumeration. We then separate trapezoidal words into two disjoint
classes: open and closed. A trapezoidal word is closed if it has a factor that
occurs only as a prefix and as a suffix; otherwise it is open. We investigate
open and closed trapezoidal words, in relation with their special factors. We
prove that Sturmian palindromes are closed trapezoidal words and that a closed
trapezoidal word is a Sturmian palindrome if and only if its longest repeated
prefix is a palindrome. We also define a new class of words, \emph{semicentral
words}, and show that they are characterized by the property that they can be
written as $uxyu$, for a central word $u$ and two different letters $x,y$.
Finally, we investigate the prefixes of the Fibonacci word with respect to the
property of being open or closed trapezoidal words, and show that the sequence
of open and closed prefixes of the Fibonacci word follows the Fibonacci
sequence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1212</identifier>
 <datestamp>2012-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1212</id><created>2012-03-06</created><authors><author><keyname>Panek</keyname><forenames>Luciano</forenames></author><author><keyname>Firer</keyname><forenames>Marcelo</forenames></author></authors><title>Codes Satisfying the Chain Condition with a Poset Weights</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we extend the concept of generalized Wei weights for
poset-weight codes and show that all linear codes C satisfy the chain condition
if support of C is a subposet totally ordered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1226</identifier>
 <datestamp>2012-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1226</id><created>2012-03-06</created><authors><author><keyname>Kesselheim</keyname><forenames>Thomas</forenames></author></authors><title>Dynamic Packet Scheduling in Wireless Networks</title><categories>cs.NI cs.DS</categories><comments>23 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider protocols that serve communication requests arising over time in
a wireless network that is subject to interference. Unlike previous approaches,
we take the geometry of the network and power control into account, both
allowing to increase the network's performance significantly. We introduce a
stochastic and an adversarial model to bound the packet injection. Although
taken as the primary motivation, this approach is not only suitable for models
based on the signal-to-interference-plus-noise ratio (SINR). It also covers
virtually all other common interference models, for example the multiple-access
channel, the radio-network model, the protocol model, and distance-2 matching.
Packet-routing networks allowing each edge or each node to transmit or receive
one packet at a time can be modeled as well.
  Starting from algorithms for the respective scheduling problem with static
transmission requests, we build distributed stable protocols. This is more
involved than in previous, similar approaches because the algorithms we
consider do not necessarily scale linearly when scaling the input instance. We
can guarantee a throughput that is as large as the one of the original static
algorithm. In particular, for SINR models the competitive ratios of the
protocol in comparison to optimal ones in the respective model are between
constant and O(log^2 m) for a network of size m.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1250</identifier>
 <datestamp>2012-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1250</id><created>2012-03-03</created><authors><author><keyname>Folorunso</keyname><forenames>Olusegun</forenames></author><author><keyname>Vincent</keyname><forenames>Olufunke R.</forenames></author><author><keyname>Salako</keyname><forenames>Oluwatimilehin</forenames></author></authors><title>An Exploratory Study of Critical Factors Affecting the Efficiency of
  Sorting Techniques (Shell, Heap and Treap)</title><categories>cs.DS</categories><comments>10 pages</comments><journal-ref>Ann. Univ. Tibiscus Comp. Sci. Series VIII / 1 (2010), 163-172</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The efficiency of sorting techniques has a significant impact on the overall
efficiency of a program. The efficiency of Shell, Heap and Treap sorting
techniques in terms of both running time and memory usage was studied,
experiments conducted and results subjected to factor analysis by SPSS. The
study revealed the main factor affecting these sorting techniques was time
taken to sort.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1251</identifier>
 <datestamp>2012-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1251</id><created>2012-03-06</created><updated>2012-06-04</updated><authors><author><keyname>Wang</keyname><forenames>Yongqiang</forenames></author><author><keyname>Hori</keyname><forenames>Yutaka</forenames></author><author><keyname>Hara</keyname><forenames>Shinji</forenames></author><author><keyname>Doyle</keyname><forenames>Francis J.</forenames><suffix>III</suffix></author></authors><title>The collective oscillation period of inter-coupled Goodwin oscillators</title><categories>cs.SY nlin.CD physics.bio-ph q-bio.MN</categories><comments>Technical Report accompanying CDC 2012 submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many biological oscillators are arranged in networks composed of many
inter-coupled cellular oscillators. However, results are still lacking on the
collective oscillation period of inter-coupled gene regulatory oscillators,
which, as has been reported, may be different from the oscillation period of an
autonomous cellular oscillator. Based on the Goodwin oscillator, we analyze the
collective oscillation pattern of coupled cellular oscillator networks. First
we give a condition under which the oscillator network exhibits oscillatory and
synchronized behavior, then we estimate the collective oscillation period based
on a multivariable harmonic balance technique. Analytical results are derived
in terms of biochemical parameters, thus giving insight into the basic
mechanism of biological oscillation and providing guidance in synthetic biology
design. Simulation results are given to confirm the theoretical predictions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1257</identifier>
 <datestamp>2012-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1257</id><created>2012-03-06</created><authors><author><keyname>Erd&#x151;s</keyname><forenames>P&#xe9;ter L.</forenames></author><author><keyname>Tardif</keyname><forenames>Claude</forenames></author><author><keyname>Tardos</keyname><forenames>G&#xe1;bor</forenames></author></authors><title>On infinite-finite duality pairs of directed graphs</title><categories>math.CO cs.LO</categories><msc-class>05C60 (Primary), 68R10 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The (A,D) duality pairs play crucial role in the theory of general relational
structures and in the Constraint Satisfaction Problem. The case where both
classes are finite is fully characterized. The case when both side are infinite
seems to be very complex. It is also known that no finite-infinite duality pair
is possible if we make the additional restriction that both classes are
antichains. In this paper (which is the first one of a series) we start the
detailed study of the infinite-finite case.
  Here we concentrate on directed graphs. We prove some elementary properties
of the infinite-finite duality pairs, including lower and upper bounds on the
size of D, and show that the elements of A must be equivalent to forests if A
is an antichain. Then we construct instructive examples, where the elements of
A are paths or trees. Note that the existence of infinite-finite antichain
dualities was not previously known.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1263</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1263</id><created>2012-03-06</created><updated>2012-12-03</updated><authors><author><keyname>Caplan</keyname><forenames>R. M.</forenames></author></authors><title>NLSEmagic: Nonlinear Schr\&quot;odinger Equation Multidimensional
  Matlab-based GPU-accelerated Integrators using Compact High-order Schemes</title><categories>cs.MS cs.CE physics.comp-ph</categories><comments>37 pages, 13 figures</comments><msc-class>35-04</msc-class><doi>10.1016/j.cpc.2012.12.010</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a simple to use, yet powerful code package called NLSEmagic to
numerically integrate the nonlinear Schr\&quot;odinger equation in one, two, and
three dimensions. NLSEmagic is a high-order finite-difference code package
which utilizes graphic processing unit (GPU) parallel architectures. The codes
running on the GPU are many times faster than their serial counterparts, and
are much cheaper to run than on standard parallel clusters. The codes are
developed with usability and portability in mind, and therefore are written to
interface with MATLAB utilizing custom GPU-enabled C codes with the
MEX-compiler interface. The packages are freely distributed, including user
manuals and set-up files.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1276</identifier>
 <datestamp>2013-12-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1276</id><created>2012-03-06</created><updated>2013-12-04</updated><authors><author><keyname>Farokhi</keyname><forenames>Farhad</forenames></author><author><keyname>Johansson</keyname><forenames>Karl H.</forenames></author></authors><title>Optimal Control Design under Limited Model Information for Discrete-Time
  Linear Systems with Stochastically-Varying Parameters</title><categories>math.OC cs.SY</categories><comments>In comparison to the previous version, we have updated the proof of
  Corollary 3 and fixed some typos. We have also included the proof of Theorem
  4</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The value of plant model information available in the control design process
is discussed. We design optimal state-feedback controllers for interconnected
discrete-time linear systems with stochastically-varying parameters. The
parameters are assumed to be independently and identically distributed random
variables in time. The design of each controller relies only on (i) exact local
plant model information and (ii) statistical beliefs about the model of the
rest of the system. We consider both finite-horizon and infinite-horizon
quadratic cost functions. The optimal state-feedback controller is derived in
both cases. The optimal controller is shown to be linear in the state and to
depend on the model parameters and their statistics in a particular way.
Furthermore, we study the value of model information in optimal control design
using the performance degradation ratio which is defined as the supremum (over
all possible initial conditions) of the ratio of the cost of the optimal
controller with limited model information scaled by the cost of the optimal
controller with full model information. An upper bound for the performance
degradation ratio is presented for the case of fully-actuated subsystems.
Comparisons are made between designs based on limited, statistical, and full
model information. Throughout the paper, we use a power network example to
illustrate concepts and results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1278</identifier>
 <datestamp>2012-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1278</id><created>2012-03-06</created><authors><author><keyname>Gonz&#xe1;lez-Estrada</keyname><forenames>Octavio A.</forenames></author><author><keyname>Natarajan</keyname><forenames>Sundararajan</forenames></author><author><keyname>R&#xf3;denas</keyname><forenames>Juan Jos&#xe9;</forenames></author><author><keyname>Nguyen-Xuan</keyname><forenames>Hung</forenames></author><author><keyname>Bordas</keyname><forenames>St&#xe9;phane P. A.</forenames></author></authors><title>Efficient recovery-based error estimation for the smoothed finite
  element method for smooth and singular linear elasticity</title><categories>cs.NA cs.CE math.NA</categories><comments>submitted to Computational Mechanics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An error control technique aimed to assess the quality of smoothed finite
element approximations is presented in this paper. Finite element techniques
based on strain smoothing appeared in 2007 were shown to provide significant
advantages compared to conventional finite element approximations. In
particular, a widely cited strength of such methods is improved accuracy for
the same computational cost. Yet, few attempts have been made to directly
assess the quality of the results obtained during the simulation by evaluating
an estimate of the discretization error. Here we propose a recovery type error
estimator based on an enhanced recovery technique. The salient features of the
recovery are: enforcement of local equilibrium and, for singular problems a
&quot;smooth+singular&quot; decomposition of the recovered stress. We evaluate the
proposed estimator on a number of test cases from linear elastic structural
mechanics and obtain precise error estimations whose effectivities, both at
local and global levels, are improved compared to recovery procedures not
implementing these features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1287</identifier>
 <datestamp>2012-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1287</id><created>2012-03-06</created><authors><author><keyname>Dixit</keyname><forenames>Narendra M.</forenames></author><author><keyname>Srivastava</keyname><forenames>Piyush</forenames></author><author><keyname>Vishnoi</keyname><forenames>Nisheeth K.</forenames></author></authors><title>A Finite Population Model of Molecular Evolution: Theory and Computation</title><categories>q-bio.PE cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is concerned with the evolution of haploid organisms that
reproduce asexually. In a seminal piece of work, Eigen and coauthors proposed
the quasispecies model in an attempt to understand such an evolutionary
process. Their work has impacted antiviral treatment and vaccine design
strategies. Yet, predictions of the quasispecies model are at best viewed as a
guideline, primarily because it assumes an infinite population size, whereas
realistic population sizes can be quite small. In this paper we consider a
population genetics-based model aimed at understanding the evolution of such
organisms with finite population sizes and present a rigorous study of the
convergence and computational issues that arise therein. Our first result is
structural and shows that, at any time during the evolution, as the population
size tends to infinity, the distribution of genomes predicted by our model
converges to that predicted by the quasispecies model. This justifies the
continued use of the quasispecies model to derive guidelines for intervention.
While the stationary state in the quasispecies model is readily obtained, due
to the explosion of the state space in our model, exact computations are
prohibitive. Our second set of results are computational in nature and address
this issue. We derive conditions on the parameters of evolution under which our
stochastic model mixes rapidly. Further, for a class of widely used fitness
landscapes we give a fast deterministic algorithm which computes the stationary
distribution of our model. These computational tools are expected to serve as a
framework for the modeling of strategies for the deployment of mutagenic drugs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1295</identifier>
 <datestamp>2012-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1295</id><created>2012-03-06</created><authors><author><keyname>Stoutemyer</keyname><forenames>David R.</forenames></author></authors><title>Subtotal ordering -- a pedagogically advantageous algorithm for
  computing total degree reverse lexicographic order</title><categories>cs.SC cs.MS</categories><comments>12 pages, 2 Algorithms, 1 Table</comments><msc-class>12, 13, 14</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Total degree reverse lexicographic order is currently generally regarded as
most often fastest for computing Groebner bases. This article describes an
alternate less mysterious algorithm for computing this order using exponent
subtotals and describes why it should be very nearly the same speed the
traditional algorithm, all other things being equal. However, experimental
evidence suggests that subtotal order is actually slightly faster for the
Mathematica Groebner basis implementation more often than not. This is probably
because the weight vectors associated with the natural subtotal weight matrix
and with the usual total degree reverse lexicographic weight matrix are
different, and Mathematica also uses those the corresponding weight vectors to
help select successive S polynomials and divisor polynomials: Those selection
heuristics appear to work slightly better more often with subtotal weight
vectors.
  However, the most important advantage of exponent subtotals is pedagogical.
It is easier to understand than the total degree reverse lexicographic
algorithm, and it is more evident why the resulting order is often the fastest
known order for computing Groebner bases.
  Keywords: Term order, Total degree reverse lexicographic, tdeg, grevlex,
Groebner basis
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1301</identifier>
 <datestamp>2012-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1301</id><created>2012-03-06</created><authors><author><keyname>Gou</keyname><forenames>Tiangao</forenames></author><author><keyname>Jafar</keyname><forenames>Syed A.</forenames></author></authors><title>Optimal Use of Current and Outdated Channel State Information - Degrees
  of Freedom of the MISO BC with Mixed CSIT</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a multiple-input-single-output (MISO) broadcast channel with
mixed channel state information at the transmitter (CSIT) that consists of
imperfect current CSIT and perfect outdated CSIT. Recent work by Kobayashi et
al. presented a scheme which exploits both imperfect current CSIT and perfect
outdated CSIT and achieves higher degrees of freedom (DoF) than possible with
only imperfect current CSIT or only outdated CSIT individually. In this work,
we further improve the achievable DoF in this setting by incorporating
additional private messages, and provide a tight information theoretic DoF
outer bound, thereby identifying the DoF optimal use of mixed CSIT. The new
result is stronger even in the original setting of only delayed CSIT, because
it allows us to remove the restricting assumption of statistically equivalent
fading for all users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1304</identifier>
 <datestamp>2013-03-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1304</id><created>2012-03-06</created><updated>2013-03-21</updated><authors><author><keyname>Novlan</keyname><forenames>Thomas D.</forenames></author><author><keyname>Dhillon</keyname><forenames>Harpreet S.</forenames></author><author><keyname>Andrews</keyname><forenames>Jeffrey G.</forenames></author></authors><title>Analytical Modeling of Uplink Cellular Networks</title><categories>cs.IT cs.NI math.IT math.PR</categories><comments>to appear, IEEE Transactions on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cellular uplink analysis has typically been undertaken by either a simple
approach that lumps all interference into a single deterministic or random
parameter in a Wyner-type model, or via complex system level simulations that
often do not provide insight into why various trends are observed. This paper
proposes a novel middle way using point processes that is both accurate and
also results in easy-to-evaluate integral expressions based on the Laplace
transform of the interference. We assume mobiles and base stations are randomly
placed in the network with each mobile pairing up to its closest base station.
Compared to related recent work on downlink analysis, the proposed uplink model
differs in two key features. First, dependence is considered between user and
base station point processes to make sure each base station serves a single
mobile in the given resource block. Second, per-mobile power control is
included, which further couples the transmission of mobiles due to
location-dependent channel inversion. Nevertheless, we succeed in deriving the
coverage (equivalently outage) probability of a typical link in the network.
This model can be used to address a wide variety of system design questions in
the future. In this paper we focus on the implications for power control and
see that partial channel inversion should be used at low
signal-to-interference-plus-noise ratio (SINR), while full power transmission
is optimal at higher SINR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1314</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1314</id><created>2012-03-06</created><authors><author><keyname>basha</keyname><forenames>N Md Jubair</forenames></author><author><keyname>Moiz</keyname><forenames>Salman Abdul</forenames></author><author><keyname>Rizwanullah</keyname><forenames>Mohammed</forenames></author></authors><title>Model based Software Develeopment: Issues &amp; Challenges</title><categories>cs.SE</categories><comments>6 pages, 1 figure, NCRTCST-12</comments><journal-ref>Special Issue of International Journal of Computer Science &amp;
  Informatics (IJCSI), ISSN (PRINT) : 2231--5292, 2012 Vol.- II, Issue-1, 2</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the goals of software design is to model a system in such a way that
it is easily understandable. Nowadays the tendency for software development is
changing from manual coding to automatic code generation; it is becoming
model-based. This is a response to the software crisis, in which the cost of
hardware has decreased and conversely the cost of software development has
increased sharply. The methodologies that allowed this change are model-based,
thus relieving the human from detailed coding. Still there is a long way to
achieve this goal, but work is being done worldwide to achieve this objective.
This paper presents the drastic changes related to modeling and important
challenging issues and techniques that recur in MBSD.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1328</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1328</id><created>2012-03-06</created><authors><author><keyname>Basha</keyname><forenames>N. Md. Jubair</forenames></author><author><keyname>Moiz</keyname><forenames>Salman Abdul</forenames></author><author><keyname>Qyser</keyname><forenames>A. A. Moiz</forenames></author></authors><title>Performance Analysis of HR Portal Domain Components Extraction</title><categories>cs.SE</categories><comments>6 pages, 10 figures</comments><journal-ref>International Journal of Computer Science and Information
  Technologies, Vol. 2 (5) , 2011, 2326-2331</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Extraction of components pertaining to a particular domain not only reduces
the cost but also helps in delivering a quality product. However, the
advantages of the Component Level Interaction's (CLI's) are not clearly
presented. In the first part of the paper the design of HR Portal application
is described. Later the results are simulated using the Netbeans Profiler tool
which exposes and highlights the performance characteristics of component based
system pertaining to HR domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1335</identifier>
 <datestamp>2012-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1335</id><created>2012-03-06</created><authors><author><keyname>Capuni</keyname><forenames>Ilir</forenames></author><author><keyname>Gacs</keyname><forenames>Peter</forenames></author></authors><title>A Turing Machine Resisting Isolated Bursts Of Faults</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider computations of a Turing machine under noise that causes
consecutive violations of the machine's transition function. Given a constant
upper bound B on the size of bursts of faults, we construct a Turing machine
M(B) subject to faults that can simulate any fault-free machine under the
condition that bursts are not closer to each other than V for an appropriate V
= O(B^2).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1338</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1338</id><created>2012-03-06</created><authors><author><keyname>Lerman</keyname><forenames>Kristina</forenames></author><author><keyname>Ghosh</keyname><forenames>Rumi</forenames></author></authors><title>Network Structure, Topology and Dynamics in Generalized Models of
  Synchronization</title><categories>cond-mat.dis-nn cs.SI nlin.CD physics.soc-ph</categories><doi>10.1103/PhysRevE.86.026108</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explore the interplay of network structure, topology, and dynamic
interactions between nodes using the paradigm of distributed synchronization in
a network of coupled oscillators. As the network evolves to a global steady
state, interconnected oscillators synchronize in stages, revealing network's
underlying community structure. Traditional models of synchronization assume
that interactions between nodes are mediated by a conservative process, such as
diffusion. However, social and biological processes are often non-conservative.
We propose a new model of synchronization in a network of oscillators coupled
via non-conservative processes. We study dynamics of synchronization of a
synthetic and real-world networks and show that different synchronization
models reveal different structures within the same network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1347</identifier>
 <datestamp>2013-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1347</id><created>2012-03-06</created><authors><author><keyname>Erd&#x151;s</keyname><forenames>P&#xe9;ter L.</forenames></author><author><keyname>Tardif</keyname><forenames>Claude</forenames></author><author><keyname>Tardos</keyname><forenames>G&#xe1;bor</forenames></author></authors><title>Caterpillar dualities and regular languages</title><categories>math.CO cs.DM</categories><msc-class>05C60 (Primary) 68R10 (Secondary)</msc-class><journal-ref>SIAM J. Discrete Math. 27-3 (2013), pp. 1287-1294</journal-ref><doi>10.1137/120879270</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We characterize obstruction sets in caterpillar dualities in terms of regular
languages, and give a construction of the dual of a regular family of
caterpillars. We show that these duals correspond to the constraint
satisfaction problems definable by a monadic linear Datalog program with at
most one EDB per rule.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1349</identifier>
 <datestamp>2012-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1349</id><created>2012-03-06</created><authors><author><keyname>Caldarelli</keyname><forenames>Guido</forenames></author><author><keyname>Chessa</keyname><forenames>Alessandro</forenames></author><author><keyname>Crimaldi</keyname><forenames>Irene</forenames></author><author><keyname>Pammolli</keyname><forenames>Fabio</forenames></author></authors><title>The Evolution of Complex Networks: A New Framework</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI stat.AP</categories><comments>4 pages and 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new framework for the analysis of the dynamics of networks,
based on randomly reinforced urn (RRU) processes, in which the weight of the
edges is determined by a reinforcement mechanism. We rigorously explain the
empirical evidence that in many real networks there is a subset of &quot;dominant
edges&quot; that control a major share of the total weight of the network.
Furthermore, we introduce a new statistical procedure to study the evolution of
networks over time, assessing if a given instance of the nework is taken at its
steady state or not. Our results are quite general, since they are not based on
a particular probability distribution or functional form of the weights. We
test our model in the context of the International Trade Network, showing the
existence of a core of dominant links and determining its size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1350</identifier>
 <datestamp>2012-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1350</id><created>2012-03-06</created><authors><author><keyname>Stoutemyer</keyname><forenames>David R.</forenames></author></authors><title>Simplifying products of fractional powers of powers</title><categories>cs.SC cs.MS</categories><comments>34 pages. 17 tables. Includes Mathematica rewrite rules. To appear in
  Communications in Computer Algebra</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most computer algebra systems incorrectly simplify (z - z)/(sqrt(w^2)/w^3 -
1/(w*sqrt(w^2))) to 0 rather than to 0/0. The reasons for this are:
  1. The default simplification doesn't succeed in simplifying the denominator
to 0.
  2. There is a rule that 0 is the result of 0 divided by anything that doesn't
simplify to either 0 or 0/0.
  Try it on your computer algebra systems!
  This article describes how to simplify products of the form w^a*(w^b1)^g1 ...
(w^bn)^gn correctly and well, where w is any real or complex expression and the
exponents are rational numbers.
  It might seem that correct good simplification of such a restrictive
expression class must already be published and/or built into at least one
widely used computer-algebra system, but apparently this issue has been
overlooked. Default and relevant optional simplification was tested with 86
examples for n=1 on Derive, Maple, Mathematica, Maxima and TI-CAS. Totaled over
all five systems, 11% of the results were not equivalent to the input
everywhere, 50% of the results did not simplify to 0 a result that was
equivalent to 0, and at least 16% of the results exhibited one or more of four
additional flaw types. There was substantial room for improvement in all five
systems, including the two for which I was a co-author.
  The good news is: These flaws are easy to fix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1352</identifier>
 <datestamp>2012-06-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1352</id><created>2012-03-06</created><updated>2012-06-20</updated><authors><author><keyname>Abramsky</keyname><forenames>Samson</forenames></author><author><keyname>Hardy</keyname><forenames>Lucien</forenames></author></authors><title>Logical Bell Inequalities</title><categories>quant-ph cs.LO</categories><comments>12 pages</comments><journal-ref>Phys. Rev. A 85, 062114 (2012) [11 pages]</journal-ref><doi>10.1103/PhysRevA.85.062114</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bell inequalities play a central role in the study of quantum non-locality
and entanglement, with many applications in quantum information. Despite the
huge literature on Bell inequalities, it is not easy to find a clear conceptual
answer to what a Bell inequality is, or a clear guiding principle as to how
they may be derived. In this paper, we introduce a notion of logical Bell
inequality which can be used to systematically derive testable inequalities for
a very wide variety of situations. There is a single clear conceptual
principle, based on purely logical consistency conditions, which underlies our
notion of logical Bell inequalities. We show that in a precise sense, all Bell
inequalities can be taken to be of this form. Our approach is very general. It
applies directly to any family of sets of commuting observables. Thus it covers
not only the n-partite scenarios to which Bell inequalities are standardly
applied, but also Kochen-Specker configurations, and many other examples. There
is much current work on experimental tests for contextuality. Our approach
directly yields, in a systematic fashion, testable inequalities for a very
general notion of contextuality.
  There has been much work on obtaining proofs of Bell's theorem `without
inequalities' or `without probabilities'. These proofs are seen as being in a
sense more definitive and logically robust than the inequality-based proofs. On
the hand, they lack the fault-tolerant aspect of inequalities. Our approach
reconciles these aspects, and in fact shows how the logical robustness can be
converted into systematic, general derivations of inequalities with provable
violations. Moreover, the kind of strong non-locality or contextuality
exhibited by the GHZ argument or by Kochen-Specker configurations can be shown
to lead to maximal violations of the corresponding logical Bell inequalities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1355</identifier>
 <datestamp>2012-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1355</id><created>2012-03-06</created><updated>2012-03-12</updated><authors><author><keyname>Silva</keyname><forenames>Pedro V.</forenames></author></authors><title>Fixed points of endomorphisms of virtually free groups</title><categories>math.GR cs.FL math.GT</categories><comments>29 pages</comments><msc-class>20F67, 20E05, 20E36, 68Q45, 37B25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A fixed point theorem is proved for inverse transducers, leading to an
automata-theoretic proof of the fixed point subgroup of an endomorphism of a
finitely generated virtually free group being finitely generated. If the
endomorphism is uniformly continuous for the hyperbolic metric, it is proved
that the set of regular fixed points in the hyperbolic boundary has finitely
many orbits under the action of the finite fixed points. In the automorphism
case, it is shown that these regular fixed points are either exponentially
stable attractors or exponentially stable repellers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1357</identifier>
 <datestamp>2013-05-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1357</id><created>2012-03-06</created><authors><author><keyname>Stoutemyer</keyname><forenames>David R.</forenames></author></authors><title>Series misdemeanors</title><categories>cs.SC cs.MS</categories><comments>21 pages; 1 Algorithm; 2 Tables; To appear in the ACM Communications
  in Computer Algebra</comments><msc-class>41</msc-class><journal-ref>ACM Communications in Computer Algebra 12/2012; 46
  (4)(182):134-153</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Puiseux series are power series in which the exponents can be fractional
and/or negative rational numbers. Several computer algebra systems have one or
more built-in or loadable functions for computing truncated Puiseux series --
perhaps generalized to allow coefficients containing functions of the series
variable that are dominated by any power of that variable, such as logarithms
and nested logarithms of the series variable. Some computer-algebra systems
also offer functions that can compute more-general truncated recursive
hierarchical series. However, for all of these kinds of truncated series there
are important implementation details that haven't been addressed before in the
published literature and in current implementations.
  For implementers this article contains ideas for designing more convenient,
correct, and efficient implementations or improving existing ones. For users,
this article is a warning about some of these limitations. Many of the ideas in
this article have been implemented in the computer-algebra within the TI-Nspire
calculator, Windows and Macintosh products.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1376</identifier>
 <datestamp>2012-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1376</id><created>2012-03-06</created><authors><author><keyname>He</keyname><forenames>Xiang</forenames></author><author><keyname>Khisti</keyname><forenames>Ashish</forenames></author><author><keyname>Yener</keyname><forenames>Aylin</forenames></author></authors><title>MIMO Multiple Access Channel with an Arbitrarily Varying Eavesdropper</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Transactions on Information on 02/24/2012. The
  ordering of authors is alphabetical</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A two-transmitter Gaussian multiple access wiretap channel with multiple
antennas at each of the nodes is investigated. The channel matrices at the
legitimate terminals are fixed and revealed to all the terminals, whereas the
channel matrix of the eavesdropper is arbitrarily varying and only known to the
eavesdropper. The secrecy degrees of freedom (s.d.o.f.) region under a strong
secrecy constraint is characterized. A transmission scheme that orthogonalizes
the transmit signals of the two users at the intended receiver and uses a
single-user wiretap code is shown to be sufficient to achieve the s.d.o.f.
region. The converse involves establishing an upper bound on a
weighted-sum-rate expression. This is accomplished by using induction, where at
each step one combines the secrecy and multiple-access constraints associated
with an adversary eavesdropping a carefully selected group of sub-channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1378</identifier>
 <datestamp>2012-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1378</id><created>2012-03-05</created><authors><author><keyname>Diaz-Aviles</keyname><forenames>Ernesto</forenames><affiliation>L3S Research Center / University of Hannover. Hannover, Germany</affiliation></author><author><keyname>Stewart</keyname><forenames>Avar&#xe9;</forenames><affiliation>L3S Research Center / University of Hannover. Hannover, Germany</affiliation></author><author><keyname>Velasco</keyname><forenames>Edward</forenames><affiliation>Robert Koch Institute. Berlin, Germany</affiliation></author><author><keyname>Denecke</keyname><forenames>Kerstin</forenames><affiliation>L3S Research Center / University of Hannover. Hannover, Germany</affiliation></author><author><keyname>Nejdl</keyname><forenames>Wolfgang</forenames><affiliation>L3S Research Center / University of Hannover. Hannover, Germany</affiliation></author></authors><title>Epidemic Intelligence for the Crowd, by the Crowd (Full Version)</title><categories>cs.SI cs.CY physics.soc-ph</categories><comments>A short version of this work has been accepted for publication at the
  International AAAI Conference on Weblogs and Social Media (ICWSM 2012)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tracking Twitter for public health has shown great potential. However, most
recent work has been focused on correlating Twitter messages to influenza
rates, a disease that exhibits a marked seasonal pattern. In the presence of
sudden outbreaks, how can social media streams be used to strengthen
surveillance capacity? In May 2011, Germany reported an outbreak of
Enterohemorrhagic Escherichia coli (EHEC). It was one of the largest described
outbreaks of EHEC/HUS worldwide and the largest in Germany. In this work, we
study the crowd's behavior in Twitter during the outbreak. In particular, we
report how tracking Twitter helped to detect key user messages that triggered
signal detection alarms before MedISys and other well established early warning
systems. We also introduce a personalized learning to rank approach that
exploits the relationships discovered by: (i) latent semantic topics computed
using Latent Dirichlet Allocation (LDA), and (ii) observing the social tagging
behavior in Twitter, to rank tweets for epidemic intelligence. Our results
provide the grounds for new public health research based on social media.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1390</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1390</id><created>2012-03-07</created><updated>2012-03-10</updated><authors><author><keyname>Kiyavash</keyname><forenames>Negar</forenames></author><author><keyname>Houmansadr</keyname><forenames>Amir</forenames></author><author><keyname>Borisov</keyname><forenames>Nikita</forenames></author></authors><title>Multi-Flow Attacks Against Network Flow Watermarks: Analysis and
  Countermeasures</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we analyze several recent schemes for watermarking network
flows that are based on splitting the flow into timing intervals. We show that
this approach creates time-dependent correlations that enable an attack that
combines multiple watermarked flows. Such an attack can easily be mounted in
nearly all applications of network flow watermarking, both in anonymous
communication and stepping stone detection. The attack can be used to detect
the presence of a watermark, recover the secret parameters, and remove the
watermark from a flow. The attack can be effective even if different flows are
marked with different values of a watermark.
  We analyze the efficacy of our attack using a probabilistic model and a
Markov-Modulated Poisson Process (MMPP) model of interactive traffic. We also
implement our attack and test it using both synthetic and real-world traces,
showing that our attack is effective with as few as 10 watermarked flows.
Finally, we propose possible countermeasures to defeat the multi-flow attack.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1392</identifier>
 <datestamp>2012-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1392</id><created>2012-03-07</created><authors><author><keyname>Phan</keyname><forenames>Quan</forenames></author><author><keyname>Janssens</keyname><forenames>Gerda</forenames></author><author><keyname>Somogyi</keyname><forenames>Zoltan</forenames></author></authors><title>Region-based memory management for Mercury programs</title><categories>cs.PL</categories><comments>74 pages, 23 figures, 11 tables. A shorter version of this paper,
  without proofs, is to appear in the journal Theory and Practice of Logic
  Programming (TPLP)</comments><acm-class>D.3.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Region-based memory management (RBMM) is a form of compile time memory
management, well-known from the functional programming world. In this paper we
describe our work on implementing RBMM for the logic programming language
Mercury. One interesting point about Mercury is that it is designed with strong
type, mode, and determinism systems. These systems not only provide Mercury
programmers with several direct software engineering benefits, such as
self-documenting code and clear program logic, but also give language
implementors a large amount of information that is useful for program analyses.
In this work, we make use of this information to develop program analyses that
determine the distribution of data into regions and transform Mercury programs
by inserting into them the necessary region operations. We prove the
correctness of our program analyses and transformation. To execute the
annotated programs, we have implemented runtime support that tackles the two
main challenges posed by backtracking. First, backtracking can require regions
removed during forward execution to be &quot;resurrected&quot;; and second, any memory
allocated during a computation that has been backtracked over must be recovered
promptly and without waiting for the regions involved to come to the end of
their life. We describe in detail our solution of both these problems. We study
in detail how our RBMM system performs on a selection of benchmark programs,
including some well-known difficult cases for RBMM. Even with these difficult
cases, our RBMM-enabled Mercury system obtains clearly faster runtimes for 15
out of 18 benchmarks compared to the base Mercury system with its Boehm runtime
garbage collector, with an average runtime speedup of 24%, and an average
reduction in memory requirements of 95%. In fact, our system achieves optimal
memory consumption in some programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1394</identifier>
 <datestamp>2012-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1394</id><created>2012-03-07</created><authors><author><keyname>Pelino</keyname><forenames>Vinicio</forenames></author><author><keyname>Maimone</keyname><forenames>Filippo</forenames></author></authors><title>Towards a class of complex networks models for conflict dynamics</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI math-ph math.MP</categories><comments>28 pages, 15 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using properties of isospectral flows we introduce a class of equations
useful to represent signed complex networks free continuous time evolution
Jammed and balanced states are obtained introducing a class of link potentials
breaking isospectral invariance of the network. Applications to conflict
dynamics in social and international relations networks are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1395</identifier>
 <datestamp>2012-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1395</id><created>2012-03-07</created><authors><author><keyname>Kundu</keyname><forenames>Anirban</forenames></author><author><keyname>Ji</keyname><forenames>Chunlin</forenames></author></authors><title>Swarm Behavior of Intelligent Cloud</title><categories>cs.DC cs.NI</categories><comments>15 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the main aim is to exhibit swarm intelligence power in cloud
based scenario. Heterogeneous environment has been configured at server-side
network of the whole cloud network. In the proposed system, different types of
servers are being used to manage useful assorted atmosphere. Swarm intelligence
has been adopted for enhancing the performance of overall system network.
Specific location at server-side of the network is going to be selected by the
swarm intelligence concept for accessing desired elements. Flexibility,
robustness and self-organization, which are to be considered at the time of
designing the system environment, are the main features of swarm intelligence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1398</identifier>
 <datestamp>2012-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1398</id><created>2012-03-07</created><authors><author><keyname>Gerbner</keyname><forenames>D&#xe1;niel</forenames></author><author><keyname>Katona</keyname><forenames>Gyula O. H.</forenames></author><author><keyname>P&#xe1;lv&#xf6;lgyi</keyname><forenames>D&#xf6;m&#xf6;t&#xf6;r</forenames></author><author><keyname>Patk&#xf3;s</keyname><forenames>Bal&#xe1;zs</forenames></author></authors><title>Majority and Plurality Problems</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a set of n balls each colored with a color, a ball is said to be
majority, k-majority, plurality if its color class has size larger than half of
the number of balls, has size at least k, has size larger than any other color
class; respectively. We address the problem of finding the minimum number of
queries (a comparison of a pair of balls if they have the same color or not)
that is needed to decide whether a majority, k-majority or plurality ball
exists and if so then show one such ball. We consider both adaptive and
non-adaptive strategies and in certain cases, we also address weighted versions
of the problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1406</identifier>
 <datestamp>2012-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1406</id><created>2012-03-07</created><authors><author><keyname>Lomnitz</keyname><forenames>Yuval</forenames></author><author><keyname>Feder</keyname><forenames>Meir</forenames></author></authors><title>Communication over Individual Channels -- a general framework</title><categories>cs.IT math.IT</categories><comments>The current paper is rather long; we plan to split the paper into two</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of communicating over a channel for which no
mathematical model is specified, and the achievable rates are determined as a
function of the channel input and output sequences known a-posteriori, without
assuming any a-priori relation between them. In a previous paper we have shown
that the empirical mutual information between the input and output sequences is
achievable without specifying the channel model, by using feedback and common
randomness, and a similar result for real-valued input and output alphabets. In
this paper, we present a unifying framework which includes the two previous
results as particular cases. We characterize the region of rate functions which
are achievable, and show that asymptotically the rate function is equivalent to
a conditional distribution of the channel input given the output. We present a
scheme that achieves these rates with asymptotically vanishing overheads.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1410</identifier>
 <datestamp>2012-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1410</id><created>2012-03-07</created><authors><author><keyname>Trifina</keyname><forenames>Lucian</forenames></author><author><keyname>Tarniceriu</keyname><forenames>Daniela</forenames></author></authors><title>Improved Method for Searching of Interleavers Using Garello's Method</title><categories>cs.IT math.IT</categories><comments>submitted to Wireless Communications and Mobile Computing, 24 pages,
  5 tables, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper an improved method for searching good interleavers from a
certain set is proposed. The first few terms, corresponding to maximum distance
of approximately 40 of the distance spectra, for turbo codes using these
interleavers are determined by means of Garello's method. The method is applied
to find quadratic permutation polynomials (QPP) based interleavers. Compared to
previous methods for founding QPP based interleavers, the search complexity is
reduced, allowing to find interleavers of higher length. This method has been
applied for QPP interleavers with lengths from the LTE (Long Term Evolution)
standard up to 1504. The analyzed classes are those with the largest spread QPP
(LS-QPP), with the D parameter equal to that of LTE interleaver (D_L_T_E-QPP),
and the class consisting of all QPP interleavers for lengths up to 1008. The
distance spectrum optimization is made for all classes. For the class of LS-QPP
interleavers of small lengths, the search led to superior or at least equal
performances with those of the LTE standard. For larger lengths the search in
the class of D_L_T_E-QPP interleavers is preferred. The interleavers from the
entire class of QPPs lead, in general, to weaker FER (Frame Error Rate)
performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1418</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1418</id><created>2012-03-07</created><updated>2012-04-16</updated><authors><author><keyname>Su</keyname><forenames>Wei</forenames></author><author><keyname>Tang</keyname><forenames>Xiaohu</forenames></author><author><keyname>Pott</keyname><forenames>Alexander</forenames></author></authors><title>A Note on a Conjecture for Balanced Elementary Symmetric Boolean
  Functions</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In 2008, Cusick {\it et al.} conjectured that certain elementary symmetric
Boolean functions of the form $\sigma_{2^{t+1}l-1, 2^t}$ are the only nonlinear
balanced ones, where $t$, $l$ are any positive integers, and
$\sigma_{n,d}=\bigoplus_{1\le i_1&lt;...&lt;i_d\le n}x_{i_1}x_{i_2}...x_{i_d}$ for
positive integers $n$, $1\le d\le n$. In this note, by analyzing the weight of
$\sigma_{n, 2^t}$ and $\sigma_{n, d}$, we prove that ${\rm wt}(\sigma_{n,
d})&lt;2^{n-1}$ holds in most cases, and so does the conjecture. According to the
remainder of modulo 4, we also consider the weight of $\sigma_{n, d}$ from two
aspects: $n\equiv 3({\rm mod\}4)$ and $n\not\equiv 3({\rm mod\}4)$. Thus, we
can simplify the conjecture. In particular, our results cover the most known
results. In order to fully solve the conjecture, we also consider the weight of
$\sigma_{n, 2^t+2^s}$ and give some experiment results on it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1426</identifier>
 <datestamp>2013-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1426</id><created>2012-03-07</created><updated>2013-05-24</updated><authors><author><keyname>Altarelli</keyname><forenames>Fabrizio</forenames></author><author><keyname>Braunstein</keyname><forenames>Alfredo</forenames></author><author><keyname>Dall'Asta</keyname><forenames>Luca</forenames></author><author><keyname>Zecchina</keyname><forenames>Riccardo</forenames></author></authors><title>Optimizing spread dynamics on graphs by message passing</title><categories>cond-mat.dis-nn cs.SI math.OC</categories><comments>Replacement for &quot;The Spread Optimization Problem&quot;</comments><journal-ref>J. Stat. Mech. 2013, P09011 (2013)</journal-ref><doi>10.1088/1742-5468/2013/09/P09011</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cascade processes are responsible for many important phenomena in natural and
social sciences. Simple models of irreversible dynamics on graphs, in which
nodes activate depending on the state of their neighbors, have been
successfully applied to describe cascades in a large variety of contexts. Over
the last decades, many efforts have been devoted to understand the typical
behaviour of the cascades arising from initial conditions extracted at random
from some given ensemble. However, the problem of optimizing the trajectory of
the system, i.e. of identifying appropriate initial conditions to maximize (or
minimize) the final number of active nodes, is still considered to be
practically intractable, with the only exception of models that satisfy a sort
of diminishing returns property called submodularity. Submodular models can be
approximately solved by means of greedy strategies, but by definition they lack
cooperative characteristics which are fundamental in many real systems. Here we
introduce an efficient algorithm based on statistical physics for the
optimization of trajectories in cascade processes on graphs. We show that for a
wide class of irreversible dynamics, even in the absence of submodularity, the
spread optimization problem can be solved efficiently on large networks.
Analytic and algorithmic results on random graphs are complemented by the
solution of the spread maximization problem on a real-world network (the
Epinions consumer reviews network).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1429</identifier>
 <datestamp>2013-06-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1429</id><created>2012-03-07</created><updated>2013-06-05</updated><authors><author><keyname>Dabbene</keyname><forenames>Fabrizio</forenames></author><author><keyname>Sznaier</keyname><forenames>Mario</forenames></author><author><keyname>Tempo</keyname><forenames>Roberto</forenames></author></authors><title>Probabilistic Optimal Estimation and Filtering under Uncertainty</title><categories>cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The classical approach to system identification is based on stochastic
assumptions about the measurement error, and provides estimates that have
random nature. Worst-case identification, on the other hand, only assumes the
knowledge of deterministic error bounds, and establishes guaranteed estimates,
thus being in principle better suited for the use in control design. However, a
main limitation of such deterministic bounds lies on their potential
conservatism, thus leading to estimates of restricted use.
  In this paper, we propose a rapprochement between the stochastic and
worst-case paradigms. In particular, based on a probabilistic framework for
linear estimation problems, we derive new computational results. These results
combine elements from information-based complexity with recent developments in
the theory of randomized algorithms. The main idea in this line of research is
to &quot;discard&quot; sets of measure at most \epsilon, where \epsilon is a
probabilistic accuracy, from the set of deterministic estimates. Therefore, we
are decreasing the so-called worst-case radius of information at the expense of
a given probabilistic ``risk.&quot;
  In this setting, we compute a trade-off curve, called violation function,
which shows how the radius of information decreases as a function of the
accuracy. To this end, we construct randomized and deterministic algorithms
which provide approximations of this function. We report extensive simulations
showing numerical comparisons between the stochastic, worst-case and
probabilistic approaches, thus demonstrating the efficacy of the methods
proposed in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1435</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1435</id><created>2012-03-07</created><updated>2013-01-17</updated><authors><author><keyname>Bercher</keyname><forenames>J. -F.</forenames></author></authors><title>On a (\beta,q)-generalized Fisher information and inequalities involving
  q-Gaussian distributions</title><categories>math-ph cond-mat.stat-mech cs.IT math.IT math.MP</categories><comments>v2: corrected equation (A5)</comments><journal-ref>J. Math. Phys. 53, 063303 (2012)</journal-ref><doi>10.1063/1.4726197</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the present paper, we would like to draw attention to a possible
generalized Fisher information that fits well in the formalism of nonextensive
thermostatistics. This generalized Fisher information is defined for densities
on $\mathbb{R}^{n}.$ Just as the maximum R\'enyi or Tsallis entropy subject to
an elliptic moment constraint is a generalized q-Gaussian, we show that the
minimization of the generalized Fisher information also leads a generalized
q-Gaussian. This yields a generalized Cram\'er-Rao inequality. In addition, we
show that the generalized Fisher information naturally pops up in a simple
inequality that links the generalized entropies, the generalized Fisher
information and an elliptic moment. Finally, we give an extended Stam
inequality. In this series of results, the extremal functions are the
generalized q-Gaussians. Thus, these results complement the classical
characterization of the generalized q-Gaussian and introduce a generalized
Fisher information as a new information measure in nonextensive
thermostatistics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1439</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1439</id><created>2012-03-07</created><updated>2012-12-10</updated><authors><author><keyname>Prignano</keyname><forenames>Luce</forenames></author><author><keyname>Moreno</keyname><forenames>Yamir</forenames></author><author><keyname>Diaz-Guilera</keyname><forenames>Albert</forenames></author></authors><title>Exploring complex networks by means of adaptive walkers</title><categories>nlin.AO cond-mat.dis-nn cs.SI physics.soc-ph</categories><comments>10 pages, 10 figures</comments><doi>10.1103/PhysRevE.86.066116</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Finding efficient algorithms to explore large networks with the aim of
recovering information about their structure is an open problem. Here, we
investigate this challenge by proposing a model in which random walkers with
previously assigned home nodes navigate through the network during a fixed
amount of time. We consider that the exploration is successful if the walker
gets the information gathered back home, otherwise, no data is retrieved.
Consequently, at each time step, the walkers, with some probability, have the
choice to either go backward approaching their home or go farther away. We show
that there is an optimal solution to this problem in terms of the average
information retrieved and the degree of the home nodes and design an adaptive
strategy based on the behavior of the random walker. Finally, we compare
different strategies that emerge from the model in the context of network
reconstruction. Our results could be useful for the discovery of unknown
connections in large scale networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1448</identifier>
 <datestamp>2012-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1448</id><created>2012-03-07</created><updated>2012-03-08</updated><authors><author><keyname>Radul</keyname><forenames>Alexey</forenames></author><author><keyname>Pearlmutter</keyname><forenames>Barak A.</forenames></author><author><keyname>Siskind</keyname><forenames>Jeffrey Mark</forenames></author></authors><title>AD in Fortran, Part 1: Design</title><categories>cs.PL cs.MS cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose extensions to Fortran which integrate forward and reverse
Automatic Differentiation (AD) directly into the programming model.
Irrespective of implementation technology, embedding AD constructs directly
into the language extends the reach and convenience of AD while allowing
abstraction of concepts of interest to scientific-computing practice, such as
root finding, optimization, and finding equilibria of continuous games.
Multiple different subprograms for these tasks can share common interfaces,
regardless of whether and how they use AD internally. A programmer can maximize
a function F by calling a library maximizer, XSTAR=ARGMAX(F,X0), which
internally constructs derivatives of F by AD, without having to learn how to
use any particular AD tool. We illustrate the utility of these extensions by
example: programs become much more concise and closer to traditional
mathematical notation. A companion paper describes how these extensions can be
implemented by a program that generates input to existing Fortran-based AD
tools.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1450</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1450</id><created>2012-03-07</created><updated>2012-03-08</updated><authors><author><keyname>Radul</keyname><forenames>Alexey</forenames></author><author><keyname>Pearlmutter</keyname><forenames>Barak A.</forenames></author><author><keyname>Siskind</keyname><forenames>Jeffrey Mark</forenames></author></authors><title>AD in Fortran, Part 2: Implementation via Prepreprocessor</title><categories>cs.PL cs.MS cs.NA</categories><journal-ref>Recent Advances in Algorithmic Differentiation, Springer Lecture
  Notes in Computational Science and Engineering volume 87, 2012, ISBN
  978-3-642-30022-6, pages 273-284</journal-ref><doi>10.1007/978-3-642-30023-3_25</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe an implementation of the Farfel Fortran AD extensions. These
extensions integrate forward and reverse AD directly into the programming
model, with attendant benefits to flexibility, modularity, and ease of use. The
implementation we describe is a &quot;prepreprocessor&quot; that generates input to
existing Fortran-based AD tools. In essence, blocks of code which are targeted
for AD by Farfel constructs are put into subprograms which capture their
lexical variable context, and these are closure-converted into top-level
subprograms and specialized to eliminate EXTERNAL arguments, rendering them
amenable to existing AD preprocessors, which are then invoked, possibly
repeatedly if the AD is nested.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1457</identifier>
 <datestamp>2012-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1457</id><created>2012-03-07</created><authors><author><keyname>Fercoq</keyname><forenames>Olivier</forenames></author></authors><title>PageRank optimization applied to spam detection</title><categories>math.OC cs.IR</categories><comments>8 pages, 6 figures</comments><msc-class>90C40, 90C90</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a new link spam detection and PageRank demotion algorithm called
MaxRank. Like TrustRank and AntiTrustRank, it starts with a seed of hand-picked
trusted and spam pages. We define the MaxRank of a page as the frequency of
visit of this page by a random surfer minimizing an average cost per time unit.
On a given page, the random surfer selects a set of hyperlinks and clicks with
uniform probability on any of these hyperlinks. The cost function penalizes
spam pages and hyperlink removals. The goal is to determine a hyperlink
deletion policy that minimizes this score. The MaxRank is interpreted as a
modified PageRank vector, used to sort web pages instead of the usual PageRank
vector. The bias vector of this ergodic control problem, which is unique up to
an additive constant, is a measure of the &quot;spamicity&quot; of each page, used to
detect spam pages. We give a scalable algorithm for MaxRank computation that
allowed us to perform experimental results on the WEBSPAM-UK2007 dataset. We
show that our algorithm outperforms both TrustRank and AntiTrustRank for spam
and nonspam page detection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1463</identifier>
 <datestamp>2012-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1463</id><created>2012-03-07</created><updated>2012-07-10</updated><authors><author><keyname>Gupta</keyname><forenames>Anuj</forenames></author><author><keyname>Gopal</keyname><forenames>Prasant</forenames></author><author><keyname>Bansal</keyname><forenames>Piyush</forenames></author><author><keyname>Srinathan</keyname><forenames>Kannan</forenames></author></authors><title>A New Look at Composition of Authenticated Byzantine Generals</title><categories>cs.CR cs.DC</categories><comments>27 pages. Keywords: Protocol composition, Authenticated Byzantine
  Generals, Universal composability, Unique session identifiers</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of Authenticated Byzantine Generals (ABG) aims to simulate a
virtual reliable broadcast channel from the General to all the players via a
protocol over a real (point-to-point) network in the presence of faults. We
propose a new model to study the self-composition of ABG protocols. The central
dogma of our approach can be phrased as follows: Consider a player who
diligently executes (only) the delegated protocol but the adversary steals some
private information from him. Should such a player be considered faulty? With
respect to ABG protocols, we argue that the answer has to be no.
  In the new model we show that in spite of using unique session identifiers,
if $n &lt; 2t$, there cannot exist any ABG protocol that composes in parallel even
twice. Further, for $n \geq 2t$, we design ABG protocols that compose for any
number of parallel executions. Besides investigating the composition of ABG
under a new light, our work also brings out several new insights into Canetti's
Universal Composability framework. Specifically, we show that there are several
undesirable effects if one deviates from our dogma. This provides further
evidence as to why our dogma is the right framework to study the composition of
ABG protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1466</identifier>
 <datestamp>2012-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1466</id><created>2012-03-07</created><authors><author><keyname>Murri</keyname><forenames>Riccardo</forenames></author><author><keyname>Maffioletti</keyname><forenames>Sergio</forenames></author></authors><title>Batch-oriented software appliances</title><categories>cs.DC</categories><comments>11 pages, no figures. Submitted to VTDC'12</comments><acm-class>D.4.9; C.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents AppPot, a system for creating Linux software appliances.
AppPot can be run as a regular batch or grid job and executed in user space,
and requires no special virtualization support in the infrastructure.
  The main design goal of AppPot is to bring the benefits of a
virtualization-based IaaS cloud to existing batch-oriented computing
infrastructures.
  In particular, AppPot addresses the application deployment and configuration
on large heterogeneous computing infrastructures: users are enabled to prepare
their own customized virtual appliance for providing a safe execution
environment for their applications. These appliances can then be executed on
virtually any computing infrastructure being in a private or public cloud as
well as any batch-controlled computing clusters the user may have access to.
  We give an overview of AppPot and its features, the technology that makes it
possible, and report on experiences running it in production use within the
Swiss National Grid infrastructure SMSCG.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1468</identifier>
 <datestamp>2012-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1468</id><created>2012-03-07</created><updated>2012-03-07</updated><authors><author><keyname>Liu</keyname><forenames>Di</forenames></author><author><keyname>Lease</keyname><forenames>Matthew</forenames></author><author><keyname>Kuipers</keyname><forenames>Rebecca</forenames></author><author><keyname>Bias</keyname><forenames>Randolph</forenames></author></authors><title>Crowdsourcing for Usability Testing</title><categories>cs.HC</categories><comments>10 pages</comments><acm-class>H.5.2; H.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While usability evaluation is critical to designing usable websites,
traditional usability testing can be both expensive and time consuming. The
advent of crowdsourcing platforms such as Amazon Mechanical Turk and
CrowdFlower offer an intriguing new avenue for performing remote usability
testing with potentially many users, quick turn-around, and significant cost
savings. To investigate the potential of such crowdsourced usability testing,
we conducted two similar (though not completely parallel) usability studies
which evaluated a graduate school's website: one via a traditional usability
lab setting, and the other using crowdsourcing. While we find crowdsourcing
exhibits some notable limitations in comparison to the traditional lab
environment, its applicability and value for usability testing is clearly
evidenced. We discuss both methodological differences for crowdsourced
usability testing, as well as empirical contrasts to results from more
traditional, face-to-face usability testing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1483</identifier>
 <datestamp>2012-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1483</id><created>2012-03-07</created><authors><author><keyname>B&#x103;z&#x103;van</keyname><forenames>Eduard Gabriel</forenames></author><author><keyname>Li</keyname><forenames>Fuxin</forenames></author><author><keyname>Sminchisescu</keyname><forenames>Cristian</forenames></author></authors><title>Learning Random Kernel Approximations for Object Recognition</title><categories>cs.CV cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Approximations based on random Fourier features have recently emerged as an
efficient and formally consistent methodology to design large-scale kernel
machines. By expressing the kernel as a Fourier expansion, features are
generated based on a finite set of random basis projections, sampled from the
Fourier transform of the kernel, with inner products that are Monte Carlo
approximations of the original kernel. Based on the observation that different
kernel-induced Fourier sampling distributions correspond to different kernel
parameters, we show that an optimization process in the Fourier domain can be
used to identify the different frequency bands that are useful for prediction
on training data. Moreover, the application of group Lasso to random feature
vectors corresponding to a linear combination of multiple kernels, leads to
efficient and scalable reformulations of the standard multiple kernel learning
model \cite{Varma09}. In this paper we develop the linear Fourier approximation
methodology for both single and multiple gradient-based kernel learning and
show that it produces fast and accurate predictors on a complex dataset such as
the Visual Object Challenge 2011 (VOC2011).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1495</identifier>
 <datestamp>2012-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1495</id><created>2012-03-07</created><authors><author><keyname>Genet</keyname><forenames>Thomas</forenames></author><author><keyname>Gall</keyname><forenames>Tristan Le</forenames></author><author><keyname>Legay</keyname><forenames>Axel</forenames></author><author><keyname>Murat</keyname><forenames>Valerie</forenames></author></authors><title>Tree Regular Model Checking for Lattice-Based Automata</title><categories>cs.FL cs.LO</categories><comments>Technical report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tree Regular Model Checking (TRMC) is the name of a family of techniques for
analyzing infinite-state systems in which states are represented by terms, and
sets of states by Tree Automata (TA). The central problem in TRMC is to decide
whether a set of bad states is reachable. The problem of computing a TA
representing (an over- approximation of) the set of reachable states is
undecidable, but efficient solutions based on completion or iteration of tree
transducers exist. Unfortunately, the TRMC framework is unable to efficiently
capture both the complex structure of a system and of some of its features. As
an example, for JAVA programs, the structure of a term is mainly exploited to
capture the structure of a state of the system. On the counter part, integers
of the java programs have to be encoded with Peano numbers, which means that
any algebraic operation is potentially represented by thousands of applications
of rewriting rules. In this paper, we propose Lattice Tree Automata (LTAs), an
extended version of tree automata whose leaves are equipped with lattices. LTAs
allow us to represent possibly infinite sets of interpreted terms. Such terms
are capable to represent complex domains and related operations in an efficient
manner. We also extend classical Boolean operations to LTAs. Finally, as a
major contribution, we introduce a new completion-based algorithm for computing
the possibly infinite set of reachable interpreted terms in a finite amount of
time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1502</identifier>
 <datestamp>2012-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1502</id><created>2012-02-27</created><authors><author><keyname>Giot</keyname><forenames>Romain</forenames><affiliation>GREYC</affiliation></author><author><keyname>Rosenberger</keyname><forenames>Christophe</forenames><affiliation>GREYC</affiliation></author><author><keyname>Dorizzi</keyname><forenames>Bernadette</forenames><affiliation>SAMOVAR</affiliation></author></authors><title>Performance Evaluation of Biometric Template Update</title><categories>cs.OH cs.CR</categories><comments>International Biometric Performance Testing Conference 2012,
  Gaithersburg, MD, USA : United States (2012)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Template update allows to modify the biometric reference of a user while he
uses the biometric system. With such kind of mechanism we expect the biometric
system uses always an up to date representation of the user, by capturing his
intra-class (temporary or permanent) variability. Although several studies
exist in the literature, there is no commonly adopted evaluation scheme. This
does not ease the comparison of the different systems of the literature. In
this paper, we show that using different evaluation procedures can lead in
different, and contradictory, interpretations of the results. We use a
keystroke dynamics (which is a modality suffering of template ageing quickly)
template update system on a dataset consisting of height different sessions to
illustrate this point. Even if we do not answer to this problematic, it shows
that it is necessary to normalize the template update evaluation procedures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1505</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1505</id><created>2012-03-07</created><updated>2013-12-02</updated><authors><author><keyname>Bianchi</keyname><forenames>Pascal</forenames></author><author><keyname>Fort</keyname><forenames>Gersende</forenames></author><author><keyname>Hachem</keyname><forenames>Walid</forenames></author></authors><title>Performance of a Distributed Stochastic Approximation Algorithm</title><categories>math.OC cs.DC cs.SY</categories><comments>IEEE Transactions on Information Theory 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a distributed stochastic approximation algorithm is studied.
Applications of such algorithms include decentralized estimation, optimization,
control or computing. The algorithm consists in two steps: a local step, where
each node in a network updates a local estimate using a stochastic
approximation algorithm with decreasing step size, and a gossip step, where a
node computes a local weighted average between its estimates and those of its
neighbors. Convergence of the estimates toward a consensus is established under
weak assumptions. The approach relies on two main ingredients: the existence of
a Lyapunov function for the mean field in the agreement subspace, and a
contraction property of the random matrices of weights in the subspace
orthogonal to the agreement subspace. A second order analysis of the algorithm
is also performed under the form of a Central Limit Theorem. The
Polyak-averaged version of the algorithm is also considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1506</identifier>
 <datestamp>2012-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1506</id><created>2012-03-07</created><updated>2012-04-27</updated><authors><author><keyname>Dietzfelbinger</keyname><forenames>Martin</forenames></author><author><keyname>Rink</keyname><forenames>Michael</forenames></author></authors><title>Towards Optimal Degree-distributions for Left-perfect Matchings in
  Random Bipartite Graphs</title><categories>cs.DM</categories><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a random bipartite multigraph $G$ with $n$ left nodes and $m \geq n
\geq 2$ right nodes. Each left node $x$ has $d_x \geq 1$ random right
neighbors. The average left degree $\Delta$ is fixed, $\Delta \geq 2$. We ask
whether for the probability that $G$ has a left-perfect matching it is
advantageous not to fix $d_x$ for each left node $x$ but rather choose it at
random according to some (cleverly chosen) distribution. We show the following,
provided that the degrees of the left nodes are independent: If $\Delta$ is an
integer then it is optimal to use a fixed degree of $\Delta$ for all left
nodes. If $\Delta$ is non-integral then an optimal degree-distribution has the
property that each left node $x$ has two possible degrees, $\floor{\Delta}$ and
$\ceil{\Delta}$, with probability $p_x$ and $1-p_x$, respectively, where $p_x$
is from the closed interval $[0,1]$ and the average over all $p_x$ equals
$\ceil{\Delta}-\Delta$. Furthermore, if $n=c\cdot m$ and $\Delta&gt;2$ is
constant, then each distribution of the left degrees that meets the conditions
above determines the same threshold $c^*(\Delta)$ that has the following
property as $n$ goes to infinity: If $c&lt;c^*(\Delta)$ then there exists a
left-perfect matching with high probability. If $c&gt;c^*(\Delta)$ then there
exists no left-perfect matching with high probability. The threshold
$c^*(\Delta)$ is the same as the known threshold for offline $k$-ary cuckoo
hashing for integral or non-integral $k=\Delta$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1513</identifier>
 <datestamp>2012-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1513</id><created>2012-03-05</created><updated>2012-03-08</updated><authors><author><keyname>Bruna</keyname><forenames>Joan</forenames></author><author><keyname>Mallat</keyname><forenames>St&#xe9;phane</forenames></author></authors><title>Invariant Scattering Convolution Networks</title><categories>cs.CV</categories><comments>15 pages double column, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A wavelet scattering network computes a translation invariant image
representation, which is stable to deformations and preserves high frequency
information for classification. It cascades wavelet transform convolutions with
non-linear modulus and averaging operators. The first network layer outputs
SIFT-type descriptors whereas the next layers provide complementary invariant
information which improves classification. The mathematical analysis of wavelet
scattering networks explains important properties of deep convolution networks
for classification.
  A scattering representation of stationary processes incorporates higher order
moments and can thus discriminate textures having the same Fourier power
spectrum. State of the art classification results are obtained for handwritten
digits and texture discrimination, using a Gaussian kernel SVM and a generative
PCA classifier.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1515</identifier>
 <datestamp>2015-05-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1515</id><created>2012-03-07</created><updated>2015-05-11</updated><authors><author><keyname>Khaleghi</keyname><forenames>Azadeh</forenames></author><author><keyname>Ryabko</keyname><forenames>Daniil</forenames></author></authors><title>Multiple Change Point Estimation in Stationary Ergodic Time Series</title><categories>stat.ML cs.IT math.IT math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a heterogeneous time-series sample, the objective is to find points in
time (called change points) where the probability distribution generating the
data has changed. The data are assumed to have been generated by arbitrary
unknown stationary ergodic distributions. No modelling, independence or mixing
assumptions are made. A novel, computationally efficient, nonparametric method
is proposed, and is shown to be asymptotically consistent in this general
framework. The theoretical results are complemented with experimental
evaluations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1521</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1521</id><created>2012-03-07</created><updated>2013-03-09</updated><authors><author><keyname>Chen</keyname><forenames>Laming</forenames></author><author><keyname>Gu</keyname><forenames>Yuantao</forenames></author></authors><title>Oracle-order Recovery Performance of Greedy Pursuits with Replacement
  against General Perturbations</title><categories>cs.IT math.IT</categories><comments>27 pages, 4 figures, 5 tables</comments><doi>10.1109/TSP.2013.2272551</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Applying the theory of compressive sensing in practice always takes different
kinds of perturbations into consideration. In this paper, the recovery
performance of greedy pursuits with replacement for sparse recovery is analyzed
when both the measurement vector and the sensing matrix are contaminated with
additive perturbations. Specifically, greedy pursuits with replacement include
three algorithms, compressive sampling matching pursuit (CoSaMP), subspace
pursuit (SP), and iterative hard thresholding (IHT), where the support
estimation is evaluated and updated in each iteration. Based on restricted
isometry property, a unified form of the error bounds of these recovery
algorithms is derived under general perturbations for compressible signals. The
results reveal that the recovery performance is stable against both
perturbations. In addition, these bounds are compared with that of oracle
recovery--- least squares solution with the locations of some largest entries
in magnitude known a priori. The comparison shows that the error bounds of
these algorithms only differ in coefficients from the lower bound of oracle
recovery for some certain signal and perturbations, as reveals that
oracle-order recovery performance of greedy pursuits with replacement is
guaranteed. Numerical simulations are performed to verify the conclusions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1524</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1524</id><created>2012-03-07</created><authors><author><keyname>Tu</keyname><forenames>Sheng-Yuan</forenames></author><author><keyname>Sayed</keyname><forenames>Ali H.</forenames></author></authors><title>On the Influence of Informed Agents on Learning and Adaptation over
  Networks</title><categories>cs.IT cs.SI math.IT</categories><comments>35 pages, 8 figures</comments><doi>10.1109/TSP.2012.2230167</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Adaptive networks consist of a collection of agents with adaptation and
learning abilities. The agents interact with each other on a local level and
diffuse information across the network through their collaborations. In this
work, we consider two types of agents: informed agents and uninformed agents.
The former receive new data regularly and perform consultation and in-network
tasks, while the latter do not collect data and only participate in the
consultation tasks. We examine the performance of adaptive networks as a
function of the proportion of informed agents and their distribution in space.
The results reveal some interesting and surprising trade-offs between
convergence rate and mean-square performance. In particular, among other
results, it is shown that the performance of adaptive networks does not
necessarily improve with a larger proportion of informed agents. Instead, it is
established that the larger the proportion of informed agents is, the faster
the convergence rate of the network becomes albeit at the expense of some
deterioration in mean-square performance. The results further establish that
uninformed agents play an important role in determining the steady-state
performance of the network, and that it is preferable to keep some of the
highly connected agents uninformed. The arguments reveal an important interplay
among three factors: the number and distribution of informed agents in the
network, the convergence rate of the learning process, and the estimation
accuracy in steady-state. Expressions that quantify these relations are
derived, and simulations are included to support the theoretical findings. We
further apply the results to two models that are widely used to represent
behavior over complex networks, namely, the Erdos-Renyi and scale-free models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1525</identifier>
 <datestamp>2012-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1525</id><created>2012-03-07</created><authors><author><keyname>H&#xe4;hnle</keyname><forenames>Nicolai</forenames></author></authors><title>Constructing subset partition graphs with strong adjacency and end-point
  count properties</title><categories>math.CO cs.CG cs.DM</categories><msc-class>52B05, 05C12, 90C05</msc-class><acm-class>G.2.2; G.1.6</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Kim defined a very general combinatorial abstraction of the diameter of
polytopes called subset partition graphs to study how certain combinatorial
properties of such graphs may be achieved in lower bound constructions. Using
Lov\'asz' Local Lemma, we give a general randomized construction for subset
partition graphs satisfying strong adjacency and end-point count properties.
This can be used as a building block to conceptually simplify the constructions
given in [Kim11].
  We also use our method to construct abstract spindles, an analogy to the
spindles used by Santos to disprove the Hirsch conjecture, of exponential
length which satisfy the adjacency and end-point count properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1527</identifier>
 <datestamp>2012-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1527</id><created>2012-03-07</created><updated>2012-10-22</updated><authors><author><keyname>Freibert</keyname><forenames>Finley</forenames></author><author><keyname>Kim</keyname><forenames>Jon-Lark</forenames></author></authors><title>Optimum Subcodes of Self-Dual Codes and Their Optimum Distance Profiles</title><categories>cs.IT math.CO math.IT</categories><comments>This paper is a revised version with the title &quot;Optimal Subcodes of
  Self-Dual Codes and Their Optimum Distance Profiles&quot;</comments><msc-class>94B05, 11T71</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Binary optimal codes often contain optimal or near-optimal subcodes. In this
paper we show that this is true for the family of self-dual codes. One approach
is to compute the optimum distance profiles (ODPs) of linear codes, which was
introduced by Luo, et. al. (2010). One of our main results is the development
of general algorithms, called the Chain Algorithms, for finding ODPs of linear
codes. Then we determine the ODPs for the Type II codes of lengths up to 24 and
the extremal Type II codes of length 32, give a partial result of the ODP of
the extended quadratic residue code $q_{48}$ of length 48. We also show that
there does not exist a $[48,k,16]$ subcode of $q_{48}$ for $k \ge 17$, and we
find a first example of a doubly-even self-complementary $[48, 16, 16]$ code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1528</identifier>
 <datestamp>2015-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1528</id><created>2012-03-07</created><authors><author><keyname>Karout</keyname><forenames>Johnny</forenames><affiliation>Student Member, IEEE</affiliation></author><author><keyname>Kramer</keyname><forenames>Gerhard</forenames><affiliation>Fellow, IEEE</affiliation></author><author><keyname>Kschischang</keyname><forenames>Frank R.</forenames><affiliation>Fellow, IEEE</affiliation></author><author><keyname>Agrell</keyname><forenames>Erik</forenames></author></authors><title>A Two-Dimensional Signal Space for Intensity-Modulated Channels</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Communications Letters, Feb. 2012</comments><journal-ref>IEEE Communications Letters, vol. 16, no. 9, pp. 1361-1364, Sept.
  2012</journal-ref><doi>10.1109/LCOMM.2012.072012.121057</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A two-dimensional signal space for intensity- modulated channels is
presented. Modulation formats using this signal space are designed to maximize
the minimum distance between signal points while satisfying average and peak
power constraints. The uncoded, high-signal-to-noise ratio, power and spectral
efficiencies are compared to those of the best known formats. The new formats
are simpler than existing subcarrier formats, and are superior if the bandwidth
is measured as 90% in-band power. Existing subcarrier formats are better if the
bandwidth is measured as 99% in-band power.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1535</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1535</id><created>2012-03-07</created><updated>2013-03-09</updated><authors><author><keyname>Su</keyname><forenames>Guolong</forenames></author><author><keyname>Jin</keyname><forenames>Jian</forenames></author><author><keyname>Gu</keyname><forenames>Yuantao</forenames></author><author><keyname>Wang</keyname><forenames>Jian</forenames></author></authors><title>Performance Analysis of l_0 Norm Constraint Least Mean Square Algorithm</title><categories>cs.IT cs.PF math.IT</categories><comments>31 pages, 8 figures</comments><journal-ref>IEEE Transactions on Signal Processing, 60(5): 2223-2235, 2012</journal-ref><doi>10.1109/TSP.2012.2184537</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As one of the recently proposed algorithms for sparse system identification,
$l_0$ norm constraint Least Mean Square ($l_0$-LMS) algorithm modifies the cost
function of the traditional method with a penalty of tap-weight sparsity. The
performance of $l_0$-LMS is quite attractive compared with its various
precursors. However, there has been no detailed study of its performance. This
paper presents all-around and throughout theoretical performance analysis of
$l_0$-LMS for white Gaussian input data based on some reasonable assumptions.
Expressions for steady-state mean square deviation (MSD) are derived and
discussed with respect to algorithm parameters and system sparsity. The
parameter selection rule is established for achieving the best performance.
Approximated with Taylor series, the instantaneous behavior is also derived. In
addition, the relationship between $l_0$-LMS and some previous arts and the
sufficient conditions for $l_0$-LMS to accelerate convergence are set up.
Finally, all of the theoretical results are compared with simulations and are
shown to agree well in a large range of parameter setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1536</identifier>
 <datestamp>2012-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1536</id><created>2012-03-07</created><authors><author><keyname>Biagioni</keyname><forenames>Andrea</forenames></author><author><keyname>Cicero</keyname><forenames>Francesca Lo</forenames></author><author><keyname>Lonardo</keyname><forenames>Alessandro</forenames></author><author><keyname>Paolucci</keyname><forenames>Pier Stanislao</forenames></author><author><keyname>Perra</keyname><forenames>Mersia</forenames></author><author><keyname>Rossetti</keyname><forenames>Davide</forenames></author><author><keyname>Sidore</keyname><forenames>Carlo</forenames></author><author><keyname>Simula</keyname><forenames>Francesco</forenames></author><author><keyname>Tosoratto</keyname><forenames>Laura</forenames></author><author><keyname>Vicini</keyname><forenames>Piero</forenames></author></authors><title>The Distributed Network Processor: a novel off-chip and on-chip
  interconnection network architecture</title><categories>cs.AR cs.NI</categories><comments>8 pages, 11 figures, submitted to Hot Interconnect 2009</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the most demanding challenges for the designers of parallel computing
architectures is to deliver an efficient network infrastructure providing low
latency, high bandwidth communications while preserving scalability. Besides
off-chip communications between processors, recent multi-tile (i.e. multi-core)
architectures face the challenge for an efficient on-chip interconnection
network between processor's tiles. In this paper, we present a configurable and
scalable architecture, based on our Distributed Network Processor (DNP) IP
Library, targeting systems ranging from single MPSoCs to massive HPC platforms.
The DNP provides inter-tile services for both on-chip and off-chip
communications with a uniform RDMA style API, over a multi-dimensional direct
network with a (possibly) hybrid topology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1538</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1538</id><created>2012-03-07</created><updated>2013-03-09</updated><authors><author><keyname>Wang</keyname><forenames>Xiaohan</forenames></author><author><keyname>Gu</keyname><forenames>Yuantao</forenames></author><author><keyname>Chen</keyname><forenames>Laming</forenames></author></authors><title>Proof of Convergence and Performance Analysis for Sparse Recovery via
  Zero-point Attracting Projection</title><categories>cs.IT cs.PF math.IT</categories><comments>29 pages, 6 figures</comments><journal-ref>IEEE Transactions on Signal Processing, 60(8): 4081-4093, 2012</journal-ref><doi>10.1109/TSP.2012.2195660</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A recursive algorithm named Zero-point Attracting Projection (ZAP) is
proposed recently for sparse signal reconstruction. Compared with the reference
algorithms, ZAP demonstrates rather good performance in recovery precision and
robustness. However, any theoretical analysis about the mentioned algorithm,
even a proof on its convergence, is not available. In this work, a strict proof
on the convergence of ZAP is provided and the condition of convergence is put
forward. Based on the theoretical analysis, it is further proved that ZAP is
non-biased and can approach the sparse solution to any extent, with the proper
choice of step-size. Furthermore, the case of inaccurate measurements in noisy
scenario is also discussed. It is proved that disturbance power linearly
reduces the recovery precision, which is predictable but not preventable. The
reconstruction deviation of $p$-compressible signal is also provided. Finally,
numerical simulations are performed to verify the theoretical analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1539</identifier>
 <datestamp>2015-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1539</id><created>2012-03-07</created><authors><author><keyname>Bauer</keyname><forenames>Andrej</forenames></author><author><keyname>Pretnar</keyname><forenames>Matija</forenames></author></authors><title>Programming with Algebraic Effects and Handlers</title><categories>cs.PL</categories><acm-class>D.3.3; F.3.3</acm-class><journal-ref>Journal of Logical and Algebraic Methods in Programming. Volume
  84, Issue 1, January 2015, Pages 108-123</journal-ref><doi>10.1016/j.jlamp.2014.02.001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Eff is a programming language based on the algebraic approach to
computational effects, in which effects are viewed as algebraic operations and
effect handlers as homomorphisms from free algebras. Eff supports first-class
effects and handlers through which we may easily define new computational
effects, seamlessly combine existing ones, and handle them in novel ways. We
give a denotational semantics of eff and discuss a prototype implementation
based on it. Through examples we demonstrate how the standard effects are
treated in eff, and how eff supports programming techniques that use various
forms of delimited continuations, such as backtracking, breadth-first search,
selection functionals, cooperative multi-threading, and others.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1548</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1548</id><created>2012-03-07</created><updated>2013-03-09</updated><authors><author><keyname>You</keyname><forenames>Yang</forenames></author><author><keyname>Chen</keyname><forenames>Laming</forenames></author><author><keyname>Gu</keyname><forenames>Yuantao</forenames></author><author><keyname>Feng</keyname><forenames>Wei</forenames></author><author><keyname>Dai</keyname><forenames>Hui</forenames></author></authors><title>Retrieval of Sparse Solutions of Multiple-Measurement Vectors via
  Zero-point Attracting Projection</title><categories>cs.IT math.IT</categories><comments>9 pages, 4 figures</comments><journal-ref>Signal Processing, 92(12): 3075-3079, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new sparse signal recovery algorithm for multiple-measurement vectors (MMV)
problem is proposed in this paper. The sparse representation is iteratively
drawn based on the idea of zero-point attracting projection (ZAP). In each
iteration, the solution is first updated along the negative gradient direction
of an approximate $\ell_{2,0}$ norm to encourage sparsity, and then projected
to the solution space to satisfy the under-determined equation. A variable step
size scheme is adopted further to accelerate the convergence as well as to
improve the recovery accuracy. Numerical simulations demonstrate that the
performance of the proposed algorithm exceeds the references in various
aspects, as well as when applied to the Modulated Wideband Converter, where
recovering MMV problem is crucial to its performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1554</identifier>
 <datestamp>2012-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1554</id><created>2012-03-07</created><authors><author><keyname>Mehrotra</keyname><forenames>Sanjay</forenames></author><author><keyname>Papp</keyname><forenames>D&#xe1;vid</forenames></author></authors><title>Generating nested quadrature formulas for general weight functions with
  known moments</title><categories>math.NA cs.NA</categories><msc-class>65D32</msc-class><acm-class>G.1.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We revisit the problem of extending quadrature formulas for general weight
functions, and provide a generalization of Patterson's method for the constant
weight function. The method can be used to compute a nested sequence of
quadrature formulas for integration with respect to any continuous probability
measure on the real line with finite moments. The advantages of the method
include that it works directly with the moments of the underlying distribution,
and that for distributions with rational moments the existence of the formulas
can be verified by exact rational arithmetic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1568</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1568</id><created>2012-03-07</created><updated>2012-03-08</updated><authors><author><keyname>Houmansadr</keyname><forenames>Amir</forenames></author><author><keyname>Borisov</keyname><forenames>Nikita</forenames></author></authors><title>BotMosaic: Collaborative Network Watermark for Botnet Detection</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent research has made great strides in the field of detecting botnets.
However, botnets of all kinds continue to plague the Internet, as many ISPs and
organizations do not deploy these techniques. We aim to mitigate this state by
creating a very low-cost method of detecting infected bot host. Our approach is
to leverage the botnet detection work carried out by some organizations to
easily locate collaborating bots elsewhere. We created BotMosaic as a
countermeasure to IRC-based botnets. BotMosaic relies on captured bot instances
controlled by a watermarker, who inserts a particular pattern into their
network traffic. This pattern can then be detected at a very low cost by client
organizations and the watermark can be tuned to provide acceptable
false-positive rates. A novel feature of the watermark is that it is inserted
collaboratively into the flows of multiple captured bots at once, in order to
ensure the signal is strong enough to be detected. BotMosaic can also be used
to detect stepping stones and to help trace back to the botmaster. It is
content agnostic and can operate on encrypted traffic. We evaluate BotMosaic
using simulations and a testbed deployment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1569</identifier>
 <datestamp>2012-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1569</id><created>2012-03-07</created><updated>2012-04-06</updated><authors><author><keyname>Hartig</keyname><forenames>Olaf</forenames></author></authors><title>SPARQL for a Web of Linked Data: Semantics and Computability (Extended
  Version)</title><categories>cs.DB</categories><comments>v2: 55 pages, added Appendix D about constant reachability criteria,
  aligned with the final version published in ESWC 2012; v1: 52 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The World Wide Web currently evolves into a Web of Linked Data where content
providers publish and link data as they have done with hypertext for the last
20 years. While the declarative query language SPARQL is the de facto for
querying a-priory defined sets of data from the Web, no language exists for
querying the Web of Linked Data itself. However, it seems natural to ask
whether SPARQL is also suitable for such a purpose.
  In this paper we formally investigate the applicability of SPARQL as a query
language for Linked Data on the Web. In particular, we study two query models:
1) a full-Web semantics where the scope of a query is the complete set of
Linked Data on the Web and 2) a family of reachability-based semantics which
restrict the scope to data that is reachable by traversing certain data links.
For both models we discuss properties such as monotonicity and computability as
well as the implications of querying a Web that is infinitely large due to data
generating servers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1570</identifier>
 <datestamp>2013-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1570</id><created>2012-03-07</created><authors><author><keyname>Mardani</keyname><forenames>Morteza</forenames></author><author><keyname>Mateos</keyname><forenames>Gonzalo</forenames></author><author><keyname>Giannakis</keyname><forenames>Georgios B.</forenames></author></authors><title>In-network Sparsity-regularized Rank Minimization: Algorithms and
  Applications</title><categories>cs.MA cs.IT cs.NI math.IT stat.ML</categories><comments>30 pages, submitted for publication on the IEEE Trans. Signal Process</comments><doi>10.1109/TSP.2013.2279080</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a limited number of entries from the superposition of a low-rank matrix
plus the product of a known fat compression matrix times a sparse matrix,
recovery of the low-rank and sparse components is a fundamental task subsuming
compressed sensing, matrix completion, and principal components pursuit. This
paper develops algorithms for distributed sparsity-regularized rank
minimization over networks, when the nuclear- and $\ell_1$-norm are used as
surrogates to the rank and nonzero entry counts of the sought matrices,
respectively. While nuclear-norm minimization has well-documented merits when
centralized processing is viable, non-separability of the singular-value sum
challenges its distributed minimization. To overcome this limitation, an
alternative characterization of the nuclear norm is adopted which leads to a
separable, yet non-convex cost minimized via the alternating-direction method
of multipliers. The novel distributed iterations entail reduced-complexity
per-node tasks, and affordable message passing among single-hop neighbors.
Interestingly, upon convergence the distributed (non-convex) estimator provably
attains the global optimum of its centralized counterpart, regardless of
initialization. Several application domains are outlined to highlight the
generality and impact of the proposed framework. These include unveiling
traffic anomalies in backbone networks, predicting networkwide path latencies,
and mapping the RF ambiance using wireless cognitive radios. Simulations with
synthetic and real network data corroborate the convergence of the novel
distributed algorithm, and its centralized performance guarantees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1588</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1588</id><created>2012-03-07</created><updated>2013-07-02</updated><authors><author><keyname>Haija</keyname><forenames>Ahmad Abu Al</forenames></author><author><keyname>Vu</keyname><forenames>Mai</forenames></author></authors><title>Rate Maximization for Half-Duplex Multiple Access with Cooperating
  Transmitters</title><categories>cs.IT math.IT</categories><comments>To appear in IEEE Transactions on Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We derive the optimal resource allocation of a practical half-duplex scheme
for the Gaussian multiple access channel with transmitter cooperation (MAC-TC).
Based on rate splitting and superposition coding, two users transmit
information to a destination over 3 phases, such that the users partially
exchange their information during the first 2 phases and cooperatively transmit
to the destination during the last one. This scheme is near capacity-achieving
when the inter-user links are stronger than each user-destination link; it also
includes partial decode-forward relaying as a special case. We propose
efficient algorithms to find the optimal resource allocation for maximizing
either the individual or the sum rate and identify the corresponding optimal
scheme for each channel configuration. For fixed phase durations, the power
allocation problem is convex and can be solved analytically based on the KKT
conditions. The optimal phase durations can then be obtained numerically using
simple search methods. Results show that as the interuser link qualities
increase, the optimal scheme moves from no cooperation to partial then to full
cooperation, in which the users fully exchange their information and
cooperatively send it to the destination. Therefore, in practical systems with
strong inter-user links, simple decode-forward relaying at both users is
rate-optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1592</identifier>
 <datestamp>2012-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1592</id><created>2012-03-07</created><authors><author><keyname>Saucan</keyname><forenames>Emil</forenames></author></authors><title>Metric Ricci curvature for $PL$ manifolds</title><categories>math.DG cs.CG</categories><comments>20 pages</comments><msc-class>51K10, 53C21, 65D18</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a metric notion of Ricci curvature for $PL$ manifolds and study
its convergence properties. We also prove a fitting version of the Bonnet-Myers
Theorem, for surfaces as well as for a large class of higher dimensional
manifolds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1596</identifier>
 <datestamp>2012-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1596</id><created>2012-03-07</created><updated>2012-06-14</updated><authors><author><keyname>Kadri</keyname><forenames>Hachem</forenames><affiliation>INRIA Lille - Nord Europe</affiliation></author><author><keyname>Rakotomamonjy</keyname><forenames>Alain</forenames><affiliation>LITIS</affiliation></author><author><keyname>Bach</keyname><forenames>Francis</forenames><affiliation>INRIA Paris - Rocquencourt, LIENS</affiliation></author><author><keyname>Preux</keyname><forenames>Philippe</forenames><affiliation>INRIA Lille - Nord Europe</affiliation></author></authors><title>Multiple Operator-valued Kernel Learning</title><categories>stat.ML cs.LG</categories><comments>No. RR-7900 (2012)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Positive definite operator-valued kernels generalize the well-known notion of
reproducing kernels, and are naturally adapted to multi-output learning
situations. This paper addresses the problem of learning a finite linear
combination of infinite-dimensional operator-valued kernels which are suitable
for extending functional data analysis methods to nonlinear contexts. We study
this problem in the case of kernel ridge regression for functional responses
with an lr-norm constraint on the combination coefficients. The resulting
optimization problem is more involved than those of multiple scalar-valued
kernel learning since operator-valued kernels pose more technical and
theoretical issues. We propose a multiple operator-valued kernel learning
algorithm based on solving a system of linear operator equations by using a
block coordinatedescent procedure. We experimentally validate our approach on a
functional regression task in the context of finger movement prediction in
brain-computer interfaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1633</identifier>
 <datestamp>2012-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1633</id><created>2012-03-07</created><authors><author><keyname>Johnston</keyname><forenames>Nathaniel</forenames></author></authors><title>The Complexity of the Puzzles of Final Fantasy XIII-2</title><categories>cs.CC</categories><comments>16 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze the computational complexity of solving the three &quot;temporal rift&quot;
puzzles in the recent popular video game Final Fantasy XIII-2. We show that the
Tile Trial puzzle is NP-hard and we provide an efficient algorithm for solving
the Crystal Bonds puzzle. We also show that slight generalizations of the
Crystal Bonds and Hands of Time puzzles are NP-hard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1643</identifier>
 <datestamp>2012-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1643</id><created>2012-03-07</created><authors><author><keyname>Heidarzadeh</keyname><forenames>Anoosheh</forenames></author><author><keyname>Banihashemi</keyname><forenames>Amir H.</forenames></author></authors><title>Coding Delay Analysis of Chunked Codes over Line Networks</title><categories>cs.IT math.IT</categories><comments>15 pages; submitted to IEEE NetCod 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we analyze the coding delay and the average coding delay of
Chunked network Codes (CC) over line networks with Bernoulli losses and
deterministic regular or Poisson transmissions. Chunked codes are an attractive
alternative to random linear network codes due to their lower complexity. Our
results, which include upper bounds on the delay and the average delay, are the
first of their kind for CC over networks with such probabilistic traffics.
These results demonstrate that a stand-alone CC or a precoded CC provides a
better tradeoff between the computational complexity and the convergence speed
to the network capacity over the probabilistic traffics compared to arbitrary
deterministic traffics. The performance of CC over the latter traffics has
already been studied in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1647</identifier>
 <datestamp>2012-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1647</id><created>2012-03-07</created><authors><author><keyname>Yu</keyname><forenames>Sheng</forenames></author><author><keyname>Kak</keyname><forenames>Subhash</forenames></author></authors><title>A Survey of Prediction Using Social Media</title><categories>cs.SI physics.soc-ph</categories><comments>20 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social media comprises interactive applications and platforms for creating,
sharing and exchange of user-generated contents. The past ten years have
brought huge growth in social media, especially online social networking
services, and it is changing our ways to organize and communicate. It
aggregates opinions and feelings of diverse groups of people at low cost.
Mining the attributes and contents of social media gives us an opportunity to
discover social structure characteristics, analyze action patterns
qualitatively and quantitatively, and sometimes the ability to predict future
human related events. In this paper, we firstly discuss the realms which can be
predicted with current social media, then overview available predictors and
techniques of prediction, and finally discuss challenges and possible future
directions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1673</identifier>
 <datestamp>2012-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1673</id><created>2012-03-07</created><updated>2012-03-09</updated><authors><author><keyname>Wang</keyname><forenames>Qiyan</forenames></author><author><keyname>Gong</keyname><forenames>Xun</forenames></author><author><keyname>Nguyen</keyname><forenames>Giang T. K.</forenames></author><author><keyname>Houmansadr</keyname><forenames>Amir</forenames></author><author><keyname>Borisov</keyname><forenames>Nikita</forenames></author></authors><title>CensorSpoofer: Asymmetric Communication with IP Spoofing for
  Censorship-Resistant Web Browsing</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A key challenge in censorship-resistant web browsing is being able to direct
legitimate users to redirection proxies while preventing censors, posing as
insiders, from discovering their addresses and blocking them. We propose a new
framework for censorship-resistant web browsing called {\it CensorSpoofer} that
addresses this challenge by exploiting the asymmetric nature of web browsing
traffic and making use of IP spoofing. CensorSpoofer de-couples the upstream
and downstream channels, using a low-bandwidth indirect channel for delivering
outbound requests (URLs) and a high-bandwidth direct channel for downloading
web content. The upstream channel hides the request contents using
steganographic encoding within email or instant messages, whereas the
downstream channel uses IP address spoofing so that the real address of the
proxies is not revealed either to legitimate users or censors. We built a
proof-of-concept prototype that uses encrypted VoIP for this downstream channel
and demonstrated the feasibility of using the CensorSpoofer framework in a
realistic environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1681</identifier>
 <datestamp>2012-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1681</id><created>2012-03-07</created><updated>2012-08-30</updated><authors><author><keyname>Lychev</keyname><forenames>Robert</forenames></author><author><keyname>Goldberg</keyname><forenames>Sharon</forenames></author><author><keyname>Schapira</keyname><forenames>Michael</forenames></author></authors><title>Network-Destabilizing Attacks</title><categories>cs.NI cs.DC</categories><comments>14 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Border Gateway Protocol (BGP) sets up routes between the smaller networks
that make up the Internet. Despite its crucial role, BGP is notoriously
vulnerable to serious problems, including (1) propagation of bogus routing
information due to attacks or misconfigurations, and (2) network instabilities
in the form of persistent routing oscillations. The conditions required to
avoid BGP instabilities are quite delicate. How, then, can we explain the
observed stability of today's Internet in the face of common configuration
errors and attacks? This work explains this phenomenon by first noticing that
almost every observed attack and misconfiguration to date shares a common
characteristic: even when a router announces egregiously bogus information, it
will continue to announce the same bogus information for the duration of its
attack/misconfiguration. We call these the &quot;fixed-route attacks&quot;, and show
that, while even simple fixed-route attacks can destabilize a network, the
commercial routing policies used in today's Internet prevent such attacks from
creating instabilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1685</identifier>
 <datestamp>2012-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1685</id><created>2012-03-07</created><authors><author><keyname>Thant</keyname><forenames>Win Win</forenames></author><author><keyname>Htwe</keyname><forenames>Tin Myat</forenames></author><author><keyname>Thein</keyname><forenames>Ni Lar</forenames></author></authors><title>Statistical Function Tagging and Grammatical Relations of Myanmar
  Sentences</title><categories>cs.CL</categories><comments>16 pages, 7 figures, 8 tables, AIAA-2011 (India). arXiv admin note:
  text overlap with arXiv:0912.1820 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a context free grammar (CFG) based grammatical relations
for Myanmar sentences which combine corpus-based function tagging system. Part
of the challenge of statistical function tagging for Myanmar sentences comes
from the fact that Myanmar has free-phrase-order and a complex morphological
system. Function tagging is a pre-processing step to show grammatical relations
of Myanmar sentences. In the task of function tagging, which tags the function
of Myanmar sentences with correct segmentation, POS (part-of-speech) tagging
and chunking information, we use Naive Bayesian theory to disambiguate the
possible function tags of a word. We apply context free grammar (CFG) to find
out the grammatical relations of the function tags. We also create a functional
annotated tagged corpus for Myanmar and propose the grammar rules for Myanmar
sentences. Experiments show that our analysis achieves a good result with
simple sentences and complex sentences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1687</identifier>
 <datestamp>2012-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1687</id><created>2012-03-07</created><authors><author><keyname>Khouzani</keyname><forenames>M. H. R.</forenames></author><author><keyname>Sen</keyname><forenames>Soumya</forenames></author><author><keyname>Shroff</keyname><forenames>Ness B.</forenames></author></authors><title>An Analytical Approach to the Adoption of Asymmetric Bidirectional
  Firewalls: Need for Regulation?</title><categories>cs.SY math.OC</categories><comments>9 pages, 1 figure, technical report (detailed version) of a
  conference submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent incidents of cybersecurity violations have revealed the importance of
having firewalls and other intrusion detection systems to monitor traffic
entering and leaving access networks. But the adoption of such security
measures is often stymied by `free-riding' effects and `shortsightedness' among
Internet service providers (ISPs). In this work, we develop an analytical
framework that not only accounts for these issues but also incorporates
technological factors, like asymmetries in the performance of bidirectional
firewalls. Results on the equilibrium adoption and stability are presented,
along with detailed analysis on several policy issues related to social
welfare, price of anarchy, and price of shortsightedness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1692</identifier>
 <datestamp>2012-09-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1692</id><created>2012-03-08</created><updated>2012-09-04</updated><authors><author><keyname>Bock</keyname><forenames>Nicolas</forenames></author><author><keyname>Challacombe</keyname><forenames>Matt</forenames></author></authors><title>An Optimized Sparse Approximate Matrix Multiply for Matrices with Decay</title><categories>cs.NA cs.DS cs.MS</categories><report-no>LA-UR 11-06091</report-no><msc-class>65F15, 65-04, 65Z15, 15-04</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an optimized single-precision implementation of the Sparse
Approximate Matrix Multiply (\SpAMM{}) [M. Challacombe and N. Bock, arXiv {\bf
1011.3534} (2010)], a fast algorithm for matrix-matrix multiplication for
matrices with decay that achieves an $\mathcal{O} (n \log n)$ computational
complexity with respect to matrix dimension $n$. We find that the max norm of
the error achieved with a \SpAMM{} tolerance below $2 \times 10^{-8}$ is lower
than that of the single-precision {\tt SGEMM} for dense quantum chemical
matrices, while outperforming {\tt SGEMM} with a cross-over already for small
matrices ($n \sim 1000$). Relative to naive implementations of \SpAMM{} using
Intel's Math Kernel Library ({\tt MKL}) or AMD's Core Math Library ({\tt
ACML}), our optimized version is found to be significantly faster. Detailed
performance comparisons are made for quantum chemical matrices with differently
structured sub-blocks. Finally, we discuss the potential of improved hardware
prefetch to yield 2--3x speedups.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1711</identifier>
 <datestamp>2012-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1711</id><created>2012-03-08</created><authors><author><keyname>Wang</keyname><forenames>Yaming</forenames></author><author><keyname>Chen</keyname><forenames>Laming</forenames></author><author><keyname>Gu</keyname><forenames>Yuantao</forenames></author></authors><title>Quantization Reference Voltage of the Modulated Wideband Converter</title><categories>cs.IT math.IT</categories><comments>4 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Modulated Wideband Converter (MWC) is a recently proposed
analog-to-digital converter (ADC) based on Compressive Sensing (CS) theory.
Unlike conventional ADCs, its quantization reference voltage, which is
important to the system performance, does not equal the maximum amplitude of
original analog signal. In this paper, the quantization reference voltage of
the MWC is theoretically analyzed and the conclusion demonstrates that the
reference voltage is proportional to the square root of $q$, which is a
trade-off parameter between sampling rate and number of channels. Further
discussions and simulation results show that the reference voltage is
proportional to the square root of $Nq$ when the signal consists of $N$
narrowband signals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1714</identifier>
 <datestamp>2012-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1714</id><created>2012-03-08</created><authors><author><keyname>Liu</keyname><forenames>Jingbo</forenames></author><author><keyname>Jin</keyname><forenames>Jian</forenames></author><author><keyname>Gu</keyname><forenames>Yuantao</forenames></author></authors><title>Efficient Recovery of Block Sparse Signals via Zero-point Attracting
  Projection</title><categories>cs.IT math.IT</categories><comments>4 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider compressed sensing (CS) of block-sparse signals,
i.e., sparse signals that have nonzero coefficients occurring in clusters. An
efficient algorithm, called zero-point attracting projection (ZAP) algorithm,
is extended to the scenario of block CS. The block version of ZAP algorithm
employs an approximate $l_{2,0}$ norm as the cost function, and finds its
minimum in the solution space via iterations. For block sparse signals, an
analysis of the stability of the local minimums of this cost function under the
perturbation of noise reveals an advantage of the proposed algorithm over its
original non-block version in terms of reconstruction error. Finally, numerical
experiments show that the proposed algorithm outperforms other state of the art
methods for the block sparse problem in various respects, especially the
stability under noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1715</identifier>
 <datestamp>2012-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1715</id><created>2012-03-08</created><updated>2012-03-12</updated><authors><author><keyname>Hong</keyname><forenames>Dohy</forenames></author></authors><title>D-iteration: Evaluation of a Dynamic Partition Strategy</title><categories>cs.DC math.DS math.NA</categories><comments>9 pages</comments><acm-class>G.1.0; G.1.3; C.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this paper is to present a first evaluation of a dynamic partition
strategy associated to the recently proposed asynchronous distributed
computation scheme based on the D-iteration approach. The D-iteration is a
fluid diffusion point of view based iteration method to solve numerically
linear equations. Using a simple static partition strategy, it has been shown
that, when the computation is distributed over K virtual machines (PIDs), the
memory size to be handled by each virtual machine decreases linearly with K and
the computation speed increases almost linearly with K with a slope becoming
closer to one when the number N of linear equations to be solved increases.
Here, we want to evaluate how further those results can be improved when a
simple dynamic partition strategy is deployed and to show that the dynamic
partition strategy allows one to control and equalize the computation load
between PIDs without any deep analysis of the matrix or of the underlying graph
structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1717</identifier>
 <datestamp>2012-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1717</id><created>2012-03-08</created><authors><author><keyname>Jureta</keyname><forenames>Ivan</forenames></author></authors><title>Requirements Engineering Methods: A Classification Framework and
  Research Challenges</title><categories>cs.SE</categories><comments>10 pages, 1 figure</comments><acm-class>D.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Requirements Engineering Methods (REMs) support Requirements Engineering (RE)
tasks, from elicitation, through modeling and analysis, to validation and
evolution of requirements. Despite the growing interest to design, validate and
teach REMs, it remains unclear what components REMs should have. A
classification framework for REMs is proposed. It distinguishes REMs based on
the domain-independent properties of their components. The classification
framework is intended to facilitate (i) analysis, teaching and extension of
existing REMs, (ii) engineering and validation of new REMs, and (iii)
identifying research challenges in REM design. The framework should help
clarify further the relations between REM and other concepts of interest in and
to RE, including Requirements Problem and Solution, Requirements Modeling
Language, and Formal Method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1728</identifier>
 <datestamp>2012-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1728</id><created>2012-03-08</created><authors><author><keyname>Singh</keyname><forenames>N. Ajith</forenames></author><author><keyname>Hemalatha</keyname><forenames>M.</forenames></author></authors><title>High performance computing network for cloud environment using
  simulators</title><categories>cs.DC cs.NI</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud computing is the next generation computing. Adopting the cloud
computing is like signing up new form of a website. The GUI which controls the
cloud computing make is directly control the hardware resource and your
application. The difficulty part in cloud computing is to deploy in real
environment. Its' difficult to know the exact cost and it's requirement until
and unless we buy the service not only that whether it will support the
existing application which is available on traditional data center or had to
design a new application for the cloud computing environment. The security
issue, latency, fault tolerance are some parameter which we need to keen care
before deploying, all this we only know after deploying but by using simulation
we can do the experiment before deploying it to real environment. By simulation
we can understand the real environment of cloud computing and then after it
successful result we can start deploying your application in cloud computing
environment. By using the simulator it will save us lots of time and money.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1730</identifier>
 <datestamp>2014-05-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1730</id><created>2012-03-08</created><updated>2014-05-12</updated><authors><author><keyname>Le</keyname><forenames>Anh</forenames></author><author><keyname>Markopoulou</keyname><forenames>Athina</forenames></author><author><keyname>Dimakis</keyname><forenames>Alexandros G.</forenames></author></authors><title>Auditing for Distributed Storage Systems</title><categories>cs.CR cs.DC cs.NI</categories><comments>ToN 2014 Submission with Data Dynamics</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed storage codes have recently received a lot of attention in the
community. Independently, another body of work has proposed integrity checking
schemes for cloud storage, none of which, however, is customized for
coding-based storage or can efficiently support repair. In this work, we bridge
the gap between these two currently disconnected bodies of work. We propose
NC-Audit, a novel cryptography-based remote data integrity checking scheme,
designed specifically for network coding-based distributed storage systems.
NC-Audit combines, for the first time, the following desired properties: (i)
efficient checking of data integrity, (ii) efficient support for repairing
failed nodes, and (iii) protection against information leakage when checking is
performed by a third party. The key ingredient of the design of NC-Audit is a
novel combination of SpaceMac, a homomorphic message authentication code (MAC)
scheme for network coding, and NCrypt, a novel chosen-plaintext attack (CPA)
secure encryption scheme that is compatible with SpaceMac. Our evaluation of a
Java implementation of NC-Audit shows that an audit costs the storage node and
the auditor a modest amount computation time and lower bandwidth than prior
work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1740</identifier>
 <datestamp>2012-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1740</id><created>2012-03-08</created><updated>2012-04-28</updated><authors><author><keyname>Sun</keyname><forenames>Yajuan</forenames></author><author><keyname>Lin</keyname><forenames>Hai</forenames></author><author><keyname>Chen</keyname><forenames>Ben M.</forenames></author></authors><title>An Input-Output Simulation Approach to Controlling Multi-AffineSystems
  for Linear Temporal Logic Specifications</title><categories>cs.SY</categories><comments>personal reason</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an input-output simulation approach to controlling
multi-affine systems for linear temporal logic (LTL) specifications, which
consists of the following steps. First, we partition the state space into
rectangles, each of which satisfies atomic LTL propositions. Then, we study the
control of multi-affine systems on rectangles including the control of driving
all trajectories starting from a rectangle to exit through a facet and the
control of stabilizing the system towards a desired point. With the proposed
controllers, a finitely abstracted transition system is constructed which is
shown to be input-output simulated by the rectangular transition system of the
multi-affine system. Since input-output simulation preserves LTL properties,
the controller synthesis of the multi-affine system for LTL specifications is
achieved by designing a nonblocking supervisor for the abstracted transition
system and by continuously implementing the resulting supervisor for the
original multi-affine system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1743</identifier>
 <datestamp>2012-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1743</id><created>2012-03-08</created><authors><author><keyname>Retor&#xe9;</keyname><forenames>Christian</forenames><affiliation>LaBRI</affiliation></author></authors><title>Variable types for meaning assembly: a logical syntax for generic noun
  phrases introduced by most</title><categories>math.LO cs.CL cs.LO</categories><proxy>ccsd</proxy><journal-ref>Recherches linguistiques de Vincennes 41 (2012) 18</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a way to compute the meanings associated with sentences
with generic noun phrases corresponding to the generalized quantifier most. We
call these generics specimens and they resemble stereotypes or prototypes in
lexical semantics. The meanings are viewed as logical formulae that can
thereafter be interpreted in your favourite models. To do so, we depart
significantly from the dominant Fregean view with a single untyped universe.
Indeed, our proposal adopts type theory with some hints from Hilbert
\epsilon-calculus (Hilbert, 1922; Avigad and Zach, 2008) and from medieval
philosophy, see e.g. de Libera (1993, 1996). Our type theoretic analysis bears
some resemblance with ongoing work in lexical semantics (Asher 2011; Bassac et
al. 2010; Moot, Pr\'evot and Retor\'e 2011). Our model also applies to
classical examples involving a class, or a generic element of this class, which
is not uttered but provided by the context. An outcome of this study is that,
in the minimalism-contextualism debate, see Conrad (2011), if one adopts a type
theoretical view, terms encode the purely semantic meaning component while
their typing is pragmatically determined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1745</identifier>
 <datestamp>2012-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1745</id><created>2012-03-08</created><authors><author><keyname>Sun</keyname><forenames>Yajuan</forenames></author><author><keyname>Lin</keyname><forenames>Hai</forenames></author><author><keyname>Chen</keyname><forenames>Ben M.</forenames></author></authors><title>Bisimilarity Enforcing Supervisory Control for Deterministic
  Specifications</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the supervisory control of nondeterministic discrete
event systems to enforce bisimilarity with respect to deterministic
specifications. A notion of synchronous simulation-based controllability is
introduced as a necessary and sufficient condition for the existence of a
bisimilarity enforcing supervisor, and a polynomial algorithm is developed to
verify such a condition. When the existence condition holds, a supervisor
achieving bisimulation equivalence is constructed. Furthermore, when the
existence condition does not hold, two different methods are provided for
synthesizing maximal permissive sub-specifications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1749</identifier>
 <datestamp>2012-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1749</id><created>2012-03-08</created><authors><author><keyname>Aldabbas</keyname><forenames>Hamza</forenames></author><author><keyname>Alwada'n</keyname><forenames>Tariq</forenames></author><author><keyname>Janicke</keyname><forenames>Helge</forenames></author><author><keyname>Al-Bayatti</keyname><forenames>Ali</forenames></author></authors><title>Data Confidentiality in Mobile Ad hoc Networks</title><categories>cs.CR</categories><comments>12 pages</comments><journal-ref>International Journal of Wireless &amp; Mobile Networks (IJWMN) Vol.
  4, No. 1, February 2012</journal-ref><doi>10.5121/ijwmn.2012.4117</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile ad hoc networks (MANETs) are self-configuring infrastructure-less
networks comprised of mobile nodes that communicate over wireless links without
any central control on a peer-to-peer basis. These individual nodes act as
routers to forward both their own data and also their neighbours' data by
sending and receiving packets to and from other nodes in the network. The
relatively easy configuration and the quick deployment make ad hoc networks
suitable the emergency situations (such as human or natural disasters) and for
military units in enemy territory. Securing data dissemination between these
nodes in such networks, however, is a very challenging task. Exposing such
information to anyone else other than the intended nodes could cause a privacy
and confidentiality breach, particularly in military scenarios. In this paper
we present a novel framework to enhance the privacy and data confidentiality in
mobile ad hoc networks by attaching the originator policies to the messages as
they are sent between nodes. We evaluate our framework using the Network
Simulator (NS-2) to check whether the privacy and confidentiality of the
originator are met. For this we implemented the Policy Enforcement Points
(PEPs), as NS-2 agents that manage and enforce the policies attached to packets
at every node in the MANET.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1751</identifier>
 <datestamp>2012-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1751</id><created>2012-03-08</created><authors><author><keyname>cellatoglu</keyname><forenames>Akin</forenames></author><author><keyname>Karuppanan</keyname><forenames>Balasubramanian</forenames></author></authors><title>Remote Sensing and Control for Establishing and Maintaining Digital
  Irrigation</title><categories>cs.SY</categories><comments>15 pages 8 figures and two tables. CCSIT International conference Jan
  2012-Bangalore; International Journal of Advanced Information Technology
  (IJAIT) Vol. 2, No.1, February 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The remotely sensed data from an unknown location is transmitted in real time
through internet and gathered in a PC. The data is collected for a considerable
period of time and analyzed in PC as to assess the suitability and fertility of
the land for establishing an electronic plantation in that area. The analysis
also helps deciding the plantation of appropriate plants in the location
identified. The system performing this task with appropriate transducers
installed in remote area, the methodologies involved in transmission and data
gathering are reported.. The second part of the project deals with data
gathering from remote site and issuing control signals to remote appliances in
the site; all performed through internet. Therefore, this control scheme is a
duplex system monitoring the irrigation activities by collecting data in one
direction and issuing commands on the opposite direction. This scheme maintains
the digital irrigation systems effectively and efficiently as to utilize the
resources optimally for yielding enhanced production. The methodologies
involved in extending the two way communication of data are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1754</identifier>
 <datestamp>2012-09-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1754</id><created>2012-03-08</created><updated>2012-09-26</updated><authors><author><keyname>Cygan</keyname><forenames>Marek</forenames></author><author><keyname>Pilipczuk</keyname><forenames>Marcin</forenames></author><author><keyname>Pilipczuk</keyname><forenames>Micha&#x142;</forenames></author></authors><title>Known algorithms for EDGE CLIQUE COVER are probably optimal</title><categories>cs.DS cs.CC cs.DM</categories><comments>To appear in SODA 2013</comments><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the EDGE CLIQUE COVER (ECC) problem, given a graph G and an integer k, we
ask whether the edges of G can be covered with k complete subgraphs of G or,
equivalently, whether G admits an intersection model on k-element universe.
Gramm et al. [JEA 2008] have shown a set of simple rules that reduce the number
of vertices of G to 2^k, and no algorithm is known with significantly better
running time bound than a brute-force search on this reduced instance. In this
paper we show that the approach of Gramm et al. is essentially optimal: we
present a polynomial time algorithm that reduces an arbitrary 3-CNF-SAT formula
with n variables and m clauses to an equivalent ECC instance (G,k) with k =
O(log n) and |V(G)| = O(n + m). Consequently, there is no 2^{2^{o(k)}}poly(n)
time algorithm for the ECC problem, unless the Exponential Time Hypothesis
fails. To the best of our knowledge, these are the first results for a natural,
fixed-parameter tractable problem, and proving that a doubly-exponential
dependency on the parameter is essentially necessary.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1757</identifier>
 <datestamp>2012-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1757</id><created>2012-03-08</created><authors><author><keyname>Kafhali</keyname><forenames>Said El</forenames></author><author><keyname>Bouchti</keyname><forenames>Abdelali El</forenames></author><author><keyname>Hanini</keyname><forenames>Mohamed</forenames></author><author><keyname>Haqiq</keyname><forenames>Abdelkrim</forenames></author></authors><title>Performance Analysis for Bandwidth Allocation in IEEE 802.16 Broadband
  Wireless Networks using BMAP Queueing</title><categories>cs.NI cs.PF</categories><comments>16 pages</comments><journal-ref>International Journal of Wireless &amp; Mobile Networks (IJWMN) Vol.
  4, No. 1, February 2012</journal-ref><doi>10.5121/ijwmn.2012.4110</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a performance analysis for the bandwidth allocation in
IEEE 802.16 broadband wireless access (BWA) networks considering the
packet-level quality-of-service (QoS) constraints. Adaptive Modulation and
Coding (AMC) rate based on IEEE 802.16 standard is used to adjust the
transmission rate adaptively in each frame time according to channel quality in
order to obtain multiuser diversity gain. To model the arrival process and the
traffic source we use the Batch Markov Arrival Process (BMAP), which enables
more realistic and more accurate traffic modelling. We determine analytically
different performance parameters, such as average queue length, packet dropping
probability, queue throughput and average packet delay. Finally, the analytical
results are validated numerically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1758</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1758</id><created>2012-03-08</created><updated>2012-10-02</updated><authors><author><keyname>Park</keyname><forenames>Juho</forenames></author><author><keyname>Lee</keyname><forenames>Gilwon</forenames></author><author><keyname>Sung</keyname><forenames>Youngchul</forenames></author><author><keyname>Yukawa</keyname><forenames>Masahiro</forenames></author></authors><title>Coordinated Beamforming with Relaxed Zero Forcing: The Sequential
  Orthogonal Projection Combining Method and Rate Control</title><categories>cs.IT math.IT</categories><comments>Lemma 1 proof corrected; a new SOPC algorithm invented; K &gt; N case
  considered</comments><acm-class>E.4; H.1.1</acm-class><doi>10.1109/TSP.2013.2258343</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, coordinated beamforming based on relaxed zero forcing (RZF)
for K transmitter-receiver pair multiple-input single-output (MISO) and
multiple-input multiple-output (MIMO) interference channels is considered. In
the RZF coordinated beamforming, conventional zero-forcing interference leakage
constraints are relaxed so that some predetermined interference leakage to
undesired receivers is allowed in order to increase the beam design space for
larger rates than those of the zero-forcing (ZF) scheme or to make beam design
feasible when ZF is impossible. In the MISO case, it is shown that the
rate-maximizing beam vector under the RZF framework for a given set of
interference leakage levels can be obtained by sequential orthogonal projection
combining (SOPC). Based on this, exact and approximate closed-form solutions
are provided in two-user and three-user cases, respectively, and an efficient
beam design algorithm for RZF coordinated beamforming is provided in general
cases. Furthermore, the rate control problem under the RZF framework is
considered. A centralized approach and a distributed heuristic approach are
proposed to control the position of the designed rate-tuple in the achievable
rate region. Finally, the RZF framework is extended to MIMO interference
channels by deriving a new lower bound on the rate of each user.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1760</identifier>
 <datestamp>2012-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1760</id><created>2012-03-08</created><authors><author><keyname>Mateo</keyname><forenames>Jose Antonio</forenames></author><author><keyname>Valero</keyname><forenames>Valent\in</forenames></author><author><keyname>D\iaz</keyname><forenames>Gregorio</forenames></author></authors><title>BPEL-RF: A formal framework for BPEL orchestrations integrating
  distributed resources</title><categories>cs.SE</categories><comments>LNCS</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Web service compositions are gaining attention to develop complex web systems
by combination of existing services. Thus, there are many works that leverage
the advantages of this approach. However, there are only few works that use web
service compositions to manage distributed resources. In this paper, we then
present a formal model that combines orchestrations written in BPEL with
distributed resources, by using WSRF.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1765</identifier>
 <datestamp>2012-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1765</id><created>2012-03-08</created><authors><author><keyname>Kom</keyname><forenames>Guillaume</forenames></author><author><keyname>Tiedeu</keyname><forenames>Alain</forenames></author><author><keyname>Kom</keyname><forenames>Martin</forenames></author><author><keyname>Ngundam</keyname><forenames>John</forenames></author></authors><title>A comparative evaluation of two algorithms of detection of masses on
  mammograms</title><categories>cs.CV</categories><comments>9 pages, 5 figures, 1 table, Vol.3, No.1, February 2012,pp19-27;
  Signal &amp; Image Processing : An International Journal (SIPIJ),2012</comments><doi>10.5121/sipij.2012.3102</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we implement and carry out the comparison of two methods of
computer-aided-detection of masses on mammograms. The two algorithms basically
consist of 3 steps each: segmentation, binarization and noise suppression using
different techniques for each step. A database of 60 images was used to compare
the performance of the two algorithms in terms of general detection efficiency,
conservation of size and shape of detected masses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1777</identifier>
 <datestamp>2012-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1777</id><created>2012-03-08</created><authors><author><keyname>Bhattasali</keyname><forenames>Tapalina</forenames></author><author><keyname>Chaki</keyname><forenames>Rituparna</forenames></author></authors><title>AMC Model for Denial of Sleep Attack Detection</title><categories>cs.NI</categories><comments>4 pages,Journal Paper</comments><journal-ref>Journal of Recent Research Trends (JRRT), February 2012 edition,
  ISSN: 2250-3951[Online], 2250-3943[Print]</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to deployment in hostile environment, wireless sensor network is
vulnerable to various attacks. Exhausted sensor nodes in sensor network become
a challenging issue because it disrupts the normal connectivity of the network.
Affected nodes give rise to denial of service that resists to get the objective
of sensor network in real life. A mathematical model based on Absorbing Markov
Chain (AMC)is proposed for Denial of Sleep attack detection in sensor network.
In this mechanism, whether sensor network is affected by denial of sleep attack
or not can be decided by considering expected death time of sensor network
under normal scenario.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1778</identifier>
 <datestamp>2012-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1778</id><created>2012-03-08</created><authors><author><keyname>Haque</keyname><forenames>Md. Emdadul</forenames></author><author><keyname>Rashed</keyname><forenames>Md. Golam</forenames></author><author><keyname>Kabir</keyname><forenames>M. Hasnat</forenames></author></authors><title>A Comprehensive Study and Performance Comparison of M-ary Modulation
  Schemes for an Efficient Wireless Mobile Communication System</title><categories>cs.NI cs.PF</categories><comments>International Journal of Computer Science, Engineering and
  Applications (IJCSEA) Vol.1, No.3, June 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless communications has become one of the fastest growing areas in our
modern life and creates enormous impact on nearly every feature of our daily
life. In this paper, the performance of M-ary modulations schemes (MPSK, MQAM,
MFSK) based wireless communication system on audio signal transmission over
Additive Gaussian Noise (AWGN) channel are analyzed in terms of bit error
probability as a function of SNR. Based on the results obtained in the present
study, MPSK and MQAM are showing better performance for lower modulation order
whereas these are inferior with higher M. The BER value is smaller in MFSK for
higher M, but it is worse due to the distortion in the reproduce signal at the
receiver end. The lossless reproduction of recorded voice signal can be
achieved at the receiver end with a lower modulation order.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1792</identifier>
 <datestamp>2012-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1792</id><created>2012-03-08</created><authors><author><keyname>Wu</keyname><forenames>Xue</forenames></author></authors><title>Calculation of the minimum computational complexity based on information
  entropy</title><categories>cs.CC</categories><comments>10pages, 2 figures, journal</comments><journal-ref>International Journal on Computational Sciences &amp; Applications
  (IJCSA) Vo2, No.1, February 2012, pp. 73-82</journal-ref><doi>10.5121/ijcsa.2012.2107</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to find out the limiting speed of solving a specific problem using
computer, this essay provides a method based on information entropy. The
relationship between the minimum computational complexity and information
entropy change is illustrated. A few examples are served as evidence of such
connection. Meanwhile some basic rules of modeling problems are established.
Finally, the nature of solving problems with computer programs is disclosed to
support this theory and a redefinition of information entropy in this filed is
proposed. This will develop a new field of science.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1793</identifier>
 <datestamp>2012-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1793</id><created>2012-03-08</created><authors><author><keyname>Bouslimi</keyname><forenames>Riadh</forenames></author><author><keyname>Akaichi</keyname><forenames>Jalel</forenames></author></authors><title>Using Hausdorff Distance for New Medical Image Annotation</title><categories>cs.IR cs.CV</categories><comments>7 pages, 3 figures, 2 tables; International Journal of Database
  Management Systems (IJDMS) Vol.4, No.1, February 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Medical images annotation is most of the time a repetitive hard task.
Collecting old similar annotations and assigning them to new medical images may
not only enhance the annotation process, but also reduce ambiguity caused by
repetitive annotations. The goal of this work is to propose an approach based
on Hausdorff distance able to compute similarity between a new medical image
and old stored images. User has to choose then one of the similar images and
annotations related to the selected one are assigned to the new one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1799</identifier>
 <datestamp>2012-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1799</id><created>2012-03-08</created><authors><author><keyname>C.</keyname><forenames>Ramya Menon</forenames></author><author><keyname>Pangracious</keyname><forenames>Vinod</forenames></author></authors><title>Thermal analysis &amp; optimization of a 3 dimensional heterogeneous
  structure</title><categories>cs.ET</categories><comments>Computer Science &amp; Engineering: An International Journal (CSEIJ),
  Vol.2, No.1, February 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Besides the lot of advantages offered by the 3D stacking of devices in an
integrated circuit there is a chance of device damage due to rise in peak
temperature value. Hence, in order to make use of all the potential benefits of
the vertical stacking a thermal aware design is very essential. The first step
for designing a thermal aware architecture is to analyze the hotspot
temperature generated by the devices. In this paper we are presenting the
results of our thermal analysis experiments of a 3D heterogeneous structure
with three layers. The bottom layer had eight identical processors at 2.4 GHz
and the top layer was with four memory units. The intermediate layer was a
thermal interface material (TIM). The 2D thermal analysis of the top and bottom
layers was also done separately. In the next step simulations were carried out
by varying TIM thickness and conductivity to study its affect on hotspot
temperature so as to optimize the temperature distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1804</identifier>
 <datestamp>2012-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1804</id><created>2012-03-08</created><updated>2012-05-08</updated><authors><author><keyname>Malloy</keyname><forenames>Matthew L.</forenames></author><author><keyname>Nowak</keyname><forenames>Robert D.</forenames></author></authors><title>Near-Optimal Compressive Binary Search</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a simple modification to the recently proposed compressive binary
search. The modification removes an unnecessary and suboptimal factor of log
log n from the SNR requirement, making the procedure optimal (up to a small
constant). Simulations show that the new procedure performs significantly
better in practice as well. We also contrast this problem with the more well
known problem of noisy binary search.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1809</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1809</id><created>2012-03-08</created><updated>2013-01-04</updated><authors><author><keyname>Guo</keyname><forenames>Mingyu</forenames></author><author><keyname>Markakis</keyname><forenames>Evangelos</forenames></author><author><keyname>Apt</keyname><forenames>Krzysztof R.</forenames></author><author><keyname>Conitzer</keyname><forenames>Vincent</forenames></author></authors><title>Undominated Groves Mechanisms</title><categories>cs.GT</categories><comments>34 pages. To appear in Journal of AI Research (JAIR)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The family of Groves mechanisms, which includes the well-known VCG mechanism
(also known as the Clarke mechanism), is a family of efficient and
strategy-proof mechanisms. Unfortunately, the Groves mechanisms are generally
not budget balanced. That is, under such mechanisms, payments may flow into or
out of the system of the agents, resulting in deficits or reduced utilities for
the agents. We consider the following problem: within the family of Groves
mechanisms, we want to identify mechanisms that give the agents the highest
utilities, under the constraint that these mechanisms must never incur
deficits.
  We adopt a prior-free approach. We introduce two general measures for
comparing mechanisms in prior-free settings. We say that a non-deficit Groves
mechanism $M$ {\em individually dominates} another non-deficit Groves mechanism
$M'$ if for every type profile, every agent's utility under $M$ is no less than
that under $M'$, and this holds with strict inequality for at least one type
profile and one agent. We say that a non-deficit Groves mechanism $M$ {\em
collectively dominates} another non-deficit Groves mechanism $M'$ if for every
type profile, the agents' total utility under $M$ is no less than that under
$M'$, and this holds with strict inequality for at least one type profile. The
above definitions induce two partial orders on non-deficit Groves mechanisms.
We study the maximal elements corresponding to these two partial orders, which
we call the {\em individually undominated} mechanisms and the {\em collectively
undominated} mechanisms, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1820</identifier>
 <datestamp>2012-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1820</id><created>2012-01-25</created><authors><author><keyname>Simone</keyname><forenames>Antonino</forenames></author><author><keyname>Skoric</keyname><forenames>Boris</forenames></author><author><keyname>Zannone</keyname><forenames>Nicola</forenames></author></authors><title>Flow-based reputation: more than just ranking</title><categories>cs.CY cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The last years have seen a growing interest in collaborative systems like
electronic marketplaces and P2P file sharing systems where people are intended
to interact with other people. Those systems, however, are subject to security
and operational risks because of their open and distributed nature. Reputation
systems provide a mechanism to reduce such risks by building trust
relationships among entities and identifying malicious entities. A popular
reputation model is the so called flow-based model. Most existing reputation
systems based on such a model provide only a ranking, without absolute
reputation values; this makes it difficult to determine whether entities are
actually trustworthy or untrustworthy. In addition, those systems ignore a
significant part of the available information; as a consequence, reputation
values may not be accurate. In this paper, we present a flow-based reputation
metric that gives absolute values instead of merely a ranking. Our metric makes
use of all the available information. We study, both analytically and
numerically, the properties of the proposed metric and the effect of attacks on
reputation values.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1823</identifier>
 <datestamp>2012-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1823</id><created>2012-03-08</created><authors><author><keyname>Josephus</keyname><forenames>Chelsy Sapna</forenames></author><author><keyname>Remya</keyname><forenames>S.</forenames></author></authors><title>Enhancement Techniques for Local Content Preservation and Contrast
  Improvement in Images</title><categories>cs.CV cs.MM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are several images that do not have uniform brightness which pose a
challenging problem for image enhancement systems. As histogram equalization
has been successfully used to correct for uniform brightness problems, a
histogram equalization method that utilizes human visual system based
thresholding(human vision thresholding) as well as logarithmic processing
techniques were introduced later . But these methods are not good for
preserving the local content of the image which is a major factor for various
images like medical and aerial images. Therefore new method is proposed here.
This method is referred as &quot;Human vision thresholding with enhancement
technique for dark blurred images for local content preservation&quot;. It uses
human vision thresholding together with an existing enhancement method for dark
blurred images. Furthermore a comparative study with another method for local
content preservation is done which is further extended to make it suitable for
contrast improvement. Experimental results shows that the proposed methods
outperforms the former existing methods in preserving the local content for
standard images, medical and aerial images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1830</identifier>
 <datestamp>2012-03-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1830</id><created>2012-03-08</created><updated>2012-03-27</updated><authors><author><keyname>Singh</keyname><forenames>Niraj Kumar</forenames></author><author><keyname>Pal</keyname><forenames>Mita</forenames></author><author><keyname>Chakraborty</keyname><forenames>Soubhik</forenames></author></authors><title>Partition Sort Revisited: Reconfirming the Robustness in Average Case
  and much more!</title><categories>cs.DS</categories><comments>8 pages</comments><msc-class>68U20</msc-class><acm-class>B.2.4</acm-class><journal-ref>International Journal of Computer Science, Engineering and
  Applications,Vol.2, No.1, Feb 2012, p 23-30</journal-ref><doi>10.5121/ijcsea.2012.2103</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In our previous work there was some indication that Partition Sort could be
having a more robust average case O(nlogn) complexity than the popular Quick
Sort. In our first study in this paper, we reconfirm this through computer
experiments for inputs from Cauchy distribution for which expectation
theoretically does not exist. Additionally, the algorithm is found to be
sensitive to parameters of the input probability distribution demanding further
investigation on parameterized complexity. The results on this algorithm for
Binomial inputs in our second study are very encouraging in that direction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1833</identifier>
 <datestamp>2014-05-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1833</id><created>2012-03-08</created><authors><author><keyname>Bongard</keyname><forenames>Josh C.</forenames></author><author><keyname>Hines</keyname><forenames>Paul D. H.</forenames></author><author><keyname>Conger</keyname><forenames>Dylan</forenames></author><author><keyname>Hurd</keyname><forenames>Peter</forenames></author><author><keyname>Lu</keyname><forenames>Zhenyu</forenames></author></authors><title>Crowdsourcing Predictors of Behavioral Outcomes</title><categories>cs.CY cs.SI physics.soc-ph</categories><journal-ref>IEEE Transactions on Systems, Man, and Cybernetics: Systems, vol.
  43, no. 1, pp. 176 - 185, 2013</journal-ref><doi>10.1109/TSMCA.2012.2195168</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Generating models from large data sets -- and determining which subsets of
data to mine -- is becoming increasingly automated. However choosing what data
to collect in the first place requires human intuition or experience, usually
supplied by a domain expert. This paper describes a new approach to machine
science which demonstrates for the first time that non-domain experts can
collectively formulate features, and provide values for those features such
that they are predictive of some behavioral outcome of interest. This was
accomplished by building a web platform in which human groups interact to both
respond to questions likely to help predict a behavioral outcome and pose new
questions to their peers. This results in a dynamically-growing online survey,
but the result of this cooperative behavior also leads to models that can
predict user's outcomes based on their responses to the user-generated survey
questions. Here we describe two web-based experiments that instantiate this
approach: the first site led to models that can predict users' monthly electric
energy consumption; the other led to models that can predict users' body mass
index. As exponential increases in content are often observed in successful
online collaborative communities, the proposed methodology may, in the future,
lead to similar exponential rises in discovery and insight into the causal
factors of behavioral outcomes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1849</identifier>
 <datestamp>2013-05-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1849</id><created>2012-03-08</created><authors><author><keyname>Ghorpade</keyname><forenames>Sudhir R.</forenames></author><author><keyname>Ram</keyname><forenames>Samrith</forenames></author></authors><title>Enumeration of Splitting Subspaces over Finite Fields</title><categories>math.CO cs.IT math.IT</categories><comments>10 pages; to appear in the Proceedings of AGCT (Luminy, France, 2011)</comments><msc-class>Primary 15A03, 11T06 05E99 Secondary 11T71</msc-class><journal-ref>Arithmetic, Geometry, Cryptography and Coding Theory, 49-58,
  Contemp. Math. 574, Amer. Math. Soc., Providence, RI, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss an elementary, yet unsolved, problem of Niederreiter concerning
the enumeration of a class of subspaces of finite dimensional vector spaces
over finite fields. A short and self-contained account of some recent progress
on this problem is included and some related problems are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1850</identifier>
 <datestamp>2012-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1850</id><created>2012-03-08</created><authors><author><keyname>Gidon</keyname><forenames>Ohad</forenames></author><author><keyname>Be'ery</keyname><forenames>Yair</forenames></author></authors><title>On Pseudocodewords and Improved Union Bound of Linear Programming
  Decoding of HDPC Codes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present an improved union bound on the Linear Programming
(LP) decoding performance of the binary linear codes transmitted over an
additive white Gaussian noise channels. The bounding technique is based on the
second-order of Bonferroni-type inequality in probability theory, and it is
minimized by Prim's minimum spanning tree algorithm. The bound calculation
needs the fundamental cone generators of a given parity-check matrix rather
than only their weight spectrum, but involves relatively low computational
complexity. It is targeted to high-density parity-check codes, where the number
of their generators is extremely large and these generators are spread densely
in the Euclidean space. We explore the generator density and make a comparison
between different parity-check matrix representations. That density effects on
the improvement of the proposed bound over the conventional LP union bound. The
paper also presents a complete pseudo-weight distribution of the fundamental
cone generators for the BCH[31,21,5] code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1854</identifier>
 <datestamp>2012-08-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1854</id><created>2012-03-08</created><updated>2012-08-14</updated><authors><author><keyname>Even</keyname><forenames>Guy</forenames></author><author><keyname>Halabi</keyname><forenames>Nissim</forenames></author></authors><title>Local-Optimality Guarantees for Optimal Decoding Based on Paths</title><categories>cs.IT math.CO math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a unified analysis framework that captures recent
advances in the study of local-optimality characterizations for codes on
graphs. These local-optimality characterizations are based on combinatorial
structures embedded in the Tanner graph of the code. Local-optimality implies
both unique maximum-likelihood (ML) optimality and unique linear-programming
(LP) decoding optimality. Also, an iterative message-passing decoding algorithm
is guaranteed to find the unique locally-optimal codeword, if one exists.
  We demonstrate this proof technique by considering a definition of
local-optimality that is based on the simplest combinatorial structures in
Tanner graphs, namely, paths of length $h$. We apply the technique of
local-optimality to a family of Tanner codes. Inverse polynomial bounds in the
code length are proved on the word error probability of LP-decoding for this
family of Tanner codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1858</identifier>
 <datestamp>2012-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1858</id><created>2012-03-08</created><authors><author><keyname>Mohammad</keyname><forenames>Saif M.</forenames></author><author><keyname>Hirst</keyname><forenames>Graeme</forenames></author></authors><title>Distributional Measures of Semantic Distance: A Survey</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ability to mimic human notions of semantic distance has widespread
applications. Some measures rely only on raw text (distributional measures) and
some rely on knowledge sources such as WordNet. Although extensive studies have
been performed to compare WordNet-based measures with human judgment, the use
of distributional measures as proxies to estimate semantic distance has
received little attention. Even though they have traditionally performed poorly
when compared to WordNet-based measures, they lay claim to certain uniquely
attractive features, such as their applicability in resource-poor languages and
their ability to mimic both semantic similarity and semantic relatedness.
Therefore, this paper presents a detailed study of distributional measures.
Particular attention is paid to flesh out the strengths and limitations of both
WordNet-based and distributional measures, and how distributional measures of
distance can be brought more in line with human notions of semantic distance.
We conclude with a brief discussion of recent work on hybrid measures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1866</identifier>
 <datestamp>2014-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1866</id><created>2012-03-08</created><updated>2014-05-08</updated><authors><author><keyname>Roux</keyname><forenames>St&#xe9;phane Le</forenames></author></authors><title>From winning strategy to Nash equilibrium</title><categories>math.LO cs.FL cs.GT</categories><msc-class>91A05 (game theory)</msc-class><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Game theory is usually considered applied mathematics, but a few
game-theoretic results, such as Borel determinacy, were developed by
mathematicians for mathematics in a broad sense. These results usually state
determinacy, i.e. the existence of a winning strategy in games that involve two
players and two outcomes saying who wins. In a multi-outcome setting, the
notion of winning strategy is irrelevant yet usually replaced faithfully with
the notion of (pure) Nash equilibrium. This article shows that every
determinacy result over an arbitrary game structure, e.g. a tree, is
transferable into existence of multi-outcome (pure) Nash equilibrium over the
same game structure. The equilibrium-transfer theorem requires cardinal or
order-theoretic conditions on the strategy sets and the preferences,
respectively, whereas counter-examples show that every requirement is relevant,
albeit possibly improvable. When the outcomes are finitely many, the proof
provides an algorithm computing a Nash equilibrium without significant
complexity loss compared to the two-outcome case. As examples of application,
this article generalises Borel determinacy, positional determinacy of parity
games, and finite-memory determinacy of Muller games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1868</identifier>
 <datestamp>2012-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1868</id><created>2012-03-08</created><authors><author><keyname>Gonz&#xe1;lez-Bail&#xf3;n</keyname><forenames>Sandra</forenames></author><author><keyname>Borge-Holthoefer</keyname><forenames>Javier</forenames></author><author><keyname>Moreno</keyname><forenames>Yamir</forenames></author></authors><title>Broadcasters and Hidden Influentials in Online Protest Diffusion</title><categories>physics.soc-ph cs.SI</categories><comments>32 pages, 5 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper explores the growth of online mobilizations using data from the
'indignados' (the 'outraged') movement in Spain, which emerged under the
influence of the revolution in Egypt and as a precursor to the global Occupy
mobilizations. The data tracks Twitter activity around the protests that took
place in May 2011, which led to the formation of camp sites in dozens of cities
all over the country and massive daily demonstrations during the week prior to
the elections of May 22. We reconstruct the network of tens of thousands of
users, and monitor their message activity for a month (25 April 2011 to 25 May
2011). Using both the structure of the network and levels of activity in
message exchange, we identify four types of users and we analyze their role in
the growth of the protest. Drawing from theories of online collective action
and research on information diffusion in networks the paper centers on the
following questions: How does protest information spread in online networks?
How do different actors contribute to that diffusion? How do mainstream media
interact with new media? Do they help amplify protest messages? And what is the
role of less popular but far more frequent users in the growth of online
mobilizations? This paper aims to inform the theoretical debate on whether
digital technologies are changing the logic of collective action, and provide
evidence of how new media facilitates the coordination of offline
mobilizations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1869</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1869</id><created>2012-03-08</created><authors><author><keyname>Li</keyname><forenames>Min</forenames></author><author><keyname>Simeone</keyname><forenames>Osvaldo</forenames></author><author><keyname>Yener</keyname><forenames>Aylin</forenames></author></authors><title>Degraded Broadcast Diamond Channels with Non-Causal State Information at
  the Source</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory, Feb. 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A state-dependent degraded broadcast diamond channel is studied where the
source-to-relays cut is modeled with two noiseless, finite-capacity digital
links with a degraded broadcasting structure, while the relays-to-destination
cut is a general multiple access channel controlled by a random state. It is
assumed that the source has non-causal channel state information and the relays
have no state information. Under this model, first, the capacity is
characterized for the case where the destination has state information, i.e.,
has access to the state sequence. It is demonstrated that in this case, a joint
message and state transmission scheme via binning is optimal. Next, the case
where the destination does not have state information, i.e., the case with
state information at the source only, is considered. For this scenario, lower
and upper bounds on the capacity are derived for the general discrete
memoryless model. Achievable rates are then computed for the case in which the
relays-to-destination cut is affected by an additive Gaussian state. Numerical
results are provided that illuminate the performance advantages that can be
accrued by leveraging non-causal state information at the source.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1876</identifier>
 <datestamp>2012-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1876</id><created>2012-03-08</created><updated>2012-12-02</updated><authors><author><keyname>Bodirsky</keyname><forenames>Manuel</forenames></author><author><keyname>Pinsker</keyname><forenames>Michael</forenames></author></authors><title>Topological Birkhoff</title><categories>math.LO cs.CC cs.LO math.RA</categories><comments>21 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the most fundamental mathematical contributions of Garrett Birkhoff is
the HSP theorem, which implies that a finite algebra B satisfies all equations
that hold in a finite algebra A of the same signature if and only if B is a
homomorphic image of a subalgebra of a finite power of A. On the other hand, if
A is infinite, then in general one needs to take an infinite power in order to
obtain a representation of B in terms of A, even if B is finite.
  We show that by considering the natural topology on the functions of A and B
in addition to the equations that hold between them, one can do with finite
powers even for many interesting infinite algebras A. More precisely, we prove
that if A and B are at most countable algebras which are oligomorphic, then the
mapping which sends each function from A to the corresponding function in B
preserves equations and is continuous if and only if B is a homomorphic image
of a subalgebra of a finite power of A.
  Our result has the following consequences in model theory and in theoretical
computer science: two \omega-categorical structures are primitive positive
bi-interpretable if and only if their topological polymorphism clones are
isomorphic. In particular, the complexity of the constraint satisfaction
problem of an \omega-categorical structure only depends on its topological
polymorphism clone.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1878</identifier>
 <datestamp>2012-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1878</id><created>2012-03-08</created><authors><author><keyname>Goswami</keyname><forenames>Saptarsi</forenames></author><author><keyname>Ghosh</keyname><forenames>Samiran</forenames></author><author><keyname>Chakrabarti</keyname><forenames>Amlan</forenames></author></authors><title>Outlier detection from ETL Execution trace</title><categories>cs.DB</categories><comments>2011 3rd International Conference on Electronics Computer Technology
  (ICECT 2011)</comments><doi>10.1109/ICECTECH.2011.5942112</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Extract, Transform, Load (ETL) is an integral part of Data Warehousing (DW)
implementation. The commercial tools that are used for this purpose captures
lot of execution trace in form of various log files with plethora of
information. However there has been hardly any initiative where any proactive
analyses have been done on the ETL logs to improve their efficiency. In this
paper we utilize outlier detection technique to find the processes varying most
from the group in terms of execution trace. As our experiment was carried on
actual production processes, any outlier we would consider as a signal rather
than a noise. To identify the input parameters for the outlier detection
algorithm we employ a survey among developer community with varied mix of
experience and expertise. We use simple text parsing to extract these features
from the logs, as shortlisted from the survey. Subsequently we applied outlier
detection technique (Clustering based) on the logs. By this process we reduced
our domain of detailed analysis from 500 logs to 44 logs (8 Percentage). Among
the 5 outlier cluster, 2 of them are genuine concern, while the other 3 figure
out because of the huge number of rows involved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1882</identifier>
 <datestamp>2012-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1882</id><created>2012-03-08</created><authors><author><keyname>Meenakshi</keyname><forenames>G.</forenames></author></authors><title>Multi source feedback based performance appraisal system using Fuzzy
  logic decision support system</title><categories>cs.AI</categories><comments>16 pages</comments><doi>10.5121/ijsc.2012.3108</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In Multi-Source Feedback or 360 Degree Feedback, data on the performance of
an individual are collected systematically from a number of stakeholders and
are used for improving performance. The 360-Degree Feedback approach provides a
consistent management philosophy meeting the criterion outlined previously. The
360-degree feedback appraisal process describes a human resource methodology
that is frequently used for both employee appraisal and employee development.
Used in employee performance appraisals, the 360-degree feedback methodology is
differentiated from traditional, top-down appraisal methods in which the
supervisor responsible for the appraisal provides the majority of the data.
Instead it seeks to use information gained from other sources to provide a
fuller picture of employees' performances. Similarly, when this technique used
in employee development it augments employees' perceptions of training needs
with those of the people with whom they interact. The 360-degree feedback based
appraisal is a comprehensive method where in the feedback about the employee
comes from all the sources that come into contact with the employee on his/her
job. The respondents for an employee can be her/his peers, managers,
subordinates team members, customers, suppliers and vendors. Hence anyone who
comes into contact with the employee, the 360 degree appraisal has four
components that include self-appraisal, superior's appraisal, subordinate's
appraisal student's appraisal and peer's appraisal .The proposed system is an
attempt to implement the 360 degree feedback based appraisal system in
academics especially engineering colleges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1888</identifier>
 <datestamp>2012-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1888</id><created>2012-03-08</created><authors><author><keyname>Vaidya</keyname><forenames>Nitin</forenames></author></authors><title>Matrix Representation of Iterative Approximate Byzantine Consensus in
  Directed Graphs</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a proof of correctness of an iterative approximate
Byzantine consensus (IABC) algorithm for directed graphs. The iterative
algorithm allows fault- free nodes to reach approximate conensus despite the
presence of up to f Byzantine faults. Necessary conditions on the underlying
network graph for the existence of a correct IABC algorithm were shown in our
recent work [15, 16]. [15] also analyzed a specific IABC algorithm and showed
that it performs correctly in any network graph that satis?es the necessary
condition, proving that the necessary condition is also sufficient. In this
paper, we present an alternate proof of correctness of the IABC algorithm,
using a familiar technique based on transition matrices [9, 3, 17, 19].
  The key contribution of this paper is to exploit the following observation:
for a given evolution of the state vector corresponding to the state of the
fault-free nodes, many alternate state transition matrices may be chosen to
model that evolution cor- rectly. For a given state evolution, we identify one
approach to suitably &quot;design&quot; the transition matrices so that the standard
tools for proving convergence can be applied to the Byzantine fault-tolerant
algorithm as well. In particular, the transition matrix for each iteration is
designed such that each row of the matrix contains a large enough number of
elements that are bounded away from 0.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1889</identifier>
 <datestamp>2012-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1889</id><created>2012-03-08</created><authors><author><keyname>Mohammad</keyname><forenames>Saif M</forenames></author><author><keyname>Hirst</keyname><forenames>Graeme</forenames></author></authors><title>Distributional Measures as Proxies for Semantic Relatedness</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The automatic ranking of word pairs as per their semantic relatedness and
ability to mimic human notions of semantic relatedness has widespread
applications. Measures that rely on raw data (distributional measures) and
those that use knowledge-rich ontologies both exist. Although extensive studies
have been performed to compare ontological measures with human judgment, the
distributional measures have primarily been evaluated by indirect means. This
paper is a detailed study of some of the major distributional measures; it
lists their respective merits and limitations. New measures that overcome these
drawbacks, that are more in line with the human notions of semantic
relatedness, are suggested. The paper concludes with an exhaustive comparison
of the distributional and ontology-based measures. Along the way, significant
research problems are identified. Work on these problems may lead to a better
understanding of how semantic relatedness is to be measured.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1891</identifier>
 <datestamp>2012-12-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1891</id><created>2012-03-08</created><updated>2012-12-05</updated><authors><author><keyname>van de Ven</keyname><forenames>Peter M.</forenames></author><author><keyname>Hegde</keyname><forenames>Nidhi</forenames></author><author><keyname>Massoulie</keyname><forenames>Laurent</forenames></author><author><keyname>Salonidis</keyname><forenames>Theodoros</forenames></author></authors><title>Optimal control of end-user energy storage</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An increasing number of retail energy markets show price fluctuations,
providing users with the opportunity to buy energy at lower than average
prices. We propose to temporarily store this inexpensive energy in a battery,
and use it to satisfy demand when energy prices are high, thus allowing users
to exploit the price variations without having to shift their demand to the
low-price periods. We study the battery control policy that yields the best
performance, i.e., minimizes the total discounted costs. The optimal policy is
shown to have a threshold structure, and we derive these thresholds in a few
special cases. The cost savings obtained from energy storage are demonstrated
through extensive numerical experiments, and we offer various directions for
future research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1892</identifier>
 <datestamp>2012-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1892</id><created>2012-03-08</created><updated>2012-03-14</updated><authors><author><keyname>Nabaee</keyname><forenames>Mahdy</forenames></author><author><keyname>Labeau</keyname><forenames>Fabrice</forenames></author></authors><title>Restricted Isometry Property in Quantized Network Coding of Sparse
  Messages</title><categories>cs.IT math.IT</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study joint network coding and distributed source coding of
inter-node dependent messages, with the perspective of compressed sensing.
Specifically, the theoretical guarantees for robust $\ell_1$-min recovery of an
under-determined set of linear network coded sparse messages are investigated.
We discuss the guarantees for $\ell_1$-min decoding of quantized network coded
messages, using the proposed local network coding coefficients in \cite{naba},
based on Restricted Isometry Property (RIP) of the resulting measurement
matrix. Moreover, the relation between tail probability of $\ell_2$-norms and
satisfaction of RIP is derived and used to compare our designed measurement
matrix, with i.i.d. Gaussian measurement matrix. Finally, we present our
numerical evaluations, which shows that the proposed design of network coding
coefficients result in a measurement matrix with an RIP behavior, similar to
that of i.i.d. Gaussian matrix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1895</identifier>
 <datestamp>2015-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1895</id><created>2012-03-08</created><updated>2015-02-08</updated><authors><author><keyname>Aloupis</keyname><forenames>Greg</forenames></author><author><keyname>Demaine</keyname><forenames>Erik D.</forenames></author><author><keyname>Guo</keyname><forenames>Alan</forenames></author><author><keyname>Viglietta</keyname><forenames>Giovanni</forenames></author></authors><title>Classic Nintendo Games are (Computationally) Hard</title><categories>cs.CC cs.GT</categories><comments>36 pages, 36 figures. Fixed some typos. Added NP-hardness results
  (with proofs and figures) for American SMB2 and Zelda 2</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove NP-hardness results for five of Nintendo's largest video game
franchises: Mario, Donkey Kong, Legend of Zelda, Metroid, and Pokemon. Our
results apply to generalized versions of Super Mario Bros. 1-3, The Lost
Levels, and Super Mario World; Donkey Kong Country 1-3; all Legend of Zelda
games; all Metroid games; and all Pokemon role-playing games. In addition, we
prove PSPACE-completeness of the Donkey Kong Country games and several Legend
of Zelda games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1897</identifier>
 <datestamp>2012-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1897</id><created>2012-03-08</created><authors><author><keyname>Alshalabi</keyname><forenames>Ibrahim Alkore</forenames></author><author><keyname>Elleithy</keyname><forenames>Khaled</forenames></author></authors><title>Effective M-learning design Strategies for computer science and
  Engineering courses</title><categories>cs.HC cs.CY</categories><journal-ref>International Journal of Mobile Network Communications &amp;
  Telematics (IJMNCT) Vol.2, No.1, February 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile learning (M-learning) is receiving more attention as a method of
delivering to learners study materials anytime and anywhere. It is a necessity
for educators to come up with a layout for learning that can be accessed
through mobile devices. These learning materials should consist of good quality
learning theories and accurate instructional layout in order to maintain the
learning as effective as possible. It is important to follow certain strategies
that can help the developers for M-learning applications. In this paper we
proposed a set of strategies that are useful for creating mobile prototype for
Computer Science and Engineering courses or M-learning application for course
content.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1900</identifier>
 <datestamp>2012-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1900</id><created>2012-03-08</created><authors><author><keyname>Arendt</keyname><forenames>Hannah</forenames></author><author><keyname>Jost</keyname><forenames>Jorgensen</forenames></author></authors><title>Consensus on Moving Neighborhood Model of Peterson Graph</title><categories>cs.DC math-ph math.MP</categories><comments>Submitted to European Physical Journal B</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the consensus problem of multiple agents on a kind of
famous graph, Peterson graph. It is an undirected graph with 10 vertices and 15
edges. Each agent randomly walks on this graph and communicates with each other
if and only if they coincide on a node at the same time. We conduct numerical
study on the consensus problem in this framework and show that global consensus
can be achieved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1905</identifier>
 <datestamp>2013-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1905</id><created>2012-03-08</created><authors><author><keyname>Vieira</keyname><forenames>Fabio R. J.</forenames></author><author><keyname>de Rezende</keyname><forenames>Jos&#xe9; F.</forenames></author><author><keyname>Barbosa</keyname><forenames>Valmir C.</forenames></author><author><keyname>Fdida</keyname><forenames>Serge</forenames></author></authors><title>Local heuristic for the refinement of multi-path routing in wireless
  mesh networks</title><categories>cs.NI</categories><journal-ref>Computer Networks 57 (2013), 273-285</journal-ref><doi>10.1016/j.comnet.2012.09.009</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider wireless mesh networks and the problem of routing end-to-end
traffic over multiple paths for the same origin-destination pair with minimal
interference. We introduce a heuristic for path determination with two
distinguishing characteristics. First, it works by refining an extant set of
paths, determined previously by a single- or multi-path routing algorithm.
Second, it is totally local, in the sense that it can be run by each of the
origins on information that is available no farther than the node's immediate
neighborhood. We have conducted extensive computational experiments with the
new heuristic, using AODV and OLSR, as well as their multi-path variants, as
underlying routing methods. For two different CSMA settings (as implemented by
802.11) and one TDMA setting running a path-oriented link scheduling algorithm,
we have demonstrated that the new heuristic is capable of improving the average
throughput network-wide. When working from the paths generated by the
multi-path routing algorithms, the heuristic is also capable to provide a more
evenly distributed traffic pattern.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1940</identifier>
 <datestamp>2013-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1940</id><created>2012-03-08</created><updated>2013-11-12</updated><authors><author><keyname>Chalermsook</keyname><forenames>Parinya</forenames></author><author><keyname>Kintali</keyname><forenames>Shiva</forenames></author><author><keyname>Lipton</keyname><forenames>Richard</forenames></author><author><keyname>Nanongkai</keyname><forenames>Danupon</forenames></author></authors><title>Graph Pricing Problem on Bounded Treewidth, Bounded Genus and k-partite
  graphs</title><categories>cs.GT cs.DS</categories><comments>Preprint of the paper to appear in Chicago Journal of Theoretical
  Computer Science</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider the following problem. A seller has infinite copies of $n$ products
represented by nodes in a graph. There are $m$ consumers, each has a budget and
wants to buy two products. Consumers are represented by weighted edges. Given
the prices of products, each consumer will buy both products she wants, at the
given price, if she can afford to. Our objective is to help the seller price
the products to maximize her profit.
  This problem is called {\em graph vertex pricing} ({\sf GVP}) problem and has
resisted several recent attempts despite its current simple solution. This
motivates the study of this problem on special classes of graphs. In this
paper, we study this problem on a large class of graphs such as graphs with
bounded treewidth, bounded genus and $k$-partite graphs.
  We show that there exists an {\sf FPTAS} for {\sf GVP} on graphs with bounded
treewidth. This result is also extended to an {\sf FPTAS} for the more general
{\em single-minded pricing} problem. On bounded genus graphs we present a {\sf
PTAS} and show that {\sf GVP} is {\sf NP}-hard even on planar graphs.
  We study the Sherali-Adams hierarchy applied to a natural Integer Program
formulation that $(1+\epsilon)$-approximates the optimal solution of {\sf GVP}.
Sherali-Adams hierarchy has gained much interest recently as a possible
approach to develop new approximation algorithms. We show that, when the input
graph has bounded treewidth or bounded genus, applying a constant number of
rounds of Sherali-Adams hierarchy makes the integrality gap of this natural
{\sf LP} arbitrarily small, thus giving a $(1+\epsilon)$-approximate solution
to the original {\sf GVP} instance.
  On $k$-partite graphs, we present a constant-factor approximation algorithm.
We further improve the approximation factors for paths, cycles and graphs with
degree at most three.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1952</identifier>
 <datestamp>2012-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1952</id><created>2012-03-08</created><authors><author><keyname>Ngo</keyname><forenames>Hung Q.</forenames></author><author><keyname>Porat</keyname><forenames>Ely</forenames></author><author><keyname>R&#xe9;</keyname><forenames>Christopher</forenames></author><author><keyname>Rudra</keyname><forenames>Atri</forenames></author></authors><title>Worst-case Optimal Join Algorithms</title><categories>cs.DB cs.DS math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Efficient join processing is one of the most fundamental and well-studied
tasks in database research. In this work, we examine algorithms for natural
join queries over many relations and describe a novel algorithm to process
these queries optimally in terms of worst-case data complexity. Our result
builds on recent work by Atserias, Grohe, and Marx, who gave bounds on the size
of a full conjunctive query in terms of the sizes of the individual relations
in the body of the query. These bounds, however, are not constructive: they
rely on Shearer's entropy inequality which is information-theoretic. Thus, the
previous results leave open the question of whether there exist algorithms
whose running time achieve these optimal bounds. An answer to this question may
be interesting to database practice, as it is known that any algorithm based on
the traditional select-project-join style plans typically employed in an RDBMS
are asymptotically slower than the optimal for some queries. We construct an
algorithm whose running time is worst-case optimal for all natural join
queries. Our result may be of independent interest, as our algorithm also
yields a constructive proof of the general fractional cover bound by Atserias,
Grohe, and Marx without using Shearer's inequality. This bound implies two
famous inequalities in geometry: the Loomis-Whitney inequality and the
Bollob\'as-Thomason inequality. Hence, our results algorithmically prove these
inequalities as well. Finally, we discuss how our algorithm can be used to
compute a relaxed notion of joins.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1964</identifier>
 <datestamp>2012-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1964</id><created>2012-03-08</created><authors><author><keyname>Maitem</keyname><forenames>Jean</forenames></author><author><keyname>Cabauatan</keyname><forenames>Rosmina Joy</forenames></author><author><keyname>Rabago</keyname><forenames>Lorena</forenames></author><author><keyname>Tanguilig</keyname><forenames>Bartolome</forenames><suffix>III</suffix></author></authors><title>Math world: A game-based 3D Virtual Learning Environment (3D VLE) for
  second graders</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper intends to introduce a game-based 3D Virtual Learning Environment
(VLE) to second graders. The impetus arose from the need to make learning in
mathematics more effective and interesting through multimedia. Applied in a
game, the basic mathematical operations such as addition, subtraction,
multiplication, and division are expected to performed by learners as they
represent themselves as avatars while they immerse in a quest of digital
objects in the VLE called Math World. Educational attributes such as mentality
change, emotional fulfillment, knowledge enhancement, thinking skills
development, and bodily coordination are evaluated to ensure learning
effectiveness. Also, game playability measured in terms of game plays, story,
mechanics and interface usability are examined for its educative design. With
an aggregate of these enhanced indices, results attest that objectives were met
while making mathematics an interesting, motivating and enjoyable subject,
hence VLE a significant tool to complement the conventional approaches of
teaching.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1971</identifier>
 <datestamp>2012-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1971</id><created>2012-03-08</created><authors><author><keyname>Bhat</keyname><forenames>Naagesh S.</forenames></author></authors><title>Design and modelling of different SRAM's based on CNTFET 32nm technology</title><categories>cs.ET</categories><comments>15 Pages</comments><journal-ref>International Journal of VLSI design &amp; Communication Systems
  (VLSICS) Vol.3, No.1, February 2012</journal-ref><doi>10.5121/vlsic.2012.3106</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Carbon nanotube field-effect transistor (CNTFET) refers to a field-effect
transistor that utilizes a single carbon nanotube or an array of carbon
nanotubes as the channel material instead of bulk silicon in the traditional
MOSFET structure. Since it was first demonstrated in 1998, there have been
tremendous developments in CNTFETs, which promise for an alternative material
to replace silicon in future electronics. Carbon nanotubes are promising
materials for the nano-scale electron devices such as nanotube FETs for
ultra-high density integrated circuits and quantum-effect devices for novel
intelligent circuits, which are expected to bring a breakthrough in the present
silicon technology.
  A Static Random Access Memory (SRAM) is designed to plug two needs: i) The
SRAM provides as cache memory, communicating between central processing unit
and Dynamic Random Access Memory (DRAM). ii) The SRAM technology act as driving
force for low power application since SRAM is portable compared to DRAM, and
SRAM doesn't require any refresh current. On the basis of acquired knowledge,
we present different SRAM's designed for the conventional CNTFET. HSPICE
simulations of this circuit using Stanford CNTFET model shows a great
improvement in power saving.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1979</identifier>
 <datestamp>2012-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1979</id><created>2012-03-08</created><updated>2012-05-16</updated><authors><author><keyname>Ford</keyname><forenames>Bryan</forenames></author></authors><title>Icebergs in the Clouds: the Other Risks of Cloud Computing</title><categories>cs.CY cs.CR</categories><comments>6 pages, 3 figures</comments><journal-ref>4th USENIX Workshop on Hot Topics in Cloud Computing (HotCloud
  '12), June 12-13, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud computing is appealing from management and efficiency perspectives, but
brings risks both known and unknown. Well-known and hotly-debated information
security risks, due to software vulnerabilities, insider attacks, and
side-channels for example, may be only the &quot;tip of the iceberg.&quot; As diverse,
independently developed cloud services share ever more fluidly and aggressively
multiplexed hardware resource pools, unpredictable interactions between
load-balancing and other reactive mechanisms could lead to dynamic
instabilities or &quot;meltdowns.&quot; Non-transparent layering structures, where
alternative cloud services may appear independent but share deep, hidden
resource dependencies, may create unexpected and potentially catastrophic
failure correlations, reminiscent of financial industry crashes. Finally, cloud
computing exacerbates already-difficult digital preservation challenges,
because only the provider of a cloud-based application or service can archive a
&quot;live,&quot; functional copy of a cloud artifact and its data for long-term cultural
preservation. This paper explores these largely unrecognized risks, making the
case that we should study them before our socioeconomic fabric becomes
inextricably dependent on a convenient but potentially unstable computing
model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1981</identifier>
 <datestamp>2012-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1981</id><created>2012-03-08</created><authors><author><keyname>Das</keyname><forenames>Sanjoy</forenames></author><author><keyname>Lobiyal</keyname><forenames>D . K.</forenames></author></authors><title>Analysis of neighbour and isolated node of intersection area based
  geocasting protocol (IBGP) in VANET</title><categories>cs.OH</categories><journal-ref>International Journal of Wireless &amp; Mobile Networks (IJWMN) Vol.
  4, No. 1, February 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Geocasting is a special variant of multicasting, where data packet or message
is transmitted to a predefined geographical location i.e., known as geocast
region. The applications of geocasting in VANET are to disseminate information
like, collision warning, advertising, alerts message, etc. In this paper, we
have proposed a model for highway scenario where the highway is divided into
number of cells. The intersection area between two successive cells is computed
to find the number of common nodes. Therefore, probabilistic analysis of the
nodes present and void occurrence in the intersection area is carried out.
Further, we have defined different forwarding zones to restrict the number of
participated nodes for data delivery. Number of nodes present and void
occurrence in the different forwarding zones have also been analysed based on
various node density in the network to determine the successful delivery of
data. Our analytical results show that in a densely populated network, data can
be transmitted with low radio transmission range. In a densely populated
network smaller forwarding zones will be selected for data delivery.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1985</identifier>
 <datestamp>2012-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1985</id><created>2012-03-08</created><authors><author><keyname>Wang</keyname><forenames>Zhaowen</forenames></author><author><keyname>Wang</keyname><forenames>Jinjun</forenames></author><author><keyname>Xiao</keyname><forenames>Jing</forenames></author><author><keyname>Lin</keyname><forenames>Kai-Hsiang</forenames></author><author><keyname>Huang</keyname><forenames>Thomas</forenames></author></authors><title>Substructure and Boundary Modeling for Continuous Action Recognition</title><categories>cs.CV</categories><comments>Detailed version of the CVPR 2012 paper. 15 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a probabilistic graphical model for continuous action
recognition with two novel components: substructure transition model and
discriminative boundary model. The first component encodes the sparse and
global temporal transition prior between action primitives in state-space model
to handle the large spatial-temporal variations within an action class. The
second component enforces the action duration constraint in a discriminative
way to locate the transition boundaries between actions more accurately. The
two components are integrated into a unified graphical structure to enable
effective training and inference. Our comprehensive experimental results on
both public and in-house datasets show that, with the capability to incorporate
additional information that had not been explicitly or efficiently modeled by
previous methods, our proposed algorithm achieved significantly improved
performance for continuous action recognition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1986</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1986</id><created>2012-03-08</created><authors><author><keyname>Glew</keyname><forenames>Neal</forenames></author><author><keyname>Petersen</keyname><forenames>Leaf</forenames></author></authors><title>Type-Preserving Flow Analysis and Interprocedural Unboxing (Extended
  Version)</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interprocedural flow analysis can be used to eliminate otherwise unnecessary
heap allocated objects (unboxing), and in previous work we have shown how to do
so while maintaining correctness with respect to the garbage collector. In this
paper, we extend the notion of flow analysis to incorporate types, enabling
analysis and optimization of typed programs. We apply this typed analysis to
specify a type preserving interprocedural unboxing optimization, and prove that
the optimization preserves both type and GC safety along with program
semantics. We also show that the unboxing optimization can be applied
independently to separately compiled program modules, and prove via a
contextual equivalence result that unboxing a module in isolation preserves
program semantics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1995</identifier>
 <datestamp>2012-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1995</id><created>2012-03-09</created><authors><author><keyname>Wang</keyname><forenames>Xuequn</forenames></author><author><keyname>Yu</keyname><forenames>Yanjun</forenames></author></authors><title>Classify Participants in Online Communities</title><categories>cs.SI</categories><comments>13 pages, 2 tables, 2 figures</comments><journal-ref>International Journal of Managing Information Technology (IJMIT)
  Vol.4, No.1, February 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As online communities become increasingly popular, researchers have tried to
examine participating activities in online communities as well as how to
sustain online communities. However, relatively few studies have tried to
understand what kinds of participants constitute online communities. In this
study, we try to contribute online community research by developing &quot;common
language&quot; to classify different participants in online communities.
Specifically, we argue that the previous way to classify participants is not
sufficient and accurate, and we propose a continuum to classify participants
based on participants' overall trend of posting activities. In order to further
online community research, we also propose potential directions for future
studies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.1997</identifier>
 <datestamp>2012-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.1997</id><created>2012-03-09</created><authors><author><keyname>Reddy</keyname><forenames>Akula Aneesh</forenames></author><author><keyname>Sanghavi</keyname><forenames>Sujay</forenames></author><author><keyname>Shakkottai</keyname><forenames>Sanjay</forenames></author></authors><title>On the Effect of Channel Fading on Greedy Scheduling</title><categories>cs.NI</categories><comments>A preliminary version has appeared in the proceedings of IEEE INFOCOM
  2012. The full version is submitted to IEEE TON</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Greedy Maximal Scheduling (GMS) is an attractive low-complexity scheme for
scheduling in wireless networks. Recent work has characterized its throughput
for the case when there is no fading/channel variations. This paper aims to
understand the effect of channel variations on the relative throughput
performance of GMS vis-a-vis that of an optimal scheduler facing the same
fading. The effect is not a-priori obvious because, on the one hand, fading
could help by decoupling/precluding global states that lead to poor GMS
performance, while on the other hand fading adds another degree of freedom in
which an event unfavourable to GMS could occur.
  We show that both these situations can occur when fading is adversarial. In
particular, we first define the notion of a {\em Fading Local Pooling factor
(F-LPF)}, and show that it exactly characterizes the throughput of GMS in this
setting. We also derive general upper and lower bounds on F-LPF. Using these
bounds, we provide two example networks - one where the relative performance of
GMS is worse than if there were no fading, and one where it is better.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2000</identifier>
 <datestamp>2012-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2000</id><created>2012-03-09</created><authors><author><keyname>Madhulatha</keyname><forenames>T Soni</forenames></author></authors><title>Overview of streaming-data algorithms</title><categories>cs.DB cs.IR</categories><comments>10 pages</comments><journal-ref>Advanced Computing: An International Journal ( ACIJ ), November
  2011, Volume 2, Number 6 Advanced Computing: An International Journal ( ACIJ
  ) ISSN : 2229 - 6727 [Online] ; 2229 - 726X [Print]</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Due to recent advances in data collection techniques, massive amounts of data
are being collected at an extremely fast pace. Also, these data are potentially
unbounded. Boundless streams of data collected from sensors, equipments, and
other data sources are referred to as data streams. Various data mining tasks
can be performed on data streams in search of interesting patterns. This paper
studies a particular data mining task, clustering, which can be used as the
first step in many knowledge discovery processes. By grouping data streams into
homogeneous clusters, data miners can learn about data characteristics which
can then be developed into classification models for new data or predictive
models for unknown events. Recent research addresses the problem of data-stream
mining to deal with applications that require processing huge amounts of data
such as sensor data analysis and financial applications. For such analysis,
single-pass algorithms that consume a small amount of memory are critical.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2002</identifier>
 <datestamp>2012-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2002</id><created>2012-03-09</created><authors><author><keyname>Madhulatha</keyname><forenames>T Soni</forenames></author></authors><title>Graph partitioning advance clustering technique</title><categories>cs.LG cs.DB</categories><comments>14 pages</comments><journal-ref>International Journal of Computer Science and Engineering
  Survey(IJCSES), February 2012, Volume 3, Number 1</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Clustering is a common technique for statistical data analysis, Clustering is
the process of grouping the data into classes or clusters so that objects
within a cluster have high similarity in comparison to one another, but are
very dissimilar to objects in other clusters. Dissimilarities are assessed
based on the attribute values describing the objects. Often, distance measures
are used. Clustering is an unsupervised learning technique, where interesting
patterns and structures can be found directly from very large data sets with
little or none of the background knowledge. This paper also considers the
partitioning of m-dimensional lattice graphs using Fiedler's approach, which
requires the determination of the eigenvector belonging to the second smallest
Eigenvalue of the Laplacian with K-means partitioning algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2015</identifier>
 <datestamp>2012-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2015</id><created>2012-03-09</created><authors><author><keyname>H&#xe4;gglund</keyname><forenames>Jonas</forenames></author></authors><title>On snarks that are far from being 3-edge colorable</title><categories>math.CO cs.DM</categories><comments>10 pages, 7 figures</comments><msc-class>05C70</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note we construct two infinite snark families which have high oddness
and low circumference compared to the number of vertices. Using this
construction, we also give a counterexample to a suggested strengthening of
Fulkerson's conjecture by showing that the Petersen graph is not the only
cyclically 4-edge connected cubic graph which require at least five perfect
matchings to cover its edges. Furthermore the counterexample presented has the
interesting property that no 2-factor can be part of a cycle double cover.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2021</identifier>
 <datestamp>2012-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2021</id><created>2012-03-09</created><authors><author><keyname>Lespinats</keyname><forenames>Sylvain</forenames></author><author><keyname>Meyer-Baese</keyname><forenames>Anke</forenames></author><author><keyname>Aupetit</keyname><forenames>Michael</forenames></author></authors><title>A new supervised non-linear mapping</title><categories>cs.IR</categories><comments>2 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Supervised mapping methods project multi-dimensional labeled data onto a
2-dimensional space attempting to preserve both data similarities and topology
of classes. Supervised mappings are expected to help the user to understand the
underlying original class structure and to classify new data visually. Several
methods have been designed to achieve supervised mapping, but many of them
modify original distances prior to the mapping so that original data
similarities are corrupted and even overlapping classes tend to be separated
onto the map ignoring their original topology. We propose ClassiMap, an
alternative method for supervised mapping. Mappings come with distortions which
can be split between tears (close points mapped far apart) and false
neighborhoods (points far apart mapped as neighbors). Some mapping methods
favor the former while others favor the latter. ClassiMap switches between such
mapping methods so that tears tend to appear between classes and false
neighborhood within classes, better preserving classes' topology. We also
propose two new objective criteria instead of the usual subjective visual
inspection to perform fair comparisons of supervised mapping methods. ClassiMap
appears to be the best supervised mapping method according to these criteria in
our experiments on synthetic and real datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2024</identifier>
 <datestamp>2012-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2024</id><created>2012-03-09</created><authors><author><keyname>Sridharan</keyname><forenames>Arun</forenames></author><author><keyname>Koksal</keyname><forenames>C. Emre</forenames></author></authors><title>A Greedy Link Scheduler for Wireless Networks with Fading Channels</title><categories>cs.NI cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of link scheduling for wireless networks with fading
channels, where the link rates are varying with time. Due to the high
computational complexity of the throughput optimal scheduler, we provide a low
complexity greedy link scheduler GFS, with provable performance guarantees. We
show that the performance of our greedy scheduler can be analyzed using the
Local Pooling Factor (LPF) of a network graph, which has been previously used
to characterize the stability of the Greedy Maximal Scheduling (GMS) policy for
networks with static channels. We conjecture that the performance of GFS is a
lower bound on the performance of GMS for wireless networks with fading
channels
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2031</identifier>
 <datestamp>2012-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2031</id><created>2012-03-09</created><authors><author><keyname>Levin</keyname><forenames>Mark Sh.</forenames></author><author><keyname>Fimin</keyname><forenames>Alexander V.</forenames></author></authors><title>Design of modular wireless sensor</title><categories>cs.SE cs.NI cs.SY math.OC</categories><comments>7 pages, 9 figures, 4 tables</comments><msc-class>68T20, 94C30, 90B50, 90C27</msc-class><acm-class>D.2.2; G.2.1; I.2.8; J.6; C.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper addresses combinatorial approach to design of modular wireless
sensor as composition of the sensor element from its component alternatives and
aggregation of the obtained solutions into a resultant aggregated solution. A
hierarchical model is used for the wireless sensor element. The solving process
consists of three stages: (i) multicriteria ranking of design alternatives for
system components/parts, (ii) composing the selected design alternatives into
composite solution(s) while taking into account ordinal quality of the design
alternatives above and their compatibility (this stage is based on Hierarchical
Morphological Multicriteria Design - HMMD), and (iii) aggregation of the
obtained composite solutions into a resultant aggregated solution(s). A
numerical example describes the problem structuring and solving processes for
modular alarm wireless sensor element.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2041</identifier>
 <datestamp>2012-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2041</id><created>2012-03-09</created><authors><author><keyname>Zubow</keyname><forenames>Anatolij</forenames></author><author><keyname>Sombrutzki</keyname><forenames>Robert</forenames></author><author><keyname>Scheidgen</keyname><forenames>Markus</forenames></author></authors><title>Towards MAC/Anycast Diversity in IEEE 802.11n MIMO Networks</title><categories>cs.NI</categories><report-no>SAR-PR-2012-03</report-no><acm-class>C.2.1; C.2.2; C.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Opportunistic Routing (OR) is a novel routing technique for wireless mesh
networks that exploits the broadcast nature of the wireless medium. OR combines
frames from multiple receivers and therefore creates a form of Spatial
Diversity, called MAC Diversity. The gain from OR is especially high in
networks where the majority of links has a high packet loss probability. The
updated IEEE 802.11n standard improves the physical layer with the ability to
use multiple transmit and receive antennas, i.e. Multiple-Input and
Multiple-Output (MIMO), and therefore already offers spatial diversity on the
physical layer, i.e. called Physical Diversity, which improves the reliability
of a wireless link by reducing its error rate. In this paper we quantify the
gain from MAC diversity as utilized by OR in the presence of PHY diversity as
provided by a MIMO system like 802.11n. We experimented with an IEEE 802.11n
indoor testbed and analyzed the nature of packet losses. Our experiment results
show negligible MAC diversity gains for both interference-prone 2.4 GHz and
interference-free 5 GHz channels when using 802.11n. This is different to the
observations made with single antenna systems based on 802.11b/g, as well as in
initial studies with 802.11n.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2044</identifier>
 <datestamp>2012-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2044</id><created>2012-03-09</created><authors><author><keyname>Haboub</keyname><forenames>Rachid</forenames></author><author><keyname>Ouzzif</keyname><forenames>Mohammed</forenames></author></authors><title>Secure and reliable routing in mobile adhoc networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The growing diffusion of wireless-enabled portable devices and the recent
advances in Mobile Ad-hoc NETworks (MANETs) open new scenarios where users can
benefit from anywhere and at any time for impromptu collaboration. However,
energy constrained nodes, low channel bandwidth, node mobility, high channel
error rates, channel variability and packet loss are some of the limitations of
MANETs. MANETs presents also security challenges. These networks are prone to
malicious users attack, because any device within the frequency range can get
access to the MANET. There is a need for security mechanisms aware of these
challenges. Thus, this work aims to provide a secure MANET by changing the
frequency of data transmission. This security approach was tested, and the
results shows an interesting decreased of throughput from malicious node when
the number of frequency used is increased, that way the MANET will not waste
it's resources treating malicious packets. The other contribution of this work
is a mobility aware routing approach, which aims to provide a more reliable
routing by handling effectively the nodes mobility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2081</identifier>
 <datestamp>2012-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2081</id><created>2012-03-09</created><updated>2012-06-15</updated><authors><author><keyname>Pace</keyname><forenames>Matthew Felice</forenames><affiliation>University of Warwick</affiliation></author></authors><title>BSP vs MapReduce</title><categories>cs.DC</categories><comments>13 pages, appeared at ICCS 2012</comments><doi>10.1016/j.procs.2012.04.026</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The MapReduce framework has been generating a lot of interest in a wide range
of areas. It has been widely adopted in industry and has been used to solve a
number of non-trivial problems in academia. Putting MapReduce on strong
theoretical foundations is crucial in understanding its capabilities. This work
links MapReduce to the BSP model of computation, underlining the relevance of
BSP to modern parallel algorithm design and defining a subclass of BSP
algorithms that can be efficiently implemented in MapReduce.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2109</identifier>
 <datestamp>2012-11-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2109</id><created>2012-03-09</created><updated>2012-11-26</updated><authors><author><keyname>Krioukov</keyname><forenames>Dmitri</forenames></author><author><keyname>Kitsak</keyname><forenames>Maksim</forenames></author><author><keyname>Sinkovits</keyname><forenames>Robert S.</forenames></author><author><keyname>Rideout</keyname><forenames>David</forenames></author><author><keyname>Meyer</keyname><forenames>David</forenames></author><author><keyname>Boguna</keyname><forenames>Marian</forenames></author></authors><title>Network Cosmology</title><categories>gr-qc cond-mat.dis-nn cs.NI cs.SI physics.soc-ph</categories><journal-ref>Nature Scientific Reports, v.2, p.793, 2012</journal-ref><doi>10.1038/srep00793</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Prediction and control of the dynamics of complex networks is a central
problem in network science. Structural and dynamical similarities of different
real networks suggest that some universal laws might accurately describe the
dynamics of these networks, albeit the nature and common origin of such laws
remain elusive. Here we show that the causal network representing the
large-scale structure of spacetime in our accelerating universe is a power-law
graph with strong clustering, similar to many complex networks such as the
Internet, social, or biological networks. We prove that this structural
similarity is a consequence of the asymptotic equivalence between the
large-scale growth dynamics of complex networks and causal networks. This
equivalence suggests that unexpectedly similar laws govern the dynamics of
complex networks and spacetime in the universe, with implications to network
science and cosmology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2122</identifier>
 <datestamp>2014-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2122</id><created>2012-03-09</created><authors><author><keyname>Eger</keyname><forenames>Steffen</forenames></author></authors><title>Stirling's approximation for central polynomial coefficients</title><categories>math.PR cs.DM math.CO</categories><comments>5 pages, 2 figures</comments><journal-ref>The American Mathematical Monthly Vol. 121, No. 4 (April) (pp.
  344-349), 2014</journal-ref><doi>10.4169/amer.math.monthly.121.04.344</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We derive asymptotic formulae for central polynomial coefficients, a
generalization of binomial coefficients, using the distribution of the sum of
independent uniform random variables and the CLT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2147</identifier>
 <datestamp>2012-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2147</id><created>2012-03-09</created><authors><author><keyname>Sudheer</keyname><forenames>G.</forenames></author><author><keyname>Devi</keyname><forenames>B. V. S. Renuka</forenames></author></authors><title>A Hybrid Image Cryptosystem Based On OMFLIP Permutation Cipher</title><categories>cs.MM cs.IT math.IT</categories><comments>8 pages</comments><journal-ref>Signal &amp; Image Processing : An International Journal (SIPIJ)
  Vol.3, No.1, February 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The protection of confidential image data from unauthorized access is an
important area of research in network communication. This paper presents a
high-level security encryption scheme for gray scale images. The gray level
image is first decomposed into binary images using bit scale decomposition.
Each binary image is then compressed by selecting a good scanning path that
minimizes the total number of bits needed to encode the bit sequence along the
scanning path using two dimensional run encoding. The compressed bit string is
then scrambled iteratively using a pseudo-random number generator and finally
encrypted using a bit level permutation OMFLIP. The performance is tested,
illustrated and discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2163</identifier>
 <datestamp>2012-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2163</id><created>2012-03-09</created><authors><author><keyname>Hong</keyname><forenames>Chi-Yao</forenames></author><author><keyname>Caesar</keyname><forenames>Matthew</forenames></author><author><keyname>Duffield</keyname><forenames>Nick</forenames></author><author><keyname>Wang</keyname><forenames>Jia</forenames></author></authors><title>Tiresias: Online Anomaly Detection for Hierarchical Operational Network
  Data</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Operational network data, management data such as customer care call logs and
equipment system logs, is a very important source of information for network
operators to detect problems in their networks. Unfortunately, there is lack of
efficient tools to automatically track and detect anomalous events on
operational data, causing ISP operators to rely on manual inspection of this
data. While anomaly detection has been widely studied in the context of network
data, operational data presents several new challenges, including the
volatility and sparseness of data, and the need to perform fast detection
(complicating application of schemes that require offline processing or
large/stable data sets to converge).
  To address these challenges, we propose Tiresias, an automated approach to
locating anomalous events on hierarchical operational data. Tiresias leverages
the hierarchical structure of operational data to identify high-impact
aggregates (e.g., locations in the network, failure modes) likely to be
associated with anomalous events. To accommodate different kinds of operational
network data, Tiresias consists of an online detection algorithm with low time
and space complexity, while preserving high detection accuracy. We present
results from two case studies using operational data collected at a large
commercial IP network operated by a Tier-1 ISP: customer care call logs and
set-top box crash logs. By comparing with a reference set verified by the ISP's
operational group, we validate that Tiresias can achieve &gt;94% accuracy in
locating anomalies. Tiresias also discovered several previously unknown
anomalies in the ISP's customer care cases, demonstrating its effectiveness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2167</identifier>
 <datestamp>2012-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2167</id><created>2012-03-09</created><authors><author><keyname>Bhat</keyname><forenames>Naagesh S.</forenames></author></authors><title>Design and Implementation of IEEE 802.15.4 Mac Protocol on FPGA</title><categories>cs.NI</categories><comments>5 Pages, 11 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The IEEE 802.15.4 is a wireless standard introduced for low power, low cost
wireless communication with moderate data rates. In the next few years, it is
expected that Low Rate Wireless Personal Area Networks (LR-WPAN) will be used
in a wide variety of embedded applications, including home automation,
industrial sensing and control, environmental monitoring and sensing. In these
applications, numerous embedded devices running on batteries are distributed in
an area communicating via wireless radios. This work presents a method which
can be used for comparing current consumption of wireless data transfer
embedded systems. This paper implements a small subset of the IEEE 802.15.4
protocol to achieve a point to point communication. The implemented protocol
uses 802.15.4 MAC compliant data and acknowledgment packets. Current
consumption is measured while doing one data packet transmission. Measurements
are compared with existing work. IEEE 802.15.4 protocol implementation is done
using Verilog language. Code implementation is done in such a manner so that it
can be ported to any platform with minimal changes. It can also be modified to
suit any special experimental setup requirements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2168</identifier>
 <datestamp>2012-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2168</id><created>2012-03-09</created><authors><author><keyname>Cook</keyname><forenames>Stephen</forenames></author></authors><title>Relativized Propositional Calculus</title><categories>cs.CC</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Proof systems for the Relativized Propositional Calculus are defined and
compared.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2169</identifier>
 <datestamp>2012-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2169</id><created>2012-03-09</created><authors><author><keyname>Slimane</keyname><forenames>Emna Ben</forenames></author><author><keyname>Jarboui</keyname><forenames>Slaheddine</forenames></author><author><keyname>Bouall&#xe8;gue</keyname><forenames>Ammar</forenames></author></authors><title>Blind Carrier Phase Recovery for General 2{\pi}/M-rotationally Symmetric
  Constellations</title><categories>cs.IT math.IT</categories><comments>14 pages, 12 figures, International Journal of Wireless &amp; Mobile
  Networks (IJWMN)</comments><journal-ref>International Journal of Wireless &amp; Mobile Networks (IJWMN) Vol.
  4, No. 1, February 2012</journal-ref><doi>10.5121/ijwmn.2012.4104</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper introduces a novel blind carrier phase recovery estimator for
general 2{\Pi}/M-rotationally symmetric constellations. This estimation method
is a generalization of the non-data-aided (NDA) nonlinear Phase Metric Method
(PMM) estimator already designed for general quadrature amplitude
constellations. This unbiased estimator is seen here as a fourth order PMM then
generalized to Mth order (Mth PMM) in such manner that it covers general
2{\Pi}/M-rotationally symmetric constellations such as PAM, QAM, PSK.
Simulation results demonstrate the good performance of this Mth PMM estimation
algorithm against competitive blind phase estimators already published for
various modulation systems of practical interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2171</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2171</id><created>2012-03-09</created><updated>2013-04-05</updated><authors><author><keyname>Kawarabayashi</keyname><forenames>Ken-ichi</forenames></author><author><keyname>Norine</keyname><forenames>Serguei</forenames></author><author><keyname>Thomas</keyname><forenames>Robin</forenames></author><author><keyname>Wollan</keyname><forenames>Paul</forenames></author></authors><title>K_6 minors in 6-connected graphs of bounded tree-width</title><categories>math.CO cs.DM</categories><comments>33 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that every sufficiently big 6-connected graph of bounded tree-width
either has a K_6 minor, or has a vertex whose deletion makes the graph planar.
This is a step toward proving that the same conclusion holds for all
sufficiently big 6-connected graphs. Jorgensen conjectured that it holds for
all 6-connected graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2177</identifier>
 <datestamp>2012-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2177</id><created>2012-03-09</created><authors><author><keyname>de Freitas</keyname><forenames>Nando</forenames></author><author><keyname>Smola</keyname><forenames>Alex</forenames></author><author><keyname>Zoghi</keyname><forenames>Masrour</forenames></author></authors><title>Regret Bounds for Deterministic Gaussian Process Bandits</title><categories>cs.LG stat.ML</categories><comments>17 pages, 5 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper analyses the problem of Gaussian process (GP) bandits with
deterministic observations. The analysis uses a branch and bound algorithm that
is related to the UCB algorithm of (Srinivas et al., 2010). For GPs with
Gaussian observation noise, with variance strictly greater than zero, (Srinivas
et al., 2010) proved that the regret vanishes at the approximate rate of
$O(\frac{1}{\sqrt{t}})$, where t is the number of observations. To complement
their result, we attack the deterministic case and attain a much faster
exponential convergence rate. Under some regularity assumptions, we show that
the regret decreases asymptotically according to $O(e^{-\frac{\tau t}{(\ln
t)^{d/4}}})$ with high probability. Here, d is the dimension of the search
space and $\tau$ is a constant that depends on the behaviour of the objective
function near its global maximum.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2192</identifier>
 <datestamp>2012-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2192</id><created>2012-03-09</created><authors><author><keyname>Kawarabayashi</keyname><forenames>Ken-ichi</forenames></author><author><keyname>Norine</keyname><forenames>Serguei</forenames></author><author><keyname>Thomas</keyname><forenames>Robin</forenames></author><author><keyname>Wollan</keyname><forenames>Paul</forenames></author></authors><title>K_6 minors in large 6-connected graphs</title><categories>math.CO cs.DM</categories><comments>47 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Jorgensen conjectured that every 6-connected graph with no K_6 minor has a
vertex whose deletion makes the graph planar. We prove the conjecture for all
sufficiently large graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2195</identifier>
 <datestamp>2012-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2195</id><created>2012-03-09</created><authors><author><keyname>Nidhi</keyname></author><author><keyname>Lobiyal</keyname><forenames>D. K.</forenames></author></authors><title>Performance Evaluation of Realistic Vanet Using Traffic Light Scenario</title><categories>cs.NI cs.PF</categories><journal-ref>International Journal of Wireless &amp; Mobile Networks (IJWMN) Vol.
  4, No. 1, February 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vehicular Ad-hoc Networks (VANETs) is attracting considerable attention from
the research community and the automotive industry to improve the services of
Intelligent Transportation System (ITS). As today's transportation system faces
serious challenges in terms of road safety, efficiency, and environmental
friendliness, the idea of so called &quot;ITS&quot; has emerged. Due to the expensive
cost of deployment and complexity of implementing such a system in real world,
research in VANET relies on simulation. This paper attempts to evaluate the
performance of VANET in a realistic environment. The paper contributes by
generating a real world road Map of JNU using existing Google Earth and GIS
tools. Traffic data from a limited region of road Map is collected to capture
the realistic mobility. In this work, the entire region has been divided into
various smaller routes. The realistic mobility model used here considers the
driver's route choice at the run time. It also studies the clustering effect
caused by traffic lights used at the intersection to regulate traffic movement
at different directions. Finally, the performance of the VANET is evaluated in
terms of average delivery ratio, packet loss, and router drop as statistical
measures for driver route choice with traffic light scenario. This experiment
has provided insight into the performance of vehicular traffic communication
for a small realistic scenario.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2200</identifier>
 <datestamp>2012-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2200</id><created>2012-03-09</created><authors><author><keyname>Rossi</keyname><forenames>Ryan</forenames></author><author><keyname>Gallagher</keyname><forenames>Brian</forenames></author><author><keyname>Neville</keyname><forenames>Jennifer</forenames></author><author><keyname>Henderson</keyname><forenames>Keith</forenames></author></authors><title>Role-Dynamics: Fast Mining of Large Dynamic Networks</title><categories>cs.SI cs.AI cs.LG stat.ML</categories><acm-class>H.2.8; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To understand the structural dynamics of a large-scale social, biological or
technological network, it may be useful to discover behavioral roles
representing the main connectivity patterns present over time. In this paper,
we propose a scalable non-parametric approach to automatically learn the
structural dynamics of the network and individual nodes. Roles may represent
structural or behavioral patterns such as the center of a star, peripheral
nodes, or bridge nodes that connect different communities. Our novel approach
learns the appropriate structural role dynamics for any arbitrary network and
tracks the changes over time. In particular, we uncover the specific global
network dynamics and the local node dynamics of a technological, communication,
and social network. We identify interesting node and network patterns such as
stationary and non-stationary roles, spikes/steps in role-memberships (perhaps
indicating anomalies), increasing/decreasing role trends, among many others.
Our results indicate that the nodes in each of these networks have distinct
connectivity patterns that are non-stationary and evolve considerably over
time. Overall, the experiments demonstrate the effectiveness of our approach
for fast mining and tracking of the dynamics in large networks. Furthermore,
the dynamic structural representation provides a basis for building more
sophisticated models and tools that are fast for exploring large dynamic
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2202</identifier>
 <datestamp>2013-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2202</id><created>2012-03-09</created><updated>2013-01-21</updated><authors><author><keyname>Xie</keyname><forenames>Hongmei</forenames></author><author><keyname>Yan</keyname><forenames>Zhiyuan</forenames></author></authors><title>Exact-MSR Codes for Distributed Storage with Low Repair Complexity</title><categories>cs.IT math.IT</categories><comments>This paper has been withdrawn by the author due to incorrect
  statements</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose two new constructions of exact-repair minimum
storage regenerating (exact-MSR) codes. For both constructions, the encoded
symbols are obtained by treating the message vector over GF(q) as a linearized
polynomial and evaluating it over an extension field GF(q^m). For our exact-MSR
codes, data repair does not need matrix inversion, and can be implemented by
additions and multiplications over GF$(q)$ as well as cyclic shifts when a
normal basis is used. The two constructions assume a base field of GF(q) (q&gt;2)
and GF(2), respectively. In contrast to existing constructions of exact-MSR
codes, the former construction works for arbitrary code parameters, provided
that $q$ is large enough. This is the first construction of exact-MSR codes
with arbitrary code parameters, to the best of our knowledge. In comparison to
existing exact-MSR codes, while data construction of our exact-MSR codes has a
higher complexity, the complexity of data repair is lower. Thus, they are
attractive for applications that need a small number of data reconstructions
along with a large number of data repairs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2205</identifier>
 <datestamp>2012-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2205</id><created>2012-03-09</created><authors><author><keyname>Puy</keyname><forenames>Gilles</forenames></author><author><keyname>Marques</keyname><forenames>Jose P.</forenames></author><author><keyname>Gruetter</keyname><forenames>Rolf</forenames></author><author><keyname>Thiran</keyname><forenames>Jean-Philippe</forenames></author><author><keyname>Van De Ville</keyname><forenames>Dimitri</forenames></author><author><keyname>Vandergheynst</keyname><forenames>Pierre</forenames></author><author><keyname>Wiaux</keyname><forenames>Yves</forenames></author></authors><title>Spread spectrum magnetic resonance imaging</title><categories>cs.OH</categories><journal-ref>IEEE Transactions on Medical Imaging, vol. 31(3), pp. 586-598,
  2012</journal-ref><doi>10.1109/TMI.2011.2173698</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel compressed sensing technique to accelerate the magnetic
resonance imaging (MRI) acquisition process. The method, coined spread spectrum
MRI or simply s2MRI, consists of pre-modulating the signal of interest by a
linear chirp before random k-space under-sampling, and then reconstructing the
signal with non-linear algorithms that promote sparsity. The effectiveness of
the procedure is theoretically underpinned by the optimization of the coherence
between the sparsity and sensing bases. The proposed technique is thoroughly
studied by means of numerical simulations, as well as phantom and in vivo
experiments on a 7T scanner. Our results suggest that s2MRI performs better
than state-of-the-art variable density k-space under-sampling approaches
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2209</identifier>
 <datestamp>2012-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2209</id><created>2012-03-09</created><updated>2012-09-11</updated><authors><author><keyname>Sato</keyname><forenames>Cristiane M.</forenames></author></authors><title>On the robustness of random k-cores</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The k-core of a graph is its maximal subgraph with minimum degree at least k.
In this paper, we address robustness questions about k-cores. Given a k-core,
remove one edge uniformly at random and find its new k-core. We are interested
in how many vertices are deleted from the original k-core to find the new one.
This can be seem as a measure of robustness of the original k-core. We prove
that, if the initial k-core is chosen uniformly at random from the k-cores with
n vertices and m edges, its robustness depends essentially on its average
degree c. We prove that, if c converges to k, then the new k-core is empty with
probability 1+o(1). We define a constant c(k)' such that when k+epsilon &lt; c &lt;
c(k)'- epsilon, the new k-core is empty with probability bounded away from zero
and, if c &gt; c(k)'+ psi with psi = omega(n^{-1/4}), psi(n) &gt; 0 and c is bounded,
then the probability that the new k-core has less than n-h(n) vertices goes to
zero, for every h(n) = omega(1/psi).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2210</identifier>
 <datestamp>2012-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2210</id><created>2012-03-09</created><updated>2012-04-17</updated><authors><author><keyname>Liu</keyname><forenames>Risheng</forenames></author><author><keyname>Lin</keyname><forenames>Zhouchen</forenames></author><author><keyname>De la Torre</keyname><forenames>Fernando</forenames></author><author><keyname>Su</keyname><forenames>Zhixun</forenames></author></authors><title>Fixed-Rank Representation for Unsupervised Visual Learning</title><categories>cs.CV cs.NA</categories><comments>accepted by CVPR 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Subspace clustering and feature extraction are two of the most commonly used
unsupervised learning techniques in computer vision and pattern recognition.
State-of-the-art techniques for subspace clustering make use of recent advances
in sparsity and rank minimization. However, existing techniques are
computationally expensive and may result in degenerate solutions that degrade
clustering performance in the case of insufficient data sampling. To partially
solve these problems, and inspired by existing work on matrix factorization,
this paper proposes fixed-rank representation (FRR) as a unified framework for
unsupervised visual learning. FRR is able to reveal the structure of multiple
subspaces in closed-form when the data is noiseless. Furthermore, we prove that
under some suitable conditions, even with insufficient observations, FRR can
still reveal the true subspace memberships. To achieve robustness to outliers
and noise, a sparse regularizer is introduced into the FRR framework. Beyond
subspace clustering, FRR can be used for unsupervised feature extraction. As a
non-trivial byproduct, a fast numerical solver is developed for FRR.
Experimental results on both synthetic data and real applications validate our
theoretical analysis and demonstrate the benefits of FRR for unsupervised
visual learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2213</identifier>
 <datestamp>2012-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2213</id><created>2012-03-09</created><authors><author><keyname>Xu</keyname><forenames>Weiyu</forenames></author><author><keyname>Dimakis</keyname><forenames>Alex</forenames></author><author><keyname>Hassibi</keyname><forenames>Babak</forenames></author></authors><title>On the Mixing Time of Markov Chain Monte Carlo for Integer Least-Square
  Problems</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the mixing time of Markov Chain Monte Carlo (MCMC)
for integer least-square (LS) optimization problems. It is found that the
mixing time of MCMC for integer LS problems depends on the structure of the
underlying lattice. More specifically, the mixing time of MCMC is closely
related to whether there is a local minimum in the lattice structure. For some
lattices, the mixing time of the Markov chain is independent of the
signal-to-noise ($SNR$) ratio and grows polynomially in the problem dimension;
while for some lattices, the mixing time grows unboundedly as $SNR$ grows. Both
theoretical and empirical results suggest that to ensure fast mixing, the
temperature for MCMC should often grow positively as the $SNR$ increases. We
also derive the probability that there exist local minima in an integer
least-square problem, which can be as high as
$1/3-\frac{1}{\sqrt{5}}+\frac{2\arctan(\sqrt{5/3})}{\sqrt{5}\pi}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2226</identifier>
 <datestamp>2014-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2226</id><created>2012-03-09</created><updated>2014-04-15</updated><authors><author><keyname>Galanis</keyname><forenames>Andreas</forenames></author><author><keyname>Stefankovic</keyname><forenames>Daniel</forenames></author><author><keyname>Vigoda</keyname><forenames>Eric</forenames></author></authors><title>Inapproximability of the Partition Function for the Antiferromagnetic
  Ising and Hard-Core Models</title><categories>cs.DM math-ph math.MP math.PR</categories><comments>47 pages. Added explicit version of Lemma 19 which is used elsewhere</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent inapproximability results of Sly (2010), together with an
approximation algorithm presented by Weitz (2006) establish a beautiful picture
for the computational complexity of approximating the partition function of the
hard-core model. Let $\lambda_c(T_\Delta)$ denote the critical activity for the
hard-model on the infinite $\Delta$-regular tree. Weitz presented an FPTAS for
the partition function when $\lambda&lt;\lambda_c(T_\Delta)$ for graphs with
constant maximum degree $\Delta$. In contrast, Sly showed that for all
$\Delta\geq 3$, there exists $\epsilon_\Delta&gt;0$ such that (unless RP=NP) there
is no FPRAS for approximating the partition function on graphs of maximum
degree $\Delta$ for activities $\lambda$ satisfying
$\lambda_c(T_\Delta)&lt;\lambda&lt;\lambda_c(T_\Delta)+\epsilon_\Delta$.
  We prove that a similar phenomenon holds for the antiferromagnetic Ising
model. Recent results of Li et al. and Sinclair et al. extend Weitz's approach
to any 2-spin model, which includes the antiferromagnetic Ising model, to yield
an FPTAS for the partition function for all graphs of constant maximum degree
$\Delta$ when the parameters of the model lie in the uniqueness regime of the
infinite tree $T_\Delta$. We prove the complementary result that for the
antiferrogmanetic Ising model without external field that, unless RP=NP, for
all $\Delta\geq 3$, there is no FPRAS for approximating the partition function
on graphs of maximum degree $\Delta$ when the inverse temperature lies in the
non-uniqueness regime of the infinite tree $T_\Delta$. Our results extend to a
region of the parameter space for general 2-spin models. Our proof works by
relating certain second moment calculations for random $\Delta$-regular
bipartite graphs to the tree recursions used to establish the critical points
on the infinite tree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2228</identifier>
 <datestamp>2012-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2228</id><created>2012-03-09</created><updated>2012-12-05</updated><authors><author><keyname>Motegi</keyname><forenames>Shun</forenames></author><author><keyname>Masuda</keyname><forenames>Naoki</forenames></author></authors><title>A network-based dynamical ranking system for competitive sports</title><categories>physics.soc-ph cs.SI</categories><comments>6 figures</comments><journal-ref>Scientific Reports, 2, 904 (2012)</journal-ref><doi>10.1038/srep00904</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  From the viewpoint of networks, a ranking system for players or teams in
sports is equivalent to a centrality measure for sports networks, whereby a
directed link represents the result of a single game. Previously proposed
network-based ranking systems are derived from static networks, i.e.,
aggregation of the results of games over time. However, the score of a player
(or team) fluctuates over time. Defeating a renowned player in the peak
performance is intuitively more rewarding than defeating the same player in
other periods. To account for this factor, we propose a dynamic variant of such
a network-based ranking system and apply it to professional men's tennis data.
We derive a set of linear online update equations for the score of each player.
The proposed ranking system predicts the outcome of the future games with a
higher accuracy than the static counterparts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2231</identifier>
 <datestamp>2012-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2231</id><created>2012-03-10</created><authors><author><keyname>Bansal</keyname><forenames>Nidhi</forenames></author><author><keyname>Awasthi</keyname><forenames>Amit</forenames></author></authors><title>Accomplish the Application Area in Cloud Computing</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the cloud computing application area of accomplish, we find the fact that
cloud computing covers a lot of areas are its main asset. At a top level, it is
an approach to IT where many users, some even from different companies get
access to shared IT resources such as servers, routers and various file
extensions, instead of each having their own dedicated servers. This offers
many advantages like lower costs and higher efficiency. Unfortunately there
have been some high profile incidents where some of the largest cloud providers
have had outages and even lost data, and this underscores that it is important
to have backup, security and disaster recovery capabilities. In education
field, it gives better choice and flexibility to IT departments than others.
The platform and applications you use can be on-premises, off-premises, or a
combination of both, depending on your academic organization's needs. With
cloud computing in education, you get powerful software and massive computing
resources where and when you need them. Use cloud services to best combine:
*On-demand computing and storage. *A familiar development experience with
on-demand scalability. *Online services for anywhere, anytime access to
powerful web-based tools.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2234</identifier>
 <datestamp>2012-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2234</id><created>2012-03-10</created><authors><author><keyname>Karjee</keyname><forenames>Jyotirmoy</forenames></author><author><keyname>Jamadagni</keyname><forenames>H. S</forenames></author></authors><title>Optimal Node Selection using Estimated Data Accuracy Model in Wireless
  Sensor Networks</title><categories>cs.NI</categories><comments>6 pages</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  One of the major task of wireless sensor network is to sense accurate data
from the physical environment. Hence in this paper, we develop an estimated
data accuracy model for randomly deployed sensor nodes which can sense more
accurate data from the physical environment. We compare our results with other
information accuracy models and shows that our estimated data accuracy model
performs better than the other models. Moreover we simulate our estimated data
accuracy model under such situation when some of the sensor nodes become
malicious due to extreme physical environment. Finally using our estimated data
accuracy model we construct a probabilistic approach for selecting an optimal
set of sensor nodes from the randomly deployed maximal set of sensor nodes in
the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2236</identifier>
 <datestamp>2012-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2236</id><created>2012-03-10</created><authors><author><keyname>Li</keyname><forenames>Yongming</forenames></author><author><keyname>Wang</keyname><forenames>Qian</forenames></author><author><keyname>Li</keyname><forenames>Sanjiang</forenames></author></authors><title>On Quotients of Formal Power Series</title><categories>cs.FL</categories><comments>48 pages, 3 figures, 30 conferences</comments><msc-class>68Q45</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quotient is a basic operation of formal languages, which plays a key role in
the construction of minimal deterministic finite automata (DFA) and the
universal automata. In this paper, we extend this operation to formal power
series and systemically investigate its implications in the study of weighted
automata. In particular, we define two quotient operations for formal power
series that coincide when calculated by a word. We term the first operation as
(left or right) \emph{quotient}, and the second as (left or right)
\emph{residual}. To support the definitions of quotients and residuals, the
underlying semiring is restricted to complete semirings or complete
c-semirings. Algebraical properties that are similar to the classical case are
obtained in the formal power series case. Moreover, we show closure properties,
under quotients and residuals, of regular series and weighted context-free
series are similar as in formal languages. Using these operations, we define
for each formal power series $A$ two weighted automata ${\cal M}_A$ and ${\cal
U}_A$. Both weighted automata accepts $A$, and ${\cal M}_A$ is the minimal
deterministic weighted automaton of $A$. The universality of ${\cal U}_A$ is
justified and, in particular, we show that ${\cal M}_A$ is a sub-automaton of
${\cal U}_A$. Last but not least, an effective method to construct the
universal automaton is also presented in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2241</identifier>
 <datestamp>2012-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2241</id><created>2012-03-10</created><authors><author><keyname>Li</keyname><forenames>Yongming</forenames></author><author><keyname>Li</keyname><forenames>Lijun</forenames></author></authors><title>Model-Checking of Linear-Time Properties Based on Possibility Measure</title><categories>cs.LO</categories><comments>22pages,5 figures</comments><msc-class>68Q60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the LTL model-checking in possibilistic Kripke structure using
possibility measure. First, the notion of possibilistic Kripke structure and
the related possibility measure are introduced, then model-checking of
reachability and repeated reachability linear-time properties in finite
possibilistic Kripke structure are studied. Standard safety property and
-regular property in possibilistic Kripke structure are introduced, the
verification of regular safety property and -regular property using finite
automata are thoroughly studied. It has been shown that the verification of
regular safety property and -regular property in finite possibilistic Kripke
structure can be transformed into the verification of reachability property and
repeated reachability property in the product possibilistic Kripke structure
introduced in this paper. Several examples are given to illustrate the methods
presented in the paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2245</identifier>
 <datestamp>2012-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2245</id><created>2012-03-10</created><authors><author><keyname>Adriaans</keyname><forenames>Pieter</forenames></author></authors><title>Facticity as the amount of self-descriptive information in a data set</title><categories>cs.IT math.IT</categories><comments>10 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using the theory of Kolmogorov complexity the notion of facticity {\phi}(x)
of a string is defined as the amount of self-descriptive information it
contains. It is proved that (under reasonable assumptions: the existence of an
empty machine and the availability of a faithful index) facticity is definite,
i.e. random strings have facticity 0 and for compressible strings 0 &lt; {\phi}(x)
&lt; 1/2 |x| + O(1). Consequently facticity measures the tension in a data set
between structural and ad-hoc information objectively. For binary strings there
is a so-called facticity threshold that is dependent on their entropy. Strings
with facticty above this threshold have no optimal stochastic model and are
essentially computational. The shape of the facticty versus entropy plot
coincides with the well-known sawtooth curves observed in complex systems. The
notion of factic processes is discussed. This approach overcomes problems with
earlier proposals to use two-part code to define the meaningfulness or
usefulness of a data set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2247</identifier>
 <datestamp>2012-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2247</id><created>2012-03-10</created><authors><author><keyname>Raheja</keyname><forenames>Supriya</forenames></author><author><keyname>Dadhich</keyname><forenames>Reena</forenames></author><author><keyname>Rajpal</keyname><forenames>Smita</forenames></author></authors><title>An Optimum Time Quantum Using Linguistic Synthesis for Round Robin
  Scheduling Algorithm</title><categories>cs.OH</categories><comments>International Journal of Soft Computing, Feb 2012</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  In Round Robin CPU scheduling algorithm the main concern is with the size of
time quantum and the increased waiting and turnaround time. Decision for these
is usually based on parameters which are assumed to be precise. However, in
many cases the values of these parameters are vague and imprecise. The
performance of fuzzy logic depends upon the ability to deal with Linguistic
variables. With this intent, this paper attempts to generate an Optimal Time
Quantum dynamically based on the parameters which are treated as Linguistic
variables. This paper also includes Mamdani Fuzzy Inference System using
Trapezoidal membership function, results in LRRTQ Fuzzy Inference System. In
this paper, we present an algorithm to improve the performance of round robin
scheduling algorithm. Numerical analysis based on LRRTQ results on proposed
algorithm show the improvement in the performance of the system by reducing
unnecessary context switches and also by providing reasonable turnaround time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2268</identifier>
 <datestamp>2013-02-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2268</id><created>2012-03-10</created><updated>2013-02-25</updated><authors><author><keyname>Mason</keyname><forenames>Winter</forenames></author><author><keyname>Clauset</keyname><forenames>Aaron</forenames></author></authors><title>Friends FTW! Friendship, Collaboration and Competition in Halo: Reach</title><categories>cs.SI cs.CY cs.HC physics.soc-ph</categories><comments>12 pages, 12 figures, 4 tables</comments><journal-ref>Proceedings of the 2013 Conference on Computer Supported
  Cooperative Work (CSCW '13), 375-386 (2013)</journal-ref><doi>10.1145/2441776.2441820</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How important are friendships in determining success by individuals and teams
in complex collaborative environments? By combining a novel data set containing
the dynamics of millions of ad hoc teams from the popular multiplayer online
first person shooter Halo: Reach with survey data on player demographics, play
style, psychometrics and friendships derived from an anonymous online survey,
we investigate the impact of friendship on collaborative and competitive
performance. In addition to finding significant differences in player behavior
across these variables, we find that friendships exert a strong influence,
leading to both improved individual and team performance--even after
controlling for the overall expertise of the team--and increased pro-social
behaviors. Players also structure their in-game activities around social
opportunities, and as a result hidden friendship ties can be accurately
inferred directly from behavioral time series. Virtual environments that enable
such friendship effects will thus likely see improved collaboration and
competition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2272</identifier>
 <datestamp>2012-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2272</id><created>2012-03-10</created><authors><author><keyname>Singh</keyname><forenames>Gurpreet</forenames></author><author><keyname>Roy</keyname><forenames>Ajay Kumar</forenames></author><author><keyname>S</keyname><forenames>Surekha K</forenames></author><author><keyname>Pujari</keyname><forenames>S.</forenames></author></authors><title>System on Programable Chip for Performance Estimation of Loom Machine</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  System on programmable chip for the performance estimation of loom machine,
which calculates the efficiency and meter count for weaved cloth automatically.
Also it calculates the efficiency of loom machine. Previously the same was done
using manual process which was not efficient. This article is intended for loom
machines which are not modern.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2273</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2273</id><created>2012-03-10</created><authors><author><keyname>Houmansadr</keyname><forenames>Amir</forenames></author><author><keyname>Kiyavash</keyname><forenames>Negar</forenames></author><author><keyname>Borisov</keyname><forenames>Nikita</forenames></author></authors><title>Non-blind watermarking of network flows</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linking network flows is an important problem in intrusion detection as well
as anonymity. Passive traffic analysis can link flows but requires long periods
of observation to reduce errors. Active traffic analysis, also known as flow
watermarking, allows for better precision and is more scalable. Previous flow
watermarks introduce significant delays to the traffic flow as a side effect of
using a blind detection scheme; this enables attacks that detect and remove the
watermark, while at the same time slowing down legitimate traffic. We propose
the first non-blind approach for flow watermarking, called RAINBOW, that
improves watermark invisibility by inserting delays hundreds of times smaller
than previous blind watermarks, hence reduces the watermark interference on
network flows. We derive and analyze the optimum detectors for RAINBOW as well
as the passive traffic analysis under different traffic models by using
hypothesis testing. Comparing the detection performance of RAINBOW and the
passive approach we observe that both RAINBOW and passive traffic analysis
perform similarly good in the case of uncorrelated traffic, however, the
RAINBOW detector drastically outperforms the optimum passive detector in the
case of correlated network flows. This justifies the use of non-blind
watermarks over passive traffic analysis even though both approaches have
similar scalability constraints. We confirm our analysis by simulating the
detectors and testing them against large traces of real network flows.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2289</identifier>
 <datestamp>2012-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2289</id><created>2012-03-08</created><authors><author><keyname>Jadhav</keyname><forenames>Vitthal</forenames></author><author><keyname>Buchade</keyname><forenames>Amar</forenames></author></authors><title>Modified Quine-McCluskey Method</title><categories>cs.OH</categories><comments>4 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The digital gates are basic electronic component of any digital circuit.
Digital circuit should be simplified in order to reduce its cost by reducing
number of digital gates required to implement it. To achieve this, we use
Boolean expression that helps in obtaining minimum number of terms and does not
contain any redundant pair. Karnaugh map(K-map) and Quine-McCluskey(QM) methods
are well known methods to simplify Boolean expression. K-map method becomes
complex beyond five variable Boolean expression. Quine-McCluskey method is
computer based technique for minimization of Boolean function and it is faster
than K-map method. This paper proposes E-sum based optimization to
Quine-McCluskey Method to increase its performance by reducing number of
comparisons between mintermlist in determination of prime implicants. Modified
Quine-McCluskey method(MQM) can be implemented to any number of variable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2293</identifier>
 <datestamp>2012-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2293</id><created>2012-03-10</created><authors><author><keyname>Petrov</keyname><forenames>Sergey</forenames></author><author><keyname>Fontanari</keyname><forenames>Jose F.</forenames></author><author><keyname>Perlovsky</keyname><forenames>Leonid I.</forenames></author></authors><title>Categories of Emotion names in Web retrieved texts</title><categories>cs.CL cs.IR</categories><journal-ref>International Journal of Psychology and Behavioral Sciences 2
  (2012) 173-184</journal-ref><doi>10.5923/j.ijpbs.20120205.08</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The categorization of emotion names, i.e., the grouping of emotion words that
have similar emotional connotations together, is a key tool of Social
Psychology used to explore people's knowledge about emotions. Without
exception, the studies following that research line were based on the gauging
of the perceived similarity between emotion names by the participants of the
experiments. Here we propose and examine a new approach to study the categories
of emotion names - the similarities between target emotion names are obtained
by comparing the contexts in which they appear in texts retrieved from the
World Wide Web. This comparison does not account for any explicit semantic
information; it simply counts the number of common words or lexical items used
in the contexts. This procedure allows us to write the entries of the
similarity matrix as dot products in a linear vector space of contexts. The
properties of this matrix were then explored using Multidimensional Scaling
Analysis and Hierarchical Clustering. Our main findings, namely, the underlying
dimension of the emotion space and the categories of emotion names, were
consistent with those based on people's judgments of emotion names
similarities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2295</identifier>
 <datestamp>2013-05-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2295</id><created>2012-03-10</created><updated>2013-05-16</updated><authors><author><keyname>Chi</keyname><forenames>Eric C.</forenames></author><author><keyname>Lange</keyname><forenames>Kenneth</forenames></author></authors><title>Techniques for Solving Sudoku Puzzles</title><categories>math.OC cs.DS stat.CO</categories><comments>11 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Solving Sudoku puzzles is one of the most popular pastimes in the world.
Puzzles range in difficulty from easy to very challenging; the hardest puzzles
tend to have the most empty cells. The current paper explains and compares
three algorithms for solving Sudoku puzzles. Backtracking, simulated annealing,
and alternating projections are generic methods for attacking combinatorial
optimization problems. Our results favor backtracking. It infallibly solves a
Sudoku puzzle or deduces that a unique solution does not exist. However,
backtracking does not scale well in high-dimensional combinatorial
optimization. Hence, it is useful to expose students in the mathematical
sciences to the other two solution techniques in a concrete setting. Simulated
annealing shares a common structure with MCMC (Markov chain Monte Carlo) and
enjoys wide applicability. The method of alternating projections solves the
feasibility problem in convex programming. Converting a discrete optimization
problem into a continuous optimization problem opens up the possibility of
handling combinatorial problems of much higher dimensionality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2296</identifier>
 <datestamp>2012-05-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2296</id><created>2012-03-10</created><updated>2012-05-08</updated><authors><author><keyname>van Emden</keyname><forenames>M. H.</forenames></author></authors><title>Discovering Algorithms with Matrix Code</title><categories>cs.PL</categories><comments>8 pages, 7 figures</comments><report-no>DCS-345-IR</report-no><acm-class>K.3.2; D.2.4; F.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In first-year programming courses it is often difficult to show students how
an algorithm can be discovered. In this paper we present a program format that
supports the development from specification to code in small and obvious steps;
that is, a discovery process. The format, called Matrix Code, can be
interpreted as a proof according to the Floyd-Hoare program verification
method. The process consists of expressing the specification of a function body
as an initial code matrix and then growing the matrix by adding rows and
columns until the completed matrix is translated in a routine fashion to
compilable code. As worked example we develop a Java program that generates the
table of the first N prime numbers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2297</identifier>
 <datestamp>2012-04-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2297</id><created>2012-03-10</created><updated>2012-04-23</updated><authors><author><keyname>Agnihotri</keyname><forenames>Samar</forenames></author><author><keyname>Jaggi</keyname><forenames>Sidharth</forenames></author><author><keyname>Chen</keyname><forenames>Minghua</forenames></author></authors><title>Analog network coding in general SNR regime: Performance of a greedy
  scheme</title><categories>cs.IT math.IT</categories><comments>11 pages, 5 figures. Fixed an issue with the notation in the
  statement and proof of Lemma 1. arXiv admin note: substantial text overlap
  with arXiv:1204.2150 and arXiv:1202.0372</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of maximum rate achievable with analog network coding for a
unicast communication over a layered relay network with directed links is
considered. A relay node performing analog network coding scales and forwards
the signals received at its input. Recently this problem has been considered
under certain assumptions on per node scaling factor and received SNR.
Previously, we established a result that allows us to characterize the optimal
performance of analog network coding in network scenarios beyond those that can
be analyzed using the approaches based on such assumptions.
  The key contribution of this work is a scheme to greedily compute a lower
bound to the optimal rate achievable with analog network coding in the general
layered networks. This scheme allows for exact computation of the optimal
achievable rates in a wider class of layered networks than those that can be
addressed using existing approaches. For the specific case of Gaussian N-relay
diamond network, to the best of our knowledge, the proposed scheme provides the
first exact characterization of the optimal rate achievable with analog network
coding. Further, for general layered networks, our scheme allows us to compute
optimal rates within a constant gap from the cut-set upper bound asymptotically
in the source power.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2298</identifier>
 <datestamp>2012-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2298</id><created>2012-03-10</created><authors><author><keyname>Milosavljevic</keyname><forenames>Nebojsa</forenames></author><author><keyname>Pawar</keyname><forenames>Sameer</forenames></author><author><keyname>Rouayheb</keyname><forenames>Salim El</forenames></author><author><keyname>Gastpar</keyname><forenames>Michael</forenames></author><author><keyname>Ramchandran</keyname><forenames>Kannan</forenames></author></authors><title>Minimum Cost Multicast with Decentralized Sources</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the multisource multicast problem where every sink in
a given directed acyclic graph is a client and is interested in a common file.
We consider the case where each node can have partial knowledge about the file
as a side information. Assuming that nodes can communicate over the capacity
constrained links of the graph, the goal is for each client to gain access to
the file, while minimizing some linear cost function of number of bits
transmitted in the network. We consider three types of side-information
settings:(ii) side information in the form of linearly correlated packets; and
(iii) the general setting where the side information at the nodes have an
arbitrary (i.i.d.) correlation structure. In this work we 1) provide a
polynomial time feasibility test, i.e., whether or not all the clients can
recover the file, and 2) we provide a polynomial-time algorithm that finds the
optimal rate allocation among the links of the graph, and then determines an
explicit transmission scheme for cases (i) and (ii).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2299</identifier>
 <datestamp>2012-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2299</id><created>2012-03-10</created><authors><author><keyname>Makatchev</keyname><forenames>Maxim</forenames></author><author><keyname>Simmons</keyname><forenames>Reid</forenames></author><author><keyname>Sakr</keyname><forenames>Majd</forenames></author></authors><title>A Cross-cultural Corpus of Annotated Verbal and Nonverbal Behaviors in
  Receptionist Encounters</title><categories>cs.CL cs.RO</categories><comments>7 pages, 3 figures, presented at the Workshop on Gaze in HRI: From
  Modeling to Communication (a Workshop of International Conference on
  Human-Robot Interaction), March 5, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the first annotated corpus of nonverbal behaviors in receptionist
interactions, and the first nonverbal corpus (excluding the original video and
audio data) of service encounters freely available online. Native speakers of
American English and Arabic participated in a naturalistic role play at
reception desks of university buildings in Doha, Qatar and Pittsburgh, USA.
Their manually annotated nonverbal behaviors include gaze direction, hand and
head gestures, torso positions, and facial expressions. We discuss possible
uses of the corpus and envision it to become a useful tool for the human-robot
interaction community.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2301</identifier>
 <datestamp>2013-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2301</id><created>2012-03-10</created><updated>2013-09-17</updated><authors><author><keyname>Capraro</keyname><forenames>Valerio</forenames></author><author><keyname>Scarsini</keyname><forenames>Marco</forenames></author></authors><title>Existence of equilibria in countable games: an algebraic approach</title><categories>cs.GT math.GR</categories><msc-class>Primary 91A06, 91A10, secondary 43A07</msc-class><journal-ref>Games and Economic Behavior 79 (2013) 163-180</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although mixed extensions of finite games always admit equilibria, this is
not the case for countable games, the best-known example being Wald's
pick-the-larger-integer game. Several authors have provided conditions for the
existence of equilibria in infinite games. These conditions are typically of
topological nature and are rarely applicable to countable games. Here we
establish an existence result for the equilibrium of countable games when the
strategy sets are a countable group and the payoffs are functions of the group
operation. In order to obtain the existence of equilibria, finitely additive
mixed strategies have to be allowed. This creates a problem of selection of a
product measure of mixed strategies. We propose a family of such selections and
prove existence of an equilibrium that does not depend on the selection. As a
byproduct we show that if finitely additive mixed strategies are allowed, then
Wald's game admits an equilibrium. We also prove existence of equilibria for
nontrivial extensions of matching-pennies and rock-scissors-paper. Finally we
extend the main results to uncountable games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2315</identifier>
 <datestamp>2014-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2315</id><created>2012-03-11</created><authors><author><keyname>Tarasenko</keyname><forenames>Sergey</forenames></author></authors><title>Modeling multistage decision processes with Reflexive Game Theory</title><categories>cs.MA cs.AI</categories><comments>8 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces application of Reflexive Game Theory to the matter of
multistage decision making processes. The idea behind is that each decision
making session has certain parameters like &quot;when the session is taking place&quot;,
&quot;who are the group members to make decision&quot;, &quot;how group members influence on
each other&quot;, etc. This study illustrates the consecutive or sequential decision
making process, which consist of two stages. During the stage 1 decisions about
the parameters of the ultimate decision making are made. Then stage 2 is
implementation of Ultimate decision making itself. Since during stage 1 there
can be multiple decision sessions. In such a case it takes more than two
sessions to make ultimate (final) decision. Therefore the overall process of
ultimate decision making becomes multistage decision making process consisting
of consecutive decision making sessions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2316</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2316</id><created>2012-03-11</created><authors><author><keyname>Muralidhar</keyname><forenames>Anand</forenames></author><author><keyname>Kumar</keyname><forenames>P. R.</forenames></author></authors><title>Near-optimal quantization and linear network coding for relay networks</title><categories>cs.IT cs.NI math.IT</categories><comments>Submitted to Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a discrete network corresponding to any Gaussian wireless
network that is obtained by simply quantizing the received signals and
restricting the transmitted signals to a finite precision. Since signals in the
discrete network are obtained from those of a Gaussian network, the Gaussian
network can be operated on the quantization-based digital interface defined by
the discrete network. We prove that this digital interface is near-optimal for
Gaussian relay networks and the capacities of the Gaussian and the discrete
networks are within a bounded gap of O(M^2) bits, where M is the number of
nodes.
  We prove that any near-optimal coding strategy for the discrete network can
be naturally transformed into a near-optimal coding strategy for the Gaussian
network merely by quantization. We exploit this by designing a linear coding
strategy for the case of layered discrete relay networks. The linear coding
strategy is near-optimal for Gaussian and discrete networks and achieves rates
within O(M^2) bits of the capacity, independent of channel gains or SNR. The
linear code is robust and the relays need not know the channel gains. The
transmit and receive signals at all relays are simply quantized to binary
tuples of the same length $n$ . The linear network code requires all the relay
nodes to collect the received binary tuples into a long binary vector and apply
a linear transformation on the long vector. The resulting binary vector is
split into smaller binary tuples for transmission by the relays. The
quantization requirements of the linear network code are completely defined by
the parameter $n$, which also determines the resolution of the
analog-to-digital and digital-to-analog convertors for operating the network
within a bounded gap of the network's capacity. The linear network code
explicitly connects network coding for wireline networks with codes for
Gaussian networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2340</identifier>
 <datestamp>2012-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2340</id><created>2012-03-11</created><authors><author><keyname>Chaouech</keyname><forenames>Helmi</forenames></author><author><keyname>Bouallegue</keyname><forenames>Ridha</forenames></author></authors><title>Multiuser Detection in Asynchronous Multibeam Communications</title><categories>cs.NI</categories><comments>14 pages, 6 figures</comments><journal-ref>International Journal of Wireless &amp; Mobile Networks (IJWMN),
  February 2012</journal-ref><doi>10.5121/ijwmn.2012.4102</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with multi-user detection techniques in asynchronous
multibeam satellite communications. The proposed solutions are based on
successive interference cancellation architecture (SIC) and channel decoding
algorithms. The aim of these detection methods is to reduce the effect of
cochannel interference due to co-frequency access, and consequently, improves
the capacity of the mulitbeam communications systems, by improving frequency
reuse. Channel estimation allows the determination of interference
coefficients, which helps their effects compensation. The developed multiuser
detections techniques are iterative. Therefore, detection quality is improved
from a stage to another. Moreover, a signals combining method, which is
integrated into these detection solutions, enhances their capability. The
proposed solutions are evaluated through computer simulations, where an
asynchronous multibeam satellite link is considered over an AWGN channel. The
obtained simulation results showed the robustness of these multi-user detection
techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2365</identifier>
 <datestamp>2014-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2365</id><created>2012-03-11</created><updated>2013-09-06</updated><authors><author><keyname>Bettinelli</keyname><forenames>J&#xe9;r&#xe9;mie</forenames><affiliation>IECL</affiliation></author></authors><title>Increasing Forests and Quadrangulations via a Bijective Approach</title><categories>math.PR cs.DM math.CO</categories><proxy>ccsd</proxy><doi>10.1016/j.jcta.2013.10.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we expose four bijections each allowing to increase (or
decrease) one parameter in either uniform random forests with a fixed number of
edges and trees, or quadrangulations with a boundary having a fixed number of
faces and a fixed boundary length. In particular, this gives a way to sample a
uniform quadrangulation with n + 1 faces from a uniform quadrangulation with n
faces or a uniform forest with n+1 edges and p trees from a uniform forest with
n edges and p trees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2366</identifier>
 <datestamp>2012-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2366</id><created>2012-03-11</created><authors><author><keyname>Michel</keyname><forenames>Franck</forenames><affiliation>CREATIS</affiliation></author><author><keyname>Montagnat</keyname><forenames>Johan</forenames><affiliation>CREATIS</affiliation></author><author><keyname>Glatard</keyname><forenames>Tristan</forenames><affiliation>CREATIS</affiliation></author></authors><title>Technical support for Life Sciences communities on a production grid
  infrastructure</title><categories>cs.DC</categories><comments>HealthGrid'12, Amsterdam : Netherlands (2012)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Production operation of large distributed computing infrastructures (DCI)
still requires a lot of human intervention to reach acceptable quality of
service. This may be achievable for scientific communities with solid IT
support, but it remains a show-stopper for others. Some application execution
environments are used to hide runtime technical issues from end users. But they
mostly aim at fault-tolerance rather than incident resolution, and their
operation still requires substantial manpower. A longer-term support activity
is thus needed to ensure sustained quality of service for Virtual Organisations
(VO). This paper describes how the biomed VO has addressed this challenge by
setting up a technical support team. Its organisation, tooling, daily tasks,
and procedures are described. Results are shown in terms of resource usage by
end users, amount of reported incidents, and developed software tools. Based on
our experience, we suggest ways to measure the impact of the technical support,
perspectives to decrease its human cost and make it more community-specific.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2377</identifier>
 <datestamp>2012-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2377</id><created>2012-03-11</created><authors><author><keyname>Grcar</keyname><forenames>Joseph F.</forenames></author></authors><title>Matrix Stretching for Linear Equations</title><categories>math.NA cs.NA</categories><comments>68 pages, 14 figures, 2 tables</comments><report-no>SAND90-8723</report-no><msc-class>primary 65F50, secondary 15A06, 15A12, 15A23, 15A45, 65F05, 65F35,
  65G05, 68Q20, 68Q25, 68R10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stretching is a new sparse matrix method that makes matrices sparser by
making them larger. Stretching has implications for computational complexity
theory and applications in scientific and parallel computing. It changes matrix
sparsity patterns to render linear equations more easily solved by parallel and
sparse techniques. Some stretchings increase matrix condition numbers only
moderately, and thus solve linear equations stably. For example, these
stretchings solve arrow equations with accuracy and expense preferable to other
solution methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2384</identifier>
 <datestamp>2012-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2384</id><created>2012-03-11</created><authors><author><keyname>Jafar</keyname><forenames>Syed A.</forenames></author></authors><title>Elements of Cellular Blind Interference Alignment --- Aligned Frequency
  Reuse, Wireless Index Coding and Interference Diversity</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explore degrees of freedom (DoF) characterizations of partially connected
wireless networks, especially cellular networks, with no channel state
information at the transmitters. Specifically, we introduce three fundamental
elements --- aligned frequency reuse, wireless index coding and interference
diversity --- through a series of examples, focusing first on infinite regular
arrays, then on finite clusters with arbitrary connectivity and message sets,
and finally on heterogeneous settings with asymmetric multiple antenna
configurations. Aligned frequency reuse refers to the optimality of orthogonal
resource allocations in many cases, but according to unconventional reuse
patterns that are guided by interference alignment principles. Wireless index
coding highlights both the intimate connection between the index coding problem
and cellular blind interference alignment, as well as the added complexity
inherent to wireless settings. Interference diversity refers to the observation
that in a wireless network each receiver experiences a different set of
interferers, and depending on the actions of its own set of interferers, the
interference-free signal space at each receiver fluctuates differently from
other receivers, creating opportunities for robust applications of blind
interference alignment principles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2386</identifier>
 <datestamp>2012-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2386</id><created>2012-03-11</created><authors><author><keyname>Qadir</keyname><forenames>Ashraf</forenames></author><author><keyname>Neubert</keyname><forenames>Jeremiah</forenames></author><author><keyname>Semke</keyname><forenames>William</forenames></author></authors><title>On-Board Visual Tracking with Unmanned Aircraft System (UAS)</title><categories>cs.CV cs.RO</categories><comments>Infotech@Aerospace 2011 Conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the development of a real time tracking algorithm that
runs on a 1.2 GHz PC/104 computer on-board a small UAV. The algorithm uses zero
mean normalized cross correlation to detect and locate an object in the image.
A kalman filter is used to make the tracking algorithm computationally
efficient. Object position in an image frame is predicted using the motion
model and a search window, centered at the predicted position is generated.
Object position is updated with the measurement from object detection. The
detected position is sent to the motion controller to move the gimbal so that
the object stays at the center of the image frame. Detection and tracking is
autonomously carried out on the payload computer and the system is able to work
in two different methods. The first method starts detecting and tracking using
a stored image patch. The second method allows the operator on the ground to
select the interest object for the UAV to track. The system is capable of
re-detecting an object, in the event of tracking failure. Performance of the
tracking system was verified both in the lab and on the field by mounting the
payload on a vehicle and simulating a flight. Tests show that the system can
detect and track a diverse set of objects in real time. Flight testing of the
system will be conducted at the next available opportunity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2393</identifier>
 <datestamp>2012-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2393</id><created>2012-03-11</created><updated>2012-11-08</updated><authors><author><keyname>Gabran</keyname><forenames>Wesam</forenames></author><author><keyname>Liu</keyname><forenames>Chun-Hao</forenames></author><author><keyname>Pawe&#x142;czak</keyname><forenames>Przemys&#x142;aw</forenames></author><author><keyname>Cabric</keyname><forenames>Danijela</forenames></author></authors><title>Primary User Traffic Estimation for Dynamic Spectrum Access</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Accurate estimation of licensed channel Primary User's (PU) temporal
statistics is important for Dynamic Spectrum Access (DSA) systems. With
accurate estimation of the mean duty cycle, u, and the mean off- and on-times
of PUs, DSA systems can more efficiently assign PU resources to its
subscribers, thus, increasing channel utilization. This paper presents a
mathematical analysis of the accuracy of estimating u, as well as the PU mean
off- and on-times, where the estimation accuracy is expressed as the mean
squared estimation error. The analysis applies for the traffic model assuming
exponentially distributed PU off- and on-times, which is a common model in
traffic literature. The estimation accuracy is quantified as a function of the
number of samples and observation window length, hence, this work provides
guidelines on traffic parameters estimation for both energy-constrained and
delay-constrained applications. For estimating u, we consider uniform,
non-uniform, and weighted sample stream averaging, as well as maximum
likelihood estimation. The estimation accuracy of the mean PU off- and on-times
is studied when maximum likelihood estimation is employed. Furthermore, we
develop algorithms for the blind estimation of the traffic parameters based on
the derived theoretical estimation accuracy expressions. We show that the
estimation error for all traffic parameters is lower bounded for a fixed
observation window length due to the correlation between the traffic samples.
Moreover, we prove that for estimating u, maximum likelihood estimation can
yield the same estimation error as weighted sample averaging using only half
the observation window length.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2394</identifier>
 <datestamp>2012-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2394</id><created>2012-03-11</created><authors><author><keyname>Ahmed</keyname><forenames>Mohamed Osama</forenames></author><author><keyname>Bibalan</keyname><forenames>Pouyan T.</forenames></author><author><keyname>de Freitas</keyname><forenames>Nando</forenames></author><author><keyname>Fauvel</keyname><forenames>Simon</forenames></author></authors><title>Decentralized, Adaptive, Look-Ahead Particle Filtering</title><categories>stat.ML cs.LG stat.CO</categories><comments>16 pages, 11 figures, Authorship in alphabetical order</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The decentralized particle filter (DPF) was proposed recently to increase the
level of parallelism of particle filtering. Given a decomposition of the state
space into two nested sets of variables, the DPF uses a particle filter to
sample the first set and then conditions on this sample to generate a set of
samples for the second set of variables. The DPF can be understood as a variant
of the popular Rao-Blackwellized particle filter (RBPF), where the second step
is carried out using Monte Carlo approximations instead of analytical
inference. As a result, the range of applications of the DPF is broader than
the one for the RBPF. In this paper, we improve the DPF in two ways. First, we
derive a Monte Carlo approximation of the optimal proposal distribution and,
consequently, design and implement a more efficient look-ahead DPF. Although
the decentralized filters were initially designed to capitalize on parallel
implementation, we show that the look-ahead DPF can outperform the standard
particle filter even on a single machine. Second, we propose the use of bandit
algorithms to automatically configure the state space decomposition of the DPF.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2399</identifier>
 <datestamp>2012-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2399</id><created>2012-03-12</created><authors><author><keyname>Gupta</keyname><forenames>B. B.</forenames></author><author><keyname>Joshi</keyname><forenames>R. C.</forenames></author><author><keyname>Misra</keyname><forenames>Manoj</forenames></author></authors><title>Estimating strength of DDoS attack using various regression models</title><categories>cs.CR</categories><journal-ref>Int. J. Multimedia Intelligence and Security, Vol. 1, No. 4,
  pp.378-391</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Anomaly-based DDoS detection systems construct profile of the traffic
normally seen in the network, and identify anomalies whenever traffic deviate
from normal profile beyond a threshold. This extend of deviation is normally
not utilised. This paper reports the evaluation results of proposed approach
that utilises this extend of deviation from detection threshold to estimate
strength of DDoS attack using various regression models. A relationship is
established between number of zombies and observed deviation in sample entropy.
Various statistical performance measures, such as coefficient of determination
(R2), coefficient of correlation (CC), sum of square error (SSE), mean square
error (MSE), root mean square error (RMSE), normalised mean square error
(NMSE), Nash-Sutcliffe efficiency index ({\eta}) and mean absolute error (MAE)
are used to measure the performance of various regression models. Internet type
topologies used for simulation are generated using transit-stub model of GT-ITM
topology generator. NS-2 network simulator on Linux platform is used as
simulation test bed for launching DDoS attacks with varied attack strength. A
comparative study is performed using different regression models for estimating
strength of DDoS attack. The simulation results are promising as we are able to
estimate strength of DDoS attack efficiently with very less error rate using
various regression models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2400</identifier>
 <datestamp>2012-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2400</id><created>2012-03-12</created><authors><author><keyname>Gupta</keyname><forenames>B. B.</forenames></author><author><keyname>Misra</keyname><forenames>Manoj</forenames></author><author><keyname>Joshi</keyname><forenames>R. C.</forenames></author></authors><title>An ISP Level Solution to Combat DDoS Attacks using Combined Statistical
  Based Approach</title><categories>cs.CR</categories><journal-ref>International Journal of Information Assurance and Security
  (JIAS), vol. 3, no. 2, pp. 102-110, 2008</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Disruption from service caused by DDoS attacks is an immense threat to
Internet today. These attacks can disrupt the availability of Internet services
completely, by eating either computational or communication resources through
sheer volume of packets sent from distributed locations in a coordinated manner
or graceful degradation of network performance by sending attack traffic at low
rate. In this paper, we describe a novel framework that deals with the
detection of variety of DDoS attacks by monitoring propagation of abrupt
traffic changes inside ISP Domain and then characterizes flows that carry
attack traffic. Two statistical metrics namely, Volume and Flow are used as
parameters to detect DDoS attacks. Effectiveness of an anomaly based detection
and characterization system highly depends on accuracy of threshold value
settings. Inaccurate threshold values cause a large number of false positives
and negatives. Therefore, in our scheme, Six-Sigma and varying tolerance factor
methods are used to identify threshold values accurately and dynamically for
various statistical metrics. NS-2 network simulator on Linux platform is used
as simulation testbed to validate effectiveness of proposed approach. Different
attack scenarios are implemented by varying total number of zombie machines and
at different attack strengths. The comparison with volume-based approach
clearly indicates the supremacy of our proposed system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2404</identifier>
 <datestamp>2012-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2404</id><created>2012-03-12</created><authors><author><keyname>Pallath</keyname><forenames>Nobert Thomas</forenames></author><author><keyname>Thomas</keyname><forenames>Tessamma</forenames></author></authors><title>Video Object Tracking and Analysis for Computer Assisted Surgery</title><categories>cs.CV</categories><comments>11 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pedicle screw insertion technique has made revolution in the surgical
treatment of spinal fractures and spinal disorders. Although X- ray fluoroscopy
based navigation is popular, there is risk of prolonged exposure to X- ray
radiation. Systems that have lower radiation risk are generally quite
expensive. The position and orientation of the drill is clinically very
important in pedicle screw fixation. In this paper, the position and
orientation of the marker on the drill is determined using pattern recognition
based methods, using geometric features, obtained from the input video sequence
taken from CCD camera. A search is then performed on the video frames after
preprocessing, to obtain the exact position and orientation of the drill.
Animated graphics, showing the instantaneous position and orientation of the
drill is then overlaid on the processed video for real time drill control and
navigation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2414</identifier>
 <datestamp>2012-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2414</id><created>2012-03-12</created><authors><author><keyname>Pira</keyname><forenames>Einollah</forenames></author></authors><title>An Optimal Algorithm for Conflict-Free Coloring for Tree of Rings</title><categories>cs.DS</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  An optimal algorithm is presented about Conflict-Free Coloring for connected
subgraphs of tree of rings. Suppose the number of the rings in the tree is |T|
and the maximum length of rings is |R|. A presented algorithm in [1] for a Tree
of rings used O(log|T|.log|R|) colors but this algorithm uses O(log|T|+log|R|)
colors. The coloring earned by this algorithm has the unique-min property, that
is, the unique color is also minimum.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2431</identifier>
 <datestamp>2014-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2431</id><created>2012-03-12</created><authors><author><keyname>Riesco</keyname><forenames>Adri&#xe1;n</forenames></author><author><keyname>Rodr&#xed;guez-Hortal&#xe1;</keyname><forenames>Juan</forenames></author></authors><title>Singular and Plural Functions for Functional Logic Programming</title><categories>cs.PL</categories><comments>53 pages, 5 figures</comments><doi>10.1017/S147106841200004X</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Functional logic programming (FLP) languages use non-terminating and
non-confluent constructor systems (CS's) as programs in order to define
non-strict non-determi-nistic functions. Two semantic alternatives have been
usually considered for parameter passing with this kind of functions: call-time
choice and run-time choice. While the former is the standard choice of modern
FLP languages, the latter lacks some properties---mainly
compositionality---that have prevented its use in practical FLP systems.
Traditionally it has been considered that call-time choice induces a singular
denotational semantics, while run-time choice induces a plural semantics. We
have discovered that this latter identification is wrong when pattern matching
is involved, and thus we propose two novel compositional plural semantics for
CS's that are different from run-time choice.
  We study the basic properties of our plural semantics---compositionality,
polarity, monotonicity for substitutions, and a restricted form of the bubbling
property for constructor systems---and the relation between them and to
previous proposals, concluding that these semantics form a hierarchy in the
sense of set inclusion of the set of computed values. We have also identified a
class of programs characterized by a syntactic criterion for which the proposed
plural semantics behave the same, and a program transformation that can be used
to simulate one of them by term rewriting. At the practical level, we study how
to use the expressive capabilities of these semantics for improving the
declarative flavour of programs. We also propose a language which combines
call-time choice and our plural semantics, that we have implemented in Maude.
The resulting interpreter is employed to test several significant examples
showing the capabilities of the combined semantics.
  To appear in Theory and Practice of Logic Programming (TPLP)
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2434</identifier>
 <datestamp>2012-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2434</id><created>2012-03-12</created><updated>2012-08-21</updated><authors><author><keyname>Yaroshenko</keyname><forenames>Tetiana</forenames></author></authors><title>Institutional repository `eKMAIR': establishing and populating a
  research repository for the National University &quot;Kyiv Mohyla Academy&quot;</title><categories>cs.OH</categories><comments>This paper has been administratively withdrawn because it was
  submitted under false pretenses by someone other than Tetiana Yaroshenko and
  she did not write any of the text</comments><journal-ref>Naukovi zapiski UKMA, vol.12 N.3 p.13-21</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  University libraries have an increasingly important role to play in
supporting open access publishing and dissemination of research outputs.1 In
particular, many libraries are playing a leading role in establishing and
managing institutional repositories. Institutional repositories are, most
often, Open Access Initiative (OAI)-compliant databases of a university or
other research institution's intellectual output, most typically research
papers, although many other forms of digital media can also be stored and
disseminated. Their main function is to provide improved access to the full
text of research articles and improve retrieval of relevant research.
  The National University &quot;Kyiv Mohyla Academy&quot; is a small-sized institution
with approximately 3,000 students and 500 academic staff. Although it is a
teaching-intensive university, developing research and knowledge-transfer
capacity is a strategic priority and four research institutes have been
established, with further research activity going on in the academic schools
and research centres.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2437</identifier>
 <datestamp>2012-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2437</id><created>2012-03-12</created><authors><author><keyname>Claesson</keyname><forenames>Anders</forenames></author><author><keyname>&#xda;lfarsson</keyname><forenames>Henning</forenames></author></authors><title>Sorting and preimages of pattern classes</title><categories>math.CO cs.DS</categories><comments>13 pages, 5 figures, to appear at FPSAC 2012</comments><msc-class>05A05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce an algorithm to determine when a sorting operation, such as
stack-sort or bubble-sort, outputs a given pattern. The algorithm provides a
new proof of the description of West-2-stack-sortable permutations, that is
permutations that are completely sorted when passed twice through a stack, in
terms of patterns. We also solve the long-standing problem of describing
West-3-stack-sortable permutations. This requires a new type of generalized
permutation pattern we call a decorated pattern.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2456</identifier>
 <datestamp>2014-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2456</id><created>2012-03-12</created><updated>2014-01-04</updated><authors><author><keyname>Rajesh</keyname><forenames>R.</forenames></author><author><keyname>Shah</keyname><forenames>Shahid M.</forenames></author><author><keyname>Sharma</keyname><forenames>Vinod</forenames></author></authors><title>On Secrecy above Secrecy Capacity</title><categories>cs.IT math.IT</categories><comments>5 Pages, 2 Figures, Submitted to ICCS 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider secrecy obtained when one transmits on a Gaussian Wiretap channel
above the secrecy capacity. Instead of equivocation, we consider probability of
error as the criterion of secrecy. The usual channel codes are considered for
transmission. The rates obtained can reach the channel capacity. We show that
the &quot;confusion&quot; caused to the Eve when the rate of transmission is above
capacity of the Eve's channel is similar to the confusion caused by using the
wiretap channel codes used below the secrecy capacity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2468</identifier>
 <datestamp>2012-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2468</id><created>2012-03-12</created><authors><author><keyname>Iezzi</keyname><forenames>Michela</forenames></author><author><keyname>Di Renzo</keyname><forenames>Marco</forenames></author><author><keyname>Graziosi</keyname><forenames>Fabio</forenames></author></authors><title>Diversity, Coding, and Multiplexing Trade-Off of Network-Coded
  Cooperative Wireless Networks</title><categories>cs.IT math.IT</categories><comments>IEEE International Conference on Communications (ICC), 2012. Accepted
  for publication and oral presentation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the performance of network-coded cooperative
diversity systems with practical communication constraints. More specifically,
we investigate the interplay between diversity, coding, and multiplexing gain
when the relay nodes do not act as dedicated repeaters, which only forward data
packets transmitted by the sources, but they attempt to pursue their own
interest by forwarding packets which contain a network-coded version of
received and their own data. We provide a very accurate analysis of the Average
Bit Error Probability (ABEP) for two network topologies with three and four
nodes, when practical communication constraints, i.e., erroneous decoding at
the relays and fading over all the wireless links, are taken into account.
Furthermore, diversity and coding gain are studied, and advantages and
disadvantages of cooperation and binary Network Coding (NC) are highlighted.
Our results show that the throughput increase introduced by NC is offset by a
loss of diversity and coding gain. It is shown that there is neither a coding
nor a diversity gain for the source node when the relays forward a
network-coded version of received and their own data. Compared to other results
available in the literature, the conclusion is that binary NC seems to be more
useful when the relay nodes act only on behalf of the source nodes, and do not
mix their own packets to the received ones. Analytical derivation and findings
are substantiated through extensive Monte Carlo simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2485</identifier>
 <datestamp>2012-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2485</id><created>2012-03-12</created><authors><author><keyname>Tamane</keyname><forenames>Sharvari C.</forenames></author><author><keyname>Deshmukh</keyname><forenames>Ratnadeep R.</forenames></author></authors><title>Blind 3D Model Watermarking Based on Multi-Resolution Representation and
  Fuzzy Logic</title><categories>cs.MM</categories><comments>9 pages, 4 tables</comments><journal-ref>International Journal of Computer Science &amp; Information Technology
  (IJCSIT) Vol 4, No 1, Feb 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Insertion of a text message, audio data or/and an image into another image or
3D model is called as a watermarking process. Watermarking has variety of
applications like: Copyright Protection, Owner Identification, Copy Protection
and Data Hiding etc., depending upon the type of watermark insertion algorithm.
Watermark remains in the content after applying various attacks without any
distortions. The blind watermarking method used in the system is based on a
wavelet transform, a fuzzy inference system and a multi-resolution
representation (MRR) of the 3d model. The watermark scrambled by Arnold
Transform is embedded in the wavelet coefficients at third resolution level of
the MRR. Fuzzy logic approach used in the method makes it to approximate the
best possible gain with an accurate scaling factor so that the watermark
remains invisible. The fuzzy input variables are computed for each wavelet
coefficient in the 3D model. The output of the fuzzy system is a single value
which is a perceptual value for each corresponding wavelet coefficient. Thus,
the fuzzy perceptual mask combines all these non-linear variables to build a
simple, easy to use HVS model. Results shows that the system is robust against
affine transformations, smoothing, cropping and noise attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2498</identifier>
 <datestamp>2013-06-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2498</id><created>2012-03-08</created><updated>2013-06-04</updated><authors><author><keyname>Bouslimi</keyname><forenames>Riadh</forenames></author><author><keyname>Amraoui</keyname><forenames>Houda</forenames></author></authors><title>Fault detection system for Arabic language</title><categories>cs.CL</categories><comments>4th International Workshop on ICTs and Amazigh, February 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The study of natural language, especially Arabic, and mechanisms for the
implementation of automatic processing is a fascinating field of study, with
various potential applications. The importance of tools for natural language
processing is materialized by the need to have applications that can
effectively treat the vast mass of information available nowadays on electronic
forms. Among these tools, mainly driven by the necessity of a fast writing in
alignment to the actual daily life speed, our interest is on the writing
auditors. The morphological and syntactic properties of Arabic make it a
difficult language to master, and explain the lack in the processing tools for
that language. Among these properties, we can mention: the complex structure of
the Arabic word, the agglutinative nature, lack of vocalization, the
segmentation of the text, the linguistic richness, etc.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2499</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2499</id><created>2012-03-12</created><updated>2013-01-19</updated><authors><author><keyname>Svoboda</keyname><forenames>Ladislav</forenames></author><author><keyname>Nov&#xe1;k</keyname><forenames>Jan</forenames></author><author><keyname>Kurilla</keyname><forenames>Luk&#xe1;&#x161;</forenames></author><author><keyname>Zeman</keyname><forenames>Jan</forenames></author></authors><title>A framework for integrated design of algorithmic architectural forms</title><categories>cs.CE</categories><comments>22 pages, 15 figures, v2: Substantially revised after the first
  review</comments><journal-ref>Advances in Engineering Software, 72, 109--118, (2014)</journal-ref><doi>10.1016/j.advengsoft.2013.05.006</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a methodology and software tools for parametric design of
complex architectural objects, called digital or algorithmic forms. In order to
provide a flexible tool, the proposed design philosophy involves two open
source utilities Donkey and MIDAS written in Grasshopper algorithm editor and
C++, respectively, that are to be linked with a scripting-based architectural
modellers Rhinoceros, IntelliCAD and the open source Finite Element solver
OOFEM. The emphasis is put on the mechanical response in order to provide
architects with a consistent learning framework and an insight into structural
behaviour of designed objects. As demonstrated on three case studies, the
proposed modular solution is capable of handling objects of considerable
structural complexity, thereby accelerating the process of finding procedural
design parameters from orders of weeks to days or hours.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2505</identifier>
 <datestamp>2012-03-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2505</id><created>2012-03-08</created><authors><author><keyname>Sensarma</keyname><forenames>Debajit</forenames></author><author><keyname>Banerjee</keyname><forenames>Subhashis</forenames></author><author><keyname>Basuli</keyname><forenames>Krishnendu</forenames></author><author><keyname>Naskar</keyname><forenames>Saptarshi</forenames></author><author><keyname>Sarma</keyname><forenames>Samar Sen</forenames></author></authors><title>On an optimization technique using Binary Decision Diagram</title><categories>cs.DS cs.LO</categories><comments>10 pages,5 figures,Sensarma D., Banerjee S., Basuli K., Naskar S., &amp;
  Sarma, S. S &quot;Minimizing Boolean Sum of Products Functions Using Binary
  Decision Diagram&quot;, Advances in Computer Science and Information Technology:
  Computer Science and Information Technology: Second International Conference.
  Proceedings, Vol. 86, Part III, pp 36-48, CCSIT 2012</comments><journal-ref>IJCSEA, Volume 2, Number 1, pg. 73-86, February 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two-level logic minimization is a central problem in logic synthesis, and has
applications in reliability analysis and automated reasoning. This paper
represents a method of minimizing Boolean sum of products function with binary
decision diagram and with disjoint sum of product minimization. Due to the
symbolic representation of cubes for large problem instances, the method is
orders of magnitude faster than previous enumerative techniques. But the
quality of the approach largely depends on the variable ordering of the
underlying BDD. The application of Binary Decision Diagrams (BDDs) as an
efficient approach for the minimization of Disjoint Sums-of-Products (DSOPs).
DSOPs are a starting point for several applications. The use of BDDs has the
advantage of an implicit representation of terms. Due to this scheme the
algorithm is faster than techniques working on explicit representations and the
application to large circuits that could not be handled so far becomes
possible. Theoretical studies on the influence of the BDDs to the search space
are carried out. In experiments the proposed technique is compared to others.
The results with respect to the size of the resulting DSOP are as good or
better as those of the other techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2506</identifier>
 <datestamp>2012-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2506</id><created>2012-03-12</created><authors><author><keyname>Cellatoglu</keyname><forenames>Akin</forenames></author><author><keyname>Karuppanan</keyname><forenames>Balasubramanian</forenames></author></authors><title>Vibrating Cantilever Transducer Incorporated in Dual Diaphragms
  Structure for Sensing Differential Pneumatic Pressure</title><categories>cs.SY</categories><comments>15 pages 11 figures AIAA conference Tirunelveli, 2011</comments><journal-ref>International Journal on Soft Computing ( IJSC ) Vol.2, No.4,
  November 2011, 95-109</journal-ref><doi>10.5121/ijsc.2011.2409</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pneumatic pressure cells with thin metallic spherical diaphragm of shallow
spherical shell configuration linked with vibrating wire pickup or vibrating
cantilever pickup were reported in the past. In order to enhance the
sensitivity of the pressure cell this work considers dual diaphragm structure
fitted with cantilever pickup. The design and development of the pressure cell
with this dual diaphragm structure having cantilever pickup is presented here.
The geometrical design is optimally made as to sense either mono pressure or
differential pressure resources. The cantilevers of the two diaphragms are
excited to produce vibrations and the frequencies of vibrations are determined
by picking up signals from orthogonally arranged opto-coupler links. With the
computed frequency a lookup table is referred to obtain the pressure acting on
the concerned diaphragm. In the external circuits, the average pressure and the
differential pressure acting on two diaphragms are computed. Furthermore
transmitting circuits taking the average pressure and differential pressure in
digital form and analogue form to remote area are presented. Performance
analysis of the proposed mechatronic pressure cell is made and its improved
performance over other pressure cells is presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2507</identifier>
 <datestamp>2012-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2507</id><created>2012-03-12</created><updated>2012-12-12</updated><authors><author><keyname>Dai</keyname><forenames>Dong</forenames></author><author><keyname>Rigollet</keyname><forenames>Philippe</forenames></author><author><keyname>Zhang</keyname><forenames>Tong</forenames></author></authors><title>Deviation optimal learning using greedy Q-aggregation</title><categories>math.ST cs.LG stat.ML stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/12-AOS1025 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS1025</report-no><journal-ref>Annals of Statistics 2012, Vol. 40, No. 3, 1878-1905</journal-ref><doi>10.1214/12-AOS1025</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a finite family of functions, the goal of model selection aggregation
is to construct a procedure that mimics the function from this family that is
the closest to an unknown regression function. More precisely, we consider a
general regression model with fixed design and measure the distance between
functions by the mean squared error at the design points. While procedures
based on exponential weights are known to solve the problem of model selection
aggregation in expectation, they are, surprisingly, sub-optimal in deviation.
We propose a new formulation called Q-aggregation that addresses this
limitation; namely, its solution leads to sharp oracle inequalities that are
optimal in a minimax sense. Moreover, based on the new formulation, we design
greedy Q-aggregation procedures that produce sparse aggregation models
achieving the optimal rate. The convergence and performance of these greedy
procedures are illustrated and compared with other standard methods on
simulated examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2508</identifier>
 <datestamp>2012-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2508</id><created>2012-03-12</created><authors><author><keyname>Cellatoglu</keyname><forenames>A.</forenames></author><author><keyname>Balasubramanian</keyname><forenames>K.</forenames></author></authors><title>Pneumatic Pressure Cell with Twin Diaphragms Embedding Spherical
  Corrugations in a Dual Diaphragm Structure</title><categories>cs.SY</categories><comments>6 pages 9 figures; IJCSI International Journal of Computer Science
  Issues, Special Issue, ICVCI-2011, Vol. 1, Issue 1, November 2011 ISSN
  (Online): 1694-0814</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Thin metallic shallow spherical diaphragms are being used for measuring
pneumatic pressure in process industries. The drift in vertex realized due to
application of pressure is transformed into electrical signal and this is
calibrated for pressure. We now propose a modified structure for the pressure
cell by having double ended shallow spherical shells embedded with spherical
corrugations as to enhance the sensitivity to a greater extent. By having dual
such installation in the structure of the pressure cell it concedes further
increase in sensitivity. The construction details of the diaphragm structure,
theory and analysis to assess the performance are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2509</identifier>
 <datestamp>2012-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2509</id><created>2012-03-12</created><authors><author><keyname>Pisier</keyname><forenames>Gilles</forenames></author></authors><title>Tripartite Bell inequality, random matrices and trilinear forms</title><categories>math.OA cs.IT math-ph math.FA math.IT math.MP math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this seminar report, we present in detail the proof of a recent result due
to J. Bri\&quot;et and T. Vidick, improving an estimate in a 2008 paper by D.
P\'erez-Garc\'{\i}a, M. Wolf, C. Palazuelos, I. Villanueva, and M. Junge,
estimating the growth of the deviation in the tripartite Bell inequality. The
proof requires a delicate estimate of the norms of certain trilinear (or
$d$-linear) forms on Hilbert space with coefficients in the second Gaussian
Wiener chaos. Let $E^n_{\vee}$ (resp. $E^n_{\min}$) denote $ \ell_1^n \otimes
\ell_1^n\otimes \ell_1^n$ equipped with the injective (resp. minimal) tensor
norm. Here $ \ell_1^n$ is equipped with its maximal operator space structure.
The Bri\&quot;et-Vidick method yields that the identity map $I_n$ satisfies (for
some $c&gt;0$) $\|I_n:\ E^n_{\vee}\to E^n_{\min}\|\ge c n^{1/4} (\log n)^{-3/2}.$
Let $S^n_2$ denote the (Hilbert) space of $n\times n$-matrices equipped with
the Hilbert-Schmidt norm. While a lower bound closer to $n^{1/2} $ is still
open, their method produces an interesting, asymptotically almost sharp,
related estimate for the map $J_n:\ S^n_2\stackrel{\vee}{\otimes}
S^n_2\stackrel{\vee}{\otimes}S^n_2 \to \ell_2^{n^3} \stackrel{\vee}{\otimes}
\ell_2^{n^3} $ taking $e_{i,j}\otimes e_{k,l}\otimes e_{m,n}$ to
$e_{[i,k,m],[j,l,n]}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2511</identifier>
 <datestamp>2012-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2511</id><created>2012-03-09</created><authors><author><keyname>Seal</keyname><forenames>Victor</forenames></author><author><keyname>Raha</keyname><forenames>Arnab</forenames></author><author><keyname>Maity</keyname><forenames>Shovan</forenames></author><author><keyname>Mitra</keyname><forenames>Souvik Kr</forenames></author><author><keyname>Mukherjee</keyname><forenames>Amitava</forenames></author><author><keyname>Naskar</keyname><forenames>Mrinal Kanti</forenames></author></authors><title>A Simple Flood Forecasting Scheme Using Wireless Sensor Networks</title><categories>cs.LG cs.CE cs.NI cs.SY stat.AP</categories><comments>16 pages, 4 figures, published in International Journal Of Ad-Hoc,
  Sensor And Ubiquitous Computing, February 2012; V. seal et al, 'A Simple
  Flood Forecasting Scheme Using Wireless Sensor Networks', IJASUC, Feb.2012</comments><doi>10.5121/ijasuc.2012.3105</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a forecasting model designed using WSNs (Wireless Sensor
Networks) to predict flood in rivers using simple and fast calculations to
provide real-time results and save the lives of people who may be affected by
the flood. Our prediction model uses multiple variable robust linear regression
which is easy to understand and simple and cost effective in implementation, is
speed efficient, but has low resource utilization and yet provides real time
predictions with reliable accuracy, thus having features which are desirable in
any real world algorithm. Our prediction model is independent of the number of
parameters, i.e. any number of parameters may be added or removed based on the
on-site requirements. When the water level rises, we represent it using a
polynomial whose nature is used to determine if the water level may exceed the
flood line in the near future. We compare our work with a contemporary
algorithm to demonstrate our improvements over it. Then we present our
simulation results for the predicted water level compared to the actual water
level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2514</identifier>
 <datestamp>2012-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2514</id><created>2012-03-09</created><authors><author><keyname>Sreedhar</keyname><forenames>K.</forenames></author><author><keyname>Panlal</keyname><forenames>B.</forenames></author></authors><title>Enhancement of Images using Morphological Transformation</title><categories>cs.CV</categories><comments>18 pages</comments><journal-ref>International Journal of Computer Science &amp; Information Technology
  (IJCSIT) Vol 4, No 1, Feb 2012, 33-50</journal-ref><doi>10.5121/ijcsit.2012.4103</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with enhancement of images with poor contrast and detection
of background. Proposes a frame work which is used to detect the background in
images characterized by poor contrast. Image enhancement has been carried out
by the two methods based on the Weber's law notion. The first method employs
information from image background analysis by blocks, while the second
transformation method utilizes the opening operation, closing operation, which
is employed to define the multi-background gray scale images. The complete
image processing is done using MATLAB simulation model. Finally, this paper is
organized as follows as Morphological transformation and Weber's law. Image
background approximation to the background by means of block analysis in
conjunction with transformations that enhance images with poor lighting. The
multibackground notion is introduced by means of the opening by reconstruction
shows a comparison among several techniques to improve contrast in images.
Finally, conclusions are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2516</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2516</id><created>2012-03-12</created><updated>2016-01-27</updated><authors><author><keyname>Hillerkuss</keyname><forenames>David</forenames><affiliation>Karlsruhe Institute of Technology</affiliation></author><author><keyname>Schmogrow</keyname><forenames>Rene</forenames><affiliation>Karlsruhe Institute of Technology</affiliation></author><author><keyname>Meyer</keyname><forenames>Matthias</forenames><affiliation>Karlsruhe Institute of Technology</affiliation></author><author><keyname>Wolf</keyname><forenames>Stefan</forenames><affiliation>Karlsruhe Institute of Technology</affiliation></author><author><keyname>Jordan</keyname><forenames>Meinert</forenames><affiliation>Karlsruhe Institute of Technology</affiliation></author><author><keyname>Kleinow</keyname><forenames>Philipp</forenames><affiliation>Karlsruhe Institute of Technology</affiliation></author><author><keyname>Lindenmann</keyname><forenames>Nicole</forenames><affiliation>Karlsruhe Institute of Technology</affiliation></author><author><keyname>Schindler</keyname><forenames>Philipp C.</forenames><affiliation>Karlsruhe Institute of Technology</affiliation></author><author><keyname>Melikyan</keyname><forenames>Argishti</forenames><affiliation>Karlsruhe Institute of Technology</affiliation></author><author><keyname>Yang</keyname><forenames>Xin</forenames><affiliation>Optoelectronics Research Centre - University of Southampton</affiliation></author><author><keyname>Ben-Ezra</keyname><forenames>Shalva</forenames><affiliation>Finisar Corporation</affiliation></author><author><keyname>Nebendahl</keyname><forenames>Bernd</forenames><affiliation>Agilent Technologies</affiliation></author><author><keyname>Dreschmann</keyname><forenames>Michael</forenames><affiliation>Karlsruhe Institute of Technology</affiliation></author><author><keyname>Meyer</keyname><forenames>Joachim</forenames><affiliation>Karlsruhe Institute of Technology</affiliation></author><author><keyname>Parmigiani</keyname><forenames>Francesca</forenames><affiliation>Optoelectronics Research Centre - University of Southampton</affiliation></author><author><keyname>Petropoulos</keyname><forenames>Periklis</forenames><affiliation>Optoelectronics Research Centre - University of Southampton</affiliation></author><author><keyname>Resan</keyname><forenames>Bojan</forenames><affiliation>Time-Bandwidth Products</affiliation></author><author><keyname>Oehler</keyname><forenames>Aandreas</forenames><affiliation>Time-Bandwidth Products</affiliation></author><author><keyname>Weingarten</keyname><forenames>Kurt</forenames><affiliation>Time-Bandwidth Products</affiliation></author><author><keyname>Altenhain</keyname><forenames>Lars</forenames><affiliation>Micram Microelectronic GmbH</affiliation></author><author><keyname>Ellermeyer</keyname><forenames>Tobias</forenames><affiliation>Micram Microelectronic GmbH</affiliation></author><author><keyname>Moeller</keyname><forenames>Matthias</forenames><affiliation>Department of Electronics and Circuits - Saarland University</affiliation></author><author><keyname>Huebner</keyname><forenames>Michael</forenames><affiliation>Karlsruhe Institute of Technology</affiliation></author><author><keyname>Becker</keyname><forenames>Juergen</forenames><affiliation>Karlsruhe Institute of Technology</affiliation></author><author><keyname>Koos</keyname><forenames>Christian</forenames><affiliation>Karlsruhe Institute of Technology</affiliation></author><author><keyname>Freude</keyname><forenames>Wolfgang</forenames><affiliation>Karlsruhe Institute of Technology</affiliation></author><author><keyname>Leuthold</keyname><forenames>Juerg</forenames><affiliation>Karlsruhe Institute of Technology</affiliation></author></authors><title>Single-laser 32.5 Tbit/s Nyquist WDM transmission</title><categories>cs.NI physics.optics</categories><comments>(c) 2012 Optical Society of America. One print or electronic copy may
  be made for personal use only. Systematic reproduction and distribution,
  duplication of any material in this paper for a fee or for commercial
  purposes, or modifications of the content of this paper are prohibited</comments><journal-ref>J. Opt. Commun. Netw. 4 (2012) 715-723</journal-ref><doi>10.1364/JOCN.4.000715</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We demonstrate 32.5 Tbit/s 16QAM Nyquist WDM transmission over a total length
of 227 km of SMF-28 without optical dispersion compensation. A number of 325
optical carriers are derived from a single laser and encoded with
dual-polarization 16QAM data using sinc-shaped Nyquist pulses. As we use no
guard bands, the carriers have a spacing of 12.5 GHz equal to the Nyquist
bandwidth of the data. We achieve a high net spectral efficiency of 6.4
bit/s/Hz using a software-defined transmitter which generates the electrical
modulator drive signals in real-time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2528</identifier>
 <datestamp>2012-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2528</id><created>2012-03-12</created><authors><author><keyname>Robinson</keyname><forenames>Michael</forenames></author></authors><title>Knowledge-based antenna pattern extrapolation</title><categories>cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a theoretically-motivated algorithm for extrapolation of antenna
radiation patterns from a small number of measurements. This algorithm exploits
constraints on the antenna's underlying design to avoid ambiguities, but is
sufficiently general to address many different antenna types. A theoretical
basis for the robustness of this algorithm is developed, and its performance is
verified in simulation using a number of popular antenna designs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2538</identifier>
 <datestamp>2013-06-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2538</id><created>2012-03-12</created><updated>2013-05-29</updated><authors><author><keyname>Meeks</keyname><forenames>Kitty</forenames></author><author><keyname>Scott</keyname><forenames>Alexander</forenames></author></authors><title>Spanning trees and the complexity of flood-filling games</title><categories>cs.DS</categories><comments>Final typos corrected</comments><doi>10.1007/s00224-013-9482-z</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider problems related to the combinatorial game (Free-)Flood-It, in
which players aim to make a coloured graph monochromatic with the minimum
possible number of flooding operations. We show that the minimum number of
moves required to flood any given graph G is equal to the minimum, taken over
all spanning trees T of G, of the number of moves required to flood T. This
result is then applied to give two polynomial-time algorithms for flood-filling
problems. Firstly, we can compute in polynomial time the minimum number of
moves required to flood a graph with only a polynomial number of connected
subgraphs. Secondly, given any coloured connected graph and a subset of the
vertices of bounded size, the number of moves required to connect this subset
can be computed in polynomial time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2543</identifier>
 <datestamp>2013-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2543</id><created>2012-03-12</created><updated>2013-04-02</updated><authors><author><keyname>Filho</keyname><forenames>H&#xe9;lio B. Mac&#xea;do</forenames></author><author><keyname>Dantas</keyname><forenames>Simone</forenames></author><author><keyname>Machado</keyname><forenames>Raphael C. S.</forenames></author><author><keyname>de Figueiredo</keyname><forenames>Celina M. H.</forenames></author></authors><title>Biclique-colouring verification complexity and biclique-colouring power
  graphs</title><categories>cs.DS</categories><comments>21 pages, 19 distinct figures. An extended abstract published in:
  Proceedings of Cologne Twente Workshop (CTW) 2012, pp. 134--138</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Biclique-colouring is a colouring of the vertices of a graph in such a way
that no maximal complete bipartite subgraph with at least one edge is
monochromatic. We show that it is coNP-complete to check whether a given
function that associates a colour to each vertex is a biclique-colouring, a
result that justifies the search for structured classes where the
biclique-colouring problem could be efficiently solved. We consider
biclique-colouring restricted to powers of paths and powers of cycles. We
determine the biclique-chromatic number of powers of paths and powers of
cycles. The biclique-chromatic number of a power of a path P_{n}^{k} is max(2k
+ 2 - n, 2) if n &gt;= k + 1 and exactly n otherwise. The biclique-chromatic
number of a power of a cycle C_n^k is at most 3 if n &gt;= 2k + 2 and exactly n
otherwise; we additionally determine the powers of cycles that are
2-biclique-colourable. All proofs are algorithmic and provide polynomial-time
biclique-colouring algorithms for graphs in the investigated classes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2550</identifier>
 <datestamp>2012-08-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2550</id><created>2012-03-12</created><updated>2012-08-26</updated><authors><author><keyname>Yang</keyname><forenames>Sheng</forenames></author><author><keyname>Kobayashi</keyname><forenames>Mari</forenames></author><author><keyname>Gesbert</keyname><forenames>David</forenames></author><author><keyname>Yi</keyname><forenames>Xinping</forenames></author></authors><title>Degrees of Freedom of Time Correlated MISO Broadcast Channel with
  Delayed CSIT</title><categories>cs.IT math.IT</categories><comments>revised and final version, to appear in IEEE transactions on
  Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the time correlated multiple-input single-output (MISO) broadcast
channel where the transmitter has imperfect knowledge on the current channel
state, in addition to delayed channel state information. By representing the
quality of the current channel state information as P^-{\alpha} for the
signal-to-noise ratio P and some constant {\alpha} \geq 0, we characterize the
optimal degree of freedom region for this more general two-user MISO broadcast
correlated channel. The essential ingredients of the proposed scheme lie in the
quantization and multicasting of the overheard interferences, while
broadcasting new private messages. Our proposed scheme smoothly bridges between
the scheme recently proposed by Maddah-Ali and Tse with no current state
information and a simple zero-forcing beamforming with perfect current state
information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2556</identifier>
 <datestamp>2012-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2556</id><created>2012-03-12</created><authors><author><keyname>Gupta</keyname><forenames>Neeraj</forenames></author><author><keyname>Shekhar</keyname><forenames>Rajiv</forenames></author><author><keyname>Kalra</keyname><forenames>Prem Kumar</forenames></author></authors><title>A Probabilistic Transmission Expansion Planning Methodology based on
  Roulette Wheel Selection and Social Welfare</title><categories>cs.AI cs.SY</categories><comments>22 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new probabilistic methodology for transmission expansion planning (TEP)
that does not require a priori specification of new/additional transmission
capacities and uses the concept of social welfare has been proposed. Two new
concepts have been introduced in this paper: (i) roulette wheel methodology has
been used to calculate the capacity of new transmission lines and (ii) load
flow analysis has been used to calculate expected demand not served (EDNS). The
overall methodology has been implemented on a modified IEEE 5-bus test system.
Simulations show an important result: addition of only new transmission lines
is not sufficient to minimize EDNS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2557</identifier>
 <datestamp>2012-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2557</id><created>2012-03-12</created><updated>2012-06-08</updated><authors><author><keyname>Helmbold</keyname><forenames>David P.</forenames></author><author><keyname>Long</keyname><forenames>Philip M.</forenames></author></authors><title>On the Necessity of Irrelevant Variables</title><categories>cs.LG</categories><comments>A preliminary version of this paper appeared in the proceedings of
  ICML'11</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work explores the effects of relevant and irrelevant boolean variables
on the accuracy of classifiers. The analysis uses the assumption that the
variables are conditionally independent given the class, and focuses on a
natural family of learning algorithms for such sources when the relevant
variables have a small advantage over random guessing. The main result is that
algorithms relying predominately on irrelevant variables have error
probabilities that quickly go to 0 in situations where algorithms that limit
the use of irrelevant variables have errors bounded below by a positive
constant. We also show that accurate learning is possible even when there are
so few examples that one cannot determine with high confidence whether or not
any individual variable is relevant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2563</identifier>
 <datestamp>2012-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2563</id><created>2012-03-12</created><authors><author><keyname>Cai</keyname><forenames>Kai</forenames></author><author><keyname>Ishii</keyname><forenames>Hideaki</forenames></author></authors><title>Average Consensus on General Strongly Connected Digraphs</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the average consensus problem of multi-agent systems for general
network topologies with unidirectional information flow. We propose two
(linear) distributed algorithms, deterministic and gossip, respectively for the
cases where the inter-agent communication is synchronous and asynchronous. Our
contribution is that in both cases, the developed algorithms guarantee state
averaging on arbitrary strongly connected digraphs; in particular, this
graphical condition does not require that the network be balanced or symmetric,
thereby extending many previous results in the literature. The key novelty of
our approach is to augment an additional variable for each agent, called
&quot;surplus&quot;, whose function is to locally record individual state updates. For
convergence analysis, we employ graph-theoretic and nonnegative matrix tools,
with the eigenvalue perturbation theory playing a crucial role.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2569</identifier>
 <datestamp>2012-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2569</id><created>2012-03-12</created><authors><author><keyname>Melucci</keyname><forenames>Massimo</forenames></author></authors><title>When Index Term Probability Violates the Classical Probability Axioms
  Quantum Probability can be a Necessary Theory for Information Retrieval</title><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Probabilistic models require the notion of event space for defining a
probability measure. An event space has a probability measure which ensues the
Kolmogorov axioms. However, the probabilities observed from distinct sources,
such as that of relevance of documents, may not admit a single event space thus
causing some issues. In this article, some results are introduced for ensuring
whether the observed prob- abilities of relevance of documents admit a single
event space. More- over, an alternative framework of probability is introduced,
thus chal- lenging the use of classical probability for ranking documents. Some
reflections on the convenience of extending the classical probabilis- tic
retrieval toward a more general framework which encompasses the issues are
made.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2570</identifier>
 <datestamp>2012-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2570</id><created>2012-03-12</created><authors><author><keyname>Hall</keyname><forenames>Rob</forenames></author><author><keyname>Rinaldo</keyname><forenames>Alessandro</forenames></author><author><keyname>Wasserman</keyname><forenames>Larry</forenames></author></authors><title>Differential Privacy for Functions and Functional Data</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Differential privacy is a framework for privately releasing summaries of a
database. Previous work has focused mainly on methods for which the output is a
finite dimensional vector, or an element of some discrete set. We develop
methods for releasing functions while preserving differential privacy.
Specifically, we show that adding an appropriate Gaussian process to the
function of interest yields differential privacy. When the functions lie in the
same RKHS as the Gaussian process, then the correct noise level is established
by measuring the &quot;sensitivity&quot; of the function in the RKHS norm. As examples we
consider kernel density estimation, kernel support vector machines, and
functions in reproducing kernel Hilbert spaces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2574</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2574</id><created>2012-03-12</created><updated>2012-03-14</updated><authors><author><keyname>Feng</keyname><forenames>Xixuan</forenames></author><author><keyname>Kumar</keyname><forenames>Arun</forenames></author><author><keyname>Recht</keyname><forenames>Ben</forenames></author><author><keyname>R&#xe9;</keyname><forenames>Christopher</forenames></author></authors><title>Towards a Unified Architecture for in-RDBMS Analytics</title><categories>cs.DB</categories><comments>Extended version of a SIGMOD 2012 full paper</comments><msc-class>97R50</msc-class><acm-class>H.2.m</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The increasing use of statistical data analysis in enterprise applications
has created an arms race among database vendors to offer ever more
sophisticated in-database analytics. One challenge in this race is that each
new statistical technique must be implemented from scratch in the RDBMS, which
leads to a lengthy and complex development process. We argue that the root
cause for this overhead is the lack of a unified architecture for in-database
analytics. Our main contribution in this work is to take a step towards such a
unified architecture. A key benefit of our unified architecture is that
performance optimizations for analytics techniques can be studied generically
instead of an ad hoc, per-technique fashion. In particular, our technical
contributions are theoretical and empirical studies of two key factors that we
found impact performance: the order data is stored, and parallelization of
computations on a single-node multicore RDBMS. We demonstrate the feasibility
of our architecture by integrating several popular analytics techniques into
two commercial and one open-source RDBMS. Our architecture requires changes to
only a few dozen lines of code to integrate a new statistical technique. We
then compare our approach with the native analytics tools offered by the
commercial RDBMSes on various analytics tasks, and validate that our approach
achieves competitive or higher performance, while still achieving the same
quality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2602</identifier>
 <datestamp>2012-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2602</id><created>2012-03-12</created><authors><author><keyname>Sly</keyname><forenames>Allan</forenames></author><author><keyname>Sun</keyname><forenames>Nike</forenames></author></authors><title>The computational hardness of counting in two-spin models on d-regular
  graphs</title><categories>math.PR cs.CC math-ph math.MP</categories><comments>23 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The class of two-spin systems contains several important models, including
random independent sets and the Ising model of statistical physics. We show
that for both the hard-core (independent set) model and the anti-ferromagnetic
Ising model with arbitrary external field, it is NP-hard to approximate the
partition function or approximately sample from the model on d-regular graphs
when the model has non-uniqueness on the d-regular tree. Together with results
of Jerrum--Sinclair, Weitz, and Sinclair--Srivastava--Thurley giving FPRAS's
for all other two-spin systems except at the uniqueness threshold, this gives
an almost complete classification of the computational complexity of two-spin
systems on bounded-degree graphs.
  Our proof establishes that the normalized log-partition function of any
two-spin system on bipartite locally tree-like graphs converges to a limiting
&quot;free energy density&quot; which coincides with the (non-rigorous) Bethe prediction
of statistical physics. We use this result to characterize the local structure
of two-spin systems on locally tree-like bipartite expander graphs, which then
become the basic gadgets in a randomized reduction to approximate MAX-CUT. Our
approach is novel in that it makes no use of the second moment method employed
in previous works on these questions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2655</identifier>
 <datestamp>2013-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2655</id><created>2012-03-12</created><authors><author><keyname>Liu</keyname><forenames>Yang-Yu</forenames></author><author><keyname>Slotine</keyname><forenames>Jean-Jacques</forenames></author><author><keyname>Barab&#xe1;si</keyname><forenames>Albert-L&#xe1;szl&#xf3;</forenames></author></authors><title>Control centrality and hierarchical structure in complex networks</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>14 pages, 3 figures</comments><journal-ref>PLoS ONE 7(9): e44459 (2012)</journal-ref><doi>10.1371/journal.pone.0044459</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the concept of control centrality to quantify the ability of a
single node to control a directed weighted network. We calculate the
distribution of control centrality for several real networks and find that it
is mainly determined by the network's degree distribution. We rigorously prove
that in a directed network without loops the control centrality of a node is
uniquely determined by its layer index or topological position in the
underlying hierarchical structure of the network. Inspired by the deep relation
between control centrality and hierarchical structure in a general directed
network, we design an efficient attack strategy against the controllability of
malicious networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2668</identifier>
 <datestamp>2012-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2668</id><created>2012-03-12</created><authors><author><keyname>Wang</keyname><forenames>Qiyan</forenames></author><author><keyname>Borisov</keyname><forenames>Nikita</forenames></author></authors><title>Octopus: A Secure and Anonymous DHT Lookup</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed Hash Table (DHT) lookup is a core technique in structured
peer-to-peer (P2P) networks. Its decentralized nature introduces security and
privacy vulnerabilities for applications built on top of them; we thus set out
to design a lookup mechanism achieving both security and anonymity, heretofore
an open problem. We present Octopus, a novel DHT lookup which provides strong
guarantees for both security and anonymity. Octopus uses attacker
identification mechanisms to discover and remove malicious nodes, severely
limiting an adversary's ability to carry out active attacks, and splits lookup
queries over separate anonymous paths and introduces dummy queries to achieve
high levels of anonymity. We analyze the security of Octopus by developing an
event-based simulator to show that the attacker discovery mechanisms can
rapidly identify malicious nodes with low error rate. We calculate the
anonymity of Octopus using probabilistic modeling and show that Octopus can
achieve near-optimal anonymity. We evaluate Octopus's efficiency on Planetlab
with 207 nodes and show that Octopus has reasonable lookup latency and
manageable communication overhead.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2670</identifier>
 <datestamp>2012-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2670</id><created>2012-03-12</created><authors><author><keyname>Fill</keyname><forenames>James Allen</forenames></author><author><keyname>Janson</keyname><forenames>Svante</forenames></author><author><keyname>Ward</keyname><forenames>Mark Daniel</forenames></author></authors><title>Partitions with Distinct Multiplicities of Parts: On An &quot;Unsolved
  Problem&quot; Posed By Herbert Wilf</title><categories>math.CO cs.DM</categories><comments>6 pages, 1 figure</comments><msc-class>05A16, 05A17, 68W40</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wilf's Sixth Unsolved Problem asks for any interesting properties of the set
of partitions of integers for which the (nonzero) multiplicities of the parts
are all different. We refer to these as \emph{Wilf partitions}. Using $f(n)$ to
denote the number of Wilf partitions, we establish lead-order asymptotics for
$\ln{f(n)}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2672</identifier>
 <datestamp>2012-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2672</id><created>2012-03-12</created><authors><author><keyname>Bakibayev</keyname><forenames>Nurzhan</forenames></author><author><keyname>Olteanu</keyname><forenames>Dan</forenames></author><author><keyname>Z&#xe1;vodn&#xfd;</keyname><forenames>Jakub</forenames></author></authors><title>FDB: A Query Engine for Factorised Relational Databases</title><categories>cs.DB cs.DS</categories><comments>12 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Factorised databases are relational databases that use compact factorised
representations at the physical layer to reduce data redundancy and boost query
performance. This paper introduces FDB, an in-memory query engine for
select-project-join queries on factorised databases. Key components of FDB are
novel algorithms for query optimisation and evaluation that exploit the
succinctness brought by data factorisation. Experiments show that for data sets
with many-to-many relationships FDB can outperform relational engines by orders
of magnitude.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2675</identifier>
 <datestamp>2012-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2675</id><created>2012-03-12</created><authors><author><keyname>Shi</keyname><forenames>Yaoyun</forenames></author></authors><title>Quantum Simpsons Paradox and High Order Bell-Tsirelson Inequalities</title><categories>quant-ph cs.IT math-ph math.IT math.MP math.ST stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The well-known Simpson's Paradox, or Yule-Simpson Effect, in statistics is
often illustrated by the following thought experiment: A drug may be found in a
trial to increase the survival rate for both men and women, but decrease the
rate for all the subjects as a whole. This paradoxical reversal effect has been
found in numerous datasets across many disciplines, and is now included in most
introductory statistics textbooks. In the language of the drug trial, the
effect is impossible, however, if both treatment groups' survival rates are
higher than both control groups'. Here we show that for quantum probabilities,
such a reversal remains possible. In particular, a &quot;quantum drug&quot;, so to speak,
could be life-saving for both men and women yet deadly for the whole
population. We further identify a simple inequality on conditional
probabilities that must hold classically but is violated by our quantum
scenarios, and completely characterize the maximum quantum violation. As
polynomial inequalities on entries of the density operator, our inequalities
are of degree 6.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2676</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2676</id><created>2012-03-12</created><authors><author><keyname>Petersen</keyname><forenames>Ian R.</forenames></author><author><keyname>Ugrinovskii</keyname><forenames>Valery</forenames></author><author><keyname>James</keyname><forenames>Matthew R.</forenames></author></authors><title>Robust Stability of Uncertain Quantum Systems</title><categories>quant-ph cs.SY math.OC</categories><comments>A shortened version will appear in the Proceedings of the 2012
  American Control Conference</comments><doi>10.1098/rsta.2011.0527</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the problem of robust stability for a class of uncertain
quantum systems subject to unknown perturbations in the system Hamiltonian.
Some general stability results are given for different classes of perturbations
to the system Hamiltonian. Then, the special case of a nominal linear quantum
system is considered with either quadratic or non-quadratic perturbations to
the system Hamiltonian. In this case, robust stability conditions are given in
terms of strict bounded real conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2690</identifier>
 <datestamp>2012-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2690</id><created>2012-03-12</created><authors><author><keyname>Strohmer</keyname><forenames>Thomas</forenames></author><author><keyname>Friedlander</keyname><forenames>Benjamin</forenames></author></authors><title>Analysis of Sparse MIMO Radar</title><categories>cs.IT math.IT math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a multiple-input-multiple-output radar system and derive a
theoretical framework for the recoverability of targets in the azimuth-range
domain and the azimuth-range-Doppler domain via sparse approximation
algorithms. Using tools developed in the area of compressive sensing, we prove
bounds on the number of detectable targets and the achievable resolution in the
presence of additive noise. Our theoretical findings are validated by numerical
simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2704</identifier>
 <datestamp>2012-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2704</id><created>2012-03-12</created><authors><author><keyname>Coetzee</keyname><forenames>Derrick</forenames></author><author><keyname>Bhaskar</keyname><forenames>Anand</forenames></author><author><keyname>Necula</keyname><forenames>George</forenames></author></authors><title>A model and framework for reliable build systems</title><categories>cs.SE</categories><report-no>UCB/EECS-2012-27</report-no><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Reliable and fast builds are essential for rapid turnaround during
development and testing. Popular existing build systems rely on correct manual
specification of build dependencies, which can lead to invalid build outputs
and nondeterminism. We outline the challenges of developing reliable build
systems and explore the design space for their implementation, with a focus on
non-distributed, incremental, parallel build systems. We define a general model
for resources accessed by build tasks and show its correspondence to the
implementation technique of minimum information libraries, APIs that return no
information that the application doesn't plan to use. We also summarize
preliminary experimental results from several prototype build managers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2721</identifier>
 <datestamp>2012-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2721</id><created>2012-03-13</created><authors><author><keyname>Song</keyname><forenames>Guanghui</forenames></author><author><keyname>Tsujii</keyname><forenames>Yuta</forenames></author><author><keyname>Cheng</keyname><forenames>Jun</forenames></author><author><keyname>Watanabe</keyname><forenames>Yoichiro</forenames></author></authors><title>Analysis of Finite Field Spreading for Multiple-Access Channel</title><categories>cs.IT math.IT</categories><comments>18 pages, 5 figures, submitted to IEEE Transactions on Information
  Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Finite field spreading scheme is proposed for a synchronous multiple-access
channel with Gaussian noise and equal-power users. For each user, $s$
information bits are spread \emph{jointly} into a length-$sL$ vector by $L$
multiplications on GF($2^s$). Thus, each information bit is dispersed into $sL$
transmitted symbols, and the finite field despreading (FF-DES) of each bit can
take advantage of $sL$ independent receiving observations. To show the
performance gain of joint spreading quantitatively, an extrinsic information
transfer (EXIT) function analysis of the FF-DES is given. It shows that the
asymptotic slope of this EXIT function increases as $s$ increases and is in
fact the absolute slope of the bit error rate (BER) curve at the low BER
region. This means that by increasing the length $s$ of information bits for
joint spreading, a larger absolute slope of the BER curve is achieved. For $s,
L\geq 2$, the BER curve of the finite field spreading has a larger absolute
slope than that of the single-user transmission with BPSK modulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2725</identifier>
 <datestamp>2012-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2725</id><created>2012-03-13</created><authors><author><keyname>Lin</keyname><forenames>Henry</forenames></author><author><keyname>Schalekamp</keyname><forenames>Frans</forenames></author></authors><title>On the Complexity of the Minimum Latency Scheduling Problem on the
  Euclidean Plane</title><categories>cs.NI cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show NP-hardness of the minimum latency scheduling (MLS) problem under the
physical model of wireless networking. In this model a transmission is received
successfully if the Signal to Interference-plus-Noise Ratio (SINR), is above a
given threshold. In the minimum latency scheduling problem, the goal is to
assign a time slot and power level to each transmission, so that all the
messages are received successfully, and the number of distinct times slots is
minimized.
  Despite its seeming simplicity and several previous hardness results for
various settings of the minimum latency scheduling problem, it has remained an
open question whether or not the minimum latency scheduling problem is NP-hard,
when the nodes are placed in the Euclidean plane and arbitrary power levels can
be chosen for the transmissions. We resolve this open question for all path
loss exponent values $\alpha \geq 3$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2730</identifier>
 <datestamp>2012-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2730</id><created>2012-03-13</created><authors><author><keyname>Javaid</keyname><forenames>N.</forenames></author><author><keyname>Bibi</keyname><forenames>A.</forenames></author><author><keyname>Khan</keyname><forenames>Z. A.</forenames></author><author><keyname>Djouani</keyname><forenames>K.</forenames></author></authors><title>On using Multiple Quality Link Metrics with Destination Sequenced
  Distance Vector Protocol for Wireless Multi-Hop Networks</title><categories>cs.NI</categories><journal-ref>25th IEEE CCECE 2012, Montreal, Canada</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we compare and analyze performance of five quality link
metrics forWireless Multi-hop Networks (WMhNs). The metrics are based on loss
probability measurements; ETX, ETT, InvETX, ML and MD, in a distance vector
routing protocol; DSDV. Among these selected metrics, we have implemented ML,
MD, InvETX and ETT in DSDV which are previously implemented with different
protocols; ML, MD, InvETX are implemented with OLSR, while ETT is implemented
in MR-LQSR. For our comparison, we have selected Throughput, Normalized Routing
Load (NRL) and End-to-End Delay (E2ED) as performance parameters. Finally, we
deduce that InvETX due to low computational burden and link asymmetry
measurement outperforms among all metrics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2738</identifier>
 <datestamp>2012-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2738</id><created>2012-03-13</created><authors><author><keyname>Javaid</keyname><forenames>N.</forenames></author><author><keyname>Bibi</keyname><forenames>A.</forenames></author><author><keyname>Dridi</keyname><forenames>K.</forenames></author><author><keyname>Khan</keyname><forenames>Z. A.</forenames></author><author><keyname>Bouk</keyname><forenames>S. H.</forenames></author></authors><title>Modeling and Evaluating Enhancements in Expanding Ring Search Algorithm
  for Wireless Reactive Protocols</title><categories>cs.NI</categories><comments>25th IEEE CCECE, 2012, Montreal Canada</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In case of high dynamic topology, reactive routing protocols provide quick
convergence by faster route discoveries and route maintenance. Frequent
roadcasts reduce routing efficiency in terms of broadcast cost; Bk, and
expected time cost; E[t]. These costs are optimized using different mechanisms.
So, we select three reactive routing protocols; Ad-hoc On-demand Distance
Vector (AODV), Dynamic Source Routing (DSR), and DYnamic Manet On-demad (DYMO).
We model expanding Ring Search (ERS); an optimization mechanism in the selected
protocols to reduce Bk and E[t]. A novel contribution of this work is
enhancement of default ERS in the protocols to optimize Bk and E[t]. Using
NS-2, we evaluate and compare default-ERS used by these protocols; AODV-ERS1,
DSR-ERS1 and DYMO-ERS1 with enhanced-ERS; AODVERS2, DSR-ERS2 and DYMO-ERS2.
From modeling and analytical comparison, we deduce that by adjusting
Time-To-Live (T TL) value of a network, efficient optimizations of Bk and E[t]
can be achieved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2739</identifier>
 <datestamp>2013-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2739</id><created>2012-03-13</created><authors><author><keyname>Akbudak</keyname><forenames>Kadir</forenames></author><author><keyname>Kayaaslan</keyname><forenames>Enver</forenames></author><author><keyname>Aykanat</keyname><forenames>Cevdet</forenames></author></authors><title>Analyzing and enhancing OSKI for sparse matrix-vector multiplication</title><categories>cs.NA</categories><comments>arXiv admin note: substantial text overlap with arXiv:1202.3856</comments><report-no>BU-CE-1201</report-no><journal-ref>SIAM J. Sci. Comput., 35(3), C237-C262. 2013 (26 pages)</journal-ref><doi>10.1137/100813956</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sparse matrix-vector multiplication (SpMxV) is a kernel operation widely used
in iterative linear solvers. The same sparse matrix is multiplied by a dense
vector repeatedly in these solvers. Matrices with irregular sparsity patterns
make it difficult to utilize cache locality effectively in SpMxV computations.
In this work, we investigate single- and multiple-SpMxV frameworks for
exploiting cache locality in SpMxV computations. For the single-SpMxV
framework, we propose two cache-size-aware top-down row/column-reordering
methods based on 1D and 2D sparse matrix partitioning by utilizing the
column-net and enhancing the row-column-net hypergraph models of sparse
matrices. The multiple-SpMxV framework depends on splitting a given matrix into
a sum of multiple nonzero-disjoint matrices so that the SpMxV operation is
performed as a sequence of multiple input- and output-dependent SpMxV
operations. For an effective matrix splitting required in this framework, we
propose a cache-size-aware top-down approach based on 2D sparse matrix
partitioning by utilizing the row-column-net hypergraph model. The primary
objective in all of the three methods is to maximize the exploitation of
temporal locality. We evaluate the validity of our models and methods on a wide
range of sparse matrices by performing actual runs through using OSKI.
Experimental results show that proposed methods and models outperform
state-of-the-art schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2742</identifier>
 <datestamp>2012-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2742</id><created>2012-03-13</created><authors><author><keyname>Andersen</keyname><forenames>Martin S.</forenames></author><author><keyname>Dahl</keyname><forenames>Joachim</forenames></author><author><keyname>Vandenberghe</keyname><forenames>Lieven</forenames></author></authors><title>Logarithmic barriers for sparse matrix cones</title><categories>math.OC cs.NA</categories><msc-class>90C22, 65F05</msc-class><doi>10.1080/10556788.2012.684353</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Algorithms are presented for evaluating gradients and Hessians of logarithmic
barrier functions for two types of convex cones: the cone of positive
semidefinite matrices with a given sparsity pattern, and its dual cone, the
cone of sparse matrices with the same pattern that have a positive semidefinite
completion. Efficient large-scale algorithms for evaluating these barriers and
their derivatives are important in interior-point methods for nonsymmetric
conic formulations of sparse semidefinite programs. The algorithms are based on
the multifrontal method for sparse Cholesky factorization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2760</identifier>
 <datestamp>2014-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2760</id><created>2012-03-13</created><updated>2012-07-04</updated><authors><author><keyname>Andr&#xe1;s</keyname><forenames>Szil&#xe1;rd</forenames></author><author><keyname>Baricz</keyname><forenames>&#xc1;rp&#xe1;d</forenames></author></authors><title>New approximations for DQPSK transmission bit error rate</title><categories>cs.IT math.CA math.IT</categories><comments>12 pages, 4 figures, 3 tables</comments><journal-ref>Proceedings of IEEE International Symposium on Applied
  Computational Intelligence and Informatics, May 23-25, Timi\c{s}oara (2013)
  73-77</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this correspondence our aim is to use some tight lower and upper bounds
for the differential quaternary phase shift keying transmission bit error rate
in order to deduce accurate approximations for the bit error rate by improving
the known results in the literature. The computation of our new approximate
expressions are significantly simpler than that of the exact expression.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2768</identifier>
 <datestamp>2012-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2768</id><created>2012-03-13</created><authors><author><keyname>Montorsi</keyname><forenames>Francesco</forenames></author><author><keyname>Vitetta</keyname><forenames>Giorgio M.</forenames></author></authors><title>On the Performance Limits of Pilot-Based Estimation of Bandlimited
  Frequency-Selective Communication Channels</title><categories>cs.IT cs.NI math.IT</categories><journal-ref>Montorsi, F.; Vitetta, G.; &quot;On the Performance Limits of
  Pilot-Based Estimation of Bandlimited Frequency-Selective Communication
  Channels,&quot; Communications, IEEE Transactions on, vol. 59, no. 11, pp.
  2964-2969, Nov. 2011</journal-ref><doi>10.1109/TCOMM.2011.070511.100269</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper the problem of assessing bounds on the accuracy of pilot-based
estimation of a bandlimited frequency selective communication channel is
tackled. Mean square error is taken as a figure of merit in channel estimation
and a tapped-delay line model is adopted to represent a continuous time channel
via a finite number of unknown parameters. This allows to derive some
properties of optimal waveforms for channel sounding and closed form Cramer-Rao
bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2769</identifier>
 <datestamp>2012-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2769</id><created>2012-03-13</created><authors><author><keyname>Peleg</keyname><forenames>Tomer</forenames></author><author><keyname>Elad</keyname><forenames>Michael</forenames></author></authors><title>Performance Guarantees of the Thresholding Algorithm for the Co-Sparse
  Analysis Model</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Trans. on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The co-sparse analysis model for signals assumes that the signal of interest
can be multiplied by an analysis dictionary \Omega, leading to a sparse
outcome. This model stands as an interesting alternative to the more classical
synthesis based sparse representation model. In this work we propose a
theoretical study of the performance guarantee of the thresholding algorithm
for the pursuit problem in the presence of noise. Our analysis reveals two
significant properties of \Omega, which govern the pursuit performance: The
first is the degree of linear dependencies between sets of rows in \Omega,
depicted by the co-sparsity level. The second property, termed the Restricted
Orthogonal Projection Property (ROPP), is the level of independence between
such dependent sets and other rows in \Omega. We show how these dictionary
properties are meaningful and useful, both in the theoretical bounds derived,
and in a series of experiments that are shown to align well with the
theoretical prediction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2778</identifier>
 <datestamp>2012-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2778</id><created>2012-03-13</created><authors><author><keyname>Tameja</keyname><forenames>Inder Jeet</forenames></author></authors><title>Seven Means, Generalized Triangular Discrimination, and Generating
  Divergence Measures</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  From geometrical point of view, Eve (2003) studied seven means. These means
are Harmonic, Geometric, Arithmetic, Heronian, Contra-harmonic, Root-mean
square and Centroidal mean. We have considered for the first time a new measure
calling generalized triangular discrimination. Inequalities among non-negative
differences arising due to seven means and particular cases of generalized
triangular discrimination are considered. Some new generating measures and
their exponential representations are also presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2801</identifier>
 <datestamp>2012-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2801</id><created>2012-03-13</created><authors><author><keyname>Kim</keyname><forenames>Eun Jung</forenames></author><author><keyname>Goncalves</keyname><forenames>Daniel</forenames></author></authors><title>On Exact Algorithms for Permutation CSP</title><categories>cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the Permutation Constraint Satisfaction Problem (Permutation CSP) we are
given a set of variables $V$ and a set of constraints C, in which constraints
are tuples of elements of V. The goal is to find a total ordering of the
variables, $\pi\ : V \rightarrow [1,...,|V|]$, which satisfies as many
constraints as possible. A constraint $(v_1,v_2,...,v_k)$ is satisfied by an
ordering $\pi$ when $\pi(v_1)&lt;\pi(v_2)&lt;...&lt;\pi(v_k)$. An instance has arity $k$
if all the constraints involve at most $k$ elements.
  This problem expresses a variety of permutation problems including {\sc
Feedback Arc Set} and {\sc Betweenness} problems. A naive algorithm, listing
all the $n!$ permutations, requires $2^{O(n\log{n})}$ time. Interestingly, {\sc
Permutation CSP} for arity 2 or 3 can be solved by Held-Karp type algorithms in
time $O^*(2^n)$, but no algorithm is known for arity at least 4 with running
time significantly better than $2^{O(n\log{n})}$. In this paper we resolve the
gap by showing that {\sc Arity 4 Permutation CSP} cannot be solved in time
$2^{o(n\log{n})}$ unless ETH fails.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2809</identifier>
 <datestamp>2012-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2809</id><created>2012-03-11</created><authors><author><keyname>Chevalier</keyname><forenames>Yannick</forenames><affiliation>IRIT</affiliation></author><author><keyname>Kourjieh</keyname><forenames>Mounira</forenames><affiliation>INRIA Lorraine - LORIA / LIFC</affiliation></author></authors><title>Automated Synthesis of a Finite Complexity Ordering for Saturation</title><categories>cs.LO</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present in this paper a new procedure to saturate a set of clauses with
respect to a well-founded ordering on ground atoms such that A &lt; B implies
Var(A) {\subseteq} Var(B) for every atoms A and B. This condition is satisfied
by any atom ordering compatible with a lexicographic, recursive, or multiset
path ordering on terms. Our saturation procedure is based on a priori ordered
resolution and its main novelty is the on-the-fly construction of a finite
complexity atom ordering. In contrast with the usual redundancy, we give a new
redundancy notion and we prove that during the saturation a non-redundant
inference by a priori ordered resolution is also an inference by a posteriori
ordered resolution. We also prove that if a set S of clauses is saturated with
respect to an atom ordering as described above then the problem of whether a
clause C is entailed from S is decidable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2816</identifier>
 <datestamp>2012-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2816</id><created>2012-03-13</created><authors><author><keyname>Sebesta</keyname><forenames>Kenneth</forenames></author><author><keyname>Baillieul</keyname><forenames>John</forenames></author></authors><title>Animal-Inspired Agile Flight Using Optical Flow Sensing</title><categories>cs.SY</categories><comments>20 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is evidence that flying animals such as pigeons, goshawks, and bats use
optical flow sensing to enable high-speed flight through forest clutter. This
paper discusses the elements of a theory of controlled flight through obstacle
fields in which motion control laws are based on optical flow sensing.
Performance comparison is made with feedback laws that use distance and bearing
measurements, and practical challenges of implementation on an actual robotic
air vehicle are described. The related question of fundamental performance
limits due to clutter density is addressed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2821</identifier>
 <datestamp>2012-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2821</id><created>2012-03-13</created><authors><author><keyname>Soufiani</keyname><forenames>Hossein Azari</forenames></author><author><keyname>Airoldi</keyname><forenames>Edoardo M</forenames></author></authors><title>Graphlet decomposition of a weighted network</title><categories>stat.ME cs.LG cs.SI physics.soc-ph</categories><comments>25 pages, 4 figures, 3 tables</comments><journal-ref>Journal of Machine Learning Research, Workshop &amp; Conference
  Proceedings, vol. 22 (AISTATS), 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the graphlet decomposition of a weighted network, which encodes
a notion of social information based on social structure. We develop a scalable
inference algorithm, which combines EM with Bron-Kerbosch in a novel fashion,
for estimating the parameters of the model underlying graphlets using one
network sample. We explore some theoretical properties of the graphlet
decomposition, including computational complexity, redundancy and expected
accuracy. We demonstrate graphlets on synthetic and real data. We analyze
messaging patterns on Facebook and criminal associations in the 19th century.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2822</identifier>
 <datestamp>2014-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2822</id><created>2012-03-13</created><updated>2014-12-12</updated><authors><author><keyname>Kisielewicz</keyname><forenames>Andrzej</forenames></author><author><keyname>Kowalski</keyname><forenames>Jakub</forenames></author><author><keyname>Szyku&#x142;a</keyname><forenames>Marek</forenames></author></authors><title>A Fast Algorithm Finding the Shortest Reset Words</title><categories>cs.FL cs.DS</categories><comments>COCOON 2013. The final publication is available at
  http://link.springer.com/chapter/10.1007%2F978-3-642-38768-5_18</comments><journal-ref>In Computing and Combinatorics, volume 7936 of LNCS, pages
  182-196, 2013</journal-ref><doi>10.1007/978-3-642-38768-5_18</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a new fast algorithm finding minimal reset words for
finite synchronizing automata. The problem is know to be computationally hard,
and our algorithm is exponential. Yet, it is faster than the algorithms used so
far and it works well in practice. The main idea is to use a bidirectional BFS
and radix (Patricia) tries to store and compare resulted subsets. We give both
theoretical and practical arguments showing that the branching factor is
reduced efficiently. As a practical test we perform an experimental study of
the length of the shortest reset word for random automata with $n$ states and 2
input letters. We follow Skvorsov and Tipikin, who have performed such a study
using a SAT solver and considering automata up to $n=100$ states. With our
algorithm we are able to consider much larger sample of automata with up to
$n=300$ states. In particular, we obtain a new more precise estimation of the
expected length of the shortest reset word $\approx 2.5\sqrt{n-5}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2824</identifier>
 <datestamp>2012-03-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2824</id><created>2012-03-13</created><updated>2012-03-13</updated><authors><author><keyname>Heimann</keyname><forenames>C. F. Larry</forenames></author><author><keyname>Nochenson</keyname><forenames>Alan</forenames></author></authors><title>Identifying Tipping Points in a Decision-Theoretic Model of Network
  Security</title><categories>cs.CR</categories><comments>10 pages, 4 figures. Pre-print</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although system administrators are frequently urged to protect the machines
in their network, the fact remains that the decision to protect is far from
universal. To better understand this decision, we formulate a
decision-theoretic model of a system administrator responsible for a network of
size n against an attacker attempting to penetrate the network and infect the
machines with a virus or similar exploit. By analyzing the model we are able to
demonstrate the cost sensitivity of smaller networks as well as identify
tipping points that can lead the administrator to switch away from the decision
to protect.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2832</identifier>
 <datestamp>2012-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2832</id><created>2012-03-13</created><authors><author><keyname>Lehrer</keyname><forenames>Ehud</forenames></author><author><keyname>Scarsini</keyname><forenames>Marco</forenames></author></authors><title>On the Core of Dynamic Cooperative Games</title><categories>cs.GT</categories><comments>25 pages</comments><msc-class>91A12</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider dynamic cooperative games, where the worth of coalitions varies
over time according to the history of allocations. When defining the core of a
dynamic game, we allow the possibility for coalitions to deviate at any time
and thereby to give rise to a new environment. A coalition that considers a
deviation needs to take the consequences into account because from the
deviation point on, the game is no longer played with the original set of
players. The deviating coalition becomes the new grand coalition which, in
turn, induces a new dynamic game. The stage games of the new dynamical game
depend on all previous allocation including those that have materialized from
the deviating time on.
  We define three types of core solutions: fair core, stable core and credible
core. We characterize the first two in case where the instantaneous game
depends on the last allocation (rather than on the whole history of
allocations) and the third in the general case. The analysis and the results
resembles to a great extent the theory of non-cooperative dynamic games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2834</identifier>
 <datestamp>2014-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2834</id><created>2012-03-13</created><authors><author><keyname>Li</keyname><forenames>Bin</forenames></author><author><keyname>Eryilmaz</keyname><forenames>Atilla</forenames></author></authors><title>A Fast-CSMA Algorithm for Deadline-Constrained Scheduling over Wireless
  Fading Channels</title><categories>cs.NI</categories><comments>This work appears in workshop on Resource Allocation and Cooperation
  in Wireless Networks (RAWNET), Princeton, NJ, May, 2011</comments><journal-ref>The journal version of this paper is published in IEEE
  Transactions on Wireless Communications, 12(7): 3278-3288, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, low-complexity and distributed Carrier Sense Multiple Access
(CSMA)-based scheduling algorithms have attracted extensive interest due to
their throughput-optimal characteristics in general network topologies.
However, these algorithms are not well-suited for serving real-time traffic
under time-varying channel conditions for two reasons: (1) the mixing time of
the underlying CSMA Markov Chain grows with the size of the network, which, for
large networks, generates unacceptable delay for deadline-constrained traffic;
(2) since the dynamic CSMA parameters are influenced by the arrival and channel
state processes, the underlying CSMA Markov Chain may not converge to a
steady-state under strict deadline constraints and fading channel conditions.
  In this paper, we attack the problem of distributed scheduling for serving
real-time traffic over time-varying channels. Specifically, we consider
fully-connected topologies with independently fading channels (which can model
cellular networks) in which flows with short-term deadline constraints and
long-term drop rate requirements are served. To that end, we first characterize
the maximal set of satisfiable arrival processes for this system and, then,
propose a Fast-CSMA (FCSMA) policy that is shown to be optimal in supporting
any real-time traffic that is within the maximal satisfiable set. These
theoretical results are further validated through simulations to demonstrate
the relative efficiency of the FCSMA policy compared to some of the existing
CSMA-based algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2835</identifier>
 <datestamp>2012-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2835</id><created>2012-03-13</created><authors><author><keyname>Montorsi</keyname><forenames>Francesco</forenames></author><author><keyname>Pancaldi</keyname><forenames>Fabrizio</forenames></author><author><keyname>Vitetta</keyname><forenames>Giorgio M.</forenames></author></authors><title>Statistical Characterization and Mitigation of NLOS Errors in UWB
  Localization Systems</title><categories>cs.IT math.IT stat.AP</categories><journal-ref>Montorsi, F.; Pancaldi, F.; Vitetta, G.; &quot;Statistical
  Characterization and Mitigation of NLOS Errors in UWB Localization Systems,&quot;
  Ultra-Wideband (ICUWB), 2011 IEEE International Conference on, pp. 86-90,
  14-16 Sept. 2011</journal-ref><doi>10.1109/ICUWB.2011.6058928</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper some new experimental results about the statistical
characterization of the non-line-of-sight (NLOS) bias affecting time-of-arrival
(TOA) estimation in ultrawideband (UWB) wireless localization systems are
illustrated. Then, these results are exploited to assess the performance of
various maximum-likelihood (ML) based algorithms for joint TOA localization and
NLOS bias mitigation. Our numerical results evidence that the accuracy of all
the considered algorithms is appreciably influenced by the LOS/NLOS conditions
of the propagation environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2839</identifier>
 <datestamp>2012-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2839</id><created>2012-03-13</created><authors><author><keyname>Egger</keyname><forenames>Jan</forenames></author><author><keyname>Kapur</keyname><forenames>Tina</forenames></author><author><keyname>Dukatz</keyname><forenames>Thomas</forenames></author><author><keyname>Kolodziej</keyname><forenames>Malgorzata</forenames></author><author><keyname>Zukic</keyname><forenames>Dzenan</forenames></author><author><keyname>Freisleben</keyname><forenames>Bernd</forenames></author><author><keyname>Nimsky</keyname><forenames>Christopher</forenames></author></authors><title>Square-Cut: A Segmentation Algorithm on the Basis of a Rectangle Shape</title><categories>cs.CV</categories><comments>13 pages, 17 figures, 2 tables, 3 equations, 42 references</comments><journal-ref>Egger J, Kapur T, Dukatz T, Kolodziej M, Zukic D, et al. (2012)
  Square-Cut: A Segmentation Algorithm on the Basis of a Rectangle Shape. PLoS
  ONE 7(2): e31064</journal-ref><doi>10.1371/journal.pone.0031064</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a rectangle-based segmentation algorithm that sets up a graph and
performs a graph cut to separate an object from the background. However,
graph-based algorithms distribute the graph's nodes uniformly and equidistantly
on the image. Then, a smoothness term is added to force the cut to prefer a
particular shape. This strategy does not allow the cut to prefer a certain
structure, especially when areas of the object are indistinguishable from the
background. We solve this problem by referring to a rectangle shape of the
object when sampling the graph nodes, i.e., the nodes are distributed
nonuniformly and non-equidistantly on the image. This strategy can be useful,
when areas of the object are indistinguishable from the background. For
evaluation, we focus on vertebrae images from Magnetic Resonance Imaging (MRI)
datasets to support the time consuming manual slice-by-slice segmentation
performed by physicians. The ground truth of the vertebrae boundaries were
manually extracted by two clinical experts (neurological surgeons) with several
years of experience in spine surgery and afterwards compared with the automatic
segmentation results of the proposed scheme yielding an average Dice Similarity
Coefficient (DSC) of 90.97\pm62.2%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2841</identifier>
 <datestamp>2012-08-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2841</id><created>2012-03-13</created><updated>2012-08-14</updated><authors><author><keyname>Kim</keyname><forenames>MinJi</forenames></author><author><keyname>Klein</keyname><forenames>Thierry</forenames></author><author><keyname>Soljanin</keyname><forenames>Emina</forenames></author><author><keyname>Barros</keyname><forenames>Joao</forenames></author><author><keyname>Medard</keyname><forenames>Muriel</forenames></author></authors><title>Trade-off between cost and goodput in wireless: Replacing transmitters
  with coding</title><categories>cs.NI</categories><comments>5 pages, 7 figures, submitted to IEEE International Conference on
  Communications (ICC)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the cost of improving the goodput, or the useful data rate, to user
in a wireless network. We measure the cost in terms of number of base stations,
which is highly correlated to the energy cost as well as capital and
operational costs of a network provider.We show that increasing the available
bandwidth, or throughput, may not necessarily lead to increase in goodput,
particularly in lossy wireless networks in which TCP does not perform well. As
a result, much of the resources dedicated to the user may not translate to high
goodput, resulting in an inefficient use of the network resources. We show that
using protocols such as TCP/NC, which are more resilient to erasures and
failures in the network, may lead to a goodput commensurate the throughput
dedicated to each user. By increasing goodput, users' transactions are
completed faster; thus, the resources dedicated to these users can be released
to serve other requests or transactions. Consequently, we show that translating
efficiently throughput to goodput may bring forth better connection to users
while reducing the cost for the network providers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2851</identifier>
 <datestamp>2012-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2851</id><created>2012-03-13</created><authors><author><keyname>Adamatzky</keyname><forenames>Andrew</forenames></author><author><keyname>Akl</keyname><forenames>Selim</forenames></author><author><keyname>Alonso-Sanz</keyname><forenames>Ramon</forenames></author><author><keyname>van Dessel</keyname><forenames>Wesley</forenames></author><author><keyname>Ibrahim</keyname><forenames>Zuwairie</forenames></author><author><keyname>Ilachinski</keyname><forenames>Andrew</forenames></author><author><keyname>Jones</keyname><forenames>Jeff</forenames></author><author><keyname>Kayem</keyname><forenames>Anne V. D. M.</forenames></author><author><keyname>Martinez</keyname><forenames>Genaro J.</forenames></author><author><keyname>de Oliveira</keyname><forenames>Pedro</forenames></author><author><keyname>Prokopenko</keyname><forenames>Mikhail</forenames></author><author><keyname>Schubert</keyname><forenames>Theresa</forenames></author><author><keyname>Sloot</keyname><forenames>Peter</forenames></author><author><keyname>Strano</keyname><forenames>Emanuele</forenames></author><author><keyname>Yang</keyname><forenames>Xin-She</forenames></author></authors><title>Are motorways rational from slime mould's point of view?</title><categories>nlin.PS cs.ET</categories><doi>10.1080/17445760.2012.685884</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyse the results of our experimental laboratory approximation of
motorways networks with slime mould Physarum polycephalum. Motorway networks of
fourteen geographical areas are considered: Australia, Africa, Belgium, Brazil,
Canada, China, Germany, Iberia, Italy, Malaysia, Mexico, The Netherlands, UK,
USA. For each geographical entity we represented major urban areas by oat
flakes and inoculated the slime mould in a capital. After slime mould spanned
all urban areas with a network of its protoplasmic tubes we extracted a
generalised Physarum graph from the network and compared the graphs with an
abstract motorway graph using most common measures. The measures employed are
the number of independent cycles, cohesion, shortest paths lengths, diameter,
the Harary index and the Randic index. We obtained a series of intriguing
results, and found that the slime mould approximates best of all the motorway
graphs of Belgium, Canada and China, and that for all entities studied the best
match between Physarum and motorway graphs is detected by the Randic index
(molecular branching index).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2860</identifier>
 <datestamp>2012-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2860</id><created>2012-03-13</created><authors><author><keyname>Ding</keyname><forenames>Xuchu</forenames></author><author><keyname>Lazar</keyname><forenames>Mircea</forenames></author><author><keyname>Belta</keyname><forenames>Calin</forenames></author></authors><title>Receding Horizon Temporal Logic Control for Finite Deterministic Systems</title><categories>math.OC cs.SY</categories><comments>Technical report accompanying a paper to be presented at ACC 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers receding horizon control of finite deterministic
systems, which must satisfy a high level, rich specification expressed as a
linear temporal logic formula. Under the assumption that time-varying rewards
are associated with states of the system and they can be observed in real-time,
the control objective is to maximize the collected reward while satisfying the
high level task specification. In order to properly react to the changing
rewards, a controller synthesis framework inspired by model predictive control
is proposed, where the rewards are locally optimized at each time-step over a
finite horizon, and the immediate optimal control is applied. By enforcing
appropriate constraints, the infinite trajectory produced by the controller is
guaranteed to satisfy the desired temporal logic formula. Simulation results
demonstrate the effectiveness of the approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2870</identifier>
 <datestamp>2012-09-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2870</id><created>2012-03-13</created><updated>2012-09-19</updated><authors><author><keyname>Cocco</keyname><forenames>Giuseppe</forenames></author><author><keyname>G&#xfc;nd&#xfc;z</keyname><forenames>Deniz</forenames></author><author><keyname>Ibars</keyname><forenames>Christian</forenames></author></authors><title>Streaming Transmitter over Block-Fading Channels with Delay Constraint</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data streaming transmission over a block fading channel is studied. It is
assumed that the transmitter receives a new message at each channel block at a
constant rate, which is fixed by an underlying application, and tries to
deliver the arriving messages by a common deadline. Various transmission
schemes are proposed and compared with an informed transmitter upper bound in
terms of the average decoded rate. It is shown that in the single receiver case
the adaptive joint encoding (aJE) scheme is asymptotically optimal, in that it
achieves the ergodic capacity as the transmission deadline goes to infinity;
and it closely follows the performance of the informed transmitter upper bound
in the case of finite transmission deadline. On the other hand, in the presence
of multiple receivers with different signal-to-noise ratios (SNR), memoryless
transmission (MT), time sharing (TS) and superposition transmission (ST)
schemes are shown to be more robust than the joint encoding (JE) scheme as they
have gradual performance loss with decreasing SNR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2873</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2873</id><created>2012-03-13</created><updated>2012-06-21</updated><authors><author><keyname>Brzozowski</keyname><forenames>Janusz</forenames></author><author><keyname>Liu</keyname><forenames>David</forenames></author></authors><title>Syntactic Complexity of Finite/Cofinite, Definite, and Reverse Definite
  Languages</title><categories>cs.FL</categories><comments>10 pages. An error concerning the size of the alphabet has been
  corrected in Theorem 3</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the syntactic complexity of finite/cofinite, definite and reverse
definite languages. The syntactic complexity of a class of languages is defined
as the maximal size of syntactic semigroups of languages from the class, taken
as a function of the state complexity n of the languages. We prove that (n-1)!
is a tight upper bound for finite/cofinite languages and that it can be reached
only if the alphabet size is greater than or equal to (n-1)!-(n-2)!. We prove
that the bound is also (n-1)! for reverse definite languages, but the minimal
alphabet size is (n-1)!-2(n-2)!. We show that \lfloor e\cdot (n-1)!\rfloor is a
lower bound on the syntactic complexity of definite languages, and conjecture
that this is also an upper bound, and that the alphabet size required to meet
this bound is \floor{e \cdot (n-1)!} - \floor{e \cdot (n-2)!}. We prove the
conjecture for n\le 4.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2886</identifier>
 <datestamp>2012-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2886</id><created>2012-03-13</created><authors><author><keyname>Atre</keyname><forenames>Medha</forenames></author><author><keyname>Chaoji</keyname><forenames>Vineet</forenames></author><author><keyname>Zaki</keyname><forenames>Mohammed J.</forenames></author></authors><title>BitPath -- Label Order Constrained Reachability Queries over Large
  Graphs</title><categories>cs.DB cs.DS</categories><report-no>RPI-CS 12-02</report-no><acm-class>H.2.4; E.1; E.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we focus on the following constrained reachability problem over
edge-labeled graphs like RDF -- &quot;given source node x, destination node y, and a
sequence of edge labels (a, b, c, d), is there a path between the two nodes
such that the edge labels on the path satisfy a regular expression
&quot;*a.*b.*c.*d.*&quot;. A &quot;*&quot; before &quot;a&quot; allows any other edge label to appear on the
path before edge &quot;a&quot;. &quot;a.*&quot; forces at least one edge with label &quot;a&quot;. &quot;.*&quot; after
&quot;a&quot; allows zero or more edge labels after &quot;a&quot; and before &quot;b&quot;. Our query
processing algorithm uses simple divide-and-conquer and greedy pruning
procedures to limit the search space. However, our graph indexing technique --
based on &quot;compressed bit-vectors&quot; -- allows indexing large graphs which
otherwise would have been infeasible. We have evaluated our approach on graphs
with more than 22 million edges and 6 million nodes -- much larger compared to
the datasets used in the contemporary work on path queries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2888</identifier>
 <datestamp>2012-03-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2888</id><created>2012-03-13</created><updated>2012-03-21</updated><authors><author><keyname>Grochow</keyname><forenames>Joshua A.</forenames></author><author><keyname>Rusek</keyname><forenames>Korben</forenames></author></authors><title>Report on &quot;Mathematical Aspects of P vs. NP and its Variants.&quot;</title><categories>cs.CC math.AG math.NT math.RT</categories><comments>Modified Open Problem 10 and a few small edits</comments><msc-class>68Q15, 68Q17, 11C99, 14M17, 14M27, 20G05, 20C15</msc-class><acm-class>F.1.3; F.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is a report on a workshop held August 1 to August 5, 2011 at the
Institute for Computational and Experimental Research in Mathematics (ICERM) at
Brown University, Providence, Rhode Island, organized by Saugata Basu, Joseph
M. Landsberg, and J. Maurice Rojas. We provide overviews of the more recent
results presented at the workshop, including some works-in-progress as well as
tentative and intriguing ideas for new directions. The main themes we discuss
are representation theory and geometry in the Mulmuley-Sohoni Geometric
Complexity Theory Program, and number theory and other ideas in the
Blum-Shub-Smale model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2890</identifier>
 <datestamp>2012-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2890</id><created>2012-03-13</created><authors><author><keyname>Montorsi</keyname><forenames>Francesco</forenames></author><author><keyname>Pancaldi</keyname><forenames>Fabrizio</forenames></author><author><keyname>Vitetta</keyname><forenames>Giorgio M.</forenames></author></authors><title>Statistical Characterization and Mitigation of NLOS Bias in UWB
  Localization Systems</title><categories>cs.IT math.IT stat.AP</categories><journal-ref>Montorsi, F.; Pancaldi, F.; Vitetta, G.; &quot;Statistical
  Characterization and Mitigation of NLOS Bias in UWB Localization Systems,&quot;
  Advances in Electronics and Telecommunications, pp. 11-17, Issue no 4, ISSN
  2081-8580</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Propagation in non-line-of-sight (NLOS) conditions is one of the major
impairments in ultrawideband (UWB) wireless localization systems based on
time-of-arrival (TOA) measurements. In this paper the problem of the joint
statistical characterization of the NLOS bias and of the most representative
features of LOS/NLOS UWB waveforms is investigated. In addition, the
performance of various maximum-likelihood (ML) estimators for joint
localization and NLOS bias mitigation is assessed. Our numerical results
evidence that the accuracy of all the considered estimators is appreciably
influenced by the LOS/NLOS conditions of the propagation environment and that a
statistical knowledge of multiple signal features can be exploited to mitigate
the NLOS bias, reducing the overall localization error.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2900</identifier>
 <datestamp>2012-03-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2900</id><created>2012-03-13</created><authors><author><keyname>Dumas</keyname><forenames>Jean-Guillaume</forenames><affiliation>LJK</affiliation></author><author><keyname>Duval</keyname><forenames>Dominique</forenames><affiliation>LJK</affiliation></author><author><keyname>Fousse</keyname><forenames>Laurent</forenames><affiliation>LJK</affiliation></author><author><keyname>Reynaud</keyname><forenames>Jean-Claude</forenames><affiliation>RC</affiliation></author></authors><title>Decorated proofs for computational effects: Exceptions</title><categories>cs.LO math.CT</categories><comments>11 pages</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We define a proof system for exceptions which is close to the syntax for
exceptions, in the sense that the exceptions do not appear explicitly in the
type of any expression. This proof system is sound with respect to the intended
denotational semantics of exceptions. With this inference system we prove
several properties of exceptions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2936</identifier>
 <datestamp>2012-05-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2936</id><created>2012-03-13</created><updated>2012-05-09</updated><authors><author><keyname>Kyrillidis</keyname><forenames>Anastasios</forenames></author><author><keyname>Cevher</keyname><forenames>Volkan</forenames></author></authors><title>Combinatorial Selection and Least Absolute Shrinkage via the CLASH
  Algorithm</title><categories>cs.IT math.IT</categories><comments>12 pages, Submitted to ISIT 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The least absolute shrinkage and selection operator (LASSO) for linear
regression exploits the geometric interplay of the $\ell_2$-data error
objective and the $\ell_1$-norm constraint to arbitrarily select sparse models.
Guiding this uninformed selection process with sparsity models has been
precisely the center of attention over the last decade in order to improve
learning performance. To this end, we alter the selection process of LASSO to
explicitly leverage combinatorial sparsity models (CSMs) via the combinatorial
selection and least absolute shrinkage (CLASH) operator. We provide concrete
guidelines how to leverage combinatorial constraints within CLASH, and
characterize CLASH's guarantees as a function of the set restricted isometry
constants of the sensing matrix. Finally, our experimental results show that
CLASH can outperform both LASSO and model-based compressive sensing in sparse
estimation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2946</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2946</id><created>2012-03-13</created><updated>2014-04-25</updated><authors><author><keyname>Koza</keyname><forenames>Zbigniew</forenames></author><author><keyname>Matyka</keyname><forenames>Maciej</forenames></author><author><keyname>Szkoda</keyname><forenames>Sebastian</forenames></author><author><keyname>Miros&#x142;aw</keyname><forenames>&#x141;ukasz</forenames></author></authors><title>Compressed Multi-Row Storage Format for Sparse Matrices on Graphics
  Processing Units</title><categories>physics.comp-ph cs.DC</categories><journal-ref>SIAM J. Sci. Comput. 36-2 (2014), pp. C219-C239</journal-ref><doi>10.1137/120900216</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new format for storing sparse matrices is proposed for efficient sparse
matrix-vector (SpMV) product calculation on modern graphics processing units
(GPUs). This format extends the standard compressed row storage (CRS) format
and can be quickly converted to and from it. Computational performance of two
SpMV kernels for the new format is determined for over 130 sparse matrices on
Fermi-class and Kepler-class GPUs and compared with that of five existing
generic algorithms and industrial implementations, including Nvidia cuSparse
CSR and HYB kernels. We found the speedup of up to $\approx 60%$ over the best
of the five alternative kernels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2970</identifier>
 <datestamp>2014-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2970</id><created>2012-03-13</created><updated>2014-02-18</updated><authors><author><keyname>Serrano</keyname><forenames>Pablo</forenames></author><author><keyname>Patras</keyname><forenames>Paul</forenames></author><author><keyname>Mannocci</keyname><forenames>Andrea</forenames></author><author><keyname>Mancuso</keyname><forenames>Vincenzo</forenames></author><author><keyname>Banchs</keyname><forenames>Albert</forenames></author></authors><title>Control Theoretic Optimization of 802.11 WLANs: Implementation and
  Experimental Evaluation</title><categories>cs.NI</categories><comments>23 pages, 16 figures</comments><journal-ref>Computer Networks, vol. 57, no. 1, Jan. 2013</journal-ref><doi>10.1016/j.comnet.2012.09.010</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In 802.11 WLANs, adapting the contention parameters to network conditions
results in substantial performance improvements. Even though the ability to
change these parameters has been available in standard devices for years, so
far no adaptive mechanism using this functionality has been validated in a
realistic deployment. In this paper we report our experiences with implementing
and evaluating two adaptive algorithms based on control theory, one centralized
and one distributed, in a large-scale testbed consisting of 18 commercial
off-the-shelf devices. We conduct extensive measurements, considering different
network conditions in terms of number of active nodes, link qualities and
traffic generated. We show that both algorithms significantly outperform the
standard configuration in terms of total throughput. We also identify the
limitations inherent in distributed schemes, and demonstrate that the
centralized approach substantially improves performance under a large variety
of scenarios, which confirms its suitability for real deployments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2973</identifier>
 <datestamp>2012-03-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2973</id><created>2012-03-13</created><authors><author><keyname>Bindel</keyname><forenames>David</forenames></author><author><keyname>Kleinberg</keyname><forenames>Jon</forenames></author><author><keyname>Oren</keyname><forenames>Sigal</forenames></author></authors><title>How Bad is Forming Your Own Opinion?</title><categories>cs.GT physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The question of how people form their opinion has fascinated economists and
sociologists for quite some time. In many of the models, a group of people in a
social network, each holding a numerical opinion, arrive at a shared opinion
through repeated averaging with their neighbors in the network. Motivated by
the observation that consensus is rarely reached in real opinion dynamics, we
study a related sociological model in which individuals' intrinsic beliefs
counterbalance the averaging process and yield a diversity of opinions.
  By interpreting the repeated averaging as best-response dynamics in an
underlying game with natural payoffs, and the limit of the process as an
equilibrium, we are able to study the cost of disagreement in these models
relative to a social optimum. We provide a tight bound on the cost at
equilibrium relative to the optimum; our analysis draws a connection between
these agreement models and extremal problems that lead to generalized
eigenvalues. We also consider a natural network design problem in this setting:
which links can we add to the underlying network to reduce the cost of
disagreement at equilibrium?
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2982</identifier>
 <datestamp>2012-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2982</id><created>2012-03-13</created><authors><author><keyname>Zeng</keyname><forenames>An</forenames></author><author><keyname>Liu</keyname><forenames>Weiping</forenames></author></authors><title>Enhancing network robustness for malicious attacks</title><categories>physics.soc-ph cs.SI physics.comp-ph</categories><comments>6 pages, 6 figures</comments><journal-ref>Physical Review E 85, 066130 (2012)</journal-ref><doi>10.1103/PhysRevE.85.066130</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a recent work [Proc. Natl. Acad. Sci. USA 108, 3838 (2011)], the authors
proposed a simple measure for network robustness under malicious attacks on
nodes. With a greedy algorithm, they found the optimal structure with respect
to this quantity is an onion structure in which high-degree nodes form a core
surrounded by rings of nodes with decreasing degree. However, in real networks
the failure can also occur in links such as dysfunctional power cables and
blocked airlines. Accordingly, complementary to the node-robustness measurement
($R_{n}$), we propose a link-robustness index ($R_{l}$). We show that solely
enhancing $R_{n}$ cannot guarantee the improvement of $R_{l}$. Moreover, the
structure of $R_{l}$-optimized network is found to be entirely different from
that of onion network. In order to design robust networks resistant to more
realistic attack condition, we propose a hybrid greedy algorithm which takes
both the $R_{n}$ and $R_{l}$ into account. We validate the robustness of our
generated networks against malicious attacks mixed with both nodes and links
failure. Finally, some economical constraints for swapping the links in real
networks are considered and significant improvement in both aspects of
robustness are still achieved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2987</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2987</id><created>2012-03-13</created><authors><author><keyname>Yadav</keyname><forenames>Surjeet Kumar</forenames></author><author><keyname>Bharadwaj</keyname><forenames>Brijesh</forenames></author><author><keyname>Pal</keyname><forenames>Saurabh</forenames></author></authors><title>Mining Education Data to Predict Student's Retention: A comparative
  Study</title><categories>cs.LG cs.DB</categories><comments>5 pages. arXiv admin note: substantial text overlap with
  arXiv:1202.4815</comments><journal-ref>(IJCSIS) International Journal of Computer Science and Information
  Security, Vol. 10, No. 2, 2012, pp113-117</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main objective of higher education is to provide quality education to
students. One way to achieve highest level of quality in higher education
system is by discovering knowledge for prediction regarding enrolment of
students in a course. This paper presents a data mining project to generate
predictive models for student retention management. Given new records of
incoming students, these predictive models can produce short accurate
prediction lists identifying students who tend to need the support from the
student retention program most. This paper examines the quality of the
predictive models generated by the machine learning algorithms. The results
show that some of the machines learning algorithms are able to establish
effective predictive models from the existing student retention data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2990</identifier>
 <datestamp>2012-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2990</id><created>2012-03-13</created><updated>2012-11-29</updated><authors><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author></authors><title>Evolving Culture vs Local Minima</title><categories>cs.LG cs.AI</categories><acm-class>I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a theory that relates difficulty of learning in deep architectures
to culture and language. It is articulated around the following hypotheses: (1)
learning in an individual human brain is hampered by the presence of effective
local minima; (2) this optimization difficulty is particularly important when
it comes to learning higher-level abstractions, i.e., concepts that cover a
vast and highly-nonlinear span of sensory configurations; (3) such high-level
abstractions are best represented in brains by the composition of many levels
of representation, i.e., by deep architectures; (4) a human brain can learn
such high-level abstractions if guided by the signals produced by other humans,
which act as hints or indirect supervision for these high-level abstractions;
and (5), language and the recombination and optimization of mental concepts
provide an efficient evolutionary recombination operator, and this gives rise
to rapid search in the space of communicable ideas that help humans build up
better high-level internal representations of their world. These hypotheses put
together imply that human culture and the evolution of ideas have been crucial
to counter an optimization difficulty: this optimization difficulty would
otherwise make it very difficult for human brains to capture high-level
knowledge of the world. The theory is grounded in experimental observations of
the difficulties of training deep artificial neural networks. Plausible
consequences of this theory for the efficiency of cultural evolutions are
sketched.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2992</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2992</id><created>2012-03-13</created><authors><author><keyname>Williams</keyname><forenames>Jason L.</forenames></author></authors><title>Hybrid Poisson and multi-Bernoulli filters</title><categories>cs.SY cs.CV</categories><comments>Submitted to 15th International Conference on Information Fusion
  (2012)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The probability hypothesis density (PHD) and multi-target multi-Bernoulli
(MeMBer) filters are two leading algorithms that have emerged from random
finite sets (RFS). In this paper we study a method which combines these two
approaches. Our work is motivated by a sister paper, which proves that the full
Bayes RFS filter naturally incorporates a Poisson component representing
targets that have never been detected, and a linear combination of
multi-Bernoulli components representing targets under track. Here we
demonstrate the benefit (in speed of track initiation) that maintenance of a
Poisson component of undetected targets provides. Subsequently, we propose a
method of recycling, which projects Bernoulli components with a low probability
of existence onto the Poisson component (as opposed to deleting them). We show
that this allows us to achieve similar tracking performance using a fraction of
the number of Bernoulli components (i.e., tracks).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2995</identifier>
 <datestamp>2015-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2995</id><created>2012-03-13</created><updated>2015-04-30</updated><authors><author><keyname>Williams</keyname><forenames>Jason L.</forenames></author></authors><title>Marginal multi-Bernoulli filters: RFS derivation of MHT, JIPDA and
  association-based MeMBer</title><categories>cs.SY cs.CV</categories><comments>Accepted for publication, IEEE Transactions on Aerospace and
  Electronic Systems, April 2015. Matlab code of simple implementation included
  with ancillary files</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent developments in random finite sets (RFSs) have yielded a variety of
tracking methods that avoid data association. This paper derives a form of the
full Bayes RFS filter and observes that data association is implicitly present,
in a data structure similar to MHT. Subsequently, algorithms are obtained by
approximating the distribution of associations. Two algorithms result: one
nearly identical to JIPDA, and another related to the MeMBer filter. Both
improve performance in challenging environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.2999</identifier>
 <datestamp>2012-03-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.2999</id><created>2012-03-14</created><authors><author><keyname>Chasta</keyname><forenames>Neeraj K.</forenames></author></authors><title>High Speed, Low Power Current Comparators with Hysteresis</title><categories>cs.OH</categories><journal-ref>International Journal of VLSI design &amp; Communication Systems
  (VLSICS) Vol.3, No.1, February 2012, 85-96</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper, presents a novel idea for analog current comparison which
compares input signal current and reference currents with high speed, low power
and well controlled hysteresis. Proposed circuit is based on current mirror and
voltage latching techniques which produces rail to rail output voltage as a
result of current comparison. The same design can be extended to a simple
current comparator without hysteresis (or very less hysteresis), where
comparator gives high accuracy (less than 50nA) and speed at the cost of
moderate power consumption. The comparators are designed optimally and studied
at 180nm CMOS process technology for a supply voltage of 3V.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3002</identifier>
 <datestamp>2012-03-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3002</id><created>2012-03-14</created><authors><author><keyname>Xiao</keyname><forenames>Lin</forenames></author><author><keyname>Zhang</keyname><forenames>Tong</forenames></author></authors><title>A Proximal-Gradient Homotopy Method for the Sparse Least-Squares Problem</title><categories>math.OC cs.IT math.IT stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider solving the $\ell_1$-regularized least-squares ($\ell_1$-LS)
problem in the context of sparse recovery, for applications such as compressed
sensing. The standard proximal gradient method, also known as iterative
soft-thresholding when applied to this problem, has low computational cost per
iteration but a rather slow convergence rate. Nevertheless, when the solution
is sparse, it often exhibits fast linear convergence in the final stage. We
exploit the local linear convergence using a homotopy continuation strategy,
i.e., we solve the $\ell_1$-LS problem for a sequence of decreasing values of
the regularization parameter, and use an approximate solution at the end of
each stage to warm start the next stage. Although similar strategies have been
studied in the literature, there have been no theoretical analysis of their
global iteration complexity. This paper shows that under suitable assumptions
for sparse recovery, the proposed homotopy strategy ensures that all iterates
along the homotopy solution path are sparse. Therefore the objective function
is effectively strongly convex along the solution path, and geometric
convergence at each stage can be established. As a result, the overall
iteration complexity of our method is $O(\log(1/\epsilon))$ for finding an
$\epsilon$-optimal solution, which can be interpreted as global geometric rate
of convergence. We also present empirical results to support our theoretical
analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3013</identifier>
 <datestamp>2012-03-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3013</id><created>2012-03-14</created><authors><author><keyname>Bertier</keyname><forenames>Marin</forenames><affiliation>INRIA - IRISA</affiliation></author><author><keyname>Obrovac</keyname><forenames>Marko</forenames><affiliation>INRIA - IRISA</affiliation></author><author><keyname>Tedeschi</keyname><forenames>C&#xe9;dric</forenames><affiliation>INRIA - IRISA</affiliation></author></authors><title>A Protocol for the Atomic Capture of Multiple Molecules at Large Scale</title><categories>cs.DC</categories><comments>13th International Conference on Distributed Computing and Networking
  (2012)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the rise of service-oriented computing, applications are more and more
based on coordination of autonomous services. Envisioned over largely
distributed and highly dynamic platforms, expressing this coordination calls
for alternative programming models. The chemical programming paradigm, which
models applications as chemical solutions where molecules representing digital
entities involved in the computation, react together to produce a result, has
been recently shown to provide the needed abstractions for autonomic
coordination of services. However, the execution of such programs over large
scale platforms raises several problems hindering this paradigm to be actually
leveraged. Among them, the atomic capture of molecules participating in concur-
rent reactions is one of the most significant. In this paper, we propose a
protocol for the atomic capture of these molecules distributed and evolving
over a large scale platform. As the density of possible reactions is crucial
for the liveness and efficiency of such a capture, the protocol proposed is
made up of two sub-protocols, each of them aimed at addressing different levels
of densities of potential reactions in the solution. While the decision to
choose one or the other is local to each node participating in a program's
execution, a global coherent behaviour is obtained. Proof of liveness, as well
as intensive simulation results showing the efficiency and limited overhead of
the protocol are given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3023</identifier>
 <datestamp>2012-03-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3023</id><created>2012-03-14</created><authors><author><keyname>Boulares</keyname><forenames>Mehrez</forenames></author><author><keyname>Jemni</keyname><forenames>Mohamed</forenames></author></authors><title>Toward an example-based machine translation from written text to ASL
  using virtual agent animation</title><categories>cs.CL</categories><comments>10pages, 11 figures; ISSN (Online): 1694-0814</comments><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 9,
  Issue 1, No 1, January 2012</journal-ref><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Modern computational linguistic software cannot produce important aspects of
sign language translation. Using some researches we deduce that the majority of
automatic sign language translation systems ignore many aspects when they
generate animation; therefore the interpretation lost the truth information
meaning. Our goals are: to translate written text from any language to ASL
animation; to model maximum raw information using machine learning and
computational techniques; and to produce a more adapted and expressive form to
natural looking and understandable ASL animations. Our methods include
linguistic annotation of initial text and semantic orientation to generate the
facial expression. We use the genetic algorithms coupled to learning/recognized
systems to produce the most natural form. To detect emotion we are based on
fuzzy logic to produce the degree of interpolation between facial expressions.
Roughly, we present a new expressive language Text Adapted Sign Modeling
Language TASML that describes all maximum aspects related to a natural sign
language interpretation. This paper is organized as follow: the next section is
devoted to present the comprehension effect of using Space/Time/SVO form in ASL
animation based on experimentation. In section 3, we describe our technical
considerations. We present the general approach we adopted to develop our tool
in section 4. Finally, we give some perspectives and future works.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3037</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3037</id><created>2012-03-14</created><updated>2012-09-24</updated><authors><author><keyname>Stramaglia</keyname><forenames>S.</forenames></author><author><keyname>Wu</keyname><forenames>Guo-Rong</forenames></author><author><keyname>Pellicoro</keyname><forenames>M.</forenames></author><author><keyname>Marinazzo</keyname><forenames>D.</forenames></author></authors><title>Expanding the Transfer Entropy to Identify Information Subgraphs in
  Complex Systems</title><categories>q-bio.QM cs.IT math.IT physics.data-an</categories><doi>10.1103/PhysRevE.86.066211</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a formal expansion of the transfer entropy to put in evidence
irreducible sets of variables which provide information for the future state of
each assigned target. Multiplets characterized by a large contribution to the
expansion are associated to informational circuits present in the system, with
an informational character which can be associated to the sign of the
contribution. For the sake of computational complexity, we adopt the assumption
of Gaussianity and use the corresponding exact formula for the conditional
mutual information. We report the application of the proposed methodology on
two EEG data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3051</identifier>
 <datestamp>2012-03-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3051</id><created>2012-03-14</created><authors><author><keyname>Narodytska</keyname><forenames>Nina</forenames></author><author><keyname>Walsh</keyname><forenames>Toby</forenames></author><author><keyname>Xia</keyname><forenames>Lirong</forenames></author></authors><title>Combining Voting Rules Together</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a simple method for combining together voting rules that performs
a run-off between the different winners of each voting rule. We prove that this
combinator has several good properties. For instance, even if just one of the
base voting rules has a desirable property like Condorcet consistency, the
combination inherits this property. In addition, we prove that combining voting
rules together in this way can make finding a manipulation more computationally
difficult. Finally, we study the impact of this combinator on approximation
methods that find close to optimal manipulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3055</identifier>
 <datestamp>2013-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3055</id><created>2012-03-14</created><updated>2012-12-28</updated><authors><author><keyname>Sanchez</keyname><forenames>David Garcia</forenames></author><author><keyname>Lacarri&#xe8;re</keyname><forenames>Bruno</forenames></author><author><keyname>Musy</keyname><forenames>Marjorie</forenames></author><author><keyname>Bourges</keyname><forenames>Bernard</forenames></author></authors><title>Application of sensitivity analysis in building energy simulations:
  combining first and second order elementary effects Methods</title><categories>cs.CE stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sensitivity analysis plays an important role in the understanding of complex
models. It helps to identify influence of input parameters in relation to the
outputs. It can be also a tool to understand the behavior of the model and then
can help in its development stage. This study aims to analyze and illustrate
the potential usefulness of combining first and second-order sensitivity
analysis, applied to a building energy model (ESP-r). Through the example of a
collective building, a sensitivity analysis is performed using the method of
elementary effects (also known as Morris method), including an analysis of
interactions between the input parameters (second order analysis). Importance
of higher-order analysis to better support the results of first order analysis,
highlighted especially in such complex model. Several aspects are tackled to
implement efficiently the multi-order sensitivity analysis: interval size of
the variables, management of non-linearity, usefulness of various outputs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3059</identifier>
 <datestamp>2012-03-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3059</id><created>2012-03-14</created><authors><author><keyname>Turan</keyname><forenames>Erhan</forenames></author><author><keyname>Ecder</keyname><forenames>Ali</forenames></author></authors><title>Set Reduction In Nonlinear Equations</title><categories>cs.MS cs.NA math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, an idea to solve nonlinear equations is presented. During the
solution of any problem with Newton's Method, it might happen that some of the
unknowns satisfy the convergence criteria where the others fail. The
convergence happens only when all variables reach to the convergence limit. A
method to reduce the dimension of the overall system by excluding some of the
unknowns that satisfy an intermediate tolerance is introduced. In this
approach, a smaller system is solved in less amount of time and already
established local solutions are preserved and kept as constants while the other
variables that belong to the &quot;set&quot; will be relaxed. To realize the idea, an
algorithm is given that utilizes applications of pointers to reduce and
evaluate the sets. Matrix-free Newton-Krylov Techniques are used on a test
problem and it is shown that proposed idea improves the overall convergence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3065</identifier>
 <datestamp>2012-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3065</id><created>2012-03-14</created><updated>2012-07-25</updated><authors><author><keyname>Deffuant</keyname><forenames>Guillaume</forenames></author><author><keyname>Carletti</keyname><forenames>Timoteo</forenames></author><author><keyname>Huet</keyname><forenames>Sylvie</forenames></author></authors><title>The Leviathan model: Absolute dominance, generalised distrust, small
  worlds and other patterns emerging from combining vanity with opinion
  propagation</title><categories>physics.soc-ph cs.SI</categories><comments>Improved version after referees comments</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an opinion dynamics model that combines processes of vanity and
opinion propagation. The interactions take place between randomly chosen pairs.
During an interaction, the agents propagate their opinions about themselves and
about other people they know. Moreover, each individual is subject to vanity:
if her interlocutor seems to value her highly, then she increases her opinion
about this interlocutor. On the contrary she tends to decrease her opinion
about those who seem to undervalue her. The combination of these dynamics with
the hypothesis that the opinion propagation is more efficient when coming from
highly valued individuals, leads to different patterns when varying the
parameters. For instance, for some parameters the positive opinion links
between individuals generate a small world network. In one of the patterns,
absolute dominance of one agent alternates with a state of generalised
distrust, where all agents have a very low opinion of all the others (including
themselves). We provide some explanations of the mechanisms behind these
emergent behaviors and finally propose a discussion about their interest
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3085</identifier>
 <datestamp>2012-03-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3085</id><created>2012-03-14</created><authors><author><keyname>Fatehi</keyname><forenames>Pouya</forenames></author><author><keyname>Hashemi</keyname><forenames>Seyyed Mohsen</forenames></author></authors><title>An Agile Method for E-Service Composition</title><categories>cs.SE</categories><comments>IJCSI ePublication Certificate, Volume 9, Issue 1, January 2012</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Nowadays, application of Service Oriented Architecture is increasing rapidly;
especially since introduction of distributed electronic services on the web.
SOA software has a modular manner and works as a collaboration of independent
software components. As a result, e-service approach is sufficient for software
with independent components, each of which may be developed by a different
company. Such software components and their cooperation form a composite
service. Agile methodologies are the best candidate for developing small
software components. Composite services and its building blocks are small
pieces of software, making agile methodology a perfect fit for their
development. In this paper, we introduce an agile method for service
composition, inspired by agile patterns and practices. Therefore, across the
agile manifesto, we can develop low cost, high quality composite services
quickly using this method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3092</identifier>
 <datestamp>2012-03-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3092</id><created>2012-03-14</created><authors><author><keyname>Moretti</keyname><forenames>S&#xe9;bastien</forenames></author><author><keyname>Murri</keyname><forenames>Riccardo</forenames></author><author><keyname>Maffioletti</keyname><forenames>Sergio</forenames></author><author><keyname>Kuzniar</keyname><forenames>Arnold</forenames></author><author><keyname>Castella</keyname><forenames>Bris&#xe9;&#xef;s</forenames></author><author><keyname>Salamin</keyname><forenames>Nicolas</forenames></author><author><keyname>Robinson-Rechavi</keyname><forenames>Marc</forenames></author><author><keyname>Stockinger</keyname><forenames>Heinz</forenames></author></authors><title>gcodeml: A Grid-enabled Tool for Detecting Positive Selection in
  Biological Evolution</title><categories>cs.DC cs.CE q-bio.PE</categories><comments>10 pages, 4 figures. To appear in the HealthGrid 2012 conf</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the important questions in biological evolution is to know if certain
changes along protein coding genes have contributed to the adaptation of
species. This problem is known to be biologically complex and computationally
very expensive. It, therefore, requires efficient Grid or cluster solutions to
overcome the computational challenge. We have developed a Grid-enabled tool
(gcodeml) that relies on the PAML (codeml) package to help analyse large
phylogenetic datasets on both Grids and computational clusters. Although we
report on results for gcodeml, our approach is applicable and customisable to
related problems in biology or other scientific domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3097</identifier>
 <datestamp>2012-03-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3097</id><created>2012-03-14</created><authors><author><keyname>Abdoun</keyname><forenames>Otman</forenames></author><author><keyname>Abouchabaka</keyname><forenames>Jaafar</forenames></author></authors><title>A Comparative Study of Adaptive Crossover Operators for Genetic
  Algorithms to Resolve the Traveling Salesman Problem</title><categories>cs.NE cs.CE</categories><journal-ref>International Journal of Computer Applications (0975 - 8887)
  Volume 31 - No.11, October 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Genetic algorithm includes some parameters that should be adjusting so that
the algorithm can provide positive results. Crossover operators play very
important role by constructing competitive Genetic Algorithms (GAs). In this
paper, the basic conceptual features and specific characteristics of various
crossover operators in the context of the Traveling Salesman Problem (TSP) are
discussed. The results of experimental comparison of more than six different
crossover operators for the TSP are presented. The experiment results show that
OX operator enables to achieve a better solutions than other operators tested.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3098</identifier>
 <datestamp>2012-03-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3098</id><created>2012-03-14</created><authors><author><keyname>Husainov</keyname><forenames>Ahmet A.</forenames></author><author><keyname>Kudryashova</keyname><forenames>Ekaterina S.</forenames></author></authors><title>Generalized Asynchronous Systems</title><categories>cs.DC</categories><comments>8 pages</comments><msc-class>68Q10, 68Q85</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper is devoted to a mathematical model of concurrency the special case
of which is asynchronous system. Distributed asynchronous automata are
introduced here. It is proved that the Petri nets and transition systems with
independence can be considered like the distributed asynchronous automata. Time
distributed asynchronous automata are defined in standard way by the map which
assigns time intervals to events. It is proved that the time distributed
asynchronous automata are generalized the time Petri nets and asynchronous
systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3099</identifier>
 <datestamp>2012-03-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3099</id><created>2012-03-14</created><authors><author><keyname>Abdoun</keyname><forenames>Otman</forenames></author><author><keyname>Abouchabaka</keyname><forenames>Jaafar</forenames></author><author><keyname>Tajani</keyname><forenames>Chakir</forenames></author></authors><title>Analyzing the Performance of Mutation Operators to Solve the Travelling
  Salesman Problem</title><categories>cs.NE cs.CE</categories><comments>ISSN: 2222-4254</comments><journal-ref>IJES, International Journal of Emerging Sciences , 2(1), 61-77,
  March 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The genetic algorithm includes some parameters that should be adjusted, so as
to get reliable results. Choosing a representation of the problem addressed, an
initial population, a method of selection, a crossover operator, mutation
operator, the probabilities of crossover and mutation, and the insertion method
creates a variant of genetic algorithms. Our work is part of the answer to this
perspective to find a solution for this combinatorial problem. What are the
best parameters to select for a genetic algorithm that creates a variety
efficient to solve the Travelling Salesman Problem (TSP)? In this paper, we
present a comparative analysis of different mutation operators, surrounded by a
dilated discussion that justifying the relevance of genetic operators chosen to
solving the TSP problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3103</identifier>
 <datestamp>2012-03-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3103</id><created>2012-03-14</created><authors><author><keyname>Rehman</keyname><forenames>Sadaqat Ur</forenames></author><author><keyname>Bilal</keyname><forenames>Muhammad</forenames></author><author><keyname>Ahmad</keyname><forenames>Basharat</forenames></author><author><keyname>Yahya</keyname><forenames>Khawaja Muhammad</forenames></author><author><keyname>Ullah</keyname><forenames>Anees</forenames></author><author><keyname>Rehman</keyname><forenames>Obaid Ur</forenames></author></authors><title>Comparison Based Analysis of Different Cryptographic and Encryption
  Techniques Using Message Authentication Code (MAC) in Wireless Sensor
  Networks (WSN)</title><categories>cs.CR cs.NI</categories><comments>6 Pages, International Journal of Computer Science issues 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless Sensor Networks (WSN) are becoming popular day by day, however one
of the main issue in WSN is its limited resources. We have to look to the
resources to create Message Authentication Code (MAC) keeping in mind the
feasibility of technique used for the sensor network at hand. This research
work investigates different cryptographic techniques such as symmetric key
cryptography and asymmetric key cryptography. Furthermore, it compares
different encryption techniques such as stream cipher (RC4), block cipher (RC2,
RC5, RC6 etc) and hashing techniques (MD2, MD4, MD5, SHA, SHA1 etc). The result
of our work provides efficient techniques for communicating device, by
selecting different comparison matrices i.e. energy consumption, processing
time, memory and expenses that satisfies both the security and restricted
resources in WSN environment to create MAC.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3114</identifier>
 <datestamp>2012-03-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3114</id><created>2012-03-14</created><authors><author><keyname>Sosas</keyname><forenames>Maria-Luisa</forenames></author><author><keyname>Arias</keyname><forenames>Miguel-Octavio</forenames></author></authors><title>Integrated three-dimensional reconstruction using reflectance fields</title><categories>cs.CV</categories><comments>5 pages, 3 figures; Published in IJCSI Journal, Volume 9, Issue 1,
  No. 3, January 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A method to obtain three-dimensional data of real-world objects by
integrating their material properties is presented. The material properties are
defined by capturing the Reflectance Fields of the real-world objects. It is
shown, unlike conventional reconstruction methods, the method is able to use
the reflectance information to recover surface depth for objects having a
non-Lambertian surface reflectance. It is, for recovering 3D data of objects
exhibiting an anisotropic BRDF with an error less than 0.3%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3115</identifier>
 <datestamp>2012-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3115</id><created>2012-03-14</created><updated>2012-08-30</updated><authors><author><keyname>Forney</keyname><forenames>G. David</forenames><suffix>Jr</suffix></author><author><keyname>Gluesing-Luerssen</keyname><forenames>Heide</forenames></author></authors><title>Codes on Graphs: Observability, Controllability and Local Reducibility</title><categories>cs.IT cs.SY math.IT</categories><comments>16 pages. To appear in the IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates properties of realizations of linear or group codes
on general graphs that lead to local reducibility.
  Trimness and properness are dual properties of constraint codes. A linear or
group realization with a constraint code that is not both trim and proper is
locally reducible. A linear or group realization on a finite cycle-free graph
is minimal if and only if every local constraint code is trim and proper.
  A realization is called observable if there is a one-to-one correspondence
between codewords and configurations, and controllable if it has independent
constraints. A linear or group realization is observable if and only if its
dual is controllable. A simple counting test for controllability is given. An
unobservable or uncontrollable realization is locally reducible. Parity-check
realizations are controllable if and only if they have independent parity
checks. In an uncontrollable tail-biting trellis realization, the behavior
partitions into disconnected subbehaviors, but this property does not hold for
non-trellis realizations. On a general graph, the support of an unobservable
configuration is a generalized cycle.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3125</identifier>
 <datestamp>2012-03-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3125</id><created>2012-03-14</created><authors><author><keyname>Didehvar</keyname><forenames>Farzad</forenames></author></authors><title>A Semantic Without Syntax 1</title><categories>cs.OH</categories><comments>3 pages</comments><msc-class>68T27</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Here, by introducing a version of &quot;Unexpected hanging paradox&quot; we try to open
a new way and a new explanation for paradoxes, similar to liar paradox. Also,
we will show that we have a semantic situation which no syntactical logical
system could support that. In the end, we propose a claim as a question. Based
on this claim, having an axiomatic system for computability theory is not
possible. In fact we will show that the method applied here could yields us as
a generalized result, some Theories like Physic is not axiomatizable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3128</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3128</id><created>2012-03-14</created><updated>2012-05-19</updated><authors><author><keyname>Muralidharan</keyname><forenames>Vijayvaradharaj T.</forenames></author><author><keyname>Rajan</keyname><forenames>B. Sundar</forenames></author></authors><title>Distributed Space Time Coding for Wireless Two-way Relaying</title><categories>cs.IT math.IT</categories><comments>27 pages, 4 figures, A mistake in the proof of Proposition 3 given in
  Appendix B corrected</comments><doi>10.1109/TSP.2012.2231677</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the wireless two-way relay channel, in which two-way data
transfer takes place between the end nodes with the help of a relay. For the
Denoise-And-Forward (DNF) protocol, it was shown by Koike-Akino et. al. that
adaptively changing the network coding map used at the relay greatly reduces
the impact of Multiple Access interference at the relay. The harmful effect of
the deep channel fade conditions can be effectively mitigated by proper choice
of these network coding maps at the relay. Alternatively, in this paper we
propose a Distributed Space Time Coding (DSTC) scheme, which effectively
removes most of the deep fade channel conditions at the transmitting nodes
itself without any CSIT and without any need to adaptively change the network
coding map used at the relay. It is shown that the deep fades occur when the
channel fade coefficient vector falls in a finite number of vector subspaces of
$\mathbb{C}^2$, which are referred to as the singular fade subspaces. DSTC
design criterion referred to as the \textit{singularity minimization criterion}
under which the number of such vector subspaces are minimized is obtained.
Also, a criterion to maximize the coding gain of the DSTC is obtained. Explicit
low decoding complexity DSTC designs which satisfy the singularity minimization
criterion and maximize the coding gain for QAM and PSK signal sets are
provided. Simulation results show that at high Signal to Noise Ratio, the DSTC
scheme provides large gains when compared to the conventional Exclusive OR
network code and performs slightly better than the adaptive network coding
scheme proposed by Koike-Akino et. al.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3136</identifier>
 <datestamp>2012-03-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3136</id><created>2012-03-14</created><updated>2012-03-21</updated><authors><author><keyname>Arvelo</keyname><forenames>Eduardo</forenames></author><author><keyname>Martins</keyname><forenames>Nuno C.</forenames></author></authors><title>A Receding Horizon Strategy for Systems with Interval-Wise Energy
  Constraints</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a receding horizon control strategy that readily handles systems
that exhibit interval-wise total energy constraints on the input control
sequence. The approach is based on a variable optimization horizon length and
contractive final state constraint sets. The optimization horizon, which
recedes by N steps every N steps, is the key to accommodate the interval-wise
total energy constraints. The varying optimization horizon along with the
contractive constraints are used to achieve analytic asymptotic stability of
the system under the proposed scheme. The strategy is demonstrated by
simulation examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3143</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3143</id><created>2012-03-14</created><authors><author><keyname>Tapparello</keyname><forenames>Cristiano</forenames></author><author><keyname>Simeone</keyname><forenames>Osvaldo</forenames></author><author><keyname>Rossi</keyname><forenames>Michele</forenames></author></authors><title>Dynamic Compression-Transmission for Energy-Harvesting Multihop Networks
  with Correlated Sources</title><categories>cs.IT cs.NI math.IT</categories><comments>15 pages, 5 figures</comments><journal-ref>IEEE/ACM Transactions on Networking, Vol. 22, No. 6, December 2014</journal-ref><doi>10.1109/TNET.2013.2283071</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy-harvesting wireless sensor networking is an emerging technology with
applications to various fields such as environmental and structural health
monitoring. A distinguishing feature of wireless sensors is the need to perform
both source coding tasks, such as measurement and compression, and transmission
tasks. It is known that the overall energy consumption for source coding is
generally comparable to that of transmission, and that a joint design of the
two classes of tasks can lead to relevant performance gains. Moreover, the
efficiency of source coding in a sensor network can be potentially improved via
distributed techniques by leveraging the fact that signals measured by
different nodes are correlated.
  In this paper, a data gathering protocol for multihop wireless sensor
networks with energy harvesting capabilities is studied whereby the sources
measured by the sensors are correlated. Both the energy consumptions of source
coding and transmission are modeled, and distributed source coding is assumed.
The problem of dynamically and jointly optimizing the source coding and
transmission strategies is formulated for time-varying channels and sources.
The problem consists in the minimization of a cost function of the distortions
in the source reconstructions at the sink under queue stability constraints. By
adopting perturbation-based Lyapunov techniques, a close-to-optimal online
scheme is proposed that has an explicit and controllable trade-off between
optimality gap and queue sizes. The role of side information available at the
sink is also discussed under the assumption that acquiring the side information
entails an energy cost. It is shown that the presence of side information can
improve the network performance both in terms of overall network cost function
and queue sizes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3163</identifier>
 <datestamp>2012-03-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3163</id><created>2012-03-14</created><authors><author><keyname>Sergeyev</keyname><forenames>Yaroslav D.</forenames></author></authors><title>Numerical computations and mathematical modelling with infinite and
  infinitesimal numbers</title><categories>math.NA cs.ET math.PR</categories><comments>20 pages, 3 figures</comments><msc-class>65Y10, 65F05, 60A05, 26A03</msc-class><journal-ref>Journal of Applied Mathematics and Computing, 2009, 29, 177-195</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional computers work with finite numbers. Situations where the usage of
infinite or infinitesimal quantities is required are studied mainly
theoretically. In this paper, a recently introduced computational methodology
(that is not related to the non-standard analysis) is used to work with finite,
infinite, and infinitesimal numbers \textit{numerically}. This can be done on a
new kind of a computer - the Infinity Computer - able to work with all these
types of numbers. The new computational tools both give possibilities to
execute computations of a new type and open new horizons for creating new
mathematical models where a computational usage of infinite and/or
infinitesimal numbers can be useful. A number of numerical examples showing the
potential of the new approach and dealing with divergent series, limits,
probability theory, linear algebra, and calculation of volumes of objects
consisting of parts of different dimensions are given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3164</identifier>
 <datestamp>2012-03-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3164</id><created>2012-03-14</created><authors><author><keyname>Sergeyev</keyname><forenames>Yaroslav D.</forenames></author></authors><title>Higher order numerical differentiation on the Infinity Computer</title><categories>math.NA cs.ET math.OC</categories><comments>12 pages, no figures</comments><msc-class>65K05, 90C26, 90C56</msc-class><journal-ref>Optimization Letters, 2011, 5(4), 575-585</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There exist many applications where it is necessary to approximate
numerically derivatives of a function which is given by a computer procedure.
In particular, all the fields of optimization have a special interest in such a
kind of information. In this paper, a new way to do this is presented for a new
kind of a computer -- the Infinity Computer -- able to work numerically with
finite, infinite, and infinitesimal numbers. It is proved that the Infinity
Computer is able to calculate values of derivatives of a higher order for a
wide class of functions represented by computer procedures. It is shown that
the ability to compute derivatives of arbitrary order automatically and
accurate to working precision is an intrinsic property of the Infinity Computer
related to its way of functioning. Numerical examples illustrating the new
concepts and numerical tools are given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3165</identifier>
 <datestamp>2012-03-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3165</id><created>2012-03-14</created><authors><author><keyname>Sergeyev</keyname><forenames>Yaroslav D.</forenames></author></authors><title>Methodology of Numerical Computations with Infinities and Infinitesimals</title><categories>math.NA cs.ET</categories><comments>This paper presents Honorable Lagrange Lecture delivered in Turin on
  16 April 2010. It consists of 20 pages, 1 figure</comments><msc-class>03E65, 65-02, 65B10, 60A10</msc-class><journal-ref>Rendiconti del Seminario Matematico dell'Universita' e del
  Politecnico di Torino, 2010, 68(2), 95-113</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A recently developed computational methodology for executing numerical
calculations with infinities and infinitesimals is described in this paper. The
developed approach has a pronounced applied character and is based on the
principle `The part is less than the whole' introduced by Ancient Greeks. This
principle is used with respect to all numbers (finite, infinite, and
infinitesimal) and to all sets and processes (finite and infinite). The point
of view on infinities and infinitesimals (and in general, on Mathematics)
presented in this paper uses strongly physical ideas emphasizing interrelations
holding between a mathematical object under the observation and tools used for
this observation. It is shown how a new numeral system allowing one to express
different infinite and infinitesimal quantities in a unique framework can be
used for theoretical and computational purposes. Numerous examples dealing with
infinite sets, divergent series, limits, and probability theory are given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3167</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3167</id><created>2012-03-14</created><updated>2012-03-23</updated><authors><author><keyname>Kreutzer</keyname><forenames>Stephan</forenames><affiliation>Oxford University Computing Laboratory</affiliation></author></authors><title>On the Parameterized Intractability of Monadic Second-Order Logic</title><categories>cs.LO</categories><proxy>LMCS</proxy><acm-class>F.4.1</acm-class><journal-ref>Logical Methods in Computer Science, Volume 8, Issue 1 (March 26,
  2012) lmcs:785</journal-ref><doi>10.2168/LMCS-8(1:27)2012</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of Courcelle's celebrated results states that if C is a class of graphs
of bounded tree-width, then model-checking for monadic second order logic
(MSO_2) is fixed-parameter tractable (fpt) on C by linear time parameterized
algorithms, where the parameter is the tree-width plus the size of the formula.
An immediate question is whether this is best possible or whether the result
can be extended to classes of unbounded tree-width. In this paper we show that
in terms of tree-width, the theorem cannot be extended much further. More
specifically, we show that if C is a class of graphs which is closed under
colourings and satisfies certain constructibility conditions and is such that
the tree-width of C is not bounded by \log^{84} n then MSO_2-model checking is
not fpt unless SAT can be solved in sub-exponential time. If the tree-width of
C is not poly-logarithmically bounded, then MSO_2-model checking is not fpt
unless all problems in the polynomial-time hierarchy can be solved in
sub-exponential time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3170</identifier>
 <datestamp>2012-03-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3170</id><created>2012-03-14</created><authors><author><keyname>Sengupta</keyname><forenames>Shampa</forenames></author><author><keyname>Das</keyname><forenames>Asit Kr.</forenames></author></authors><title>Single Reduct Generation Based on Relative Indiscernibility of Rough Set
  Theory</title><categories>cs.CV</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In real world everything is an object which represents particular classes.
Every object can be fully described by its attributes. Any real world dataset
contains large number of attributes and objects. Classifiers give poor
performance when these huge datasets are given as input to it for proper
classification. So from these huge dataset most useful attributes need to be
extracted that contribute the maximum to the decision. In the paper, attribute
set is reduced by generating reducts using the indiscernibility relation of
Rough Set Theory (RST). The method measures similarity among the attributes
using relative indiscernibility relation and computes attribute similarity set.
Then the set is minimized and an attribute similarity table is constructed from
which attribute similar to maximum number of attributes is selected so that the
resultant minimum set of selected attributes (called reduct) cover all
attributes of the attribute similarity table. The method has been applied on
glass dataset collected from the UCI repository and the classification accuracy
is calculated by various classifiers. The result shows the efficiency of the
proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3178</identifier>
 <datestamp>2012-03-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3178</id><created>2012-03-14</created><authors><author><keyname>Mani</keyname><forenames>Ashish</forenames></author><author><keyname>Patvardhan</keyname><forenames>C.</forenames></author></authors><title>A Fast fixed-point Quantum Search Algorithm by using Disentanglement and
  Measurement</title><categories>cs.IT math.IT quant-ph</categories><comments>arXiv admin note: substantial text overlap with arXiv:1102.2332</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Generic quantum search algorithm searches for target entity in an unsorted
database by repeatedly applying canonical Grover's quantum rotation transform
to reach near the vicinity of the target entity. Thus, upon measurement, there
is a high probability of finding the target entity. However, the number of
times quantum rotation transform is to be applied for reaching near the
vicinity of the target is a function of the number of target entities present
in an unsorted database, which is generally unknown. A wrong estimate of the
number of target entities can lead to overshooting or undershooting the
targets, thus reducing the success probability. Some proposals have been made
to overcome this limitation. These proposals either employ quantum counting to
estimate the number of solutions or fixed-point schemes. This paper proposes a
new scheme for stopping the application of quantum rotation transformation on
reaching near the targets by disentanglement, measurement and subsequent
processing to estimate the distance of the state vector from the target states.
It ensures a success probability, which is greater than half for all
practically significant ratios of the number of target entities to the total
number of entities in a database. The search problem is trivial for remaining
possible ratios. The proposed scheme is simpler than quantum counting and more
efficient than the known fixed-point schemes. It has same order of
computational complexity as canonical Grover`s search algorithm but is slow by
a factor of two and requires two additional ancilla qubits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3203</identifier>
 <datestamp>2012-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3203</id><created>2012-03-14</created><authors><author><keyname>Mouhoub</keyname><forenames>Nasser Eddine</forenames></author><author><keyname>Benhocine</keyname><forenames>Abdelhamid</forenames></author></authors><title>An efficient algorithm for generating AoA networks</title><categories>cs.DM</categories><comments>7 pages, 16 figures</comments><msc-class>90B10, 90B35, 68M20</msc-class><acm-class>G.2.2</acm-class><journal-ref>IJCSI, Vol. 9, Issue 1, No 1, January 2012</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The activities, in project scheduling, can be represented graphically in two
different ways, by either assigning the activities to the nodes 'AoN' directed
acyclic graph (dag) or to the arcs 'AoA dag'. In this paper, a new algorithm is
proposed for generating, for a given project scheduling problem, an
Activity-on-Arc dag starting from the Activity-on-Node dag using the concepts
of line graphs of graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3210</identifier>
 <datestamp>2012-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3210</id><created>2012-03-14</created><authors><author><keyname>Yerramalli</keyname><forenames>Srinivas</forenames></author><author><keyname>Jain</keyname><forenames>Rahul</forenames></author><author><keyname>Mitra</keyname><forenames>Urbashi</forenames></author></authors><title>A Game Theoretic Model for the Gaussian Broadcast Channel</title><categories>cs.IT math.IT</categories><comments>5 pages, no figures, submitted to ISIT 2012</comments><msc-class>91, 94</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The behavior of rational and selfish players (receivers) over a
multiple-input multiple-output Gaussian broadcast channel is investigated using
the framework of noncooperative game theory. In contrast to the game-theoretic
model of the Gaussian multiple access channel where the set of feasible actions
for each player is independent of other players' actions, the strategies of the
players in the broadcast channel are mutually coupled, usually by a sum power
or joint covariance constraint, and hence cannot be treated using traditional
Nash equilibrium solution concepts. To characterize the strategic behavior of
receivers connected to a single transmitter, this paper models the broadcast
channel as a generalized Nash equilibrium problem with coupled constraints. The
concept of normalized equilibrium (NoE) is used to characterize the equilibrium
points and the existence and uniqueness of the NoE are proven for key
scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3217</identifier>
 <datestamp>2014-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3217</id><created>2012-03-14</created><updated>2014-04-18</updated><authors><author><keyname>Yassaee</keyname><forenames>Mohammad Hossein</forenames></author><author><keyname>Gohari</keyname><forenames>Amin</forenames></author><author><keyname>Aref</keyname><forenames>Mohammad Reza</forenames></author></authors><title>Channel simulation via interactive communications</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Trans. on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the problem of channel simulation via interactive
communication, known as the coordination capacity, in a two-terminal network.
We assume that two terminals observe i.i.d.\ copies of two random variables and
would like to generate i.i.d.\ copies of two other random variables jointly
distributed with the observed random variables. The terminals are provided with
two-way communication links, and shared common randomness, all at limited
rates. Two special cases of this problem are the interactive function
computation studied by Ma and Ishwar, and the tradeoff curve between one-way
communication and shared randomness studied by Cuff. The latter work had
inspired Gohari and Anantharam to study the general problem of channel
simulation via interactive communication stated above. However only inner and
outer bounds for the special case of no shared randomness were obtained in
their work. In this paper we settle this problem by providing an exact
computable characterization of the multi-round problem. To show this we employ
the technique of &quot;output statistics of random binning&quot; that has been recently
developed by the authors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3227</identifier>
 <datestamp>2012-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3227</id><created>2012-03-14</created><authors><author><keyname>Loss</keyname><forenames>Anton</forenames></author></authors><title>Generalisation of language and knowledge models for corpus analysis</title><categories>cs.AI cs.CL</categories><comments>13 pages, 2 figures, slightly unconventional</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper takes new look on language and knowledge modelling for corpus
linguistics. Using ideas of Chaitin, a line of argument is made against
language/knowledge separation in Natural Language Processing. A simplistic
model, that generalises approaches to language and knowledge, is proposed. One
of hypothetical consequences of this model is Strong AI.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3230</identifier>
 <datestamp>2012-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3230</id><created>2012-03-14</created><authors><author><keyname>Masiero</keyname><forenames>Andrea</forenames></author><author><keyname>Cenedese</keyname><forenames>Angelo</forenames></author></authors><title>Reconstruction error in a motion capture system</title><categories>cs.CV</categories><comments>3 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Marker-based motion capture (MoCap) systems can be composed by several dozens
of cameras with the purpose of reconstructing the trajectories of hundreds of
targets. With a large amount of cameras it becomes interesting to determine the
optimal reconstruction strategy. For such aim it is of fundamental importance
to understand the information provided by different camera measurements and how
they are combined, i.e. how the reconstruction error changes by considering
different cameras. In this work, first, an approximation of the reconstruction
error variance is derived. The results obtained in some simulations suggest
that the proposed strategy allows to obtain a good approximation of the real
error variance with significant reduction of the computational time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3240</identifier>
 <datestamp>2012-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3240</id><created>2012-03-14</created><authors><author><keyname>Paul</keyname><forenames>Bijan</forenames></author><author><keyname>Ibrahim</keyname><forenames>Md.</forenames></author><author><keyname>Bikas</keyname><forenames>Md. Abu Naser</forenames></author></authors><title>Performance Evaluation of AODV &amp; DSR with Varying Pause Time &amp; Speed
  Time Over TCP &amp; CBR Connections in VANET</title><categories>cs.NI</categories><journal-ref>International Journal of Computer Science Issues (IJCSI), Volume
  9, Issue 1, January 2012</journal-ref><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  VANET (Vehicular Ad-hoc Network) is a new technology which has taken enormous
attention in the recent years. Vehicular ad hoc network is formed by cars which
are called nodes; allow them to communicate with one another without using any
fixed road side unit. It has some unique characteristics which make it
different from other ad hoc network as well as difficult to define any exact
mobility model and routing protocols because of their high mobility and
changing mobility pattern. Hence performance of routing protocols can vary with
the various parameters such as speed, pause time, node density and traffic
scenarios. In this research paper, the performance of two on-demand routing
protocols AODV &amp; DSR has been analyzed by means of packet delivery ratio, loss
packet ratio &amp; average end-to-end delay with varying pause time, speed time and
node density under TCP &amp; CBR connection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3241</identifier>
 <datestamp>2012-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3241</id><created>2012-03-14</created><authors><author><keyname>Ree</keyname><forenames>Suhan</forenames></author></authors><title>Dynamics of periodic node states on a model of static networks with
  repeated-averaging rules</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>8 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a simple model of static networks, where nodes are located on a
ring structure, and two accompanying dynamic rules of repeated averaging on
periodic node states. We assume nodes can interact with neighbors, and will add
long-range links randomly. The number of long-range links, E, controls
structures of these networks, and we show that there exist many types of fixed
points, when E is varied. When E is low, fixed points are mostly diverse
states, in which node states are diversely populated; on the other hand, when E
is high, fixed points tend to be dominated by converged states, in which node
states converge to one value. Numerically, we observe properties of fixed
points for various E's, and also estimate points of the transition from diverse
states to converged states for four different cases. This kind of simple
network models will help us understand how diversities that we encounter in
many systems of complex networks are sustained, even when mechanisms of
averaging are at work,and when they break down if more long-range connections
are added.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3245</identifier>
 <datestamp>2012-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3245</id><created>2012-03-14</created><authors><author><keyname>Tan</keyname><forenames>Bo</forenames></author><author><keyname>Thompson</keyname><forenames>John</forenames></author></authors><title>The Parameters For Powerline Channel Modeling</title><categories>cs.IT math.IT</categories><comments>5 pages, with 9 tables, supporting document</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is a support document which describes the properties of the cable and
parameters of the formulas for the statistical powerline channel modeling. The
cable parameters help the reader build powerline channel according to the
transmission line theory. The document also presents the parameters which
describe the distribution of the number of path, path magnitude, path interval
and the cable loss feature of the powerline channel. By using the parameters in
this document, readers can model the powerline channel according to the
statistical methodology proposed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3249</identifier>
 <datestamp>2012-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3249</id><created>2012-03-14</created><authors><author><keyname>da Silva</keyname><forenames>Maise Dantas</forenames></author><author><keyname>Protti</keyname><forenames>F&#xe1;bio</forenames></author><author><keyname>Souza</keyname><forenames>U&#xe9;verton dos Santos</forenames></author></authors><title>Revisiting the Complexity of And/Or Graph Solution</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a study on two data structures that have been used to
model several problems in computer science: and/or graphs and x-y graphs. An
and/or graph is an acyclic digraph containing a source, such that every vertex
v has a label f(v) \in {and,or} and edges represent dependency relations
between vertices: a vertex labeled and depends on all of its out-neighbors,
while a vertex labeled or depends on only one of its out-neighbors. X-y graphs
are defined as a natural generalization of and/or graphs: every vertex vi of an
x-y graph has a label xi-yi to mean that vi depends on xi of its yi
out-neighbors. We analyze the complexity of the optimization problems
Min-and/or and Min-x-y, which consist of finding solution subgraphs of optimal
weight for and/or and x-y graphs, respectively. Motivated by the large
applicability as well as the hardness of Min-and/or and Min-x-y, we study new
complexity aspects of such problems, both from a classical and a parameterized
point of view. We prove that Min-and/or remains NP-hard even for a very
restricted family of and/or graphs where edges have weight one and or-vertices
have out-degree at most two (apart from other property related to some
in-degrees), and that deciding whether there is a solution subtree with weight
exactly k of a given x-y tree is also NP-hard. We also show that: (i) the
parameterized problem Min-and/or(k, r), which asks whether there is a solution
subgraph of weight at most k where every or-vertex has at most r out-edges with
the same weight, is FPT; (ii) the parameterized problem Min-and/or0(k), whose
domain includes and/or graphs allowing zero-weight edges, is W[2]-hard; (iii)
the parameterized problem Min-x-y(k) is W[1]-hard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3256</identifier>
 <datestamp>2012-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3256</id><created>2012-03-14</created><updated>2012-03-25</updated><authors><author><keyname>Cannon</keyname><forenames>Sarah</forenames></author><author><keyname>Ishaque</keyname><forenames>Mashhood</forenames></author><author><keyname>T&#xf3;th</keyname><forenames>Csaba</forenames></author></authors><title>Conflict-free graph orientations with parity constraints</title><categories>cs.CG</categories><comments>To appear, Sixth International Conference on Fun with Algorithms,
  June 4-6, 2012; 15 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is known that every multigraph with an even number of edges has an even
orientation (i.e., all indegrees are even). We study parity constrained graph
orientations under additional constraints. We consider two types of constraints
for a multigraph G=(V,E): (1) an exact conflict constraint is an edge set C in
E and a vertex v in V such that C should not equal the set of incoming edges at
v; (2) a subset conflict constraint is an edge set C in E and a vertex v in V
such that C should not be a subset of incoming edges at v. We show that it is
NP-complete to decide whether G has an even orientation with exact or subset
conflicts, for all conflict sets of size two or higher. We present efficient
algorithms for computing parity constrained orientations with disjoint exact or
subset conflict pairs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3258</identifier>
 <datestamp>2012-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3258</id><created>2012-03-14</created><authors><author><keyname>ParandehGheibi</keyname><forenames>Ali</forenames></author><author><keyname>Ozdaglar</keyname><forenames>Asuman</forenames></author><author><keyname>Medard</keyname><forenames>Muriel</forenames></author></authors><title>QoE-aware Media Streaming in Technology and Cost Heterogeneous Networks</title><categories>cs.SY cs.MM cs.NI</categories><comments>submitted to IEEE Transactions on Information Theory. arXiv admin
  note: substantial text overlap with arXiv:1004.3523</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a framework for studying the problem of media streaming in
technology and cost heterogeneous environments. We first address the problem of
efficient streaming in a technology-heterogeneous setting. We employ random
linear network coding to simplify the packet selection strategies and alleviate
issues such as duplicate packet reception. Then, we study the problem of media
streaming from multiple cost-heterogeneous access networks. Our objective is to
characterize analytically the trade-off between access cost and user
experience. We model the Quality of user Experience (QoE) as the probability of
interruption in playback as well as the initial waiting time. We design and
characterize various control policies, and formulate the optimal control
problem using a Markov Decision Process (MDP) with a probabilistic constraint.
We present a characterization of the optimal policy using the
Hamilton-Jacobi-Bellman (HJB) equation. For a fluid approximation model, we
provide an exact and explicit characterization of a threshold policy and prove
its optimality using the HJB equation.
  Our simulation results show that under properly designed control policy, the
existence of alternative access technology as a complement for a primary access
network can significantly improve the user experience without any bandwidth
over-provisioning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3263</identifier>
 <datestamp>2012-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3263</id><created>2012-03-14</created><authors><author><keyname>Su</keyname><forenames>Ziyi</forenames></author><author><keyname>Biennier</keyname><forenames>Fr&#xe9;d&#xe9;rique</forenames></author></authors><title>Originator usage control with business process slicing</title><categories>cs.SE</categories><comments>14Pages, 9figures</comments><acm-class>K.4.4; K.6.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Originator Control allows information providers to define the information
re-dissemination condition. Combined with usage control policy, fine-grained
'downstream usage control' can be achieved, which specifies what attributes the
downstream consumers should have and how data is used. This paper discusses
originator usage control, paying particular attention to enterprise-level
dynamic business federations. Rather than 'pre-defining' the information
re-dissemination paths, our business process slicing method 'capture' the asset
derivation pattern, allowing to maintain originators' policies during the full
lifecycle of assets in a collaborative context. First, we propose Service Call
Graph (SCG), based on extending the System Dependency Graph, to describe
dependencies among partners. When SCG (and corresponding 'service call tuple'
list) is built for a business process, it is analyzed to group partners into
sub-contexts, according to their dependency relations. Originator usage control
can be achieved focusing on each sub-context, by examining downstream
consumers' security profiles with upstream asset providers' policies. Second,
for analyzing SCG, we propose two 'slicing' strategies, namely 'asset-based'
and 'request-based' slicing, to deal with the scenarios of both
'pre-processing' a business process scripts and 'on-the-fly' analyzing service
compositions. Last, our implementation work involves a 'context manager'
service for processing business processes defined in WS-BPEL. It can be
composed with our former proposed policy negotiation and aggregation services
to provide policy-based end-to-end security management. We also make
experiments based on processing the sample processes that come with
'WS-BPEL2.0' specification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3269</identifier>
 <datestamp>2012-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3269</id><created>2012-03-15</created><authors><author><keyname>Namboodiri</keyname><forenames>Vishnu</forenames></author><author><keyname>Rajan</keyname><forenames>B. Sundar</forenames></author></authors><title>Physical Layer Network Coding for Two-Way Relaying with QAM and Latin
  Squares</title><categories>cs.IT math.IT</categories><comments>12 Figures and 3 Tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The design of modulation schemes for the physical layer network-coded two way
relaying scenario has been extensively studied recently with the protocol which
employs two phases: Multiple access (MA) Phase and Broadcast (BC) Phase. It was
observed by Koike-Akino et al. that adaptively changing the network coding map
used at the relay according to the channel conditions greatly reduces the
impact of multiple access interference which occurs at the relay during the MA
Phase and all these network coding maps should satisfy a requirement called the
exclusive law. Only the scenario in which the end nodes use M-PSK signal sets
is extensively studied in \cite{NVR} using Latin Sqaures. In this paper, we
address the case in which the end nodes use M-QAM signal sets (where M is of
the form $2^{2\lambda}$, $\lambda$ being any positive integer). In a fading
scenario, for certain channel conditions $\gamma e^{j \theta}$, termed singular
fade states, the MA phase performance is greatly reduced. We show that the
square QAM signal sets give lesser number of singular fade states compared to
PSK signal sets. Because of this, the complexity at the relay is enormously
reduced. Moreover, lesser number of overhead bits are required in the BC phase.
The fade state $\gamma e^{j \theta}=1$ is singular for all constellations of
arbitrary size including PSK and QAM. For arbitrary PSK constellation it is
well known that the Latin Square obtained by bit-wise XOR mapping removes this
singularity. We show that XOR mapping fails to remove this singularity for QAM
of size more greater than 4 and show that a doubly block circulant Latin Square
removes this singularity. Simulation results are presented to show the
superiority of QAM over PSK.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3270</identifier>
 <datestamp>2012-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3270</id><created>2012-03-15</created><authors><author><keyname>Paul</keyname><forenames>Sushil Kumar</forenames></author><author><keyname>Uddin</keyname><forenames>Mohammad Shorif</forenames></author><author><keyname>Bouakaz</keyname><forenames>Saida</forenames></author></authors><title>Extraction of Facial Feature Points Using Cumulative Histogram</title><categories>cs.CV</categories><comments>8 pages, 6 figures,3 equations, 2 tables, 20 references</comments><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 9,
  Issue 1, No 3, January 2012 ISSN (Online): 1694-0814 www.IJCSI.org</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a novel adaptive algorithm to extract facial feature
points automatically such as eyebrows corners, eyes corners, nostrils, nose
tip, and mouth corners in frontal view faces, which is based on cumulative
histogram approach by varying different threshold values. At first, the method
adopts the Viola-Jones face detector to detect the location of face and also
crops the face region in an image. From the concept of the human face
structure, the six relevant regions such as right eyebrow, left eyebrow, right
eye, left eye, nose, and mouth areas are cropped in a face image. Then the
histogram of each cropped relevant region is computed and its cumulative
histogram value is employed by varying different threshold values to create a
new filtering image in an adaptive way. The connected component of interested
area for each relevant filtering image is indicated our respective feature
region. A simple linear search algorithm for eyebrows, eyes and mouth filtering
images and contour algorithm for nose filtering image are applied to extract
our desired corner points automatically. The method was tested on a large BioID
frontal face database in different illuminations, expressions and lighting
conditions and the experimental results have achieved average success rates of
95.27%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3271</identifier>
 <datestamp>2012-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3271</id><created>2012-03-15</created><updated>2012-10-05</updated><authors><author><keyname>Still</keyname><forenames>Susanne</forenames></author><author><keyname>Sivak</keyname><forenames>David A.</forenames></author><author><keyname>Bell</keyname><forenames>Anthony J.</forenames></author><author><keyname>Crooks</keyname><forenames>Gavin E.</forenames></author></authors><title>The thermodynamics of prediction</title><categories>cond-mat.stat-mech cs.IT math.IT q-bio.QM</categories><comments>5 pages, 1 figure</comments><journal-ref>Phys. Rev. Lett. 109, 120604 (2012)</journal-ref><doi>10.1103/PhysRevLett.109.120604</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A system responding to a stochastic driving signal can be interpreted as
computing, by means of its dynamics, an implicit model of the environmental
variables. The system's state retains information about past environmental
fluctuations, and a fraction of this information is predictive of future ones.
The remaining nonpredictive information reflects model complexity that does not
improve predictive power, and thus represents the ineffectiveness of the model.
We expose the fundamental equivalence between this model inefficiency and
thermodynamic inefficiency, measured by dissipation. Our results hold
arbitrarily far from thermodynamic equilibrium and are applicable to a wide
range of systems, including biomolecular machines. They highlight a profound
connection between the effective use of information and efficient thermodynamic
operation: any system constructed to keep memory about its environment and to
operate with maximal energetic efficiency has to be predictive.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3274</identifier>
 <datestamp>2015-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3274</id><created>2012-03-15</created><updated>2012-07-26</updated><authors><author><keyname>Hisakado</keyname><forenames>Masato</forenames></author><author><keyname>Mori</keyname><forenames>Shintaro</forenames></author></authors><title>Two kinds of Phase transitions in a Voting model</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>24 pages, 6 figures</comments><journal-ref>J.Phys.A45,(2012)345002-345016</journal-ref><doi>10.1088/1751-8113/45/34/345002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we discuss a voting model with two candidates, C_0 and C_1. We
consider two types of voters--herders and independents. The voting of
independents is based on their fundamental values; on the other hand, the
voting of herders is based on the number of previous votes. We can identify two
kinds of phase transitions. One is an information cascade transition similar to
a phase transition seen in Ising model. The other is a transition of super and
normal diffusions. These phase transitions coexist. We compared our results to
the conclusions of experiments and identified the phase transitions in the
upper limit of the time t by using analysis of human behavior obtained from
experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3280</identifier>
 <datestamp>2012-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3280</id><created>2012-03-15</created><authors><author><keyname>Demontis</keyname><forenames>Roberto</forenames></author></authors><title>What is the least number of moves needed to solve the 4-peg Towers of
  Hanoi problem?</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that the solutions to the 4-peg Tower of Hanoi Problem given by
Frame and Stewart are minimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3282</identifier>
 <datestamp>2013-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3282</id><created>2012-03-15</created><updated>2013-01-07</updated><authors><author><keyname>Harshan</keyname><forenames>J.</forenames></author><author><keyname>Viterbo</keyname><forenames>Emanuele</forenames></author><author><keyname>Belfiore</keyname><forenames>Jean-Claude</forenames></author></authors><title>Practical Encoders and Decoders for Euclidean Codes from Barnes-Wall
  Lattices</title><categories>cs.IT math.IT</categories><comments>30 pages with 10 figures and 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we address the design of high spectral-efficiency Barnes-Wall
(BW) lattice codes which are amenable to low-complexity decoding in additive
white Gaussian noise (AWGN) channels. We propose a new method of constructing
complex BW lattice codes from linear codes over polynomial rings, and show that
the proposed construction provides an explicit method of bit-labeling complex
BW lattice codes. To decode the code, we adapt the low-complexity sequential BW
lattice decoder (SBWD) recently proposed by Micciancio and Nicolosi. First, we
study the error performance of SBWD in decoding the infinite lattice, wherein
we analyze the noise statistics in the algorithm, and propose a new upper bound
on its error performance. We show that the SBWD is powerful in making correct
decisions well beyond the packing radius. Subsequently, we use the SBWD to
decode lattice codes through a novel noise-trimming technique. This is the
first work that showcases the error performance of SBWD in decoding BW lattice
codes of large block lengths.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3284</identifier>
 <datestamp>2012-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3284</id><created>2012-03-15</created><authors><author><keyname>Kiyomi</keyname><forenames>Masashi</forenames></author><author><keyname>Okamoto</keyname><forenames>Yoshio</forenames></author><author><keyname>Saitoh</keyname><forenames>Toshiki</forenames></author></authors><title>Efficient Enumeration of the Directed Binary Perfect Phylogenies from
  Incomplete Data</title><categories>cs.DS q-bio.PE</categories><comments>Extended abstract to appear in 11th International Symposium on
  Experimental Algorithms (SEA 2012)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a character-based phylogeny reconstruction problem when an
incomplete set of data is given. More specifically, we consider the situation
under the directed perfect phylogeny assumption with binary characters in which
for some species the states of some characters are missing. Our main object is
to give an efficient algorithm to enumerate (or list) all perfect phylogenies
that can be obtained when the missing entries are completed. While a simple
branch-and-bound algorithm (B&amp;B) shows a theoretically good performance, we
propose another approach based on a zero-suppressed binary decision diagram
(ZDD). Experimental results on randomly generated data exhibit that the ZDD
approach outperforms B&amp;B. We also prove that counting the number of
phylogenetic trees consistent with a given data is #P-complete, thus providing
an evidence that an efficient random sampling seems hard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3287</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3287</id><created>2012-03-15</created><updated>2013-06-12</updated><authors><author><keyname>Altieri</keyname><forenames>Andr&#xe9;s</forenames></author><author><keyname>Vega</keyname><forenames>Leonardo Rey</forenames></author><author><keyname>Piantanida</keyname><forenames>Pablo</forenames></author><author><keyname>Galarza</keyname><forenames>Cecilia</forenames></author></authors><title>Analysis of a Cooperative Strategy for a Large Decentralized Wireless
  Network</title><categories>cs.IT math.IT</categories><comments>Updated version. To appear in IEEE Transactions on Networking</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the benefits of cooperation and proposes a relay
activation strategy for a large wireless network with multiple transmitters. In
this framework, some nodes cooperate with a nearby node that acts as a relay,
using the decode-and-forward protocol, and others use direct transmission. The
network is modeled as an independently marked Poisson point process and the
source nodes may choose their relays from the set of inactive nodes. Although
cooperation can potentially lead to significant improvements in the performance
of a communication pair, relaying causes additional interference in the
network, increasing the average noise that other nodes see. We investigate how
source nodes should balance cooperation vs. interference to obtain reliable
transmissions, and for this purpose we study and optimize a relay activation
strategy with respect to the outage probability. Surprisingly, in the high
reliability regime, the optimized strategy consists on the activation of all
the relays or none at all, depending on network parameters. We provide a simple
closed-form expression that indicates when the relays should be active, and we
introduce closed form expressions that quantify the performance gains of this
scheme with respect to a network that only uses direct transmission.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3288</identifier>
 <datestamp>2012-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3288</id><created>2012-03-15</created><updated>2012-07-16</updated><authors><author><keyname>Zheng</keyname><forenames>Zhong</forenames></author><author><keyname>Wei</keyname><forenames>Lu</forenames></author><author><keyname>H&#xe4;m&#xe4;l&#xe4;inen</keyname><forenames>Jyri</forenames></author><author><keyname>Tirkkonen</keyname><forenames>Olav</forenames></author></authors><title>Approximation to Distribution of Product of Random Variables Using
  Orthogonal Polynomials for Lognormal Density</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE Communications Letter</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We derive a closed-form expression for the orthogonal polynomials associated
with the general lognormal density. The result can be utilized to construct
easily computable approximations for probability density function of a product
of random variables, when the considered variates are either independent or
correlated. As an example, we have calculated the approximative distribution
for the product of Nakagami-m variables. Simulations indicate that accuracy of
the proposed approximation is good with small cross-correlations under light
fading condition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3298</identifier>
 <datestamp>2012-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3298</id><created>2012-03-15</created><authors><author><keyname>Sergeyev</keyname><forenames>Yaroslav D.</forenames></author><author><keyname>Garro</keyname><forenames>Alfredo</forenames></author></authors><title>Observability of Turing Machines: a Refinement of the Theory of
  Computation</title><categories>cs.CC cs.LO</categories><comments>31 pages, 1 figure</comments><msc-class>68Q05, 03D10</msc-class><acm-class>F.1.1; F.4.1</acm-class><journal-ref>Informatica, 2010, 21(3), 425-454</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Turing machine is one of the simple abstract computational devices that
can be used to investigate the limits of computability. In this paper, they are
considered from several points of view that emphasize the importance and the
relativity of mathematical languages used to describe the Turing machines. A
deep investigation is performed on the interrelations between mechanical
computations and their mathematical descriptions emerging when a human (the
researcher) starts to describe a Turing machine (the object of the study) by
different mathematical languages (the instruments of investigation). Together
with traditional mathematical languages using such concepts as 'enumerable
sets' and 'continuum' a new computational methodology allowing one to measure
the number of elements of different infinite sets is used in this paper. It is
shown how mathematical languages used to describe the machines limit our
possibilities to observe them. In particular, notions of observable
deterministic and non-deterministic Turing machines are introduced and
conditions ensuring that the latter can be simulated by the former are
established.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3316</identifier>
 <datestamp>2012-04-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3316</id><created>2012-03-15</created><updated>2012-04-25</updated><authors><author><keyname>Jucovschi</keyname><forenames>Constantin</forenames></author></authors><title>Cost-Effective Integration of MKM Semantic Services into Editing
  Environments</title><categories>cs.HC</categories><comments>Intelligent Computer Mathematics, CICM 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Integration of MKM services into editors has been of big interest in both
formal as well as informal areas of MKM. Until now, most of the efforts to
integrate MKM services into editing environments are done on an individual
basis which results in high creation and maintenance costs. In this paper, I
propose an architecture which allows editing environments and MKM services to
be integrated in a more efficient way. This is accomplished by integrating
editors and services only once with a real-time document synchronization and
service broker. Doing so, simplifies the development of services as well as of
editor integrations. My experience suggests that integrating new services into
an arbitrary number of already integrated editors can take as little as 3-4
hours of work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3321</identifier>
 <datestamp>2012-09-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3321</id><created>2012-03-15</created><updated>2012-09-25</updated><authors><author><keyname>Borello</keyname><forenames>Martino</forenames></author></authors><title>The Automorphism Group of an Extremal [72,36,16] Code does not contain
  elements of order 6</title><categories>cs.IT math.IT</categories><comments>15 pages, 0 figures. A revised version of the paper is published on
  IEEE Transactions on Information Theory</comments><doi>10.1109/TIT.2012.2211095</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The existence of an extremal code of length 72 is a long-standing open
problem. Let C be a putative extremal code of length 72 and suppose that C has
an automorphism g of order 6. We show that C, as an F_2&lt;g&gt;-module, is the
direct sum of two modules, one easily determinable and the other one which has
a very restrictive structure. We use this fact to do an exhaustive search and
we do not find any code. This proves that the automorphism group of an extremal
code of length 72 does not contain elements of order 6.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3322</identifier>
 <datestamp>2012-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3322</id><created>2012-03-15</created><authors><author><keyname>Sobieszek</keyname><forenames>Tomasz</forenames></author></authors><title>A note on Shannon entropy</title><categories>cs.IT math.IT</categories><comments>4 pages</comments><msc-class>94A17 (Primary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a somewhat different way of looking on Shannon entropy. This leads
to an axiomatisation of Shannon entropy that is essentially equivalent to that
of Fadeev. In particular we give a new proof of Fadeev theorem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3323</identifier>
 <datestamp>2012-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3323</id><created>2012-03-15</created><authors><author><keyname>Alsafi</keyname><forenames>Hassen Mohammed</forenames></author><author><keyname>Abduallah</keyname><forenames>Wafaa Mustafa</forenames></author><author><keyname>Pathan</keyname><forenames>Al-Sakib Khan</forenames></author></authors><title>IDPS: An Integrated Intrusion Handling Model for Cloud</title><categories>cs.NI cs.CR</categories><comments>18 pages. Accepted paper and to be published in International Journal
  of Computing &amp; Information Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Today, many organizations are moving their computing services towards the
Cloud. This makes their computer processing available much more conveniently to
users. However, it also brings new security threats and challenges about safety
and reliability. In fact, Cloud Computing is an attractive and cost-saving
service for buyers as it provides accessibility and reliability options for
users and scalable sales for providers. In spite of being attractive, Cloud
feature poses various new security threats and challenges when it comes to
deploying Intrusion Detection System (IDS) in Cloud environments. Most
Intrusion Detection Systems (IDSs) are designed to handle specific types of
attacks. It is evident that no single technique can guarantee protection
against future attacks. Hence, there is a need for an integrated scheme which
can provide robust protection against a complete spectrum of threats. On the
other hand, there is great need for technology that enables the network and its
hosts to defend themselves with some level of intelligence in order to
accurately identify and block malicious traffic and activities. In this case,
it is called Intrusion prevention system (IPS). Therefore, in this paper, we
emphasize on recent implementations of IDS on Cloud Computing environments in
terms of security and privacy. We propose an effective and efficient model
termed as the Integrated Intrusion Detection and Prevention System (IDPS) which
combines both IDS and IPS in a single mechanism. Our mechanism also integrates
two techniques namely, Anomaly Detection (AD) and Signature Detection (SD) that
can work in cooperation to detect various numbers of attacks and stop them
through the capability of IPS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3324</identifier>
 <datestamp>2013-06-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3324</id><created>2012-03-15</created><updated>2013-06-17</updated><authors><author><keyname>Kindy</keyname><forenames>Diallo Abdoulaye</forenames></author><author><keyname>Pathan</keyname><forenames>Al-Sakib Khan</forenames></author></authors><title>A Detailed Survey on Various Aspects of SQL Injection in Web
  Applications: Vulnerabilities, Innovative Attacks, and Remedies</title><categories>cs.CR cs.DB cs.NI</categories><comments>Due to unresolved publication fee issue, authors withdrew paper after
  acceptance in INFORMATION Journal, Japan. To be published elsewhere</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In today's world, Web applications play a very important role in individual
life as well as in any country's development. Web applications have gone
through a very rapid growth in the recent years and their adoption is moving
faster than that was expected few years ago. Now-a-days, billions of
transactions are done online with the aid of different Web applications. Though
these applications are used by hundreds of people, in many cases the security
level is weak, which makes them vulnerable to get compromised. In most of the
scenarios, a user has to be identified before any communication is established
with the backend database. An arbitrary user should not be allowed access to
the system without proof of valid credentials. However, a crafted injection
gives access to unauthorized users. This is mostly accomplished via SQL
Injection input. In spite of the development of different approaches to prevent
SQL injection, it still remains an alarming threat to Web applications. In this
paper, we present a detailed survey on various types of SQL Injection
vulnerabilities, attacks, and their prevention techniques. Alongside presenting
our findings from the study, we also note down future expectations and possible
development of countermeasures against SQL Injection attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3329</identifier>
 <datestamp>2012-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3329</id><created>2012-03-15</created><authors><author><keyname>Paszkiewicz</keyname><forenames>Adam</forenames></author><author><keyname>Sobieszek</keyname><forenames>Tomasz</forenames></author></authors><title>On quantum information</title><categories>cs.IT math-ph math.IT math.MP</categories><comments>20 pages</comments><msc-class>Primary 81P45, Secondary 60A10, 47B65, 94A17, 46C07</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the following generalisation of the entropy of quantum
measurement. Let H be an infinite-dimensional separable Hilbert space with a
'density' operator {\rho}, tr {\rho}=1. Let I(P)\in R be defined for any
partition P = (P_1,...,P_m), P_1+ ... +P_m=1_H, P_i \in proj H$ and let I(P_i
Qj, i \leq m, j \leq n) = I(P) + I(Q) for Q =(Q_1,..., Q_n), \sum Q_j = 1_H and
P_iQ_j = Q_j P_i, tr {\rho} P_iQ_j = tr {\rho} P_i tr {\rho} Q_j (P, Q are
physically independent). Assuming some continuity properties we give a general
form of generalised information I.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3341</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3341</id><created>2012-03-15</created><updated>2013-12-22</updated><authors><author><keyname>Meyer</keyname><forenames>Richard</forenames></author><author><keyname>&#x17d;efran</keyname><forenames>Milo&#x161;</forenames></author><author><keyname>DeCarlo</keyname><forenames>Raymond A.</forenames></author></authors><title>A Comparison of the Embedding Method to Multi-Parametric Programming,
  Mixed-Integer Programming, Gradient-Descent, and Hybrid Minimum Principle
  Based Methods</title><categories>math.OC cs.SY</categories><comments>Accepted to IEEE Transactions on Control Systems Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, the embedding approach for solving switched optimal control
problems has been developed in a series of papers. However, the embedding
approach, which advantageously converts the hybrid optimal control problem to a
classical nonlinear optimization, has not been extensively compared to
alternative solution approaches. The goal of this paper is thus to compare the
embedding approach to multi-parametric programming, mixed-integer programming
(e.g., CPLEX), and gradient-descent based methods in the context of five
recently published examples: a spring-mass system, moving-target tracking for a
mobile robot, two-tank filling, DC-DC boost converter, and skid-steered
vehicle. A sixth example, an autonomous switched 11-region linear system, is
used to compare a hybrid minimum principle method and traditional numerical
programming. For a given performance index for each case, cost and solution
times are presented. It is shown that there are numerical advantages of the
embedding approach: lower performance index cost (except in some instances when
autonomous switches are present), generally faster solution time, and
convergence to a solution when other methods may fail. In addition, the
embedding method requires no ad hoc assumptions (e.g., predetermined mode
sequences) or specialized control models. Theoretical advantages of the
embedding approach over the other methods are also described: guaranteed
existence of a solution under mild conditions, convexity of the embedded hybrid
optimization problem (under the customary conditions on the performance index),
solvability with traditional techniques (e.g., sequential quadratic
programming) avoiding the combinatorial complexity in the number of
modes/discrete variables of mixed-integer programming, applicability to affine
nonlinear systems, and no need to explicitly assign discrete/mode variables to
autonomous switches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3351</identifier>
 <datestamp>2012-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3351</id><created>2012-03-15</created><authors><author><keyname>Amad</keyname><forenames>Mourad</forenames></author><author><keyname>Meddahi</keyname><forenames>Ahmed</forenames></author><author><keyname>A&#xef;ssani</keyname><forenames>Djamil</forenames></author></authors><title>Peer to Peer Networks Management Survey</title><categories>cs.NI cs.DC</categories><comments>10 pages, 1 figure</comments><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 9,
  Issue 1, No 3, January 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Peer-to-Peer systems are based on the concept of resources localization and
mutualisation in dynamic context. In specific environment such as mobile
networks, characterized by high variability and dynamicity of network
conditions and performances, where nodes can join and leave the network
dynamically, resources reliability and availability constitute a critical
issue. The resource discovery problem arises in the context of peer to peer
(P2P) networks, where at any point of time a peer may be placed at or removed
from any location over a general purpose network. Locating a resource or
service efficiently is one of the most important issues related to peer-to-peer
networks. The objective of a search mechanism is to successfully locate
resources while incurring low overhead and low delay. This paper presents a
survey on P2P networks management: classification, applications, platforms,
simulators and security.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3368</identifier>
 <datestamp>2012-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3368</id><created>2012-03-14</created><authors><author><keyname>Falik</keyname><forenames>Dvir</forenames></author><author><keyname>Friedgut</keyname><forenames>Ehud</forenames></author></authors><title>Between Arrow and Gibbard-Satterthwaite; A representation theoretic
  approach</title><categories>math.CO cs.CC cs.DM cs.GT math.RT</categories><comments>First appeared in FOCS'11</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A central theme in social choice theory is that of impossibility theorems,
such as Arrow's theorem and the Gibbard-Satterthwaite theorem, which state that
under certain natural constraints, social choice mechanisms are impossible to
construct. In recent years, beginning in Kalai`01, much work has been done in
finding \textit{robust} versions of these theorems, showing &quot;approximate&quot;
impossibility remains even when most, but not all, of the constraints are
satisfied. We study a spectrum of settings between the case where society
chooses a single outcome (\'a-la-Gibbard-Satterthwaite) and the choice of a
complete order (as in Arrow's theorem). We use algebraic techniques,
specifically representation theory of the symmetric group, and also prove
robust versions of the theorems that we state. Our relaxations of the
constraints involve relaxing of a version of &quot;independence of irrelevant
alternatives&quot;, rather than relaxing the demand of a transitive outcome, as is
done in most other robustness results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3370</identifier>
 <datestamp>2015-06-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3370</id><created>2012-03-13</created><updated>2015-02-17</updated><authors><author><keyname>Abbas</keyname><forenames>Taimoor</forenames><affiliation>Department of Electrical and Information Technology Lund University Sweden</affiliation></author><author><keyname>Sj&#xf6;berg</keyname><forenames>Katrin</forenames><affiliation>Volvo Group Trucks Technology</affiliation></author><author><keyname>Karedal</keyname><forenames>Johan</forenames><affiliation>Department of Electrical and Information Technology Lund University Sweden</affiliation></author><author><keyname>Tufvesson</keyname><forenames>Fredrik</forenames><affiliation>Department of Electrical and Information Technology Lund University Sweden</affiliation></author></authors><title>A Measurement Based Shadow Fading Model for Vehicle-to-Vehicle Network
  Simulations</title><categories>cs.NI</categories><comments>10 pages, 12 figures, submitted to Hindawi International Journal of
  Antennas and Propagation</comments><report-no>Article ID 190607, 12 pages</report-no><doi>10.1155/2015/190607</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The vehicle-to-vehicle (V2V) propagation channel has significant implications
on the design and performance of novel communication protocols for vehicular ad
hoc networks (VANETs). Extensive research efforts have been made to develop V2V
channel models to be implemented in advanced VANET system simulators for
performance evaluation. The impact of shadowing caused by other vehicles has,
however, largely been neglected in most of the models, as well as in the system
simulations. In this paper we present a shadow fading model targeting system
simulations based on real measurements performed in urban and highway
scenarios. The measurement data is separated into three categories,
line-of-sight (LOS), obstructed line-of-sight (OLOS) by vehicles, and non
line-of-sight due to buildings, with the help of video information recorded
during the measurements. It is observed that vehicles obstructing the LOS
induce an additional average attenuation of about 10 dB in the received signal
power. An approach to incorporate the LOS/OLOS model into existing VANET
simulators is also provided. Finally, system level VANET simulation results are
presented, showing the difference between the LOS/OLOS model and a channel
model based on Nakagami-m fading.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3376</identifier>
 <datestamp>2012-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3376</id><created>2012-03-15</created><authors><author><keyname>Edmonds</keyname><forenames>Bruce</forenames></author><author><keyname>Gershenson</keyname><forenames>Carlos</forenames></author></authors><title>Learning, Social Intelligence and the Turing Test - why an
  &quot;out-of-the-box&quot; Turing Machine will not pass the Turing Test</title><categories>cs.AI cs.LG nlin.AO</categories><comments>10 pages, invited talk at Turing Centenary Conference CiE 2012,
  special session on &quot;The Turing Test and Thinking Machines&quot;</comments><report-no>CPM Report No.: 12-215</report-no><acm-class>F.1.1; I.2.0</acm-class><journal-ref>Lecture Notes in Computer Science 7318 (2012) pp. 182-192</journal-ref><doi>10.1007/978-3-642-30870-3_18</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Turing Test (TT) checks for human intelligence, rather than any putative
general intelligence. It involves repeated interaction requiring learning in
the form of adaption to the human conversation partner. It is a macro-level
post-hoc test in contrast to the definition of a Turing Machine (TM), which is
a prior micro-level definition. This raises the question of whether learning is
just another computational process, i.e. can be implemented as a TM. Here we
argue that learning or adaption is fundamentally different from computation,
though it does involve processes that can be seen as computations. To
illustrate this difference we compare (a) designing a TM and (b) learning a TM,
defining them for the purpose of the argument. We show that there is a
well-defined sequence of problems which are not effectively designable but are
learnable, in the form of the bounded halting problem. Some characteristics of
human intelligence are reviewed including it's: interactive nature, learning
abilities, imitative tendencies, linguistic ability and context-dependency. A
story that explains some of these is the Social Intelligence Hypothesis. If
this is broadly correct, this points to the necessity of a considerable period
of acculturation (social learning in context) if an artificial intelligence is
to pass the TT. Whilst it is always possible to 'compile' the results of
learning into a TM, this would not be a designed TM and would not be able to
continually adapt (pass future TTs). We conclude three things, namely that: a
purely &quot;designed&quot; TM will never pass the TT; that there is no such thing as a
general intelligence since it necessary involves learning; and that
learning/adaption and computation should be clearly distinguished.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3402</identifier>
 <datestamp>2012-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3402</id><created>2012-03-15</created><authors><author><keyname>Berlinkov</keyname><forenames>Mikhail V.</forenames></author></authors><title>Synchronizing Automata on Quasi Eulerian Digraph</title><categories>cs.FL</categories><comments>8 pages, 1 figure</comments><acm-class>F.4.1; F.4.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In 1964 \v{C}ern\'{y} conjectured that each $n$-state synchronizing automaton
posesses a reset word of length at most $(n-1)^2$. From the other side the best
known upper bound on the reset length (minimum length of reset words) is cubic
in $n$. Thus the main problem here is to prove quadratic (in $n$) upper bounds.
Since 1964, this problem has been solved for few special classes of \sa. One of
this result is due to Kari \cite{Ka03} for automata with Eulerian digraphs. In
this paper we introduce a new approach to prove quadratic upper bounds and
explain it in terms of Markov chains and Perron-Frobenius theories. Using this
approach we obtain a quadratic upper bound for a generalization of Eulerian
automata.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3415</identifier>
 <datestamp>2013-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3415</id><created>2012-03-15</created><updated>2013-04-18</updated><authors><author><keyname>Meira</keyname><forenames>Luis A. A.</forenames></author><author><keyname>M&#xe1;ximo</keyname><forenames>Vinicius R.</forenames></author><author><keyname>Fazenda</keyname><forenames>&#xc1;lvaro L.</forenames></author><author><keyname>da Concei&#xe7;&#xe3;o</keyname><forenames>Arlindo F.</forenames></author></authors><title>acc-Motif Detection Tool</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network motif algorithms have been a topic of research mainly after the
2002-seminal paper from Milo \emph{et al}, that provided motifs as a way to
uncover the basic building blocks of most networks. In Bioinformatics, motifs
have been mainly applied in the field of gene regulation networks. This paper
proposes new algorithms to exactly count isomorphic pattern motifs of sizes 3,
4 and 5 in directed graphs. Let $G(V,E)$ be a directed graph with $m=|E|$. We
describe an $O({m\sqrt{m}})$ time complexity algorithm to count isomorphic
patterns of size 3. In order to count isomorphic patterns of size 4, we propose
an $O(m^2)$ algorithm. To count patterns with 5 vertices, the algorithm is
$O(m^2n)$. The new algorithms were implemented and compared with FANMOD and
Kavosh motif detection tools. The experiments show that our algorithms are
expressively faster than FANMOD and Kavosh's. We also let our motif-detecting
tool available in the Internet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3428</identifier>
 <datestamp>2012-05-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3428</id><created>2012-03-08</created><updated>2012-05-16</updated><authors><author><keyname>Ford</keyname><forenames>Bryan</forenames></author></authors><title>Plugging Side-Channel Leaks with Timing Information Flow Control</title><categories>cs.CR</categories><comments>5 pages, 3 figures</comments><journal-ref>4th USENIX Workshop on Hot Topics in Cloud Computing (HotCloud
  '12), June 12-13, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The cloud model's dependence on massive parallelism and resource sharing
exacerbates the security challenge of timing side-channels. Timing Information
Flow Control (TIFC) is a novel adaptation of IFC techniques that may offer a
way to reason about, and ultimately control, the flow of sensitive information
through systems via timing channels. With TIFC, objects such as files,
messages, and processes carry not just content labels describing the ownership
of the object's &quot;bits,&quot; but also timing labels describing information contained
in timing events affecting the object, such as process creation/termination or
message reception. With two system design tools-deterministic execution and
pacing queues-TIFC enables the construction of &quot;timing-hardened&quot; cloud
infrastructure that permits statistical multiplexing, while aggregating and
rate-limiting timing information leakage between hosted computations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3431</identifier>
 <datestamp>2012-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3431</id><created>2012-03-04</created><authors><author><keyname>Kuppusamy</keyname><forenames>K. S.</forenames></author><author><keyname>R</keyname><forenames>Senthilraja.</forenames></author><author><keyname>Aghila</keyname><forenames>G.</forenames></author></authors><title>A Model for Remote Access and Protection of Smartphones using Short
  Message Service</title><categories>cs.CR</categories><comments>10 Pages, 11 Figures</comments><msc-class>94Axx</msc-class><journal-ref>International Journal of Computer Science, Engineering and
  Information Technology (IJCSEIT), Vol.2, No.1, February 2012</journal-ref><doi>10.5121/ijcseit.2012.2109</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The smartphone usage among people is increasing rapidly. With the phenomenal
growth of smartphone use, smartphone theft is also increasing. This paper
proposes a model to secure smartphones from theft as well as provides options
to access a smartphone through other smartphone or a normal mobile via Short
Message Service. This model provides option to track and secure the mobile by
locking it. It also provides facilities to receive the incoming call and sms
information to the remotely connected device and enables the remote user to
control the mobile through SMS. The proposed model is validated by the
prototype implementation in Android platform. Various tests are conducted in
the implementation and the results are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3434</identifier>
 <datestamp>2012-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3434</id><created>2012-03-15</created><authors><author><keyname>Prost</keyname><forenames>Frederic</forenames></author></authors><title>On the Impact of Information Technologies on Society: an Historical
  Perspective through the Game of Chess</title><categories>cs.OH</categories><acm-class>K.2; K.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The game of chess as always been viewed as an iconic representation of
intellectual prowess. Since the very beginning of computer science, the
challenge of being able to program a computer capable of playing chess and
beating humans has been alive and used both as a mark to measure
hardware/software progresses and as an ongoing programming challenge leading to
numerous discoveries. In the early days of computer science it was a topic for
specialists. But as computers were democratized, and the strength of chess
engines began to increase, chess players started to appropriate to themselves
these new tools. We show how these interactions between the world of chess and
information technologies have been herald of broader social impacts of
information technologies. The game of chess, and more broadly the world of
chess (chess players, literature, computer softwares and websites dedicated to
chess, etc.), turns out to be a surprisingly and particularly sharp indicator
of the changes induced in our everyday life by the information technologies.
Moreover, in the same way that chess is a modelization of war that captures the
raw features of strategic thinking, chess world can be seen as small society
making the study of the information technologies impact easier to analyze and
to grasp.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3442</identifier>
 <datestamp>2012-07-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3442</id><created>2012-03-15</created><updated>2012-07-26</updated><authors><author><keyname>Vashkevich</keyname><forenames>Maxim</forenames></author><author><keyname>Petrovsky</keyname><forenames>Alexander</forenames></author></authors><title>A low multiplicative complexity fast recursive DCT-2 algorithm</title><categories>cs.IT math.IT</categories><comments>4 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A fast Discrete Cosine Transform (DCT) algorithm is introduced that can be of
particular interest in image processing. The main features of the algorithm are
regularity of the graph and very low arithmetic complexity. The 16-point
version of the algorithm requires only 32 multiplications and 81 additions. The
computational core of the algorithm consists of only 17 nontrivial
multiplications, the rest 15 are scaling factors that can be compensated in the
post-processing. The derivation of the algorithm is based on the algebraic
signal processing theory (ASP).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3445</identifier>
 <datestamp>2012-03-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3445</id><created>2012-03-15</created><authors><author><keyname>Courtade</keyname><forenames>Thomas A.</forenames></author><author><keyname>Wesel</keyname><forenames>Richard D.</forenames></author></authors><title>Coded Cooperative Data Exchange in Multihop Networks</title><categories>cs.IT math.IT</categories><comments>49 pages, 6 figures, submitted to Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a connected network of n nodes that all wish to recover k desired
packets. Each node begins with a subset of the desired packets and exchanges
coded packets with its neighbors. This paper provides necessary and sufficient
conditions which characterize the set of all transmission schemes that permit
every node to ultimately learn (recover) all k packets. When the network
satisfies certain regularity conditions and packets are randomly distributed,
this paper provides tight concentration results on the number of transmissions
required to achieve universal recovery. For the case of a fully connected
network, a polynomial-time algorithm for computing an optimal transmission
scheme is derived. An application to secrecy generation is discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3453</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3453</id><created>2012-03-15</created><updated>2014-05-04</updated><authors><author><keyname>Proserpio</keyname><forenames>Davide</forenames></author><author><keyname>Goldberg</keyname><forenames>Sharon</forenames></author><author><keyname>McSherry</keyname><forenames>Frank</forenames></author></authors><title>Calibrating Data to Sensitivity in Private Data Analysis</title><categories>cs.CR cs.SI</categories><comments>17 pages</comments><journal-ref>Calibrating Data to Sensitivity in Private Data Analysis
  Proserpio, Davide, Sharon Goldberg, and Frank McSherry. &quot;Calibrating Data to
  Sensitivity in Private Data Analysis.&quot; Proceedings of the VLDB Endowment 7.8
  (2014)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an approach to differentially private computation in which one
does not scale up the magnitude of noise for challenging queries, but rather
scales down the contributions of challenging records. While scaling down all
records uniformly is equivalent to scaling up the noise magnitude, we show that
scaling records non-uniformly can result in substantially higher accuracy by
bypassing the worst-case requirements of differential privacy for the noise
magnitudes. This paper details the data analysis platform wPINQ, which
generalizes the Privacy Integrated Query (PINQ) to weighted datasets. Using a
few simple operators (including a non-uniformly scaling Join operator) wPINQ
can reproduce (and improve) several recent results on graph analysis and
introduce new generalizations (e.g., counting triangles with given degrees). We
also show how to integrate probabilistic inference techniques to synthesize
datasets respecting more complicated (and less easily interpreted)
measurements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3461</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3461</id><created>2012-03-15</created><authors><author><keyname>Huang</keyname><forenames>Kaizhu</forenames></author><author><keyname>Jin</keyname><forenames>Rong</forenames></author><author><keyname>Xu</keyname><forenames>Zenglin</forenames></author><author><keyname>Liu</keyname><forenames>Cheng-Lin</forenames></author></authors><title>Robust Metric Learning by Smooth Optimization</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</comments><proxy>auai</proxy><report-no>UAI-P-2010-PG-244-251</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most existing distance metric learning methods assume perfect side
information that is usually given in pairwise or triplet constraints. Instead,
in many real-world applications, the constraints are derived from side
information, such as users' implicit feedbacks and citations among articles. As
a result, these constraints are usually noisy and contain many mistakes. In
this work, we aim to learn a distance metric from noisy constraints by robust
optimization in a worst-case scenario, to which we refer as robust metric
learning. We formulate the learning task initially as a combinatorial
optimization problem, and show that it can be elegantly transformed to a convex
programming problem. We present an efficient learning algorithm based on smooth
optimization [7]. It has a worst-case convergence rate of
O(1/{\surd}{\varepsilon}) for smooth optimization problems, where {\varepsilon}
is the desired error of the approximate solution. Finally, our empirical study
with UCI data sets demonstrate the effectiveness of the proposed method in
comparison to state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3462</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3462</id><created>2012-03-15</created><authors><author><keyname>Agovic</keyname><forenames>Amrudin</forenames></author><author><keyname>Banerjee</keyname><forenames>Arindam</forenames></author></authors><title>Gaussian Process Topic Models</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</comments><proxy>auai</proxy><report-no>UAI-P-2010-PG-10-19</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce Gaussian Process Topic Models (GPTMs), a new family of topic
models which can leverage a kernel among documents while extracting correlated
topics. GPTMs can be considered a systematic generalization of the Correlated
Topic Models (CTMs) using ideas from Gaussian Process (GP) based embedding.
Since GPTMs work with both a topic covariance matrix and a document kernel
matrix, learning GPTMs involves a novel component-solving a suitable Sylvester
equation capturing both topic and document dependencies. The efficacy of GPTMs
is demonstrated with experiments evaluating the quality of both topic modeling
and embedding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3463</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3463</id><created>2012-03-15</created><authors><author><keyname>Ahmed</keyname><forenames>Amr</forenames></author><author><keyname>Xing</keyname><forenames>Eric P.</forenames></author></authors><title>Timeline: A Dynamic Hierarchical Dirichlet Process Model for Recovering
  Birth/Death and Evolution of Topics in Text Stream</title><categories>cs.IR cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</comments><proxy>auai</proxy><report-no>UAI-P-2010-PG-20-29</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Topic models have proven to be a useful tool for discovering latent
structures in document collections. However, most document collections often
come as temporal streams and thus several aspects of the latent structure such
as the number of topics, the topics' distribution and popularity are
time-evolving. Several models exist that model the evolution of some but not
all of the above aspects. In this paper we introduce infinite dynamic topic
models, iDTM, that can accommodate the evolution of all the aforementioned
aspects. Our model assumes that documents are organized into epochs, where the
documents within each epoch are exchangeable but the order between the
documents is maintained across epochs. iDTM allows for unbounded number of
topics: topics can die or be born at any epoch, and the representation of each
topic can evolve according to a Markovian dynamics. We use iDTM to analyze the
birth and evolution of topics in the NIPS community and evaluated the efficacy
of our model on both simulated and real datasets with favorable outcome.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3464</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3464</id><created>2012-03-15</created><authors><author><keyname>Arora</keyname><forenames>Nimar S.</forenames></author><author><keyname>Braz</keyname><forenames>Rodrigo de Salvo</forenames></author><author><keyname>Sudderth</keyname><forenames>Erik B.</forenames></author><author><keyname>Russell</keyname><forenames>Stuart</forenames></author></authors><title>Gibbs Sampling in Open-Universe Stochastic Languages</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</comments><proxy>auai</proxy><report-no>UAI-P-2010-PG-30-39</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Languages for open-universe probabilistic models (OUPMs) can represent
situations with an unknown number of objects and iden- tity uncertainty. While
such cases arise in a wide range of important real-world appli- cations,
existing general purpose inference methods for OUPMs are far less efficient
than those available for more restricted lan- guages and model classes. This
paper goes some way to remedying this deficit by in- troducing, and proving
correct, a generaliza- tion of Gibbs sampling to partial worlds with possibly
varying model structure. Our ap- proach draws on and extends previous generic
OUPM inference methods, as well as aux- iliary variable samplers for
nonparametric mixture models. It has been implemented for BLOG, a well-known
OUPM language. Combined with compile-time optimizations, the resulting
algorithm yields very substan- tial speedups over existing methods on sev- eral
test cases, and substantially improves the practicality of OUPM languages
generally.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3465</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3465</id><created>2012-03-15</created><authors><author><keyname>Ayachi</keyname><forenames>Raouia</forenames></author><author><keyname>Amor</keyname><forenames>Nahla Ben</forenames></author><author><keyname>Benferhat</keyname><forenames>Salem</forenames></author><author><keyname>Haenni</keyname><forenames>Rolf</forenames></author></authors><title>Compiling Possibilistic Networks: Alternative Approaches to
  Possibilistic Inference</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</comments><proxy>auai</proxy><report-no>UAI-P-2010-PG-40-47</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Qualitative possibilistic networks, also known as min-based possibilistic
networks, are important tools for handling uncertain information in the
possibility theory frame- work. Despite their importance, only the junction
tree adaptation has been proposed for exact reasoning with such networks. This
paper explores alternative algorithms using compilation techniques. We first
propose possibilistic adaptations of standard compilation-based probabilistic
methods. Then, we develop a new, purely possibilistic, method based on the
transformation of the initial network into a possibilistic base. A comparative
study shows that this latter performs better than the possibilistic adap-
tations of probabilistic methods. This result is also confirmed by experimental
results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3466</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3466</id><created>2012-03-15</created><authors><author><keyname>Bauters</keyname><forenames>Kim</forenames></author><author><keyname>Schockaert</keyname><forenames>Steven</forenames></author><author><keyname>De Cock</keyname><forenames>Martine</forenames></author><author><keyname>Vermeir</keyname><forenames>Dirk</forenames></author></authors><title>Possibilistic Answer Set Programming Revisited</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</comments><proxy>auai</proxy><report-no>UAI-P-2010-PG-48-55</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Possibilistic answer set programming (PASP) extends answer set programming
(ASP) by attaching to each rule a degree of certainty. While such an extension
is important from an application point of view, existing semantics are not
well-motivated, and do not always yield intuitive results. To develop a more
suitable semantics, we first introduce a characterization of answer sets of
classical ASP programs in terms of possibilistic logic where an ASP program
specifies a set of constraints on possibility distributions. This
characterization is then naturally generalized to define answer sets of PASP
programs. We furthermore provide a syntactic counterpart, leading to a
possibilistic generalization of the well-known Gelfond-Lifschitz reduct, and we
show how our framework can readily be implemented using standard ASP solvers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3467</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3467</id><created>2012-03-15</created><authors><author><keyname>Bhattacharjya</keyname><forenames>Debarun</forenames></author><author><keyname>Shachter</keyname><forenames>Ross D.</forenames></author></authors><title>Three new sensitivity analysis methods for influence diagrams</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</comments><proxy>auai</proxy><report-no>UAI-P-2010-PG-56-64</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Performing sensitivity analysis for influence diagrams using the decision
circuit framework is particularly convenient, since the partial derivatives
with respect to every parameter are readily available [Bhattacharjya and
Shachter, 2007; 2008]. In this paper we present three non-linear sensitivity
analysis methods that utilize this partial derivative information and therefore
do not require re-evaluating the decision situation multiple times.
Specifically, we show how to efficiently compare strategies in decision
situations, perform sensitivity to risk aversion and compute the value of
perfect hedging [Seyller, 2008].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3468</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3468</id><created>2012-03-15</created><authors><author><keyname>Blundell</keyname><forenames>Charles</forenames></author><author><keyname>Teh</keyname><forenames>Yee Whye</forenames></author><author><keyname>Heller</keyname><forenames>Katherine A.</forenames></author></authors><title>Bayesian Rose Trees</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</comments><proxy>auai</proxy><report-no>UAI-P-2010-PG-65-72</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hierarchical structure is ubiquitous in data across many domains. There are
many hierarchical clustering methods, frequently used by domain experts, which
strive to discover this structure. However, most of these methods limit
discoverable hierarchies to those with binary branching structure. This
limitation, while computationally convenient, is often undesirable. In this
paper we explore a Bayesian hierarchical clustering algorithm that can produce
trees with arbitrary branching structure at each node, known as rose trees. We
interpret these trees as mixtures over partitions of a data set, and use a
computationally efficient, greedy agglomerative algorithm to find the rose
trees which have high marginal likelihood given the data. Lastly, we perform
experiments which demonstrate that rose trees are better models of data than
the typical binary trees returned by other hierarchical clustering algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3469</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3469</id><created>2012-03-15</created><authors><author><keyname>Brocheler</keyname><forenames>Matthias</forenames></author><author><keyname>Mihalkova</keyname><forenames>Lilyana</forenames></author><author><keyname>Getoor</keyname><forenames>Lise</forenames></author></authors><title>Probabilistic Similarity Logic</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</comments><proxy>auai</proxy><report-no>UAI-P-2010-PG-73-82</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many machine learning applications require the ability to learn from and
reason about noisy multi-relational data. To address this, several effective
representations have been developed that provide both a language for expressing
the structural regularities of a domain, and principled support for
probabilistic inference. In addition to these two aspects, however, many
applications also involve a third aspect-the need to reason about
similarities-which has not been directly supported in existing frameworks. This
paper introduces probabilistic similarity logic (PSL), a general-purpose
framework for joint reasoning about similarity in relational domains that
incorporates probabilistic reasoning about similarities and relational
structure in a principled way. PSL can integrate any existing domain-specific
similarity measures and also supports reasoning about similarities between sets
of entities. We provide efficient inference and learning techniques for PSL and
demonstrate its effectiveness both in common relational tasks and in settings
that require reasoning about similarity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3470</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3470</id><created>2012-03-15</created><authors><author><keyname>Carlin</keyname><forenames>Alan S.</forenames></author><author><keyname>Schurr</keyname><forenames>Nathan</forenames></author><author><keyname>Marecki</keyname><forenames>Janusz</forenames></author></authors><title>ALARMS: Alerting and Reasoning Management System for Next Generation
  Aircraft Hazards</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</comments><proxy>auai</proxy><report-no>UAI-P-2010-PG-93-100</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Next Generation Air Transportation System will introduce new, advanced
sensor technologies into the cockpit. With the introduction of such systems,
the responsibilities of the pilot are expected to dramatically increase. In the
ALARMS (ALerting And Reasoning Management System) project for NASA, we focus on
a key challenge of this environment, the quick and efficient handling of
aircraft sensor alerts. It is infeasible to alert the pilot on the state of all
subsystems at all times. Furthermore, there is uncertainty as to the true
hazard state despite the evidence of the alerts, and there is uncertainty as to
the effect and duration of actions taken to address these alerts. This paper
reports on the first steps in the construction of an application designed to
handle Next Generation alerts. In ALARMS, we have identified 60 different
aircraft subsystems and 20 different underlying hazards. In this paper, we show
how a Bayesian network can be used to derive the state of the underlying
hazards, based on the sensor input. Then, we propose a framework whereby an
automated system can plan to address these hazards in cooperation with the
pilot, using a Time-Dependent Markov Process (TMDP). Different hazards and
pilot states will call for different alerting automation plans. We demonstrate
this emerging application of Bayesian networks and TMDPs to cockpit automation,
for a use case where a small number of hazards are present, and analyze the
resulting alerting automation policies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3471</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3471</id><created>2012-03-15</created><authors><author><keyname>Chaudhuri</keyname><forenames>Kamalika</forenames></author><author><keyname>Freund</keyname><forenames>Yoav</forenames></author><author><keyname>Hsu</keyname><forenames>Daniel</forenames></author></authors><title>An Online Learning-based Framework for Tracking</title><categories>cs.LG cs.AI stat.ML</categories><comments>Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</comments><proxy>auai</proxy><report-no>UAI-P-2010-PG-101-108</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the tracking problem, namely, estimating the hidden state of an
object over time, from unreliable and noisy measurements. The standard
framework for the tracking problem is the generative framework, which is the
basis of solutions such as the Bayesian algorithm and its approximation, the
particle filters. However, these solutions can be very sensitive to model
mismatches. In this paper, motivated by online learning, we introduce a new
framework for tracking. We provide an efficient tracking algorithm for this
framework. We provide experimental results comparing our algorithm to the
Bayesian algorithm on simulated data. Our experiments show that when there are
slight model mismatches, our algorithm outperforms the Bayesian algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3472</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3472</id><created>2012-03-15</created><authors><author><keyname>Chen</keyname><forenames>Yutian</forenames></author><author><keyname>Welling</keyname><forenames>Max</forenames></author><author><keyname>Smola</keyname><forenames>Alex</forenames></author></authors><title>Super-Samples from Kernel Herding</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</comments><proxy>auai</proxy><report-no>UAI-P-2010-PG-109-116</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We extend the herding algorithm to continuous spaces by using the kernel
trick. The resulting &quot;kernel herding&quot; algorithm is an infinite memory
deterministic process that learns to approximate a PDF with a collection of
samples. We show that kernel herding decreases the error of expectations of
functions in the Hilbert space at a rate O(1/T) which is much faster than the
usual O(1/pT) for iid random samples. We illustrate kernel herding by
approximating Bayesian predictive distributions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3473</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3473</id><created>2012-03-15</created><authors><author><keyname>Choi</keyname><forenames>Jaesik</forenames></author><author><keyname>Amir</keyname><forenames>Eyal</forenames></author><author><keyname>Hill</keyname><forenames>David J.</forenames></author></authors><title>Lifted Inference for Relational Continuous Models</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</comments><proxy>auai</proxy><report-no>UAI-P-2010-PG-126-134</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Relational Continuous Models (RCMs) represent joint probability densities
over attributes of objects, when the attributes have continuous domains. With
relational representations, they can model joint probability distributions over
large numbers of variables compactly in a natural way. This paper presents a
new exact lifted inference algorithm for RCMs, thus it scales up to large
models of real world applications. The algorithm applies to Relational Pairwise
Models which are (relational) products of potentials of arity 2. Our algorithm
is unique in two ways. First, it substantially improves the efficiency of
lifted inference with variables of continuous domains. When a relational model
has Gaussian potentials, it takes only linear-time compared to cubic time of
previous methods. Second, it is the first exact inference algorithm which
handles RCMs in a lifted way. The algorithm is illustrated over an example from
econometrics. Experimental results show that our algorithm outperforms both a
groundlevel inference algorithm and an algorithm built with previously-known
lifted methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3474</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3474</id><created>2012-03-15</created><authors><author><keyname>Corona</keyname><forenames>Gabriel</forenames></author><author><keyname>Charpillet</keyname><forenames>Francois</forenames></author></authors><title>Distribution over Beliefs for Memory Bounded Dec-POMDP Planning</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</comments><proxy>auai</proxy><report-no>UAI-P-2010-PG-135-142</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new point-based method for approximate planning in Dec-POMDP
which outperforms the state-of-the-art approaches in terms of solution quality.
It uses a heuristic estimation of the prior probability of beliefs to choose a
bounded number of policy trees: this choice is formulated as a combinatorial
optimisation problem minimising the error induced by pruning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3475</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3475</id><created>2012-03-15</created><authors><author><keyname>Daniusis</keyname><forenames>Povilas</forenames></author><author><keyname>Janzing</keyname><forenames>Dominik</forenames></author><author><keyname>Mooij</keyname><forenames>Joris</forenames></author><author><keyname>Zscheischler</keyname><forenames>Jakob</forenames></author><author><keyname>Steudel</keyname><forenames>Bastian</forenames></author><author><keyname>Zhang</keyname><forenames>Kun</forenames></author><author><keyname>Schoelkopf</keyname><forenames>Bernhard</forenames></author></authors><title>Inferring deterministic causal relations</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</comments><proxy>auai</proxy><report-no>UAI-P-2010-PG-143-150</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider two variables that are related to each other by an invertible
function. While it has previously been shown that the dependence structure of
the noise can provide hints to determine which of the two variables is the
cause, we presently show that even in the deterministic (noise-free) case,
there are asymmetries that can be exploited for causal inference. Our method is
based on the idea that if the function and the probability density of the cause
are chosen independently, then the distribution of the effect will, in a
certain sense, depend on the function. We provide a theoretical analysis of
this method, showing that it also works in the low noise regime, and link it to
information geometry. We report strong empirical results on various real-world
data sets from different domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3476</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3476</id><created>2012-03-15</created><authors><author><keyname>Elidan</keyname><forenames>Gal</forenames></author></authors><title>Inference-less Density Estimation using Copula Bayesian Networks</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</comments><proxy>auai</proxy><report-no>UAI-P-2010-PG-151-159</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider learning continuous probabilistic graphical models in the face of
missing data. For non-Gaussian models, learning the parameters and structure of
such models depends on our ability to perform efficient inference, and can be
prohibitive even for relatively modest domains. Recently, we introduced the
Copula Bayesian Network (CBN) density model - a flexible framework that
captures complex high-dimensional dependency structures while offering direct
control over the univariate marginals, leading to improved generalization. In
this work we show that the CBN model also offers significant computational
advantages when training data is partially observed. Concretely, we leverage on
the specialized form of the model to derive a computationally amenable learning
objective that is a lower bound on the log-likelihood function. Importantly,
our energy-like bound circumvents the need for costly inference of an auxiliary
distribution, thus facilitating practical learning of highdimensional
densities. We demonstrate the effectiveness of our approach for learning the
structure and parameters of a CBN model for two reallife continuous domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3477</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3477</id><created>2012-03-15</created><authors><author><keyname>Erez</keyname><forenames>Tom</forenames></author><author><keyname>Smart</keyname><forenames>William D.</forenames></author></authors><title>A Scalable Method for Solving High-Dimensional Continuous POMDPs Using
  Local Approximation</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</comments><proxy>auai</proxy><report-no>UAI-P-2010-PG-160-167</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Partially-Observable Markov Decision Processes (POMDPs) are typically solved
by finding an approximate global solution to a corresponding belief-MDP. In
this paper, we offer a new planning algorithm for POMDPs with continuous state,
action and observation spaces. Since such domains have an inherent notion of
locality, we can find an approximate solution using local optimization methods.
We parameterize the belief distribution as a Gaussian mixture, and use the
Extended Kalman Filter (EKF) to approximate the belief update. Since the EKF is
a first-order filter, we can marginalize over the observations analytically. By
using feedback control and state estimation during policy execution, we recover
a behavior that is effectively conditioned on incoming observations despite the
unconditioned planning. Local optimization provides no guarantees of global
optimality, but it allows us to tackle domains that are at least an order of
magnitude larger than the current state-of-the-art. We demonstrate the
scalability of our algorithm by considering a simulated hand-eye coordination
domain with 16 continuous state dimensions and 6 continuous action dimensions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3478</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3478</id><created>2012-03-15</created><authors><author><keyname>Ermon</keyname><forenames>Stefano</forenames></author><author><keyname>Conrad</keyname><forenames>Jon</forenames></author><author><keyname>Gomes</keyname><forenames>Carla P.</forenames></author><author><keyname>Selman</keyname><forenames>Bart</forenames></author></authors><title>Playing games against nature: optimal policies for renewable resource
  allocation</title><categories>cs.AI cs.GT</categories><comments>Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</comments><proxy>auai</proxy><report-no>UAI-P-2010-PG-168-176</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce a class of Markov decision processes that arise as
a natural model for many renewable resource allocation problems. Upon extending
results from the inventory control literature, we prove that they admit a
closed form solution and we show how to exploit this structure to speed up its
computation. We consider the application of the proposed framework to several
problems arising in very different domains, and as part of the ongoing effort
in the emerging field of Computational Sustainability we discuss in detail its
application to the Northern Pacific Halibut marine fishery. Our approach is
applied to a model based on real world data, obtaining a policy with a
guaranteed lower bound on the utility function that is structurally very
different from the one currently employed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3479</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3479</id><created>2012-03-15</created><authors><author><keyname>Evans</keyname><forenames>Robin J.</forenames></author><author><keyname>Richardson</keyname><forenames>Thomas S.</forenames></author></authors><title>Maximum likelihood fitting of acyclic directed mixed graphs to binary
  data</title><categories>stat.ME cs.AI</categories><comments>Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</comments><proxy>auai</proxy><report-no>UAI-P-2010-PG-177-184</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Acyclic directed mixed graphs, also known as semi-Markov models represent the
conditional independence structure induced on an observed margin by a DAG model
with latent variables. In this paper we present the first method for fitting
these models to binary data using maximum likelihood estimation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3480</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3480</id><created>2012-03-15</created><authors><author><keyname>Gao</keyname><forenames>Xi Alice</forenames></author><author><keyname>Pfeffer</keyname><forenames>Avi</forenames></author></authors><title>Learning Game Representations from Data Using Rationality Constraints</title><categories>cs.GT cs.AI</categories><comments>Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</comments><proxy>auai</proxy><report-no>UAI-P-2010-PG-185-192</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While game theory is widely used to model strategic interactions, a natural
question is where do the game representations come from? One answer is to learn
the representations from data. If one wants to learn both the payoffs and the
players' strategies, a naive approach is to learn them both directly from the
data. This approach ignores the fact the players might be playing reasonably
good strategies, so there is a connection between the strategies and the data.
The main contribution of this paper is to make this connection while learning.
We formulate the learning problem as a weighted constraint satisfaction
problem, including constraints both for the fit of the payoffs and strategies
to the data and the fit of the strategies to the payoffs. We use quantal
response equilibrium as our notion of rationality for quantifying the latter
fit. Our results show that incorporating rationality constraints can improve
learning when the amount of data is limited.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3481</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3481</id><created>2012-03-15</created><authors><author><keyname>Glaubius</keyname><forenames>Robert</forenames></author><author><keyname>Tidwell</keyname><forenames>Terry</forenames></author><author><keyname>Gill</keyname><forenames>Christopher</forenames></author><author><keyname>Smart</keyname><forenames>William D.</forenames></author></authors><title>Real-Time Scheduling via Reinforcement Learning</title><categories>cs.LG cs.AI stat.ML</categories><comments>Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</comments><proxy>auai</proxy><report-no>UAI-P-2010-PG-201-209</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cyber-physical systems, such as mobile robots, must respond adaptively to
dynamic operating conditions. Effective operation of these systems requires
that sensing and actuation tasks are performed in a timely manner.
Additionally, execution of mission specific tasks such as imaging a room must
be balanced against the need to perform more general tasks such as obstacle
avoidance. This problem has been addressed by maintaining relative utilization
of shared resources among tasks near a user-specified target level. Producing
optimal scheduling strategies requires complete prior knowledge of task
behavior, which is unlikely to be available in practice. Instead, suitable
scheduling strategies must be learned online through interaction with the
system. We consider the sample complexity of reinforcement learning in this
domain, and demonstrate that while the problem state space is countably
infinite, we may leverage the problem's structure to guarantee efficient
learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3482</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3482</id><created>2012-03-15</created><authors><author><keyname>Gogate</keyname><forenames>Vibhav</forenames></author><author><keyname>Domingos</keyname><forenames>Pedro</forenames></author></authors><title>Formula-Based Probabilistic Inference</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</comments><proxy>auai</proxy><report-no>UAI-P-2010-PG-210-219</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computing the probability of a formula given the probabilities or weights
associated with other formulas is a natural extension of logical inference to
the probabilistic setting. Surprisingly, this problem has received little
attention in the literature to date, particularly considering that it includes
many standard inference problems as special cases. In this paper, we propose
two algorithms for this problem: formula decomposition and conditioning, which
is an exact method, and formula importance sampling, which is an approximate
method. The latter is, to our knowledge, the first application of model
counting to approximate probabilistic inference. Unlike conventional
variable-based algorithms, our algorithms work in the dual realm of logical
formulas. Theoretically, we show that our algorithms can greatly improve
efficiency by exploiting the structural information in the formulas.
Empirically, we show that they are indeed quite powerful, often achieving
substantial performance gains over state-of-the-art schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3483</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3483</id><created>2012-03-15</created><authors><author><keyname>Gupta</keyname><forenames>Mithun Das</forenames></author><author><keyname>Huang</keyname><forenames>Thomas S.</forenames></author></authors><title>Regularized Maximum Likelihood for Intrinsic Dimension Estimation</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</comments><proxy>auai</proxy><report-no>UAI-P-2010-PG-220-227</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new method for estimating the intrinsic dimension of a dataset
by applying the principle of regularized maximum likelihood to the distances
between close neighbors. We propose a regularization scheme which is motivated
by divergence minimization principles. We derive the estimator by a Poisson
process approximation, argue about its convergence properties and apply it to a
number of simulated and real datasets. We also show it has the best overall
performance compared with two other intrinsic dimension estimators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3484</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3484</id><created>2012-03-15</created><authors><author><keyname>Hamze</keyname><forenames>Firas</forenames></author><author><keyname>de Freitas</keyname><forenames>Nando</forenames></author></authors><title>Intracluster Moves for Constrained Discrete-Space MCMC</title><categories>stat.CO cs.AI</categories><comments>Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</comments><proxy>auai</proxy><report-no>UAI-P-2010-PG-236-243</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of sampling from binary distributions with
constraints. In particular, it proposes an MCMC method to draw samples from a
distribution of the set of all states at a specified distance from some
reference state. For example, when the reference state is the vector of zeros,
the algorithm can draw samples from a binary distribution with a constraint on
the number of active variables, say the number of 1's. We motivate the need for
this algorithm with examples from statistical physics and probabilistic
inference. Unlike previous algorithms proposed to sample from binary
distributions with these constraints, the new algorithm allows for large moves
in state space and tends to propose them such that they are energetically
favourable. The algorithm is demonstrated on three Boltzmann machines of
varying difficulty: A ferromagnetic Ising model (with positive potentials), a
restricted Boltzmann machine with learned Gabor-like filters as potentials, and
a challenging three-dimensional spin-glass (with positive and negative
potentials).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3485</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3485</id><created>2012-03-15</created><authors><author><keyname>Johnson</keyname><forenames>Matthew J.</forenames></author><author><keyname>Willsky</keyname><forenames>Alan</forenames></author></authors><title>The Hierarchical Dirichlet Process Hidden Semi-Markov Model</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</comments><proxy>auai</proxy><report-no>UAI-P-2010-PG-252-259</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is much interest in the Hierarchical Dirichlet Process Hidden Markov
Model (HDP-HMM) as a natural Bayesian nonparametric extension of the
traditional HMM. However, in many settings the HDP-HMM's strict Markovian
constraints are undesirable, particularly if we wish to learn or encode
non-geometric state durations. We can extend the HDP-HMM to capture such
structure by drawing upon explicit-duration semi-Markovianity, which has been
developed in the parametric setting to allow construction of highly
interpretable models that admit natural prior information on state durations.
In this paper we introduce the explicitduration HDP-HSMM and develop posterior
sampling algorithms for efficient inference in both the direct-assignment and
weak-limit approximation settings. We demonstrate the utility of the model and
our inference methods on synthetic data as well as experiments on a speaker
diarization problem and an example of learning the patterns in Morse code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3486</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3486</id><created>2012-03-15</created><authors><author><keyname>Kapicioglu</keyname><forenames>Berk</forenames></author><author><keyname>Schapire</keyname><forenames>Robert E.</forenames></author><author><keyname>Wikelski</keyname><forenames>Martin</forenames></author><author><keyname>Broderick</keyname><forenames>Tamara</forenames></author></authors><title>Combining Spatial and Telemetric Features for Learning Animal Movement
  Models</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</comments><proxy>auai</proxy><report-no>UAI-P-2010-PG-260-267</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new graphical model for tracking radio-tagged animals and
learning their movement patterns. The model provides a principled way to
combine radio telemetry data with an arbitrary set of userdefined, spatial
features. We describe an efficient stochastic gradient algorithm for fitting
model parameters to data and demonstrate its effectiveness via asymptotic
analysis and synthetic experiments. We also apply our model to real datasets,
and show that it outperforms the most popular radio telemetry software package
used in ecology. We conclude that integration of different data sources under a
single statistical framework, coupled with appropriate parameter and state
estimation procedures, produces both accurate location estimates and an
interpretable statistical model of animal movement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3487</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3487</id><created>2012-03-15</created><authors><author><keyname>Kask</keyname><forenames>Kalev</forenames></author><author><keyname>Dechter</keyname><forenames>Rina</forenames></author><author><keyname>Gelfand</keyname><forenames>Andrew E.</forenames></author></authors><title>BEEM : Bucket Elimination with External Memory</title><categories>cs.AI cs.DS</categories><comments>Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</comments><proxy>auai</proxy><report-no>UAI-P-2010-PG-268-276</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A major limitation of exact inference algorithms for probabilistic graphical
models is their extensive memory usage, which often puts real-world problems
out of their reach. In this paper we show how we can extend inference
algorithms, particularly Bucket Elimination, a special case of cluster (join)
tree decomposition, to utilize disk memory. We provide the underlying ideas and
show promising empirical results of exactly solving large problems not solvable
before.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3488</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3488</id><created>2012-03-15</created><authors><author><keyname>Kelly</keyname><forenames>Kevin T.</forenames></author><author><keyname>Mayo-Wilson</keyname><forenames>Conor</forenames></author></authors><title>Causal Conclusions that Flip Repeatedly and Their Justification</title><categories>cs.LG cs.AI stat.ML</categories><comments>Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</comments><proxy>auai</proxy><report-no>UAI-P-2010-PG-277-285</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the past two decades, several consistent procedures have been designed
to infer causal conclusions from observational data. We prove that if the true
causal network might be an arbitrary, linear Gaussian network or a discrete
Bayes network, then every unambiguous causal conclusion produced by a
consistent method from non-experimental data is subject to reversal as the
sample size increases any finite number of times. That result, called the
causal flipping theorem, extends prior results to the effect that causal
discovery cannot be reliable on a given sample size. We argue that since
repeated flipping of causal conclusions is unavoidable in principle for
consistent methods, the best possible discovery methods are consistent methods
that retract their earlier conclusions no more than necessary. A series of
simulations of various methods across a wide range of sample sizes illustrates
concretely both the theorem and the principle of comparing methods in terms of
retractions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3489</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3489</id><created>2012-03-15</created><authors><author><keyname>Klami</keyname><forenames>Arto</forenames></author><author><keyname>Virtanen</keyname><forenames>Seppo</forenames></author><author><keyname>Kaski</keyname><forenames>Samuel</forenames></author></authors><title>Bayesian exponential family projections for coupled data sources</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</comments><proxy>auai</proxy><report-no>UAI-P-2010-PG-286-293</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Exponential family extensions of principal component analysis (EPCA) have
received a considerable amount of attention in recent years, demonstrating the
growing need for basic modeling tools that do not assume the squared loss or
Gaussian distribution. We extend the EPCA model toolbox by presenting the first
exponential family multi-view learning methods of the partial least squares and
canonical correlation analysis, based on a unified representation of EPCA as
matrix factorization of the natural parameters of exponential family. The
models are based on a new family of priors that are generally usable for all
such factorizations. We also introduce new inference strategies, and
demonstrate how the methods outperform earlier ones when the Gaussianity
assumption does not hold.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3490</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3490</id><created>2012-03-15</created><authors><author><keyname>Kumar</keyname><forenames>Akshat</forenames></author><author><keyname>Zilberstein</keyname><forenames>Shlomo</forenames></author></authors><title>Anytime Planning for Decentralized POMDPs using Expectation Maximization</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</comments><proxy>auai</proxy><report-no>UAI-P-2010-PG-294-301</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Decentralized POMDPs provide an expressive framework for multi-agent
sequential decision making. While fnite-horizon DECPOMDPs have enjoyed
signifcant success, progress remains slow for the infnite-horizon case mainly
due to the inherent complexity of optimizing stochastic controllers
representing agent policies. We present a promising new class of algorithms for
the infnite-horizon case, which recasts the optimization problem as inference
in a mixture of DBNs. An attractive feature of this approach is the
straightforward adoption of existing inference techniques in DBNs for solving
DEC-POMDPs and supporting richer representations such as factored or continuous
states and actions. We also derive the Expectation Maximization (EM) algorithm
to optimize the joint policy represented as DBNs. Experiments on benchmark
domains show that EM compares favorably against the state-of-the-art solvers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3491</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3491</id><created>2012-03-15</created><authors><author><keyname>Li</keyname><forenames>Ping</forenames></author></authors><title>Robust LogitBoost and Adaptive Base Class (ABC) LogitBoost</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</comments><proxy>auai</proxy><report-no>UAI-P-2010-PG-302-311</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Logitboost is an influential boosting algorithm for classification. In this
paper, we develop robust logitboost to provide an explicit formulation of
tree-split criterion for building weak learners (regression trees) for
logitboost. This formulation leads to a numerically stable implementation of
logitboost. We then propose abc-logitboost for multi-class classification, by
combining robust logitboost with the prior work of abc-boost. Previously,
abc-boost was implemented as abc-mart using the mart algorithm. Our extensive
experiments on multi-class classification compare four algorithms: mart,
abcmart, (robust) logitboost, and abc-logitboost, and demonstrate the
superiority of abc-logitboost. Comparisons with other learning methods
including SVM and deep learning are also available through prior publications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3492</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3492</id><created>2012-03-15</created><authors><author><keyname>Li</keyname><forenames>Ping</forenames></author><author><keyname>Mahoney</keyname><forenames>Michael W.</forenames></author><author><keyname>She</keyname><forenames>Yiyuan</forenames></author></authors><title>Approximating Higher-Order Distances Using Random Projections</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</comments><proxy>auai</proxy><report-no>UAI-P-2010-PG-312-321</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a simple method and relevant theoretical analysis for efficiently
estimating higher-order lp distances. While the analysis mainly focuses on l4,
our methodology extends naturally to p = 6,8,10..., (i.e., when p is even).
Distance-based methods are popular in machine learning. In large-scale
applications, storing, computing, and retrieving the distances can be both
space and time prohibitive. Efficient algorithms exist for estimating lp
distances if 0 &lt; p &lt;= 2. The task for p &gt; 2 is known to be difficult. Our work
partially fills this gap.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3493</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3493</id><created>2012-03-15</created><authors><author><keyname>Li</keyname><forenames>Yijing</forenames></author><author><keyname>Shenoy</keyname><forenames>Prakash P.</forenames></author></authors><title>Solving Hybrid Influence Diagrams with Deterministic Variables</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</comments><proxy>auai</proxy><report-no>UAI-P-2010-PG-322-331</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a framework and an algorithm for solving hybrid influence
diagrams with discrete, continuous, and deterministic chance variables, and
discrete and continuous decision variables. A continuous chance variable in an
influence diagram is said to be deterministic if its conditional distributions
have zero variances. The solution algorithm is an extension of Shenoy's fusion
algorithm for discrete influence diagrams. We describe an extended
Shenoy-Shafer architecture for propagation of discrete, continuous, and utility
potentials in hybrid influence diagrams that include deterministic chance
variables. The algorithm and framework are illustrated by solving two small
examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3494</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3494</id><created>2012-03-15</created><authors><author><keyname>Liu</keyname><forenames>Qiang</forenames></author><author><keyname>Ihler</keyname><forenames>Alexander T.</forenames></author></authors><title>Negative Tree Reweighted Belief Propagation</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</comments><proxy>auai</proxy><report-no>UAI-P-2010-PG-332-339</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new class of lower bounds on the log partition function of a
Markov random field which makes use of a reversed Jensen's inequality. In
particular, our method approximates the intractable distribution using a linear
combination of spanning trees with negative weights. This technique is a
lower-bound counterpart to the tree-reweighted belief propagation algorithm,
which uses a convex combination of spanning trees with positive weights to
provide corresponding upper bounds. We develop algorithms to optimize and
tighten the lower bounds over the non-convex set of valid parameter values. Our
algorithm generalizes mean field approaches (including naive and structured
mean field approximations), which it includes as a limiting case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3495</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3495</id><created>2012-03-15</created><authors><author><keyname>Mao</keyname><forenames>Qi</forenames></author><author><keyname>Tsang</keyname><forenames>Ivor W.</forenames></author></authors><title>Parameter-Free Spectral Kernel Learning</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</comments><proxy>auai</proxy><report-no>UAI-P-2010-PG-350-357</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to the growing ubiquity of unlabeled data, learning with unlabeled data
is attracting increasing attention in machine learning. In this paper, we
propose a novel semi-supervised kernel learning method which can seamlessly
combine manifold structure of unlabeled data and Regularized Least-Squares
(RLS) to learn a new kernel. Interestingly, the new kernel matrix can be
obtained analytically with the use of spectral decomposition of graph Laplacian
matrix. Hence, the proposed algorithm does not require any numerical
optimization solvers. Moreover, by maximizing kernel target alignment on
labeled data, we can also learn model parameters automatically with a
closed-form solution. For a given graph Laplacian matrix, our proposed method
does not need to tune any model parameter including the tradeoff parameter in
RLS and the balance parameter for unlabeled data. Extensive experiments on ten
benchmark datasets show that our proposed two-stage parameter-free spectral
kernel learning algorithm can obtain comparable performance with fine-tuned
manifold regularization methods in transductive setting, and outperform
multiple kernel learning in supervised setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3496</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3496</id><created>2012-03-15</created><authors><author><keyname>Meila</keyname><forenames>Marina</forenames></author><author><keyname>Chen</keyname><forenames>Harr</forenames></author></authors><title>Dirichlet Process Mixtures of Generalized Mallows Models</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</comments><proxy>auai</proxy><report-no>UAI-P-2010-PG-358-367</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a Dirichlet process mixture model over discrete incomplete
rankings and study two Gibbs sampling inference techniques for estimating
posterior clusterings. The first approach uses a slice sampling subcomponent
for estimating cluster parameters. The second approach marginalizes out several
cluster parameters by taking advantage of approximations to the conditional
posteriors. We empirically demonstrate (1) the effectiveness of this
approximation for improving convergence, (2) the benefits of the Dirichlet
process model over alternative clustering techniques for ranked data, and (3)
the applicability of the approach to exploring large realworld ranking
datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3497</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3497</id><created>2012-03-15</created><authors><author><keyname>Morimura</keyname><forenames>Tetsuro</forenames></author><author><keyname>Sugiyama</keyname><forenames>Masashi</forenames></author><author><keyname>Kashima</keyname><forenames>Hisashi</forenames></author><author><keyname>Hachiya</keyname><forenames>Hirotaka</forenames></author><author><keyname>Tanaka</keyname><forenames>Toshiyuki</forenames></author></authors><title>Parametric Return Density Estimation for Reinforcement Learning</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</comments><proxy>auai</proxy><report-no>UAI-P-2010-PG-368-375</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most conventional Reinforcement Learning (RL) algorithms aim to optimize
decision-making rules in terms of the expected returns. However, especially for
risk management purposes, other risk-sensitive criteria such as the
value-at-risk or the expected shortfall are sometimes preferred in real
applications. Here, we describe a parametric method for estimating density of
the returns, which allows us to handle various criteria in a unified manner. We
first extend the Bellman equation for the conditional expected return to cover
a conditional probability density of the returns. Then we derive an extension
of the TD-learning algorithm for estimating the return densities in an unknown
environment. As test instances, several parametric density estimation
algorithms are presented for the Gaussian, Laplace, and skewed Laplace
distributions. We show that these algorithms lead to risk-sensitive as well as
robust RL paradigms through numerical experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3498</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3498</id><created>2012-03-15</created><authors><author><keyname>de Cote</keyname><forenames>Enrique Munoz</forenames></author><author><keyname>Chapman</keyname><forenames>Archie C.</forenames></author><author><keyname>Sykulski</keyname><forenames>Adam M.</forenames></author><author><keyname>Jennings</keyname><forenames>Nicholas R.</forenames></author></authors><title>Automated Planning in Repeated Adversarial Games</title><categories>cs.GT cs.AI</categories><comments>Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</comments><proxy>auai</proxy><report-no>UAI-P-2010-PG-376-383</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Game theory's prescriptive power typically relies on full rationality and/or
self-play interactions. In contrast, this work sets aside these fundamental
premises and focuses instead on heterogeneous autonomous interactions between
two or more agents. Specifically, we introduce a new and concise representation
for repeated adversarial (constant-sum) games that highlight the necessary
features that enable an automated planing agent to reason about how to score
above the game's Nash equilibrium, when facing heterogeneous adversaries. To
this end, we present TeamUP, a model-based RL algorithm designed for learning
and planning such an abstraction. In essence, it is somewhat similar to R-max
with a cleverly engineered reward shaping that treats exploration as an
adversarial optimization problem. In practice, it attempts to find an ally with
which to tacitly collude (in more than two-player games) and then collaborates
on a joint plan of actions that can consistently score a high utility in
adversarial repeated games. We use the inaugural Lemonade Stand Game Tournament
to demonstrate the effectiveness of our approach, and find that TeamUP is the
best performing agent, demoting the Tournament's actual winning strategy into
second place. In our experimental analysis, we show hat our strategy
successfully and consistently builds collaborations with many different
heterogeneous (and sometimes very sophisticated) adversaries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3499</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3499</id><created>2012-03-15</created><authors><author><keyname>Niepert</keyname><forenames>Mathias</forenames></author></authors><title>A Delayed Column Generation Strategy for Exact k-Bounded MAP Inference
  in Markov Logic Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</comments><proxy>auai</proxy><report-no>UAI-P-2010-PG-384-391</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper introduces k-bounded MAP inference, a parameterization of MAP
inference in Markov logic networks. k-Bounded MAP states are MAP states with at
most k active ground atoms of hidden (non-evidence) predicates. We present a
novel delayed column generation algorithm and provide empirical evidence that
the algorithm efficiently computes k-bounded MAP states for meaningful
real-world graph matching problems. The underlying idea is that, instead of
solving one large optimization problem, it is often more efficient to tackle
several small ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3500</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3500</id><created>2012-03-15</created><authors><author><keyname>Omar</keyname><forenames>Farheen</forenames></author><author><keyname>Sinn</keyname><forenames>Mathieu</forenames></author><author><keyname>Truszkowski</keyname><forenames>Jakub</forenames></author><author><keyname>Poupart</keyname><forenames>Pascal</forenames></author><author><keyname>Tung</keyname><forenames>James</forenames></author><author><keyname>Caine</keyname><forenames>Allen</forenames></author></authors><title>Comparative Analysis of Probabilistic Models for Activity Recognition
  with an Instrumented Walker</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</comments><proxy>auai</proxy><report-no>UAI-P-2010-PG-392-400</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rollating walkers are popular mobility aids used by older adults to improve
balance control. There is a need to automatically recognize the activities
performed by walker users to better understand activity patterns, mobility
issues and the context in which falls are more likely to happen. We design and
compare several techniques to recognize walker related activities. A
comprehensive evaluation with control subjects and walker users from a
retirement community is presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3501</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3501</id><created>2012-03-15</created><authors><author><keyname>Ordyniak</keyname><forenames>Sebastian</forenames></author><author><keyname>Szeider</keyname><forenames>Stefan</forenames></author></authors><title>Algorithms and Complexity Results for Exact Bayesian Structure Learning</title><categories>cs.LG cs.DS stat.ML</categories><comments>Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</comments><proxy>auai</proxy><report-no>UAI-P-2010-PG-401-408</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bayesian structure learning is the NP-hard problem of discovering a Bayesian
network that optimally represents a given set of training data. In this paper
we study the computational worst-case complexity of exact Bayesian structure
learning under graph theoretic restrictions on the super-structure. The
super-structure (a concept introduced by Perrier, Imoto, and Miyano, JMLR 2008)
is an undirected graph that contains as subgraphs the skeletons of solution
networks. Our results apply to several variants of score-based Bayesian
structure learning where the score of a network decomposes into local scores of
its nodes. Results: We show that exact Bayesian structure learning can be
carried out in non-uniform polynomial time if the super-structure has bounded
treewidth and in linear time if in addition the super-structure has bounded
maximum degree. We complement this with a number of hardness results. We show
that both restrictions (treewidth and degree) are essential and cannot be
dropped without loosing uniform polynomial time tractability (subject to a
complexity-theoretic assumption). Furthermore, we show that the restrictions
remain essential if we do not search for a globally optimal network but we aim
to improve a given network by means of at most k arc additions, arc deletions,
or arc reversals (k-neighborhood local search).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3502</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3502</id><created>2012-03-15</created><authors><author><keyname>Ottosen</keyname><forenames>Thorsten J.</forenames></author><author><keyname>Jensen</keyname><forenames>Finn Verner</forenames></author></authors><title>The Cost of Troubleshooting Cost Clusters with Inside Information</title><categories>cs.AI cs.DS</categories><comments>Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</comments><proxy>auai</proxy><report-no>UAI-P-2010-PG-409-416</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Decision theoretical troubleshooting is about minimizing the expected cost of
solving a certain problem like repairing a complicated man-made device. In this
paper we consider situations where you have to take apart some of the device to
get access to certain clusters and actions. Specifically, we investigate
troubleshooting with independent actions in a tree of clusters where actions
inside a cluster cannot be performed before the cluster is opened. The problem
is non-trivial because there is a cost associated with opening and closing a
cluster. Troubleshooting with independent actions and no clusters can be solved
in O(n lg n) time (n being the number of actions) by the well-known &quot;P-over-C&quot;
algorithm due to Kadane and Simon, but an efficient and optimal algorithm for a
tree cluster model has not yet been found. In this paper we describe a
&quot;bottom-up P-over-C&quot; O(n lg n) time algorithm and show that it is optimal when
the clusters do not need to be closed to test whether the actions solved the
problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3503</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3503</id><created>2012-03-15</created><authors><author><keyname>Pearl</keyname><forenames>Judea</forenames></author></authors><title>On a Class of Bias-Amplifying Variables that Endanger Effect Estimates</title><categories>stat.ME cs.AI</categories><comments>Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</comments><proxy>auai</proxy><report-no>UAI-P-2010-PG-417-424</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This note deals with a class of variables that, if conditioned on, tends to
amplify confounding bias in the analysis of causal effects. This class,
independently discovered by Bhattacharya and Vogt (2007) and Wooldridge (2009),
includes instrumental variables and variables that have greater influence on
treatment selection than on the outcome. We offer a simple derivation and an
intuitive explanation of this phenomenon and then extend the analysis to non
linear models. We show that: 1. the bias-amplifying potential of instrumental
variables extends over to non-linear models, though not as sweepingly as in
linear models; 2. in non-linear models, conditioning on instrumental variables
may introduce new bias where none existed before; 3. in both linear and
non-linear models, instrumental variables have no effect on selection-induced
bias.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3504</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3504</id><created>2012-03-15</created><authors><author><keyname>Pearl</keyname><forenames>Judea</forenames></author></authors><title>On Measurement Bias in Causal Inference</title><categories>stat.ME cs.AI</categories><comments>Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</comments><proxy>auai</proxy><report-no>UAI-P-2010-PG-425-432</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of measurement errors in causal inference
and highlights several algebraic and graphical methods for eliminating
systematic bias induced by such errors. In particulars, the paper discusses the
control of partially observable confounders in parametric and non parametric
models and the computational problem of obtaining bias-free effect estimates in
such models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3505</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3505</id><created>2012-03-15</created><authors><author><keyname>Pearl</keyname><forenames>Judea</forenames></author><author><keyname>Paz</keyname><forenames>Azaria</forenames></author></authors><title>Confounding Equivalence in Causal Inference</title><categories>stat.ME cs.AI</categories><comments>Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</comments><proxy>auai</proxy><report-no>UAI-P-2010-PG-433-441</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper provides a simple test for deciding, from a given causal diagram,
whether two sets of variables have the same bias-reducing potential under
adjustment. The test requires that one of the following two conditions holds:
either (1) both sets are admissible (i.e., satisfy the back-door criterion) or
(2) the Markov boundaries surrounding the manipulated variable(s) are identical
in both sets. Applications to covariate selection and model testing are
discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3506</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3506</id><created>2012-03-15</created><authors><author><keyname>Pihlaja</keyname><forenames>Miika</forenames></author><author><keyname>Gutmann</keyname><forenames>Michael</forenames></author><author><keyname>Hyvarinen</keyname><forenames>Aapo</forenames></author></authors><title>A Family of Computationally Efficient and Simple Estimators for
  Unnormalized Statistical Models</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</comments><proxy>auai</proxy><report-no>UAI-P-2010-PG-442-449</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new family of estimators for unnormalized statistical models.
Our family of estimators is parameterized by two nonlinear functions and uses a
single sample from an auxiliary distribution, generalizing Maximum Likelihood
Monte Carlo estimation of Geyer and Thompson (1992). The family is such that we
can estimate the partition function like any other parameter in the model. The
estimation is done by optimizing an algebraically simple, well defined
objective function, which allows for the use of dedicated optimization methods.
We establish consistency of the estimator family and give an expression for the
asymptotic covariance matrix, which enables us to further analyze the influence
of the nonlinearities and the auxiliary density on estimation performance. Some
estimators in our family are particularly stable for a wide range of auxiliary
densities. Interestingly, a specific choice of the nonlinearity establishes a
connection between density estimation and classification by nonlinear logistic
regression. Finally, the optimal amount of auxiliary samples relative to the
given amount of the data is considered from the perspective of computational
efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3507</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3507</id><created>2012-03-15</created><authors><author><keyname>Yuan</keyname><affiliation>Alan</affiliation></author><author><keyname>Qi</keyname></author><author><keyname>Abdel-Gawad</keyname><forenames>Ahmed H.</forenames></author><author><keyname>Minka</keyname><forenames>Thomas P.</forenames></author></authors><title>Sparse-posterior Gaussian Processes for general likelihoods</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</comments><proxy>auai</proxy><report-no>UAI-P-2010-PG-450-457</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gaussian processes (GPs) provide a probabilistic nonparametric representation
of functions in regression, classification, and other problems. Unfortunately,
exact learning with GPs is intractable for large datasets. A variety of
approximate GP methods have been proposed that essentially map the large
dataset into a small set of basis points. Among them, two state-of-the-art
methods are sparse pseudo-input Gaussian process (SPGP) (Snelson and
Ghahramani, 2006) and variablesigma GP (VSGP) Walder et al. (2008), which
generalizes SPGP and allows each basis point to have its own length scale.
However, VSGP was only derived for regression. In this paper, we propose a new
sparse GP framework that uses expectation propagation to directly approximate
general GP likelihoods using a sparse and smooth basis. It includes both SPGP
and VSGP for regression as special cases. Plus as an EP algorithm, it inherits
the ability to process data online. As a particular choice of approximating
family, we blur each basis point with a Gaussian distribution that has a full
covariance matrix representing the data distribution around that basis point;
as a result, we can summarize local data manifold information with a small set
of basis points. Our experiments demonstrate that this framework outperforms
previous GP classification methods on benchmark datasets in terms of minimizing
divergence to the non-sparse GP solution as well as lower misclassification
rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3508</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3508</id><created>2012-03-15</created><authors><author><keyname>Qi</keyname><forenames>Guilin</forenames></author><author><keyname>Du</keyname><forenames>Jianfeng</forenames></author><author><keyname>Liu</keyname><forenames>Weiru</forenames></author><author><keyname>Bell</keyname><forenames>David A.</forenames></author></authors><title>Merging Knowledge Bases in Possibilistic Logic by Lexicographic
  Aggregation</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</comments><proxy>auai</proxy><report-no>UAI-P-2010-PG-458-465</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Belief merging is an important but difficult problem in Artificial
Intelligence, especially when sources of information are pervaded with
uncertainty. Many merging operators have been proposed to deal with this
problem in possibilistic logic, a weighted logic which is powerful for handling
inconsistency and deal- ing with uncertainty. They often result in a
possibilistic knowledge base which is a set of weighted formulas. Although
possibilistic logic is inconsistency tolerant, it suers from the well-known
&quot;drowning effect&quot;. Therefore, we may still want to obtain a consistent possi-
bilistic knowledge base as the result of merg- ing. In such a case, we argue
that it is not always necessary to keep weighted informa- tion after merging.
In this paper, we define a merging operator that maps a set of pos- sibilistic
knowledge bases and a formula rep- resenting the integrity constraints to a
clas- sical knowledge base by using lexicographic ordering. We show that it
satisfies nine pos- tulates that generalize basic postulates for propositional
merging given in [11]. These postulates capture the principle of minimal change
in some sense. We then provide an algorithm for generating the resulting knowl-
edge base of our merging operator. Finally, we discuss the compatibility of our
merging operator with propositional merging and es- tablish the advantage of
our merging opera- tor over existing semantic merging operators in the
propositional case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3509</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3509</id><created>2012-03-15</created><authors><author><keyname>Quaeghebeur</keyname><forenames>Erik</forenames></author></authors><title>Characterizing the Set of Coherent Lower Previsions with a Finite Number
  of Constraints or Vertices</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</comments><proxy>auai</proxy><report-no>UAI-P-2010-PG-466-473</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The standard coherence criterion for lower previsions is expressed using an
infinite number of linear constraints. For lower previsions that are
essentially defined on some finite set of gambles on a finite possibility
space, we present a reformulation of this criterion that only uses a finite
number of constraints. Any such lower prevision is coherent if it lies within
the convex polytope defined by these constraints. The vertices of this polytope
are the extreme coherent lower previsions for the given set of gambles. Our
reformulation makes it possible to compute them. We show how this is done and
illustrate the procedure and its results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3510</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3510</id><created>2012-03-15</created><authors><author><keyname>Ramati</keyname><forenames>Michael</forenames></author><author><keyname>Shahar</keyname><forenames>Yuval</forenames></author></authors><title>Irregular-Time Bayesian Networks</title><categories>cs.AI cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</comments><proxy>auai</proxy><report-no>UAI-P-2010-PG-484-491</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many fields observations are performed irregularly along time, due to
either measurement limitations or lack of a constant immanent rate. While
discrete-time Markov models (as Dynamic Bayesian Networks) introduce either
inefficient computation or an information loss to reasoning about such
processes, continuous-time Markov models assume either a discrete state space
(as Continuous-Time Bayesian Networks), or a flat continuous state space (as
stochastic differential equations). To address these problems, we present a new
modeling class called Irregular-Time Bayesian Networks (ITBNs), generalizing
Dynamic Bayesian Networks, allowing substantially more compact representations,
and increasing the expressivity of the temporal dynamics. In addition, a
globally optimal solution is guaranteed when learning temporal systems,
provided that they are fully observed at the same irregularly spaced
time-points, and a semiparametric subclass of ITBNs is introduced to allow
further adaptation to the irregular nature of the available data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3511</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3511</id><created>2012-03-15</created><authors><author><keyname>Riedel</keyname><forenames>Sebastian</forenames></author><author><keyname>Smith</keyname><forenames>David A.</forenames></author><author><keyname>McCallum</keyname><forenames>Andrew</forenames></author></authors><title>Inference by Minimizing Size, Divergence, or their Sum</title><categories>cs.LG cs.CL stat.ML</categories><comments>Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</comments><proxy>auai</proxy><report-no>UAI-P-2010-PG-492-499</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We speed up marginal inference by ignoring factors that do not significantly
contribute to overall accuracy. In order to pick a suitable subset of factors
to ignore, we propose three schemes: minimizing the number of model factors
under a bound on the KL divergence between pruned and full models; minimizing
the KL divergence under a bound on factor count; and minimizing the weighted
sum of KL divergence and factor count. All three problems are solved using an
approximation of the KL divergence than can be calculated in terms of marginals
computed on a simple seed graph. Applied to synthetic image denoising and to
three different types of NLP parsing models, this technique performs marginal
inference up to 11 times faster than loopy BP, with graph sizes reduced up to
98%-at comparable error in marginals and parsing accuracy. We also show that
minimizing the weighted sum of divergence and size is substantially faster than
minimizing either of the other objectives based on the approximation to
divergence presented here.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3512</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3512</id><created>2012-03-15</created><authors><author><keyname>Russell</keyname><forenames>Chris</forenames></author><author><keyname>Ladicky</keyname><forenames>L'ubor</forenames></author><author><keyname>Kohli</keyname><forenames>Pushmeet</forenames></author><author><keyname>Torr</keyname><forenames>Philip H. S.</forenames></author></authors><title>Exact and Approximate Inference in Associative Hierarchical Networks
  using Graph Cuts</title><categories>cs.AI cs.CV</categories><comments>Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</comments><proxy>auai</proxy><report-no>UAI-P-2010-PG-501-508</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Markov Networks are widely used through out computer vision and machine
learning. An important subclass are the Associative Markov Networks which are
used in a wide variety of applications. For these networks a good approximate
minimum cost solution can be found efficiently using graph cut based move
making algorithms such as alpha-expansion. Recently a related model has been
proposed, the associative hierarchical network, which provides a natural
generalisation of the Associative Markov Network for higher order cliques (i.e.
clique size greater than two). This method provides a good model for object
class segmentation problem in computer vision. Within this paper we briefly
describe the associative hierarchical network and provide a computationally
efficient method for approximate inference based on graph cuts. Our method
performs well for networks containing hundreds of thousand of variables, and
higher order potentials are defined over cliques containing tens of thousands
of variables. Due to the size of these problems standard linear programming
techniques are inapplicable. We show that our method has a bound of 4 for the
solution of general associative hierarchical network with arbitrary clique size
noting that few results on bounds exist for the solution of labelling of Markov
Networks with higher order cliques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3513</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3513</id><created>2012-03-15</created><authors><author><keyname>Shachter</keyname><forenames>Ross D.</forenames></author><author><keyname>Bhattacharjya</keyname><forenames>Debarun</forenames></author></authors><title>Dynamic programming in in uence diagrams with decision circuits</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</comments><proxy>auai</proxy><report-no>UAI-P-2010-PG-509-516</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Decision circuits perform efficient evaluation of influence diagrams,
building on the ad- vances in arithmetic circuits for belief net- work
inference [Darwiche, 2003; Bhattachar- jya and Shachter, 2007]. We show how
even more compact decision circuits can be con- structed for dynamic
programming in influ- ence diagrams with separable value functions and
conditionally independent subproblems. Once a decision circuit has been
constructed based on the diagram's &quot;global&quot; graphical structure, it can be
compiled to exploit &quot;lo- cal&quot; structure for efficient evaluation and sen-
sitivity analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3514</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3514</id><created>2012-03-15</created><authors><author><keyname>Sheldon</keyname><forenames>Daniel</forenames></author><author><keyname>Dilkina</keyname><forenames>Bistra</forenames></author><author><keyname>Elmachtoub</keyname><forenames>Adam N.</forenames></author><author><keyname>Finseth</keyname><forenames>Ryan</forenames></author><author><keyname>Sabharwal</keyname><forenames>Ashish</forenames></author><author><keyname>Conrad</keyname><forenames>Jon</forenames></author><author><keyname>Gomes</keyname><forenames>Carla P.</forenames></author><author><keyname>Shmoys</keyname><forenames>David</forenames></author><author><keyname>Allen</keyname><forenames>William</forenames></author><author><keyname>Amundsen</keyname><forenames>Ole</forenames></author><author><keyname>Vaughan</keyname><forenames>William</forenames></author></authors><title>Maximizing the Spread of Cascades Using Network Design</title><categories>cs.SI physics.soc-ph</categories><comments>Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</comments><proxy>auai</proxy><report-no>UAI-P-2010-PG-517-526</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new optimization framework to maximize the expected spread of
cascades in networks. Our model allows a rich set of actions that directly
manipulate cascade dynamics by adding nodes or edges to the network. Our
motivating application is one in spatial conservation planning, where a cascade
models the dispersal of wild animals through a fragmented landscape. We propose
a mixed integer programming (MIP) formulation that combines elements from
network design and stochastic optimization. Our approach results in solutions
with stochastic optimality guarantees and points to conservation strategies
that are fundamentally different from naive approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3515</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3515</id><created>2012-03-15</created><authors><author><keyname>Shpitser</keyname><forenames>Ilya</forenames></author><author><keyname>VanderWeele</keyname><forenames>Tyler</forenames></author><author><keyname>Robins</keyname><forenames>James M.</forenames></author></authors><title>On the Validity of Covariate Adjustment for Estimating Causal Effects</title><categories>stat.ME cs.AI</categories><comments>Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</comments><proxy>auai</proxy><report-no>UAI-P-2010-PG-527-536</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Identifying effects of actions (treatments) on outcome variables from
observational data and causal assumptions is a fundamental problem in causal
inference. This identification is made difficult by the presence of confounders
which can be related to both treatment and outcome variables. Confounders are
often handled, both in theory and in practice, by adjusting for covariates, in
other words considering outcomes conditioned on treatment and covariate values,
weighed by probability of observing those covariate values. In this paper, we
give a complete graphical criterion for covariate adjustment, which we term the
adjustment criterion, and derive some interesting corollaries of the
completeness of this criterion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3516</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3516</id><created>2012-03-15</created><authors><author><keyname>Simma</keyname><forenames>Aleksandr</forenames></author><author><keyname>Jordan</keyname><forenames>Michael I.</forenames></author></authors><title>Modeling Events with Cascades of Poisson Processes</title><categories>cs.LG cs.AI stat.ML</categories><comments>Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</comments><proxy>auai</proxy><report-no>UAI-P-2010-PG-546-555</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a probabilistic model of events in continuous time in which each
event triggers a Poisson process of successor events. The ensemble of observed
events is thereby modeled as a superposition of Poisson processes. Efficient
inference is feasible under this model with an EM algorithm. Moreover, the EM
algorithm can be implemented as a distributed algorithm, permitting the model
to be applied to very large datasets. We apply these techniques to the modeling
of Twitter messages and the revision history of Wikipedia.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3517</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3517</id><created>2012-03-15</created><authors><author><keyname>Singh</keyname><forenames>Ajit P.</forenames></author><author><keyname>Gordon</keyname><forenames>Geoffrey</forenames></author></authors><title>A Bayesian Matrix Factorization Model for Relational Data</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</comments><proxy>auai</proxy><report-no>UAI-P-2010-PG-556-563</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Relational learning can be used to augment one data source with other
correlated sources of information, to improve predictive accuracy. We frame a
large class of relational learning problems as matrix factorization problems,
and propose a hierarchical Bayesian model. Training our Bayesian model using
random-walk Metropolis-Hastings is impractically slow, and so we develop a
block Metropolis-Hastings sampler which uses the gradient and Hessian of the
likelihood to dynamically tune the proposal. We demonstrate that a predictive
model of brain response to stimuli can be improved by augmenting it with side
information about the stimuli.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3518</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3518</id><created>2012-03-15</created><authors><author><keyname>Sorg</keyname><forenames>Jonathan</forenames></author><author><keyname>Singh</keyname><forenames>Satinder</forenames></author><author><keyname>Lewis</keyname><forenames>Richard L.</forenames></author></authors><title>Variance-Based Rewards for Approximate Bayesian Reinforcement Learning</title><categories>cs.LG cs.AI stat.ML</categories><comments>Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</comments><proxy>auai</proxy><report-no>UAI-P-2010-PG-564-571</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The explore{exploit dilemma is one of the central challenges in Reinforcement
Learning (RL). Bayesian RL solves the dilemma by providing the agent with
information in the form of a prior distribution over environments; however,
full Bayesian planning is intractable. Planning with the mean MDP is a common
myopic approximation of Bayesian planning. We derive a novel reward bonus that
is a function of the posterior distribution over environments, which, when
added to the reward in planning with the mean MDP, results in an agent which
explores efficiently and effectively. Although our method is similar to
existing methods when given an uninformative or unstructured prior, unlike
existing methods, our method can exploit structured priors. We prove that our
method results in a polynomial sample complexity and empirically demonstrate
its advantages in a structured exploration task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3519</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3519</id><created>2012-03-15</created><authors><author><keyname>Tesauro</keyname><forenames>Gerald</forenames></author><author><keyname>Rajan</keyname><forenames>V T</forenames></author><author><keyname>Segal</keyname><forenames>Richard</forenames></author></authors><title>Bayesian Inference in Monte-Carlo Tree Search</title><categories>cs.LG cs.AI stat.ML</categories><comments>Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</comments><proxy>auai</proxy><report-no>UAI-P-2010-PG-580-588</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Monte-Carlo Tree Search (MCTS) methods are drawing great interest after
yielding breakthrough results in computer Go. This paper proposes a Bayesian
approach to MCTS that is inspired by distributionfree approaches such as UCT
[13], yet significantly differs in important respects. The Bayesian framework
allows potentially much more accurate (Bayes-optimal) estimation of node values
and node uncertainties from a limited number of simulation trials. We further
propose propagating inference in the tree via fast analytic Gaussian
approximation methods: this can make the overhead of Bayesian inference
manageable in domains such as Go, while preserving high accuracy of
expected-value estimates. We find substantial empirical outperformance of UCT
in an idealized bandit-tree test environment, where we can obtain valuable
insights by comparing with known ground truth. Additionally we rigorously prove
on-policy and off-policy convergence of the proposed methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3520</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3520</id><created>2012-03-15</created><authors><author><keyname>Tian</keyname><forenames>Jin</forenames></author><author><keyname>He</keyname><forenames>Ru</forenames></author><author><keyname>Ram</keyname><forenames>Lavanya</forenames></author></authors><title>Bayesian Model Averaging Using the k-best Bayesian Network Structures</title><categories>cs.LG cs.AI stat.ML</categories><comments>Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</comments><proxy>auai</proxy><report-no>UAI-P-2010-PG-589-597</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of learning Bayesian network structures from data. We
develop an algorithm for finding the k-best Bayesian network structures. We
propose to compute the posterior probabilities of hypotheses of interest by
Bayesian model averaging over the k-best Bayesian networks. We present
empirical results on structural discovery over several real and synthetic data
sets and show that the method outperforms the model selection method and the
state of-the-art MCMC methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3521</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3521</id><created>2012-03-15</created><authors><author><keyname>Ueno</keyname><forenames>Maomi</forenames></author></authors><title>Learning networks determined by the ratio of prior and data</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</comments><proxy>auai</proxy><report-no>UAI-P-2010-PG-598-605</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent reports have described that the equivalent sample size (ESS) in a
Dirichlet prior plays an important role in learning Bayesian networks. This
paper provides an asymptotic analysis of the marginal likelihood score for a
Bayesian network. Results show that the ratio of the ESS and sample size
determine the penalty of adding arcs in learning Bayesian networks. The number
of arcs increases monotonically as the ESS increases; the number of arcs
monotonically decreases as the ESS decreases. Furthermore, the marginal
likelihood score provides a unified expression of various score metrics by
changing prior knowledge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3522</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3522</id><created>2012-03-15</created><authors><author><keyname>Valko</keyname><forenames>Michal</forenames></author><author><keyname>Kveton</keyname><forenames>Branislav</forenames></author><author><keyname>Huang</keyname><forenames>Ling</forenames></author><author><keyname>Ting</keyname><forenames>Daniel</forenames></author></authors><title>Online Semi-Supervised Learning on Quantized Graphs</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</comments><proxy>auai</proxy><report-no>UAI-P-2010-PG-606-614</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we tackle the problem of online semi-supervised learning
(SSL). When data arrive in a stream, the dual problems of computation and data
storage arise for any SSL method. We propose a fast approximate online SSL
algorithm that solves for the harmonic solution on an approximate graph. We
show, both empirically and theoretically, that good behavior can be achieved by
collapsing nearby points into a set of local &quot;representative points&quot; that
minimize distortion. Moreover, we regularize the harmonic solution to achieve
better stability properties. We apply our algorithm to face recognition and
optical character recognition applications to show that we can take advantage
of the manifold structure to outperform the previous methods. Unlike previous
heuristic approaches, we show that our method yields provable performance
bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3523</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3523</id><created>2012-03-15</created><authors><author><keyname>Broek</keyname><forenames>Bart van den</forenames></author><author><keyname>Wiegerinck</keyname><forenames>Wim</forenames></author><author><keyname>Kappen</keyname><forenames>Hilbert</forenames></author></authors><title>Risk Sensitive Path Integral Control</title><categories>cs.SY math.OC</categories><comments>Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</comments><proxy>auai</proxy><report-no>UAI-P-2010-PG-615-622</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently path integral methods have been developed for stochastic optimal
control for a wide class of models with non-linear dynamics in continuous
space-time. Path integral methods find the control that minimizes the expected
cost-to-go. In this paper we show that under the same assumptions, path
integral methods generalize directly to risk sensitive stochastic optimal
control. Here the method minimizes in expectation an exponentially weighted
cost-to-go. Depending on the exponential weight, risk seeking or risk averse
behaviour is obtained. We demonstrate the approach on risk sensitive stochastic
optimal control problems beyond the linear-quadratic case, showing the
intricate interaction of multi-modal control with risk sensitivity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3524</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3524</id><created>2012-03-15</created><authors><author><keyname>Vanhatalo</keyname><forenames>Jarno</forenames></author><author><keyname>Vehtari</keyname><forenames>Aki</forenames></author></authors><title>Speeding up the binary Gaussian process classification</title><categories>stat.ML cs.LG</categories><comments>Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</comments><proxy>auai</proxy><report-no>UAI-P-2010-PG-623-631</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gaussian processes (GP) are attractive building blocks for many probabilistic
models. Their drawbacks, however, are the rapidly increasing inference time and
memory requirement alongside increasing data. The problem can be alleviated
with compactly supported (CS) covariance functions, which produce sparse
covariance matrices that are fast in computations and cheap to store. CS
functions have previously been used in GP regression but here the focus is in a
classification problem. This brings new challenges since the posterior
inference has to be done approximately. We utilize the expectation propagation
algorithm and show how its standard implementation has to be modified to obtain
computational benefits from the sparse covariance matrices. We study four CS
covariance functions and show that they may lead to substantial speed up in the
inference time compared to globally supported functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3525</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3525</id><created>2012-03-15</created><authors><author><keyname>Voortman</keyname><forenames>Mark</forenames></author><author><keyname>Dash</keyname><forenames>Denver</forenames></author><author><keyname>Druzdzel</keyname><forenames>Marek J.</forenames></author></authors><title>Learning Why Things Change: The Difference-Based Causality Learner</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</comments><proxy>auai</proxy><report-no>UAI-P-2010-PG-641-650</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present the Difference- Based Causality Learner (DBCL), an
algorithm for learning a class of discrete-time dynamic models that represents
all causation across time by means of difference equations driving change in a
system. We motivate this representation with real-world mechanical systems and
prove DBCL's correctness for learning structure from time series data, an
endeavour that is complicated by the existence of latent derivatives that have
to be detected. We also prove that, under common assumptions for causal
discovery, DBCL will identify the presence or absence of feedback loops, making
the model more useful for predicting the effects of manipulating variables when
the system is in equilibrium. We argue analytically and show empirically the
advantages of DBCL over vector autoregression (VAR) and Granger causality
models as well as modified forms of Bayesian and constraintbased structure
discovery algorithms. Finally, we show that our algorithm can discover causal
directions of alpha rhythms in human brains from EEG data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3526</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3526</id><created>2012-03-15</created><authors><author><keyname>Werner</keyname><forenames>Tomas</forenames></author></authors><title>Primal View on Belief Propagation</title><categories>cs.LG cs.AI stat.ML</categories><comments>Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</comments><proxy>auai</proxy><report-no>UAI-P-2010-PG-651-657</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is known that fixed points of loopy belief propagation (BP) correspond to
stationary points of the Bethe variational problem, where we minimize the Bethe
free energy subject to normalization and marginalization constraints.
Unfortunately, this does not entirely explain BP because BP is a dual rather
than primal algorithm to solve the Bethe variational problem -- beliefs are
infeasible before convergence. Thus, we have no better understanding of BP than
as an algorithm to seek for a common zero of a system of non-linear functions,
not explicitly related to each other. In this theoretical paper, we show that
these functions are in fact explicitly related -- they are the partial
derivatives of a single function of reparameterizations. That means, BP seeks
for a stationary point of a single function, without any constraints. This
function has a very natural form: it is a linear combination of local
log-partition functions, exactly as the Bethe entropy is the same linear
combination of local entropies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3527</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3527</id><created>2012-03-15</created><authors><author><keyname>Witkowski</keyname><forenames>Jens</forenames></author></authors><title>Truthful Feedback for Sanctioning Reputation Mechanisms</title><categories>cs.GT cs.AI</categories><comments>Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</comments><proxy>auai</proxy><report-no>UAI-P-2010-PG-658-665</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For product rating environments, similar to that of Amazon Reviews, it has
been shown that the truthful elicitation of feedback is possible through
mechanisms which pay buyer reports contingent on the reports of other buyers.
We study whether similar mechanisms can be designed for reputation mechanisms
at online auction sites where the buyers' experiences are partially determined
by a strategic seller. We show that this is impossible for the basic setting.
However, introducing a small prior belief that the seller is a cooperative
commitment player leads to a payment scheme with a truthful perfect Bayesian
equilibrium.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3528</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3528</id><created>2012-03-15</created><authors><author><keyname>Wu</keyname><forenames>Feng</forenames></author><author><keyname>Zilberstein</keyname><forenames>Shlomo</forenames></author><author><keyname>Chen</keyname><forenames>Xiaoping</forenames></author></authors><title>Rollout Sampling Policy Iteration for Decentralized POMDPs</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</comments><proxy>auai</proxy><report-no>UAI-P-2010-PG-666-673</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present decentralized rollout sampling policy iteration (DecRSPI) - a new
algorithm for multi-agent decision problems formalized as DEC-POMDPs. DecRSPI
is designed to improve scalability and tackle problems that lack an explicit
model. The algorithm uses Monte- Carlo methods to generate a sample of
reachable belief states. Then it computes a joint policy for each belief state
based on the rollout estimations. A new policy representation allows us to
represent solutions compactly. The key benefits of the algorithm are its linear
time complexity over the number of agents, its bounded memory usage and good
solution quality. It can solve larger problems that are intractable for
existing planning algorithms. Experimental results confirm the effectiveness
and scalability of the approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3529</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3529</id><created>2012-03-15</created><authors><author><keyname>Yan</keyname><forenames>Yan</forenames></author><author><keyname>Rosales</keyname><forenames>Romer</forenames></author><author><keyname>Fung</keyname><forenames>Glenn</forenames></author><author><keyname>Dy</keyname><forenames>Jennifer</forenames></author></authors><title>Modeling Multiple Annotator Expertise in the Semi-Supervised Learning
  Scenario</title><categories>cs.LG cs.AI stat.ML</categories><comments>Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</comments><proxy>auai</proxy><report-no>UAI-P-2010-PG-674-682</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning algorithms normally assume that there is at most one annotation or
label per data point. However, in some scenarios, such as medical diagnosis and
on-line collaboration,multiple annotations may be available. In either case,
obtaining labels for data points can be expensive and time-consuming (in some
circumstances ground-truth may not exist). Semi-supervised learning approaches
have shown that utilizing the unlabeled data is often beneficial in these
cases. This paper presents a probabilistic semi-supervised model and algorithm
that allows for learning from both unlabeled and labeled data in the presence
of multiple annotators. We assume that it is known what annotator labeled which
data points. The proposed approach produces annotator models that allow us to
provide (1) estimates of the true label and (2) annotator variable expertise
for both labeled and unlabeled data. We provide numerical comparisons under
various scenarios and with respect to standard semi-supervised learning.
Experiments showed that the presented approach provides clear advantages over
multi-annotator methods that do not use the unlabeled data and over methods
that do not use multi-labeler information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3530</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3530</id><created>2012-03-15</created><authors><author><keyname>Yang</keyname><forenames>Shuang Hong</forenames></author><author><keyname>Bian</keyname><forenames>Jiang</forenames></author><author><keyname>Zha</keyname><forenames>Hongyuan</forenames></author></authors><title>Hybrid Generative/Discriminative Learning for Automatic Image Annotation</title><categories>cs.LG cs.CV stat.ML</categories><comments>Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</comments><proxy>auai</proxy><report-no>UAI-P-2010-PG-683-690</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatic image annotation (AIA) raises tremendous challenges to machine
learning as it requires modeling of data that are both ambiguous in input and
output, e.g., images containing multiple objects and labeled with multiple
semantic tags. Even more challenging is that the number of candidate tags is
usually huge (as large as the vocabulary size) yet each image is only related
to a few of them. This paper presents a hybrid generative-discriminative
classifier to simultaneously address the extreme data-ambiguity and
overfitting-vulnerability issues in tasks such as AIA. Particularly: (1) an
Exponential-Multinomial Mixture (EMM) model is established to capture both the
input and output ambiguity and in the meanwhile to encourage prediction
sparsity; and (2) the prediction ability of the EMM model is explicitly
maximized through discriminative learning that integrates variational inference
of graphical models and the pairwise formulation of ordinal regression.
Experiments show that our approach achieves both superior annotation
performance and better tag scalability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3531</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3531</id><created>2012-03-15</created><authors><author><keyname>Yuan</keyname><forenames>Changhe</forenames></author><author><keyname>Wu</keyname><forenames>Xiaojian</forenames></author><author><keyname>Hansen</keyname><forenames>Eric A.</forenames></author></authors><title>Solving Multistage Influence Diagrams using Branch-and-Bound Search</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</comments><proxy>auai</proxy><report-no>UAI-P-2010-PG-691-700</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A branch-and-bound approach to solving influ- ence diagrams has been
previously proposed in the literature, but appears to have never been
implemented and evaluated - apparently due to the difficulties of computing
effective bounds for the branch-and-bound search. In this paper, we describe
how to efficiently compute effective bounds, and we develop a practical
implementa- tion of depth-first branch-and-bound search for influence diagram
evaluation that outperforms existing methods for solving influence diagrams
with multiple stages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3532</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3532</id><created>2012-03-15</created><authors><author><keyname>Zhang</keyname><forenames>Bai</forenames></author><author><keyname>Wang</keyname><forenames>Yue</forenames></author></authors><title>Learning Structural Changes of Gaussian Graphical Models in Controlled
  Experiments</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</comments><proxy>auai</proxy><report-no>UAI-P-2010-PG-701-708</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graphical models are widely used in scienti fic and engineering research to
represent conditional independence structures between random variables. In many
controlled experiments, environmental changes or external stimuli can often
alter the conditional dependence between the random variables, and potentially
produce significant structural changes in the corresponding graphical models.
Therefore, it is of great importance to be able to detect such structural
changes from data, so as to gain novel insights into where and how the
structural changes take place and help the system adapt to the new environment.
Here we report an effective learning strategy to extract structural changes in
Gaussian graphical model using l1-regularization based convex optimization. We
discuss the properties of the problem formulation and introduce an efficient
implementation by the block coordinate descent algorithm. We demonstrate the
principle of the approach on a numerical simulation experiment, and we then
apply the algorithm to the modeling of gene regulatory networks under different
conditions and obtain promising yet biologically plausible results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3533</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3533</id><created>2012-03-15</created><authors><author><keyname>Zhang</keyname><forenames>Kun</forenames></author><author><keyname>Hyvarinen</keyname><forenames>Aapo</forenames></author></authors><title>Source Separation and Higher-Order Causal Analysis of MEG and EEG</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</comments><proxy>auai</proxy><report-no>UAI-P-2010-PG-709-716</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Separation of the sources and analysis of their connectivity have been an
important topic in EEG/MEG analysis. To solve this problem in an automatic
manner, we propose a two-layer model, in which the sources are conditionally
uncorrelated from each other, but not independent; the dependence is caused by
the causality in their time-varying variances (envelopes). The model is
identified in two steps. We first propose a new source separation technique
which takes into account the autocorrelations (which may be time-varying) and
time-varying variances of the sources. The causality in the envelopes is then
discovered by exploiting a special kind of multivariate GARCH (generalized
autoregressive conditional heteroscedasticity) model. The resulting causal
diagram gives the effective connectivity between the separated sources; in our
experimental results on MEG data, sources with similar functions are grouped
together, with negative influences between groups, and the groups are connected
via some interesting sources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3534</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3534</id><created>2012-03-15</created><authors><author><keyname>Zhang</keyname><forenames>Kun</forenames></author><author><keyname>Schoelkopf</keyname><forenames>Bernhard</forenames></author><author><keyname>Janzing</keyname><forenames>Dominik</forenames></author></authors><title>Invariant Gaussian Process Latent Variable Models and Application in
  Causal Discovery</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</comments><proxy>auai</proxy><report-no>UAI-P-2010-PG-717-724</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In nonlinear latent variable models or dynamic models, if we consider the
latent variables as confounders (common causes), the noise dependencies imply
further relations between the observed variables. Such models are then closely
related to causal discovery in the presence of nonlinear confounders, which is
a challenging problem. However, generally in such models the observation noise
is assumed to be independent across data dimensions, and consequently the noise
dependencies are ignored. In this paper we focus on the Gaussian process latent
variable model (GPLVM), from which we develop an extended model called
invariant GPLVM (IGPLVM), which can adapt to arbitrary noise covariances. With
the Gaussian process prior put on a particular transformation of the latent
nonlinear functions, instead of the original ones, the algorithm for IGPLVM
involves almost the same computational loads as that for the original GPLVM.
Besides its potential application in causal discovery, IGPLVM has the advantage
that its estimated latent nonlinear manifold is invariant to any nonsingular
linear transformation of the data. Experimental results on both synthetic and
realworld data show its encouraging performance in nonlinear manifold learning
and causal discovery.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3535</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3535</id><created>2012-03-15</created><authors><author><keyname>Zhang</keyname><forenames>Yu</forenames></author><author><keyname>Cao</keyname><forenames>Bin</forenames></author><author><keyname>Yeung</keyname><forenames>Dit-Yan</forenames></author></authors><title>Multi-Domain Collaborative Filtering</title><categories>cs.IR cs.AI</categories><comments>Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</comments><proxy>auai</proxy><report-no>UAI-P-2010-PG-725-732</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Collaborative filtering is an effective recommendation approach in which the
preference of a user on an item is predicted based on the preferences of other
users with similar interests. A big challenge in using collaborative filtering
methods is the data sparsity problem which often arises because each user
typically only rates very few items and hence the rating matrix is extremely
sparse. In this paper, we address this problem by considering multiple
collaborative filtering tasks in different domains simultaneously and
exploiting the relationships between domains. We refer to it as a multi-domain
collaborative filtering (MCF) problem. To solve the MCF problem, we propose a
probabilistic framework which uses probabilistic matrix factorization to model
the rating problem in each domain and allows the knowledge to be adaptively
transferred across different domains by automatically learning the correlation
between domains. We also introduce the link function for different domains to
correct their biases. Experiments conducted on several real-world applications
demonstrate the effectiveness of our methods when compared with some
representative methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3536</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3536</id><created>2012-03-15</created><authors><author><keyname>Zhang</keyname><forenames>Yu</forenames></author><author><keyname>Yeung</keyname><forenames>Dit-Yan</forenames></author></authors><title>A Convex Formulation for Learning Task Relationships in Multi-Task
  Learning</title><categories>cs.LG cs.AI stat.ML</categories><comments>Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</comments><proxy>auai</proxy><report-no>UAI-P-2010-PG-733-742</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-task learning is a learning paradigm which seeks to improve the
generalization performance of a learning task with the help of some other
related tasks. In this paper, we propose a regularization formulation for
learning the relationships between tasks in multi-task learning. This
formulation can be viewed as a novel generalization of the regularization
framework for single-task learning. Besides modeling positive task correlation,
our method, called multi-task relationship learning (MTRL), can also describe
negative task correlation and identify outlier tasks based on the same
underlying principle. Under this regularization framework, the objective
function of MTRL is convex. For efficiency, we use an alternating method to
learn the optimal model parameters for each task as well as the relationships
between tasks. We study MTRL in the symmetric multi-task learning setting and
then generalize it to the asymmetric setting as well. We also study the
relationships between MTRL and some existing multi-task learning methods.
Experiments conducted on a toy problem as well as several benchmark data sets
demonstrate the effectiveness of MTRL.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3537</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3537</id><created>2012-03-15</created><authors><author><keyname>Zhu</keyname><forenames>Qian</forenames></author><author><keyname>Kveton</keyname><forenames>Branislav</forenames></author><author><keyname>Mummert</keyname><forenames>Lily</forenames></author><author><keyname>Pillai</keyname><forenames>Padmanabhan</forenames></author></authors><title>Automatic Tuning of Interactive Perception Applications</title><categories>cs.LG cs.CV stat.ML</categories><comments>Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</comments><proxy>auai</proxy><report-no>UAI-P-2010-PG-743-751</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interactive applications incorporating high-data rate sensing and computer
vision are becoming possible due to novel runtime systems and the use of
parallel computation resources. To allow interactive use, such applications
require careful tuning of multiple application parameters to meet required
fidelity and latency bounds. This is a nontrivial task, often requiring expert
knowledge, which becomes intractable as resources and application load
characteristics change. This paper describes a method for automatic performance
tuning that learns application characteristics and effects of tunable
parameters online, and constructs models that are used to maximize fidelity for
a given latency constraint. The paper shows that accurate latency models can be
learned online, knowledge of application structure can be used to reduce the
complexity of the learning task, and operating points can be found that achieve
90% of the optimal fidelity by exploring the parameter space only 3% of the
time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3538</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3538</id><created>2012-03-15</created><authors><author><keyname>Brunskill</keyname><forenames>Emma</forenames></author><author><keyname>Russell</keyname><forenames>Stuart</forenames></author></authors><title>RAPID: A Reachable Anytime Planner for Imprecisely-sensed Domains</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</comments><proxy>auai</proxy><report-no>UAI-P-2010-PG-83-92</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite the intractability of generic optimal partially observable Markov
decision process planning, there exist important problems that have highly
structured models. Previous researchers have used this insight to construct
more efficient algorithms for factored domains, and for domains with
topological structure in the flat state dynamics model. In our work, motivated
by findings from the education community relevant to automated tutoring, we
consider problems that exhibit a form of topological structure in the factored
dynamics model. Our Reachable Anytime Planner for Imprecisely-sensed Domains
(RAPID) leverages this structure to efficiently compute a good initial envelope
of reachable states under the optimal MDP policy in time linear in the number
of state variables. RAPID performs partially-observable planning over the
limited envelope of states, and slowly expands the state space considered as
time allows. RAPID performs well on a large tutoring-inspired problem
simulation with 122 state variables, corresponding to a flat state space of
over 10^30 states.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3568</identifier>
 <datestamp>2014-08-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3568</id><created>2012-03-15</created><authors><author><keyname>Colson</keyname><forenames>Lo&#xef;c</forenames><affiliation>LITA</affiliation></author><author><keyname>Demange</keyname><forenames>Vincent</forenames><affiliation>LITA</affiliation></author></authors><title>Investigations on a Pedagogical Calculus of Constructions</title><categories>cs.LO</categories><comments>18 pages</comments><proxy>ccsd</proxy><journal-ref>J.UCS Journal of Universal Computer Science 19, 6 (2013) 729-749</journal-ref><doi>10.3217/jucs-019-06-0729</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the last few years appeared pedagogical propositional natural deduction
systems. In these systems, one must satisfy the pedagogical constraint: the
user must give an example of any introduced notion. First we expose the reasons
of such a constraint and properties of these &quot;pedagogical&quot; calculi: the absence
of negation at logical side, and the &quot;usefulness&quot; feature of terms at
computational side (through the Curry-Howard correspondence). Then we construct
a simple pedagogical restriction of the calculus of constructions (CC) called
CCr. We establish logical limitations of this system, and compare its
computational expressiveness to Godel system T. Finally, guided by the logical
limitations of CCr, we propose a formal and general definition of what a
pedagogical calculus of constructions should be.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3574</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3574</id><created>2012-03-15</created><authors><author><keyname>Steiner</keyname><forenames>Ingmar</forenames><affiliation>INRIA Lorraine - LORIA</affiliation></author><author><keyname>Ouni</keyname><forenames>Slim</forenames><affiliation>INRIA Lorraine - LORIA</affiliation></author></authors><title>Artimate: an articulatory animation framework for audiovisual speech
  synthesis</title><categories>cs.HC cs.GR</categories><comments>Workshop on Innovation and Applications in Speech Technology (2012)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a modular framework for articulatory animation synthesis using
speech motion capture data obtained with electromagnetic articulography (EMA).
Adapting a skeletal animation approach, the articulatory motion data is applied
to a three-dimensional (3D) model of the vocal tract, creating a portable
resource that can be integrated in an audiovisual (AV) speech synthesis
platform to provide realistic animation of the tongue and teeth for a virtual
character. The framework also provides an interface to articulatory animation
synthesis, as well as an example application to illustrate its use with a 3D
game engine. We rely on cross-platform, open-source software and open standards
to provide a lightweight, accessible, and portable workflow.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3575</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3575</id><created>2012-03-15</created><authors><author><keyname>Dubois</keyname><forenames>Swan</forenames><affiliation>LIP6</affiliation></author><author><keyname>Tixeuil</keyname><forenames>S&#xe9;bastien</forenames><affiliation>LIP6, IUF</affiliation></author><author><keyname>Zhu</keyname><forenames>Nini</forenames><affiliation>LIP6</affiliation></author></authors><title>The Byzantine Brides Problem</title><categories>cs.DC</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the hardness of establishing as many stable marriages (that
is, marriages that last forever) in a population whose memory is placed in some
arbitrary state with respect to the considered problem, and where traitors try
to jeopardize the whole process by behaving in a harmful manner. On the
negative side, we demonstrate that no solution that is completely insensitive
to traitors can exist, and we propose a protocol for the problem that is
optimal with respect to the traitor containment radius.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3578</identifier>
 <datestamp>2015-08-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3578</id><created>2012-03-15</created><updated>2015-08-09</updated><authors><author><keyname>Fukunaga</keyname><forenames>Takuro</forenames></author><author><keyname>Nutov</keyname><forenames>Zeev</forenames></author><author><keyname>Ravi</keyname><forenames>R.</forenames></author></authors><title>Iterative rounding approximation algorithms for degree-bounded
  node-connectivity network design</title><categories>cs.DS</categories><comments>A preliminary version of this paper appeared in proceedings of the
  53rd Annual IEEE Symposium on Foundations of Computer Science (FOCS 2012)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of finding a minimum edge cost subgraph of a graph
satisfying both given node-connectivity requirements and degree upper bounds on
nodes. We present an iterative rounding algorithm of the biset LP relaxation
for this problem. For directed graphs and $k$-out-connectivity requirements
from a root, our algorithm computes a solution that is a 2-approximation on the
cost, and the degree of each node $v$ in the solution is at most $2b(v) + O(k)$
where $b(v)$ is the degree upper bound on $v$. For undirected graphs and
element-connectivity requirements with maximum connectivity requirement $k$,
our algorithm computes a solution that is a $4$-approximation on the cost, and
the degree of each node $v$ in the solution is at most $4b(v)+O(k)$. These
ratios improve the previous $O(\log k)$-approximation on the cost and $O(2^k
b(v))$ approximation on the degrees. Our algorithms can be used to improve
approximation ratios for other node-connectivity problems such as undirected
$k$-out-connectivity, directed and undirected $k$-connectivity, and undirected
rooted $k$-connectivity and subset $k$-connectivity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3584</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3584</id><created>2012-03-15</created><authors><author><keyname>El-Shishtawy</keyname><forenames>Tarek</forenames></author><author><keyname>El-Ghannam</keyname><forenames>Fatma</forenames></author></authors><title>An Accurate Arabic Root-Based Lemmatizer for Information Retrieval
  Purposes</title><categories>cs.CL</categories><comments>9 pages</comments><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 9,
  Issue 1, No 3, January 2012 ISSN (Online): 1694-0814</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In spite of its robust syntax, semantic cohesion, and less ambiguity, lemma
level analysis and generation does not yet focused in Arabic NLP literatures.
In the current research, we propose the first non-statistical accurate Arabic
lemmatizer algorithm that is suitable for information retrieval (IR) systems.
The proposed lemmatizer makes use of different Arabic language knowledge
resources to generate accurate lemma form and its relevant features that
support IR purposes. As a POS tagger, the experimental results show that, the
proposed algorithm achieves a maximum accuracy of 94.8%. For first seen
documents, an accuracy of 89.15% is achieved, compared to 76.7% of up to date
Stanford accurate Arabic model, for the same, dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3586</identifier>
 <datestamp>2012-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3586</id><created>2012-03-15</created><authors><author><keyname>Pourvali</keyname><forenames>Mohsen</forenames></author><author><keyname>Abadeh</keyname><forenames>Mohammad Saniee</forenames></author></authors><title>Automated Text Summarization Base on Lexicales Chain and graph Using of
  WordNet and Wikipedia Knowledge Base</title><categories>cs.IR cs.CL</categories><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 9,
  Issue 1, No 3, January 2012</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The technology of automatic document summarization is maturing and may
provide a solution to the information overload problem. Nowadays, document
summarization plays an important role in information retrieval. With a large
volume of documents, presenting the user with a summary of each document
greatly facilitates the task of finding the desired documents. Document
summarization is a process of automatically creating a compressed version of a
given document that provides useful information to users, and multi-document
summarization is to produce a summary delivering the majority of information
content from a set of documents about an explicit or implicit main topic. The
lexical cohesion structure of the text can be exploited to determine the
importance of a sentence/phrase. Lexical chains are useful tools to analyze the
lexical cohesion structure in a text .In this paper we consider the effect of
the use of lexical cohesion features in Summarization, And presenting a
algorithm base on the knowledge base. Ours algorithm at first find the correct
sense of any word, Then constructs the lexical chains, remove Lexical chains
that less score than other, detects topics roughly from lexical chains,
segments the text with respect to the topics and selects the most important
sentences. The experimental results on an open benchmark datasets from DUC01
and DUC02 show that our proposed approach can improve the performance compared
to sate-of-the-art summarization approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3589</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3589</id><created>2012-03-15</created><authors><author><keyname>Ahmed</keyname><forenames>Eya Ben</forenames></author><author><keyname>Nabli</keyname><forenames>Ahlem</forenames></author><author><keyname>Gargouri</keyname><forenames>Fa&#xef;ez</forenames></author></authors><title>Building MultiView Analyst Profile From Multidimensional Query Logs:
  From Consensual to Conflicting Preferences</title><categories>cs.DB cs.IR</categories><comments>8 pages</comments><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 9,
  Issue 1, No 2, January 2012 ISSN (Online): 1694-0814 www.IJCSI.org</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In order to provide suitable results to the analyst needs, user preferences
summarization is widely used in several domains. In this paper, we introduce a
new approach for user profile construction from OLAP query logs. The key idea
is to learn the user's preferences by drawing the evidence from OLAP logs. In
fact, the analyst preferences are clustered into three main pools : (i)
consensual or non conflicting preferences referring to same preferences for all
analysts; (ii) semi-conflicting preferences corresponding to similar
preferences for some analysts; (iii) conflicting preferences related to
disjoint preferences for all analysts. To build generic and global model
accurately describing the analyst, we enrich the obtained characteristics
through including several views, namely the personal view, the professional
view and the behavioral view. After that, the multiview profile extracted from
multidimensional database can be annotated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3593</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3593</id><created>2012-03-15</created><authors><author><keyname>Chen</keyname><forenames>Peiji</forenames></author><author><keyname>Ma</keyname><forenames>Wenjing</forenames></author><author><keyname>Mandalapu</keyname><forenames>Srinath</forenames></author><author><keyname>Nagarajan</keyname><forenames>Chandrashekhar</forenames></author><author><keyname>Shanmugasundaram</keyname><forenames>Jayavel</forenames></author><author><keyname>Vassilvitskii</keyname><forenames>Sergei</forenames></author><author><keyname>Vee</keyname><forenames>Erik</forenames></author><author><keyname>Yu</keyname><forenames>Manfai</forenames></author><author><keyname>Zien</keyname><forenames>Jason</forenames></author></authors><title>Ad Serving Using a Compact Allocation Plan</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A large fraction of online display advertising is sold via guaranteed
contracts: a publisher guarantees to the advertiser a certain number of user
visits satisfying the targeting predicates of the contract. The publisher is
then tasked with solving the ad serving problem - given a user visit, which of
the thousands of matching contracts should be displayed, so that by the
expiration time every contract has obtained the requisite number of user
visits. The challenges of the problem come from (1) the sheer size of the
problem being solved, with tens of thousands of contracts and billions of user
visits, (2) the unpredictability of user behavior, since these contracts are
sold months ahead of time, when only a forecast of user visits is available and
(3) the minute amount of resources available online, as an ad server must
respond with a matching contract in a fraction of a second.
  We present a solution to the guaranteed delivery ad serving problem using
{\em compact allocation plans}. These plans, computed offline, can be
efficiently queried by the ad server during an ad call; they are small, using
only O(1) space for contract; and are stateless, allowing for distributed
serving without any central coordination. We evaluate this approach on a real
set of user visits and guaranteed contracts and show that the compact
allocation plans are an effective way of solving the guaranteed delivery ad
serving problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3595</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3595</id><created>2012-03-15</created><authors><author><keyname>Shakhakarmi</keyname><forenames>Niraj</forenames></author></authors><title>Secured Distributed Cognitive MAC and Complexity Reduction in Channel
  Estimation for the Cross Layer based Cognitive Radio Networks</title><categories>cs.NI cs.ET cs.GT</categories><comments>10 pages, 14 figures</comments><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 9,
  Issue 1, No 1, January 2012, ISSN (Online): 1694-0814, www.IJCSI.org</journal-ref><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Secured opportunistic Medium Access Control (MAC) and complexity reduction in
channel estimation are proposed in the Cross layer design Cognitive Radio
Networks deploying the secured dynamic channel allocation from the endorsed
channel reservation. Channel Endorsement and Transmission policy is deployed to
optimize the free channel selection as well as channel utilization to cognitive
radio users. This strategy provide the secured and reliable link to secondary
users as well as the collision free link to primary users between the physical
and MAC layers which yields the better network performance. On the other hand,
Complexity Reduction in Minimum Mean Square Errror (CR-MMSE) and Maximum
Likelihood (CR-ML) algorithm on Decision Directed Channel Estimation (DDCE) is
deployed significantly to achieve computational complexity as Least Square (LS)
method. Rigorously, CR-MMSE in sample spaced channel impulse response (SS-CIR)
is implemented by allowing the computationally inspired matrix inversion.
Regarding CR-ML, Pilot Symbol Assisted Modulation (PSAM) with DDCE is
implemented such the pilot symbol sequence provides the significant performance
gain in frequency correlation using the finite delay spread. It is found that
CRMMSE demonstrates outstanding Symbol Error Rate (SER) performance over MMSE
and LS, and CR-ML over MMSE and ML.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3597</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3597</id><created>2012-03-15</created><authors><author><keyname>Vaman</keyname><forenames>Dhadesugoor R.</forenames></author><author><keyname>Shakhakarmi</keyname><forenames>Niraj</forenames></author></authors><title>Integrated Key based Strict Friendliness Verification of Neighbors in
  MANET</title><categories>cs.CR</categories><comments>6 pages, 12 figures, 2011 International Conference on Security
  Science and Technology(ICSST 2011)</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  A novel Strict Friendliness Verification (SFV) scheme based on the integrated
key consisting of symmetric node identity, geographic location and round trip
response time between the sender and the receiver radio in MANET is proposed.
This key is dynamically updated for encryption and decryption of each packet to
resolve Wormhole attack and Sybil attack. Additionally, it meets the minimal
key lengths required for symmetric ciphers to provide adequate commercial
security. Furthermore, the foe or unfriendly node detection is found
significantly increasing with the lower number of symmetric IDs. This paper
presents the simulation demonstrating the performance of SFV in terms of
dynamic range using directional antenna on radios (or nodes), and the
performance in terms of aggregate throughput, average end to end delay and
packet delivered ratio.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3601</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3601</id><created>2012-03-15</created><authors><author><keyname>Niraj</keyname><forenames>Shakhakarmi</forenames></author><author><keyname>Vaman</keyname><forenames>Dhadesugoor R.</forenames></author></authors><title>Distributed Position Localization and Tracking (DPLT) of Malicious Nodes
  in Cluster Based Mobile Ad hoc Networks (MANET)</title><categories>cs.CG</categories><comments>12 pages,23 figures, VTC conference</comments><journal-ref>WSEAS Transactions on Communications, ISSN: 1109-2742, Issue 11,
  Volume 9, November 2010</journal-ref><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  In this paper, a robust distributed malicious node detection and precise
localization and tracking method is proposed for Cluster based Mobile Ad hoc
Network (MANET). Certificate Authority (CA) node is selected as the most stable
node among trusted nodes, surrounded by Registration Authority nodes (RAs) in
each cluster to generate the Dynamic Demilitarized Zone (DDMZ) to defend CA
from probable attackers and mitigate the authentication overhead. The RAs also
co-operate with member nodes to detect a target node and determine whether it
is malicious or not, by providing the public key certificate and trust value.
In addition, Internet Protocol (IP) based Triangulation and multi-lateration
method are deployed based on using the average time difference of Time of
Arrival (ToA) and Time of Departure (ToD) of the management packets.
Triangulation uses three reference nodes which are elected within each cluster
based on Best Criterion Function (BCF) to localize each member node inside the
cluster in 2D. Multi-lateration is employed to localize the malicious target
node in 2D using four neighbor nodes. After localization of two consecutive
positions, the target node is continuously localized and tracked by a
particular node using the modified real time Position Localization and Tracking
(PL&amp;T) algorithm by adaptive beam forming and mapping the energy contours of
tracking zone into coverage radii distance. The performance of the proposed
scheme demonstrates the significant accuracy in the detection of malicious
nodes within each cluster.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3602</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3602</id><created>2012-03-15</created><updated>2014-04-26</updated><authors><author><keyname>Demaine</keyname><forenames>Erik D.</forenames></author><author><keyname>Demaine</keyname><forenames>Martin L.</forenames></author><author><keyname>Minsky</keyname><forenames>Yair N.</forenames></author><author><keyname>Mitchell</keyname><forenames>Joseph S. B.</forenames></author><author><keyname>Rivest</keyname><forenames>Ronald L.</forenames></author><author><keyname>Patrascu</keyname><forenames>Mihai</forenames></author></authors><title>Picture-Hanging Puzzles</title><categories>cs.DS math.GN math.GR</categories><comments>18 pages, 8 figures, 11 puzzles. Journal version of FUN 2012 paper</comments><journal-ref>Theory of Computing Systems, 54(4):531-550, May 2014</journal-ref><doi>10.1007/s00224-013-9501-0</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show how to hang a picture by wrapping rope around n nails, making a
polynomial number of twists, such that the picture falls whenever any k out of
the n nails get removed, and the picture remains hanging when fewer than k
nails get removed. This construction makes for some fun mathematical magic
performances. More generally, we characterize the possible Boolean functions
characterizing when the picture falls in terms of which nails get removed as
all monotone Boolean functions. This construction requires an exponential
number of twists in the worst case, but exponential complexity is almost always
necessary for general functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3606</identifier>
 <datestamp>2014-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3606</id><created>2012-03-15</created><updated>2012-12-27</updated><authors><author><keyname>Kwon</keyname><forenames>O-joung</forenames></author><author><keyname>Oum</keyname><forenames>Sang-il</forenames></author></authors><title>Graphs of Small Rank-width are Pivot-minors of Graphs of Small
  Tree-width</title><categories>math.CO cs.DM</categories><comments>16 pages, 7 figures</comments><journal-ref>Discrete Applied Math. 168(May 11, 2014), pp. 108-118</journal-ref><doi>10.1016/j.dam.2013.01.007</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that every graph of rank-width $k$ is a pivot-minor of a graph of
tree-width at most $2k$. We also prove that graphs of rank-width at most 1,
equivalently distance-hereditary graphs, are exactly vertex-minors of trees,
and graphs of linear rank-width at most 1 are precisely vertex-minors of paths.
In addition, we show that bipartite graphs of rank-width at most 1 are exactly
pivot-minors of trees and bipartite graphs of linear rank-width at most 1 are
precisely pivot-minors of paths.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3611</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3611</id><created>2012-03-15</created><authors><author><keyname>Liu</keyname><forenames>Xiaoyong</forenames></author><author><keyname>Fu</keyname><forenames>Hui</forenames></author></authors><title>Literature-based knowledge discovery: the state of the art</title><categories>cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Literature-based knowledge discovery method was introduced by Dr. Swanson in
1986. He hypothesized a connection between Raynaud's phenomenon and dietary
fish oil, the field of literature-based discovery (LBD) was born from then on.
During the subsequent two decades, LBD's research attracts some scientists
including information science, computer science, and biomedical science, etc..
It has been a part of knowledge discovery and text mining. This paper
summarizes the development of recent years about LBD and presents two parts,
methodology research and applied research. Lastly, some problems are pointed as
future research directions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3613</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3613</id><created>2012-03-16</created><authors><author><keyname>Kuppusamy</keyname><forenames>K. S.</forenames></author><author><keyname>Aghila</keyname><forenames>G.</forenames></author></authors><title>Morpes: A Model for Personalized Rendering of Web Content on Mobile
  Devices</title><categories>cs.HC</categories><comments>10 Pages, 2 Figures</comments><msc-class>68U35</msc-class><journal-ref>International Journal in Foundations of Computer Science &amp;
  Technology (IJFCST),Vol. 2, No.2, March 2012</journal-ref><doi>10.5121/ijfcst.2012.2204</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the tremendous growth in the information communication sector, the
mobile phones have become the prime information communication devices. The
convergence of traditional telephony with the modern web enabled communication
in the mobile devices has made the communication much effective and simpler. As
mobile phones are becoming the crucial source of accessing the contents of the
World Wide Web which was originally designed for personal computers, has opened
up a new challenge of accommodating the web contents in to the smaller mobile
devices. This paper proposes an approach towards building a model for rendering
the web pages in mobile devices. The proposed model is based on a
multi-dimensional web page segment evaluation model. The incorporation of
personalization in the proposed model makes the rendering user-centric. The
proposed model is validated with a prototype implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3618</identifier>
 <datestamp>2013-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3618</id><created>2012-03-16</created><updated>2013-01-22</updated><authors><author><keyname>Payne</keyname><forenames>Michael S.</forenames></author><author><keyname>Schmidt</keyname><forenames>Jens M.</forenames></author><author><keyname>Wood</keyname><forenames>David R.</forenames></author></authors><title>Which point sets admit a k-angulation?</title><categories>math.CO cs.CG</categories><comments>13 pages, 7 figures</comments><msc-class>05C10 (Primary) 68U05, 68R10 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For k &gt;= 3, a k-angulation is a 2-connected plane graph in which every
internal face is a k-gon. We say that a point set P admits a plane graph G if
there is a straight-line drawing of G that maps V(G) onto P and has the same
facial cycles and outer face as G. We investigate the conditions under which a
point set P admits a k-angulation and find that, for sets containing at least
2k^2 points, the only obstructions are those that follow from Euler's formula.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3619</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3619</id><created>2012-03-16</created><authors><author><keyname>Bharadwaj</keyname><forenames>Vijay</forenames></author><author><keyname>Chen</keyname><forenames>Peiji</forenames></author><author><keyname>Ma</keyname><forenames>Wenjing</forenames></author><author><keyname>Nagarajan</keyname><forenames>Chandrashekhar</forenames></author><author><keyname>Tomlin</keyname><forenames>John</forenames></author><author><keyname>Vassilvitskii</keyname><forenames>Sergei</forenames></author><author><keyname>Vee</keyname><forenames>Erik</forenames></author><author><keyname>Yang</keyname><forenames>Jian</forenames></author></authors><title>SHALE: An Efficient Algorithm for Allocation of Guaranteed Display
  Advertising</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by the problem of optimizing allocation in guaranteed display
advertising, we develop an efficient, lightweight method of generating a
compact {\em allocation plan} that can be used to guide ad server decisions.
The plan itself uses just O(1) state per guaranteed contract, is robust to
noise, and allows us to serve (provably) nearly optimally. The optimization
method we develop is scalable, with a small in-memory footprint, and working in
linear time per iteration. It is also &quot;stop-anytime&quot;, meaning that
time-critical applications can stop early and still get a good serving
solution. Thus, it is particularly useful for optimizing the large problems
arising in the context of display advertising. We demonstrate the effectiveness
of our algorithm using actual Yahoo! data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3620</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3620</id><created>2012-03-16</created><authors><author><keyname>Parmar</keyname><forenames>Keyur</forenames></author><author><keyname>Jinwala</keyname><forenames>Devesh</forenames></author></authors><title>A Novel Approach for Verifiable Secret Sharing by using a One Way Hash
  Function</title><categories>cs.CR</categories><comments>10 pages, Proceedings of the Proceedings of the 10th National
  Workshop on Cryptology, Cryptology Research Society of India and PSG
  Institute of Technology, Coimbatore, pp. 18-23, 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Threshold secret sharing schemes do not prevent any malicious behavior of the
dealer or shareholders and so we need verifiable secret sharing, to detect and
identify the cheaters, to achieve fair reconstruction of a secret. The problem
of verifiable secret sharing is to verify the shares distributed by the dealer.
A novel approach for verifiable secret sharing is presented in this paper where
both the dealer and shareholders are not assumed to be honest. In this paper,
we extend the term verifiable secret sharing to verify the shares, distributed
by a dealer as well as shares submitted by shareholders for secret
reconstruction, and to verify the reconstructed secret. Our proposed scheme
uses a one way hash function and probabilistic homomorphic encryption function
to provide verifiability and fair reconstruction of a secret.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3621</identifier>
 <datestamp>2012-08-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3621</id><created>2012-03-16</created><updated>2012-08-02</updated><authors><author><keyname>Hasegawa</keyname><forenames>Takehisa</forenames></author><author><keyname>Konno</keyname><forenames>Keita</forenames></author><author><keyname>Nemoto</keyname><forenames>Koji</forenames></author></authors><title>Robustness of correlated networks against propagating attacks</title><categories>physics.soc-ph cs.SI</categories><comments>6 pages, 13 figures</comments><journal-ref>Euro. Phys. J. B 85 (2012) 262</journal-ref><doi>10.1140/epjb/e2012-30290-0</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate robustness of correlated networks against propagating attacks
modeled by a susceptible-infected-removed model. By Monte-Carlo simulations, we
numerically determine the first critical infection rate, above which a global
outbreak of disease occurs, and the second critical infection rate, above which
disease disintegrates the network. Our result shows that correlated networks
are robust compared to the uncorrelated ones, regardless of whether they are
assortative or disassortative, when a fraction of infected nodes in an initial
state is not too large. For large initial fraction, disassortative network
becomes fragile while assortative network holds robustness. This behavior is
related to the layered network structure inevitably generated by a rewiring
procedure we adopt to realize correlated networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3622</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3622</id><created>2012-03-16</created><authors><author><keyname>Bhaladhare</keyname><forenames>Pawan R</forenames></author><author><keyname>Jinwala</keyname><forenames>Devesh</forenames></author></authors><title>A Sensitive Attribute based Clustering Method for kanonymization</title><categories>cs.CR</categories><comments>9 pages, Proceedings of the Cryptology Research Society of India and
  NIIT University sponsored National Workshop on Cryptology, 2011, India</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In medical organizations large amount of personal data are collected and
analyzed by the data miner or researcher, for further perusal. However, the
data collected may contain sensitive information such as specific disease of a
patient and should be kept confidential. Hence, the analysis of such data must
ensure due checks that ensure protection against threats to the individual
privacy. In this context, greater emphasis has now been given to the privacy
preservation algorithms in data mining research. One of the approaches is
anonymization approach that is able to protect private information; however,
valuable information can be lost. Therefore, the main challenge is how to
minimize the information loss during an anonymization process. The proposed
method is grouping similar data together based on sensitive attribute and then
anonymizes them. Our experimental results show the proposed method offers
better outcomes with respect to information loss and execution time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3623</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3623</id><created>2012-03-16</created><updated>2012-11-23</updated><authors><author><keyname>Wang</keyname><forenames>Zhe</forenames></author><author><keyname>Hu</keyname><forenames>Kai</forenames></author><author><keyname>Yin</keyname><forenames>Baolin</forenames></author></authors><title>An Improved Traffic Matrix Decomposition Method with Frequency-Domain
  Regularization</title><categories>cs.NI cs.NA</categories><comments>Accepted to IEICE Transactions on Information and Systems</comments><doi>10.1587/transinf.E96.D.731</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel network traffic matrix decomposition method named Stable
Principal Component Pursuit with Frequency-Domain Regularization (SPCP-FDR),
which improves the Stable Principal Component Pursuit (SPCP) method by using a
frequency-domain noise regularization function. An experiment demonstrates the
feasibility of this new decomposition method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3636</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3636</id><created>2012-03-16</created><authors><author><keyname>Berger</keyname><forenames>Annabell</forenames></author><author><keyname>M&#xfc;ller-Hannemann</keyname><forenames>Matthias</forenames></author></authors><title>How to Attack the NP-complete Dag Realization Problem in Practice</title><categories>cs.DS cs.DM</categories><comments>20 pages, 11 figures, extended abstract to appear in Proceedings of
  SEA 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the following fundamental realization problem of directed acyclic
graphs (dags). Given a sequence S:=(a_1,b_1),...,(a_n, b_n) with a_i, b_i in
Z_0^+, does there exist a dag (no parallel arcs allowed) with labeled vertex
set V:= {v_1,...,v_n} such that for all v_i in V indegree and outdegree of v_i
match exactly the given numbers a_i and b_i, respectively? Recently this
decision problem has been shown to be NP-complete by Nichterlein (2011).
However, we can show that several important classes of sequences are
efficiently solvable.
  In previous work (Berger and Mueller-Hannemann, FCT2011), we have proved that
yes-instances always have a special kind of topological order which allows us
to reduce the number of possible topological orderings in most cases
drastically. This leads to an exact exponential-time algorithm which
significantly improves upon a straightforward approach. Moreover, a combination
of this exponential-time algorithm with a special strategy gives a linear-time
algorithm. Interestingly, in systematic experiments we observed that we could
solve a huge majority of all instances by the linear-time heuristic. This
motivates us to develop characteristics like dag density and &quot;distance to
provably easy sequences&quot; which can give us an indicator how easy or difficult a
given sequence can be realized.
  Furthermore, we propose a randomized algorithm which exploits our structural
insight on topological sortings and uses a number of reduction rules. We
observe that it clearly outperforms all other variants and behaves surprisingly
well for almost all instances. Another striking observation is that our simple
linear-time algorithm solves a set of real-world instances from different
domains, namely ordered binary decision diagrams (OBDDs), train and flight
schedules, as well as instances derived from food-web networks without any
exception.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3644</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3644</id><created>2012-03-16</created><authors><author><keyname>Dulera</keyname><forenames>Shraddha</forenames></author><author><keyname>Jinwala</keyname><forenames>Devesh</forenames></author><author><keyname>Dasgupta</keyname><forenames>Aroop</forenames></author></authors><title>Experimenting with the Novel Approaches in Text Steganography</title><categories>cs.CR cs.MM</categories><comments>13 pages</comments><journal-ref>International Journal of Network Security &amp; Its Applications
  (IJNSA), Vol 3, Issue 6, pp. 213-225, ISSN: Online - 0974 - 9330, ISSN: Print
  - 0975 - 2307, November 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As is commonly known, the steganographic algorithms employ images, audio,
video or text files as the medium to ensure hidden exchange of information
between multiple contenders to protect the data from the prying eyes. However,
using text as the target medium is relatively difficult as compared to the
other target media, because of the lack of available redundant information in a
text file. In this paper, in the backdrop of the limitations in the prevalent
text based steganographic approaches, we propose simple, yet novel approaches
that overcome the same. Our approaches are based on combining the random
character sequence and feature coding methods to hide a character. We also
analytically evaluate the approaches based on metrics viz. hiding strength,
time overhead and memory overhead entailed. As compared to other methods, we
believe the approaches proposed impart increased randomness and thus aid higher
security at lower overhead.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3654</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3654</id><created>2012-03-16</created><authors><author><keyname>Patel</keyname><forenames>Sanjeev</forenames></author><author><keyname>Gupta</keyname><forenames>P. K.</forenames></author><author><keyname>Garg</keyname><forenames>Arjun</forenames></author><author><keyname>Mehrotra</keyname><forenames>Prateek</forenames></author><author><keyname>Chhabra</keyname><forenames>Manish</forenames></author></authors><title>Comparative Analysis of Congestion Control Algorithms Using ns-2</title><categories>cs.NI</categories><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 8,
  Issue 5, No 1, September 2011, ISSN (Online): 1694-0814</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to curtail the escalating packet loss rates caused by an exponential
increase in network traffic, active queue management techniques such as Random
Early Detection (RED) have come into picture. Flow Random Early Drop (FRED)
keeps state based on instantaneous queue occupancy of a given flow. FRED
protects fragile flows by deterministically accepting flows from low bandwidth
connections and fixes several shortcomings of RED by computing queue length
during both arrival and departure of the packet. Stochastic Fair Queuing (SFQ)
ensures fair access to network resources and prevents a busty flow from
consuming more than its fair share. In case of (Random Exponential Marking)
REM, the key idea is to decouple congestion measure from performance measure
(loss, queue length or delay). Stabilized RED (SRED) is another approach of
detecting nonresponsive flows. In this paper, we have shown a comparative
analysis of throughput, delay and queue length for the various congestion
control algorithms RED, SFQ and REM. We also included the comparative analysis
of loss rate having different bandwidth for these algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3659</identifier>
 <datestamp>2013-10-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3659</id><created>2012-03-16</created><updated>2013-10-09</updated><authors><author><keyname>Lapidoth</keyname><forenames>Amos</forenames><affiliation>Shitz</affiliation></author><author><keyname>Levy</keyname><forenames>Nathan</forenames><affiliation>Shitz</affiliation></author><author><keyname>Shamai</keyname><forenames>Shlomo</forenames><affiliation>Shitz</affiliation></author><author><keyname>Wigger</keyname><forenames>Michele</forenames></author></authors><title>Cognitive Wyner Networks with Clustered Decoding</title><categories>cs.IT math.IT</categories><comments>Second revision submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study an interference network where equally-numbered transmitters and
receivers lie on two parallel lines, each transmitter opposite its intended
receiver. We consider two short-range interference models: the &quot;asymmetric
network,&quot; where the signal sent by each transmitter is interfered only by the
signal sent by its left neighbor (if present), and a &quot;symmetric network,&quot; where
it is interfered by both its left and its right neighbors. Each transmitter is
cognizant of its own message, the messages of the $t_\ell$ transmitters to its
left, and the messages of the $t_r$ transmitters to its right. Each receiver
decodes its message based on the signals received at its own antenna, at the
$r_\ell$ receive antennas to its left, and the $r_r$ receive antennas to its
right. For such networks we provide upper and lower bounds on the multiplexing
gain, i.e., on the high-SNR asymptotic logarithmic growth of the sum-rate
capacity. In some cases our bounds meet, e.g., for the asymmetric network. Our
results exhibit an equivalence between the transmitter side-information
parameters $t_\ell, t_r$ and the receiver side-information parameters $r_\ell,
r_r$ in the sense that increasing/decreasing $t_\ell$ or $t_r$ by a positive
integer $\delta$ has the same effect on the multiplexing gain as
increasing/decreasing $r_\ell$ or $r_r$ by $\delta$. Moreover---even in
asymmetric networks---there is an equivalence between the left side-information
parameters $t_\ell, r_\ell$ and the right side-information parameters $t_r,
r_r$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3660</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3660</id><created>2012-03-16</created><authors><author><keyname>Khanum</keyname><forenames>Mohammadi Akheela</forenames></author><author><keyname>Fatima</keyname><forenames>Shameem</forenames></author><author><keyname>Chaurasia</keyname><forenames>Mousmi A.</forenames></author></authors><title>Arabic Interface Analysis Based on Cultural Markers</title><categories>cs.HC</categories><comments>8 pages, 9 figures, IJCSI International Journal of Computer Science
  Issues</comments><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 9,
  IJCSI Issue 1, No 2, January 2012, pp.255-262</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study examines the Arabic interface design elements that are largely
influenced by the cultural values. Cultural markers are examined in websites
from educational, business, and media. Cultural values analysis is based on
Geert Hofstede's cultural dimensions. The findings show that there are cultural
markers which are largely influenced by the culture and that the Hofstede's
score for Arab countries is partially supported by the website design
components examined in this study. Moderate support was also found for the long
term orientation, for which Hoftsede has no score.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3674</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3674</id><created>2012-03-16</created><authors><author><keyname>Musatov</keyname><forenames>Daniil</forenames></author></authors><title>Space-Bounded Kolmogorov Extractors</title><categories>cs.CC</categories><comments>12 pages, accepted to CSR2012</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  An extractor is a function that receives some randomness and either
&quot;improves&quot; it or produces &quot;new&quot; randomness. There are statistical and
algorithmical specifications of this notion. We study an algorithmical one
called Kolmogorov extractors and modify it to resource-bounded version of
Kolmogorov complexity. Following Zimand we prove the existence of such objects
with certain parameters. The utilized technique is &quot;naive&quot; derandomization: we
replace random constructions employed by Zimand by pseudo-random ones obtained
by Nisan-Wigderson generator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3688</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3688</id><created>2012-03-16</created><authors><author><keyname>Alshehri</keyname><forenames>Mohammad</forenames></author><author><keyname>Aldabbas</keyname><forenames>Hamza</forenames></author><author><keyname>Sawle</keyname><forenames>James</forenames></author><author><keyname>Baqar</keyname><forenames>Mai Abu</forenames></author></authors><title>Adopting E-commerce to User's Needs</title><categories>cs.CY</categories><comments>12 pages, 3 figures; Inteernation Journal of Computer Science &amp;
  Engineering Survey, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The objectives of this paper are to identify and analyse the extent to which
the site is fulfilling all the user's requirements and needs. The related works
comprise the history of interactive design and the benefits of user-centered
development, which is the methodology followed in this survey. Moreover, there
is a brief comparison between Waterfall and User-centered methodology in terms
of addressing the issues of time saving and addressing fulfilment of users'
needs. The data required to conduct this study was acquired using two research
methods; the questionnaire and direct user observation, in order to address all
the performance related attributes in the usability stage of the evaluation. An
evaluation of the website, based on statements of usability goals and criteria,
was undertaken in relation to the implementation and testing of the new design.
JARIR bookstore website was chosen as a case study in this paper to investigate
the usability and interactivity of the website design. The analysis section
includes needs, users and tasks and data analysis, whereas the design phase
covers the user interface and database design. At the end of this paper, some
recommendations are presented regarding JARIR website that can be taken into
account when developing the website in the future.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3704</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3704</id><created>2012-03-16</created><authors><author><keyname>Karagiannis</keyname><forenames>Marios</forenames></author><author><keyname>Chatzigiannakis</keyname><forenames>Ioannis</forenames></author><author><keyname>Rolim</keyname><forenames>Jose</forenames></author></authors><title>Multilateration: Methods For Clustering Intersection Points For Wireless
  Sensor Networks Localization With Distance Estimation Error</title><categories>cs.NI</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we describe three methods for localizing a wireless sensor
network node, using anchor nodes in its neighbourhood, when there is an error
in distance estimation present. We use the intersection points of the circles
formed with the estimated distances from each anchors and we apply different
methods to form clusters. We then use the cluster points to calculate the final
position.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3705</identifier>
 <datestamp>2014-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3705</id><created>2012-03-16</created><updated>2014-03-18</updated><authors><author><keyname>Je&#x17c;</keyname><forenames>Artur</forenames></author></authors><title>Recompression: a simple and powerful technique for word equations</title><categories>cs.FL cs.LO</categories><comments>Submitted to a journal. Since previous version the proofs were
  simplified, overall presentation improved</comments><acm-class>F.2.2; F.4.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present an application of a simple technique of local
recompression, previously developed by the author in the context of compressed
membership problems and compressed pattern matching, to word equations. The
technique is based on local modification of variables (replacing X by aX or Xa)
and iterative replacement of pairs of letters appearing in the equation by a
`fresh' letter, which can be seen as a bottom-up compression of the solution of
the given word equation, to be more specific, building an SLP (Straight-Line
Programme) for the solution of the word equation.
  Using this technique we give a new, independent and self-contained proofs of
most of the known results for word equations. To be more specific, the
presented (nondeterministic) algorithm runs in O(n log n) space and in time
polynomial in log N, where N is the size of the length-minimal solution of the
word equation. The presented algorithm can be easily generalised to a generator
of all solutions of the given word equation (without increasing the space
usage). Furthermore, a further analysis of the algorithm yields a doubly
exponential upper bound on the size of the length-minimal solution. The
presented algorithm does not use exponential bound on the exponent of
periodicity. Conversely, the analysis of the algorithm yields an independent
proof of the exponential bound on exponent of periodicity.
  We believe that the presented algorithm, its idea and analysis are far
simpler than all previously applied. Furthermore, thanks to it we can obtain a
unified and simple approach to most of known results for word equations.
  As a small additional result we show that for O(1) variables (with arbitrary
many appearances in the equation) word equations can be solved in linear space,
i.e. they are context-sensitive.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3706</identifier>
 <datestamp>2012-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3706</id><created>2012-03-16</created><updated>2012-03-23</updated><authors><author><keyname>Hantry</keyname><forenames>Francois</forenames></author><author><keyname>Sa&#xef;s</keyname><forenames>Lakhdar</forenames></author><author><keyname>Hacid</keyname><forenames>Mohand-Sa&#xef;d</forenames></author></authors><title>On the Complexity of Computing Minimal Unsatisfiable LTL formulas</title><categories>cs.LO</categories><comments>Minimal unsatisfiable cores For LTL causes inherent vacuity checking
  redundancy coverage</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that (1) the Minimal False QCNF search-problem (MF-search) and the
Minimal Unsatisfiable LTL formula search problem (MU-search) are FPSPACE
complete because of the very expressive power of QBF/LTL, (2) we extend the
PSPACE-hardness of the MF decision problem to the MU decision problem. As a
consequence, we deduce a positive answer to the open question of PSPACE
hardness of the inherent Vacuity Checking problem. We even show that the
Inherent Non Vacuous formula search problem is also FPSPACE-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3722</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3722</id><created>2012-03-15</created><authors><author><keyname>Mahmou</keyname><forenames>Raja</forenames></author><author><keyname>Faitah</keyname><forenames>Khalid</forenames></author></authors><title>Designing of RF Single Balanced Mixer with a 65nm CMOS Technology
  Dedicated to Low Power Consumption Wireless Applications</title><categories>cs.OH</categories><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 9,
  Issue 1, No 3, January 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The present work consists of designing a Single Balanced Mixer(SBM) with the
65 nm CMOS technology, this for a 1.9 GHz RF channel, dedicated to wireless
applications. This paper shows; the polarization chosen for this structure,
models of evaluating parameters of the mixer, then simulation of the circuit in
65nm CMOS technology and comparison with previously treated. Keywords: SBM
Mixer, Radio Frequency, 65 nm CMOS Technology, Non-Linearity, Power
Consumption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3724</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3724</id><created>2012-03-16</created><updated>2012-03-22</updated><authors><author><keyname>Min&#xe9;</keyname><forenames>Antoine</forenames></author></authors><title>Static Analysis of Run-Time Errors in Embedded Real-Time Parallel C
  Programs</title><categories>cs.PL cs.LO</categories><proxy>Logical Methods In Computer Science</proxy><acm-class>D.2.4; F.3.1; F.3.2</acm-class><journal-ref>Logical Methods in Computer Science, Volume 8, Issue 1 (March 26,
  2012) lmcs:799</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We present a static analysis by Abstract Interpretation to check for run-time
errors in parallel and multi-threaded C programs. Following our work on
Astr\'ee, we focus on embedded critical programs without recursion nor dynamic
memory allocation, but extend the analysis to a static set of threads
communicating implicitly through a shared memory and explicitly using a finite
set of mutual exclusion locks, and scheduled according to a real-time
scheduling policy and fixed priorities. Our method is thread-modular. It is
based on a slightly modified non-parallel analysis that, when analyzing a
thread, applies and enriches an abstract set of thread interferences. An
iterator then re-analyzes each thread in turn until interferences stabilize. We
prove the soundness of our method with respect to the sequential consistency
semantics, but also with respect to a reasonable weakly consistent memory
semantics. We also show how to take into account mutual exclusion and thread
priorities through a partitioning over an abstraction of the scheduler state.
We present preliminary experimental results analyzing an industrial program
with our prototype, Th\'es\'ee, and demonstrate the scalability of our
approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3725</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3725</id><created>2012-03-14</created><authors><author><keyname>Everitt</keyname><forenames>Richard G.</forenames></author></authors><title>Bayesian Parameter Estimation for Latent Markov Random Fields and Social
  Networks</title><categories>stat.CO cond-mat.stat-mech cs.AI cs.SI physics.data-an</categories><comments>26 pages, 2 figures, accepted in Journal of Computational and
  Graphical Statistics (http://www.amstat.org/publications/jcgs.cfm)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Undirected graphical models are widely used in statistics, physics and
machine vision. However Bayesian parameter estimation for undirected models is
extremely challenging, since evaluation of the posterior typically involves the
calculation of an intractable normalising constant. This problem has received
much attention, but very little of this has focussed on the important practical
case where the data consists of noisy or incomplete observations of the
underlying hidden structure. This paper specifically addresses this problem,
comparing two alternative methodologies. In the first of these approaches
particle Markov chain Monte Carlo (Andrieu et al., 2010) is used to efficiently
explore the parameter space, combined with the exchange algorithm (Murray et
al., 2006) for avoiding the calculation of the intractable normalising constant
(a proof showing that this combination targets the correct distribution in
found in a supplementary appendix online). This approach is compared with
approximate Bayesian computation (Pritchard et al., 1999). Applications to
estimating the parameters of Ising models and exponential random graphs from
noisy data are presented. Each algorithm used in the paper targets an
approximation to the true posterior due to the use of MCMC to simulate from the
latent graphical model, in lieu of being able to do this exactly in general.
The supplementary appendix also describes the nature of the resulting
approximation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3727</identifier>
 <datestamp>2012-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3727</id><created>2012-03-16</created><updated>2012-10-25</updated><authors><author><keyname>Perez</keyname><forenames>Anthony</forenames></author></authors><title>Linear vertex-kernels for several dense ranking r-CSPs</title><categories>cs.DM cs.DS</categories><comments>Several flaws (Lemma 2.17) appeared in the previous version; this
  version corrects them</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Ranking r-Constraint Satisfaction Problem (ranking r-CSP) consists of a
ground set of vertices V, an arity r &gt;= 2, a parameter k and a constraint
system c, where c is a function which maps rankings of r-sized subsets of V to
{0,1}. The objective is to decide if there exists a ranking of the vertices
satisfying all but at most k constraints. Famous ranking r-CSP include the
Feedback Arc Set in Tournaments and Betweenness in Tournaments problems. We
consider these problems from the kernelization viewpoint. We prove that
so-called l_r-simply characterized ranking r-CSPs admit linear vertex-kernels
whenever they admit constant-factor approximation algorithms. This implies that
r-Betweenness in Tournaments and r-Transitive Feedback Arc Set In Tournaments,
two natural generalizations of the previously mentioned problems, admit linear
vertex-kernels. Moreover, we introduce another generalization of Feedback Arc
Set in Tournaments, which does not fit the aforementioned framework. We obtain
a 5-approximation and a linear vertex-kernel for this problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3729</identifier>
 <datestamp>2012-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3729</id><created>2012-03-16</created><authors><author><keyname>Kayarkar</keyname><forenames>Harshavardhan</forenames></author></authors><title>A Survey on Security Issues in Ad Hoc Routing Protocols and their
  Mitigation Techniques</title><categories>cs.CR</categories><comments>14 Pages, 1 Table</comments><journal-ref>International Journal of Advanced Networking and Application, Vol.
  03, Issue 05, pp. 1338-1351, March-April, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile Ad hoc Networks (MANETS) are transient networks of mobile nodes,
connected through wireless links, without any fixed infrastructure or central
management. Due to the self-configuring nature of these networks, the topology
is highly dynamic. This makes the Ad Hoc Routing Protocols in MANETS highly
vulnerable to serious security issues. In this paper, we survey the common
security threats and attacks and summarize the solutions suggested in the
survey to mitigate these security vulnerabilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3730</identifier>
 <datestamp>2012-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3730</id><created>2012-03-16</created><updated>2012-04-24</updated><authors><author><keyname>Bruttomesso</keyname><forenames>Roberto</forenames></author><author><keyname>Ghilardi</keyname><forenames>Silvio</forenames></author><author><keyname>Ranise</keyname><forenames>Silvio</forenames></author></authors><title>From Strong Amalgamability to Modularity of Quantifier-Free
  Interpolation</title><categories>cs.LO</categories><report-no>RI 337-12</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of interpolants in verification is gaining more and more importance.
Since theories used in applications are usually obtained as (disjoint)
combinations of simpler theories, it is important to modularly re-use
interpolation algorithms for the component theories. We show that a sufficient
and necessary condition to do this for quantifier-free interpolation is that
the component theories have the 'strong (sub-)amalgamation' property. Then, we
provide an equivalent syntactic characterization, identify a sufficient
condition, and design a combined quantifier-free interpolation algorithm
capable of handling both convex and non-convex theories, that subsumes and
extends most existing work on combined interpolation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3744</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3744</id><created>2012-03-16</created><authors><author><keyname>Blundo</keyname><forenames>Carlo</forenames></author><author><keyname>Cimato</keyname><forenames>Stelvio</forenames></author></authors><title>Constrained Role Mining</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Role Based Access Control (RBAC) is a very popular access control model, for
long time investigated and widely deployed in the security architecture of
different enterprises. To implement RBAC, roles have to be firstly identified
within the considered organization. Usually the process of (automatically)
defining the roles in a bottom up way, starting from the permissions assigned
to each user, is called {\it role mining}. In literature, the role mining
problem has been formally analyzed and several techniques have been proposed in
order to obtain a set of valid roles.
  Recently, the problem of defining different kind of constraints on the number
and the size of the roles included in the resulting role set has been
addressed. In this paper we provide a formal definition of the role mining
problem under the cardinality constraint, i.e. restricting the maximum number
of permissions that can be included in a role. We discuss formally the
computational complexity of the problem and propose a novel heuristic.
Furthermore we present experimental results obtained after the application of
the proposed heuristic on both real and synthetic datasets, and compare the
resulting performance to previous proposals
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3758</identifier>
 <datestamp>2012-03-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3758</id><created>2012-03-16</created><updated>2012-03-29</updated><authors><author><keyname>Henshall</keyname><forenames>Dane</forenames></author><author><keyname>Shallit</keyname><forenames>Jeffrey</forenames></author></authors><title>Automatic Theorem-Proving in Combinatorics on Words</title><categories>cs.FL cs.LO</categories><comments>This revision includes a new result on the Rudin-Shapiro sequence</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a technique for mechanically proving certain kinds of theorems in
combinatorics on words, using automata and a package for manipulating them. We
illustrate our technique by solving, purely mechanically, an open problem of
Currie and Saari on the lengths of unbordered factors in the Thue-Morse
sequence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3764</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3764</id><created>2012-03-16</created><authors><author><keyname>Ashish</keyname><forenames>Naveen</forenames></author><author><keyname>Biswas</keyname><forenames>Antarip</forenames></author><author><keyname>Das</keyname><forenames>Sumit</forenames></author><author><keyname>Nag</keyname><forenames>Saurav</forenames></author><author><keyname>Pratap</keyname><forenames>Rajiv</forenames></author></authors><title>The Abzooba Smart Health Informatics Platform (SHIP) TM - From Patient
  Experiences to Big Data to Insights</title><categories>cs.IR cs.AI</categories><comments>3 pages</comments><report-no>ABZ-TR-2012-1</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a technology to connect patients to information in the
experiences of other patients by using the power of structured big data. The
approach, implemented in the Abzooba Smart Health Informatics Platform
(SHIP),is to distill concepts of facts and expressions from conversations and
discussions in health social media forums, and use those distilled concepts in
connecting patients to experiences and insights that are highly relevant to
them in particular. We envision our work, in progress, to provide new and
effective tools to exploit the richness of content in social media in health
for outcomes research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3772</identifier>
 <datestamp>2012-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3772</id><created>2012-03-16</created><authors><author><keyname>Babaie</keyname><forenames>Shahram</forenames></author><author><keyname>Pirahesh</keyname><forenames>Seyed Sajad</forenames></author></authors><title>Hole Detection for Increasing Coverage in Wireless Sensor Network Using
  Triangular Structure</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The emerging technology of wireless sensor network (WSN) is expected to
provide a broad range of applications, such as battlefield surveillance,
environmental monitoring, smart spaces and so on. The coverage problem is a
fundamental issue in WSN, which mainly concerns with a fundamental question:
How well a sensor field is observed by the deployed sensors? Mobility is
exploited to improve area coverage in a kind of hybrid sensor networks. The
main objective for using mobile sensor nodes is to heal coverage holes after
the initial network deployment, when designing a hole healing algorithm, the
following issues need to be addressed. First, how to decide the existence of a
coverage hole and how to estimate the size of a hole. Second, what are the best
target locations to relocate mobile nodes to repair coverage holes? We use the
triangular oriented diagram (HSTT) for aim to goal where its simple, have low
calculation among construction and it is great to calculate the size of hole
exactly .
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3783</identifier>
 <datestamp>2012-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3783</id><created>2012-03-16</created><authors><author><keyname>Montavon</keyname><forenames>Gr&#xe9;goire</forenames></author><author><keyname>M&#xfc;ller</keyname><forenames>Klaus-Robert</forenames></author></authors><title>Learning Feature Hierarchies with Centered Deep Boltzmann Machines</title><categories>stat.ML cs.AI cs.LG</categories><doi>10.1007/978-3-642-35289-8_33</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep Boltzmann machines are in principle powerful models for extracting the
hierarchical structure of data. Unfortunately, attempts to train layers jointly
(without greedy layer-wise pretraining) have been largely unsuccessful. We
propose a modification of the learning algorithm that initially recenters the
output of the activation functions to zero. This modification leads to a better
conditioned Hessian and thus makes learning easier. We test the algorithm on
real data and demonstrate that our suggestion, the centered deep Boltzmann
machine, learns a hierarchy of increasingly abstract representations and a
better generative model of data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3800</identifier>
 <datestamp>2015-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3800</id><created>2012-03-16</created><updated>2015-04-10</updated><authors><author><keyname>Lopez-Sandoval</keyname><forenames>E.</forenames></author><author><keyname>Mello</keyname><forenames>A.</forenames></author><author><keyname>Godina-Nava</keyname><forenames>J. J.</forenames></author><author><keyname>Samana</keyname><forenames>A. R.</forenames></author></authors><title>Power Series Method applied to Inverse Analysis in Chemical Kinetics
  Problem</title><categories>cs.NA math.NA</categories><comments>12 pages, 2 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Power Series Solution Method has been traditionally used to solve Ordinary
and Partial Linear Differential Equations. However, despite their usefulness
the application of this method has been limited to this particular kind of
equations. In this work we use the method of power series to solve nonlinear
partial differential equations. The method is applied to solve three versions
of nonlinear time-dependent Burgers-Type differential equations in order to
demonstrate its scope and applicability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3811</identifier>
 <datestamp>2012-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3811</id><created>2012-03-16</created><authors><author><keyname>Sitharam</keyname><forenames>Meera</forenames></author><author><keyname>Ozkan</keyname><forenames>Aysegul</forenames></author><author><keyname>Pence</keyname><forenames>James</forenames></author><author><keyname>Peters</keyname><forenames>Jorg</forenames></author></authors><title>EASAL: Efficient Atlasing, Analysis and Search of Molecular Assembly
  Landscapes</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To elucidate the structure of assembly configuration spaces, the EASAL
software combines classical concepts, such as stratifications of semialgebraic
sets, with recent algorithms for efficiently realizing geometric constraint
systems, and theoretical advances concerning convex parametrization: in
contrast to folding configuration spaces, most regions of assembly and packing
configurations admit a convex parametrization. This allows for a novel,
efficient and intuitive representation of configuration spaces so that the
corresponding atlas can be efficiently generated and sampled. This paper
describes the approach, theory, structure and algorithms underlying EASAL and
outlines its use for generating atlases of dimeric assemblies of the AAV2 coat
protein, and alpha helix packing in transmembrane proteins.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3814</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3814</id><created>2012-03-16</created><updated>2012-03-28</updated><authors><author><keyname>Adler</keyname><forenames>Isolde</forenames></author><author><keyname>Weyer</keyname><forenames>Mark</forenames></author></authors><title>Tree-width for first order formulae</title><categories>cs.LO</categories><proxy>Logical Methods In Computer Science</proxy><acm-class>F.2; F.4.1; H.2.3</acm-class><journal-ref>Logical Methods in Computer Science, Volume 8, Issue 1 (March 29,
  2012) lmcs:786</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce tree-width for first order formulae \phi, fotw(\phi). We show
that computing fotw is fixed-parameter tractable with parameter fotw. Moreover,
we show that on classes of formulae of bounded fotw, model checking is fixed
parameter tractable, with parameter the length of the formula. This is done by
translating a formula \phi\ with fotw(\phi)&lt;k into a formula of the k-variable
fragment L^k of first order logic. For fixed k, the question whether a given
first order formula is equivalent to an L^k formula is undecidable. In
contrast, the classes of first order formulae with bounded fotw are fragments
of first order logic for which the equivalence is decidable.
  Our notion of tree-width generalises tree-width of conjunctive queries to
arbitrary formulae of first order logic by taking into account the quantifier
interaction in a formula. Moreover, it is more powerful than the notion of
elimination-width of quantified constraint formulae, defined by Chen and Dalmau
(CSL 2005): for quantified constraint formulae, both bounded elimination-width
and bounded fotw allow for model checking in polynomial time. We prove that
fotw of a quantified constraint formula \phi\ is bounded by the
elimination-width of \phi, and we exhibit a class of quantified constraint
formulae with bounded fotw, that has unbounded elimination-width. A similar
comparison holds for strict tree-width of non-recursive stratified datalog as
defined by Flum, Frick, and Grohe (JACM 49, 2002).
  Finally, we show that fotw has a characterization in terms of a cops and
robbers game without monotonicity cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3815</identifier>
 <datestamp>2012-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3815</id><created>2012-03-15</created><updated>2012-08-28</updated><authors><author><keyname>Kutyniok</keyname><forenames>Gitta</forenames></author></authors><title>Theory and Applications of Compressed Sensing</title><categories>cs.IT math.IT math.NA</categories><comments>22 pages, 2 figures, minor revisions, slightly changed title</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compressed sensing is a novel research area, which was introduced in 2006,
and since then has already become a key concept in various areas of applied
mathematics, computer science, and electrical engineering. It surprisingly
predicts that high-dimensional signals, which allow a sparse representation by
a suitable basis or, more generally, a frame, can be recovered from what was
previously considered highly incomplete linear measurements by using efficient
algorithms. This article shall serve as an introduction to and a survey about
compressed sensing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3832</identifier>
 <datestamp>2012-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3832</id><created>2012-03-16</created><authors><author><keyname>Yadav</keyname><forenames>Surjeet Kumar</forenames></author><author><keyname>Pal</keyname><forenames>Saurabh</forenames></author></authors><title>Data Mining: A Prediction for Performance Improvement of Engineering
  Students using Classification</title><categories>cs.LG</categories><comments>6 pages, 3 Figures. arXiv admin note: substantial text overlap with
  arXiv:1202.4815</comments><journal-ref>World of Computer Science and Information Technology Journal
  WCSIT, Vol. 2, No. 2, 51-56, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Now-a-days the amount of data stored in educational database increasing
rapidly. These databases contain hidden information for improvement of
students' performance. Educational data mining is used to study the data
available in the educational field and bring out the hidden knowledge from it.
Classification methods like decision trees, Bayesian network etc can be applied
on the educational data for predicting the student's performance in
examination. This prediction will help to identify the weak students and help
them to score better marks. The C4.5, ID3 and CART decision tree algorithms are
applied on engineering student's data to predict their performance in the final
exam. The outcome of the decision tree predicted the number of students who are
likely to pass, fail or promoted to next year. The results provide steps to
improve the performance of the students who were predicted to fail or promoted.
After the declaration of the results in the final examination the marks
obtained by the students are fed into the system and the results were analyzed
for the next session. The comparative analysis of the results states that the
prediction has helped the weaker students to improve and brought out betterment
in the result.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3838</identifier>
 <datestamp>2012-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3838</id><created>2012-03-17</created><authors><author><keyname>Chittineni</keyname><forenames>Suneetha</forenames></author><author><keyname>Bhogapathi</keyname><forenames>Raveendra Babu</forenames></author></authors><title>A Study on the Behavior of a Neural Network for Grouping the Data</title><categories>cs.NE cs.RO</categories><comments>7 pages,2 figures,9 tables</comments><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 9,
  Issue 1, No 1, January 2012, pp:228-234</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the frequently stated advantages of neural networks is that they can
work effectively with non-normally distributed data. But optimal results are
possible with normalized data.In this paper, how normality of the input affects
the behaviour of a K-means fast learning artificial neural network(KFLANN) for
grouping the data is presented. Basically, the grouping of high dimensional
input data is controlled by additional neural network input parameters namely
vigilance and tolerance.Neural networks learn faster and give better
performance if the input variables are pre-processed before being fed to the
input units of the neural network. A common way of dealing with data that is
not normally distributed is to perform some form of mathematical transformation
on the data that shifts it towards a normal distribution.In a neural network,
data preprocessing transforms the data into a format that will be more easily
and effectively processed for the purpose of the user. Among various methods,
Normalization is one which organizes data for more efficient access.
Experimental results on several artificial and synthetic data sets indicate
that the groups formed in the data vary with non-normally distributed data and
normalized data and also depends on the normalization method used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3847</identifier>
 <datestamp>2012-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3847</id><created>2012-03-17</created><authors><author><keyname>Sharma</keyname><forenames>Anshuman</forenames></author></authors><title>Handwritten digit Recognition using Support Vector Machine</title><categories>cs.NE</categories><comments>7 page</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Handwritten Numeral recognition plays a vital role in postal automation
services especially in countries like India where multiple languages and
scripts are used Discrete Hidden Markov Model (HMM) and hybrid of Neural
Network (NN) and HMM are popular methods in handwritten word recognition
system. The hybrid system gives better recognition result due to better
discrimination capability of the NN. A major problem in handwriting recognition
is the huge variability and distortions of patterns. Elastic models based on
local observations and dynamic programming such HMM are not efficient to absorb
this variability. But their vision is local. But they cannot face to length
variability and they are very sensitive to distortions. Then the SVM is used to
estimate global correlations and classify the pattern. Support Vector Machine
(SVM) is an alternative to NN. In Handwritten recognition, SVM gives a better
recognition result. The aim of this paper is to develop an approach which
improve the efficiency of handwritten recognition using artificial neural
network
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3854</identifier>
 <datestamp>2012-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3854</id><created>2012-03-17</created><authors><author><keyname>Letchford</keyname><forenames>Adam N.</forenames></author><author><keyname>Nasiri</keyname><forenames>Saeideh D.</forenames></author><author><keyname>Theis</keyname><forenames>Dirk Oliver</forenames></author></authors><title>Compact Formulations of the Steiner Traveling Salesman Problem and
  Related Problems</title><categories>math.OC cs.DM</categories><comments>submitted to EJOR</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Steiner Traveling Salesman Problem (STSP) is a variant of the Traveling
Salesman Problem (TSP) that is particularly suitable when dealing with sparse
networks, such as road networks. The standard integer programming formulation
of the STSP has an exponential number of constraints, just like the standard
formulation of the TSP. On the other hand, there exist several known {\em
compact} formulations of the TSP, i.e., formulations with a polynomial number
of both variables and constraints. In this paper, we show that some of these
compact formulations can be adapted to the STSP. We also briefly discuss the
adaptation of our formulations to some closely-related problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3864</identifier>
 <datestamp>2012-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3864</id><created>2012-03-17</created><updated>2012-06-21</updated><authors><author><keyname>Kyrillidis</keyname><forenames>Anastasios</forenames></author><author><keyname>Cevher</keyname><forenames>Volkan</forenames></author></authors><title>Matrix ALPS: Accelerated Low Rank and Sparse Matrix Reconstruction</title><categories>cs.IT math.IT</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose Matrix ALPS for recovering a sparse plus low-rank decomposition of
a matrix given its corrupted and incomplete linear measurements. Our approach
is a first-order projected gradient method over non-convex sets, and it
exploits a well-known memory-based acceleration technique. We theoretically
characterize the convergence properties of Matrix ALPS using the stable
embedding properties of the linear measurement operator. We then numerically
illustrate that our algorithm outperforms the existing convex as well as
non-convex state-of-the-art algorithms in computational efficiency without
sacrificing stability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3866</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3866</id><created>2012-03-17</created><updated>2013-01-08</updated><authors><author><keyname>Tsay</keyname><forenames>Joe-Kai</forenames></author><author><keyname>Mj&#xf8;lsnes</keyname><forenames>Stig</forenames></author></authors><title>Computational Security Analysis of the UMTS and LTE Authentication and
  Key Agreement Protocols</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a computational security analysis of the Authentication and Key
Agreement (AKA) protocols for both Long-Term Evolution (LTE) and Universal
Mobile Telecommunications System (UMTS). This work constitutes the first
security analysis of LTE AKA to date and the first computationally sound
analysis of UMTS AKA. Our work is the first formal analysis to consider
messages that are sent in the core network, where we take into account details
of the carrying protocol (i.e., MAP or Diameter) and of the mechanism for
secure transport (i.e., MAPsec/TCAPsec or IPsec ESP). Moreover, we report on a
deficiency in the protocol specifications of UMTS AKA and LTE AKA and the
specifications of the core network security (called network domain security),
which may enable efficient attacks. The vulnerability allows an inside attacker
not only to impersonate an honest protocol participant during a run of the
protocol but also to subsequently use wireless services on his behalf. UMTS AKA
run over MAP with MAPsec seems vulnerable in the most straight-forward
application of the attack. On the other hand, our analysis shows that UMTS and
LTE AKA over Diameter/IPsec and UMTS AKA over MAP/TCAPsec (with sufficiently
long session identifiers) computationally satisfy intended authentication
properties as well as some key secrecy properties, assuming that the used
primitives meet standard cryptographic assumptions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3870</identifier>
 <datestamp>2012-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3870</id><created>2012-03-17</created><authors><author><keyname>D'Acquisto</keyname><forenames>Giuseppe</forenames></author><author><keyname>Naldi</keyname><forenames>Maurizio</forenames></author><author><keyname>Italiano</keyname><forenames>Giuseppe F.</forenames></author></authors><title>Personal data disclosure and data breaches: the customer's viewpoint</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Every time the customer (individual or company) has to release personal
information to its service provider (e.g., an online store or a cloud computing
provider), it faces a trade-off between the benefits gained (enhanced or
cheaper services) and the risks it incurs (identity theft and fraudulent uses).
The amount of personal information released is the major decision variable in
that trade-off problem, and has a proxy in the maximum loss the customer may
incur. We find the conditions for a unique optimal solution to exist for that
problem as that maximizing the customer's surplus. We also show that the
optimal amount of personal information is influenced most by the immediate
benefits the customer gets, i.e., the price and the quantity of service offered
by the service provider, rather than by maximum loss it may incur. Easy
spenders take larger risks with respect to low-spenders, but an increase in
price drives customers towards a more careful risk-taking attitude anyway. A
major role is also played by the privacy level, which the service provider
employs to regulate the benefits released to the customers. We also provide a
closed form solution for the limit case of a perfectly secure provider, showing
that the results do not differ significantly from those obtained in the general
case. The trade-off analysis may be employed by the customer to determine its
level of exposure in the relationship with its service provider.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3879</identifier>
 <datestamp>2012-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3879</id><created>2012-03-17</created><authors><author><keyname>Tan</keyname><forenames>Bo</forenames></author><author><keyname>Tompson</keyname><forenames>John</forenames></author></authors><title>Powerline Communications Channel Modelling Methodology Based on
  Statistical Features</title><categories>cs.IT math.IT</categories><comments>9 pages, 16 figures, final version for IEEE Power Delivery Trans</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a new channel modelling method for powerline
communications networks based on the multipath profile in the time domain. The
new channel model is developed to be applied in a range of Powerline
Communications (PLC) research topics such as impulse noise modelling,
deployment and coverage studies, and communications theory analysis. To develop
the methodology, channels are categorised according to their propagation
distance and power delay profile. The statistical multipath parameters such as
path arrival time, magnitude and interval for each category are analyzed to
build the model. Each generated channel based on the proposed statistical model
represents a different realisation of a PLC network. Simulation results in
similar the time and frequency domains show that the proposed statistical
modelling method, which integrates the impact of network topology presents the
PLC channel features as the underlying transmission line theory model.
Furthermore, two potential application scenarios are described to show the
channel model is applicable to capacity analysis and correlated impulse noise
modelling for PLC networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3883</identifier>
 <datestamp>2012-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3883</id><created>2012-03-17</created><authors><author><keyname>Sergeev</keyname><forenames>Igor S.</forenames></author></authors><title>A note on the fast power series' exponential</title><categories>cs.DS</categories><comments>7 pages, in English; 7 pages, in Russian</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is shown that the exponential of a complex power series up to order n can
be implemented via (23/12+o(1))M(n) binary arithmetic operations over complex
field, where M(n) stands for the (smoothed) complexity of multiplication of
polynomials of degree &lt;n in FFT-model. Yet, it is shown how to raise a power
series to a constant power with the complexity (27/8+o(1))M(n).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3885</identifier>
 <datestamp>2012-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3885</id><created>2012-03-17</created><authors><author><keyname>Caprarescu</keyname><forenames>Bogdan Alexandru</forenames></author><author><keyname>Petcu</keyname><forenames>Dana</forenames></author></authors><title>Decentralized Probabilistic Auto-Scaling for Heterogeneous Systems</title><categories>cs.DC</categories><comments>Submitted to ADAPTIVE2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The DEPAS (Decentralized Probabilistic Auto-Scaling) algorithm assumes an
overlay network of computing nodes where each node probabilistically decides to
shut down, allocate one or more other nodes or do nothing. DEPAS was
formulated, tested, and theoretically analyzed for the simplified case of
homogenous systems. In this paper, we extend DEPAS to heterogeneous systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3887</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3887</id><created>2012-03-17</created><updated>2013-04-22</updated><authors><author><keyname>Anandkumar</keyname><forenames>Animashree</forenames></author><author><keyname>Valluvan</keyname><forenames>Ragupathyraj</forenames></author></authors><title>Learning loopy graphical models with latent variables: Efficient methods
  and guarantees</title><categories>stat.ML cs.AI cs.LG math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/12-AOS1070 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS1070</report-no><journal-ref>Annals of Statistics 2013, Vol. 41, No. 2, 401-435</journal-ref><doi>10.1214/12-AOS1070</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of structure estimation in graphical models with latent variables
is considered. We characterize conditions for tractable graph estimation and
develop efficient methods with provable guarantees. We consider models where
the underlying Markov graph is locally tree-like, and the model is in the
regime of correlation decay. For the special case of the Ising model, the
number of samples $n$ required for structural consistency of our method scales
as $n=\Omega(\theta_{\min}^{-\delta\eta(\eta+1)-2}\log p)$, where p is the
number of variables, $\theta_{\min}$ is the minimum edge potential, $\delta$ is
the depth (i.e., distance from a hidden node to the nearest observed nodes),
and $\eta$ is a parameter which depends on the bounds on node and edge
potentials in the Ising model. Necessary conditions for structural consistency
under any algorithm are derived and our method nearly matches the lower bound
on sample requirements. Further, the proposed method is practical to implement
and provides flexibility to control the number of latent variables and the
cycle lengths in the output graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3909</identifier>
 <datestamp>2012-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3909</id><created>2012-03-17</created><updated>2012-09-23</updated><authors><author><keyname>Jaberi</keyname><forenames>Nasrin</forenames></author></authors><title>An Open Question about Dependency of Life Time of Hardware Components
  and Dynamic Voltage Scaling</title><categories>cs.OH</categories><comments>Some problem in the equtions</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Open question about Dependency of Life Time of Hardware Components and
Dynamic Voltage Scaling (A primary idea)
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3919</identifier>
 <datestamp>2012-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3919</id><created>2012-03-18</created><updated>2012-07-31</updated><authors><author><keyname>Anshari</keyname><forenames>Muhammad</forenames></author><author><keyname>Almunawar</keyname><forenames>Mohammad Nabil</forenames></author></authors><title>Building Healthcare - Patient Relationship with CRM 2.0: Lesson Learnt
  from Prita Mulyasari's Case</title><categories>cs.OH</categories><comments>5 pages; International Seminar ASEAN on Green Technology, Social Work
  and Public Health for Development of Indonesia, 2011. arXiv admin note: text
  overlap with arXiv:1204.3689, arXiv:1204.3685, arXiv:1203.4309</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Healthcare is implementing CRM as a strategy for managing interactions and
communication with patients which involves using Information and Communication
Technology (ICT) to organize, automate, and coordinate business processes. CRM
with the Web technology provides healthcare the ability to broaden service
beyond its usual practices, and thus provides a particular advantageous
environment for them that want to use ICT to achieve complex healthcare goal.
This paper we will discuss and demonstrate how a new approach in CRM will help
the healthcare increasing their customer support, and promoting better health
to patient. The patients benefited from the customized personal service so that
they have full information access to perform self managed their own health and
the healthcare provider will have a loyal and retains the right customer. A
conceptual framework of approach will be highlighted. Customer centric paradigm
in social network's era and value creation of healthcare's business process
will be taken into consideration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3920</identifier>
 <datestamp>2012-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3920</id><created>2012-03-18</created><authors><author><keyname>Ahuja</keyname><forenames>A.</forenames></author><author><keyname>Venkateswarlu</keyname><forenames>K.</forenames></author><author><keyname>Krishna</keyname><forenames>P. Venkata</forenames></author></authors><title>Stochastic Characteristics and Simulation of the Random Waypoint
  Mobility Model</title><categories>cs.NI</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Simulation results for Mobile Ad-Hoc Networks (MANETs) are fundamentally
governed by the underlying Mobility Model. Thus it is imperative to find
whether events functionally dependent on the mobility model 'converge' to well
defined functions or constants. This shall ensure the long-run consistency
among simulation performed by disparate parties. This paper reviews a work on
the discrete Random Waypoint Mobility Model (RWMM), addressing its long run
stochastic stability. It is proved that each model in the targeted discrete
class of the RWMM satisfies Birkhoff's pointwise ergodic theorem [13], and
hence time averaged functions on the mobility model surely converge. We also
simulate the most common and general version of the RWMM to give insight into
its working.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3923</identifier>
 <datestamp>2012-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3923</id><created>2012-03-18</created><authors><author><keyname>Almunawar</keyname><forenames>Mohammad Nabil</forenames></author><author><keyname>Anshari</keyname><forenames>Muhammad</forenames></author></authors><title>Health Information Systems (HIS): Concept and Technology</title><categories>cs.OH</categories><comments>International Conference Informatics Development, 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A health information system (HIS) is the intersection of between healthcare's
business process, and information systems to deliver better healthcare
services. The nature of healthcare industry, which is highly influenced by
economic, social, politic, and technological factors, has changed over time.
This paper will address some important concepts of healthcare and related
terminologies to provide a holistic view for HIS. Related technological
milestones and major events are briefly summarized. The trends and rapid
development of health information technologies are also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3935</identifier>
 <datestamp>2012-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3935</id><created>2012-03-18</created><authors><author><keyname>Saad</keyname><forenames>Hussein</forenames></author><author><keyname>Mohamed</keyname><forenames>Amr</forenames></author><author><keyname>ElBatt</keyname><forenames>Tamer</forenames></author></authors><title>Distributed Cooperative Q-learning for Power Allocation in Cognitive
  Femtocell Networks</title><categories>cs.LG cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a distributed reinforcement learning (RL) technique
called distributed power control using Q-learning (DPC-Q) to manage the
interference caused by the femtocells on macro-users in the downlink. The DPC-Q
leverages Q-Learning to identify the sub-optimal pattern of power allocation,
which strives to maximize femtocell capacity, while guaranteeing macrocell
capacity level in an underlay cognitive setting. We propose two different
approaches for the DPC-Q algorithm: namely, independent, and cooperative. In
the former, femtocells learn independently from each other while in the latter,
femtocells share some information during learning in order to enhance their
performance. Simulation results show that the independent approach is capable
of mitigating the interference generated by the femtocells on macro-users.
Moreover, the results show that cooperation enhances the performance of the
femtocells in terms of speed of convergence, fairness and aggregate femtocell
capacity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3946</identifier>
 <datestamp>2012-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3946</id><created>2012-03-18</created><updated>2012-03-30</updated><authors><author><keyname>Camilli</keyname><forenames>Matteo</forenames></author></authors><title>Preserving Co-Location Privacy in Geo-Social Networks</title><categories>cs.SI cs.CY physics.soc-ph</categories><comments>10 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The number of people on social networks has grown exponentially. Users share
very large volumes of personal informations and content every days. This
content could be tagged with geo-spatial and temporal coordinates that may be
considered sensitive for some users. While there is clearly a demand for users
to share this information with each other, there is also substantial demand for
greater control over the conditions under which their information is shared.
Content published in a geo-aware social networks (GeoSN) often involves
multiple users and it is often accessible to multiple users, without the
publisher being aware of the privacy preferences of those users. This makes
difficult for GeoSN users to control which information about them is available
and to whom it is available. Thus, the lack of means to protect users privacy
scares people bothered about privacy issues. This paper addresses a particular
privacy threats that occur in GeoSNs: the Co-location privacy threat. It
concerns the availability of information about the presence of multiple users
in a same locations at given times, against their will. The challenge addressed
is that of supporting privacy while still enabling useful services.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3951</identifier>
 <datestamp>2012-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3951</id><created>2012-03-18</created><authors><author><keyname>Kumar</keyname><forenames>M. P. Sivaram</forenames></author><author><keyname>Rajasekaran</keyname><forenames>S.</forenames></author></authors><title>Path Planning Algorithm for Extinguishing Forest Fires</title><categories>cs.RO</categories><comments>6 pages</comments><journal-ref>Journal of Computing, Volume 4, Issue 2, February 2012, 108-113</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the major impacts of climatic changes is due to destroying of forest.
Destroying of forest takes place in many ways but the majority of the forest is
destroyed due to wild forest fires. In this paper we have presented a path
planning algorithm for extinguishing fires which uses Wireless Sensor and Actor
Networks (WSANs) for detecting fires. Since most of the works on forest fires
are based on Wireless Sensor Networks (WSNs) and a collection of work has been
done on coverage, message transmission, deployment of nodes, battery power
depletion of sensor nodes in WSNs we focused our work in path planning approach
of the Actor to move to the target area where the fire has occurred and
extinguish it. An incremental approach is presented in order to determine the
successive moves of the Actor to extinguish fire in an environment with and
without obstacles. This is done by comparing the moves determined with target
location readings obtained using sensors until the Actor reaches the target
area to extinguish fires.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3961</identifier>
 <datestamp>2013-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3961</id><created>2012-03-18</created><updated>2013-11-15</updated><authors><author><keyname>Lee</keyname><forenames>Troy</forenames></author><author><keyname>Theis</keyname><forenames>Dirk Oliver</forenames></author></authors><title>Support-based lower bounds for the positive semidefinite rank of a
  nonnegative matrix</title><categories>math.CO cs.DM math.OC</categories><comments>9 pages</comments><msc-class>15B48, 90C22</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The positive semidefinite rank of a nonnegative $(m\times n)$-matrix~$S$ is
the minimum number~$q$ such that there exist positive semidefinite $(q\times
q)$-matrices $A_1,\dots,A_m$, $B_1,\dots,B_n$ such that $S(k,\ell) =
\mbox{tr}(A_k^* B_\ell)$.
  The most important, lower bound technique for nonnegative rank is solely
based on the support of the matrix S, i.e., its zero/non-zero pattern. In this
paper, we characterize the power of lower bounds on positive semidefinite rank
based on solely on the support.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3962</identifier>
 <datestamp>2012-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3962</id><created>2012-03-18</created><authors><author><keyname>Asgeirsson</keyname><forenames>Eyjolfur I.</forenames></author><author><keyname>Halldorsson</keyname><forenames>Magnus M.</forenames></author><author><keyname>Mitra</keyname><forenames>Pradipta</forenames></author></authors><title>A Fully Distributed Algorithm for Throughput Performance in Wireless
  Networks</title><categories>cs.NI</categories><comments>9 pages, to appear in CISS 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study link scheduling in wireless networks under stochastic arrival
processes of packets, and give an algorithm that achieves stability in the
physical (SINR) interference model. The efficiency of such an algorithm is the
fraction of the maximum feasible traffic that the algorithm can handle without
queues growing indefinitely. Our algorithm achieves two important goals: (i)
efficiency is independent of the size of the network, and (ii) the algorithm is
fully distributed, i.e., individual nodes need no information about the overall
network topology, not even local information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3967</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3967</id><created>2012-03-18</created><updated>2012-08-20</updated><authors><author><keyname>Rothe</keyname><forenames>Joerg</forenames></author><author><keyname>Schend</keyname><forenames>Lena</forenames></author></authors><title>Control Complexity in Bucklin, Fallback, and Plurality Voting: An
  Experimental Approach</title><categories>cs.GT cs.CC cs.MA</categories><comments>Complete version with all results for plurality voting, 370 pages,
  numerous figures, a short version appeared in the proceedings of the 11th
  International Symposium on Experimental Algorithms (SEA-2012)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Walsh [Wal10, Wal09], Davies et al. [DKNW10, DKNW11], and Narodytska et al.
[NWX11] studied various voting systems empirically and showed that they can
often be manipulated effectively, despite their manipulation problems being
NP-hard. Such an experimental approach is sorely missing for NP-hard control
problems, where control refers to attempts to tamper with the outcome of
elections by adding/deleting/partitioning either voters or candidates. We
experimentally tackle NP-hard control problems for Bucklin and fallback voting.
Among natural voting systems with efficient winner determination, fallback
voting is currently known to display the broadest resistance to control in
terms of NP-hardness, and Bucklin voting has been shown to behave almost as
well in terms of control resistance [ER10, EPR11, EFPR11]. We also investigate
control resistance experimentally for plurality voting, one of the first voting
systems analyzed with respect to electoral control [BTT92, HHR07]. Our findings
indicate that NP-hard control problems can often be solved effectively in
practice. Moreover, our experiments allow a more fine-grained analysis and
comparison-across various control scenarios, vote distribution models, and
voting systems-than merely stating NP-hardness for all these control problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.3997</identifier>
 <datestamp>2012-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.3997</id><created>2012-03-18</created><authors><author><keyname>Menzel</keyname><forenames>Michael</forenames></author><author><keyname>Ranjan</keyname><forenames>Rajiv</forenames></author></authors><title>CloudGenius: Decision Support for Web Server Cloud Migration</title><categories>cs.DC cs.SE</categories><comments>10 pages, Proceedings of the 21st International Conference on World
  Wide Web</comments><acm-class>D.2.2; H.4.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud computing is the latest computing paradigm that delivers hardware and
software resources as virtualized services in which users are free from the
burden of worrying about the low-level system administration details. Migrating
Web applications to Cloud services and integrating Cloud services into existing
computing infrastructures is non-trivial. It leads to new challenges that often
require innovation of paradigms and practices at all levels: technical,
cultural, legal, regulatory, and social. The key problem in mapping Web
applications to virtualized Cloud services is selecting the best and compatible
mix of software images (e.g., Web server image) and infrastructure services to
ensure that Quality of Service (QoS) targets of an application are achieved.
The fact that, when selecting Cloud services, engineers must consider
heterogeneous sets of criteria and complex dependencies between infrastructure
services and software images, which are impossible to resolve manually, is a
critical issue. To overcome these challenges, we present a framework (called
CloudGenius) which automates the decision-making process based on a model and
factors specifically for Web server migration to the Cloud. CloudGenius
leverages a well known multi-criteria decision making technique, called
Analytic Hierarchy Process, to automate the selection process based on a model,
factors, and QoS parameters related to an application. An example application
demonstrates the applicability of the theoretical CloudGenius approach.
Moreover, we present an implementation of CloudGenius that has been validated
through experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4008</identifier>
 <datestamp>2012-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4008</id><created>2012-03-18</created><updated>2012-04-11</updated><authors><author><keyname>Yang</keyname><forenames>Lei</forenames></author><author><keyname>Sagduyu</keyname><forenames>Yalin Evren</forenames></author><author><keyname>Li</keyname><forenames>Jason Hongjun</forenames></author><author><keyname>Zhang</keyname><forenames>Junshan</forenames></author></authors><title>Adaptive Network Coding for Scheduling Real-time Traffic with Hard
  Deadlines</title><categories>cs.SY cs.NI</categories><comments>11 pages, 13 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study adaptive network coding (NC) for scheduling real-time traffic over a
single-hop wireless network. To meet the hard deadlines of real-time traffic,
it is critical to strike a balance between maximizing the throughput and
minimizing the risk that the entire block of coded packets may not be decodable
by the deadline. Thus motivated, we explore adaptive NC, where the block size
is adapted based on the remaining time to the deadline, by casting this
sequential block size adaptation problem as a finite-horizon Markov decision
process. One interesting finding is that the optimal block size and its
corresponding action space monotonically decrease as the deadline approaches,
and the optimal block size is bounded by the &quot;greedy&quot; block size. These unique
structures make it possible to narrow down the search space of dynamic
programming, building on which we develop a monotonicity-based backward
induction algorithm (MBIA) that can solve for the optimal block size in
polynomial time. Since channel erasure probabilities would be time-varying in a
mobile network, we further develop a joint real-time scheduling and channel
learning scheme with adaptive NC that can adapt to channel dynamics. We also
generalize the analysis to multiple flows with hard deadlines and long-term
delivery ratio constraints, devise a low-complexity online scheduling algorithm
integrated with the MBIA, and then establish its asymptotical
throughput-optimality. In addition to analysis and simulation results, we
perform high fidelity wireless emulation tests with real radio transmissions to
demonstrate the feasibility of the MBIA in finding the optimal block size in
real time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4009</identifier>
 <datestamp>2012-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4009</id><created>2012-03-18</created><authors><author><keyname>Fabbri</keyname><forenames>Ricardo</forenames></author><author><keyname>Bruno</keyname><forenames>Odemir Martinez</forenames></author><author><keyname>Costa</keyname><forenames>Luciano da Fontoura</forenames></author></authors><title>Scilab and SIP for Image Processing</title><categories>cs.MS cs.CV</categories><comments>16 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is an overview of Image Processing and Analysis using Scilab, a
free prototyping environment for numerical calculations similar to Matlab. We
demonstrate the capabilities of SIP -- the Scilab Image Processing Toolbox --
which extends Scilab with many functions to read and write images in over 100
major file formats, including PNG, JPEG, BMP, and TIFF. It also provides
routines for image filtering, edge detection, blurring, segmentation, shape
analysis, and image recognition. Basic directions to install Scilab and SIP are
given, and also a mini-tutorial on Scilab. Three practical examples of image
analysis are presented, in increasing degrees of complexity, showing how
advanced image analysis techniques seems uncomplicated in this environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4011</identifier>
 <datestamp>2012-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4011</id><created>2012-03-15</created><authors><author><keyname>Ramanujan</keyname><forenames>Raghuram</forenames></author><author><keyname>Sabharwal</keyname><forenames>Ashish</forenames></author><author><keyname>Selman</keyname><forenames>Bart</forenames></author></authors><title>Understanding Sampling Style Adversarial Search Methods</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Twenty-Sixth Conference on Uncertainty
  in Artificial Intelligence (UAI2010)</comments><proxy>auai</proxy><report-no>UAI-P-2010-PG-474-483</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  UCT has recently emerged as an exciting new adversarial reasoning technique
based on cleverly balancing exploration and exploitation in a Monte-Carlo
sampling setting. It has been particularly successful in the game of Go but the
reasons for its success are not well understood and attempts to replicate its
success in other domains such as Chess have failed. We provide an in-depth
analysis of the potential of UCT in domain-independent settings, in cases where
heuristic values are available, and the effect of enhancing random playouts to
more informed playouts between two weak minimax players. To provide further
insights, we develop synthetic game tree instances and discuss interesting
properties of UCT, both empirically and analytically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4031</identifier>
 <datestamp>2015-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4031</id><created>2012-03-18</created><updated>2015-06-16</updated><authors><author><keyname>Polizzi</keyname><forenames>Eric</forenames></author><author><keyname>Kestyn</keyname><forenames>James</forenames></author></authors><title>FEAST Eigenvalue Solver v3.0 User Guide</title><categories>cs.MS cs.CE physics.chem-ph physics.comp-ph</categories><comments>37 pages, 10 Figures, 12 Tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The FEAST eigensolver package is a free high-performance numerical library
for solving the Hermitian and non-Hermitian eigenvalue problems, and obtaining
all the eigenvalues and (right/left) eigenvectors within a given search
interval or arbitrary contour in the complex plane. Its originality lies with a
new transformative numerical approach to the traditional eigenvalue algorithm
design - the FEAST algorithm. The FEAST eigensolver combines simplicity and
efficiency and it offers many important capabilities for achieving high
performance, robustness, accuracy, and scalability on parallel architectures.
FEAST is both a comprehensive library package, and an easy to use software. It
includes flexible reverse communication interfaces and ready to use predefined
interfaces for dense, banded and sparse systems. The current version v3.0 of
the FEAST package can address both Hermitian and non-Hermitian eigenvalue
problems (real symmetric, real non-symmetric, complex Hermitian, complex
symmetric, or complex general systems) on both shared-memory and distributed
memory architectures (i.e contains both FEAST-SMP and FEAST-MPI packages). This
User's guide provides instructions for installation setup, a detailed
description of the FEAST interfaces and a large number of examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4035</identifier>
 <datestamp>2012-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4035</id><created>2012-03-19</created><authors><author><keyname>Shakhakarmi</keyname><forenames>Niraj</forenames></author></authors><title>Quantitative Multiscale Analysis using Different Wavelets in 1D Voice
  Signal and 2D Image</title><categories>cs.MM</categories><comments>10 pages, 37 figures, Full paper is available at
  http://sites.google.com/site/nirajskorg/Home/wavelets; IJCSI International
  Journal of Computer Science Issues, Vol. 9, Issue 1, No 2, January 2012</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Mutiscale analysis represents multiresolution scrutiny of a signal to improve
its signal quality. Multiresolution analysis of 1D voice signal and 2D image is
conducted using DCT, FFT and different wavelets such as Haar, Deubachies,
Morlet, Cauchy, Shannon, Biorthogonal, Symmlet and Coiflet deploying the
cascaded filter banks based decomposition and reconstruction. The outstanding
quantitative analysis of the specified wavelets is done to investigate the
signal quality, mean square error, entropy and peak-to-peak SNR at multiscale
stage-4 for both 1D voice signal and 2D image. In addition, the 2D image
compression performance is significantly found 93.00% in DB-4, 93.68% in
bior-4.4, 93.18% in Sym-4 and 92.20% in Coif-2 during the multiscale analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4040</identifier>
 <datestamp>2012-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4040</id><created>2012-03-19</created><authors><author><keyname>Shin</keyname><forenames>Beomkyu</forenames></author><author><keyname>Hong</keyname><forenames>Seokbeom</forenames></author><author><keyname>Park</keyname><forenames>Hosung</forenames></author><author><keyname>No</keyname><forenames>Jong-Seon</forenames></author><author><keyname>Shin</keyname><forenames>Dong-Joon</forenames></author></authors><title>New decoding scheme for LDPC codes based on simple product code
  structure</title><categories>cs.IT math.IT</categories><comments>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a new decoding scheme for low-density parity-check (LDPC)
codes using the concept of simple product code structure is proposed based on
combining two independently received soft-decision data for the same codeword.
LDPC codes act as horizontal codes of the product codes and simple algebraic
codes are used as vertical codes to help decoding of the LDPC codes. The
decoding capability of the proposed decoding scheme is defined and analyzed
using the paritycheck matrices of vertical codes and especially the
combined-decodability is derived for the case of single parity-check (SPC) and
Hamming codes being used as vertical codes. It is also shown that the proposed
decoding scheme achieves much better error-correcting capability in high signal
to noise ratio (SNR) region with low additional decoding complexity, compared
with a conventional decoding scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4041</identifier>
 <datestamp>2012-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4041</id><created>2012-03-19</created><authors><author><keyname>Chakrabarti</keyname><forenames>Amit</forenames></author><author><keyname>Fleischer</keyname><forenames>Lisa</forenames></author><author><keyname>Weibel</keyname><forenames>Christophe</forenames></author></authors><title>When the Cut Condition is Enough: A Complete Characterization for
  Multiflow Problems in Series-Parallel Networks</title><categories>cs.DM cs.DS</categories><comments>An extended abstract of this paper will be published at the 44th
  Symposium on Theory of Computing (STOC 2012)</comments><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $G=(V,E)$ be a supply graph and $H=(V,F)$ a demand graph defined on the
same set of vertices. An assignment of capacities to the edges of $G$ and
demands to the edges of $H$ is said to satisfy the \emph{cut condition} if for
any cut in the graph, the total demand crossing the cut is no more than the
total capacity crossing it. The pair $(G,H)$ is called \emph{cut-sufficient} if
for any assignment of capacities and demands that satisfy the cut condition,
there is a multiflow routing the demands defined on $H$ within the network with
capacities defined on $G$. We prove a previous conjecture, which states that
when the supply graph $G$ is series-parallel, the pair $(G,H)$ is
cut-sufficient if and only if $(G,H)$ does not contain an \emph{odd spindle} as
a minor; that is, if it is impossible to contract edges of $G$ and delete edges
of $G$ and $H$ so that $G$ becomes the complete bipartite graph $K_{2,p}$, with
$p\geq 3$ odd, and $H$ is composed of a cycle connecting the $p$ vertices of
degree 2, and an edge connecting the two vertices of degree $p$. We further
prove that if the instance is \emph{Eulerian} --- that is, the demands and
capacities are integers and the total of demands and capacities incident to
each vertex is even --- then the multiflow problem has an integral solution. We
provide a polynomial-time algorithm to find an integral solution in this case.
In order to prove these results, we formulate properties of tight cuts (cuts
for which the cut condition inequality is tight) in cut-sufficient pairs. We
believe these properties might be useful in extending our results to planar
graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4042</identifier>
 <datestamp>2012-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4042</id><created>2012-03-19</created><authors><author><keyname>Arye</keyname><forenames>Matvey</forenames></author><author><keyname>Nordstrom</keyname><forenames>Erik</forenames></author><author><keyname>Kiefer</keyname><forenames>Robert</forenames></author><author><keyname>Rexford</keyname><forenames>Jennifer</forenames></author><author><keyname>Freedman</keyname><forenames>Michael J.</forenames></author></authors><title>A Provably-Correct Protocol for Seamless Communication with Mobile,
  Multi-Homed Hosts</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern consumer devices, like smartphones and tablets, have multiple
interfaces (e.g., WiFi and 3G) that attach to new access points as users move.
These mobile, multi-homed computers are a poor match with an Internet
architecture that binds connections to fixed end-points with topology-
dependent addresses. As a result, hosts typically cannot spread a connection
over multiple interfaces or paths, or change locations without breaking
existing connections.
  In this paper, we introduce ECCP, an end-host connection control protocol
that allows hosts to communicate over mul- tiple interfaces with
dynamically-changing IP addresses. Each ECCP connection consists of one or more
flows, each associated with an interface or path. A host can move an existing
flow from one interface to another or change the IP address using in-band
signaling, without any support from the underlying network. We use formal
models to verify that ECCP works correctly in the presence of packet loss,
out-of-order delivery, and frequent mobility, and to identify bugs and design
limitations in earlier mobility protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4043</identifier>
 <datestamp>2012-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4043</id><created>2012-03-19</created><authors><author><keyname>Mahmood</keyname><forenames>Shah</forenames></author><author><keyname>Desmedt</keyname><forenames>Yvo</forenames></author></authors><title>Your Facebook Deactivated Friend or a Cloaked Spy (Extended Abstract)</title><categories>cs.SI cs.CY physics.soc-ph</categories><comments>Full paper is presented at IEEE International Workshop on Security
  and Social Networking SESOC 2012, Lugano, Switzerland</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With over 750 million active users, Facebook is the most famous social
networking website. One particular aspect of Facebook widely discussed in the
news and heavily researched in academic circles is the privacy of its users. In
this paper we introduce a zero day privacy loophole in Facebook. We call this
the deactivated friend attack. The concept of the attack is very similar to
cloaking in Star Trek while its seriousness could be estimated from the fact
that once the attacker is a friend of the victim, it is highly probable the
attacker has indefinite access to the victims private information in a cloaked
way. We demonstrate the impact of the attack by showing the ease of gaining
trust of Facebook users and being befriended online. With targeted friend
requests we were able to add over 4300 users and maintain access to their
Facebook profile information for at least 261 days. No user was able to
unfriend us during this time due to cloaking and short de-cloaking sessions.
The short de-cloaking sessions were enough to get updates about the victims. We
also provide several solutions for the loophole, which range from mitigation to
a permanent solution
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4049</identifier>
 <datestamp>2013-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4049</id><created>2012-03-19</created><updated>2013-08-12</updated><authors><author><keyname>Bonnabel</keyname><forenames>Silvere</forenames></author><author><keyname>Sepulchre</keyname><forenames>Rodolphe</forenames></author></authors><title>The geometry of low-rank Kalman filters</title><categories>math.OC cs.SY</categories><comments>Final version published in Matrix Information Geometry, pp53-68,
  Springer Verlag, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An important property of the Kalman filter is that the underlying Riccati
flow is a contraction for the natural metric of the cone of symmetric positive
definite matrices. The present paper studies the geometry of a low-rank version
of the Kalman filter. The underlying Riccati flow evolves on the manifold of
fixed rank symmetric positive semidefinite matrices. Contraction properties of
the low-rank flow are studied by means of a suitable metric recently introduced
by the authors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4054</identifier>
 <datestamp>2012-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4054</id><created>2012-03-19</created><updated>2012-07-26</updated><authors><author><keyname>Rizvandi</keyname><forenames>Nikzad Babaii</forenames></author><author><keyname>Taheri</keyname><forenames>Javid</forenames></author><author><keyname>Moraveji</keyname><forenames>Reza</forenames></author><author><keyname>Zomaya</keyname><forenames>Albert Y.</forenames></author></authors><title>On Modelling and Prediction of Total CPU Usage for Applications in
  MapReduce Environments</title><categories>cs.DC cs.PF</categories><comments>This paper has been accepted to 12th International Conference on
  Algorithms and Architectures for Parallel Processing (ICA3PP 2012)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, businesses have started using MapReduce as a popular computation
framework for processing large amount of data, such as spam detection, and
different data mining tasks, in both public and private clouds. Two of the
challenging questions in such environments are (1) choosing suitable values for
MapReduce configuration parameters -e.g., number of mappers, number of
reducers, and DFS block size-, and (2) predicting the amount of resources that
a user should lease from the service provider. Currently, the tasks of both
choosing configuration parameters and estimating required resources are solely
the users' responsibilities. In this paper, we present an approach to provision
the total CPU usage in clock cycles of jobs in MapReduce environment. For a
MapReduce job, a profile of total CPU usage in clock cycles is built from the
job past executions with different values of two configuration parameters e.g.,
number of mappers, and number of reducers. Then, a polynomial regression is
used to model the relation between these configuration parameters and total CPU
usage in clock cycles of the job. We also briefly study the influence of input
data scaling on measured total CPU usage in clock cycles. This derived model
along with the scaling result can then be used to provision the total CPU usage
in clock cycles of the same jobs with different input data size. We validate
the accuracy of our models using three realistic applications (WordCount, Exim
MainLog parsing, and TeraSort). Results show that the predicted total CPU usage
in clock cycles of generated resource provisioning options are less than 8% of
the measured total CPU usage in clock cycles in our 20-node virtual Hadoop
cluster.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4063</identifier>
 <datestamp>2012-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4063</id><created>2012-03-19</created><authors><author><keyname>Kaski</keyname><forenames>Petteri</forenames></author><author><keyname>Koivisto</keyname><forenames>Mikko</forenames></author><author><keyname>Nederlof</keyname><forenames>Jesper</forenames></author></authors><title>Homomorphic Hashing for Sparse Coefficient Extraction</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study classes of Dynamic Programming (DP) algorithms which, due to their
algebraic definitions, are closely related to coefficient extraction methods.
DP algorithms can easily be modified to exploit sparseness in the DP table
through memorization. Coefficient extraction techniques on the other hand are
both space-efficient and parallelisable, but no tools have been available to
exploit sparseness.
  We investigate the systematic use of homomorphic hash functions to combine
the best of these methods and obtain improved space-efficient algorithms for
problems including LINEAR SAT, SET PARTITION, and SUBSET SUM. Our algorithms
run in time proportional to the number of nonzero entries of the last segment
of the DP table, which presents a strict improvement over sparse DP. The last
property also gives an improved algorithm for CNF SAT with sparse projections.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4069</identifier>
 <datestamp>2012-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4069</id><created>2012-03-19</created><authors><author><keyname>Miller</keyname><forenames>Mirka</forenames></author><author><keyname>Perez-Roses</keyname><forenames>Hebert</forenames></author><author><keyname>Ryan</keyname><forenames>Joe</forenames></author></authors><title>The Maximum Degree-and-Diameter-Bounded Subgraph in the Mesh</title><categories>math.CO cs.DM</categories><comments>accepted, 18 pages, 7 figures; Discrete Applied Mathematics, 2012</comments><msc-class>05C35 (Primary) 05C12, 05C90, 68R10, 95C15 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of finding the largest connected subgraph of a given undirected
host graph, subject to constraints on the maximum degree $\Delta$ and the
diameter $D$, was introduced in \cite{maxddbs}, as a generalization of the
Degree-Diameter Problem. A case of special interest is when the host graph is a
common parallel architecture. Here we discuss the case when the host graph is a
$k$-dimensional mesh. We provide some general bounds for the order of the
largest subgraph in arbitrary dimension $k$, and for the particular cases of
$k=3, \Delta = 4$ and $k=2, \Delta = 3$, we give constructions that result in
sharper lower bounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4070</identifier>
 <datestamp>2012-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4070</id><created>2012-03-19</created><authors><author><keyname>Annergren</keyname><forenames>Mariette</forenames></author><author><keyname>Hansson</keyname><forenames>Anders</forenames></author><author><keyname>Wahlberg</keyname><forenames>Bo</forenames></author></authors><title>An ADMM Algorithm for Solving l_1 Regularized MPC</title><categories>cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an Alternating Direction Method of Multipliers (ADMM) algorithm
for solving optimization problems with an l_1 regularized least-squares cost
function subject to recursive equality constraints. The considered optimization
problem has applications in control, for example in l_1 regularized MPC. The
ADMM algorithm is easy to implement, converges fast to a solution of moderate
accuracy, and enables separation of the optimization problem into sub-problems
that may be solved in parallel. We show that the most costly step of the
proposed ADMM algorithm is equivalent to solving an LQ regulator problem with
an extra linear term in the cost function, a problem that can be solved
efficiently using a Riccati recursion. We apply the ADMM algorithm to an
example of l_1 regularized MPC. The numerical examples confirm fast convergence
to moderate accuracy and a linear complexity in the MPC prediction horizon.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4077</identifier>
 <datestamp>2015-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4077</id><created>2012-03-19</created><updated>2015-08-23</updated><authors><author><keyname>Poulakis</keyname><forenames>Dimitrios</forenames></author><author><keyname>Rolland</keyname><forenames>Robert</forenames></author></authors><title>A Digital Signature Scheme for Long-Term Security</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a signature scheme based on two intractable
problems, namely the integer factorization problem and the discrete logarithm
problem for elliptic curves. It is suitable for applications requiring
long-term security and provides a more efficient solution than the existing
ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4084</identifier>
 <datestamp>2012-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4084</id><created>2012-03-19</created><authors><author><keyname>McKinley</keyname><forenames>Richard</forenames></author></authors><title>Canonical Proof nets for Classical Logic</title><categories>math.LO cs.LO</categories><comments>Accepted for publication in APAL (Special issue, Classical Logic and
  Computation)</comments><msc-class>03F03</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Proof nets provide abstract counterparts to sequent proofs modulo rule
permutations; the idea being that if two proofs have the same underlying
proof-net, they are in essence the same proof. Providing a convincing proof-net
counterpart to proofs in the classical sequent calculus is thus an important
step in understanding classical sequent calculus proofs. By convincing, we mean
that (a) there should be a canonical function from sequent proofs to proof
nets, (b) it should be possible to check the correctness of a net in polynomial
time, (c) every correct net should be obtainable from a sequent calculus proof,
and (d) there should be a cut-elimination procedure which preserves
correctness. Previous attempts to give proof-net-like objects for propositional
classical logic have failed at least one of the above conditions. In [23], the
author presented a calculus of proof nets (expansion nets) satisfying (a) and
(b); the paper defined a sequent calculus corresponding to expansion nets but
gave no explicit demonstration of (c). That sequent calculus, called LK\ast in
this paper, is a novel one-sided sequent calculus with both additively and
multiplicatively formulated disjunction rules. In this paper (a self-contained
extended version of [23]), we give a full proof of (c) for expansion nets with
respect to LK\ast, and in addition give a cut-elimination procedure internal to
expansion nets - this makes expansion nets the first notion of proof-net for
classical logic satisfying all four criteria.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4111</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4111</id><created>2012-03-19</created><authors><author><keyname>Doerr</keyname><forenames>Benjamin</forenames></author><author><keyname>Winzen</keyname><forenames>Carola</forenames></author></authors><title>Reducing the Arity in Unbiased Black-Box Complexity</title><categories>cs.NE</categories><comments>An extended abstract of this paper has been accepted for inclusion in
  the proceedings of the Genetic and Evolutionary Computation Conference (GECCO
  2012)</comments><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that for all $1&lt;k \leq \log n$ the $k$-ary unbiased black-box
complexity of the $n$-dimensional $\onemax$ function class is $O(n/k)$. This
indicates that the power of higher arity operators is much stronger than what
the previous $O(n/\log k)$ bound by Doerr et al. (Faster black-box algorithms
through higher arity operators, Proc. of FOGA 2011, pp. 163--172, ACM, 2011)
suggests.
  The key to this result is an encoding strategy, which might be of independent
interest. We show that, using $k$-ary unbiased variation operators only, we may
simulate an unrestricted memory of size $O(2^k)$ bits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4117</identifier>
 <datestamp>2012-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4117</id><created>2012-03-19</created><authors><author><keyname>Dietzfelbinger</keyname><forenames>Martin</forenames></author><author><keyname>Peilke</keyname><forenames>Hendrik</forenames></author><author><keyname>Rink</keyname><forenames>Michael</forenames></author></authors><title>A More Reliable Greedy Heuristic for Maximum Matchings in Sparse Random
  Graphs</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new greedy algorithm for the maximum cardinality matching
problem. We give experimental evidence that this algorithm is likely to find a
maximum matching in random graphs with constant expected degree c&gt;0,
independent of the value of c. This is contrary to the behavior of commonly
used greedy matching heuristics which are known to have some range of c where
they probably fail to compute a maximum matching.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4134</identifier>
 <datestamp>2012-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4134</id><created>2012-03-19</created><authors><author><keyname>Pellicer-Lostao</keyname><forenames>Carmen</forenames></author><author><keyname>Lopez-Ruiz</keyname><forenames>Ricardo</forenames></author></authors><title>Notions of Chaotic Cryptography: Sketch of a Chaos based Cryptosystem</title><categories>nlin.CD cs.CR</categories><comments>28 pages, 9 figures, 8 tables</comments><journal-ref>Applied Cryptography and Network Security (InTech Books, 1st
  Edition in Mars 2012), Ch. 12, pp. 267-294</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Chaotic cryptography describes the use of chaos theory (in particular
physical dynamical systems working in chaotic regime as part of communication
techniques and computation algorithms) to perform different cryptographic tasks
in a cryptographic system. In the end, the question is, can chaotic systems
provide alternative techniques able to enhance cryptographic algorithms?. This
chapter can be a worthy material to guide the reader in order to answer himself
this question. Thus, the objective of this chapter is to give a general vision
of what chaotic cryptography is and a comprehensive example that illustrates
the main techniques used in this field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4135</identifier>
 <datestamp>2012-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4135</id><created>2012-03-19</created><authors><author><keyname>Seidel</keyname><forenames>Eric L.</forenames></author></authors><title>Metadata Management in Scientific Computing</title><categories>cs.DL</categories><comments>8 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Complex scientific codes and the datasets they generate are in need of a
sophisticated categorization environment that allows the community to store,
search, and enhance metadata in an open, dynamic system. Currently, data is
often presented in a read-only format, distilled and curated by a select group
of researchers. We envision a more open and dynamic system, where authors can
publish their data in a writeable format, allowing users to annotate the
datasets with their own comments and data. This would enable the scientific
community to collaborate on a higher level than before, where researchers could
for example annotate a published dataset with their citations.
  Such a system would require a complete set of permissions to ensure that any
individual's data cannot be altered by others unless they specifically allow
it. For this reason datasets and codes are generally presented read-only, to
protect the author's data; however, this also prevents the type of social
revolutions that the private sector has seen with Facebook and Twitter.
  In this paper, we present an alternative method of publishing codes and
datasets, based on Fluidinfo, which is an openly writeable and social metadata
engine. We will use the specific example of the Einstein Toolkit, a shared
scientific code built using the Cactus Framework, to illustrate how the code's
metadata may be published in writeable form via Fluidinfo.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4150</identifier>
 <datestamp>2012-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4150</id><created>2012-03-19</created><authors><author><keyname>Soliman</keyname><forenames>Ahmed H. M.</forenames></author><author><keyname>Saad</keyname><forenames>E. M.</forenames></author><author><keyname>El-Bably</keyname><forenames>M.</forenames></author><author><keyname>Keshk</keyname><forenames>Hesham M. A. M.</forenames></author></authors><title>Designing a WISHBONE Protocol Network Adapter for an Asynchronous
  Network-on-Chip</title><categories>cs.AR</categories><comments>7 pages, 6 figures; ISSN (Online): 1694-0814</comments><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 8,
  Issue 4, No 2, July 2011, 262-268</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Scaling of microchip technologies, from micron to submicron and now to
deep sub-micron (DSM) range, has enabled large scale systems-on-chip (SoC). In
future deep submicron (DSM) designs, the interconnect effect will definitely
dominate performance. Network-on-Chip (NoC) has become a promising solution to
bus-based communication infrastructure limitations. NoC designs usually targets
Application Specific Integrated Circuits (ASICs), however, the fabrication
process costs a lot. Implementing a NoC on an FPGA does not only reduce the
cost but also decreases programming and verification cycles. In this paper, an
Asynchronous NoC has been implemented on a SPARTAN-3E\textregistered device.
The NoC supports basic transactions of both widely used on-chip interconnection
standards, the Open Core Protocol (OCP) and the WISHBONE Protocol. Although,
FPGA devices are synchronous in nature, it has been shown that they can be used
to prototype a Global Asynchronous Local Synchronous (GALS) systems, comprising
an Asynchronous NoC connecting IP cores operating in different clock domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4153</identifier>
 <datestamp>2012-07-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4153</id><created>2012-03-19</created><updated>2012-07-17</updated><authors><author><keyname>Tunc</keyname><forenames>Sait</forenames></author><author><keyname>Donmez</keyname><forenames>Mehmet A.</forenames></author><author><keyname>Kozat</keyname><forenames>Suleyman S.</forenames></author></authors><title>Optimal Investment Under Transaction Costs</title><categories>q-fin.PM cs.SY</categories><comments>Submitted to IEEE Transactions on Signal Processing, 12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate how and when to diversify capital over assets, i.e., the
portfolio selection problem, from a signal processing perspective. To this end,
we first construct portfolios that achieve the optimal expected growth in
i.i.d. discrete-time two-asset markets under proportional transaction costs. We
then extend our analysis to cover markets having more than two stocks. The
market is modeled by a sequence of price relative vectors with arbitrary
discrete distributions, which can also be used to approximate a wide class of
continuous distributions. To achieve the optimal growth, we use threshold
portfolios, where we introduce a recursive update to calculate the expected
wealth. We then demonstrate that under the threshold rebalancing framework, the
achievable set of portfolios elegantly form an irreducible Markov chain under
mild technical conditions. We evaluate the corresponding stationary
distribution of this Markov chain, which provides a natural and efficient
method to calculate the cumulative expected wealth. Subsequently, the
corresponding parameters are optimized yielding the growth optimal portfolio
under proportional transaction costs in i.i.d. discrete-time two-asset markets.
As a widely known financial problem, we next solve optimal portfolio selection
in discrete-time markets constructed by sampling continuous-time Brownian
markets. For the case that the underlying discrete distributions of the price
relative vectors are unknown, we provide a maximum likelihood estimator that is
also incorporated in the optimization framework in our simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4154</identifier>
 <datestamp>2012-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4154</id><created>2012-03-19</created><authors><author><keyname>Karagiannis</keyname><forenames>Marios</forenames></author><author><keyname>Dallinge</keyname><forenames>Laetitia</forenames></author><author><keyname>Rolim</keyname><forenames>Jose</forenames></author></authors><title>Irida: A real-time Wireless Sensor Network visualization feedback
  protocol</title><categories>cs.NI</categories><comments>7 pages</comments><acm-class>I.3.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we describe the implementation of a real time visualization
and feedback system for Wireless Sensor Network algorithms. The system is based
on a fixed hardware testbed, which is deployed on a vertical flat surface and a
feedback loop system that takes information about the current state of the
network and projects this state, in a visual way, on the surface itself using a
video projector. The protocol used is open and simple to use, and can be easily
adapted for different hardware configurations. We call our system Irida.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4155</identifier>
 <datestamp>2014-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4155</id><created>2012-03-19</created><updated>2014-07-02</updated><authors><author><keyname>Laplante</keyname><forenames>S.</forenames></author><author><keyname>Lerays</keyname><forenames>V.</forenames></author><author><keyname>Roland</keyname><forenames>J.</forenames></author></authors><title>Classical and quantum partition bound and detector inefficiency</title><categories>quant-ph cs.CC</categories><comments>21 pages, extended version</comments><journal-ref>Lecture Notes in Computer Science, Volume 7391, pages 617-628,
  2012</journal-ref><doi>10.1007/978-3-642-31594-7_52</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study randomized and quantum efficiency lower bounds in communication
complexity. These arise from the study of zero-communication protocols in which
players are allowed to abort. Our scenario is inspired by the physics setup of
Bell experiments, where two players share a predefined entangled state but are
not allowed to communicate. Each is given a measurement as input, which they
perform on their share of the system. The outcomes of the measurements should
follow a distribution predicted by quantum mechanics; however, in practice, the
detectors may fail to produce an output in some of the runs. The efficiency of
the experiment is the probability that the experiment succeeds (neither of the
detectors fails).
  When the players share a quantum state, this gives rise to a new bound on
quantum communication complexity (eff*) that subsumes the factorization norm.
When players share randomness instead of a quantum state, the efficiency bound
(eff), coincides with the partition bound of Jain and Klauck. This is one of
the strongest lower bounds known for randomized communication complexity, which
subsumes all the known combinatorial and algebraic methods including the
rectangle (corruption) bound, the factorization norm, and discrepancy.
  The lower bound is formulated as a convex optimization problem. In practice,
the dual form is more feasible to use, and we show that it amounts to
constructing an explicit Bell inequality (for eff) or Tsirelson inequality (for
eff*). We give an example of a quantum distribution where the violation can be
exponentially bigger than the previously studied class of normalized Bell
inequalities.
  For one-way communication, we show that the quantum one-way partition bound
is tight for classical communication with shared entanglement up to arbitrarily
small error.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4156</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4156</id><created>2012-03-19</created><authors><author><keyname>Tunc</keyname><forenames>Sait</forenames></author><author><keyname>Kozat</keyname><forenames>Suleyman S.</forenames></author></authors><title>Optimal Investment Under Transaction Costs: A Threshold Rebalanced
  Portfolio Approach</title><categories>q-fin.PM cs.SY</categories><comments>Submitted to IEEE Transactions on Signal Processing</comments><doi>10.1109/TSP.2013.2258339</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study optimal investment in a financial market having a finite number of
assets from a signal processing perspective. We investigate how an investor
should distribute capital over these assets and when he should reallocate the
distribution of the funds over these assets to maximize the cumulative wealth
over any investment period. In particular, we introduce a portfolio selection
algorithm that maximizes the expected cumulative wealth in i.i.d. two-asset
discrete-time markets where the market levies proportional transaction costs in
buying and selling stocks. We achieve this using &quot;threshold rebalanced
portfolios&quot;, where trading occurs only if the portfolio breaches certain
thresholds. Under the assumption that the relative price sequences have
log-normal distribution from the Black-Scholes model, we evaluate the expected
wealth under proportional transaction costs and find the threshold rebalanced
portfolio that achieves the maximal expected cumulative wealth over any
investment period. Our derivations can be readily extended to markets having
more than two stocks, where these extensions are pointed out in the paper. As
predicted from our derivations, we significantly improve the achieved wealth
over portfolio selection algorithms from the literature on historical data
sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4157</identifier>
 <datestamp>2012-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4157</id><created>2012-03-19</created><authors><author><keyname>Goswami</keyname><forenames>Saptarsi</forenames></author><author><keyname>Chakrabarti</keyname><forenames>Amlan</forenames></author></authors><title>Quartile Clustering: A quartile based technique for Generating
  Meaningful Clusters</title><categories>cs.DB</categories><comments>ISSN 2151-9617</comments><journal-ref>Journal of Computing, Volume 4, Issue 2, February 2012, 48-55</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Clustering is one of the main tasks in exploratory data analysis and
descriptive statistics where the main objective is partitioning observations in
groups. Clustering has a broad range of application in varied domains like
climate, business, information retrieval, biology, psychology, to name a few. A
variety of methods and algorithms have been developed for clustering tasks in
the last few decades. We observe that most of these algorithms define a cluster
in terms of value of the attributes, density, distance etc. However these
definitions fail to attach a clear meaning/semantics to the generated clusters.
We argue that clusters having understandable and distinct semantics defined in
terms of quartiles/halves are more appealing to business analysts than the
clusters defined by data boundaries or prototypes. On the samepremise, we
propose our new algorithm named as quartile clustering technique. Through a
series of experiments we establish efficacy of this algorithm. We demonstrate
that the quartile clustering technique adds clear meaning to each of the
clusters compared to K-means. We use DB Index to measure goodness of the
clusters and show our method is comparable to EM (Expectation Maximization),
PAM (Partition around Medoid) and K Means. We have explored its capability in
detecting outlier and the benefit of added semantics. We discuss some of the
limitations in its present form and also provide a rough direction in
addressing the issue of merging the generated clusters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4160</identifier>
 <datestamp>2012-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4160</id><created>2012-03-19</created><authors><author><keyname>Kalantarova</keyname><forenames>Nargiz</forenames></author><author><keyname>Donmez</keyname><forenames>Mehmet A.</forenames></author><author><keyname>Kozat</keyname><forenames>Suleyman S.</forenames></author></authors><title>A Novel Robust Approach to Least Squares Problems with Bounded Data
  Uncertainties</title><categories>cs.SY</categories><comments>Submitted to the IEEE Transactions on Signal Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this correspondence, we introduce a minimax regret criteria to the least
squares problems with bounded data uncertainties and solve it using
semi-definite programming. We investigate a robust minimax least squares
approach that minimizes a worst case difference regret. The regret is defined
as the difference between a squared data error and the smallest attainable
squared data error of a least squares estimator. We then propose a robust
regularized least squares approach to the regularized least squares problem
under data uncertainties by using a similar framework. We show that both
unstructured and structured robust least squares problems and robust
regularized least squares problem can be put in certain semi-definite
programming forms. Through several simulations, we demonstrate the merits of
the proposed algorithms with respect to the the well-known alternatives in the
literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4163</identifier>
 <datestamp>2012-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4163</id><created>2012-03-19</created><authors><author><keyname>Goswami</keyname><forenames>Saptarsi</forenames></author><author><keyname>Ghosh</keyname><forenames>Samiran</forenames></author><author><keyname>Chakrabarti</keyname><forenames>Amlan</forenames></author></authors><title>Outlier Detection Techniques for SQL and ETL Tuning</title><categories>cs.DB</categories><journal-ref>International Journal of Computer Applications (0975 - 8887)
  Volume 23 - No.8, June 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  RDBMS is the heart for both OLTP and OLAP types of applications. For both
types of applications thousands of queries expressed in terms of SQL are
executed on daily basis. All the commercial DBMS engines capture various
attributes in system tables about these executed queries. These queries need to
conform to best practices and need to be tuned to ensure optimal performance.
While we use checklists, often tools to enforce the same, a black box technique
on the queries for profiling, outlier detection is not employed for a summary
level understanding. This is the motivation of the paper, as this not only
points out to inefficiencies built in the system, but also has the potential to
point evolving best practices and inappropriate usage. Certainly this can
reduce latency in information flow and optimal utilization of hardware and
software capacity. In this paper we start with formulating the problem. We
explore four outlier detection techniques. We apply these techniques over rich
corpora of production queries and analyze the results. We also explore benefit
of an ensemble approach. We conclude with future courses of action. The same
philosophy we have used for optimization of extraction, transform, load (ETL)
jobs in one of our previous work. We give a brief introduction of the same in
section four.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4168</identifier>
 <datestamp>2012-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4168</id><created>2012-03-19</created><authors><author><keyname>Kalantarova</keyname><forenames>Nargiz</forenames></author><author><keyname>Kim</keyname><forenames>Kyeongyeon</forenames></author><author><keyname>Kozat</keyname><forenames>Suleyman S.</forenames></author><author><keyname>Singer</keyname><forenames>Andrew C.</forenames></author></authors><title>Linear MMSE-Optimal Turbo Equalization Using Context Trees</title><categories>cs.SY</categories><comments>Submitted to the IEEE Transactions on Signal Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Formulations of the turbo equalization approach to iterative equalization and
decoding vary greatly when channel knowledge is either partially or completely
unknown. Maximum aposteriori probability (MAP) and minimum mean square error
(MMSE) approaches leverage channel knowledge to make explicit use of soft
information (priors over the transmitted data bits) in a manner that is
distinctly nonlinear, appearing either in a trellis formulation (MAP) or inside
an inverted matrix (MMSE). To date, nearly all adaptive turbo equalization
methods either estimate the channel or use a direct adaptation equalizer in
which estimates of the transmitted data are formed from an expressly linear
function of the received data and soft information, with this latter
formulation being most common. We study a class of direct adaptation turbo
equalizers that are both adaptive and nonlinear functions of the soft
information from the decoder. We introduce piecewise linear models based on
context trees that can adaptively approximate the nonlinear dependence of the
equalizer on the soft information such that it can choose both the partition
regions as well as the locally linear equalizer coefficients in each region
independently, with computational complexity that remains of the order of a
traditional direct adaptive linear equalizer. This approach is guaranteed to
asymptotically achieve the performance of the best piecewise linear equalizer
and we quantify the MSE performance of the resulting algorithm and the
convergence of its MSE to that of the linear minimum MSE estimator as the depth
of the context tree and the data length increase.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4176</identifier>
 <datestamp>2012-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4176</id><created>2012-03-10</created><authors><author><keyname>Riad</keyname><forenames>A. M.</forenames></author><author><keyname>Elmonier</keyname><forenames>Hamdy K.</forenames></author><author><keyname>Shohieb</keyname><forenames>Samaa. M.</forenames></author><author><keyname>Asem</keyname><forenames>A. S.</forenames></author></authors><title>SignsWorld; Deeping Into the Silence World and Hearing Its Signs (State
  of the Art)</title><categories>cs.CL cs.CV</categories><comments>20 pages, A state of art paper so it contains many references</comments><journal-ref>International Journal of Computer Science &amp; Information Technology
  (IJCSIT) Vol 4, No 1, Feb 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatic speech processing systems are employed more and more often in real
environments. Although the underlying speech technology is mostly language
independent, differences between languages with respect to their structure and
grammar have substantial effect on the recognition systems performance. In this
paper, we present a review of the latest developments in the sign language
recognition research in general and in the Arabic sign language (ArSL) in
specific. This paper also presents a general framework for improving the deaf
community communication with the hearing people that is called SignsWorld. The
overall goal of the SignsWorld project is to develop a vision-based technology
for recognizing and translating continuous Arabic sign language ArSL.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4184</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4184</id><created>2012-03-19</created><updated>2012-11-07</updated><authors><author><keyname>Kitaura</keyname><forenames>Francisco-Shu</forenames></author></authors><title>The Initial Conditions of the Universe from Constrained Simulations</title><categories>astro-ph.CO cs.AI</categories><comments>6 pages, 4 figures; accepted at MNRAS after minor corrections</comments><doi>10.1093/mnrasl/sls029</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  I present a new approach to recover the primordial density fluctuations and
the cosmic web structure underlying a galaxy distribution. The method is based
on sampling Gaussian fields which are compatible with a galaxy distribution and
a structure formation model. This is achieved by splitting the inversion
problem into two Gibbs-sampling steps: the first being a Gaussianisation step
transforming a distribution of point sources at Lagrangian positions -which are
not a priori given- into a linear alias-free Gaussian field. This step is based
on Hamiltonian sampling with a Gaussian-Poisson model. The second step consists
on a likelihood comparison in which the set of matter tracers at the initial
conditions is constrained on the galaxy distribution and the assumed structure
formation model. For computational reasons second order Lagrangian Perturbation
Theory is used. However, the presented approach is flexible to adopt any
structure formation model. A semi-analytic halo-model based galaxy mock catalog
is taken to demonstrate that the recovered initial conditions are closely
unbiased with respect to the actual ones from the corresponding N-body
simulation down to scales of a ~ 5 Mpc/h. The cross-correlation between them
shows a substantial gain of information, being at k ~ 0.3 h/Mpc more than
doubled. In addition the initial conditions are extremely well Gaussian
distributed and the power-spectra follow the shape of the linear power-spectrum
being very close to the actual one from the simulation down to scales of k ~ 1
h/Mpc.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4194</identifier>
 <datestamp>2012-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4194</id><created>2012-03-19</created><authors><author><keyname>Tijssen</keyname><forenames>Robert J. W.</forenames></author><author><keyname>Waltman</keyname><forenames>Ludo</forenames></author><author><keyname>van Eck</keyname><forenames>Nees Jan</forenames></author></authors><title>Research collaboration and the expanding science grid: Measuring
  globalization processes worldwide</title><categories>cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper applies a new model and analytical tool to measure and study
contemporary globalization processes in collaborative science - a world in
which scientists, scholars, technicians and engineers interact within a 'grid'
of interconnected research sites and collaboration networks. The building
blocks of our metrics are the cities where scientific research is conducted, as
mentioned in author addresses on research publications. The unit of analysis is
the geographical distance between those cities. In our macro-level trend
analysis, covering the years 2000-2010, we observe that research collaboration
distances have been increasing, while the share of collaborative contacts with
foreign cities has leveled off. Collaboration distances and growth rates differ
significantly between countries and between fields of science. The application
of a distance metrics to compare and track these processes opens avenues for
further studies, both at the meso-level and at the micro-level, into how
research collaboration patterns and trends are driving and shaping the
connectivity fabric of world science.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4200</identifier>
 <datestamp>2012-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4200</id><created>2012-03-19</created><authors><author><keyname>Chen</keyname><forenames>Shaoshi</forenames></author><author><keyname>Singer</keyname><forenames>Michael F.</forenames></author></authors><title>Residues and Telescopers for Rational Functions</title><categories>math.CO cs.DM cs.SC</categories><comments>30 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give necessary and sufficient conditions for the existence of telescopers
for rational functions of two variables in the continuous, discrete and
q-discrete settings and characterize which operators can occur as telescopers.
Using this latter characterization, we reprove results of Furstenberg and
Zeilberger concerning diagonals of power series representing rational
functions. The key concept behind these considerations is a generalization of
the notion of residue in the continuous case to an analogous concept in the
discrete and q-discrete cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4204</identifier>
 <datestamp>2012-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4204</id><created>2012-03-19</created><authors><author><keyname>Daneshgar</keyname><forenames>Amir</forenames></author><author><keyname>Javadi</keyname><forenames>Ramin</forenames></author><author><keyname>Razavi</keyname><forenames>Basir Shariat</forenames></author></authors><title>Clustering Using Isoperimetric Number of Trees</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a graph-based data clustering algorithm which is
based on exact clustering of a minimum spanning tree in terms of a minimum
isoperimetry criteria. We show that our basic clustering algorithm runs in $O(n
\log n)$ and with post-processing in $O(n^2)$ (worst case) time where $n$ is
the size of the data set. We also show that our generalized graph model which
also allows the use of potentials at vertices can be used to extract a more
detailed pack of information as the {\it outlier profile} of the data set. In
this direction we show that our approach can be used to define the concept of
an outlier-set in a precise way and we propose approximation algorithms for
finding such sets. We also provide a comparative performance analysis of our
algorithm with other related ones and we show that the new clustering algorithm
(without the outlier extraction procedure) behaves quite effectively even on
hard benchmarks and handmade examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4206</identifier>
 <datestamp>2012-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4206</id><created>2012-03-19</created><authors><author><keyname>Kim</keyname><forenames>Kyeongyeon</forenames></author><author><keyname>Choi</keyname><forenames>Jun Won</forenames></author><author><keyname>Kozat</keyname><forenames>Suleyman S.</forenames></author><author><keyname>Singer</keyname><forenames>Andrew C.</forenames></author></authors><title>Low Complexity Turbo-Equalization: A Clustering Approach</title><categories>cs.SY</categories><comments>Submitted to the IEEE Signal Processing Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a low complexity approach to iterative equalization and
decoding, or &quot;turbo equalization&quot;, that uses clustered models to better match
the nonlinear relationship that exists between likelihood information from a
channel decoder and the symbol estimates that arise in soft-input channel
equalization. The introduced clustered turbo equalizer uses piecewise linear
models to capture the nonlinear dependency of the linear minimum mean square
error (MMSE) symbol estimate on the symbol likelihoods produced by the channel
decoder and maintains a computational complexity that is only linear in the
channel memory. By partitioning the space of likelihood information from the
decoder, based on either hard or soft clustering, and using locally-linear
adaptive equalizers within each clustered region, the performance gap between
the linear MMSE equalizer and low-complexity, LMS-based linear turbo equalizers
can be dramatically narrowed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4209</identifier>
 <datestamp>2012-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4209</id><created>2012-03-19</created><authors><author><keyname>Donmez</keyname><forenames>Mehmet A.</forenames></author><author><keyname>Tunc</keyname><forenames>Sait</forenames></author><author><keyname>Kozat</keyname><forenames>Suleyman S.</forenames></author></authors><title>A New Analysis of an Adaptive Convex Mixture: A Deterministic Approach</title><categories>cs.SY</categories><comments>Submitted to the IEEE Signal Processing Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new analysis of an adaptive mixture method that combines
outputs of two constituent filters running in parallel to model an unknown
desired signal. This adaptive mixture is shown to achieve the mean square error
(MSE) performance of the best constituent filter, and in some cases outperforms
both, in the steady-state. However, the MSE analysis of this mixture in the
steady-state and during the transient regions uses approximations and relies on
statistical models on the underlying signals and systems. Hence, such an
analysis may not be useful or valid for signals generated by various real life
systems that show high degrees of nonstationarity, limit cycles and, in many
cases, that are even chaotic. To this end, we perform the transient and the
steady-state analysis of this adaptive mixture in a &quot;strong&quot; deterministic
sense without any approximations in the derivations or statistical assumptions
on the underlying signals such that our results are guaranteed to hold. In
particular, we relate the time-accumulated squared estimation error of this
adaptive mixture at any time to the time-accumulated squared estimation error
of the optimal convex mixture of the constituent filters directly tuned to the
underlying signal in an individual sequence manner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4238</identifier>
 <datestamp>2012-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4238</id><created>2012-03-19</created><authors><author><keyname>Guerini</keyname><forenames>Marco</forenames></author><author><keyname>Pepe</keyname><forenames>Alberto</forenames></author><author><keyname>Lepri</keyname><forenames>Bruno</forenames></author></authors><title>Do Linguistic Style and Readability of Scientific Abstracts affect their
  Virality?</title><categories>cs.SI cs.CL cs.DL</categories><comments>Proceedings of the Sixth International AAAI Conference on Weblogs and
  Social Media (ICWSM 2012), 4-8 June 2012, Dublin, Ireland</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reactions to textual content posted in an online social network show
different dynamics depending on the linguistic style and readability of the
submitted content. Do similar dynamics exist for responses to scientific
articles? Our intuition, supported by previous research, suggests that the
success of a scientific article depends on its content, rather than on its
linguistic style. In this article, we examine a corpus of scientific abstracts
and three forms of associated reactions: article downloads, citations, and
bookmarks. Through a class-based psycholinguistic analysis and readability
indices tests, we show that certain stylistic and readability features of
abstracts clearly concur in determining the success and viral capability of a
scientific article.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4257</identifier>
 <datestamp>2012-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4257</id><created>2012-03-19</created><authors><author><keyname>Abdelkafi</keyname><forenames>Mahdi</forenames></author><author><keyname>Bouzguenda</keyname><forenames>Lotfi</forenames></author><author><keyname>Gargouri</keyname><forenames>Faiez</forenames></author></authors><title>DiscopFlow: A new Tool for Discovering Organizational Structures and
  Interaction Protocols in WorkFlow</title><categories>cs.DC</categories><comments>Journal of E-Technology, Volume: 2, Issue: 2 May 2011;
  http://www.dline.info/jet/v2n2.php, 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work deals with Workflow Mining (WM) a very active and promising
research area. First, in this paper we give a critical and comparative study of
three representative WM systems of this area: the ProM, InWolve and
WorkflowMiner systems. The comparison is made according to quality criteria
that we have defined such as the capacity to filter and convert a Workflow log,
the capacity to discover workflow perspectives and the capacity to support
Multi-Analysis of processes. The major drawback of these systems is the non
possibility to deal with organizational perspective discovering issue. We mean
by organizational perspective, the organizational structures (federation,
coalition, market or hierarchy) and interaction protocols (contract net,
auction or vote). This paper defends the idea that organizational dimension in
Multi-Agent System is an appropriate approach to support the discovering of
this organizational perspective. Second, the paper proposes a Workflow log
meta-model which extends the classical one by considering the interactions
among actors thanks to the FIPA-ACL Performatives. Third, it describes in
details our DiscopFlow tool which validates our contribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4280</identifier>
 <datestamp>2015-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4280</id><created>2012-03-19</created><authors><author><keyname>Gupta</keyname><forenames>Otkrist</forenames></author><author><keyname>Velten</keyname><forenames>Andreas</forenames></author><author><keyname>Willwacher</keyname><forenames>Thomas</forenames></author><author><keyname>Veeraraghavan</keyname><forenames>Ashok</forenames></author><author><keyname>Raskar</keyname><forenames>Ramesh</forenames></author></authors><title>Reconstruction of hidden 3D shapes using diffuse reflections</title><categories>physics.optics cs.CV</categories><doi>10.1364/OE.20.019096</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze multi-bounce propagation of light in an unknown hidden volume and
demonstrate that the reflected light contains sufficient information to recover
the 3D structure of the hidden scene. We formulate the forward and inverse
theory of secondary and tertiary scattering reflection using ideas from energy
front propagation and tomography. We show that using careful choice of
approximations, such as Fresnel approximation, greatly simplifies this problem
and the inversion can be achieved via a backpropagation process. We provide a
theoretical analysis of the invertibility, uniqueness and choices of
space-time-angle dimensions using synthetic examples. We show that a 2D streak
camera can be used to discover and reconstruct hidden geometry. Using a 1D high
speed time of flight camera, we show that our method can be used recover 3D
shapes of objects &quot;around the corner&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4287</identifier>
 <datestamp>2012-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4287</id><created>2012-03-19</created><authors><author><keyname>Islam</keyname><forenames>Muhammad Asiful</forenames></author><author><keyname>Ramakrishnan</keyname><forenames>C. R.</forenames></author><author><keyname>Ramakrishnan</keyname><forenames>I. V.</forenames></author></authors><title>Parameter Learning in PRISM Programs with Continuous Random Variables</title><categories>cs.AI</categories><comments>7 pages. Main contribution: Learning algorithm. Inference appears in
  http://arxiv.org/abs/1112.2681</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Probabilistic Logic Programming (PLP), exemplified by Sato and Kameya's
PRISM, Poole's ICL, De Raedt et al's ProbLog and Vennekens et al's LPAD,
combines statistical and logical knowledge representation and inference.
Inference in these languages is based on enumerative construction of proofs
over logic programs. Consequently, these languages permit very limited use of
random variables with continuous distributions. In this paper, we extend PRISM
with Gaussian random variables and linear equality constraints, and consider
the problem of parameter learning in the extended language. Many statistical
models such as finite mixture models and Kalman filter can be encoded in
extended PRISM. Our EM-based learning algorithm uses a symbolic inference
procedure that represents sets of derivations without enumeration. This permits
us to learn the distribution parameters of extended PRISM programs with
discrete as well as Gaussian variables. The learning algorithm naturally
generalizes the ones used for PRISM and Hybrid Bayesian Networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4309</identifier>
 <datestamp>2012-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4309</id><created>2012-03-19</created><updated>2012-07-31</updated><authors><author><keyname>Anshari</keyname><forenames>Muhammad</forenames></author><author><keyname>Almunawar</keyname><forenames>Mohammad Nabil</forenames></author><author><keyname>Low</keyname><forenames>Patrick Kim Cheng</forenames></author></authors><title>CRM 2.0 within E-Health Systems: Towards Achieving Health Literacy &amp;
  Customer Satisfaction</title><categories>cs.OH</categories><comments>Journal of Development Informatics Vol.1, No. 1, 2012. arXiv admin
  note: text overlap with arXiv:1203.3923, arXiv:1204.3691, arXiv:1204.3685,
  arXiv:1203.3919</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Customer Relationship Management (CRM) within healthcare organization can be
viewed as a strategy to attract new customers and retaining them throughout
their entire lifetime of relationships. At the same time, the advancement of
Web technology known as Web 2.0 plays a significant part in the CRM transition
which drives social change that impacts all institutions including business and
healthcare organizations. This new paradigm has been named as Social CRM or CRM
2.0 because it is based on Web 2.0. We conducted survey to examine the features
of CRM 2.0 in healthcare scenario to the customer in Brunei Darussalam. We draw
the conclusion that the CRM 2.0 in healthcare technologies has brought a
possibility to extend the services of e-health by enabling patients, patient's
families, and community at large to participate more actively in the process of
health education; it helps improve health literacy through empowerment, social
networking process, and online health educator. This paper is based on our
works presented at ICID 2011.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4311</identifier>
 <datestamp>2012-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4311</id><created>2012-03-19</created><authors><author><keyname>Chia</keyname><forenames>Yeow-Khiang</forenames></author><author><keyname>Soundararajan</keyname><forenames>Rajiv</forenames></author><author><keyname>Weissman</keyname><forenames>Tsachy</forenames></author></authors><title>Estimation with a helper who knows the interference</title><categories>cs.IT math.IT</categories><comments>33 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of estimating a signal corrupted by independent
interference with the assistance of a cost-constrained helper who knows the
interference causally or noncausally. When the interference is known causally,
we characterize the minimum distortion incurred in estimating the desired
signal. In the noncausal case, we present a general achievable scheme for
discrete memoryless systems and novel lower bounds on the distortion for the
binary and Gaussian settings. Our Gaussian setting coincides with that of
assisted interference suppression introduced by Grover and Sahai. Our lower
bound for this setting is based on the relation recently established by Verd\'u
between divergence and minimum mean squared error. We illustrate with a few
examples that this lower bound can improve on those previously developed. Our
bounds also allow us to characterize the optimal distortion in several
interesting regimes. Moreover, we show that causal and noncausal estimation are
not equivalent for this problem. Finally, we consider the case where the
desired signal is also available at the helper. We develop new lower bounds for
this setting that improve on those previously developed, and characterize the
optimal distortion up to a constant multiplicative factor for some regimes of
interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4319</identifier>
 <datestamp>2012-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4319</id><created>2012-03-20</created><authors><author><keyname>Azni</keyname><forenames>A. H.</forenames></author><author><keyname>Ahmad</keyname><forenames>Rabiah</forenames></author><author><keyname>Noh</keyname><forenames>Zul Azri Muhamad</forenames></author><author><keyname>Basari</keyname><forenames>Abd Samad Hasan</forenames></author><author><keyname>Hussin</keyname><forenames>Burairah</forenames></author></authors><title>Correlated Node Behavior Model based on Semi Markov Process for MANETS</title><categories>cs.CR</categories><comments>IJCSI Volume 9, Issue 1, January 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a new model for node behavior namely Correlated Node
Behavior Model which is an extension of Node Behavior Model. The model adopts
semi Markov process in continuous time which clusters the node that has
correlation. The key parameter of the process is determined by five
probabilistic parameters based on the Markovian model. Computed from the
transition probabilities of the semi-Markov process, the node correlation
impact on network survivability and resilience can be measure quantitatively.
From the result, the quantitative analysis of correlated node behavior on the
survivability is obtained through mathematical description, and the
effectiveness and rationality of the proposed model are verified through
numerical analysis. The analytical results show that the effect from correlated
failure nodes on network survivability is much severer than other misbehaviors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4324</identifier>
 <datestamp>2012-06-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4324</id><created>2012-03-20</created><updated>2012-06-06</updated><authors><author><keyname>Bei</keyname><forenames>Xiaohui</forenames></author><author><keyname>Chen</keyname><forenames>Wei</forenames></author><author><keyname>Zhang</keyname><forenames>Jialin</forenames></author></authors><title>Distributed Consensus Resilient to Both Crash Failures and Strategic
  Manipulations</title><categories>cs.DC cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study distributed consensus in synchronous systems subject
to both unexpected crash failures and strategic manipulations by rational
agents in the system. We adapt the concept of collusion-resistant Nash
equilibrium to model protocols that are resilient to both crash failures and
strategic manipulations of a group of colluding agents. For a system with $n$
distributed agents, we design a deterministic protocol that tolerates 2
colluding agents and a randomized protocol that tolerates $n - 1$ colluding
agents, and both tolerate any number of failures. We also show that if
colluders are allowed an extra communication round after each synchronous
round, there is no protocol that can tolerate even 2 colluding agents and 1
crash failure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4339</identifier>
 <datestamp>2012-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4339</id><created>2012-03-20</created><authors><author><keyname>Bouchti</keyname><forenames>Abdelali El</forenames></author><author><keyname>Haqiq</keyname><forenames>Abdelkrim</forenames></author><author><keyname>Kafhali</keyname><forenames>Said El</forenames></author></authors><title>Analysis of Quality of Service Performances of Connection Admission
  Control Mechanisms in OFDMA IEEE 802.16 Network using BMAP Queuing</title><categories>cs.NI</categories><comments>9 pages, This paper has been published in the International Journal
  of Computer Science Issues, IJCSI, Volume 9, Issue 1,No 2, January 2012, pp
  302-310</comments><journal-ref>International Journal of Computer Science Issues, IJCSI, Volume 9,
  Issue 1,No 2, January 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider a single-cell IEEE 802.16 environment in which the
base station allocates subchannels to the subscriber stations in its coverage
area. The subchannels allocated to a subscriber station are shared by multiple
connections at that subscriber station. To ensure the Quality of Service (QoS)
performances, two Connection Admission Control (CAC) mechanisms, namely,
threshold-based and queue-aware CAC mechanisms are considered at a subscriber
station. A queuing analytical framework for these admission control mechanisms
is presented considering Orthogonal Frequency Division Multiple Access (OFDMA)
based transmission at the physical layer. Then, based on the queuing model,
both the connection-level and the packet-level performances are studied and
compared with their analogues in the case without CAC. The connection arrival
is modeled by a Poisson process and the packet arrival for a connection by
Batch Markov Arrival Process (BMAP). We determine analytically and numerically
different QoS performance measures (connection blocking probability, average
number of ongoing connections, average queue length, packet dropping
probability, queue throughput and average packet delay).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4345</identifier>
 <datestamp>2012-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4345</id><created>2012-03-20</created><authors><author><keyname>Deisenroth</keyname><forenames>Marc Peter</forenames></author><author><keyname>Turner</keyname><forenames>Ryan</forenames></author><author><keyname>Huber</keyname><forenames>Marco F.</forenames></author><author><keyname>Hanebeck</keyname><forenames>Uwe D.</forenames></author><author><keyname>Rasmussen</keyname><forenames>Carl Edward</forenames></author></authors><title>Robust Filtering and Smoothing with Gaussian Processes</title><categories>cs.SY cs.AI cs.RO stat.ML</categories><comments>7 pages, 1 figure, draft version of paper accepted at IEEE
  Transactions on Automatic Control</comments><doi>10.1109/TAC.2011.2179426</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a principled algorithm for robust Bayesian filtering and smoothing
in nonlinear stochastic dynamic systems when both the transition function and
the measurement function are described by non-parametric Gaussian process (GP)
models. GPs are gaining increasing importance in signal processing, machine
learning, robotics, and control for representing unknown system functions by
posterior probability distributions. This modern way of &quot;system identification&quot;
is more robust than finding point estimates of a parametric function
representation. In this article, we present a principled algorithm for robust
analytic smoothing in GP dynamic systems, which are increasingly used in
robotics and control. Our numerical evaluations demonstrate the robustness of
the proposed approach in situations where other state-of-the-art Gaussian
filters and smoothers can fail.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4349</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4349</id><created>2012-03-20</created><updated>2012-03-27</updated><authors><author><keyname>Lim</keyname><forenames>Hyon</forenames></author><author><keyname>Lee</keyname><forenames>Hyeonbeom</forenames></author><author><keyname>Kim</keyname><forenames>H. Jin</forenames></author></authors><title>Onboard Flight Control of a Small Quadrotor Using Single Strapdown
  Optical Flow Sensor</title><categories>cs.RO</categories><comments>I would like to remove this article due to copyright problem. Please
  remove my article as soon as possible</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers onboard control of a small-sized quadrotor using a
strapdown embedded optical flow sensor which is conventionally used for desktop
mice. The vehicle considered in this paper can carry only few dozen grams of
payload, therefore conventional camera-based optical flow methods are not
applicable. We present hovering control of the small-sized quadrotor using a
single-chip optical flow sensor, implemented on an 8-bit microprocessor without
external sensors or communication with a ground control station. Detailed
description of all the system components is provided along with evaluation of
the accuracy. Experimental results from flight tests are validated with the
ground-truth data provided by a high-accuracy reference system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4355</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4355</id><created>2012-03-20</created><updated>2012-03-27</updated><authors><author><keyname>Lim</keyname><forenames>Hyon</forenames></author><author><keyname>Sinha</keyname><forenames>Sudipta</forenames></author><author><keyname>Cohen</keyname><forenames>Michael</forenames></author><author><keyname>Uyttendaele</keyname><forenames>Matt</forenames></author></authors><title>Real-time Image-based 6-DOF Localization in Large-Scale Environments</title><categories>cs.CV cs.RO</categories><comments>I would like to withdraw this paper due to copyright problem. Please
  remove my article completely</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a real-time approach for image-based localization within large
scenes that have been reconstructed offline using structure from motion (Sfm).
From monocular video, our method continuously computes a precise 6-DOF camera
pose, by efficiently tracking natural features and matching them to 3D points
in the Sfm point cloud. Our main contribution lies in efficiently interleaving
a fast keypoint tracker that uses inexpensive binary feature descriptors with a
new approach for direct 2D-to-3D matching. The 2D-to-3D matching avoids the
need for online extraction of scale-invariant features. Instead, offline we
construct an indexed database containing multiple DAISY descriptors per 3D
point extracted at multiple scales. The key to the efficiency of our method
lies in invoking DAISY descriptor extraction and matching sparingly during
localization, and in distributing this computation over a window of successive
frames. This enables the algorithm to run in real-time, without fluctuations in
the latency over long durations. We evaluate the method in large indoor and
outdoor scenes. Our algorithm runs at over 30 Hz on a laptop and at 12 Hz on a
low-power, mobile computer suitable for onboard computation on a quadrotor
micro aerial vehicle.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4358</identifier>
 <datestamp>2012-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4358</id><created>2012-03-20</created><authors><author><keyname>Merhav</keyname><forenames>Neri</forenames></author></authors><title>On optimum parameter modulation-estimation from a large deviations
  perspective</title><categories>cs.IT math.IT</categories><comments>26 pages; Submitted to the IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of jointly optimum modulation and estimation of a
real-valued random parameter, conveyed over an additive white Gaussian noise
(AWGN) channel, where the performance metric is the large deviations behavior
of the estimator, namely, the exponential decay rate (as a function of the
observation time) of the probability that the estimation error would exceed a
certain threshold. Our basic result is in providing an exact characterization
of the fastest achievable exponential decay rate, among all possible
modulator-estimator (transmitter-receiver) pairs, where the modulator is
limited only in the signal power, but not in bandwidth. This exponential rate
turns out to be given by the reliability function of the AWGN channel. We also
discuss several ways to achieve this optimum performance, and one of them is
based on quantization of the parameter, followed by optimum channel coding and
modulation, which gives rise to a separation-based transmitter, if one views
this setting from the perspective of joint source-channel coding. This is in
spite of the fact that, in general, when error exponents are considered, the
source-channel separation theorem does not hold true. We also discuss several
observations, modifications and extensions of this result in several
directions, including other channels, and the case of multidimensional
parameter vectors. One of our findings concerning the latter, is that there is
an abrupt threshold effect in the dimensionality of the parameter vector: below
a certain critical dimension, the probability of excess estimation error may
still decay exponentially, but beyond this value, it must converge to unity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4364</identifier>
 <datestamp>2013-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4364</id><created>2012-03-20</created><authors><author><keyname>Rosselle</keyname><forenames>Marilyne</forenames></author></authors><title>Teacher Module in an Assistance Tool - Adaptating a device to a teaching
  context and and teacher's preferences</title><categories>cs.CY</categories><comments>6 pages, 3 figures. This article is a long version of the one edited
  in ICALT'2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This communication presents the genesis and the implementation of a teacher
module, which is included in an Assistance Tool (AT). The teacher module is
based on a teacher model for which we did a thorough analysis of the state of
the art. The aim of the AT is to help a teacher to design pedagogical devices.
Teachers can formulate their needs (assistance in the design) and the AT can
relieve them from repetitive tasks related to the deployment of a teaching
device (assistance in the deployment).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4367</identifier>
 <datestamp>2012-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4367</id><created>2012-03-20</created><authors><author><keyname>Barati</keyname><forenames>Hamidreza</forenames></author><author><keyname>Jaberi</keyname><forenames>Nasrin</forenames></author></authors><title>Thesis Report: Resource Utilization Provisioning in MapReduce</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this thesis report, we have a survey on state-of-the-art methods for
modelling resource utilization of MapReduce applications regard to its
configuration parameters. After implementation of one of the algorithms in
literature, we tried to find that if CPU usage modelling of a MapReduce
application can be used to predict CPU usage of another MapReduce application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4374</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4374</id><created>2012-03-20</created><updated>2013-04-22</updated><authors><author><keyname>Mazurczyk</keyname><forenames>Wojciech</forenames></author></authors><title>VoIP Steganography and Its Detection - A Survey</title><categories>cs.CR cs.MM</categories><comments>19 pages, 4 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Steganography is an ancient art that encompasses various techniques of
information hiding, the aim of which is to secret information into a carrier
message. Steganographic methods are usually aimed at hiding the very existence
of the communication. Due to the rise in popularity of IP telephony, together
with the large volume of data and variety of protocols involved, it is
currently attracting the attention of the research community as a perfect
carrier for steganographic purposes. This paper is a survey of the existing
VoIP steganography (steganophony) methods and their countermeasures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4380</identifier>
 <datestamp>2014-06-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4380</id><created>2012-03-20</created><updated>2014-06-16</updated><authors><author><keyname>Vanetik</keyname><forenames>Natalia</forenames></author></authors><title>Analyzing closed frequent itemsets with convex polytopes</title><categories>cs.DB</categories><comments>Published as another paper with different data model</comments><acm-class>G.1.6; H.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computing frequent itemsets in transactional databases is a vital but
computationally expensive task. Measuring the difference of two datasets is
often done by computing their respective frequent itemsets despite high
computational cost. This paper proposes a linear programming-based approach to
this problem and shows that there exists a distance measure for transactional
database that relies on closed frequent itemsets but does not require their
generation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4385</identifier>
 <datestamp>2012-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4385</id><created>2012-03-20</created><authors><author><keyname>Tavakoli</keyname><forenames>H.</forenames></author><author><keyname>Attari</keyname><forenames>M. Ahmadian</forenames></author><author><keyname>Peyghami</keyname><forenames>M. R.</forenames></author></authors><title>Optimal Rate and Maximum Erasure Probability LDPC Codes in Binary
  Erasure Channel</title><categories>cs.IT math.IT</categories><comments>6</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a novel way for solving the main problem of
designing the capacity approaching irregular low-density parity-check (LDPC)
code ensemble over binary erasure channel (BEC). The proposed method is much
simpler, faster, accurate and practical than other methods. Our method does not
use any relaxation or any approximate solution like previous works. Our method
works and finds optimal answer for any given check node degree distribution.
The proposed method was implemented and it works well in practice with
polynomial time complexity. As a result, we represent some degree distributions
that their rates are close to the capacity with maximum erasure probability and
maximum code rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4416</identifier>
 <datestamp>2012-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4416</id><created>2012-03-20</created><authors><author><keyname>Desjardins</keyname><forenames>Guillaume</forenames></author><author><keyname>Courville</keyname><forenames>Aaron</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author></authors><title>On Training Deep Boltzmann Machines</title><categories>cs.NE cs.AI cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The deep Boltzmann machine (DBM) has been an important development in the
quest for powerful &quot;deep&quot; probabilistic models. To date, simultaneous or joint
training of all layers of the DBM has been largely unsuccessful with existing
training methods. We introduce a simple regularization scheme that encourages
the weight vectors associated with each hidden unit to have similar norms. We
demonstrate that this regularization can be easily combined with standard
stochastic maximum likelihood to yield an effective training strategy for the
simultaneous training of all layers of the deep Boltzmann machine.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4422</identifier>
 <datestamp>2012-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4422</id><created>2012-03-20</created><authors><author><keyname>Michaeli</keyname><forenames>Tomer</forenames></author><author><keyname>Eldar</keyname><forenames>Yonina C.</forenames></author><author><keyname>Sapiro</keyname><forenames>Guillermo</forenames></author></authors><title>Semi-Supervised Single- and Multi-Domain Regression with Multi-Domain
  Training</title><categories>stat.ML cs.LG</categories><comments>24 pages, 6 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problems of multi-domain and single-domain regression based on
distinct and unpaired labeled training sets for each of the domains and a large
unlabeled training set from all domains. We formulate these problems as a
Bayesian estimation with partial knowledge of statistical relations. We propose
a worst-case design strategy and study the resulting estimators. Our analysis
explicitly accounts for the cardinality of the labeled sets and includes the
special cases in which one of the labeled sets is very large or, in the other
extreme, completely missing. We demonstrate our estimators in the context of
removing expressions from facial images and in the context of audio-visual word
recognition, and provide comparisons to several recently proposed multi-modal
learning algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4434</identifier>
 <datestamp>2012-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4434</id><created>2012-03-20</created><authors><author><keyname>Zaier</keyname><forenames>Aida</forenames></author><author><keyname>Bouallegue</keyname><forenames>Ridha</forenames></author></authors><title>Blind Channel Estimation Enhancement for Mimo- OFDM Systems under High
  Mobility Conditions</title><categories>cs.NI</categories><comments>8 pages, 4 figures</comments><journal-ref>International Journal of Wireless &amp; Mobile Networks (IJWMN) Vol.
  4, No. 1, February 2012</journal-ref><doi>10.5121/ijwmn.2012.4115</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose an enhancement of a blind channel estimator based
on a subspace approach in a MIMO OFDM context (Multi Input Multi Output
Orthogonal Frequency Division Multiplexing) in high mobility scenario. As
known, the combination between the MIMO context and the OFDM system has
stimulated mainly the evolution of the fourth generation broadband wireless
communications. The simulations results have demonstrated the effectiveness of
the approach for a 16 QAM modulation scheme and had been evaluated in term of
bit error rate BER and mean square error MSE versus the signal to noise ratio
SNR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4455</identifier>
 <datestamp>2012-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4455</id><created>2012-03-20</created><authors><author><keyname>Bei</keyname><forenames>Xiaohui</forenames></author><author><keyname>Chen</keyname><forenames>Ning</forenames></author><author><keyname>Gravin</keyname><forenames>Nick</forenames></author><author><keyname>Lu</keyname><forenames>Pinyan</forenames></author></authors><title>Budget Feasible Mechanism Design: From Prior-Free to Bayesian</title><categories>cs.GT</categories><comments>to appear in STOC 2012</comments><journal-ref>Proceedings of STOC 2012</journal-ref><doi>10.1145/2213977.2214020</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Budget feasible mechanism design studies procurement combinatorial auctions
where the sellers have private costs to produce items, and the
buyer(auctioneer) aims to maximize a social valuation function on subsets of
items, under the budget constraint on the total payment. One of the most
important questions in the field is &quot;which valuation domains admit truthful
budget feasible mechanisms with `small' approximations (compared to the social
optimum)?&quot; Singer showed that additive and submodular functions have such
constant approximations. Recently, Dobzinski, Papadimitriou, and Singer gave an
O(log^2 n)-approximation mechanism for subadditive functions; they also
remarked that: &quot;A fundamental question is whether, regardless of computational
constraints, a constant-factor budget feasible mechanism exists for subadditive
functions.&quot;
  We address this question from two viewpoints: prior-free worst case analysis
and Bayesian analysis. For the prior-free framework, we use an LP that
describes the fractional cover of the valuation function; it is also connected
to the concept of approximate core in cooperative game theory. We provide an
O(I)-approximation mechanism for subadditive functions, via the worst case
integrality gap I of LP. This implies an O(log n)-approximation for subadditive
valuations, O(1)-approximation for XOS valuations, and for valuations with a
constant I. XOS valuations are an important class of functions that lie between
submodular and subadditive classes. We give another polynomial time O(log
n/loglog n) sub-logarithmic approximation mechanism for subadditive valuations.
  For the Bayesian framework, we provide a constant approximation mechanism for
all subadditive functions, using the above prior-free mechanism for XOS
valuations as a subroutine. Our mechanism allows correlations in the
distribution of private information and is universally truthful.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4472</identifier>
 <datestamp>2012-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4472</id><created>2012-03-19</created><authors><author><keyname>Shakhakarmi</keyname><forenames>Niraj</forenames></author></authors><title>Optimization of Cellular Resources Evading Intra and Inter Tier
  Interference in Femto cells Equipped Macro cell Networks</title><categories>cs.NI</categories><comments>10 pages 19 figures</comments><journal-ref>International Journal of Computer Science Issues (IJCSI), Volume
  9, Issue 2, March 2012</journal-ref><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Cellular network resources are essential to be optimized in Femto cells
equipped macro cell networks. This is achieved by increasing the cellular
coverage and channel capacity, and reducing power usage and interference
between femto cells and macro cells. In this paper, the optimization approach
for cellular resources with installed femto cells in macro cell networks has
been addressed by deploying smart antennas applications and effect power
adaptation method which significantly optimize the cellular coverage, channel
capacity, power usage, and intra and inter tier interference. The simulation
results also illustrate the outstanding performance of this optimization
methodology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4473</identifier>
 <datestamp>2012-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4473</id><created>2012-03-19</created><authors><author><keyname>Shakhakarmi</keyname><forenames>Niraj</forenames></author><author><keyname>Vaman</keyname><forenames>Dhadesugoor R.</forenames></author></authors><title>Dynamic PL&amp;T using Two Reference Nodes Equipped with Steered Directional
  Antenna for Significant PL&amp;T Accuracy</title><categories>cs.NI</categories><comments>7 pages, 12 figures</comments><journal-ref>Wireless Telecommunications Symposium,UK London,April 16-18, 2012</journal-ref><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Dynamic Position Location and Tracking (PL&amp;T) is proposed deploying the
integrated approach of zone finding and triangulation using two friendly nodes
equipped with Steered Directional Antenna (DA) in Mobile Ad hoc Networks
(MANET). This approach allows the system to use only two references instead of
a typical 3 references for a straight triangulation. Moreover, the performance
of the proposed algorithm with references using directional antennas shows
significant improvement over triangulation using references with
Omnidirectional antennas as the beam power is concentrated. However, dynamic
switching of reference nodes is frequently required as the target moves outside
the predicted zone. This paper presents a better tracking accuracy in using
proposed dynamic PL&amp;T as compared to other PL&amp;T techniques. The multipath
fading is also addressed with the use of KV transform coding technique which
uses forward error correction and sample interleaving achieves greater than 90%
tracking accuracy with BERs of 10-6 or better.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4474</identifier>
 <datestamp>2012-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4474</id><created>2012-03-19</created><authors><author><keyname>Niraj</keyname><forenames>Shakhakarmi</forenames></author><author><keyname>Vaman</keyname><forenames>Dhadesugoor R.</forenames></author></authors><title>Real Time Position Location &amp; Tracking (PL&amp;T) using Prediction Filter
  and Integrated Zone Finding in OFDM Channel</title><categories>cs.NI</categories><comments>10 pages, 13 figures, WSEAS Transaction on Communicaitons, March,
  2012</comments><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  The nature of pre-determined and on-demand mobile network fabrics can be
exploited for real time Position Location and Tracking (PL&amp;T) of radios and
sensors (nodes) for Global Positioning System (GPS) denied or GPS-free systems.
This issue is addressed by a novel system of integrated zone finding and
triangulation method for determining the PL&amp;T of nodes when mobile network
fabrics are employed based on using directional antennas for radio
communications. Each mobile node is switched dynamically between being a
reference and a target node in PL&amp;T operation to improve the PL&amp;T accuracy of a
target node. This paper presents the Baseline PL&amp;T with predictive Kalman
filter and Integrated Zone based PL&amp;T algorithm design that integrates zone
finding and triangulation method. The performance of the proposed algorithm is
analysed using Interleaving-KV sample coding &amp; error correction in Rayleigh and
Rician channel using Orthogonal Frequency Division Multiplexing (OFDM) system
under the severe multipath fading.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4475</identifier>
 <datestamp>2012-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4475</id><created>2012-03-20</created><authors><author><keyname>Talpur</keyname><forenames>Mir Sajjad Hussain</forenames></author><author><keyname>Shaikh</keyname><forenames>Murtaza Hussain</forenames></author></authors><title>Automation of Mobile Pick and Place Robotic System for Small Food
  Industry</title><categories>cs.ET cs.RO</categories><comments>5 Pages, 11 Figures, 2 Tables</comments><report-no>2012 IEEE 978-1-4577-1139-8/12</report-no><acm-class>D.3.2; H.3.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of robotics in food industry is becoming more popular in recent
years. The trend seems to continue as long as the robotics technology meets
diverse and challenging needs of the food producers. Rapid developments in
digital computers and control systems technologies have significant impact in
robotics like any other engineering fields. By utilizing new hardware and
software tools, design of these complex systems that need strong integration of
distinct disciplines is no longer difficult compared to the past. Therefore,
the purpose of this paper is to design and implement a micro-controller based
on reliable and high performance robotic system for food / biscuit
manufacturing line. We propose a design of a vehicle. The robot is capable of
picking unbaked biscuits tray and places them into furnace and then after
baking it picks the biscuits tray from the furnace. A special gripper is
designed to pick and place the biscuits tray with flexibility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4481</identifier>
 <datestamp>2013-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4481</id><created>2012-03-20</created><updated>2013-01-12</updated><authors><author><keyname>Kyrillidis</keyname><forenames>Anastasios</forenames></author><author><keyname>Cevher</keyname><forenames>Volkan</forenames></author></authors><title>Matrix Recipes for Hard Thresholding Methods</title><categories>cs.NA</categories><comments>26 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present and analyze a new set of low-rank recovery
algorithms for linear inverse problems within the class of hard thresholding
methods. We provide strategies on how to set up these algorithms via basic
ingredients for different configurations to achieve complexity vs. accuracy
tradeoffs. Moreover, we study acceleration schemes via memory-based techniques
and randomized, $\epsilon$-approximate matrix projections to decrease the
computational costs in the recovery process. For most of the configurations, we
present theoretical analysis that guarantees convergence under mild problem
conditions. Simulation results demonstrate notable performance improvements as
compared to state-of-the-art algorithms both in terms of reconstruction
accuracy and computational complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4483</identifier>
 <datestamp>2012-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4483</id><created>2012-03-20</created><authors><author><keyname>Gaspers</keyname><forenames>Serge</forenames></author></authors><title>From edge-disjoint paths to independent paths</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let f(k) denote the maximum such that every simple undirected graph
containing two vertices s,t and k edge-disjoint s-t paths, also contains two
vertices u,v and f(k) independent u-v paths. Here, a set of paths is
independent if none of them contains an interior vertex of another. We prove
that f(k)=k if k&lt;3, and f(k)=3 otherwise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4487</identifier>
 <datestamp>2012-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4487</id><created>2012-03-20</created><updated>2012-05-14</updated><authors><author><keyname>Meyer</keyname><forenames>Frank</forenames></author></authors><title>Recommender systems in industrial contexts</title><categories>cs.IR</categories><comments>version 3.30, May 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This thesis consists of four parts: - An analysis of the core functions and
the prerequisites for recommender systems in an industrial context: we identify
four core functions for recommendation systems: Help do Decide, Help to
Compare, Help to Explore, Help to Discover. The implementation of these
functions has implications for the choices at the heart of algorithmic
recommender systems. - A state of the art, which deals with the main techniques
used in automated recommendation system: the two most commonly used algorithmic
methods, the K-Nearest-Neighbor methods (KNN) and the fast factorization
methods are detailed. The state of the art presents also purely content-based
methods, hybridization techniques, and the classical performance metrics used
to evaluate the recommender systems. This state of the art then gives an
overview of several systems, both from academia and industry (Amazon, Google
...). - An analysis of the performances and implications of a recommendation
system developed during this thesis: this system, Reperio, is a hybrid
recommender engine using KNN methods. We study the performance of the KNN
methods, including the impact of similarity functions used. Then we study the
performance of the KNN method in critical uses cases in cold start situation. -
A methodology for analyzing the performance of recommender systems in
industrial context: this methodology assesses the added value of algorithmic
strategies and recommendation systems according to its core functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4494</identifier>
 <datestamp>2012-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4494</id><created>2012-03-20</created><authors><author><keyname>Giacomelli</keyname><forenames>Piero</forenames></author><author><keyname>Munaro</keyname><forenames>Giulia</forenames></author><author><keyname>Rosso</keyname><forenames>Roberto</forenames></author></authors><title>Can an Ad-hoc ontology Beat a Medical Search Engine? The Chronious
  Search Engine case</title><categories>cs.IR cs.DL</categories><comments>presented at the The Fourth International Conference on eHealth,
  Telemedicine, and Social Medicine (eTELEMED2012)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Chronious is an Open, Ubiquitous and Adaptive Chronic Disease Management
Platform for Chronic Obstructive Pulmonary Disease(COPD) Chronic Kidney Disease
(CKD) and Renal Insufficiency. It consists of several modules: an ontology
based literature search engine, a rule based decision support system, remote
sensors interacting with lifestyle interfaces (PDA, monitor touch-screen) and a
machine learning module. All these modules interact each other to allow the
monitoring of two types of chronic diseases and to help clinician in taking
decision for care purpose. This paper illustrates how the ontology search
engine was created and fed and how some comparative test indicated that the
ontology based approach give better results, on some estimation parameters,
than the main reference web search engine.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4499</identifier>
 <datestamp>2012-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4499</id><created>2012-03-20</created><authors><author><keyname>Oliveira</keyname><forenames>Bruno C. d. S.</forenames></author><author><keyname>Schrijvers</keyname><forenames>Tom</forenames></author><author><keyname>Choi</keyname><forenames>Wontae</forenames></author><author><keyname>Lee</keyname><forenames>Wonchan</forenames></author><author><keyname>Yi</keyname><forenames>Kwangkeun</forenames></author></authors><title>Extended Report: The Implicit Calculus</title><categories>cs.PL</categories><comments>13 pages, extended report of paper accepted at PLDI 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Generic programming (GP) is an increasingly important trend in programming
languages. Well-known GP mechanisms, such as type classes and the C++0x
concepts proposal, usually combine two features: 1) a special type of
interfaces; and 2) implicit instantiation of implementations of those
interfaces.
  Scala implicits are a GP language mechanism, inspired by type classes, that
break with the tradition of coupling implicit instantiation with a special type
of interface. Instead, implicits provide only implicit instantiation, which is
generalized to work for any types. This turns out to be quite powerful and
useful to address many limitations that show up in other GP mechanisms.
  This paper synthesizes the key ideas of implicits formally in a minimal and
general core calculus called the implicit calculus, and it shows how to build
source languages supporting implicit instantiation on top of it. A novelty of
the calculus is its support for partial resolution and higher-order rules (a
feature that has been proposed before, but was never formalized or
implemented). Ultimately, the implicit calculus provides a formal model of
implicits, which can be used by language designers to study and inform
implementations of similar mechanisms in their own languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4519</identifier>
 <datestamp>2012-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4519</id><created>2012-03-19</created><authors><author><keyname>Shakhakarmi</keyname><forenames>Niraj</forenames></author><author><keyname>Vaman</keyname><forenames>Dhadesugoor R.</forenames></author></authors><title>Secured Position Location and Tracking (SPL&amp;T) for Detection of Multiple
  Malicious Nodes Maintaining Two Friendly References in Mobile Ad hoc Networks</title><categories>cs.NI</categories><comments>8 pages, 11 figures</comments><journal-ref>International Journal of Computer Science Issues (IJCSI), Volume
  9, Issue 2, March 2012)</journal-ref><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Secured Position Location and Tracking (PL&amp;T) scheme is developed for
multiple malicious radios or nodes detection using integrated key based strict
friendly scheme and position location and tracking by multi-sectored based
multiple target's PL&amp;T. The friendly and malicious nodes detection is based on
the integrated key consisting of symmetric keys, geographic location and round
trip response time. Two strictly friend references dynamically form the
tracking zone over the detected multiple malicious nodes using the
multi-sectored adaptive beam forming. This PL&amp;T technique is robust, precise,
scalable, and faster than using the single reference, two reference and three
reference nodes based PL&amp;T method in the battlefield oriented Mobile Ad hoc
Networks. The simulation results show that the lower relative speed bound of
any participating node increased the switching overhead, the decreasing
received energy with increasing number of the multi-sectored beams reduced
tracking accuracy and the strict friendly authentication overhead depends upon
the time period between two latest periodic authentication failures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4523</identifier>
 <datestamp>2012-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4523</id><created>2012-03-20</created><updated>2012-09-11</updated><authors><author><keyname>Bach</keyname><forenames>Francis</forenames><affiliation>INRIA Paris - Rocquencourt, LIENS</affiliation></author><author><keyname>Lacoste-Julien</keyname><forenames>Simon</forenames><affiliation>INRIA Paris - Rocquencourt, LIENS</affiliation></author><author><keyname>Obozinski</keyname><forenames>Guillaume</forenames><affiliation>INRIA Paris - Rocquencourt, LIENS</affiliation></author></authors><title>On the Equivalence between Herding and Conditional Gradient Algorithms</title><categories>cs.LG math.OC stat.ML</categories><proxy>ccsd</proxy><journal-ref>ICML 2012 International Conference on Machine Learning, Edimburgh
  : Royaume-Uni (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the herding procedure of Welling (2009) takes exactly the form
of a standard convex optimization algorithm--namely a conditional gradient
algorithm minimizing a quadratic moment discrepancy. This link enables us to
invoke convergence results from convex optimization and to consider faster
alternatives for the task of approximating integrals in a reproducing kernel
Hilbert space. We study the behavior of the different variants through
numerical simulations. The experiments indicate that while we can improve over
herding on the task of approximating integrals, the original herding algorithm
tends to approach more often the maximum entropy distribution, shedding more
light on the learning bias behind herding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4532</identifier>
 <datestamp>2012-03-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4532</id><created>2012-03-20</created><authors><author><keyname>Dvir</keyname><forenames>Zeev</forenames></author><author><keyname>Koll&#xe1;r</keyname><forenames>J&#xe1;nos</forenames></author><author><keyname>Lovett</keyname><forenames>Shachar</forenames></author></authors><title>Variety Evasive Sets</title><categories>cs.CC cs.DM math.AG math.CO</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give an explicit construction of a large subset of F^n, where F is a
finite field, that has small intersection with any affine variety of fixed
dimension and bounded degree. Our construction generalizes a recent result of
Dvir and Lovett (STOC 2012) who considered varieties of degree one (affine
subspaces).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4544</identifier>
 <datestamp>2013-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4544</id><created>2012-03-20</created><authors><author><keyname>Hansen</keyname><forenames>Johan P.</forenames></author></authors><title>Quantum Codes from Toric Surfaces</title><categories>math.AG cs.IT math.IT</categories><comments>IEEE copyright</comments><msc-class>14M25, 81P68</msc-class><doi>10.1109/TIT.2012.2220523</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A theory for constructing quantum error correcting codes from Toric surfaces
by the Calderbank-Shor-Steane method is presented. In particular we study the
method on toric Hirzebruch surfaces. The results are obtained by constructing a
dualizing differential form for the toric surface and by using the cohomology
and the intersection theory of toric varieties. In earlier work the author
developed methods to construct linear error correcting codes from toric
varieties and derive the code parameters using the cohomology and the
intersection theory on toric varieties. This method is generalized in section
to construct linear codes suitable for constructing quantum codes by the
Calderbank-Shor-Steane method. Essential for the theory is the existence and
the application of a dualizing differential form on the toric surface. A.R.
Calderbank, P.W. Shor and A.M. Steane produced stabilizer codes from linear
codes containing their dual codes. These two constructions are merged to obtain
results for toric surfaces. Similar merging has been done for algebraic curves
with different methods by A. Ashikhmin, S. Litsyn and M.A. Tsfasman.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4547</identifier>
 <datestamp>2012-05-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4547</id><created>2012-03-20</created><updated>2012-04-29</updated><authors><author><keyname>Kulshrestha</keyname><forenames>Anunay</forenames></author></authors><title>On the Hamming Distance between base-n representations of whole numbers</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We first introduce the Hamming distance between two strings. Then, we apply
this concept to the representations of whole numbers in base n for all positive
integers n &gt; 2. We claim that a simple formula exists for the sum of all
Hamming distances between pairs of consec- utive integers from 1 to m, which we
will derive. We also state and prove other interesting results concerning the
aforementioned topic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4580</identifier>
 <datestamp>2012-03-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4580</id><created>2012-03-20</created><authors><author><keyname>Beck</keyname><forenames>Amir</forenames></author><author><keyname>Eldar</keyname><forenames>Yonina C.</forenames></author></authors><title>Sparsity Constrained Nonlinear Optimization: Optimality Conditions and
  Algorithms</title><categories>cs.IT math.IT math.OC</categories><comments>submitted to SIAM Optimization</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper treats the problem of minimizing a general continuously
differentiable function subject to sparsity constraints. We present and analyze
several different optimality criteria which are based on the notions of
stationarity and coordinate-wise optimality. These conditions are then used to
derive three numerical algorithms aimed at finding points satisfying the
resulting optimality criteria: the iterative hard thresholding method and the
greedy and partial sparse-simplex methods. The first algorithm is essentially a
gradient projection method while the remaining two algorithms are of coordinate
descent type. The theoretical convergence of these methods and their relations
to the derived optimality conditions are studied. The algorithms and results
are illustrated by several numerical examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4583</identifier>
 <datestamp>2012-03-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4583</id><created>2012-03-20</created><authors><author><keyname>Li</keyname><forenames>Liangbin</forenames></author><author><keyname>Jafarkhani</keyname><forenames>Hamid</forenames></author></authors><title>Multi-Antenna System Design with Bright Transmitters and Blind Receivers</title><categories>cs.IT math.IT</categories><comments>submitted to IEEE TWC</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers a scenario for multi-input multi-output (MIMO)
communication systems when perfect channel state information at the transmitter
(CSIT) is given while the equivalent channel state information at the receiver
(CSIR) is not available. Such an assumption is valid for the downlink
multi-user MIMO systems with linear precoders that depend on channels to all
receivers. We propose a concept called dual systems with zero-forcing designs
based on the duality principle, originally proposed to relate Gaussian
multi-access channels (MACs) and Gaussian broadcast channels (BCs). For the
two-user N*2 MIMO BC with N antennas at the transmitter and two antennas at
each of the receivers, we design a downlink interference cancellation (IC)
transmission scheme using the dual of uplink MAC systems employing IC methods.
The transmitter simultaneously sends two precoded Alamouti codes, one for each
user. Each receiver can zero-force the unintended user's Alamouti codes and
decouple its own data streams using two simple linear operations independent of
CSIR. Analysis shows that the proposed scheme achieves a diversity gain of
2(N-1) for equal energy constellations with short-term power and rate
constraints. Power allocation between two users can also be performed, and it
improves the array gain but not the diversity gain. Numerical results
demonstrate that the bit error rate of the downlink IC scheme has a substantial
gain compared to the block diagonalization method, which requires global
channel information at each node.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4587</identifier>
 <datestamp>2014-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4587</id><created>2012-03-20</created><authors><author><keyname>Bilen</keyname><forenames>Cagdas</forenames></author><author><keyname>Wang</keyname><forenames>Yao</forenames></author><author><keyname>Selesnick</keyname><forenames>Ivan</forenames></author></authors><title>High Speed Compressed Sensing Reconstruction in Dynamic Parallel MRI
  Using Augmented Lagrangian and Parallel Processing</title><categories>cs.IT cs.DS math.IT</categories><comments>Submitted to IEEE JETCAS, Special Issue on Circuits, Systems and
  Algorithms for Compressed Sensing</comments><doi>10.1109/JETCAS.2012.2217032</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Magnetic Resonance Imaging (MRI) is one of the fields that the compressed
sensing theory is well utilized to reduce the scan time significantly leading
to faster imaging or higher resolution images. It has been shown that a small
fraction of the overall measurements are sufficient to reconstruct images with
the combination of compressed sensing and parallel imaging. Various
reconstruction algorithms has been proposed for compressed sensing, among which
Augmented Lagrangian based methods have been shown to often perform better than
others for many different applications. In this paper, we propose new Augmented
Lagrangian based solutions to the compressed sensing reconstruction problem
with analysis and synthesis prior formulations. We also propose a computational
method which makes use of properties of the sampling pattern to significantly
improve the speed of the reconstruction for the proposed algorithms in
Cartesian sampled MRI. The proposed algorithms are shown to outperform earlier
methods especially for the case of dynamic MRI for which the transfer function
tends to be a very large matrix and significantly ill conditioned. It is also
demonstrated that the proposed algorithm can be accelerated much further than
other methods in case of a parallel implementation with graphics processing
units (GPUs).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4592</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4592</id><created>2012-03-20</created><updated>2013-04-07</updated><authors><author><keyname>Ballet</keyname><forenames>St&#xe9;phane</forenames></author><author><keyname>Rolland</keyname><forenames>Robert</forenames></author></authors><title>Remarks on low weight codewords of generalized affine and projective
  Reed-Muller codes</title><categories>cs.IT math.IT</categories><comments>New version taking into account recent results from Elodie Leducq on
  the characterization of the next-to-minimal codewords (cf. arXiv:1203.5244)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose new results on low weight codewords of affine and projective
generalized Reed-Muller codes. In the affine case we prove that if the size of
the working finite field is large compared to the degree of the code, the low
weight codewords are products of affine functions. Then in the general case we
study some types of codewords and prove that they cannot be second, thirds or
fourth weight depending on the hypothesis. In the projective case the second
distance of generalized Reed-Muller codes is estimated, namely a lower bound
and an upper bound of this weight are given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4597</identifier>
 <datestamp>2012-03-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4597</id><created>2012-03-20</created><authors><author><keyname>Ozkan</keyname><forenames>Huseyin</forenames></author><author><keyname>Akman</keyname><forenames>Arda</forenames></author><author><keyname>Kozat</keyname><forenames>Suleyman S.</forenames></author></authors><title>A Novel Training Algorithm for HMMs with Partial and Noisy Access to the
  States</title><categories>cs.LG stat.ML</categories><comments>Submitted to Digital Signal Processing, Elsevier</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a new estimation algorithm for the parameters of an HMM
as to best account for the observed data. In this model, in addition to the
observation sequence, we have \emph{partial} and \emph{noisy} access to the
hidden state sequence as side information. This access can be seen as &quot;partial
labeling&quot; of the hidden states. Furthermore, we model possible mislabeling in
the side information in a joint framework and derive the corresponding EM
updates accordingly. In our simulations, we observe that using this side
information, we considerably improve the state recognition performance, up to
70%, with respect to the &quot;achievable margin&quot; defined by the baseline
algorithms. Moreover, our algorithm is shown to be robust to the training
conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4598</identifier>
 <datestamp>2012-03-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4598</id><created>2012-03-20</created><authors><author><keyname>Donmez</keyname><forenames>Mehmet A.</forenames></author><author><keyname>Inan</keyname><forenames>Huseyin A.</forenames></author><author><keyname>Kozat</keyname><forenames>Suleyman S.</forenames></author></authors><title>Adaptive Mixture Methods Based on Bregman Divergences</title><categories>cs.LG</categories><comments>Submitted to Digital Signal Processing, Elsevier; IEEE.org</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate adaptive mixture methods that linearly combine outputs of $m$
constituent filters running in parallel to model a desired signal. We use
&quot;Bregman divergences&quot; and obtain certain multiplicative updates to train the
linear combination weights under an affine constraint or without any
constraints. We use unnormalized relative entropy and relative entropy to
define two different Bregman divergences that produce an unnormalized
exponentiated gradient update and a normalized exponentiated gradient update on
the mixture weights, respectively. We then carry out the mean and the
mean-square transient analysis of these adaptive algorithms when they are used
to combine outputs of $m$ constituent filters. We illustrate the accuracy of
our results and demonstrate the effectiveness of these updates for sparse
mixture systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4600</identifier>
 <datestamp>2015-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4600</id><created>2012-03-20</created><updated>2015-07-08</updated><authors><author><keyname>Zahl</keyname><forenames>Joshua</forenames></author></authors><title>A Szemeredi-Trotter type theorem in $\mathbb{R}^4$</title><categories>math.CO cs.CG</categories><comments>50 pages. V3: final version. To appear in Discrete and Computational
  Geometry</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that $m$ points and $n$ two-dimensional algebraic surfaces in
$\mathbb{R}^4$ can have at most
$O(m^{\frac{k}{2k-1}}n^{\frac{2k-2}{2k-1}}+m+n)$ incidences, provided that the
algebraic surfaces behave like pseudoflats with $k$ degrees of freedom, and
that $m\leq n^{\frac{2k+2}{3k}}$. As a special case, we obtain a
Szemer\'edi-Trotter type theorem for 2--planes in $\mathbb{R}^4$, provided
$m\leq n$ and the planes intersect transversely. As a further special case, we
obtain a Szemer\'edi-Trotter type theorem for complex lines in $\mathbb{C}^2$
with no restrictions on $m$ and $n$ (this theorem was originally proved by
T\'oth using a different method). As a third special case, we obtain a
Szemer\'edi-Trotter type theorem for complex unit circles in $\mathbb{C}^2$. We
obtain our results by combining several tools, including a two-level analogue
of the discrete polynomial partitioning theorem and the crossing lemma.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4605</identifier>
 <datestamp>2012-03-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4605</id><created>2012-03-20</created><authors><author><keyname>El-shishtawy</keyname><forenames>Tarek</forenames></author><author><keyname>Al-sammak</keyname><forenames>Abdulwahab</forenames></author></authors><title>Arabic Keyphrase Extraction using Linguistic knowledge and Machine
  Learning Techniques</title><categories>cs.CL</categories><comments>Proceedings of the Second International Conference on Arabic Language
  Resources and Tools, 2009</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a supervised learning technique for extracting keyphrases of
Arabic documents is presented. The extractor is supplied with linguistic
knowledge to enhance its efficiency instead of relying only on statistical
information such as term frequency and distance. During analysis, an annotated
Arabic corpus is used to extract the required lexical features of the document
words. The knowledge also includes syntactic rules based on part of speech tags
and allowed word sequences to extract the candidate keyphrases. In this work,
the abstract form of Arabic words is used instead of its stem form to represent
the candidate terms. The Abstract form hides most of the inflections found in
Arabic words. The paper introduces new features of keyphrases based on
linguistic knowledge, to capture titles and subtitles of a document. A simple
ANOVA test is used to evaluate the validity of selected features. Then, the
learning model is built using the LDA - Linear Discriminant Analysis - and
training documents. Although, the presented system is trained using documents
in the IT domain, experiments carried out show that it has a significantly
better performance than the existing Arabic extractor systems, where precision
and recall values reach double their corresponding values in the other systems
especially for lengthy and non-scientific articles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4614</identifier>
 <datestamp>2012-03-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4614</id><created>2012-03-20</created><authors><author><keyname>Pathan</keyname><forenames>Al-Sakib Khan</forenames></author></authors><title>On the Boundaries of Trust and Security in Computing and Communications
  Systems</title><categories>cs.CR</categories><comments>Not copyrighted, own version manuscript to be published in IJTMCC,
  Inderscience, 5 pages in single column single space, 12 font Times New Roman</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article analyzes trust and security in computing and communications
systems. While in human-life, trust usually has some kind of commonly
understood meaning, in the realm of computing and communications systems, it
could be interpreted differently in different environments and settings. On the
other hand, security is about making sure that the participating entities are
legitimate in a communication event or incident so that the core requirements
of privacy, integrity, and authenticity are maintained. This notion is also
true for our human life, even for example entering a house needs legitimacy of
a person. Some boundary lines preserve the security; otherwise an unwanted
access is called a 'security breach'. The intent of this article is to compare
and discuss these two terms with our societal behavior and understanding
amongst entities. To illustrate these issues especially in computing and
communications world, some of the innovating and recent technologies are
discussed which demand trust and security within their core operational
structures. Alongside presenting generally established ideas, some critical
points are mentioned that may be sometimes debatable within the research
community.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4617</identifier>
 <datestamp>2012-03-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4617</id><created>2012-03-20</created><authors><author><keyname>Lindgren</keyname><forenames>John</forenames></author><author><keyname>Libby</keyname><forenames>Vibeke</forenames></author></authors><title>An Arithmetic and Geometric Mean Invariant</title><categories>cs.NA math.CA</categories><comments>5 pages, 1 figure</comments><msc-class>03, 65</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A positive real interval, [a, b], can be partitioned into sub-intervals such
that sub-interval widths divided by sub-interval &quot;average&quot; values remains
constant. That both Arithmetic Mean and Geometric Mean &quot;average&quot; values produce
constant ratios for the same log scale is the stated invariance proved in this
short note. The continuous analog is briefly considered and shown to have
similar properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4619</identifier>
 <datestamp>2012-03-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4619</id><created>2012-03-20</created><authors><author><keyname>Azar</keyname><forenames>Yossi</forenames></author><author><keyname>Panigrahi</keyname><forenames>Debmalya</forenames></author></authors><title>Online Load Balancing on Unrelated Machines with Startup Costs</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by applications in energy-efficient scheduling in data centers,
Khuller, Li, and Saha introduced the {\em machine activation} problem as a
generalization of the classical optimization problems of set cover and load
balancing on unrelated machines. In this problem, a set of $n$ jobs have to be
distributed among a set of $m$ (unrelated) machines, given the processing time
of each job on each machine, where each machine has a startup cost. The goal is
to produce a schedule of minimum total startup cost subject to a constraint
$\bf L$ on its makespan. While Khuller {\em et al} considered the offline
version of this problem, a typical scenario in scheduling is one where jobs
arrive online and have to be assigned to a machine immediately on arrival. We
give an $(O(\log (mn)\log m), O(\log m))$-competitive randomized online
algorithm for this problem, i.e. the schedule produced by our algorithm has a
makespan of $O({\bf L} \log m)$ with high probability, and a total expected
startup cost of $O(\log (mn)\log m)$ times that of an optimal offline schedule
with makespan $\bf L$. The competitive ratios of our algorithm are (almost)
optimal.
  Our algorithms use the online primal dual framework introduced by Alon {\em
et al} for the online set cover problem, and subsequently developed further by
Buchbinder, Naor, and co-authors. To the best of our knowledge, all previous
applications of this framework have been to linear programs (LPs) with either
packing or covering constraints. One novelty of our application is that we use
this framework for a mixed LP that has both covering and packing constraints.
We hope that the algorithmic techniques developed in this paper to
simultaneously handle packing and covering constraints will be useful for
solving other online optimization problems as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4626</identifier>
 <datestamp>2013-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4626</id><created>2012-03-20</created><updated>2013-12-18</updated><authors><author><keyname>Naghshvar</keyname><forenames>Mohammad</forenames></author><author><keyname>Javidi</keyname><forenames>Tara</forenames></author></authors><title>Active sequential hypothesis testing</title><categories>cs.IT math.IT math.OC math.ST stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/13-AOS1144 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS1144</report-no><journal-ref>Annals of Statistics 2013, Vol. 41, No. 6, 2703-2738</journal-ref><doi>10.1214/13-AOS1144</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a decision maker who is responsible to dynamically collect
observations so as to enhance his information about an underlying phenomena of
interest in a speedy manner while accounting for the penalty of wrong
declaration. Due to the sequential nature of the problem, the decision maker
relies on his current information state to adaptively select the most
``informative'' sensing action among the available ones. In this paper, using
results in dynamic programming, lower bounds for the optimal total cost are
established. The lower bounds characterize the fundamental limits on the
maximum achievable information acquisition rate and the optimal reliability.
Moreover, upper bounds are obtained via an analysis of two heuristic policies
for dynamic selection of actions. It is shown that the first proposed heuristic
achieves asymptotic optimality, where the notion of asymptotic optimality, due
to Chernoff, implies that the relative difference between the total cost
achieved by the proposed policy and the optimal total cost approaches zero as
the penalty of wrong declaration (hence the number of collected samples)
increases. The second heuristic is shown to achieve asymptotic optimality only
in a limited setting such as the problem of a noisy dynamic search. However, by
considering the dependency on the number of hypotheses, under a technical
condition, this second heuristic is shown to achieve a nonzero information
acquisition rate, establishing a lower bound for the maximum achievable rate
and error exponent. In the case of a noisy dynamic search with size-independent
noise, the obtained nonzero rate and error exponent are shown to be maximum.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4627</identifier>
 <datestamp>2012-07-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4627</id><created>2012-03-20</created><updated>2012-07-06</updated><authors><author><keyname>Cole</keyname><forenames>Richard</forenames></author><author><keyname>Gkatzelis</keyname><forenames>Vasilis</forenames></author><author><keyname>Goel</keyname><forenames>Gagan</forenames></author></authors><title>Truthfulness, Proportional Fairness, and Efficiency</title><categories>cs.GT cs.DS cs.MA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How does one allocate a collection of resources to a set of strategic agents
in a fair and efficient manner without using money? For in many scenarios it is
not feasible to use money to compensate agents for otherwise unsatisfactory
outcomes. This paper studies this question, looking at both fairness and
efficiency measures.
  We employ the proportionally fair solution, which is a well-known fairness
concept for money-free settings. But although finding a proportionally fair
solution is computationally tractable, it cannot be implemented in a truthful
fashion. Consequently, we seek approximate solutions. We give several truthful
mechanisms which achieve proportional fairness in an approximate sense. We use
a strong notion of approximation, requiring the mechanism to give each agent a
good approximation of its proportionally fair utility. In particular, one of
our mechanisms provides a better and better approximation factor as the minimum
demand for every good increases. A motivating example is provided by the
massive privatization auction in the Czech republic in the early 90s.
  With regard to efficiency, prior work has shown a lower bound of 0.5 on the
approximation factor of any swap-dictatorial mechanism approximating a social
welfare measure even for the two agents and multiple goods case. We surpass
this lower bound by designing a non-swap-dictatorial mechanism for this case.
Interestingly, the new mechanism builds on the notion of proportional fairness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4642</identifier>
 <datestamp>2015-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4642</id><created>2012-03-20</created><authors><author><keyname>Wong</keyname><forenames>Felix Ming Fai</forenames></author><author><keyname>Sen</keyname><forenames>Soumya</forenames></author><author><keyname>Chiang</keyname><forenames>Mung</forenames></author></authors><title>Why Watching Movie Tweets Won't Tell the Whole Story?</title><categories>cs.SI physics.soc-ph</categories><comments>6 pages, 4 figures</comments><acm-class>H.1.2; H.3.1; J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data from Online Social Networks (OSNs) are providing analysts with an
unprecedented access to public opinion on elections, news, movies etc. However,
caution must be taken to determine whether and how much of the opinion
extracted from OSN user data is indeed reflective of the opinion of the larger
online population. In this work we study this issue in the context of movie
reviews on Twitter and compare the opinion of Twitter users with that of the
online population of IMDb and Rotten Tomatoes. We introduce new metrics to show
that the Twitter users can be characteristically different from general users,
both in their rating and their relative preference for Oscar-nominated and
non-nominated movies. Additionally, we investigate whether such data can truly
predict a movie's box-office success.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4649</identifier>
 <datestamp>2012-03-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4649</id><created>2012-03-21</created><authors><author><keyname>Saravanan</keyname><forenames>K.</forenames></author><author><keyname>Vijayanand</keyname><forenames>L.</forenames></author><author><keyname>Negesh</keyname><forenames>R. K.</forenames></author></authors><title>A Novel Bluetooth Man-In-The-Middle Attack Based On SSP using OOB
  Association model</title><categories>cs.CR</categories><report-no>EMICS12</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As an interconnection technology, Bluetooth has to address all traditional
security problems, well known from the distributed networks. Moreover, as
Bluetooth networks are formed by the radio links, there are also additional
security aspects whose impact is yet not well understood. In this paper, we
propose a novel Man-In-The-Middle (MITM) attack against Bluetooth enabled
mobile phone that support Simple Secure Pairing(SSP). From the literature it
was proved that the SSP association models such as Numeric comparison, Just
works and passkey Entry are not more secure. Here we propose the Out Of Band
(OOB) channeling with enhanced security than the previous methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4667</identifier>
 <datestamp>2016-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4667</id><created>2012-03-21</created><updated>2016-01-20</updated><authors><author><keyname>Pouly</keyname><forenames>Amaury</forenames></author><author><keyname>Bournez</keyname><forenames>Olivier</forenames></author><author><keyname>Gra&#xe7;a</keyname><forenames>Daniel S.</forenames></author></authors><title>Turing machines can be efficiently simulated by the General Purpose
  Analog Computer</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Church-Turing thesis states that any sufficiently powerful computational
model which captures the notion of algorithm is computationally equivalent to
the Turing machine. This equivalence usually holds both at a computability
level and at a computational complexity level modulo polynomial reductions.
However, the situation is less clear in what concerns models of computation
using real numbers, and no analog of the Church-Turing thesis exists for this
case. Recently it was shown that some models of computation with real numbers
were equivalent from a computability perspective. In particular it was shown
that Shannon's General Purpose Analog Computer (GPAC) is equivalent to
Computable Analysis. However, little is known about what happens at a
computational complexity level. In this paper we shed some light on the
connections between this two models, from a computational complexity level, by
showing that, modulo polynomial reductions, computations of Turing machines can
be simulated by GPACs, without the need of using more (space) resources than
those used in the original Turing computation, as long as we are talking about
bounded computations. In other words, computations done by the GPAC are as
space-efficient as computations done in the context of Computable Analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4685</identifier>
 <datestamp>2012-03-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4685</id><created>2012-03-21</created><updated>2012-03-23</updated><authors><author><keyname>Singh</keyname><forenames>Sumit</forenames></author></authors><title>A Local Approach for Identifying Clusters in Networks</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph clustering is a fundamental problem that has been extensively studied
both in theory and practice. The problem has been defined in several ways in
literature and most of them have been proven to be NP-Hard. Due to their high
practical relevancy, several heuristics for graph clustering have been
introduced which constitute a central tool for coping with NP-completeness, and
are used in applications of clustering ranging from computer vision, to data
analysis, to learning. There exist many methodologies for this problem, however
most of them are global in nature and are unlikely to scale well for very large
networks. In this paper, we propose two scalable local approaches for
identifying the clusters in any network. We further extend one of these
approaches for discovering the overlapping clusters in these networks. Some
experimentation results obtained for the proposed approaches are also
presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4693</identifier>
 <datestamp>2012-03-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4693</id><created>2012-03-21</created><authors><author><keyname>Kissling</keyname><forenames>Christian</forenames></author></authors><title>On the Stability of Contention Resolution Diversity Slotted ALOHA</title><categories>cs.IT math.IT</categories><comments>10 pages, 12 figures This paper is submitted to the IEEE Transactions
  on Communications for possible publication. The IEEE copyright notice applies</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper a Time Division Multiple Access (TDMA) based Random Access (RA)
channel with Successive Interference Cancellation (SIC) is considered for a
finite user population and reliable retransmission mechanism on the basis of
Contention Resolution Diversity Slotted ALOHA (CRDSA). A general mathematical
model based on Markov Chains is derived which makes it possible to predict the
stability regions of SIC-RA channels, the expected delays in equilibrium and
the selection of parameters for a stable channel configuration. Furthermore the
model enables the estimation of the average time before reaching instability.
The presented model is verified against simulations and numerical results are
provided for comparison of the stability of CRDSA versus the stability of
traditional Slotted ALOHA (SA). The presented results show that CRDSA has not
only a high gain over SA in terms of throughput but also in its stability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4694</identifier>
 <datestamp>2012-03-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4694</id><created>2012-03-21</created><authors><author><keyname>Jinwala</keyname><forenames>Devesh C.</forenames></author><author><keyname>Patel</keyname><forenames>Dhiren R.</forenames></author><author><keyname>Patel</keyname><forenames>Sankita</forenames></author><author><keyname>Dasgupta</keyname><forenames>Kankar S.</forenames></author></authors><title>Optimizing the Replay Protection at the Link Layer Security Framework in
  Wireless Sensor Networks</title><categories>cs.CR</categories><comments>12 pages, Accepted for publication in International Journal of
  Computer Science, IAENG Publication - BUT NOT PUBLISHED</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ensuring communications security in Wireless Sensor Networks (WSNs) is very
vital because the security protocols therein, should be devised to work at the
link layer. Theoretically, any link layer security protocol must support three
vital security attributes viz. Confidentiality, Message Integrity and Replay
protection. However, in order to ensure lesser overhead, replay protection is
often not incorporated as part of the link layer security framework. We argue
here, that it is essential to implement replay protection at the link layer
only and devise a simple scheme to do so. We first survey the common approaches
to ensuring replay protection in conventional networks. We also implement the
conventional algorithms for replay protection using the link layer framework
for WSNs viz. TinySec as the underlying platform. Subsequently analyzing their
limitations, we propose a novel Bloom-filter based replay protection algorithm
for unicast communications. We show that our algorithm is better than the other
contemporary approaches for ensuring replay protection in unicast
communications in the WSNs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4697</identifier>
 <datestamp>2012-03-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4697</id><created>2012-03-21</created><authors><author><keyname>Jinwala</keyname><forenames>Devesh</forenames></author><author><keyname>Patel</keyname><forenames>Dhiren</forenames></author><author><keyname>Dasgupta</keyname><forenames>Kankar</forenames></author></authors><title>FlexiSec: A Configurable Link Layer Security Architecture for Wireless
  Sensor Networks</title><categories>cs.CR</categories><comments>22 pages</comments><journal-ref>Journal of Information Assurance and Security 4 (2009) pp. 582-603</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ensuring communications security in Wireless Sensor Networks (WSNs) indeed is
critical; due to the criticality of the resources in the sensor nodes as well
as due to their ubiquitous and pervasive deployment, with varying attributes
and degrees of security required. The proliferation of the next generation
sensor nodes, has not solved this problem, because of the greater emphasis on
low-cost deployment. In addition, the WSNs use data-centric multi-hop
communication that in turn, necessitates the security support to be devised at
the link layer (increasing the cost of security related operations), instead of
being at the application layer, as in general networks. Therefore, an
energy-efficient link layer security framework is necessitated. There do exists
a number of link layer security architectures that offer some combinations of
the security attributes desired by different WSN applications. However, as we
show in this paper, none of them is responsive to the actual security demands
of the applications. Therefore, we believe that there is a need for
investigating the feasibility of a configurable software-based link layer
security architecture wherein an application can be compiled flexibly, with
respect to its actual security demands. In this paper, we analyze, propose and
experiment with the basic design of such configurable link layer security
architecture for WSNs. We also experimentally evaluate various aspects related
to our scheme viz. configurable block ciphers, configurable block cipher modes
of operations, configurable MAC sizes and configurable replay protection. The
architecture proposed is aimed to offer the optimal level of security at the
minimal overhead, thus saving the precious resources in the WSNs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4698</identifier>
 <datestamp>2012-03-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4698</id><created>2012-03-21</created><authors><author><keyname>Jariwala</keyname><forenames>Vivaksha</forenames></author><author><keyname>Jinwala</keyname><forenames>Devesh</forenames></author></authors><title>A Novel Approach for Secure Data Aggregation in Wireless Sensor Networks</title><categories>cs.CR</categories><comments>12 pages, Proceedings of the 10th National Workshop on Cryptology,
  PSG Institute, coimbatore, Indian, Sep 2-4, 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Wireless Sensor Networks (WSNs) are composed of resource starved sensor
nodes that are deployed to sense, process and communicate vital information to
the base station. Due to the stringent constraints on the resources in the
sensor nodes on one hand and due to the communications costs being always
significantly higher than the data processing costs, the WSNs typically, employ
in-network processing, which aims at reducing effectively, the total number of
packets eventually transmitted to the base station. Such innetwork processing
largely employs data aggregation operations that aggregate the data into a
compact representation for further transmission. However, due to the ubiquitous
&amp; pervasive deployment, heavier resource demands of the security protocols and
due to the stringent resource constraints in WSN nodes, the security concerns
in WSNs are even otherwise critical. These concerns assume alarming proportions
when using data aggregation in which the output of the data aggregator nodes
depends on that of various other nodes. Hence, the protocols for data
aggregation have to carefully devised with a constant vigil on ensuring
security of the data. In this paper, based on our survey of the existing
research efforts for ensuring secure data aggregation, we propose a novel
approach using homomorphic encryption and additive digital signatures to
achieve confidentiality, integrity and availability for secure data aggregation
in wireless sensor networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1203.4705</identifier>
 <datestamp>2012-03-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1203.4705</id><created>2012-03-21</created><authors><author><keyname>Bang-Jensen</keyname><forenames>J&#xf8;rgen</forenames></author><author><keyname>Simonsen</keyname><forenames>Sven</forenames></author></authors><title>Arc-Disjoint Paths and Trees in 2-Regular Digraphs</title><categories>math.CO cs.DM</categories><comments>9 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An out-(in-)branching B_s^+ (B_s^-) rooted at s in a digraph D is a connected
spanning subdigraph of D in which every vertex x != s has precisely one arc
entering (leaving) it and s has no arcs entering (leaving) it. We settle the
complexity of the following two problems:
  1) Given a 2-regular digraph $D$, decide if it contains two arc-disjoint
branchings B^+_u, B^-_v.
  2) Given a 2-regular digraph D, decide if it contains an out-branching B^+_u
such that D remains connected after removing the arcs of B^+_u.
  Both problems are NP-complete for general digraphs. We prove that the first
problem remains NP-complete for 2-regular digraphs, whereas the second problem
turns out to be polynomial when we do not prescribe the root in advance. We
also prove that, for 2-regular digraphs, the latter problem is in fact
equivalent to deciding if $D$ contains two arc-disjoint out-branchings. We
generalize this result to k-regular digraphs where we want to find a number of
pairwise arc-disjoint spanning trees and out-branchings such that there are k
in total, again without prescribing any roots.
</abstract></arXiv>
</metadata>
</record>
<resumptionToken cursor="29000" completeListSize="102538">1122234|30001</resumptionToken>
</ListRecords>
</OAI-PMH>
