<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
<responseDate>2016-03-09T03:55:31Z</responseDate>
<request verb="ListRecords" resumptionToken="1122234|83001">http://export.arxiv.org/oai2</request>
<ListRecords>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07275</identifier>
 <datestamp>2015-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07275</id><created>2015-08-28</created><authors><author><keyname>Nassif</keyname><forenames>Ali Bou</forenames></author><author><keyname>Azzeh</keyname><forenames>Mohammad</forenames></author><author><keyname>Capretz</keyname><forenames>Luiz Fernando</forenames></author><author><keyname>Ho</keyname><forenames>Danny</forenames></author></authors><title>A Comparison Between Decision Trees and Decision Tree Forest Models for
  Software Development Effort Estimation</title><categories>cs.SE cs.AI</categories><comments>3rd International Conference on Communications and Information
  Technology (ICCIT), Beirut, Lebanon, pp. 220-224, 2013</comments><doi>10.1109/ICCITechnology.2013.6579553</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Accurate software effort estimation has been a challenge for many software
practitioners and project managers. Underestimation leads to disruption in the
projects estimated cost and delivery. On the other hand, overestimation causes
outbidding and financial losses in business. Many software estimation models
exist; however, none have been proven to be the best in all situations. In this
paper, a decision tree forest (DTF) model is compared to a traditional decision
tree (DT) model, as well as a multiple linear regression model (MLR). The
evaluation was conducted using ISBSG and Desharnais industrial datasets.
Results show that the DTF model is competitive and can be used as an
alternative in software effort prediction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07283</identifier>
 <datestamp>2015-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07283</id><created>2015-08-28</created><authors><author><keyname>Ahmed</keyname><forenames>Faheem</forenames></author><author><keyname>Campbell</keyname><forenames>Piers</forenames></author><author><keyname>Beg</keyname><forenames>Azam</forenames></author><author><keyname>Capretz</keyname><forenames>Luiz Fernando</forenames></author></authors><title>What Soft Skills Software Architect Should Have? A Reflection from
  Software Industry</title><categories>cs.SE</categories><comments>International Conference on Computer Communication and Management
  (ICCCM), Sydney, Australia pp. 565-569, 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The job of software architect is considered very crucial in the life cycle of
the software development because software architecture has a pivotal role in
the success and failure of the software project in terms of cost and quality.
People have different personality traits, and the way they perceive, plan and
execute any assigned task is influenced by it. These personality traits are
characterized by soft skills. Most of the time, software development is a team
work involving several people executing different tasks. The success and
failure stories of software projects revealed the human factor as one of the
critical importance. In this work we are presenting an exploratory study about
the soft skills requirements for a software architects job. We analyzed 124
advertised jobs in the title of software architect and explore the soft skills
requirements. The results of this investigation help in understanding software
skills requirement set for a job of software architect and presents the current
status of their use in job advertisements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07289</identifier>
 <datestamp>2015-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07289</id><created>2015-08-28</created><authors><author><keyname>Dumitrescu</keyname><forenames>Adrian</forenames></author><author><keyname>T&#xf3;th</keyname><forenames>Csaba D.</forenames></author></authors><title>A problem on track runners</title><categories>math.CO cs.DM</categories><comments>4 pages, 0 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider the unit circle $C$ and a circular arc $A$ of length $\ell=|A| &lt; 1$.
It is shown that there exists $k=k(\ell) \in \mathbb{N}$, and a schedule for
$k$ runners with $k$ distinct but constant speeds so that at any time $t \geq
0$, at least one of the $k$ runners is \emph{not} in $A$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07292</identifier>
 <datestamp>2015-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07292</id><created>2015-08-27</created><authors><author><keyname>Noulas</keyname><forenames>Anastasios</forenames></author><author><keyname>Salnikov</keyname><forenames>Vsevolod</forenames></author><author><keyname>Lambiotte</keyname><forenames>Renaud</forenames></author><author><keyname>Mascolo</keyname><forenames>Cecilia</forenames></author></authors><title>Mining Open Datasets for Transparency in Taxi Transport in Metropolitan
  Environments</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Uber has recently been introducing novel practices in urban taxi transport.
Journey prices can change dynamically in almost real time and also vary
geographically from one area to another in a city, a strategy known as surge
pricing. In this paper, we explore the power of the new generation of open
datasets towards understanding the impact of the new disruption technologies
that emerge in the area of public transport. With our primary goal being a more
transparent economic landscape for urban commuters, we provide a direct price
comparison between Uber and the Yellow Cab company in New York. We discover
that Uber, despite its lower standard pricing rates, effectively charges higher
fares on average, especially during short in length, but frequent in
occurrence, taxi journeys. Building on this insight, we develop a smartphone
application, OpenStreetCab, that offers a personalized consultation to mobile
users on which taxi provider is cheaper for their journey. Almost five months
after its launch, the app has attracted more than three thousand users in a
single city. Their journey queries have provided additional insights on the
potential savings similar technologies can have for urban commuters, with a
highlight being that on average, a user in New York saves 6 U.S. Dollars per
taxi journey if they pick the cheapest taxi provider. We run extensive
experiments to show how Uber's surge pricing is the driving factor of higher
journey prices and therefore higher potential savings for our application's
users. Finally, motivated by the observation that Uber's surge pricing is
occurring more frequently that intuitively expected, we formulate a prediction
task where the aim becomes to predict a geographic area's tendency to surge.
Using exogenous to Uber datasets we show how it is possible to estimate
customer demand within an area, and by extension surge pricing, with high
accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07306</identifier>
 <datestamp>2015-08-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07306</id><created>2015-08-28</created><authors><author><keyname>Chen</keyname><forenames>Yan</forenames></author><author><keyname>Machanavajjhala</keyname><forenames>Ashwin</forenames></author></authors><title>On the Privacy Properties of Variants on the Sparse Vector Technique</title><categories>cs.DB cs.CR</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The sparse vector technique is a powerful differentially private primitive
that allows an analyst to check whether queries in a stream are greater or
lesser than a threshold. This technique has a unique property -- the algorithm
works by adding noise with a finite variance to the queries and the threshold,
and guarantees privacy that only degrades with (a) the maximum sensitivity of
any one query in stream, and (b) the number of positive answers output by the
algorithm. Recent work has developed variants of this algorithm, which we call
{\em generalized private threshold testing}, and are claimed to have privacy
guarantees that do not depend on the number of positive or negative answers
output by the algorithm. These algorithms result in a significant improvement
in utility over the sparse vector technique for a given privacy budget, and
have found applications in frequent itemset mining, feature selection in
machine learning and generating synthetic data.
  In this paper we critically analyze the privacy properties of generalized
private threshold testing. We show that generalized private threshold testing
does not satisfy \epsilon-differential privacy for any finite \epsilon. We
identify a subtle error in the privacy analysis of this technique in prior
work. Moreover, we show an adversary can use generalized private threshold
testing to recover counts from the datasets (especially small counts) exactly
with high accuracy, and thus can result in individuals being reidentified. We
demonstrate our attacks empirically on real datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07313</identifier>
 <datestamp>2016-02-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07313</id><created>2015-08-28</created><updated>2016-02-26</updated><authors><author><keyname>Garnier</keyname><forenames>Josselin</forenames></author><author><keyname>Papanicolaou</keyname><forenames>George</forenames></author><author><keyname>Yang</keyname><forenames>Tzu-Wei</forenames></author></authors><title>Consensus Convergence with Stochastic Effects</title><categories>cs.SI nlin.AO physics.soc-ph</categories><comments>Dedication to Willi J\&quot;{a}ger's 75th Birthday</comments><msc-class>92D25, 35Q84, 60K35</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a stochastic, continuous state and time opinion model where each
agent's opinion locally interacts with other agents' opinions in the system,
and there is also exogenous randomness. The interaction tends to create
clusters of common opinion. By using linear stability analysis of the
associated nonlinear Fokker-Planck equation that governs the empirical density
of opinions in the limit of infinitely many agents, we can estimate the number
of clusters, the time to cluster formation and the critical strength of
randomness so as to have cluster formation. We also discuss the cluster
dynamics after their formation, the width and the effective diffusivity of the
clusters. Finally, the long term behavior of clusters is explored numerically.
Extensive numerical simulations confirm our analytical findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07333</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07333</id><created>2015-08-28</created><updated>2016-02-17</updated><authors><author><keyname>Mirza</keyname><forenames>Jawad</forenames></author><author><keyname>Smith</keyname><forenames>Peter J.</forenames></author><author><keyname>Dmochowski</keyname><forenames>Pawel A.</forenames></author><author><keyname>Shafi</keyname><forenames>Mansoor</forenames></author></authors><title>Coordinated Regularized Zero-Forcing Precoding for Multicell MISO
  Systems with Limited Feedback</title><categories>cs.IT math.IT</categories><comments>9 pages, 7 figures, accepted in IEEE Transactions on Vehicular
  Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate coordinated regularized zero-forcing precoding for limited
feedback multicell multiuser multiple-input single-output systems. We begin by
deriving an approximation to the expected signal-to-interference-plus-noise
ratio for the proposed scheme with perfect channel direction information (CDI)
at the base station (BS). We also derive an expected SINR approximation for
limited feedback systems with random vector quantization (RVQ) based codebook
CDI at the BS. Using the expected interference result for the RVQ based limited
feedback CDI, we propose an adaptive feedback bit allocation strategy to
minimize the expected interference by partitioning the total number of bits
between the serving and out-of-cell interfering channels. Numerical results
show that the proposed adaptive feedback bit allocation method offers a
spectral efficiency gain over the existing coordinated zero-forcing scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07338</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07338</id><created>2015-08-28</created><authors><author><keyname>de Beaudrap</keyname><forenames>Niel</forenames></author><author><keyname>Gharibian</keyname><forenames>Sevag</forenames></author></authors><title>A linear time algorithm for quantum 2-SAT</title><categories>quant-ph cs.CC cs.DS</categories><comments>21 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Boolean constraint satisfaction problem 3-SAT is arguably the canonical
NP-complete problem. In contrast, 2-SAT can not only be decided in polynomial
time, but in fact in deterministic linear time. In 2006, Bravyi proposed a
physically motivated generalization of k-SAT to the quantum setting, defining
the problem &quot;quantum k-SAT&quot;. He showed that quantum 2-SAT is also solvable in
polynomial time on a classical computer, in particular in deterministic time
O(n^4), assuming unit-cost arithmetic over a field extension of the rational
numbers, where n is number of variables. In this paper, we present an algorithm
for quantum 2-SAT which runs in linear time, i.e. deterministic time O(n+m) for
n and m the number of variables and clauses, respectively. Our approach
exploits the transfer matrix techniques of Laumann et al. [QIC, 2010] used in
the study of phase transitions for random quantum 2-SAT, and bears similarities
with both the linear time 2-SAT algorithms of Even, Itai, and Shamir (based on
backtracking) [SICOMP, 1976] and Aspvall, Plass, and Tarjan (based on strongly
connected components) [IPL, 1979].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07343</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07343</id><created>2015-08-28</created><authors><author><keyname>Pourazarm</keyname><forenames>Sepideh</forenames></author><author><keyname>Cassandras</keyname><forenames>Christos G.</forenames></author></authors><title>Lifetime Maximization of Wireless Sensor Networks with a Mobile Source
  Node</title><categories>cs.NI math.OC</categories><comments>A shorter version of this work will be published in Proceedings of
  2016 IEEE Conference on Decision and Control</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of routing in sensor networks where the goal is to
maximize the network's lifetime. Previous work has considered this problem for
fixed-topology networks. Here, we add mobility to the source node, which
requires a new definition of the network lifetime. In particular, we redefine
lifetime to be the time until the source node depletes its energy. When the
mobile node's trajectory is unknown in advance, we formulate three versions of
an optimal control problem aiming at this lifetime maximization. We show that
in all cases, the solution can be reduced to a sequence of Non- Linear
Programming (NLP) problems solved on line as the source node trajectory
evolves.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07370</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07370</id><created>2015-08-28</created><updated>2016-03-04</updated><authors><author><keyname>Cole</keyname><forenames>Richard</forenames></author><author><keyname>Tao</keyname><forenames>Yixin</forenames></author></authors><title>When does the Price of Anarchy tend to 1 in Large Walrasian Auctions and
  Fisher Markets?</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As is well known, many classes of markets have efficient equilibria, but this
depends on agents being non-strategic, i.e. that they declare their true
demands when offered goods at particular prices, or in other words, that they
are price-takers. An important question is how much the equilibria degrade in
the face of strategic behavior, i.e. what is the Price of Anarchy (PoA) of the
market viewed as a mechanism?
  Often, PoA bounds are modest constants such as 4/3 or 2. Nonetheless, in
practice a guarantee that no more than 25% or 50% of the economic value is lost
may be unappealing. This paper asks whether significantly better bounds are
possible under plausible assumptions. In particular, we look at how these worst
case guarantees improve in the following large settings.
  Large Walrasian auctions: These are auctions with many copies of each item
and many agents. We show that the PoA tends to 1 as the market size increases,
under suitable conditions, mainly that there is some uncertainty about the
numbers of copies of each good and demands obey the gross substitutes
condition. We also note that some such assumption is unavoidable.
  Large Fisher markets: Fisher markets are a class of economies that has
received considerable attention in the computer science literature. A large
market is one in which at equilibrium, each buyer makes only a small fraction
of the total purchases; the smaller the fraction, the larger the market. Here
the main condition is that demands are based on homogeneous monotone utility
functions that satisfy the gross substitutes condition. Again, the PoA tends to
1 as the market size increases.
  Furthermore, in each setting, we quantify the tradeoff between market size
and the PoA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07371</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07371</id><created>2015-08-28</created><authors><author><keyname>Gadepally</keyname><forenames>Vijay</forenames></author><author><keyname>Kepner</keyname><forenames>Jeremy</forenames></author><author><keyname>Arcand</keyname><forenames>William</forenames></author><author><keyname>Bestor</keyname><forenames>David</forenames></author><author><keyname>Bergeron</keyname><forenames>Bill</forenames></author><author><keyname>Byun</keyname><forenames>Chansup</forenames></author><author><keyname>Edwards</keyname><forenames>Lauren</forenames></author><author><keyname>Hubbell</keyname><forenames>Matthew</forenames></author><author><keyname>Michaleas</keyname><forenames>Peter</forenames></author><author><keyname>Mullen</keyname><forenames>Julie</forenames></author><author><keyname>Prout</keyname><forenames>Andrew</forenames></author><author><keyname>Rosa</keyname><forenames>Antonio</forenames></author><author><keyname>Yee</keyname><forenames>Charles</forenames></author><author><keyname>Reuther</keyname><forenames>Albert</forenames></author></authors><title>D4M: Bringing Associative Arrays to Database Engines</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ability to collect and analyze large amounts of data is a growing problem
within the scientific community. The growing gap between data and users calls
for innovative tools that address the challenges faced by big data volume,
velocity and variety. Numerous tools exist that allow users to store, query and
index these massive quantities of data. Each storage or database engine comes
with the promise of dealing with complex data. Scientists and engineers who
wish to use these systems often quickly find that there is no single technology
that offers a panacea to the complexity of information. When using multiple
technologies, however, there is significant trouble in designing the movement
of information between storage and database engines to support an end-to-end
application along with a steep learning curve associated with learning the
nuances of each underlying technology. In this article, we present the Dynamic
Distributed Dimensional Data Model (D4M) as a potential tool to unify database
and storage engine operations. Previous articles on D4M have showcased the
ability of D4M to interact with the popular NoSQL Accumulo database. Recently
however, D4M now operates on a variety of backend storage or database engines
while providing a federated look to the end user through the use of associative
arrays. In order to showcase how new databases may be supported by D4M, we
describe the process of building the D4M-SciDB connector and present
performance of this connection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07372</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07372</id><created>2015-08-28</created><updated>2015-10-05</updated><authors><author><keyname>Gadepally</keyname><forenames>Vijay</forenames></author><author><keyname>Bolewski</keyname><forenames>Jake</forenames></author><author><keyname>Hook</keyname><forenames>Dan</forenames></author><author><keyname>Hutchison</keyname><forenames>Dylan</forenames></author><author><keyname>Miller</keyname><forenames>Ben</forenames></author><author><keyname>Kepner</keyname><forenames>Jeremy</forenames></author></authors><title>Graphulo: Linear Algebra Graph Kernels for NoSQL Databases</title><categories>cs.DS cs.DB</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Big data and the Internet of Things era continue to challenge computational
systems. Several technology solutions such as NoSQL databases have been
developed to deal with this challenge. In order to generate meaningful results
from large datasets, analysts often use a graph representation which provides
an intuitive way to work with the data. Graph vertices can represent users and
events, and edges can represent the relationship between vertices. Graph
algorithms are used to extract meaningful information from these very large
graphs. At MIT, the Graphulo initiative is an effort to perform graph
algorithms directly in NoSQL databases such as Apache Accumulo or SciDB, which
have an inherently sparse data storage scheme. Sparse matrix operations have a
history of efficient implementations and the Graph Basic Linear Algebra
Subprogram (GraphBLAS) community has developed a set of key kernels that can be
used to develop efficient linear algebra operations. However, in order to use
the GraphBLAS kernels, it is important that common graph algorithms be recast
using the linear algebra building blocks. In this article, we look at common
classes of graph algorithms and recast them into linear algebra operations
using the GraphBLAS building blocks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07380</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07380</id><created>2015-08-28</created><authors><author><keyname>Alsarhan</keyname><forenames>Hamza</forenames></author><author><keyname>Chia</keyname><forenames>Davin</forenames></author><author><keyname>Christman</keyname><forenames>Ananya</forenames></author><author><keyname>Fu</keyname><forenames>Shannia</forenames></author><author><keyname>Jin</keyname><forenames>Tony</forenames></author></authors><title>Colored Bin Packing</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the Colored Bin Packing Problem: we are given a set of items where
each item has a weight and color. We must pack the items in bins of uniform
capacity such that no two items of the same color may be adjacent within in a
bin. The goal is to perform this packing using the fewest number of bins. We
consider a version of the problem where reordering is allowed. We first
consider the zero-weight and unit weight versions of this problem, i.e. where
the items have weight zero and one, respectively. We present linear time
optimal algorithms for both versions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07387</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07387</id><created>2015-08-28</created><authors><author><keyname>Zhang</keyname><forenames>Xi</forenames></author><author><keyname>Jia</keyname><forenames>Ming</forenames></author><author><keyname>Chen</keyname><forenames>Lei</forenames></author><author><keyname>Ma</keyname><forenames>Jianglei</forenames></author><author><keyname>Qiu</keyname><forenames>Jing</forenames></author></authors><title>Filtered-OFDM - Enabler for Flexible Waveform in The 5th Generation
  Cellular Networks</title><categories>cs.IT math.IT</categories><comments>Accepted to IEEE Globecom, San Diego, CA, Dec. 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The underlying waveform has always been a shaping factor for each generation
of the cellular networks, such as orthogonal frequency division multiplexing
(OFDM) for the 4th generation cellular networks (4G). To meet the diversified
and pronounced expectations upon the upcoming 5G cellular networks, here we
present an enabler for flexible waveform configuration, named as filtered-OFDM
(f-OFDM). With the conventional OFDM, a unified numerology is applied across
the bandwidth provided, balancing among the channel characteristics and the
service requirements, and the spectrum efficiency is limited by the compromise
we made. In contrast, with f-OFDM, the assigned bandwidth is split up into
several subbands, and different types of services are accommodated in different
subbands with the most suitable waveform and numerology, leading to an improved
spectrum utilization. After outlining the general framework of f-OFDM, several
important design aspects are also discussed, including filter design and guard
tone arrangement. In addition, an extensive comparison among the existing 5G
waveform candidates is also included to illustrate the advantages of f-OFDM.
Our simulations indicate that, in a specific scenario with four distinct types
of services, f-OFDM provides up to 46% of throughput gains over the
conventional OFDM scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07390</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07390</id><created>2015-08-28</created><authors><author><keyname>Song</keyname><forenames>Tianyu</forenames></author><author><keyname>Kam</keyname><forenames>Pooi-Yuen</forenames></author></authors><title>Background Radiation Cancellation for Free-Space Optical Communications
  with IM/DD</title><categories>cs.IT math.IT</categories><comments>13 pages, submitted to the IEEE/OSA Journal of Lightwave Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Besides atmospheric turbulence and pointing errors which cause the signal
intensity fluctuation, background radiation also impairs the free-space optical
intensity-modulation / direct-detection link performance by introducing a noisy
photocurrent component in the receiver. Methods such as adopting some specific
optics systems, using pilot symbols or line codes to estimate the background
information and cancel it accordingly, and pulse-position modulation (PPM), can
be adopted to mitigate the impact of background radiation. However, purely
depending on the optics system, the background radiation can only be reduced,
but not completely cancelled; and the use of any of pilot symbols, line codes
and PPM reduces the system spectral efficiency drastically. In this paper,
based on the generalized likelihood ratio test (GLRT) principle, we develop a
Viterbi-type trellis-search sequence receiver (the GLRT sequence receiver) that
can estimate the unknown channel gain and the background radiation
simultaneously, and detect the data sequence accordingly. This receiver
requires very few pilot symbols, and therefore, does not significantly reduce
the bandwidth efficiency. Its error performance can approach that of detection
with perfect information of the channel state and the background radiation, as
the observation window length used for forming the decision metric increases.
Since a Viterbi-type trellis-search algorithm is adopted, the search complexity
is very low and is independent of the observation window size. However, the
search complexity of the GLRT sequence receiver grows exponentially with the
modulation order. To further simplify the implementation, we derive a more
efficient decision-feedback symbol-by-symbol receiver which retains the same
error performance as that of the GLRT sequence receiver.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07393</identifier>
 <datestamp>2015-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07393</id><created>2015-08-28</created><authors><author><keyname>Akhtar</keyname><forenames>Yasmeen</forenames></author><author><keyname>Maity</keyname><forenames>Soumen</forenames></author></authors><title>Mixed Covering Arrays on 3-Uniform Hypergraphs</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Covering arrays are combinatorial objects that have been successfully applied
in the design of test suites for testing systems such as software, circuits and
networks, where failures can be caused by the interaction between their
parameters. In this paper, we perform a new generalization of covering arrays
called covering arrays on 3-uniform hypergraphs. Let $n, k$ be positive
integers with $k\geq 3$. Three vectors $x\in \mathbb Z_{g_1}^n$, $y\in \mathbb
Z_{g_2}^n$, $z\in \mathbb Z_{g_3}^n$ are {\it 3-qualitatively independent} if
for any triplet $(a, b, c) \in \mathbb Z_{g_1}\,\times\, \mathbb
Z_{g_2}\,\times\,\mathbb Z_{g_3}$, there exists an index $ j\in \lbrace 1,
2,...,n \rbrace $ such that $( x(j), y(j), z(j)) = (a, b, c)$. Let $H$ be a
3-uniform hypergraph with $k$ vertices $v_1,v_2,\ldots,v_k$ with respective
vertex weights $g_1,g_2,\ldots,g_k$. A mixed covering array on $H$, denoted by
$3-CA(n,H, \prod_{i=1}^{k}g_{i})$, is a $k\times n$ array such that row $i$
corresponds to vertex $v_i$, entries in row $i$ are from $Z_{g_i}$; and if
$\{v_x,v_y,v_z\}$ is a hyperedge in $H$, then the rows $x,y,z$ are
3-qualitatively independent. The parameter $n$ is called the size of the array.
Given a weighted 3-uniform hypergraph $H$, a mixed covering array on $H$ with
minimum size is called optimal. We outline necessary background in the theory
of hypergraphs that is relevant to the study of covering arrays on hypergraphs.
In this article, we introduce five basic hypergraph operations to construct
optimal mixed covering arrays on hypergraphs. Using these operations, we
provide constructions for optimal mixed covering arrays on $\alpha$-acyclic
3-uniform hypergraphs, conformal 3-uniform hypertrees having a binary tree as
host tree, and on some specific 3-uniform cycle hypergraphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07415</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07415</id><created>2015-08-29</created><updated>2015-12-27</updated><authors><author><keyname>Eslahi</keyname><forenames>Nasser</forenames></author><author><keyname>Mahdavinataj</keyname><forenames>Hami</forenames></author><author><keyname>Aghagolzadeh</keyname><forenames>Ali</forenames></author></authors><title>Mixed Gaussian-Impulse Noise Removal from Highly Corrupted Images via
  Adaptive Local and Nonlocal Statistical Priors</title><categories>cs.CV</categories><comments>11 Pages, 7 Figures, 2 Tables, In Proceeding of 9th Iranian Conf.
  Machine Vis. Image Process</comments><msc-class>62F15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The motivation of this paper is to introduce a novel framework for the
restoration of images corrupted by mixed Gaussian-impulse noise. To this aim,
first, an adaptive curvelet thresholding criterion is proposed which tries to
adaptively remove the perturbations appeared during denoising process. Then, a
new statistical regularization term, called joint adaptive statistical prior
(JASP), is established which enforces both the local and nonlocal statistical
consistencies, simultaneously, in a unified manner. Furthermore, a novel
technique for mixed Gaussian plus impulse noise removal using JASP in a
variational scheme is developed--we refer to it as De-JASP. To efficiently
solve the above variational scheme, an efficient alternating minimization
algorithm based on split Bregman iterative framework is developed. Extensive
experimental results manifest the effectiveness of the proposed method
comparing with the current state-of-the-art methods in mixed Gaussian-impulse
noise removal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07416</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07416</id><created>2015-08-29</created><authors><author><keyname>Zhou</keyname><forenames>Guoxu</forenames></author><author><keyname>Zhao</keyname><forenames>Qibin</forenames></author><author><keyname>Zhang</keyname><forenames>Yu</forenames></author><author><keyname>Adal&#x131;</keyname><forenames>T&#xfc;lay</forenames></author><author><keyname>Xie</keyname><forenames>Shengli</forenames></author><author><keyname>Cichocki</keyname><forenames>Andrzej</forenames></author></authors><title>Linked Component Analysis from Matrices to High Order Tensors:
  Applications to Biomedical Data</title><categories>cs.CE cs.LG cs.NA</categories><comments>20 pages, 11 figures, Proceedings of the IEEE, 2015</comments><doi>10.1109/JPROC.2015.2474704</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the increasing availability of various sensor technologies, we now have
access to large amounts of multi-block (also called multi-set,
multi-relational, or multi-view) data that need to be jointly analyzed to
explore their latent connections. Various component analysis methods have
played an increasingly important role for the analysis of such coupled data. In
this paper, we first provide a brief review of existing matrix-based (two-way)
component analysis methods for the joint analysis of such data with a focus on
biomedical applications. Then, we discuss their important extensions and
generalization to multi-block multiway (tensor) data. We show how constrained
multi-block tensor decomposition methods are able to extract similar or
statistically dependent common features that are shared by all blocks, by
incorporating the multiway nature of data. Special emphasis is given to the
flexible common and individual feature analysis of multi-block data with the
aim to simultaneously extract common and individual latent components with
desired properties and types of diversity. Illustrative examples are given to
demonstrate their effectiveness for biomedical data analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07433</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07433</id><created>2015-08-29</created><authors><author><keyname>Ding</keyname><forenames>Zhiguo</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>A General MIMO Framework for NOMA Downlink and Uplink Transmission Based
  on Signal Alignment</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The application of multiple-input multiple-output (MIMO) techniques to
non-orthogonal multiple access (NOMA) systems is important to enhance the
performance gains of NOMA. In this paper, a novel MIMO-NOMA framework for
downlink and uplink transmission is proposed by applying the concept of signal
alignment. By using stochastic geometry, closed-form analytical results are
developed to facilitate the performance evaluation of the proposed framework
for randomly deployed users and interferers. The impact of different power
allocation strategies, such as fixed power allocation and cognitive radio
inspired power allocation, on the performance of MIMO-NOMA is also
investigated. Computer simulation results are provided to demonstrate the
performance of the proposed framework and the accuracy of the developed
analytical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07435</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07435</id><created>2015-08-29</created><authors><author><keyname>Sysala</keyname><forenames>Stanislav</forenames></author></authors><title>An improved return-mapping scheme for nonsmooth plastic potentials: PART
  II - the Mohr-Coulomb yield criterion</title><categories>cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper is devoted to the numerical solution of an elastoplastic
constitutive initial value problems containing the Mohr-Coulomb yield
criterion, a nonassociative plastic flow rule and a nonlinear isotropic
hardening. The plastic flow rule is formulated in a subdifferential form to
keep just one plastic multiplier. It is shown that such a formulation is
suitable also for numerical purposes. Namely, it leads to simplification of the
implicit return-mapping scheme and the consistent tangent operator. This paper
is a free continuation of arXiv:1503.03605.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07440</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07440</id><created>2015-08-29</created><authors><author><keyname>Ghavami</keyname><forenames>Siavash</forenames></author><author><keyname>Adve</keyname><forenames>Raviraj</forenames></author><author><keyname>Lahouti</keyname><forenames>Farshad</forenames></author></authors><title>Bounds on the Capacity of ASK Molecular Communication Channels with ISI</title><categories>cs.IT cs.ET math.IT</categories><comments>7 pages, 4 figures, Accepted in IEEE GLOBECOM 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are now several works on the use of the additive inverse Gaussian noise
(AIGN) model for the random transit time in molecular communication~(MC)
channels. The randomness invariably causes inter-symbol interference (ISI) in
MC, an issue largely ignored or simplified. In this paper we derive an upper
bound and two lower bounds for MC based on amplitude shift keying (ASK) in
presence of ISI. The Blahut-Arimoto algorithm~(BAA) is modified to find the
input distribution of transmitted symbols to maximize the lower bounds. Our
results show that over wide parameter values the bounds are close.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07446</identifier>
 <datestamp>2015-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07446</id><created>2015-08-29</created><updated>2015-10-29</updated><authors><author><keyname>Jamali</keyname><forenames>Mohammad Vahid</forenames></author><author><keyname>Salehi</keyname><forenames>Jawad A.</forenames></author></authors><title>On the BER of Multiple-Input Multiple-Output Underwater Wireless Optical
  Communication Systems</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we analyze and investigate the bit error rate (BER) performance
of multiple-input multiple-output underwater wireless optical communication
(MIMO-UWOC) systems. In addition to exact BER expressions, we also obtain an
upper bound on the system BER. To effectively estimate the BER expressions, we
use Gauss-Hermite quadrature formula as well as approximation to the sum of
log-normal random variables. We confirm the accuracy of our analytical
expressions by evaluating the BER through photon-counting approach. Our
simulation results show that MIMO technique can mitigate the channel
turbulence-induced fading and consequently, can partially extend the viable
communication range, especially for channels with stronger turbulence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07455</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07455</id><created>2015-08-29</created><authors><author><keyname>Gounaris</keyname><forenames>Anastasios</forenames></author></authors><title>Towards Automated Performance Optimization of BPMN Business Processes</title><categories>cs.DB cs.SE</categories><comments>17 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Business Process Model and Notation (BPMN) provides a standard for the design
of business processes. It focuses on bridging the gap between the analysis and
the technical perspectives, and aims to deliver process automation. The aim of
this technical report is to complement this effort by transferring knowledge
from the related field of data-centric workflows aiming to provide automated
performance optimization of the business process execution. Automated
optimization lifts a burden from BPMN designers and increases workflow
flexibility and resilience. As a key step towards this goal, the contribution
of this work is to provide a methodology to map BPMNv2.0 models to annotated
directed acyclic graphs, which emphasize the volume of the tokens exchanged and
are amenable to existing automated optimization algorithms. In addition,
concrete examples of mappings are given, while the optimization opportunities
that are opened are explained, thus providing insights into the potential
performance benefits and we discuss technical research issues.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07468</identifier>
 <datestamp>2016-03-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07468</id><created>2015-08-29</created><updated>2016-02-29</updated><authors><author><keyname>Hou</keyname><forenames>Yuqing</forenames></author></authors><title>Image Annotation Incorporating Low-Rankness, Tag and Visual Correlation
  and Inhomogeneous Errors</title><categories>cs.CV</categories><comments>This paper has been withdrawn by the author to update more
  experiments and some errors in the algorithm</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tag-based image retrieval (TBIR) has drawn much attention in recent years due
to the explosive amount of digital images and crowdsourcing tags. However, TBIR
is still suffering from the incomplete and inaccurate tags provided by users,
posing a great challenge for tag-based image management applications. In this
work, we proposed a novel method for image annotation, incorporating several
priors: Low-Rankness, Tag and Visual Correlation and Inhomogeneous Errors.
Highly representative CNN feature vectors are adopt to model the tag-visual
correlation and narrow the semantic gap. And we extract word vectors for tags
to measure similarity between tags in the semantic level, which is more
accurate than traditional frequency-based or graph-based methods. We utilize
the accelerated proximal gradient (APG) method to solve our model efficiently.
Extensive experiments conducted on multiple benchmark datasets demonstrate the
effectiveness and robustness of the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07480</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07480</id><created>2015-08-29</created><authors><author><keyname>Tramullas</keyname><forenames>Jes&#xfa;s</forenames></author><author><keyname>S&#xe1;nchez-Casab&#xf3;n</keyname><forenames>Ana I.</forenames></author><author><keyname>Garrido-Picazo</keyname><forenames>Piedad</forenames></author></authors><title>Studies and analysis of reference management software: a literature
  review</title><categories>cs.DL</categories><comments>Preprint. Accepted to be published in El Profesional de la
  Informaci\'on, 2015. ISSN: 1386-6710</comments><doi>10.3145/epi.2015.sep.17</doi><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Reference management software is a well-known tool for scientific research
work. Since the 1980s, it has been the subject of reviews and evaluations in
library and information science literature. This paper presents a systematic
review of published studies that evaluate reference management software with a
comparative approach. The objective is to identify the types, models, and
evaluation criteria that authors have adopted, in order to determine whether
the methods used provide adequate methodological rigor and useful contributions
to the field of study.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07482</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07482</id><created>2015-08-29</created><authors><author><keyname>Garcia-Serrano</keyname><forenames>Alberto</forenames></author></authors><title>Anomaly Detection for malware identification using Hardware Performance
  Counters</title><categories>cs.CR</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Computers are widely used today by most people. Internet based applications,
like ecommerce or ebanking attracts criminals, who using sophisticated
techniques, tries to introduce malware on the victim computer. But not only
computer users are in risk, also smartphones or smartwatch users, smart cities,
Internet of Things devices, etc. Different techniques has been tested against
malware. Currently, pattern matching is the default approach in antivirus
software. Also, Machine Learning is successfully being used. Continuing this
trend, in this article we propose an anomaly based method using the hardware
performance counters (HPC) available in almost any modern computer
architecture. Because anomaly detection is an unsupervised process, new malware
and APTs can be detected even if they are unknown.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07503</identifier>
 <datestamp>2015-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07503</id><created>2015-08-29</created><updated>2015-12-01</updated><authors><author><keyname>Jiang</keyname><forenames>Zhi-Qiang</forenames><affiliation>ECUST</affiliation></author><author><keyname>Xie</keyname><forenames>Wen-Jie</forenames><affiliation>ECUST</affiliation></author><author><keyname>Li</keyname><forenames>Ming-Xia</forenames><affiliation>ECUST</affiliation></author><author><keyname>Zhou</keyname><forenames>Wei-Xing</forenames><affiliation>ECUST</affiliation></author><author><keyname>Sornette</keyname><forenames>Didier</forenames><affiliation>ETH Zurich</affiliation></author></authors><title>Two-state Markov-chain Poisson nature of individual cellphone call
  statistics</title><categories>physics.soc-ph cs.SI</categories><comments>16 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Humans are heterogenous and the behaviors of individuals could be different
from that at the population level. We conduct an in-depth study of the temporal
patterns of cellphone conversation activities of 73'339 anonymous cellphone
users with the same truncated Weibull distribution of inter-call durations. We
find that the individual call events exhibit a pattern of bursts, in which high
activity periods are alternated with low activity periods. Surprisingly, the
number of events in high activity periods are found to conform to a power-law
distribution at the population level, but follow an exponential distribution at
the individual level, which is a hallmark of absence of memory in individual
call activities. Such exponential distribution is also observed for the number
of events in low activity periods. Together with the exponential distributions
of inter-call durations within bursts and of the intervals between consecutive
bursts, we demonstrate that the individual call activities are driven by two
independent Poisson processes, which can be combined within a minimal model in
terms of a two-state first-order Markov chain giving very good agreement with
the empirical distributions using the parameters estimated from real data for
about half of the individuals in our sample. By measuring directly the
distributions of call rates across the population, which exhibit power-law
tails, we explain the difference with previous population level studies,
purporting the existence of power-law distributions, via the &quot;Superposition of
Distributions&quot; mechanism: The superposition of many exponential distributions
of activities with a power-law distribution of their characteristic scales
leads to a power-law distribution of the activities at the population level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07504</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07504</id><created>2015-08-29</created><authors><author><keyname>Cheriyan</keyname><forenames>Joe</forenames></author><author><keyname>Gao</keyname><forenames>Zhihan</forenames></author></authors><title>Approximating (Unweighted) Tree Augmentation via Lift-and-Project, Part
  I: Stemless TAP</title><categories>cs.DS</categories><comments>24 pages, 11 figures</comments><msc-class>68W25, 90C22, 90C27, 90C35, 05C85, 05C40,</msc-class><acm-class>F.2.2; G.1.6; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Part I, we study a special case of the unweighted Tree Augmentation
Problem (TAP) via the Lasserre (Sum of Squares) system. In the special case, we
forbid so-called stems; these are a particular type of subtree configuration.
For stemless TAP, we prove that the integrality ratio of an SDP relaxation (the
Lasserre tightening of an LP relaxation) is $\leq \frac{3}{2}+\epsilon$, where
$\epsilon&gt;0$ can be any small constant. We obtain this result by designing a
polynomial-time algorithm for stemless TAP that achieves an approximation
guarantee of ($\frac32+\epsilon$) relative to the SDP relaxation. The algorithm
is combinatorial and does not solve the SDP relaxation, but our analysis relies
on the SDP relaxation.
  We generalize the combinatorial analysis of integral solutions from the
previous literature to fractional solutions by identifying some properties of
fractional solutions of the Lasserre system via the decomposition result of
Karlin, Mathieu and Nguyen (IPCO 2011).
  Also, we present an example of stemless TAP such that the approximation
guarantee of $\frac32$ is tight for the algorithm.
  In Part II of this paper, we extend the methods of Part I to prove the same
results relative to the same SDP relaxation for TAP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07527</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07527</id><created>2015-08-29</created><authors><author><keyname>Mezlini</keyname><forenames>Aziz M.</forenames></author><author><keyname>Fuligni</keyname><forenames>Fabio</forenames></author><author><keyname>Shlien</keyname><forenames>Adam</forenames></author><author><keyname>Goldenberg</keyname><forenames>Anna</forenames></author></authors><title>Combining exome and gene expression datasets in one graphical model of
  disease to empower the discovery of disease mechanisms</title><categories>q-bio.GN cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Identifying genes associated with complex human diseases is one of the main
challenges of human genetics and computational medicine. To answer this
question, millions of genetic variants get screened to identify a few of
importance. To increase the power of identifying genes associated with diseases
and to account for other potential sources of protein function aberrations, we
propose a novel factor-graph based model, where much of the biological
knowledge is incorporated through factors and priors. Our extensive simulations
show that our method has superior sensitivity and precision compared to
variant-aggregating and differential expression methods. Our integrative
approach was able to identify important genes in breast cancer, identifying
genes that had coding aberrations in some patients and regulatory abnormalities
in others, emphasizing the importance of data integration to explain the
disease in a larger number of patients.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07532</identifier>
 <datestamp>2015-12-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07532</id><created>2015-08-30</created><updated>2015-12-09</updated><authors><author><keyname>Joglekar</keyname><forenames>Manas</forenames></author><author><keyname>Puttagunta</keyname><forenames>Rohan</forenames></author><author><keyname>R&#xe9;</keyname><forenames>Christopher</forenames></author></authors><title>Aggregations over Generalized Hypertree Decompositions</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a class of aggregate-join queries with multiple aggregation
operators evaluated over annotated relations. We show that straightforward
extensions of standard multiway join algorithms and generalized hypertree
decompositions (GHDs) provide best-known runtime guarantees. In contrast, prior
work uses bespoke algorithms and data structures and does not match these
guarantees. Our extensions to the standard techniques are a pair of simple
tests that (1) determine if two orderings of aggregation operators are
equivalent and (2) determine if a GHD is compatible with a given ordering.
These tests provide a means to find an optimal GHD that, when provided to
standard join algorithms, will correctly answer a given aggregate-join query.
The second class of our contributions is a pair of complete characterizations
of (1) the set of orderings equivalent to a given ordering and (2) the set of
GHDs compatible with some equivalent ordering. We show by example that previous
approaches are incomplete. The key technical consequence of our
characterizations is a decomposition of a compatible GHD into a set of
(smaller) {\em unconstrained} GHDs, i.e. into a set of GHDs of sub-queries
without aggregations. Since this decomposition is comprised of unconstrained
GHDs, we are able to connect to the wide literature on GHDs for join query
processing, thereby obtaining improved runtime bounds, MapReduce variants, and
an efficient method to find approximately optimal GHDs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07544</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07544</id><created>2015-08-30</created><authors><author><keyname>Nguyen</keyname><forenames>Dong</forenames></author><author><keyname>Do&#x11f;ru&#xf6;z</keyname><forenames>A. Seza</forenames></author><author><keyname>Ros&#xe9;</keyname><forenames>Carolyn P.</forenames></author><author><keyname>de Jong</keyname><forenames>Franciska</forenames></author></authors><title>Computational Sociolinguistics: A Survey</title><categories>cs.CL</categories><comments>52 pages. Submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Language is a social phenomenon and inherent to its social nature is that it
is constantly changing. Recently, a surge of interest can be observed within
the computational linguistics (CL) community in the social dimension of
language. In this article we present a survey of the emerging field of
&quot;Computational Sociolinguistics&quot; that reflects this increased interest. We aim
to provide a comprehensive overview of CL research on sociolinguistic themes,
featuring topics such as the relation between language and social identity,
language use in social interaction and multilingual communication. Moreover, we
demonstrate the potential for synergy between the research communities
involved, by showing how the large-scale data-driven methods that are widely
used in CL can complement existing sociolinguistic studies, and how
sociolinguistics can inform and challenge the methods and assumptions employed
in CL studies. We hope to convey the possible benefits of a closer
collaboration between the two communities and conclude with a discussion of
open challenges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07547</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07547</id><created>2015-08-30</created><authors><author><keyname>Chen</keyname><forenames>Yanping</forenames></author></authors><title>Protocol Programming: A Connection of the Digit World</title><categories>cs.PL cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The current computer programmings encapsulate attributes and behaviours into
objects, while without a mechanism to support connections between them. A
protocol programming paradigm is presented here, which builds an infrastructure
(a connecting thread network) to connect all instantiated objects. Then,
protocols are defined to support the communications between them. All
behaviours between them are managed by protocols, which enable the interactions
across heterogeneous programs, systems or the Internet. And mechanisms (e.g.,
concurrency, parallelism, distribution, pipeline, adaptability) are
autonomously, transparently or adaptively governed. Because one protocol
implementation can be shared by different applications, it ensures a
wide-ranging of code reuse. In this paper, an implementation is developed to
show methodologies of protocol programming. Open issues arose from protocol
programming are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07551</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07551</id><created>2015-08-30</created><authors><author><keyname>Karim</keyname><forenames>Awudu</forenames></author><author><keyname>Zhou</keyname><forenames>Shangbo</forenames></author></authors><title>X-TREPAN: a multi class regression and adapted extraction of
  comprehensible decision tree in artificial neural networks</title><categories>cs.LG cs.NE</categories><comments>17 Pages, 8 Tables, 8 Figures, 6 Equations</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, the TREPAN algorithm is enhanced and extended for extracting
decision trees from neural networks. We empirically evaluated the performance
of the algorithm on a set of databases from real world events. This benchmark
enhancement was achieved by adapting Single-test TREPAN and C4.5 decision tree
induction algorithms to analyze the datasets. The models are then compared with
X-TREPAN for comprehensibility and classification accuracy. Furthermore, we
validate the experimentations by applying statistical methods. Finally, the
modified algorithm is extended to work with multi-class regression problems and
the ability to comprehend generalized feed forward networks is achieved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07555</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07555</id><created>2015-08-30</created><authors><author><keyname>Chen</keyname><forenames>Yanping</forenames></author></authors><title>An Event Network for Exploring Open Information</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, an event network is presented for exploring open information,
where linguistic units about an event are organized for analysing. The process
is divided into three steps: document event detection, event network
construction and event network analysis. First, by implementing event detection
or tracking, documents are retrospectively (or on-line) organized into document
events. Secondly, for each of the document event, linguistic units are
extracted and combined into event networks. Thirdly, various analytic methods
are proposed for event network analysis. In our application methodologies are
presented for exploring open information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07557</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07557</id><created>2015-08-30</created><authors><author><keyname>Angelini</keyname><forenames>Patrizio</forenames></author><author><keyname>Da Lozzo</keyname><forenames>Giordano</forenames></author><author><keyname>Di Battista</keyname><forenames>Giuseppe</forenames></author><author><keyname>Frati</keyname><forenames>Fabrizio</forenames></author><author><keyname>Patrignani</keyname><forenames>Maurizio</forenames></author><author><keyname>Rutter</keyname><forenames>Ignaz</forenames></author></authors><title>Intersection-Link Representations of Graphs</title><categories>cs.DS cs.CG cs.DM</categories><comments>15 pages, 8 figures, extended version of 'Intersection-Link
  Representations of Graphs' (23rd International Symposium on Graph Drawing,
  2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider drawings of graphs that contain dense subgraphs. We introduce
intersection-link representations for such graphs, in which each vertex $u$ is
represented by a geometric object $R(u)$ and in which each edge $(u,v)$ is
represented by the intersection between $R(u)$ and $R(v)$ if it belongs to a
dense subgraph or by a curve connecting the boundaries of $R(u)$ and $R(v)$
otherwise. We study a notion of planarity, called Clique Planarity, for
intersection-link representations of graphs in which the dense subgraphs are
cliques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07563</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07563</id><created>2015-08-30</created><authors><author><keyname>Liu</keyname><forenames>Chin-Fu</forenames></author><author><keyname>Lu</keyname><forenames>Hsiao-feng</forenames></author><author><keyname>Chen</keyname><forenames>Po-ning</forenames></author></authors><title>Analysis and Practice of Uniquely Decodable One-to-One Code</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the so-called uniquely decodable one-to-one code
(UDOOC) that is formed by inserting a &quot;comma&quot; indicator, termed the unique word
(UW), between consecutive one-to-one codewords for separation. Along this
research direction, we first investigate several general combinatorial
properties of UDOOCs, in particular the enumeration of the number of UDOOC
codewords for any (finite) codeword length. Based on the obtained formula on
the number of length-n codewords for a given UW, the per-letter average
codeword length of UDOOC for the optimal compression of a given source
statistics can be computed. Several upper bounds on the average codeword length
of such UDOOCs are next established. The analysis on the bounds of average
codeword length then leads to two asymptotic bounds for sources having
infinitely many alphabets, one of which is achievable and hence tight for a
certain source statistics and UW, and the other of which proves the
achievability of source entropy rate of UDOOCs when both the block size of
source letters for UDOOC compression and UW length go to infinity. Efficient
encoding and decoding algorithms for UDOOCs are also given in this paper.
Numerical results show that the proposed UDOOCs can potentially result in
comparable compression rate to the Huffman code under similar decoding
complexity and yield a smaller average codeword length than that of the
Lempel-Ziv code, thereby confirming the practicability of UDOOCs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07569</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07569</id><created>2015-08-30</created><updated>2016-01-19</updated><authors><author><keyname>Choi</keyname><forenames>Pui Tung</forenames></author><author><keyname>Ho</keyname><forenames>Kin Tat</forenames></author><author><keyname>Lui</keyname><forenames>Lok Ming</forenames></author></authors><title>Spherical Conformal Parameterization of Genus-0 Point Clouds for Meshing</title><categories>cs.CG cs.CV cs.GR math.DG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Point cloud is the most fundamental representation of 3D geometric objects.
Analyzing and processing point cloud surfaces is important in computer graphics
and computer vision. However, most of the existing algorithms for surface
analysis require connectivity information. Therefore, it is desirable to
develop a mesh structure on point clouds. This task can be simplified with the
aid of a parameterization. In particular, conformal parameterizations are
advantageous in preserving the geometric information of the point cloud data.
In this paper, we extend a state-of-the-art spherical conformal
parameterization algorithm for genus-0 closed meshes to the case of point
clouds, using an improved approximation of the Laplace-Beltrami operator on
data points. Then, we propose an iterative scheme called the North-South
reiteration for achieving a spherical conformal parameterization. A balancing
scheme is introduced to enhance the distribution of the spherical
parameterization. High quality triangulations and quadrangulations can then be
built on the point clouds with the aid of the parameterizations. Also, the
meshes generated are guaranteed to be genus-0 closed meshes. Moreover, using
our proposed spherical conformal parameterization, multilevel representations
of point clouds can be easily constructed. Experimental results demonstrate the
effectiveness of our proposed framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07575</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07575</id><created>2015-08-30</created><updated>2015-12-11</updated><authors><author><keyname>Parker</keyname><forenames>Jason T.</forenames></author><author><keyname>Schniter</keyname><forenames>Philip</forenames></author></authors><title>Parametric Bilinear Generalized Approximate Message Passing</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a scheme to estimate the parameters $b_i$ and $c_j$ of the
bilinear form $z_m=\sum_{i,j} b_i z_m^{(i,j)} c_j$ from noisy measurements
$\{y_m\}_{m=1}^M$, where $y_m$ and $z_m$ are related through an arbitrary
likelihood function and $z_m^{(i,j)}$ are known. Our scheme is based on
generalized approximate message passing (G-AMP): it treats $b_i$ and $c_j$ as
random variables and $z_m^{(i,j)}$ as an i.i.d.\ Gaussian 3-way tensor in order
to derive a tractable simplification of the sum-product algorithm in the
large-system limit. It generalizes previous instances of bilinear G-AMP, such
as those that estimate matrices $\boldsymbol{B}$ and $\boldsymbol{C}$ from a
noisy measurement of $\boldsymbol{Z}=\boldsymbol{BC}$, allowing the application
of AMP methods to problems such as self-calibration, blind deconvolution, and
matrix compressive sensing. Numerical experiments confirm the accuracy and
computational efficiency of the proposed approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07577</identifier>
 <datestamp>2015-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07577</id><created>2015-08-30</created><updated>2015-12-02</updated><authors><author><keyname>Gopalakrishnan</keyname><forenames>Karthik</forenames></author><author><keyname>Pandey</keyname><forenames>Arun</forenames></author><author><keyname>Chandra</keyname><forenames>Joydeep</forenames></author></authors><title>Social Interaction in the Flickr Social Network</title><categories>cs.SI</categories><comments>COMSNETS 2016 (Social Networking Workshop) - 6 pages, 5 figures</comments><acm-class>C.4; H.3.5; J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online social networking sites such as Facebook, Twitter and Flickr are among
the most popular sites on the Web, providing platforms for sharing information
and interacting with a large number of people. The different ways for users to
interact, such as liking, retweeting and favoriting user-generated content, are
among the defining and extremely popular features of these sites. While
empirical studies have been done to learn about the network growth processes in
these sites, few studies have focused on social interaction behaviour and the
effect of social interaction on network growth.
  In this paper, we analyze large-scale data collected from the Flickr social
network to learn about individual favoriting behaviour and examine the
occurrence of link formation after a favorite is created. We do this using a
systematic formulation of Flickr as a two-layer temporal multiplex network: the
first layer describes the follow relationship between users and the second
layer describes the social interaction between users in the form of favorite
markings to photos uploaded by them. Our investigation reveals that (a)
favoriting is well-described by preferential attachment, (b) over 50% of
favorites are reciprocated within 10 days if at all they are reciprocated, (c)
different kinds of favorites differ in how fast they are reciprocated, and (d)
after a favorite is created, multiplex triangles are closed by the creation of
follow links by the favoriter's followers to the favorite receiver.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07582</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07582</id><created>2015-08-30</created><authors><author><keyname>Rook</keyname><forenames>Christopher J.</forenames></author><author><keyname>Kerman</keyname><forenames>Mitchell</forenames></author></authors><title>Approximating the Sum of Correlated Lognormals: An Implementation</title><categories>q-fin.GN cs.MS stat.AP</categories><comments>Fully documented source code is included</comments><acm-class>D.2.4; G.3; J.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lognormal random variables appear naturally in many engineering disciplines,
including wireless communications, reliability theory, and finance. So, too,
does the sum of (correlated) lognormal random variables. Unfortunately, no
closed form probability distribution exists for such a sum, and it requires
approximation. Some approximation methods date back over 80 years and most take
one of two approaches, either: 1) an approximate probability distribution is
derived mathematically, or 2) the sum is approximated by a single lognormal
random variable. In this research, we take the latter approach and review a
fairly recent approximation procedure proposed by Mehta, Wu, Molisch, and Zhang
(2007), then implement it using C++. The result is applied to a discrete time
model commonly encountered within the field of financial economics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07588</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07588</id><created>2015-08-30</created><authors><author><keyname>Kalamkar</keyname><forenames>Sanket S.</forenames></author><author><keyname>Majhi</keyname><forenames>Subhajit</forenames></author><author><keyname>Banerjee</keyname><forenames>Adrish</forenames></author></authors><title>Outage Analysis of Spectrum Sharing Energy Harvesting Cognitive Relays
  in Nakagami-$m$ Channels</title><categories>cs.IT cs.NI math.IT</categories><comments>To be presented at IEEE GLOBECOM 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy harvesting (EH) cognitive relays are an exciting solution to the
problem of inefficient use of spectrum while achieving green communications and
spatial diversity. In a spectrum sharing scenario, we investigate the
performance of a cognitive relay network, where a secondary source communicates
with its destination over Nakagami-$m$ channels via decode-and-forward EH
relays while maintaining the outage probability of the primary user below a
predefined threshold. Specifically, we derive a closed-form expression for the
secondary outage probability and show that it is a function of the probability
of an EH relay having sufficient energy for relaying, which in turn, depends on
the energy harvesting and consumption rates of the EH relay and the primary
outage probability threshold. We also show that relaxing the primary outage
constraint may not always benefit the cognitive EH relay network due to the
limitations imposed on the relay's transmit power by the energy constraint.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07590</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07590</id><created>2015-08-30</created><authors><author><keyname>Li</keyname><forenames>Kangquan</forenames></author><author><keyname>Qu</keyname><forenames>Longjiang</forenames></author><author><keyname>Chen</keyname><forenames>Xi</forenames></author></authors><title>New Classes of Permutation Binomials and Permutation Trinomials over
  Finite Fields</title><categories>cs.IT math.IT math.NT</categories><comments>18 pages. Submitted to a journal on Aug. 15th</comments><msc-class>11T06, 11T55, 05A05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Permutation polynomials over finite fields play important roles in finite
fields theory. They also have wide applications in many areas of science and
engineering such as coding theory, cryptography, combinatorial design,
communication theory and so on. Permutation binomials and trinomials attract
people's interest due to their simple algebraic form and additional
extraordinary properties. In this paper, several new classes of permutation
binomials and permutation trinomials are constructed. Some of these permutation
polynomials are generalizations of known ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07593</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07593</id><created>2015-08-30</created><authors><author><keyname>Ronfard</keyname><forenames>Remi</forenames></author><author><keyname>Gandhi</keyname><forenames>Vineet</forenames></author><author><keyname>Boiron</keyname><forenames>Laurent</forenames></author></authors><title>The Prose Storyboard Language: A Tool for Annotating and Directing
  Movies</title><categories>cs.GR</categories><comments>8 pages, presented at Workshop on Intelligent Cinematography and
  Editing (WICED'2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The prose storyboard language is a formal language for describing movies shot
by shot, where each shot is described with a unique sentence. The language uses
a simple syntax and limited vocabulary borrowed from working practices in
traditional movie-making, and is intended to be readable both by machines and
humans. The language is designed to serve as a high-level user interface for
intelligent cinematography and editing systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07603</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07603</id><created>2015-08-30</created><authors><author><keyname>Berry</keyname><forenames>Lindsay</forenames></author><author><keyname>Beveridge</keyname><forenames>Andrew</forenames></author><author><keyname>Butterfield</keyname><forenames>Jane</forenames></author><author><keyname>Isler</keyname><forenames>Volkan</forenames></author><author><keyname>Keller</keyname><forenames>Zachary</forenames></author><author><keyname>Shine</keyname><forenames>Alana</forenames></author><author><keyname>Wang</keyname><forenames>Junyi</forenames></author></authors><title>Line-of-Sight Pursuit in Strictly Sweepable Polygons</title><categories>cs.CG</categories><comments>36 pages, 21 figures</comments><msc-class>68U05, 91A24, 49N75</msc-class><acm-class>I.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a turn-based game in a simply connected polygonal environment $Q$
between a pursuer $P$ and an adversarial evader $E$. Both players can move in a
straight line to any point within unit distance during their turn. The pursuer
$P$ wins by capturing the evader, meaning that their distance satisfies $d(P,
E) \leq 1$, while the evader wins by eluding capture forever. Both players have
a map of the environment, but they have different sensing capabilities. The
evader $E$ always knows the location of $P$. Meanwhile, $P$ only has
line-of-sight visibility: $P$ observes the evader's position only when the line
segment connecting them lies entirely within the polygon. Therefore $P$ must
search for $E$ when the evader is hidden from view.
  We provide a winning strategy for $P$ in the family of strictly sweepable
polygons, meaning that a straight line $L$ can be moved continuously over $Q$
so that (1) $L \cap Q$ is always convex and (2) every point on the boundary
$\partial Q$ is swept exactly once. This strict sweeping requires that $L$
moves along $Q$ via a sequence of translations and rotations. We develop our
main result by first considering pursuit in the subfamilies of monotone
polygons (where $L$ moves by translation) and scallop polygons (where $L$ moves
by a single rotation). Our algorithm uses rook strategy during its active
pursuit phase, rather than the well-known lion strategy. The rook strategy is
crucial for obtaining a capture time that is linear in the area of $Q$. For
monotone and scallop polygons, our algorithm has a capture time of $O(n(Q) +
\mbox{area}(Q))$, where $n(Q)$ is the number of polygon vertices. The capture
time bound for strictly sweepable polygons is $O( n(Q) \cdot \mbox{area}(Q) )$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07630</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07630</id><created>2015-08-30</created><updated>2016-03-05</updated><authors><author><keyname>Aksakalli</keyname><forenames>Vural</forenames></author><author><keyname>Malekipirbazari</keyname><forenames>Milad</forenames></author></authors><title>Feature Selection via Binary Simultaneous Perturbation Stochastic
  Approximation</title><categories>stat.ML cs.LG</categories><comments>This is the Istanbul Sehir University Technical Report
  #SHR-ISE-2016.01. A short version of this report has been accepted for
  publication at Pattern Recognition Letters</comments><report-no>SHR-ISE-2016.01</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Feature selection (FS) has become an indispensable task in dealing with
today's highly complex pattern recognition problems with massive number of
features. In this study, we propose a new wrapper approach for FS based on
binary simultaneous perturbation stochastic approximation (BSPSA). This
pseudo-gradient descent stochastic algorithm starts with an initial feature
vector and moves toward the optimal feature vector via successive iterations.
In each iteration, the current feature vector's individual components are
perturbed simultaneously by random offsets from a qualified probability
distribution. We present computational experiments on datasets with numbers of
features ranging from a few dozens to thousands using three widely-used
classifiers as wrappers: nearest neighbor, decision tree, and linear support
vector machine. We compare our methodology against the full set of features as
well as a binary genetic algorithm and sequential FS methods using
cross-validated classification error rate and AUC as the performance criteria.
Our results indicate that features selected by BSPSA compare favorably to
alternative methods in general and BSPSA can yield superior feature sets for
datasets with tens of thousands of features by examining an extremely small
fraction of the solution space. We are not aware of any other wrapper FS
methods that are computationally feasible with good convergence properties for
such large datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07640</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07640</id><created>2015-08-30</created><authors><author><keyname>Eslahi</keyname><forenames>Nasser</forenames></author><author><keyname>Aghagolzadeh</keyname><forenames>Ali</forenames></author><author><keyname>Andargoli</keyname><forenames>Seyed Mehdi Hosseini</forenames></author></authors><title>Compressive Video Sensing via Dictionary Learning and Forward Prediction</title><categories>cs.MM</categories><comments>26 Pages, 5 Figures, 3 Tables, This paper was presented in part at
  the 7th International Symposium on Telecommunications. arXiv admin note: text
  overlap with arXiv:1404.7566 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a new framework for compressive video sensing (CVS)
that exploits the inherent spatial and temporal redundancies of a video
sequence, effectively. The proposed method splits the video sequence into the
key and non-key frames followed by dividing each frame into small
non-overlapping blocks of equal sizes. At the decoder side, the key frames are
reconstructed using adaptively learned sparsifying (ALS) basis via $\ell_0$
minimization, in order to exploit the spatial redundancy. Also, the
effectiveness of three well-known dictionary learning algorithms is
investigated in our method. For recovery of the non-key frames, a prediction of
the current frame is initialized, by using the previous reconstructed frame, in
order to exploit the temporal redundancy. The prediction is employed in a
proper optimization problem to recover the current non-key frame. To compare
our experimental results with the results of some other methods, we employ peak
signal to noise ratio (PSNR) and structural similarity (SSIM) index as the
quality assessor. The numerical results show the adequacy of our proposed
method in CVS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07643</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07643</id><created>2015-08-30</created><updated>2016-01-10</updated><authors><author><keyname>Goessling</keyname><forenames>Marc</forenames></author><author><keyname>Kang</keyname><forenames>Shan</forenames></author></authors><title>Directional Decision Lists</title><categories>stat.ML cs.LG stat.CO</categories><comments>IEEE Big Data for Advanced Manufacturing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce a novel family of decision lists consisting of
highly interpretable models which can be learned efficiently in a greedy
manner. The defining property is that all rules are oriented in the same
direction. Particular examples of this family are decision lists with
monotonically decreasing (or increasing) probabilities. On simulated data we
empirically confirm that the proposed model family is easier to train than
general decision lists. We exemplify the practical usability of our approach by
identifying problem symptoms in a manufacturing process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07647</identifier>
 <datestamp>2015-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07647</id><created>2015-08-30</created><updated>2015-09-21</updated><authors><author><keyname>Johnson</keyname><forenames>Justin</forenames></author><author><keyname>Ballan</keyname><forenames>Lamberto</forenames></author><author><keyname>Li</keyname><forenames>Fei-Fei</forenames></author></authors><title>Love Thy Neighbors: Image Annotation by Exploiting Image Metadata</title><categories>cs.CV</categories><comments>Accepted to ICCV 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Some images that are difficult to recognize on their own may become more
clear in the context of a neighborhood of related images with similar
social-network metadata. We build on this intuition to improve multilabel image
annotation. Our model uses image metadata nonparametrically to generate
neighborhoods of related images using Jaccard similarities, then uses a deep
neural network to blend visual information from the image and its neighbors.
Prior work typically models image metadata parametrically, in contrast, our
nonparametric treatment allows our model to perform well even when the
vocabulary of metadata changes between training and testing. We perform
comprehensive experiments on the NUS-WIDE dataset, where we show that our model
outperforms state-of-the-art methods for multilabel image annotation even when
our model is forced to generalize to new types of metadata.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07648</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07648</id><created>2015-08-30</created><authors><author><keyname>Zayyani</keyname><forenames>Hadi</forenames></author><author><keyname>Korki</keyname><forenames>Mehdi</forenames></author><author><keyname>Marvasti</keyname><forenames>Farrokh</forenames></author></authors><title>Dictionary Learning for Blind One Bit Compressed Sensing</title><categories>stat.ML cs.IT math.IT</categories><comments>5 pages, 3 figures</comments><doi>10.1109/LSP.2015.2503804</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This letter proposes a dictionary learning algorithm for blind one bit
compressed sensing. In the blind one bit compressed sensing framework, the
original signal to be reconstructed from one bit linear random measurements is
sparse in an unknown domain. In this context, the multiplication of measurement
matrix $\Ab$ and sparse domain matrix $\Phi$, \ie $\Db=\Ab\Phi$, should be
learned. Hence, we use dictionary learning to train this matrix. Towards that
end, an appropriate continuous convex cost function is suggested for one bit
compressed sensing and a simple steepest-descent method is exploited to learn
the rows of the matrix $\Db$. Experimental results show the effectiveness of
the proposed algorithm against the case of no dictionary learning, specially
with increasing the number of training signals and the number of sign
measurements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07654</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07654</id><created>2015-08-30</created><authors><author><keyname>Lan</keyname><forenames>Tian</forenames></author><author><keyname>Zhu</keyname><forenames>Yuke</forenames></author><author><keyname>Zamir</keyname><forenames>Amir Roshan</forenames></author><author><keyname>Savarese</keyname><forenames>Silvio</forenames></author></authors><title>Action Recognition by Hierarchical Mid-level Action Elements</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Realistic videos of human actions exhibit rich spatiotemporal structures at
multiple levels of granularity: an action can always be decomposed into
multiple finer-grained elements in both space and time. To capture this
intuition, we propose to represent videos by a hierarchy of mid-level action
elements (MAEs), where each MAE corresponds to an action-related spatiotemporal
segment in the video. We introduce an unsupervised method to generate this
representation from videos. Our method is capable of distinguishing
action-related segments from background segments and representing actions at
multiple spatiotemporal resolutions. Given a set of spatiotemporal segments
generated from the training data, we introduce a discriminative clustering
algorithm that automatically discovers MAEs at multiple levels of granularity.
We develop structured models that capture a rich set of spatial, temporal and
hierarchical relations among the segments, where the action label and multiple
levels of MAE labels are jointly inferred. The proposed model achieves
state-of-the-art performance in multiple action recognition benchmarks.
Moreover, we demonstrate the effectiveness of our model in real-world
applications such as action recognition in large-scale untrimmed videos and
action parsing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07677</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07677</id><created>2015-08-31</created><authors><author><keyname>Rossman</keyname><forenames>Benjamin</forenames></author></authors><title>The Average Sensitivity of Bounded-Depth Formulas</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that unbounded fan-in boolean formulas of depth $d+1$ and size $s$
have average sensitivity $O(\frac{1}{d}\log s)^d$. In particular, this gives a
tight $2^{\Omega(d(n^{1/d}-1))}$ lower bound on the size of depth $d+1$
formulas computing the \textsc{parity} function. These results strengthen the
corresponding $2^{\Omega(n^{1/d})}$ and $O(\log s)^d$ bounds for circuits due
to H{\aa}stad (1986) and Boppana (1997). Our proof technique studies a random
process where the Switching Lemma is applied to formulas in an efficient
manner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07678</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07678</id><created>2015-08-31</created><authors><author><keyname>Shariat</keyname><forenames>Shahriar</forenames></author><author><keyname>Orten</keyname><forenames>Burkay</forenames></author><author><keyname>Dasdan</keyname><forenames>Ali</forenames></author></authors><title>Online Model Evaluation in a Large-Scale Computational Advertising
  Platform</title><categories>cs.AI stat.ME stat.ML</categories><comments>Accepted to ICDM2015</comments><journal-ref>ICDM (2015) pp. 369 - 378</journal-ref><doi>10.1109/ICDM.2015.32</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online media provides opportunities for marketers through which they can
deliver effective brand messages to a wide range of audiences. Advertising
technology platforms enable advertisers to reach their target audience by
delivering ad impressions to online users in real time. In order to identify
the best marketing message for a user and to purchase impressions at the right
price, we rely heavily on bid prediction and optimization models. Even though
the bid prediction models are well studied in the literature, the equally
important subject of model evaluation is usually overlooked. Effective and
reliable evaluation of an online bidding model is crucial for making faster
model improvements as well as for utilizing the marketing budgets more
efficiently. In this paper, we present an experimentation framework for bid
prediction models where our focus is on the practical aspects of model
evaluation. Specifically, we outline the unique challenges we encounter in our
platform due to a variety of factors such as heterogeneous goal definitions,
varying budget requirements across different campaigns, high seasonality and
the auction-based environment for inventory purchasing. Then, we introduce
return on investment (ROI) as a unified model performance (i.e., success)
metric and explain its merits over more traditional metrics such as
click-through rate (CTR) or conversion rate (CVR). Most importantly, we discuss
commonly used evaluation and metric summarization approaches in detail and
propose a more accurate method for online evaluation of new experimental models
against the baseline. Our meta-analysis-based approach addresses various
shortcomings of other methods and yields statistically robust conclusions that
allow us to conclude experiments more quickly in a reliable manner. We
demonstrate the effectiveness of our evaluation strategy on real campaign data
through some experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07680</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07680</id><created>2015-08-31</created><authors><author><keyname>Ghifary</keyname><forenames>Muhammad</forenames></author><author><keyname>Kleijn</keyname><forenames>W. Bastiaan</forenames></author><author><keyname>Zhang</keyname><forenames>Mengjie</forenames></author><author><keyname>Balduzzi</keyname><forenames>David</forenames></author></authors><title>Domain Generalization for Object Recognition with Multi-task
  Autoencoders</title><categories>cs.CV</categories><comments>accepted in ICCV 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of domain generalization is to take knowledge acquired from a
number of related domains where training data is available, and to then
successfully apply it to previously unseen domains. We propose a new feature
learning algorithm, Multi-Task Autoencoder (MTAE), that provides good
generalization performance for cross-domain object recognition.
  Our algorithm extends the standard denoising autoencoder framework by
substituting artificially induced corruption with naturally occurring
inter-domain variability in the appearance of objects. Instead of
reconstructing images from noisy versions, MTAE learns to transform the
original image into analogs in multiple related domains. It thereby learns
features that are robust to variations across domains. The learnt features are
then used as inputs to a classifier.
  We evaluated the performance of the algorithm on benchmark image recognition
datasets, where the task is to learn features from multiple datasets and to
then predict the image label from unseen datasets. We found that (denoising)
MTAE outperforms alternative autoencoder-based models as well as the current
state-of-the-art algorithms for domain generalization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07690</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07690</id><created>2015-08-31</created><updated>2016-01-30</updated><authors><author><keyname>Schneider</keyname><forenames>Johannes</forenames></author></authors><title>A General Multi-Party Protocol for Minimal Communication and Local
  Computation</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A client wishes to outsource computation on confidential data to a network of
servers. He does not trust a server on its own, but believes that servers do
not collude. To solve this problem we introduce a new scheme called \emph{JOS}
for perfect security in the context of secure multiparty computation in the
semi-honest model that naturally requires at least three parties. It differs
from classical work such as Yao, GMW, BGW or GRR through an explicit
distinction of keys and encrypted values rather than having (equal) shares of a
secret. Furthermore, JOS makes use of the distributive and associative nature
of its encryption schemes and, at times, &quot;double&quot; encrypts values. Any Boolean
circuit $C$ in disjunctive normal form with $w$ variables per clause can be
evaluated in O($\log k$) rounds using messages of size O($2^{w-k}$) for an
arbitrary parameter $k \in [2,w]$ and O($|C|\cdot 2^{w-k}$) bit operations. We
allow for collusion of up to $n-2$ parties. On the theoretical side JOS
improves a large body of work in one or several metrics. On the practical side,
our local computation requirements improve on the run-time of GRR using
Shamir's secret sharing up to several orders of magnitude.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07698</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07698</id><created>2015-08-31</created><authors><author><keyname>El-Khamy</keyname><forenames>Mostafa</forenames></author><author><keyname>Lin</keyname><forenames>Hsien-Ping</forenames></author><author><keyname>Lee</keyname><forenames>Jungwon</forenames></author><author><keyname>Mahdavifar</keyname><forenames>Hessam</forenames></author><author><keyname>Kang</keyname><forenames>Inyup</forenames></author></authors><title>Rate-Compatible Polar Codes for Wireless Channels</title><categories>cs.IT math.IT</categories><comments>Accepted for publication at 2015 IEEE Global Communications
  Conference (Globecom)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A design of rate-compatible polar codes suitable for HARQ communications is
proposed in this paper. An important feature of the proposed design is that the
puncturing order is chosen with low complexity on a base code of short length,
which is then further polarized to the desired length. A practical
rate-matching system that has the flexibility to choose any desired rate
through puncturing or repetition while preserving the polarization is
suggested. The proposed rate-matching system is combined with channel
interleaving and a bit-mapping procedure that preserves the polarization of the
rate-compatible polar code family over bit-interleaved coded modulation
systems. Simulation results on AWGN and fast fading channels with different
modulation orders show the robustness of the proposed rate-compatible polar
code in both Chase combining and incremental redundancy HARQ communications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07700</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07700</id><created>2015-08-31</created><authors><author><keyname>Howard</keyname><forenames>David</forenames></author><author><keyname>Bull</keyname><forenames>Larry</forenames></author><author><keyname>Lanzi</keyname><forenames>Pier-Luca</forenames></author></authors><title>A Cognitive Architecture Based on a Learning Classifier System with
  Spiking Classifiers</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning Classifier Systems (LCS) are population-based reinforcement learners
that were originally designed to model various cognitive phenomena. This paper
presents an explicitly cognitive LCS by using spiking neural networks as
classifiers, providing each classifier with a measure of temporal dynamism. We
employ a constructivist model of growth of both neurons and synaptic
connections, which permits a Genetic Algorithm (GA) to automatically evolve
sufficiently-complex neural structures. The spiking classifiers are coupled
with a temporally-sensitive reinforcement learning algorithm, which allows the
system to perform temporal state decomposition by appropriately rewarding
&quot;macro-actions,&quot; created by chaining together multiple atomic actions. The
combination of temporal reinforcement learning and neural information
processing is shown to outperform benchmark neural classifier systems, and
successfully solve a robotic navigation task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07705</identifier>
 <datestamp>2015-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07705</id><created>2015-08-31</created><authors><author><keyname>Fang</keyname><forenames>Wenjie</forenames></author><author><keyname>Mantaci</keyname><forenames>Roberto</forenames></author></authors><title>A recursive structure of sand pile model and its applications</title><categories>math.CO cs.DM</categories><comments>17 pages, 6 figures. Accepted by Pure Mathematics and Applications,
  to appear</comments><journal-ref>Pure Mathematics and Applications 25 (2015) 63-78</journal-ref><doi>10.1515/puma-2015-0006</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Sand Pile Model (SPM) and its generalization, the Ice Pile Model (IPM),
originate from physics and have various applications in the description of the
evolution of granular systems. In this article, we deal with the enumeration
and the exhaustive generation of the accessible configuration of the system.
Our work is based on a new recursive decomposition theorem for SPM
configurations using the notion of staircase bases. Based on this theorem, we
provide a recursive formula for the enumeration of SPM(n) and a constant
amortized time (CAT) algorithm for the generation of all SPM(n) configurations.
The extension of the same approach to the Ice Pile Model is also discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07709</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07709</id><created>2015-08-31</created><updated>2016-02-05</updated><authors><author><keyname>&#x160;uster</keyname><forenames>Simon</forenames></author><author><keyname>van Noord</keyname><forenames>Gertjan</forenames></author><author><keyname>Titov</keyname><forenames>Ivan</forenames></author></authors><title>Word Representations, Tree Models and Syntactic Functions</title><categories>cs.CL cs.LG stat.ML</categories><comments>Add github code repository link. Fix equation 4.1</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Word representations induced from models with discrete latent variables
(e.g.\ HMMs) have been shown to be beneficial in many NLP applications. In this
work, we exploit labeled syntactic dependency trees and formalize the induction
problem as unsupervised learning of tree-structured hidden Markov models.
Syntactic functions are used as additional observed variables in the model,
influencing both transition and emission components. Such syntactic information
can potentially lead to capturing more fine-grain and functional distinctions
between words, which, in turn, may be desirable in many NLP applications. We
evaluate the word representations on two tasks -- named entity recognition and
semantic frame identification. We observe improvements from exploiting
syntactic function information in both cases, and the results rivaling those of
state-of-the-art representation learning methods. Additionally, we revisit the
relationship between sequential and unlabeled-tree models and find that the
advantage of the latter is not self-evident.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07723</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07723</id><created>2015-08-31</created><authors><author><keyname>Pham</keyname><forenames>Hung</forenames></author><author><keyname>Smolka</keyname><forenames>Scott A.</forenames></author><author><keyname>Stoller</keyname><forenames>Scott D.</forenames></author><author><keyname>Phan</keyname><forenames>Dung</forenames></author><author><keyname>Yang</keyname><forenames>Junxing</forenames></author></authors><title>A survey on unmanned aerial vehicle collision avoidance systems</title><categories>cs.SY cs.RO</categories><comments>This is only a draft</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Collision avoidance is a key factor in enabling the integration of unmanned
aerial vehicle into real life use, whether it is in military or civil
application. For a long time there have been a large number of works to address
this problem; therefore a comparative summary of them would be desirable. This
paper presents a survey on the major collision avoidance systems developed in
up to date publications. Each collision avoidance system contains two main
parts: sensing and detection, and collision avoidance. Based on their
characteristics each part is divided into different categories; and those
categories are explained, compared and discussed about advantages and
disadvantages in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07724</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07724</id><created>2015-08-31</created><authors><author><keyname>Giridharan</keyname><forenames>Anandi</forenames></author><author><keyname>Venkataram</keyname><forenames>Pallapa</forenames></author></authors><title>SDL based validation of a node monitoring protocol</title><categories>cs.NI cs.DC</categories><comments>16 pages, 24 figures, International Conference of Networks,
  Communications, Wireless and Mobile 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile ad hoc network is a wireless, self-configured, infrastructureless
network of mobile nodes. The nodes are highly mobile, which makes the
application running on them face network related problems like node failure,
link failure, network level disconnection, scarcity of resources, buffer
degradation, and intermittent disconnection etc. Node failure and Network fault
are need to be monitored continuously by supervising the network status. Node
monitoring protocol is crucial, so it is required to test the protocol
exhaustively to verify and validate the functionality and accuracy of the
designed protocol. This paper presents a validation model for Node Monitoring
Protocol using Specification and Description Llanguage (SDL) using both Static
Agent (SA) and Mobile Agent (MA). We have verified properties of the Node
Monitoring Protocol (NMP) based on the global states with no exits, deadlock
states or proper termination states using reachability graph. Message Sequence
Chart (MSC) gives an intuitive understanding of the described system behavior
with varying node density and complex behavior etc.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07727</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07727</id><created>2015-08-31</created><authors><author><keyname>Chen</keyname><forenames>Jian</forenames></author><author><keyname>Chen</keyname><forenames>Xiaoming</forenames></author><author><keyname>Gerstacker</keyname><forenames>Wolfgang</forenames></author></authors><title>Optimal Power Allocation for A Massive MIMO Relay Aided Secure
  Communication</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we address the problem of optimal power allocation at the
relay in two-hop secure communications under practical conditions. To guarantee
secure communication during the long-distance transmission, the massive MIMO
(M-MIMO) relaying techniques are explored to significantly enhance wireless
security. The focus of this paper is on the analysis and design of optimal
power assignment for a decode-and-forward (DF) M-MIMO relay, so as to maximize
the secrecy outage capacity and minimize the interception probability,
respectively. Our study reveals the condition for a nonnegative the secrecy
outage capacity, obtains closed-form expressions for optimal power, and
presents the asymptotic characteristics of secrecy performance. Finally,
simulation results validate the effectiveness of the proposed schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07733</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07733</id><created>2015-08-31</created><updated>2015-09-14</updated><authors><author><keyname>Brandstadt</keyname><forenames>Andreas</forenames></author><author><keyname>Mosca</keyname><forenames>Raffaele</forenames></author></authors><title>Weighted Efficient Domination for $P_6$-Free Graphs in Polynomial Time</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a finite undirected graph $G=(V,E)$, a vertex $v \in V$ {\em dominates}
itself and its neighbors in $G$. A vertex set $D \subseteq V$ is an {\em
efficient dominating set} ({\em e.d.} for short) of $G$ if every $v \in V$ is
dominated in $G$ by exactly one vertex of $D$. The {\em Efficient Domination}
(ED) problem, which asks for the existence of an e.d. in $G$, is known to be
NP-complete for $P_7$-free graphs but solvable in polynomial time for
$P_5$-free graphs. The $P_6$-free case was the last open question for the
complexity of ED on $F$-free graphs.
  Recently, Lokshtanov, Pilipczuk and van Leeuwen showed that weighted ED is
solvable in polynomial time for $P_6$-free graphs, based on their
sub-exponential algorithm for the Maximum Weight Independent Set problem for
$P_6$-free graphs. Independently, at the same time, Mosca found a polynomial
time algorithm for weighted ED on $P_6$-free graphs using a direct approach. In
this paper, we describe the details of this approach which is simpler and much
faster, namely its time bound is ${\cal O}(n^6 m)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07738</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07738</id><created>2015-08-31</created><authors><author><keyname>Miridakis</keyname><forenames>Nikolaos I.</forenames></author></authors><title>On the Ergodic Capacity of Underlay Cognitive Dual-Hop AF Relayed
  Systems under Non-Identical Generalized-K Fading Channels</title><categories>cs.IT math.IT</categories><comments>11 pages, 2 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ergodic capacity of underlay cognitive (secondary) dual-hop relaying
systems is analytically investigated. Specifically, the amplify-and-forward
transmission protocol is considered, while the received signals undergo
multipath fading and shadowing with non-identical statistics. To efficiently
describe this composite type of fading, the well-known generalized-$K$ fading
model is used. New analytical expressions and quite accurate closed-form
approximations regarding the ergodic capacity of the end-to-end communication
are obtained, in terms of finite sum series of the Meijer's-$G$ function. The
analytical results are verified with the aid of computer simulations, while
useful insights are revealed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07739</identifier>
 <datestamp>2015-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07739</id><created>2015-08-31</created><updated>2015-09-03</updated><authors><author><keyname>Ryan</keyname><forenames>David</forenames></author></authors><title>A new Computer Friendly Notation for Musical Composition in Just
  Intonation allowing Easy Transposition by Multiplication of Notations</title><categories>cs.SD</categories><comments>This fourth draft has the complete text with all appendices. It is
  now awaiting peer review. Journals have not yet been selected (please contact
  author if you are interested in publishing)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Musical notation systems provide ways of recording which notes musicians
should play at which times. One essential parameter described is the frequency.
For twelve note tuning systems the frequency can be described using letters A
to G with sharp or flat symbols. For Just Intonation tuning systems these
symbols are insufficient. This paper provides a system for describing any
frequency which is a rational number multiplied by a suitable base frequency.
Explicit notation is given for low prime numbers, and an algorithm for higher
primes described.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07740</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07740</id><created>2015-08-31</created><authors><author><keyname>De Vogeleer</keyname><forenames>Karel</forenames></author><author><keyname>Memmi</keyname><forenames>Gerard</forenames></author><author><keyname>Jouvelot</keyname><forenames>Pierre</forenames></author></authors><title>Parameter Sensitivity Analysis of the Energy/Frequency Convexity Rule
  for Nanometer-scale Application Processors</title><categories>cs.DC cs.PF</categories><comments>In submission to the Special Issue on Energy Efficient Multi-Core and
  Many-Core Systems (The Elsevier Journal of Parallel and Distributed
  Computing)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Both theoretical and experimental evidence are presented in this work in
order to validate the existence of an Energy/Frequency Convexity Rule, which
relates energy consumption and microprocessor frequency for nanometer-scale
microprocessors. Data gathered during several month-long experimental
acquisition campaigns, supported by several independent publications, suggest
that energy consumed is indeed depending on the microprocessor's clock
frequency, and, more interestingly, the curve exhibits a clear minimum over the
processor's frequency range. An analytical model for this behavior is presented
and motivated, which fits well with the experimental data. A parameter
sensitivity analysis shows how parameters affect the energy minimum in the
clock frequency space. The conditions are discussed under which this convexity
rule can be exploited, and when other methods are more effective, with the aim
of improving the computer system's energy management efficiency. We show that
the power requirements of the computer system, besides the microprocessor, and
the overhead affect the location of the energy minimum the most. The
sensitivity analysis of the Energy/Frequency Convexity Rule puts forward a
number of simple guidelines especially for by low-power systems, such as
battery-powered and embedded systems, and less likely by high-performance
computer systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07741</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07741</id><created>2015-08-31</created><authors><author><keyname>Bajer</keyname><forenames>Lukas</forenames></author><author><keyname>Holena</keyname><forenames>Martin</forenames></author></authors><title>Model Guided Sampling Optimization for Low-dimensional Problems</title><categories>cs.NE stat.ML</categories><journal-ref>Bajer, L. &amp; Holena, M. Model Guided Sampling Optimization for
  Low-dimensional Problems. in ICAART 2015 Proceedings of the International
  Conference on Agents and Artificial Intelligence, Volume 2 451-456
  (SCITEPRESS, Lisbon, 2015)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optimization of very expensive black-box functions requires utilization of
maximum information gathered by the process of optimization. Model Guided
Sampling Optimization (MGSO) forms a more robust alternative to Jones'
Gaussian-process-based EGO algorithm. Instead of EGO's maximizing expected
improvement, the MGSO uses sampling the probability of improvement which is
shown to be helpful against trapping in local minima. Further, the MGSO can
reach close-to-optimum solutions faster than standard optimization algorithms
on low dimensional or smooth problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07742</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07742</id><created>2015-08-31</created><authors><author><keyname>Pourmoghadas</keyname><forenames>A.</forenames></author><author><keyname>Poonacha</keyname><forenames>P. G.</forenames></author></authors><title>Performance Analysis of Split Preamble RAN Over-load Protocol for M2M
  Communications in Cellular Networks</title><categories>cs.IT cs.NI math.IT</categories><comments>10 pages, 11 figures, journal paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Machine type communications (MTC) in 3G/4G networks is getting more attention
recently due to bursty nature of traffic characteristics in contrast to Poisson
type H2H traffic. A large number of methods have been suggested in the
literature. In this paper we give a mathematical model on performance analysis
of Disjoint Allocation (DA) and Joint Allocation (JA) methods for allocating
preambles to M2M and H2H users. In an earlier work we had investigated the
performance of two possible splitting preamble methods on collision probability
and energy reduction for MTC subscribers. In this paper we develop a stochastic
model for JA/DA method in RACH procedure using a $K$th order Markov chain
approach and carry out performance analysis in terms of collision probability,
access success probability, average access delay, statistics of preamble
transmissions and statistics of access delay for M2M users. The optimal number
of reserved preamble set is derived based on the given success access delay
threshold. Numerical results verifies the simulation results and demonstrates
that this model can estimate the performance of the JA/DA method accurately.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07744</identifier>
 <datestamp>2015-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07744</id><created>2015-08-31</created><authors><author><keyname>Louppe</keyname><forenames>Gilles</forenames></author><author><keyname>Al-Natsheh</keyname><forenames>Hussein</forenames></author><author><keyname>Susik</keyname><forenames>Mateusz</forenames></author><author><keyname>Maguire</keyname><forenames>Eamonn</forenames></author></authors><title>Ethnicity sensitive author disambiguation using semi-supervised learning</title><categories>cs.DL cs.IR stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Author name disambiguation in bibliographic databases is the problem of
grouping together scientific publications written by the same person,
accounting for potential homonyms and/or synonyms. Among solutions to this
problem, digital libraries are increasingly offering tools for authors to
manually curate their publications and claim those that are theirs. Indirectly,
these tools allow for the inexpensive collection of large annotated training
data, which can be further leveraged to build a complementary automated
disambiguation system capable of inferring patterns for identifying
publications written by the same person. Building on more than 1 million
publicly released crowdsourced annotations, we propose an automated author
disambiguation solution exploiting this data (i) to learn an accurate
classifier for identifying coreferring authors and (ii) to guide the clustering
of scientific publications by distinct authors in a semi-supervised way. To the
best of our knowledge, our analysis is the first to be carried out on data of
this size and coverage. With respect to the state of the art, we validate the
general pipeline used in most existing solutions, and improve by: (i) proposing
phonetic-based blocking strategies, thereby increasing recall; and (ii) adding
strong ethnicity-sensitive features for learning a linkage function, thereby
tailoring disambiguation to non-Western author names whenever necessary.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07749</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07749</id><created>2015-08-31</created><authors><author><keyname>Antcheva</keyname><forenames>Ilka</forenames></author><author><keyname>Ballintijn</keyname><forenames>Maarten</forenames></author><author><keyname>Bellenot</keyname><forenames>Bertrand</forenames></author><author><keyname>Biskup</keyname><forenames>Marek</forenames></author><author><keyname>Brun</keyname><forenames>Rene</forenames></author><author><keyname>Buncic</keyname><forenames>Nenad</forenames></author><author><keyname>Canal</keyname><forenames>Philippe</forenames></author><author><keyname>Casadei</keyname><forenames>Diego</forenames></author><author><keyname>Couet</keyname><forenames>Olivier</forenames></author><author><keyname>Fine</keyname><forenames>Valery</forenames></author><author><keyname>Franco</keyname><forenames>Leandro</forenames></author><author><keyname>Ganis</keyname><forenames>Gerardo</forenames></author><author><keyname>Gheata</keyname><forenames>Andrei</forenames></author><author><keyname>Maline</keyname><forenames>David Gonzalez</forenames></author><author><keyname>Goto</keyname><forenames>Masaharu</forenames></author><author><keyname>Iwaszkiewicz</keyname><forenames>Jan</forenames></author><author><keyname>Kreshuk</keyname><forenames>Anna</forenames></author><author><keyname>Segura</keyname><forenames>Diego Marcos</forenames></author><author><keyname>Maunder</keyname><forenames>Richard</forenames></author><author><keyname>Moneta</keyname><forenames>Lorenzo</forenames></author><author><keyname>Naumann</keyname><forenames>Axel</forenames></author><author><keyname>Offermann</keyname><forenames>Eddy</forenames></author><author><keyname>Onuchin</keyname><forenames>Valeriy</forenames></author><author><keyname>Panacek</keyname><forenames>Suzanne</forenames></author><author><keyname>Rademakers</keyname><forenames>Fons</forenames></author><author><keyname>Russo</keyname><forenames>Paul</forenames></author><author><keyname>Tadel</keyname><forenames>Matevz</forenames></author></authors><title>ROOT - A C++ Framework for Petabyte Data Storage, Statistical Analysis
  and Visualization</title><categories>physics.data-an cs.DC</categories><journal-ref>Computer Physics Communications Volume 180, Issue 12, December
  2009, Pages 2499-2512</journal-ref><doi>10.1016/j.cpc.2009.08.005</doi><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  ROOT is an object-oriented C++ framework conceived in the high-energy physics
(HEP) community, designed for storing and analyzing petabytes of data in an
efficient way. Any instance of a C++ class can be stored into a ROOT file in a
machine-independent compressed binary format. In ROOT the TTree object
container is optimized for statistical data analysis over very large data sets
by using vertical data storage techniques. These containers can span a large
number of files on local disks, the web, or a number of different shared file
systems. In order to analyze this data, the user can chose out of a wide set of
mathematical and statistical functions, including linear algebra classes,
numerical algorithms such as integration and minimization, and various methods
for performing regression analysis (fitting). In particular, ROOT offers
packages for complex data modeling and fitting, as well as multivariate
classification based on machine learning techniques. A central piece in these
analysis tools are the histogram classes which provide binning of one- and
multi-dimensional data. Results can be saved in high-quality graphical formats
like Postscript and PDF or in bitmap formats like JPG or GIF. The result can
also be stored into ROOT macros that allow a full recreation and rework of the
graphics. Users typically create their analysis macros step by step, making use
of the interactive C++ interpreter CINT, while running over small data samples.
Once the development is finished, they can run these macros at full compiled
speed over large data sets, using on-the-fly compilation, or by creating a
stand-alone batch program. Finally, if processing farms are available, the user
can reduce the execution time of intrinsically parallel tasks - e.g. data
mining in HEP - by using PROOF, which will take care of optimally distributing
the work over the available resources in a transparent way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07753</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07753</id><created>2015-08-31</created><authors><author><keyname>Parviainen</keyname><forenames>Pekka</forenames></author><author><keyname>Kaski</keyname><forenames>Samuel</forenames></author></authors><title>Bayesian Networks for Variable Groups</title><categories>stat.ML cs.AI</categories><comments>29 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bayesian networks, and especially their structures, are powerful tools for
representing conditional independencies and dependencies between random
variables. In applications where related variables form a priori known groups,
chosen to represent different &quot;views&quot; to or aspects of the same entities, one
may be more interested in modeling dependencies between groups of variables
rather than between individual variables. Motivated by this, we study prospects
of representing relationships between variable groups using Bayesian network
structures. We show that for dependency structures between groups to be
learnable, the data have to satisfy the so-called groupwise faithfulness
assumption. We also show that one cannot learn causal relations between groups
using only groupwise conditional independencies, but also variable-wise
relations are needed. Additionally, we present algorithms for finding the
groupwise dependency structures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07755</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07755</id><created>2015-08-31</created><authors><author><keyname>Ivanyos</keyname><forenames>G&#xe1;bor</forenames></author><author><keyname>Kutas</keyname><forenames>P&#xe9;ter</forenames></author><author><keyname>R&#xf3;nyai</keyname><forenames>Lajos</forenames></author></authors><title>Computing explicit isomorphisms with full matrix algebras over
  $\mathbb{F}_q(x)$</title><categories>math.RA cs.SC math.NT</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a polynomial time $f$-algorithm (a deterministic algorithm which
uses an oracle for factoring univariate polynomials over $\mathbb{F}_q$) for
computing an isomorphism (if there is any) of a finite dimensional
$\mathbb{F}_q(x)$-algebra $A$ given by structure constants with the algebra of
$n$ by $n$ matrices with entries from $\mathbb{F}_q(x)$. The method is based on
computing a finite $\mathbb{F}_q$-subalgebra of $A$ which is the intersection
of a maximal $\mathbb{F}_q[x]$-order and a maximal $R$-order, where $R$ is the
subring of $\mathbb{F}_q(x)$ consisting of fractions of polynomials with
denominator having degree not less than that of the numerator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07756</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07756</id><created>2015-08-31</created><authors><author><keyname>Bouftass</keyname><forenames>Samir</forenames></author></authors><title>On a new fast public key cryptosystem</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new fast public key cryptosystem namely : a key
exchange algorithm, a public key encryption algorithm and a digital signature
algorithm, based on the difficulty to invert the following function :
  $F(x) =(a\times x)Mod(2^p)Div(2^q)$ .\\* Mod is modulo operation , Div is
integer division operation , a , p and q are integers where $( p &gt; q )$ .\\* In
this paper we also evaluate the hardness of this problem by reducing it to SAT .
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07771</identifier>
 <datestamp>2015-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07771</id><created>2015-08-31</created><updated>2015-09-02</updated><authors><author><keyname>Adamczyk</keyname><forenames>Marek</forenames></author></authors><title>Non-negative submodular stochastic probing via stochastic contention
  resolution schemes</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The abstract model of stochastic probing was presented by Gupta and Nagarajan
(IPCO'13), and provides a unified view of a number of problems. Adamczyk,
Sviridenko, Ward (STACS'14) gave better approximation for matroid environments
and linear objectives. At the same time this method was easily extendable to
settings, where the objective function was monotone submodular. However, the
case of non-negative submodular function could not be handled by previous
techniques. In this paper we address this problem, and our results are twofold.
First, we adapt the notion of contention resolution schemes of Chekuri,
Vondr\'ak, Zenklusen (SICOMP'14) to show that we can optimize non-negative
submodular functions in this setting with a constant factor loss with respect
to the deterministic setting. Second, we show a new contention resolution
scheme for transversal matroids, which yields better approximations in the
stochastic probing setting than the previously known tools. The rounding
procedure underlying the scheme can be of independent interest --- Bansal,
Gupta, Li, Mestre, Nagarajan, Rudra (Algorithmica'12) gave two seemingly
different algorithms for stochastic matching and stochastic $k$-set packing
problems with two different analyses, but we show that our single technique can
be used to analyze both their algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07820</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07820</id><created>2015-08-31</created><updated>2015-10-30</updated><authors><author><keyname>M&#xe4;kinen</keyname><forenames>Veli</forenames></author><author><keyname>Staneva</keyname><forenames>Valeria</forenames></author><author><keyname>Tomescu</keyname><forenames>Alexandru</forenames></author><author><keyname>Valenzuela</keyname><forenames>Daniel</forenames></author></authors><title>Interval scheduling maximizing minimum coverage</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the classical interval scheduling type of problems, a set of $n$ jobs,
characterized by their start and end time, need to be executed by a set of
machines, under various constraints. In this paper we study a new variant in
which the jobs need to be assigned to at most $k$ identical machines, such that
the minimum number of machines that are busy at the same time is maximized.
This is relevant in the context of genome sequencing and haplotyping,
specifically when a set of DNA reads aligned to a genome needs to be pruned so
that no more than $k$ reads overlap, while maintaining as much read coverage as
possible across the entire genome. We show that the problem can be solved in
time $\min\left(O(n^2\log k / \log n),O(nk\log k)\right)$ by using max-flows.
We also give an $O(n\log n)$-time approximation algorithm with approximation
ratio $\rho =\frac{k}{\lfloor k/2 \rfloor}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07828</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07828</id><created>2015-08-31</created><authors><author><keyname>Mizera</keyname><forenames>Andrzej</forenames></author><author><keyname>Pang</keyname><forenames>Jun</forenames></author><author><keyname>Yuan</keyname><forenames>Qixia</forenames></author></authors><title>Parallel Approximate Steady-state Analysis of Large Probabilistic
  Boolean Networks (Technical Report)</title><categories>cs.DC q-bio.QM</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Probabilistic Boolean networks (PBNs) is a widely used computational
framework for modelling biological systems. The steady-state dynamics of PBNs
is of special interest in the analysis of biological systems. However,
obtaining the steady-state distributions for such systems poses a significant
challenge due to the state space explosion problem which often arises in the
case of large PBNs. The only viable way is to use statistical methods. We have
considered the two-state Markov chain approach and the Skart method for the
analysis of large PBNs in our previous work. However, the sample size required
in both methods is often huge in the case of large PBNs and generating them is
expensive in terms of computation time. Parallelising the sample generation is
an ideal way to solve this issue. In this paper, we consider combining the
German &amp; Rubin method with either the two-state Markov chain approach or the
Skart method for parallelisation. The first method can be used to run multiple
independent Markov chains in parallel and to control their convergence to the
steady-state while the other two methods can be used to determine the sample
size required for computing the steady-state probability of states of interest.
Experimental results show that our proposed combinations can reduce time cost
of computing stead-state probabilities of large PBNs significantly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07829</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07829</id><created>2015-08-31</created><authors><author><keyname>David</keyname><forenames>Cristina</forenames></author><author><keyname>Kroening</keyname><forenames>Daniel</forenames></author><author><keyname>Lewis</keyname><forenames>Matt</forenames></author></authors><title>Using Program Synthesis for Program Analysis</title><categories>cs.LO cs.PL</categories><comments>19 pages, to appear in LPAR 2015. arXiv admin note: text overlap with
  arXiv:1409.4925</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we identify a fragment of second-order logic with restricted
quantification that is expressive enough to capture numerous static analysis
problems (e.g. safety proving, bug finding, termination and non-termination
proving, superoptimisation). We call this fragment the {\it synthesis
fragment}. Satisfiability of a formula in the synthesis fragment is decidable
over finite domains; specifically the decision problem is NEXPTIME-complete. If
a formula in this fragment is satisfiable, a solution consists of a satisfying
assignment from the second order variables to \emph{functions over finite
domains}. To concretely find these solutions, we synthesise \emph{programs}
that compute the functions. Our program synthesis algorithm is complete for
finite state programs, i.e. every \emph{function} over finite domains is
computed by some \emph{program} that we can synthesise. We can therefore use
our synthesiser as a decision procedure for the synthesis fragment of
second-order logic, which in turn allows us to use it as a powerful backend for
many program analysis tasks. To show the tractability of our approach, we
evaluate the program synthesiser on several static analysis problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07845</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07845</id><created>2015-08-31</created><updated>2016-02-20</updated><authors><author><keyname>Peng</keyname><forenames>Peng</forenames></author><author><keyname>Zou</keyname><forenames>Lei</forenames></author><author><keyname>Chen</keyname><forenames>Lei</forenames></author><author><keyname>Zhao</keyname><forenames>Dongyan</forenames></author></authors><title>Query Workload-based RDF Graph Fragmentation and Allocation</title><categories>cs.DC cs.DB</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As the volume of the RDF data becomes increasingly large, it is essential for
us to design a distributed database system to manage it. For distributed RDF
data design, it is quite common to partition the RDF data into some parts,
called fragments, which are then distributed. Thus, the distribution design
consists of two steps: fragmentation and allocation. In this paper, we propose
a method to explore the intrinsic similarities among the structures of queries
in a workload for fragmentation and allocation, which aims to reduce the number
of crossing matches and the communication cost during SPARQL query processing.
Specifically, we mine and select some frequent access patterns to reflect the
characteristics of the workload. Here, although we prove that selecting the
optimal set of frequent access patterns is NP-hard, we propose a heuristic
algorithm which guarantees both the data integrity and the approximation ratio.
Based on the selected frequent access patterns, we propose two fragmentation
strategies, vertical and horizontal fragmentation strategies, to divide RDF
graphs while meeting different kinds of query processing objectives. Vertical
fragmentation is for better throughput and horizontal fragmentation is for
better performance. After fragmentation, we discuss how to allocate these
fragments to various sites. Finally, we discuss how to process a query based on
the results of fragmentation and allocation. Extensive experiments confirm the
superior performance of our proposed solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07851</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07851</id><created>2015-08-31</created><authors><author><keyname>Krupski</keyname><forenames>Vladimir N.</forenames></author><author><keyname>Yatmanov</keyname><forenames>Alexey</forenames></author></authors><title>Sequent Calculus for Intuitionistic Epistemic Logic</title><categories>cs.LO</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The formal system of intuitionistic epistemic logic IEL was proposed by S.
Artemov and T. Protopopescu. It provides the formal foundation for the study of
knowledge from an intuitionistic point of view based on
Brouwer-Hayting-Kolmogorov semantics of intuitionism. We construct a cut-free
sequent calculus for IEL and establish that polynomial space is sufficient for
the proof search in it. So, we prove that IEL is PSPACE-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07859</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07859</id><created>2015-08-31</created><authors><author><keyname>Je</keyname><forenames>Changsoo</forenames></author><author><keyname>Lee</keyname><forenames>Kwang Hee</forenames></author><author><keyname>Lee</keyname><forenames>Sang Wook</forenames></author></authors><title>Multi-Projector Color Structured-Light Vision</title><categories>cs.CV cs.GR physics.optics</categories><comments>25 pages, 13 figures</comments><acm-class>I.2.10; I.4.8</acm-class><journal-ref>Signal Processing: Image Communication, Volume 28, Issue 9, pp.
  1046-1058, October, 2013</journal-ref><doi>10.1016/j.image.2013.05.005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Research interest in rapid structured-light imaging has grown increasingly
for the modeling of moving objects, and a number of methods have been suggested
for the range capture in a single video frame. The imaging area of a 3D object
using a single projector is restricted since the structured light is projected
only onto a limited area of the object surface. Employing additional projectors
to broaden the imaging area is a challenging problem since simultaneous
projection of multiple patterns results in their superposition in the
light-intersected areas and the recognition of original patterns is by no means
trivial. This paper presents a novel method of multi-projector color
structured-light vision based on projector-camera triangulation. By analyzing
the behavior of superposed-light colors in a chromaticity domain, we show that
the original light colors cannot be properly extracted by the conventional
direct estimation. We disambiguate multiple projectors by multiplexing the
orientations of projector patterns so that the superposed patterns can be
separated by explicit derivative computations. Experimental studies are carried
out to demonstrate the validity of the presented method. The proposed method
increases the efficiency of range acquisition compared to conventional active
stereo using multiple projectors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07876</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07876</id><created>2015-08-31</created><authors><author><keyname>Hristova</keyname><forenames>Desislava</forenames></author><author><keyname>Noulas</keyname><forenames>Anastasios</forenames></author><author><keyname>Brown</keyname><forenames>Chlo&#xeb;</forenames></author><author><keyname>Musolesi</keyname><forenames>Mirco</forenames></author><author><keyname>Mascolo</keyname><forenames>Cecilia</forenames></author></authors><title>A Multilayer Approach to Multiplexity and Link Prediction in Online
  Geo-Social Networks</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online social systems are multiplex in nature as multiple links may exist
between the same two users across different social networks. In this work, we
introduce a framework for studying links and interactions between users beyond
the individual social network. Exploring the cross-section of two popular
online platforms - Twitter and location-based social network Foursquare - we
represent the two together as a composite multilayer online social network.
Through this paradigm we study the interactions of pairs of users
differentiating between those with links on one or both networks. We find that
users with multiplex links, who are connected on both networks, interact more
and have greater neighbourhood overlap on both platforms, in comparison with
pairs who are connected on just one of the social networks. In particular, the
most frequented locations of users are considerably closer, and similarity is
considerably greater among multiplex links. We present a number of structural
and interaction features, such as the multilayer Adamic/Adar coefficient, which
are based on the extension of the concept of the node neighbourhood beyond the
single network. Our evaluation, which aims to shed light on the implications of
multiplexity for the link generation process, shows that multilayer features,
constructed from properties across social networks, perform better than their
single network counterparts in predicting links across networks. We propose
that combining information from multiple networks in a multilayer configuration
can provide new insights into user interactions on online social networks, and
can significantly improve link prediction overall with valuable applications to
social bootstrapping and friend recommendations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07885</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07885</id><created>2015-08-31</created><authors><author><keyname>Kotson</keyname><forenames>Michael C.</forenames></author><author><keyname>Schulz</keyname><forenames>Alexia</forenames></author></authors><title>Characterizing Phishing Threats with Natural Language Processing</title><categories>cs.CR</categories><comments>This paper has been accepted for publication by the IEEE Conference
  on Communications and Network Security in September 2015 at Florence, Italy.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spear phishing is a widespread concern in the modern network security
landscape, but there are few metrics that measure the extent to which
reconnaissance is performed on phishing targets. Spear phishing emails closely
match the expectations of the recipient, based on details of their experiences
and interests, making them a popular propagation vector for harmful malware. In
this work we use Natural Language Processing techniques to investigate a
specific real-world phishing campaign and quantify attributes that indicate a
targeted spear phishing attack. Our phishing campaign data sample comprises 596
emails - all containing a web bug and a Curriculum Vitae (CV) PDF attachment -
sent to our institution by a foreign IP space. The campaign was found to
exclusively target specific demographics within our institution. Performing a
semantic similarity analysis between the senders' CV attachments and the
recipients' LinkedIn profiles, we conclude with high statistical certainty (p
$&lt; 10^{-4}$) that the attachments contain targeted rather than randomly
selected material. Latent Semantic Analysis further demonstrates that
individuals who were a primary focus of the campaign received CVs that are
highly topically clustered. These findings differentiate this campaign from one
that leverages random spam.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07902</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07902</id><created>2015-08-31</created><updated>2015-12-13</updated><authors><author><keyname>Shekhovtsov</keyname><forenames>Alexander</forenames></author><author><keyname>Swoboda</keyname><forenames>Paul</forenames></author><author><keyname>Savchynskyy</keyname><forenames>Bogdan</forenames></author></authors><title>Maximum Persistency via Iterative Relaxed Inference with Graphical
  Models</title><categories>cs.CV cs.DS</categories><comments>Reworked version, submitted to PAMI</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We consider the NP-hard problem of MAP-inference for graphical models. We
propose a polynomial time practically efficient algorithm for finding a part of
its optimal solution. Specifically, our algorithm marks each label in each node
of the considered graphical model either as (i) optimal, meaning that it
belongs to all optimal solutions of the inference problem; (ii) non-optimal if
it provably does not belong to any solution; or (iii) undefined, which means
our algorithm can not make a decision regarding the label. Moreover, we prove
optimality of our approach: it delivers in a certain sense the largest total
number of labels marked as optimal or non-optimal. We demonstrate superiority
of our approach on problems from machine learning and computer vision
benchmarks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07909</identifier>
 <datestamp>2015-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07909</id><created>2015-08-31</created><updated>2015-11-27</updated><authors><author><keyname>Sennrich</keyname><forenames>Rico</forenames></author><author><keyname>Haddow</keyname><forenames>Barry</forenames></author><author><keyname>Birch</keyname><forenames>Alexandra</forenames></author></authors><title>Neural Machine Translation of Rare Words with Subword Units</title><categories>cs.CL</categories><comments>new results with improved baseline; more references in related work;
  cuts to fit conference proceedings</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Neural machine translation (NMT) models typically operate with a fixed
vocabulary, so the translation of rare and unknown words is an open problem.
Previous work addresses this problem through back-off dictionaries. In this
paper, we introduce a simpler and more effective approach, making the NMT model
capable of open-vocabulary translation by encoding rare and unknown words as
sequences of subword units, based on the intuition that various word classes
are translatable via smaller units than words, for instance names (via
character copying or transliteration), compounds (via compositional
translation), and cognates and loanwords (via phonological and morphological
transformations). We discuss the suitability of different word segmentation
techniques, including simple character $n$-gram models and a segmentation based
on the \emph{byte pair encoding} compression algorithm, and empirically show
that subword models improve over a back-off dictionary baseline for the WMT 15
translation tasks English-German and English-Russian by 1.1 and 1.3 BLEU,
respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07913</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07913</id><created>2015-08-31</created><updated>2016-01-19</updated><authors><author><keyname>Maleki</keyname><forenames>Nahal</forenames></author><author><keyname>Vosoughi</keyname><forenames>Azadeh</forenames></author><author><keyname>Rahnavard</keyname><forenames>Nazanin</forenames></author></authors><title>Distributed Binary Detection over Fading Channels: Cooperative and
  Parallel Architectures</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the problem of binary distributed detection of a known
signal in correlated Gaussian sensing noise in a wireless sensor network, where
the sensors are restricted to use likelihood ratio test (LRT), and communicate
with the fusion center (FC) over bandwidth-constrained channels that are
subject to fading and noise. To mitigate the deteriorating effect of fading
encountered in the conventional parallel fusion architecture, in which the
sensors directly communicate with the FC, we propose new fusion architectures
that enhance the detection performance, via harvesting cooperative gain
(so-called decision diversity gain). In particular, we propose: (i) cooperative
fusion architecture with Alamouti's space-time coding (STC) scheme at sensors,
(ii) cooperative fusion architecture with signal fusion at sensors, and (iii)
parallel fusion architecture with local threshold changing at sensors. For
these schemes, we derive the LRT and majority fusion rules at the FC, and
provide upper bounds on the average error probabilities for homogeneous
sensors, subject to uncorrelated Gaussian sensing noise, in terms of
signal-to-noise ratio (SNR) of communication and sensing channels. Our
simulation results indicate that, when the FC employs the LRT rule, unless for
low communication SNR and moderate/high sensing SNR, performance improvement is
feasible with the new fusion architectures. When the FC utilizes the majority
rule, such improvement is possible, unless for high sensing SNR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07920</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07920</id><created>2015-08-31</created><authors><author><keyname>Chou</keyname><forenames>Remi A.</forenames></author><author><keyname>Vellambi</keyname><forenames>Badri</forenames></author><author><keyname>Bloch</keyname><forenames>Matthieu</forenames></author><author><keyname>Kliewer</keyname><forenames>Joerg</forenames></author></authors><title>Coding Schemes for Achieving Strong Secrecy at Negligible Cost</title><categories>cs.IT math.IT</categories><comments>31 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of achieving strong secrecy over wiretap channels at
negligible cost, in the sense of maintaining the overall communication rate of
the same channel without secrecy constraints. Specifically, we propose and
analyze two source-channel coding architectures, in which secrecy is achieved
by multiplexing public and confidential messages. In both cases, our main
contribution is to circumvent the assumption that random numbers with uniform
distributions are available, and to show that secrecy can still be achieved.
Our first source-channel coding architecture relies on a modified wiretap
channel code, in which the randomization is performed using the output of a
source code. In contrast, our second architecture relies on a standard wiretap
code combined with a modified source code, which we call a uniform compression
code, in which a small shared secret seed is used to enhance the uniformity of
the source code output. We carry out an extensive analysis of uniform
compression codes and characterize the optimal size of the seed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07921</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07921</id><created>2015-08-31</created><authors><author><keyname>Frati</keyname><forenames>Fabrizio</forenames></author><author><keyname>Hoffmann</keyname><forenames>Michael</forenames></author><author><keyname>Kusters</keyname><forenames>Vincent</forenames></author></authors><title>Simultaneous Embeddings with Few Bends and Crossings</title><categories>cs.CG cs.DS math.CO</categories><comments>Full version of the paper &quot;Simultaneous Embeddings with Few Bends and
  Crossings&quot; accepted at GD '15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A simultaneous embedding with fixed edges (SEFE) of two planar graphs $R$ and
$B$ is a pair of plane drawings of $R$ and $B$ that coincide when restricted to
the common vertices and edges of $R$ and $B$. We show that whenever $R$ and $B$
admit a SEFE, they also admit a SEFE in which every edge is a polygonal curve
with few bends and every pair of edges has few crossings. Specifically: (1) if
$R$ and $B$ are trees then one bend per edge and four crossings per edge pair
suffice (and one bend per edge is sometimes necessary), (2) if $R$ is a planar
graph and $B$ is a tree then six bends per edge and eight crossings per edge
pair suffice, and (3) if $R$ and $B$ are planar graphs then six bends per edge
and sixteen crossings per edge pair suffice. Our results improve on a paper by
Grilli et al. (GD'14), which proves that nine bends per edge suffice, and on a
paper by Chan et al. (GD'14), which proves that twenty-four crossings per edge
pair suffice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07931</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07931</id><created>2015-08-31</created><authors><author><keyname>Ho</keyname><forenames>Shan-Yuan</forenames></author><author><keyname>Krishnan</keyname><forenames>Abijith</forenames></author></authors><title>A Secretary Problem with a Sliding Window for Recalling Applicants</title><categories>math.PR cs.IT math.CO math.IT</categories><comments>28 pages, 8 figures, 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Sliding Window Secretary Problem allows a window of choices to the
Classical Secretary Problem, in which there is the option to choose the
previous $K$ choices immediately prior to the current choice. We consider a
case of this sequential choice problem in which the interviewer has a finite,
known number of choices and can only discern the relative ranks of choices, and
in which every permutation of ranks is equally likely. We examine three cases
of the problem: (i) the interviewer has one choice to choose the best
applicant; (ii) the interviewer has one choice to choose one of the top two
applicants; and (iii) the interviewer has two choices to choose the best
applicant. The form of the optimal strategy is shown, the probability of
winning as a function of the window size is derived, and the limiting behavior
is discussed for all three cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07933</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07933</id><created>2015-08-31</created><authors><author><keyname>Lee</keyname><forenames>Soomin</forenames></author><author><keyname>Nedi&#x107;</keyname><forenames>Angelia</forenames></author><author><keyname>Raginsky</keyname><forenames>Maxim</forenames></author></authors><title>Decentralized Online Optimization with Global Objectives and Local
  Communication</title><categories>math.OC cs.LG cs.SY</categories><comments>10 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a decentralized online convex optimization problem in a network
of agents, where each agent controls only a coordinate (or a part) of the
global decision vector. For such a problem, we propose two decentralized
variants (ODA-LM and OPDA-TV) of Nesterov's primal-dual algorithm with dual
averaging. In ODA-LM, to mitigate the disagreements on the primal-vector
updates, the agents implement a generalization of the local
information-exchange dynamics recently proposed by Li and Marden over a static
undirected graph. In OPDA-TV, the agents implement the broadcast-based push-sum
dynamics over a time-varying sequence of uniformly connected digraphs. We show
that the regret bounds in both cases have sublinear growth of $O(\sqrt{T})$,
with the time horizon $T$, when the stepsize is of the form $1/\sqrt{t}$ and
the objective functions are Lipschitz-continuous convex functions with
Lipschitz gradients. We also implement the proposed algorithms on a sensor
network to complement our theoretical analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07951</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07951</id><created>2015-08-31</created><authors><author><keyname>Riquelme</keyname><forenames>Fabi&#xe1;n</forenames></author></authors><title>Measuring user influence on Twitter: A survey</title><categories>cs.SI</categories><comments>24 pages, 3 tables, 1 figure</comments><msc-class>90Bxx</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Centrality is one of the most studied concepts in social network analysis.
There is a huge literature regarding centrality measures, as ways to identify
the most relevant users in a social network. The challenge is to find measures
that can be computed efficiently, and that can be able to classify the users
according to relevance criteria as close as possible to reality.
  We address this problem in the context of the Twitter network, an online
social networking service with millions of users and an impressive flow of
messages that are published and spread daily by interactions between users. In
Twitter there are different types of users, but most utility lies in finding
the most influential ones.
  The purpose of this article is to collect and classify the different measures
of influence of Twitter that exist so far in literature. The diversity of these
measures is very high. Some are based on simple metrics, and others are based
on complex mathematical models. Several measures are based on the PageRank
algorithm, traditionally used to rank the websites on the Internet. Some
measures consider the timeline of publication, others the content of the
messages, some are focused on specific topics, and others try to make
predictions. We consider all these aspects, and some additional others.
  Furthermore, we mention measures of activity and popularity, the traditional
mechanisms to correlate measures, and some simple but important aspects of
computational complexity for this particular context.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07953</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07953</id><created>2015-08-31</created><authors><author><keyname>Ben-Zrihem</keyname><forenames>Nir</forenames></author><author><keyname>Zelnik-Manor</keyname><forenames>Lihi</forenames></author></authors><title>Approximate Nearest Neighbor Fields in Video</title><categories>cs.CV</categories><comments>A CVPR 2015 oral paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce RIANN (Ring Intersection Approximate Nearest Neighbor search),
an algorithm for matching patches of a video to a set of reference patches in
real-time. For each query, RIANN finds potential matches by intersecting rings
around key points in appearance space. Its search complexity is reversely
correlated to the amount of temporal change, making it a good fit for videos,
where typically most patches change slowly with time. Experiments show that
RIANN is up to two orders of magnitude faster than previous ANN methods, and is
the only solution that operates in real-time. We further demonstrate how RIANN
can be used for real-time video processing and provide examples for a range of
real-time video applications, including colorization, denoising, and several
artistic effects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07964</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07964</id><created>2015-08-31</created><authors><author><keyname>Teng</keyname><forenames>Diyan</forenames></author><author><keyname>Ertin</keyname><forenames>Emre</forenames></author></authors><title>Learning to Aggregate Information for Sequential Inferences</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of training a binary sequential classifier under an
error rate constraint. It is well known that for known densities, accumulating
the likelihood ratio statistics is time optimal under a fixed error rate
constraint. For the case of unknown densities, we formulate the learning for
sequential detection problem as a constrained density ratio estimation problem.
Specifically, we show that the problem can be posed as a convex optimization
problem using a Reproducing Kernel Hilbert Space representation for the
log-density ratio function. The proposed binary sequential classifier is tested
on synthetic data set and UC Irvine human activity recognition data set,
together with previous approaches for density ratio estimation. Our empirical
results show that the classifier trained through the proposed technique
achieves smaller average sampling cost than previous classifiers proposed in
the literature for the same error rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07969</identifier>
 <datestamp>2015-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07969</id><created>2015-08-31</created><updated>2015-10-07</updated><authors><author><keyname>Sugavanam</keyname><forenames>Nithin</forenames></author><author><keyname>Ertin</keyname><forenames>Emre</forenames></author></authors><title>Recovery guarantees for multifrequency chirp waveforms in compressed
  radar sensing</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Radar imaging systems transmit modulated wideband waveform to achieve high
range resolution resulting in high sampling rates at the receiver proportional
to the bandwidth of the transmit waveform. Analog processing techniques can be
used on receive to reduce the number of measurements to N, the number of
potential delay bins. If the scene interrogated by the radar is assumed to be
sparse consisting of K point targets, results from compressive sensing suggest
that number of measurements can be further reduced to scale with K logN for
stable recovery of a sparse scene from measurements with additive noise. While
unstructured random projectors guarantee successful recovery under sparsity
constraints, they cannot be implemented in the radar hardware in practice.
Recently, structured random Toeplitz and Circulant matrices that result from
using stochastic waveforms in time delay estimation setting have been shown to
yield recovery guarantees similar to unstructured sensing matrices. However,
the corresponding transmitter and receiver structures have high complexity and
large storage requirements. In this paper, we propose an alternative low
complexity compressive wideband radar sensor which combines multitone signal
chirp waveform on transmit with a receiver that utilizes an analog mixer
followed with a uniform sub-Nyquist sampling stage. We derive the recovery
guarantees for the resulting structured measurement matrix and sufficient
conditions for the number of tones. The only random component of our design is
the sparse tone spectrum implementable efficiently in hardware. Our analytical
and empirical results show that the performance of our scheme is in par with
unstructured random sensing matrices and structured Toeplitz and Circulant
matrices with random entries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07977</identifier>
 <datestamp>2015-09-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07977</id><created>2015-08-28</created><authors><author><keyname>Richter-Gottfried</keyname><forenames>Franz</forenames></author><author><keyname>Ditter</keyname><forenames>Alexander</forenames></author><author><keyname>Fey</keyname><forenames>Dietmar</forenames></author></authors><title>OpenCL 2.0 for FPGAs using OCLAcc</title><categories>cs.SE</categories><comments>Presented at Second International Workshop on FPGAs for Software
  Programmers (FSP 2015) (arXiv:1508.06320)</comments><proxy>Frank Hannig</proxy><report-no>FSP/2015/11</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Designing hardware is a time-consuming and complex process. Realization of
both, embedded and high-performance applications can benefit from a design
process on a higher level of abstraction. This helps to reduce development time
and allows to iteratively test and optimize the hardware design during
development, as common in software development. We present our tool, OCLAcc,
which allows the generation of entire FPGA-based hardware accelerators from
OpenCL and discuss the major novelties of OpenCL 2.0 and how they can be
realized in hardware using OCLAcc.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1508.07982</identifier>
 <datestamp>2016-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1508.07982</id><created>2015-08-31</created><updated>2016-01-21</updated><authors><author><keyname>Schornbaum</keyname><forenames>Florian</forenames></author><author><keyname>R&#xfc;de</keyname><forenames>Ulrich</forenames></author></authors><title>Massively Parallel Algorithms for the Lattice Boltzmann Method on
  Non-uniform Grids</title><categories>cs.DC cs.CE</categories><comments>32 pages, 20 figures, 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The lattice Boltzmann method exhibits excellent scalability on current
supercomputing systems and has thus increasingly become an alternative method
for large-scale non-stationary flow simulations, reaching up to a trillion grid
nodes. Additionally, grid refinement can lead to substantial savings in memory
and compute time. These saving, however, come at the cost of much more complex
data structures and algorithms. In particular, the interface between subdomains
with different grid sizes must receive special treatment. In this article, we
present parallel algorithms, distributed data structures, and communication
routines that are implemented in the software framework waLBerla in order to
support large-scale, massively parallel lattice Boltzmann-based simulations on
non-uniform grids. Additionally, we evaluate the performance of our approach on
two current petascale supercomputers. On an IBM Blue Gene/Q system, the largest
weak scaling benchmarks with refined grids are executed with almost two million
threads, demonstrating not only near-perfect scalability but also an absolute
performance of close to a trillion lattice Boltzmann cell updates per second.
On an Intel-based system, the strong scaling of a simulation with refined grids
and a total of more than 8.5 million cells is demonstrated to reach a
performance of less than one millisecond per time step. This enables
simulations with complex, non-uniform grids and four million time steps per
hour compute time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00016</identifier>
 <datestamp>2015-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00016</id><created>2015-08-31</created><updated>2015-12-11</updated><authors><author><keyname>Nassar</keyname><forenames>Huda</forenames></author><author><keyname>Kloster</keyname><forenames>Kyle</forenames></author><author><keyname>Gleich</keyname><forenames>David F.</forenames></author></authors><title>Strong Localization in Personalized PageRank Vectors</title><categories>cs.SI</categories><comments>13 pages. 4 figures. Codes available at
  https://github.com/nassarhuda/pprlocal</comments><acm-class>F.2.1; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The personalized PageRank diffusion is a fundamental tool in network analysis
tasks like community detection and link prediction. This tool models the spread
of a quantity from a small, initial set of seed nodes, and has long been
observed to stay localized near this seed set. We derive a sublinear
upper-bound on the number of nonzeros necessary to approximate a personalized
PageRank vector on a power-law graph. Our experimental results on power-law
graphs with a wide variety of parameter settings demonstrate that the bound is
loose, and instead supports a new conjectured bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00025</identifier>
 <datestamp>2015-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00025</id><created>2015-08-28</created><updated>2015-09-23</updated><authors><author><keyname>Vogt</keyname><forenames>Markus</forenames></author><author><keyname>Hempel</keyname><forenames>Gerald</forenames></author><author><keyname>Castrillon</keyname><forenames>Jeronimo</forenames></author><author><keyname>Hochberger</keyname><forenames>Christian</forenames></author></authors><title>GCC-Plugin for Automated Accelerator Generation and Integration on
  Hybrid FPGA-SoCs</title><categories>cs.OH</categories><comments>Presented at Second International Workshop on FPGAs for Software
  Programmers (FSP 2015) (arXiv:1508.06320)</comments><proxy>Frank Hannig</proxy><report-no>FSP/2015/17</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, architectures combining a reconfigurable fabric and a
general purpose processor on a single chip became increasingly popular. Such
hybrid architectures allow extending embedded software with application
specific hardware accelerators to improve performance and/or energy efficiency.
Aiding system designers and programmers at handling the complexity of the
required process of hardware/software (HW/SW) partitioning is an important
issue. Current methods are often restricted, either to bare-metal systems, to
subsets of mainstream programming languages, or require special coding
guidelines, e.g., via annotations. These restrictions still represent a high
entry barrier for the wider community of programmers that new hybrid
architectures are intended for. In this paper we revisit HW/SW partitioning and
present a seamless programming flow for unrestricted, legacy C code. It
consists of a retargetable GCC plugin that automatically identifies code
sections for hardware acceleration and generates code accordingly. The proposed
workflow was evaluated on the Xilinx Zynq platform using unmodified code from
an embedded benchmark suite.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00028</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00028</id><created>2015-08-31</created><authors><author><keyname>Sarkar</keyname><forenames>Kanchan</forenames></author><author><keyname>Bhattacharyya</keyname><forenames>S. P.</forenames></author></authors><title>Pure and Hybrid Evolutionary Computing in Global Optimization of
  Chemical Structures: from Atoms and Molecules to Clusters and Crystals</title><categories>cond-mat.mtrl-sci cs.NE physics.chem-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The growth of evolutionary computing (EC) methods in the exploration of
complex potential energy landscapes of atomic and molecular clusters, as well
as crystals over the last decade or so is reviewed. The trend of growth
indicates that pure as well as hybrid evolutionary computing techniques in
conjunction of DFT has been emerging as a powerful tool, although work on
molecular clusters has been rather limited so far. Some attempts to solve the
atomic/molecular Schrodinger Equation (SE) directly by genetic algorithms (GA)
are available in literature. At the Born-Oppenheimer level of approximation
GA-density methods appear to be a viable tool which could be more extensively
explored in the coming years, specially in the context of designing molecules
and materials with targeted properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00036</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00036</id><created>2015-08-28</created><authors><author><keyname>Qin</keyname><forenames>Shaodong</forenames></author><author><keyname>Berekovic</keyname><forenames>Mladen</forenames></author></authors><title>A Comparison of High-Level Design Tools for SoC-FPGA on Disparity Map
  Calculation Example</title><categories>cs.OH</categories><comments>Presented at Second International Workshop on FPGAs for Software
  Programmers (FSP 2015) (arXiv:1508.06320)</comments><proxy>Frank Hannig</proxy><report-no>FSP/2015/15</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern SoC-FPGA that consists of FPGA with embedded ARM cores is being
popularized as an embedded vision system platform. However, the design approach
of SoC-FPGA applications still follows traditional hardware-software separate
workflow, which becomes the barrier of rapid product design and iteration on
SoC-FPGA. High-Level Synthesis (HLS) and OpenCL-based system-level design
approaches provide programmers the possibility to design SoC-FGPA at
system-level with an unified development environment for both hardware and
software. To evaluate the feasibility of high-level design approach especially
for embedded vision applications, Vivado HLS and Altera SDK for OpenCL,
representative and most popular commercial tools in market, are selected as
evaluation design tools, disparity map calculation as targeting application. In
this paper, hardware accelerators of disparity map calculation are designed
with both tools and implemented on Zedboard and SoCKit development board,
respectively. Comparisons between design tools are made in aspects of
supporting directives, accelerator design process, and generated hardware
performance. The results show that both tools can generate efficient hardware
for disparity map calculation application with much less developing time.
Moreover, we can also state that, more directives (e.g., interface type, array
reshape, resource type specification) are supported, but more hardware
knowledge is required, in Vivado HLS. In contrast, Altera SDK for OpenCL is
relatively easier for software programmers who is new to hardware, but with the
price of more resources usage on FPGA for similar hardware accelerator
generation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00040</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00040</id><created>2015-08-27</created><authors><author><keyname>Sano</keyname><forenames>Kentaro</forenames></author></authors><title>DSL-based Design Space Exploration for Temporal and Spatial Parallelism
  of Custom Stream Computing</title><categories>cs.AR cs.CE cs.DC cs.PL</categories><comments>Presented at Second International Workshop on FPGAs for Software
  Programmers (FSP 2015) (arXiv:1508.06320)</comments><proxy>Frank Hannig</proxy><report-no>FSP/2015/06</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Stream computation is one of the approaches suitable for FPGA-based custom
computing due to its high throughput capability brought by pipelining with
regular memory access. To increase performance of iterative stream computation,
we can exploit both temporal and spatial parallelism by deepening and
duplicating pipelines, respectively. However, the performance is constrained by
several factors including available hardware resources on FPGA, an external
memory bandwidth, and utilization of pipeline stages, and therefore we need to
find the best mix of the different parallelism to achieve the highest
performance per power. In this paper, we present a domain-specific language
(DSL) based design space exploration for temporally and/or spatially parallel
stream computation with FPGA. We define a DSL where we can easily design a
hierarchical structure of parallel stream computation with abstract description
of computation. For iterative stream computation of fluid dynamics simulation,
we design hardware structures with a different mix of the temporal and spatial
parallelism. By measuring the performance and the power consumption, we find
the best among them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00042</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00042</id><created>2015-08-27</created><authors><author><keyname>Liu</keyname><forenames>Cheng</forenames></author><author><keyname>Ng</keyname><forenames>Ho-Cheung</forenames></author><author><keyname>So</keyname><forenames>Hayden Kwok-Hay</forenames></author></authors><title>Automatic Nested Loop Acceleration on FPGAs Using Soft CGRA Overlay</title><categories>cs.AR cs.DC</categories><comments>Presented at Second International Workshop on FPGAs for Software
  Programmers (FSP 2015) (arXiv:1508.06320)</comments><proxy>Frank Hannig</proxy><report-no>FSP/2015/03</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Offloading compute intensive nested loops to execute on FPGA accelerators
have been demonstrated by numerous researchers as an effective performance
enhancement technique across numerous application domains. To construct such
accelerators with high design productivity, researchers have increasingly
turned to the use of overlay architectures as an intermediate generation target
built on top of off-the-shelf FPGAs. However, achieving the desired
performance-overhead trade-off remains a major productivity challenge as
complex application-specific customizations over a large design space covering
multiple architectural parameters are needed.
  In this work, an automatic nested loop acceleration framework utilizing a
regular soft coarse-grained reconfigurable array (SCGRA) overlay is presented.
Given high-level resource constraints, the framework automatically customizes
the overlay architectural design parameters, high-level compilation options as
well as communication between the accelerator and the host processor for
optimized performance specifically to the given application. In our
experiments, at a cost of 10 to 20 minutes additional tools run time, the
proposed customization process resulted in up to 5 times additional speedup
over a baseline accelerator generated by the same framework without
customization. Overall, when compared to the equivalent software running on the
host ARM processor alone on the Zedboard, the resulting accelerators achieved
up to 10 times speedup.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00061</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00061</id><created>2015-08-31</created><authors><author><keyname>Ong</keyname><forenames>Hao Yi</forenames></author></authors><title>Value function approximation via low-rank models</title><categories>cs.LG cs.AI</categories><comments>arXiv admin note: substantial text overlap with arXiv:0912.3599 by
  other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel value function approximation technique for Markov decision
processes. We consider the problem of compactly representing the state-action
value function using a low-rank and sparse matrix model. The problem is to
decompose a matrix that encodes the true value function into low-rank and
sparse components, and we achieve this using Robust Principal Component
Analysis (PCA). Under minimal assumptions, this Robust PCA problem can be
solved exactly via the Principal Component Pursuit convex optimization problem.
We experiment the procedure on several examples and demonstrate that our method
yields approximations essentially identical to the true function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00065</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00065</id><created>2015-08-31</created><authors><author><keyname>Glisson</keyname><forenames>William Bradley</forenames></author><author><keyname>Andel</keyname><forenames>Todd</forenames></author><author><keyname>McDonald</keyname><forenames>Todd</forenames></author><author><keyname>Jacobs</keyname><forenames>Mike</forenames></author><author><keyname>Campbell</keyname><forenames>Matt</forenames></author><author><keyname>Mayr</keyname><forenames>Johnny</forenames></author></authors><title>Compromising a Medical Mannequin</title><categories>cs.CY</categories><comments>http://aisel.aisnet.org/amcis2015/HealthIS/GeneralPresentations/5/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Medical training devices are becoming increasingly dependent on technology,
creating opportunities that are inherently conducive to security breaches.
Previous medical device research has focused on individual device security
breaches and the technical aspects involved with these breaches. This research
examines the viability of breaching a production-deployed medical training
mannequin. The results of the proof of concept research indicate that it is
possible to breach a medical training mannequin in a live environment. The
research contribution is an initial empirical analysis of the viability of
compromising a medical training mannequin along with providing the foundation
for future research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00069</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00069</id><created>2015-08-31</created><authors><author><keyname>Wang</keyname><forenames>Tianyu</forenames></author><author><keyname>Song</keyname><forenames>Lingyang</forenames></author><author><keyname>Han</keyname><forenames>Zhu</forenames></author><author><keyname>Saad</keyname><forenames>Walid</forenames></author></authors><title>Overlapping Coalition Formation Games for Emerging Communication
  Networks</title><categories>cs.GT cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern cellular networks are witnessing an unprecedented evolution from
classical, centralized and homogenous architectures into a mix of various
technologies, in which the network devices are densely and randomly deployed in
a decentralized and heterogenous architecture. This shift in network
architecture requires network devices to become more autonomous and,
potentially, cooperate with one another. Such cooperation can, for example,
take place between interfering small access points that seek to coordinate
their radio resource allocation, nearby single-antenna users that can
cooperatively perform virtual MIMO communications, or even unlicensed users
that wish to cooperatively sense the spectrum of the licensed users. Such
cooperative mechanisms involve the simultaneous sharing and distribution of
resources among a number of overlapping cooperative groups or coalitions. In
this paper, a novel mathematical framework from cooperative games, dubbed
\emph{overlapping coalition formation games} (OCF games), is introduced to
model and solve such cooperative scenarios. First, the concepts of OCF games
are presented, and then, several algorithmic aspects are studied for two main
classes of OCF games. Subsequently, two example applications, namely,
interference management and cooperative spectrum sensing, are discussed in
detail to show how the proposed models and algorithms can be used in the future
scenarios of wireless systems. Finally, we conclude by providing an overview on
future directions and applications of OCF games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00070</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00070</id><created>2015-08-31</created><authors><author><keyname>Ji</keyname><forenames>B. L.</forenames></author><author><keyname>Li</keyname><forenames>H.</forenames></author><author><keyname>Ye</keyname><forenames>Q.</forenames></author><author><keyname>Gausepohl</keyname><forenames>S.</forenames></author><author><keyname>Deora</keyname><forenames>S.</forenames></author><author><keyname>Veksler</keyname><forenames>D.</forenames></author><author><keyname>Vivekanand</keyname><forenames>S.</forenames></author><author><keyname>Chong</keyname><forenames>H.</forenames></author><author><keyname>Stamper</keyname><forenames>H.</forenames></author><author><keyname>Burroughs</keyname><forenames>T.</forenames></author><author><keyname>Johnson</keyname><forenames>C.</forenames></author><author><keyname>Smalley</keyname><forenames>M.</forenames></author><author><keyname>Bennett</keyname><forenames>S.</forenames></author><author><keyname>Kaushik</keyname><forenames>V.</forenames></author><author><keyname>Piccirillo</keyname><forenames>J.</forenames></author><author><keyname>Rodgers</keyname><forenames>M.</forenames></author><author><keyname>Passaro</keyname><forenames>M.</forenames></author><author><keyname>Liehr</keyname><forenames>M.</forenames></author></authors><title>In-Line-Test of Variability and Bit-Error-Rate of HfOx-Based Resistive
  Memory</title><categories>cs.ET cond-mat.mtrl-sci physics.data-an</categories><comments>4 pages. Memory Workshop (IMW), 2015 IEEE International</comments><journal-ref>2015 IEEE International Memory Workshop(IMW), 17-20 May 2015 URL:
  http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=7150290&amp;isnumber=7150256</journal-ref><doi>10.1109/IMW.2015.7150290</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spatial and temporal variability of HfOx-based resistive random access memory
(RRAM) are investigated for manufacturing and product designs. Manufacturing
variability is characterized at different levels including lots, wafers, and
chips. Bit-error-rate (BER) is proposed as a holistic parameter for the write
cycle resistance statistics. Using the electrical in-line-test cycle data, a
method is developed to derive BERs as functions of the design margin, to
provide guidance for technology evaluation and product design. The proposed BER
calculation can also be used in the off-line bench test and build-in-self-test
(BIST) for adaptive error correction and for the other types of random access
memories.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00083</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00083</id><created>2015-08-31</created><authors><author><keyname>Kadoury</keyname><forenames>Samuel</forenames></author><author><keyname>Vorontsov</keyname><forenames>Eugene</forenames></author><author><keyname>Tang</keyname><forenames>An</forenames></author></authors><title>Metastatic liver tumour segmentation from discriminant Grassmannian
  manifolds</title><categories>cs.LG cs.CV</categories><journal-ref>Physics in Medicine and Biology 60 (2015)</journal-ref><doi>10.1088/0031-9155/60/16/6459</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The early detection, diagnosis and monitoring of liver cancer progression can
be achieved with the precise delineation of metastatic tumours. However,
accurate automated segmentation remains challenging due to the presence of
noise, inhomogeneity and the high appearance variability of malignant tissue.
In this paper, we propose an unsupervised metastatic liver tumour segmentation
framework using a machine learning approach based on discriminant Grassmannian
manifolds which learns the appearance of tumours with respect to normal tissue.
First, the framework learns within-class and between-class similarity
distributions from a training set of images to discover the optimal manifold
discrimination between normal and pathological tissue in the liver. Second, a
conditional optimisation scheme computes nonlocal pairwise as well as
pattern-based clique potentials from the manifold subspace to recognise regions
with similar labelings and to incorporate global consistency in the
segmentation process. The proposed framework was validated on a clinical
database of 43 CT images from patients with metastatic liver cancer. Compared
to state-of-the-art methods, our method achieves a better performance on two
separate datasets of metastatic liver tumours from different clinical sites,
yielding an overall mean Dice similarity coefficient of 90.7 +/- 2.4 in over 50
tumours with an average volume of 27.3 mm3.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00091</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00091</id><created>2015-08-31</created><authors><author><keyname>Khosravani</keyname><forenames>S.</forenames></author><author><keyname>Moghaddam</keyname><forenames>I. N.</forenames></author><author><keyname>Afshar</keyname><forenames>A.</forenames></author><author><keyname>Karrari</keyname><forenames>M.</forenames></author></authors><title>Fault Tolerant Control of Power Systems in presence of Sensor Failure</title><categories>cs.SY</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses Fault Tolerant Control (FTC) of Large Power Systems
(LPS) subject to sensor failure. Hiding the fault from the controller allows
the nominal controller to remain in the loop. We assume specific faults that
violate observability of a subsystem, and we cannot rely on these faulty
subsystems when estimating states. We use a new method for reconfiguration
control of these faults that lead to unobservability of subsystems. The method
proposes augmenting a faulty subsystems with another subsystem(s) until a new
subsystem is achieved that is observable. Next, finding the best subsystems
among available candidates is considered and using structural analysis methods
and grammian definition, a complete algorithm is proposed for FTC of LPS. The
proposed approach is applied to the IEEE 14-bus test case and interactions are
considered in nonlinear form. Simulation results show that the proposed
approach works as intended.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00092</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00092</id><created>2015-08-31</created><authors><author><keyname>Meka</keyname><forenames>Raghu</forenames></author></authors><title>Explicit resilient functions matching Ajtai-Linial</title><categories>cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Boolean function on n variables is q-resilient if for any subset of at most
q variables, the function is very likely to be determined by a uniformly random
assignment to the remaining n-q variables; in other words, no coalition of at
most q variables has significant influence on the function. Resilient functions
have been extensively studied with a variety of applications in cryptography,
distributed computing, and pseudorandomness. The best known balanced resilient
function on n variables due to Ajtai and Linial ([AL93]) is Omega(n/(log^2
n))-resilient. However, the construction of Ajtai and Linial is by the
probabilistic method and does not give an efficiently computable function.
  In this work we give an explicit monotone depth three almost-balanced Boolean
function on n bits that is Omega(n/(log^2 n))-resilient matching the work of
Ajtai and Linial. The best previous explicit construction due to Meka [Meka09]
(which only gives a logarithmic depth function) and Chattopadhyay and
Zuckermman [CZ15] were only n^{1-c}-resilient for any constant c &lt; 1. Our
construction and analysis are motivated by (and simplifies parts of) the recent
breakthrough of [CZ15] giving explicit two-sources extractors for
polylogarithmic min-entropy; a key ingredient in their result was the
construction of explicit constant-depth resilient functions.
  An important ingredient in our construction is a new randomness optimal
oblivious sampler which preserves moment generating functions of sums of
variables and could be useful elsewhere.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00095</identifier>
 <datestamp>2015-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00095</id><created>2015-08-31</created><authors><author><keyname>Chen</keyname><forenames>Li</forenames></author><author><keyname>Jain</keyname><forenames>Pooja</forenames></author><author><keyname>Chow</keyname><forenames>Kingsum</forenames></author><author><keyname>Guirguis</keyname><forenames>Emad</forenames></author><author><keyname>Wu</keyname><forenames>Tony</forenames></author></authors><title>Brewing Analytics Quality for Cloud Performance</title><categories>cs.PF cs.DC stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud computing has become increasingly popular. Many options of cloud
deployments are available. Testing cloud performance would enable us to choose
a cloud deployment based on the requirements. In this paper, we present an
innovative process, implemented in software, to allow us to assess the quality
of the cloud performance data. The process combines performance data from
multiple machines, spanning across user experience data, workload performance
metrics, and readily available system performance data. Furthermore, we discuss
the major challenges of bringing raw data into tidy data formats in order to
enable subsequent analysis, and describe how our process has several layers of
assessment to validate the quality of the data processing procedure. We present
a case study to demonstrate the effectiveness of our proposed process, and
conclude our paper with several future research directions worth investigating.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00099</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00099</id><created>2015-08-31</created><authors><author><keyname>Gu&#xf0;mundsson</keyname><forenames>Bjarki &#xc1;g&#xfa;st</forenames></author><author><keyname>Magn&#xfa;sson</keyname><forenames>T&#xf3;mas Ken</forenames></author><author><keyname>S&#xe6;mundsson</keyname><forenames>Bj&#xf6;rn Orri</forenames></author></authors><title>Bounds and Fixed-Parameter Algorithms for Weighted Improper Coloring
  (Extended Version)</title><categories>cs.DM cs.CC cs.DS</categories><comments>18 pages, 5 figures, extended version for additional proofs in
  appendix for 16th Italian Conference on Theoretical Computer Science 2015</comments><msc-class>68R10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the weighted improper coloring problem, a generalization of
defective coloring. We present some hardness results and in particular we show
that weighted improper coloring is not fixed-parameter tractable when
parameterized by pathwidth. We generalize bounds for defective coloring to
weighted improper coloring and give a bound for weighted improper coloring in
terms of the sum of edge weights. Finally we give fixed-parameter algorithms
for weighted improper coloring both when parameterized by treewidth and maximum
degree and when parameterized by treewidth and precision of edge weights. In
particular, we obtain a linear-time algorithm for weighted improper coloring of
interval graphs of bounded degree.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00100</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00100</id><created>2015-08-31</created><authors><author><keyname>Hariri</keyname><forenames>Babak Bagheri</forenames></author><author><keyname>Tannen</keyname><forenames>Val</forenames></author></authors><title>Decidability of Equivalence of Aggregate Count-Distinct Queries</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of equivalence of count-distinct aggregate queries,
prove that the problem is decidable, and can be decided in the third level of
Polynomial hierarchy. We introduce the notion of core for conjunctive queries
with comparisons as an extension of the classical notion for relational
queries, and prove that the existence of isomorphism among cores of queries is
a sufficient and necessary condition for equivalence of conjunctive queries
with comparisons similar to the classical relational setting. However, it is
not a necessary condition for equivalence of count-distinct queries. We
introduce a relaxation of this condition based on a new notion, which is a
potentially new query equivalent to the initial query, introduced to capture
the behavior of count-distinct operator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00104</identifier>
 <datestamp>2016-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00104</id><created>2015-08-31</created><updated>2016-03-07</updated><authors><author><keyname>Liu</keyname><forenames>Wenqiang</forenames></author></authors><title>Truth Discovery to Resolve Object Conflicts in Linked Data</title><categories>cs.DB</categories><comments>11 pages, 12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the community of Linked Data, anyone can publish their data as Linked Data
on the web because of the openness of the Semantic Web. As such, RDF (Resource
Description Framework) triples described the same real-world entity can be
obtained from multiple sources; it inevitably results in conflicting objects
for a certain predicate of a real-world entity. The objective of this study is
to identify one truth from multiple conflicting objects for a certain predicate
of a real-world entity. An intuitive principle based on common sense is that an
object from a reliable source is trustworthy; thus, a source that provide
trustworthy object is reliable. Many truth discovery methods based on this
principle have been proposed to estimate source reliability and identify the
truth. However, the effectiveness of existing truth discovery methods is
significantly affected by the number of objects provided by each source.
Therefore, these methods cannot be trivially extended to resolve conflicts in
Linked Data with a scale-free property, i.e., most of the sources provide few
conflicting objects, whereas only a few sources have many conflicting objects.
To address this challenge, we propose a novel approach called TruthDiscover to
identify the truth in Linked Data with a scale-free property. Two strategies
are adopted in TruthDiscover to reduce the effect of the scale-free property on
truth discovery. First, this approach leverages the topological properties of
the Source Belief Graph to estimate the priori beliefs of sources, which are
utilized to smooth the trustworthiness of sources. Second, this approach
utilizes the Hidden Markov Random Field to model the interdependencies between
objects to estimate the trust values of objects accurately. Experiments are
conducted in the six datasets to evaluate TruthDiscover.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00105</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00105</id><created>2015-08-31</created><authors><author><keyname>Howard</keyname><forenames>David</forenames></author><author><keyname>Bull</keyname><forenames>Larry</forenames></author><author><keyname>Costello</keyname><forenames>Ben De Lacy</forenames></author></authors><title>Evolving Unipolar Memristor Spiking Neural Networks</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neuromorphic computing --- brainlike computing in hardware --- typically
requires myriad CMOS spiking neurons interconnected by a dense mesh of
nanoscale plastic synapses. Memristors are frequently citepd as strong synapse
candidates due to their statefulness and potential for low-power
implementations. To date, plentiful research has focused on the bipolar
memristor synapse, which is capable of incremental weight alterations and can
provide adaptive self-organisation under a Hebbian learning scheme. In this
paper we consider the Unipolar memristor synapse --- a device capable of
non-Hebbian switching between only two states (conductive and resistive)
through application of a suitable input voltage --- and discuss its suitability
for neuromorphic systems. A self-adaptive evolutionary process is used to
autonomously find highly fit network configurations. Experimentation on a two
robotics tasks shows that unipolar memristor networks evolve task-solving
controllers faster than both bipolar memristor networks and networks containing
constant nonplastic connections whilst performing at least comparably.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00107</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00107</id><created>2015-08-31</created><updated>2015-09-10</updated><authors><author><keyname>Zhang</keyname><forenames>Pan</forenames></author><author><keyname>Moore</keyname><forenames>Cristopher</forenames></author><author><keyname>Newman</keyname><forenames>M. E. J.</forenames></author></authors><title>Community detection in networks with unequal groups</title><categories>cs.SI physics.soc-ph</categories><journal-ref>Phys. Rev. E 93, 012303 (2016)</journal-ref><doi>10.1103/PhysRevE.93.012303</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, a phase transition has been discovered in the network community
detection problem below which no algorithm can tell which nodes belong to which
communities with success any better than a random guess. This result has,
however, so far been limited to the case where the communities have the same
size or the same average degree. Here we consider the case where the sizes or
average degrees are different. This asymmetry allows us to assign nodes to
communities with better-than- random success by examining their local
neighborhoods. Using the cavity method, we show that this removes the
detectability transition completely for networks with four groups or fewer,
while for more than four groups the transition persists up to a critical amount
of asymmetry but not beyond. The critical point in the latter case coincides
with the point at which local information percolates, causing a global
transition from a less-accurate solution to a more-accurate one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00111</identifier>
 <datestamp>2015-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00111</id><created>2015-08-31</created><updated>2015-10-19</updated><authors><author><keyname>Chung</keyname><forenames>Audrey G.</forenames></author><author><keyname>Shafiee</keyname><forenames>Mohammad Javad</forenames></author><author><keyname>Kumar</keyname><forenames>Devinder</forenames></author><author><keyname>Khalvati</keyname><forenames>Farzad</forenames></author><author><keyname>Haider</keyname><forenames>Masoom A.</forenames></author><author><keyname>Wong</keyname><forenames>Alexander</forenames></author></authors><title>Discovery Radiomics for Multi-Parametric MRI Prostate Cancer Detection</title><categories>cs.CV physics.med-ph q-bio.QM</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Prostate cancer is the most diagnosed form of cancer in Canadian men, and is
the third leading cause of cancer death. Despite these statistics, prognosis is
relatively good with a sufficiently early diagnosis, making fast and reliable
prostate cancer detection crucial. As imaging-based prostate cancer screening,
such as magnetic resonance imaging (MRI), requires an experienced medical
professional to extensively review the data and perform a diagnosis,
radiomics-driven methods help streamline the process and has the potential to
significantly improve diagnostic accuracy and efficiency, and thus improving
patient survival rates. These radiomics-driven methods currently rely on
hand-crafted sets of quantitative imaging-based features, which are selected
manually and can limit their ability to fully characterize unique prostate
cancer tumour phenotype. In this study, we propose a novel \textit{discovery
radiomics} framework for generating custom radiomic sequences tailored for
prostate cancer detection. Discovery radiomics aims to uncover abstract
imaging-based features that capture highly unique tumour traits and
characteristics beyond what can be captured using predefined feature models. In
this paper, we discover new custom radiomic sequencers for generating new
prostate radiomic sequences using multi-parametric MRI data. We evaluated the
performance of the discovered radiomic sequencer against a state-of-the-art
hand-crafted radiomic sequencer for computer-aided prostate cancer detection
with a feedforward neural network using real clinical prostate multi-parametric
MRI data. Results for the discovered radiomic sequencer demonstrate good
performance in prostate cancer detection and clinical decision support relative
to the hand-crafted radiomic sequencer. The use of discovery radiomics shows
potential for more efficient and reliable automatic prostate cancer detection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00114</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00114</id><created>2015-08-31</created><updated>2016-02-17</updated><authors><author><keyname>Cao</keyname><forenames>Yang</forenames></author><author><keyname>Xie</keyname><forenames>Yao</forenames></author><author><keyname>Gebraeel</keyname><forenames>Nagi</forenames></author></authors><title>Multi-Sensor Slope Change Detection</title><categories>stat.ML cs.LG math.ST stat.TH</categories><comments>Accepted with minor revision at ANOR</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a mixture procedure for multi-sensor systems to monitor data
streams for a change-point that causes a gradual degradation to a subset of the
streams. Observations are assumed to be initially normal random variables with
known constant means and variances. After the change-point, observations in the
subset will have increasing or decreasing means. The subset and the
rate-of-changes are unknown. Our procedure uses a mixture statistics, which
assumes that each sensor is affected by the change-point with probability
$p_0$. Analytic expressions are obtained for the average run length (ARL) and
the expected detection delay (EDD) of the mixture procedure, which are
demonstrated to be quite accurate numerically. We establish the asymptotic
optimality of the mixture procedure. Numerical examples demonstrate the good
performance of the proposed procedure. We also discuss an adaptive mixture
procedure using empirical Bayes. This paper extends our earlier work on
detecting an abrupt change-point that causes a mean-shift, by tackling the
challenges posed by the non-stationarity of the slope-change problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00116</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00116</id><created>2015-08-31</created><updated>2016-01-27</updated><authors><author><keyname>Asif</keyname><forenames>M. Salman</forenames></author><author><keyname>Ayremlou</keyname><forenames>Ali</forenames></author><author><keyname>Sankaranarayanan</keyname><forenames>Aswin</forenames></author><author><keyname>Veeraraghavan</keyname><forenames>Ashok</forenames></author><author><keyname>Baraniuk</keyname><forenames>Richard</forenames></author></authors><title>FlatCam: Thin, Bare-Sensor Cameras using Coded Aperture and Computation</title><categories>cs.CV</categories><comments>12 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  FlatCam is a thin form-factor lensless camera that consists of a coded mask
placed on top of a bare, conventional sensor array. Unlike a traditional,
lens-based camera where an image of the scene is directly recorded on the
sensor pixels, each pixel in FlatCam records a linear combination of light from
multiple scene elements. A computational algorithm is then used to demultiplex
the recorded measurements and reconstruct an image of the scene. FlatCam is an
instance of a coded aperture imaging system; however, unlike the vast majority
of related work, we place the coded mask extremely close to the image sensor
that can enable a thin system. We employ a separable mask to ensure that both
calibration and image reconstruction are scalable in terms of memory
requirements and computational complexity. We demonstrate the potential of the
FlatCam design using two prototypes: one at visible wavelengths and one at
infrared wavelengths.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00117</identifier>
 <datestamp>2015-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00117</id><created>2015-08-31</created><updated>2015-10-20</updated><authors><author><keyname>Kumar</keyname><forenames>Devinder</forenames></author><author><keyname>Shafiee</keyname><forenames>Mohammad Javad</forenames></author><author><keyname>Chung</keyname><forenames>Audrey G.</forenames></author><author><keyname>Khalvati</keyname><forenames>Farzad</forenames></author><author><keyname>Haider</keyname><forenames>Masoom A.</forenames></author><author><keyname>Wong</keyname><forenames>Alexander</forenames></author></authors><title>Discovery Radiomics for Computed Tomography Cancer Detection</title><categories>cs.CV</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Objective: Lung cancer is the leading cause for cancer related deaths. As
such, there is an urgent need for a streamlined process that can allow
radiologists to provide diagnosis with greater efficiency and accuracy. A
powerful tool to do this is radiomics. Method: In this study, we take the idea
of radiomics one step further by introducing the concept of discovery radiomics
for lung cancer detection using CT imaging data. Rather than using pre-defined,
hand-engineered feature models as with current radiomics-driven methods, we
discover custom radiomic sequencers that can generate radiomic sequences
consisting of abstract imaging-based features tailored for characterizing lung
tumour phenotype. In this study, we realize these custom radiomic sequencers as
deep convolutional sequencers using a deep convolutional neural network
learning architecture based on a wealth of CT imaging data. Results: To
illustrate the prognostic power and effectiveness of the radiomic sequences
produced by the discovered sequencer, we perform a classification between
malignant and benign lesions from 93 patients with diagnostic data from the
LIDC-IDRI dataset. Using the clinically provided diagnostic data as ground
truth, proposed framework provided an average accuracy of 77.52% via 10-fold
cross-validation with a sensitivity of 79.06% and specificity of 76.11%. We
also perform quantitative analysis to establish the effectiveness of the
radiomics sequences. Conclusion: The proposed framework outperforms the
state-of-the art approach for lung lesion classification. Significance: These
results illustrate the potential for the proposed discovery radiomics approach
in aiding radiologists in improving screening efficiency and accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00118</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00118</id><created>2015-08-31</created><authors><author><keyname>Indyk</keyname><forenames>Piotr</forenames></author><author><keyname>Mahabadi</keyname><forenames>Sepideh</forenames></author><author><keyname>Vakilian</keyname><forenames>Ali</forenames></author></authors><title>Towards Tight Bounds for the Streaming Set Cover Problem</title><categories>cs.DS cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the classic Set Cover problem in the data stream model. For $n$
elements and $m$ sets we give a $O(1/\delta)$-pass algorithm with a strongly
sub-linear $\tilde{O}(mn^{\delta})$ space and logarithmic approximation factor.
This yields a significant improvement over the earlier algorithm of [DIMV14]
which used exponentially larger number of passes. We complement this result by
showing that the tradeoff between the number of passes and space exhibited by
our algorithm is in fact tight, at least when the approximation factor is equal
to $1$. Specifically, we show that any algorithm that computes set cover
exactly using $({1 \over 2\delta}-1)$ passes must use $\tilde{\Omega}(m
n^{\delta})$ space.
  Finally, we show that any randomized one-pass algorithm that distinguishes
between covers of size 2 and 3 must use a linear (i.e., $\Omega(mn)$) amount of
space. The only previous randomized lower bound for Set Cover, due to
[Nisan02], provided only a sub-linear lower bound of $\Omega(m)$. This
indicates that using multiple passes might be necessary in order to achieve
sub-linear space bounds for this problem while guaranteeing small approximation
factors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00123</identifier>
 <datestamp>2016-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00123</id><created>2015-08-31</created><updated>2016-01-20</updated><authors><author><keyname>Huang</keyname><forenames>Yifei</forenames></author><author><keyname>Nasir</keyname><forenames>Ali A.</forenames></author><author><keyname>Durrani</keyname><forenames>Salman</forenames></author><author><keyname>Zhou</keyname><forenames>Xiangyun</forenames></author></authors><title>Mode Selection, Resource Allocation and Power Control for D2D-Enabled
  Two-Tier Cellular Network</title><categories>cs.IT math.IT</categories><comments>Submitted for possible journal publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a centralized decision making framework at the macro base
station (MBS) for device to device (D2D) communication underlaying a two-tier
cellular network. We consider a D2D pair in the presence of an MBS and a femto
access point, each serving a user, with quality of service constraints for all
users. Our proposed solution encompasses mode selection (choosing between
cellular or reuse or dedicated mode), resource allocation (in cellular and
dedicated mode) and power control (in reuse mode) within a single framework.
The framework prioritizes D2D dedicated mode if the D2D pair are close to each
other and orthogonal resources are available. Otherwise, it allows D2D reuse
mode if the D2D satisfies both the maximum distance and an additional
interference criteria. For reuse mode, we present a geometric vertex search
approach to solve the power allocation problem. We analytically prove the
validity of this approach and show that it achieves near optimal performance.
For cellular and dedicated modes, we show that frequency sharing maximizes sum
rate and solve the resource allocation problem in closed form. Our simulations
demonstrate the advantages of the proposed framework in terms of the
performance gains achieved in D2D mode.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00126</identifier>
 <datestamp>2015-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00126</id><created>2015-08-31</created><updated>2015-09-02</updated><authors><author><keyname>Song</keyname><forenames>Yangbo</forenames></author><author><keyname>van der Schaar</keyname><forenames>Mihaela</forenames></author></authors><title>Dynamic Network Formation with Foresighted Agents</title><categories>cs.GT cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  What networks can form and persist when agents are self-interested? Can such
networks be efficient? A substantial theoretical literature predicts that the
only networks that can form and persist must have very special shapes and that
such networks cannot be efficient, but these predictions are in stark contrast
to empirical findings. In this paper, we present a new model of network
formation. In contrast to the existing literature, our model is dynamic (rather
than static), we model agents as foresighted (rather than myopic) and we allow
for the possibility that agents are heterogeneous (rather than homogeneous). We
show that a very wide variety of networks can form and persist; in particular,
efficient networks can form and persist if they provide every agent a strictly
positive payoff. For the widely-studied connections model, we provide a full
characterization of the set of efficient networks that can form and persist.
Our predictions are consistent with empirical findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00127</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00127</id><created>2015-08-31</created><authors><author><keyname>Rivest</keyname><forenames>Ronald L.</forenames></author></authors><title>DiffSum - A Simple Post-Election Risk-Limiting Audit</title><categories>cs.CY</categories><comments>1 page</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents DiffSum, a simple post-election risk-limiting
ballot-polling audit for two-candidate plurality elections. DiffSum
sequentially draws ballots (without replacement) until the numbers $a$, $b$, of
votes for candidates $A$, $B$ satisfies $a&gt;b$ and $(a-b)^2 &gt; c(a+b)$, where $A$
is the reported winner and $c$ is a suitably chosen constant. Bounds on the
error rate (chance of approving an incorrect election outcome) are obtained via
simulations. The method is compared with the Bravo method of Lindeman, Stark,
and Yates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00130</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00130</id><created>2015-08-31</created><authors><author><keyname>Song</keyname><forenames>Ruiyang</forenames></author><author><keyname>Xie</keyname><forenames>Yao</forenames></author><author><keyname>Pokutta</keyname><forenames>Sebastian</forenames></author></authors><title>Sequential Information Guided Sensing</title><categories>cs.IT math.IT math.ST stat.ML stat.TH</categories><comments>Submitted for journal publication. arXiv admin note: substantial text
  overlap with arXiv:1501.06241</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the value of information in sequential compressed sensing by
characterizing the performance of sequential information guided sensing in
practical scenarios when information is inaccurate. In particular, we assume
the signal distribution is parameterized through Gaussian or Gaussian mixtures
with estimated mean and covariance matrices, and we can measure compressively
through a noisy linear projection or using one-sparse vectors, i.e., observing
one entry of the signal each time. We establish a set of performance bounds for
the bias and variance of the signal estimator via posterior mean, by capturing
the conditional entropy (which is also related to the size of the uncertainty),
and the additional power required due to inaccurate information to reach a
desired precision. Based on this, we further study how to estimate covariance
based on direct samples or covariance sketching. Numerical examples also
demonstrate the superior performance of Info-Greedy Sensing algorithms compared
with their random and non-adaptive counterparts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00137</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00137</id><created>2015-09-01</created><authors><author><keyname>Xie</keyname><forenames>Yao</forenames></author><author><keyname>Song</keyname><forenames>Ruiyang</forenames></author><author><keyname>Dai</keyname><forenames>Hanjun</forenames></author><author><keyname>Li</keyname><forenames>Qingbin</forenames></author><author><keyname>Song</keyname><forenames>Le</forenames></author></authors><title>Online Supervised Subspace Tracking</title><categories>cs.LG math.ST stat.ML stat.TH</categories><comments>Submitted for journal publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a framework for supervised subspace tracking, when there are two
time series $x_t$ and $y_t$, one being the high-dimensional predictors and the
other being the response variables and the subspace tracking needs to take into
consideration of both sequences. It extends the classic online subspace
tracking work which can be viewed as tracking of $x_t$ only. Our online
sufficient dimensionality reduction (OSDR) is a meta-algorithm that can be
applied to various cases including linear regression, logistic regression,
multiple linear regression, multinomial logistic regression, support vector
machine, the random dot product model and the multi-scale union-of-subspace
model. OSDR reduces data-dimensionality on-the-fly with low-computational
complexity and it can also handle missing data and dynamic data. OSDR uses an
alternating minimization scheme and updates the subspace via gradient descent
on the Grassmannian manifold. The subspace update can be performed efficiently
utilizing the fact that the Grassmannian gradient with respect to the subspace
in many settings is rank-one (or low-rank in certain cases). The optimization
problem for OSDR is non-convex and hard to analyze in general; we provide
convergence analysis of OSDR in a simple linear regression setting. The good
performance of OSDR compared with the conventional unsupervised subspace
tracking are demonstrated via numerical examples on simulated and real data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00144</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00144</id><created>2015-09-01</created><authors><author><keyname>Baudry</keyname><forenames>Benoit</forenames></author><author><keyname>Allier</keyname><forenames>Simon</forenames></author><author><keyname>Rodriguez-Cancio</keyname><forenames>Marcelino</forenames></author><author><keyname>Monperrus</keyname><forenames>Martin</forenames></author></authors><title>Automatic Software Diversity in the Light of Test Suites</title><categories>cs.SE</categories><comments>11 pages, 4 figures, 8 listings, conference</comments><acm-class>D.2.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A few works address the challenge of automating software diversification, and
they all share one core idea: using automated test suites to drive
diversification. However, there is is lack of solid understanding of how test
suites, programs and transformations interact one with another in this process.
We explore this intricate interplay in the context of a specific
diversification technique called &quot;sosiefication&quot;.
  Sosiefication generates sosie programs, i.e., variants of a program in which
some statements are deleted, added or replaced but still pass the test suite of
the original program. Our investigation of the influence of test suites on
sosiefication exploits the following observation: test suites cover the
different regions of programs in very unequal ways. Hence, we hypothesize that
sosie synthesis has different performances on a statement that is covered by
one hundred test case and on a statement that is covered by a single test case.
We synthesize 24583 sosies on 6 popular open-source Java programs. Our results
show that there are two dimensions for diversification. The first one lies in
the specification: the more test cases cover a statement, the more difficult it
is to synthesize sosies. Yet, to our surprise, we are also able to synthesize
sosies on highly tested statements (up to 600 test cases), which indicates an
intrinsic property of the programs we study. The second dimension is in the
code: we manually explore dozens of sosies and characterize new types of
forgiving code regions that are prone to diversification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00149</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00149</id><created>2015-09-01</created><authors><author><keyname>Kova&#x159;&#xed;k</keyname><forenames>Vojt&#x11b;ch</forenames></author><author><keyname>Lis&#xfd;</keyname><forenames>Viliam</forenames></author></authors><title>Analysis of Hannan Consistent Selection for Monte Carlo Tree Search in
  Simultaneous Move Games</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Monte Carlo Tree Search (MCTS) has recently been successfully used to create
strategies for playing imperfect-information games. Despite its popularity,
there are no theoretic results that guarantee its convergence to a well-defined
solution, such as Nash equilibrium, in these games. We partially fill this gap
by analysing MCTS in the class of zero-sum extensive-form games with
simultaneous moves but otherwise perfect information. The lack of information
about the opponent's concurrent moves already causes that optimal strategies
may require randomization. We present theoretic as well as empirical
investigation of the speed and quality of convergence of these algorithms to
the Nash equilibria. Primarily, we show that after minor technical
modifications, MCTS based on any (approximately) Hannan consistent selection
function always converges to an (approximate) subgame perfect Nash equilibrium.
Without these modifications, Hannan consistency is not sufficient to ensure
such convergence and the selection function must satisfy additional properties,
which empirically hold for the most common Hannan consistent algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00151</identifier>
 <datestamp>2015-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00151</id><created>2015-09-01</created><updated>2015-10-16</updated><authors><author><keyname>Wang</keyname><forenames>Zhangyang</forenames></author><author><keyname>Chang</keyname><forenames>Shiyu</forenames></author><author><keyname>Zhou</keyname><forenames>Jiayu</forenames></author><author><keyname>Wang</keyname><forenames>Meng</forenames></author><author><keyname>Huang</keyname><forenames>Thomas S.</forenames></author></authors><title>Learning A Task-Specific Deep Architecture For Clustering</title><categories>cs.LG cs.CV stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While sparse coding-based clustering methods have shown to be successful,
their bottlenecks in both efficiency and scalability limit the practical usage.
In recent years, deep learning has been proved to be a highly effective,
efficient and scalable feature learning tool. In this paper, we propose to
emulate the sparse coding-based clustering pipeline in the context of deep
learning, leading to a carefully crafted deep model benefiting from both. A
feed-forward network structure, named TAGnet, is constructed based on a
graph-regularized sparse coding algorithm. It is then trained with
task-specific loss functions from end to end. We discover that connecting deep
learning to sparse coding benefits not only the model performance, but also its
initialization and interpretation. Moreover, by introducing auxiliary
clustering tasks to the intermediate feature hierarchy, we formulate DTAGnet
and obtain a further performance boost. Extensive experiments demonstrate that
the proposed model gains remarkable margins over several state-of-the-art
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00153</identifier>
 <datestamp>2015-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00153</id><created>2015-09-01</created><updated>2015-11-22</updated><authors><author><keyname>Wang</keyname><forenames>Zhangyang</forenames></author><author><keyname>Ling</keyname><forenames>Qing</forenames></author><author><keyname>Huang</keyname><forenames>Thomas S.</forenames></author></authors><title>Learning Deep $\ell_0$ Encoders</title><categories>cs.LG stat.ML</categories><comments>Full paper at AAAI 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite its nonconvex nature, $\ell_0$ sparse approximation is desirable in
many theoretical and application cases. We study the $\ell_0$ sparse
approximation problem with the tool of deep learning, by proposing Deep
$\ell_0$ Encoders. Two typical forms, the $\ell_0$ regularized problem and the
$M$-sparse problem, are investigated. Based on solid iterative algorithms, we
model them as feed-forward neural networks, through introducing novel neurons
and pooling functions. Enforcing such structural priors acts as an effective
network regularization. The deep encoders also enjoy faster inference, larger
learning capacity, and better scalability compared to conventional sparse
coding solutions. Furthermore, under task-driven losses, the models can be
conveniently optimized from end to end. Numerical results demonstrate the
impressive performances of the proposed encoders.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00154</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00154</id><created>2015-09-01</created><updated>2015-10-14</updated><authors><author><keyname>Rasekhi</keyname><forenames>Jalil</forenames></author></authors><title>Tumor Motion Tracking in Liver Ultrasound Images Using Mean Shift and
  Active Contour</title><categories>cs.CV stat.ML</categories><comments>This paper has been withdrawn by the author due to a crucial sign
  error in equation 5,6</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a new method for motion tracking of tumors in liver
ultrasound image sequences. Our algorithm has two main steps. In the first
step, we apply mean shift algorithm with multiple features to estimate the
center of the target in each frame. Target in the first frame is defined using
an ellipse. Edge, texture, and intensity features are extracted from the first
frame, and then mean shift algorithm is applied to each feature separately to
find the center of ellipse related to that feature in the next frame. The
center of ellipse will be the weighted average of these centers. By using mean
shift actually we estimate the target movement between two consecutive frames.
Once the correct ellipsoid in each frame is known, in the second step we apply
the Dynamic Directional Gradient Vector Flow (DDGVF) version of active contour
models, in order to find the correct boundary of tumors. We sample a few points
on the boundary of active contour then translate those points based on the
translation of the center of ellipsoid in two consecutive frames to determine
the target movement. We use these translated sample points as an initial guess
for active contour in the next frame. Our experimental results show that, the
suggested method provides a reliable performance for liver tumor tracking in
ultrasound image sequences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00159</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00159</id><created>2015-09-01</created><updated>2015-09-07</updated><authors><author><keyname>Lv</keyname><forenames>Zhihan</forenames></author><author><keyname>Li</keyname><forenames>Xiaoming</forenames></author></authors><title>Preprint Virtual Reality Assistant Technology for Learning Primary
  Geography</title><categories>cs.HC</categories><comments>This is the preprint version of our paper on ICWL2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is the preprint version of our paper on ICWL2015. A virtual reality
based enhanced technology for learning primary geography is proposed, which
synthesizes several latest information technologies including virtual
reality(VR), 3D geographical information system(GIS), 3D visualization and
multimodal human-computer-interaction (HCI). The main functions of the proposed
system are introduced, i.e. Buffer analysis, Overlay analysis, Space convex
hull calculation, Space convex decomposition, 3D topology analysis and 3D space
intersection detection. The multimodal technologies are employed in the system
to enhance the immersive perception of the users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00164</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00164</id><created>2015-09-01</created><authors><author><keyname>Salehi</keyname><forenames>Saeed</forenames></author></authors><title>Theorems of Tarski's Undefinability and Godel's Second Incompleteness -
  Computationally</title><categories>math.LO cs.LO</categories><comments>10 pages</comments><msc-class>03F40, 03B25, 03D35, 03A05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the existence of a finitely axiomatized theory which can prove
all the true $\Sigma_1$ sentences may imply Godel's Second Incompleteness
Theorem, by incorporating some bi-theoretic version of the derivability
conditions (first discussed by Detlefsen~2001). We also argue that Tarski's
theorem on the undefinability of truth is Godel's first incompleteness theorem
relativized to definable oracles; here a unification of these two theorems is
shown.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00166</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00166</id><created>2015-09-01</created><authors><author><keyname>Miridakis</keyname><forenames>Nikolaos I.</forenames></author><author><keyname>Vergados</keyname><forenames>Dimitrios D.</forenames></author><author><keyname>Michalas</keyname><forenames>Angelos</forenames></author></authors><title>Cooperative Relaying in Underlay Cognitive Systems with Hardware
  Impairments</title><categories>cs.IT math.IT</categories><comments>To be published in AEU - International Journal of Electronics and
  Communications (Elsevier)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The performance of an underlay cognitive (secondary) dual-hop relaying system
with multiple antennas and hardware impairments at each transceiver is
investigated. In particular, the outage probability of the end-to-end (e2e)
communication is derived in closed-form, when either transmit antenna selection
with maximum ratio combining (TAS/MRC), or TAS with selection combining
(TAS/SC) are established in each hop. Simplified asymptotic outage expressions
are also obtained, which manifest the diversity and array order of the system,
the effectiveness of the balance on the number of transmit/receive antennas,
and the impact of hardware impairments to the e2e communication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00167</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00167</id><created>2015-09-01</created><authors><author><keyname>Karzand</keyname><forenames>Mohammad</forenames></author><author><keyname>Leith</keyname><forenames>Douglas J.</forenames></author><author><keyname>Cloud</keyname><forenames>Jason</forenames></author><author><keyname>Medard</keyname><forenames>Muriel</forenames></author></authors><title>Low Delay Random Linear Coding Over a Stream</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider use of FEC to reduce in-order delivery delay over packet erasure
channels. We propose a class of streaming codes that is capacity achieving and
provides a superior throughput-delay trade-off compared to block codes by
introducing flexibility in where and when redundancy is placed. This
flexibility results in significantly lower in-order delay for a given
throughput for a wide range of network scenarios. Furthermore, a major
contribution of this paper is the combination of queuing and coding theory to
analyze the code's performance. Finally, we present simulation and experimental
results illustrating the code's benefits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00174</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00174</id><created>2015-09-01</created><authors><author><keyname>Brunato</keyname><forenames>Mauro</forenames></author><author><keyname>Battiti</keyname><forenames>Roberto</forenames></author></authors><title>A Telescopic Binary Learning Machine for Training Neural Networks</title><categories>cs.NE</categories><comments>Submitted to IEEE Transactions on Neural Networks and Learning
  Systems, special issue on New Developments in Neural Network Structures for
  Signal Processing, Autonomous Decision, and Adaptive Control</comments><acm-class>I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a new algorithm based on multi-scale stochastic local
search with binary representation for training neural networks.
  In particular, we study the effects of neighborhood evaluation strategies,
the effect of the number of bits per weight and that of the maximum weight
range used for mapping binary strings to real values. Following this
preliminary investigation, we propose a telescopic multi-scale version of local
search where the number of bits is increased in an adaptive manner, leading to
a faster search and to local minima of better quality. An analysis related to
adapting the number of bits in a dynamic way is also presented. The control on
the number of bits, which happens in a natural manner in the proposed method,
is effective to increase the generalization performance. Benchmark tasks
include a highly non-linear artificial problem, a control problem requiring
either feed-forward or recurrent architectures for feedback control, and
challenging real-world tasks in different application domains.
  The results demonstrate the effectiveness of the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00181</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00181</id><created>2015-09-01</created><updated>2016-02-01</updated><authors><author><keyname>Zhou</keyname><forenames>Pan</forenames></author><author><keyname>Zhou</keyname><forenames>Yingxue</forenames></author><author><keyname>Wu</keyname><forenames>Dapeng</forenames></author><author><keyname>Jin</keyname><forenames>Hai</forenames></author></authors><title>Differentially Private Online Learning for Cloud-Based Video
  Recommendation with Multimedia Big Data in Social Networks</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the rapid growth in multimedia services and the enormous offers of video
contents in online social networks, users have difficulty in obtaining their
interests. Therefore, various personalized recommendation systems have been
proposed. However, they ignore that the accelerated proliferation of social
media data has led to the big data era, which has greatly impeded the process
of video recommendation. In addition, none of them has considered both the
privacy of users' contexts (e,g., social status, ages and hobbies) and video
service vendors' repositories, which are extremely sensitive and of significant
commercial value. To handle the problems, we propose a cloud-assisted
differentially private video recommendation system based on distributed online
learning. In our framework, service vendors are modeled as distributed
cooperative learners, recommending videos according to user's context, while
simultaneously adapting the video-selection strategy based on user-click
feedback to maximize total user clicks (reward). Considering the sparsity and
heterogeneity of big social media data, we also propose a novel geometric
differentially private model, which can greatly reduce the performance
(recommendation accuracy) loss. Our simulation shows the proposed algorithms
outperform other existing methods and keep a delicate balance between computing
accuracy and privacy preserving level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00189</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00189</id><created>2015-09-01</created><updated>2015-12-21</updated><authors><author><keyname>Del Vicario</keyname><forenames>Michela</forenames></author><author><keyname>Bessi</keyname><forenames>Alessandro</forenames></author><author><keyname>Zollo</keyname><forenames>Fabiana</forenames></author><author><keyname>Petroni</keyname><forenames>Fabio</forenames></author><author><keyname>Scala</keyname><forenames>Antonio</forenames></author><author><keyname>Caldarelli</keyname><forenames>Guido</forenames></author><author><keyname>Stanley</keyname><forenames>H. Eugene</forenames></author><author><keyname>Quattrociocchi</keyname><forenames>Walter</forenames></author></authors><title>Echo chambers in the age of misinformation</title><categories>cs.CY cs.HC cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The wide availability of user-provided content in online social media
facilitates the aggregation of people around common interests, worldviews, and
narratives. Despite the enthusiastic rhetoric on the part of some that this
process generates &quot;collective intelligence&quot;, the WWW also allows the rapid
dissemination of unsubstantiated conspiracy theories that often elicite rapid,
large, but naive social responses such as the recent case of Jade Helm 15 --
where a simple military exercise turned out to be perceived as the beginning of
the civil war in the US. We study how Facebook users consume information
related to two different kinds of narrative: scientific and conspiracy news. We
find that although consumers of scientific and conspiracy stories present
similar consumption patterns with respect to content, the sizes of the
spreading cascades differ. Homogeneity appears to be the primary driver for the
diffusion of contents, but each echo chamber has its own cascade dynamics. To
mimic these dynamics, we introduce a data-driven percolation model on signed
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00190</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00190</id><created>2015-09-01</created><authors><author><keyname>Stolz</keyname><forenames>Alex</forenames></author><author><keyname>Hepp</keyname><forenames>Martin</forenames></author></authors><title>GR2RSS: Publishing Linked Open Commerce Data as RSS and Atom Feeds</title><categories>cs.IR cs.AI</categories><comments>Technical report, 5 pages, 2 figures</comments><report-no>TR-2014-1</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The integration of Linked Open Data (LOD) content in Web pages is a
challenging and sometimes tedious task for Web developers. At the same moment,
most software packages for blogs, content management systems (CMS), and shop
applications support the consumption of feed formats, namely RSS and Atom. In
this technical report, we demonstrate an on-line tool that fetches e-commerce
data from a SPARQL endpoint and syndicates obtained results as RSS or Atom
feeds. Our approach combines (1) the popularity and broad tooling support of
existing feed formats, (2) the precision of queries against structured data
built upon common Web vocabularies like schema.org, GoodRelations, FOAF, VCard,
and WGS 84, and (3) the ease of integrating content from a large number of Web
sites and other data sources in RDF in general.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00202</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00202</id><created>2015-09-01</created><authors><author><keyname>Savic</keyname><forenames>Vladimir</forenames></author><author><keyname>Larsson</keyname><forenames>Erik G.</forenames></author></authors><title>Fingerprinting-Based Positioning in Distributed Massive MIMO Systems</title><categories>cs.IT cs.LG math.IT</categories><comments>Proc. of IEEE 82nd Vehicular Technology Conference (VTC2015-Fall)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Location awareness in wireless networks may enable many applications such as
emergency services, autonomous driving and geographic routing. Although there
are many available positioning techniques, none of them is adapted to work with
massive multiple-in-multiple-out (MIMO) systems, which represent a leading 5G
technology candidate. In this paper, we discuss possible solutions for
positioning of mobile stations using a vector of signals at the base station,
equipped with many antennas distributed over deployment area. Our main proposal
is to use fingerprinting techniques based on a vector of received signal
strengths. This kind of methods are able to work in highly-cluttered multipath
environments, and require just one base station, in contrast to standard
range-based and angle-based techniques. We also provide a solution for
fingerprinting-based positioning based on Gaussian process regression, and
discuss main applications and challenges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00206</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00206</id><created>2015-09-01</created><authors><author><keyname>Matthysen</keyname><forenames>Roel</forenames></author><author><keyname>Huybrechs</keyname><forenames>Daan</forenames></author></authors><title>Fast Algorithms for the computation of Fourier Extensions of arbitrary
  length</title><categories>cs.NA math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fourier series of smooth, non-periodic functions on $[-1,1]$ are known to
exhibit the Gibbs phenomenon, and exhibit overall slow convergence. One way of
overcoming these problems is by using a Fourier series on a larger domain, say
$[-T,T]$ with $T&gt;1$, a technique called Fourier extension or Fourier
continuation. When constructed as the discrete least squares minimizer in
equidistant points, the Fourier extension has been shown shown to converge
geometrically in the truncation parameter $N$. A fast ${\mathcal O}(N \log^2
N)$ algorithm has been described to compute Fourier extensions for the case
where $T=2$, compared to ${\mathcal O}(N^3)$ for solving the dense discrete
least squares problem. We present two ${\mathcal O}(N\log^2 N )$ algorithms for
the computation of these approximations for the case of general $T$, made
possible by exploiting the connection between Fourier extensions and Prolate
Spheroidal Wave theory. The first algorithm is based on the explicit
computation of so-called periodic discrete prolate spheroidal sequences, while
the second algorithm is purely algebraic and only implicitly based on the
theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00224</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00224</id><created>2015-09-01</created><updated>2016-02-05</updated><authors><author><keyname>Lee</keyname><forenames>Eun</forenames></author><author><keyname>Holme</keyname><forenames>Petter</forenames></author></authors><title>Impact of mobility structure on the optimization of small-world networks
  of mobile agents</title><categories>physics.soc-ph cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In ad hoc wireless networking, units are connected to each other rather than
to a central, fixed, infrastructure. Constructing and maintaining such networks
create several trade-off problems between robustness, communication speed,
power consumption, etc., that bridges engineering, computer science and the
physics of complex systems. In this work, we address the role of mobility
patterns of the agents on the optimal tuning of a small-world type network
construction method. By this method, the network is updated periodically and
held static between the updates. We investigate the optimal updating times for
different scenarios of the movement of agents (modeling, for example, the
fat-tailed trip distances, and periodicities, of human travel). We find that
these mobility patterns affect the power consumption in non-trivial ways and
discuss how these effects can best be handled.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00236</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00236</id><created>2015-09-01</created><authors><author><keyname>Tiamiyu</keyname><forenames>Osuolale Abdulrahamon</forenames></author></authors><title>Trusted routing vs. VPN for secured data transfer over
  IP-networks/Internet</title><categories>cs.NI</categories><comments>arXiv admin note: substantial text overlap with arXiv:1507.06538</comments><journal-ref>Proceedings of conference &quot; fundamental and applied science in
  modern world &quot;, Saint-Petersburg, 20-22 June 2013. To-Future, 2013. pp.63-70</journal-ref><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Though objectives of trusted routing and virtual private networks (VPN) data
transfer methods are to guarantee data transfer securely to from senders to
receivers over public networks like Internet yet there are paramount
differences between the two methods. This paper analyses their differences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00238</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00238</id><created>2015-09-01</created><authors><author><keyname>Savic</keyname><forenames>Vladimir</forenames></author><author><keyname>Wymeersch</keyname><forenames>Henk</forenames></author><author><keyname>Larsson</keyname><forenames>Erik G.</forenames></author></authors><title>Target Tracking in Confined Environments with Uncertain Sensor Positions</title><categories>cs.OH</categories><comments>IEEE Transactions on Vehicular Technology, 2015</comments><doi>10.1109/TVT.2015.2404132</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To ensure safety in confined environments such as mines or subway tunnels, a
(wireless) sensor network can be deployed to monitor various environmental
conditions. One of its most important applications is to track personnel,
mobile equipment and vehicles. However, the state-of-the-art algorithms assume
that the positions of the sensors are perfectly known, which is not necessarily
true due to imprecise placement and/or dropping of sensors. Therefore, we
propose an automatic approach for simultaneous refinement of sensors' positions
and target tracking. We divide the considered area in a finite number of cells,
define dynamic and measurement models, and apply a discrete variant of belief
propagation which can efficiently solve this high-dimensional problem, and
handle all non-Gaussian uncertainties expected in this kind of environments.
Finally, we use ray-tracing simulation to generate an artificial mine-like
environment and generate synthetic measurement data. According to our extensive
simulation study, the proposed approach performs significantly better than
standard Bayesian target tracking and localization algorithms, and provides
robustness against outliers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00239</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00239</id><created>2015-09-01</created><authors><author><keyname>Blocki</keyname><forenames>Jeremiah</forenames></author><author><keyname>Datta</keyname><forenames>Anupam</forenames></author></authors><title>CASH: A Cost Asymmetric Secure Hash Algorithm for Optimal Password
  Protection</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An adversary who has obtained the cryptographic hash of a user's password can
mount an offline attack to crack the password by comparing this hash value with
the cryptographic hashes of likely password guesses. This offline attacker is
limited only by the resources he is willing to invest to crack the password.
Techniques like hash iteration have been proposed to mitigate the threat of
offline attacks by making each password guess more expensive for the adversary
to verify. However, these techniques also increase costs for a legitimate
authentication server. Motivated by ideas from game theory we introduce Cost
Asymmetric Secure Hash (CASH) --- a mechanism for protecting passwords against
offline attacks while minimizing cost increases for the legitimate
authentication server. Our basic insight is that the legitimate authentication
server will typically use CASH to verify a correct password, while an offline
adversary will typically evaluate CASH with incorrect password guesses. We use
randomization to ensure that the amortized cost of running CASH to verify a
correct password guess is significantly smaller than the cost of rejecting an
incorrect password. We model the interaction between the defender and the
adversary as a Stackelberg game in which the defender commits to a CASH
distribution and the adversary plays her best response to it. We provide an
efficient algorithm for computing the defender's optimal strategy. Finally, we
analyze CASH using empirical data from two large scale password breaches. Our
analysis shows that CASH can significantly reduce (up to 21%) the probability
that an adversary with a fixed budget would be able to crack a random user's
password. In concrete terms this means that 6.9 million fewer RockYou customers
would have their passwords cracked in an offline attack if RockYou had used
CASH than if RockYou had used traditional key-stretching.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00243</identifier>
 <datestamp>2016-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00243</id><created>2015-09-01</created><updated>2016-01-07</updated><authors><author><keyname>Xu</keyname><forenames>Shengfeng</forenames></author><author><keyname>Zhu</keyname><forenames>Gang</forenames></author></authors><title>Location-Aware Dynamic Resource Management for High-Speed Railway
  Wireless Communications</title><categories>cs.NI</categories><comments>The reason for withdrawal is that this paper has been updated in our
  new paper, see http://arxiv.org/abs/1601.01526</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the fast development of high-speed railway (HSR), the demand for mobile
communication on high-speed trains is increasingly growing. This leads to
significant attention on the study of resource management in HSR wireless
communications with limited bandwidth. Resource management is a challenging
problem due to heterogenous quality of service (QoS) requirements and dynamic
characteristics of HSR wireless communications. In this article, we first
provide a state-of-the-art overview on HSR wireless communications and the
existing resource management schemes. Then a location-aware cross-layer
optimization framework is developed for the dynamic resource management with
realistic system model, where the train location information is exploited to
facilitate dynamic design. Next we demonstrate the applications of stochastic
network optimization theory in solving the dynamic resource management problem
in HSR wireless communications. Finally, we highlight some future research
directions for location-aware dynamic resource management in the conclusion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00244</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00244</id><created>2015-09-01</created><authors><author><keyname>Ding</keyname><forenames>Changxing</forenames></author><author><keyname>Tao</keyname><forenames>Dacheng</forenames></author></authors><title>Robust Face Recognition via Multimodal Deep Face Representation</title><categories>cs.CV</categories><comments>To appear in IEEE Trans. Multimedia</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Face images appeared in multimedia applications, e.g., social networks and
digital entertainment, usually exhibit dramatic pose, illumination, and
expression variations, resulting in considerable performance degradation for
traditional face recognition algorithms. This paper proposes a comprehensive
deep learning framework to jointly learn face representation using multimodal
information. The proposed deep learning structure is composed of a set of
elaborately designed convolutional neural networks (CNNs) and a three-layer
stacked auto-encoder (SAE). The set of CNNs extracts complementary facial
features from multimodal data. Then, the extracted features are concatenated to
form a high-dimensional feature vector, whose dimension is compressed by SAE.
All the CNNs are trained using a subset of 9,000 subjects from the publicly
available CASIA-WebFace database, which ensures the reproducibility of this
work. Using the proposed single CNN architecture and limited training data,
98.43% verification rate is achieved on the LFW database. Benefited from the
complementary information contained in multimodal data, our small ensemble
system achieves higher than 99.0% recognition rate on LFW using publicly
available training set.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00249</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00249</id><created>2015-09-01</created><authors><author><keyname>Even</keyname><forenames>Guy</forenames></author><author><keyname>Fais</keyname><forenames>Yaniv</forenames></author></authors><title>Algorithms for Network-on-Chip Design with Guaranteed QoS</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present algorithms that design NoCs with guaranteed quality of service.
Given a topology, a mapping of tasks to processing elements, and traffic
requirements between the tasks, the algorithm computes the interconnection
widths, a detailed static routing, and a periodic scheduling. No headers,
control messages, or acknowledgments are required. The algorithm employs
fractional Multi-Commodity Flow (MCF) that determines the widths of the
interconnections as well as the routes of the flits. The MCF is rounded to a
periodic TDM schedule which is translated to local periodic control of switches
and network interfaces. Our algorithm is applicable to large instances since
every stage is efficient. The algorithm supports arbitrary topologies and
traffic patterns. Routing along multiple paths is allowed in order to increase
utilization and decrease latency. We implemented the algorithm and tested it
with the MCSL benchmark. Experiments demonstrate that our solution is stable
and satisfies all the real-time constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00257</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00257</id><created>2015-09-01</created><authors><author><keyname>Karafyllis</keyname><forenames>Iasson</forenames></author><author><keyname>Kontorinaki</keyname><forenames>Maria</forenames></author><author><keyname>Papageorgiou</keyname><forenames>Markos</forenames></author></authors><title>Robust Global Adaptive Exponential Stabilization of Discrete-Time
  Systems with Application to Freeway Traffic Control</title><categories>math.OC cs.SY</categories><comments>18 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is devoted to the development of adaptive control schemes for
uncertain discrete-time systems, which guarantee robust, global, exponential
convergence to the desired equilibrium point of the system. The proposed
control scheme consists of a nominal feedback law, which achieves robust,
global, exponential stability properties when the vector of the parameters is
known, in conjunction with a nonlinear, dead-beat observer. The obtained
results are applicable to highly nonlinear, uncertain discrete-time systems
with unknown constant parameters. The applicability of the obtained results to
real control problems is demonstrated by the rigorous application of the
proposed adaptive control scheme to uncertain freeway models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00260</identifier>
 <datestamp>2016-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00260</id><created>2015-09-01</created><updated>2016-01-13</updated><authors><author><keyname>Dekking</keyname><forenames>F. Michel</forenames></author></authors><title>Morphisms, Symbolic sequences, and their Standard Forms</title><categories>math.CO cs.FL</categories><msc-class>Primary 68R15, Secondary 37B10, 11B85</msc-class><journal-ref>Journal of Integer Sequences Vol. 19 (2016), Article 16.1.1</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Morphisms are homomorphisms under the concatenation operation of the set of
words over a finite set. Changing the elements of the finite set does not
essentially change the morphism. We propose a way to select a unique
representing member out of all these morphisms. This has applications to the
classification of the shift dynamical systems generated by morphisms. In a
similar way, we propose the selection of a representing sequence out of the
class of symbolic sequences over an alphabet of fixed cardinality. Both methods
are useful for the storing of symbolic sequences in databases, like The On-Line
Encyclopedia of Integer Sequences. We illustrate our proposals with the
$k$-symbol Fibonacci sequences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00268</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00268</id><created>2015-09-01</created><updated>2016-01-27</updated><authors><author><keyname>Kallitsis</keyname><forenames>Michael</forenames></author><author><keyname>Stoev</keyname><forenames>Stilian</forenames></author><author><keyname>Bhattacharya</keyname><forenames>Shrijita</forenames></author><author><keyname>Michailidis</keyname><forenames>George</forenames></author></authors><title>AMON: An Open Source Architecture for Online Monitoring, Statistical
  Analysis and Forensics of Multi-gigabit Streams</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Internet, as a global system of interconnected networks, carries an
extensive array of information resources and services. Key requirements include
good quality-of-service and protection of the infrastructure from nefarious
activity (e.g. distributed denial of service--DDoS--attacks). Network
monitoring is essential to network engineering, capacity planning and
prevention / mitigation of threats. We develop an open source architecture,
AMON (All-packet MONitor), for online monitoring and analysis of multi-gigabit
network streams. It leverages the high-performance packet monitor PF RING and
is readily deployable on commodity hardware. AMON examines all packets,
partitions traffic into sub-streams by using rapid hashing and computes certain
real-time data products. The resulting data structures provide views of the
intensity and connectivity structure of network traffic at the time-scale of
routing. The proposed integrated framework includes modules for the
identification of heavy-hitters as well as for visualization and statistical
detection at the time-of-onset of high impact events such as DDoS. This allows
operators to quickly visualize and diagnose attacks, and limit offline and time
consuming post-mortem analysis. We demonstrate our system in the context of
real-world attack incidents, and validate it against state-of-the-art
alternatives. AMON has been deployed and is currently processing 10Gbps+ live
Internet traffic at Merit Network. It is extensible and allows the addition of
further statistical and filtering modules for real-time forensics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00269</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00269</id><created>2015-09-01</created><authors><author><keyname>Despr&#xe9;</keyname><forenames>Vincent</forenames></author><author><keyname>Lazarus</keyname><forenames>Francis</forenames></author></authors><title>Some Triangulated Surfaces without Balanced Splitting</title><categories>cs.CG cs.DM</categories><comments>15 pages, 7 figures</comments><msc-class>05C10, 57M07, 68R99</msc-class><acm-class>F.2.2; G.2.1; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let G be the graph of a triangulated surface $\Sigma$ of genus $g\geq 2$. A
cycle of G is splitting if it cuts $\Sigma$ into two components, neither of
which is homeomorphic to a disk. A splitting cycle has type k if the
corresponding components have genera k and g-k. It was conjectured that G
contains a splitting cycle (Barnette '1982). We confirm this conjecture for an
infinite family of triangulations by complete graphs but give counter-examples
to a stronger conjecture (Mohar and Thomassen '2001) claiming that G should
contain splitting cycles of every possible type.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00279</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00279</id><created>2015-09-01</created><authors><author><keyname>Augot</keyname><forenames>Daniel</forenames><affiliation>GRACE</affiliation></author><author><keyname>Levy-Dit-Vehel</keyname><forenames>Fran&#xe7;oise</forenames><affiliation>ENSTA ParisTech UMA</affiliation></author><author><keyname>Ng&#xf4;</keyname><forenames>Man Cuong</forenames><affiliation>GRACE</affiliation></author></authors><title>Information Sets of Multiplicity Codes</title><categories>cs.IT cs.CC cs.CR math.IT</categories><comments>International Symposium on Information Theory, Jun 2015, Hong-Kong,
  China. IEEE</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We here provide a method for systematic encoding of the Multiplicity codes
introduced by Kopparty, Saraf and Yekhanin in 2011. The construction is built
on an idea of Kop-party. We properly define information sets for these codes
and give detailed proofs of the validity of Kopparty's construction, that use
generating functions. We also give a complexity estimate of the associated
encoding algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00290</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00290</id><created>2015-09-01</created><authors><author><keyname>Faridi</keyname><forenames>Azadeh</forenames></author><author><keyname>Bellalta</keyname><forenames>Boris</forenames></author><author><keyname>Checco</keyname><forenames>Alessandro</forenames></author></authors><title>Analysis of Dynamic Channel Bonding in Dense Networks of WLANs</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dynamic Channel Bonding (DCB) allows for the dynamic selection and use of
multiple contiguous basic channels in Wireless Local Area Networks (WLANs). A
WLAN operating under DCB can enjoy a larger bandwidth, when available, and
therefore achieve a higher throughput. However, the use of larger bandwidths
also increases the contention with adjacent WLANs, which can result in longer
delays in accessing the channel and consequently, a lower throughput. In this
paper, a scenario consisting of multiple WLANs using DCB and operating within
carrier-sensing range of one another is considered. An analytical framework for
evaluating the performance of such networks is presented. The analysis is
carried out using a Markov chain model that characterizes the interactions
between adjacent WLANs with overlapping channels. An algorithm is proposed for
systematically constructing the Markov chain corresponding to any given
scenario. The analytical model is then used to highlight and explain the key
properties that differentiate DCB networks of WLANs from those operating on a
single shared channel. Furthermore, the analysis is applied to networks of IEEE
802.11ac WLANs operating under DCB--which do not fully comply with some of the
simplifying assumptions in our analysis--to show that the analytical model can
give accurate results in more realistic scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00291</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00291</id><created>2015-09-01</created><updated>2015-09-29</updated><authors><author><keyname>Weber</keyname><forenames>Jos H.</forenames></author><author><keyname>Immink</keyname><forenames>Kees A. Schouhamer</forenames></author><author><keyname>Blackburn</keyname><forenames>Simon R.</forenames></author></authors><title>Pearson codes</title><categories>cs.IT math.IT</categories><comments>17 pages. Minor revisions and corrections since previous version.
  Author biographies added. To appear in IEEE Trans. Inform. Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Pearson distance has been advocated for improving the error performance
of noisy channels with unknown gain and offset. The Pearson distance can only
fruitfully be used for sets of $q$-ary codewords, called Pearson codes, that
satisfy specific properties. We will analyze constructions and properties of
optimal Pearson codes. We will compare the redundancy of optimal Pearson codes
with the redundancy of prior art $T$-constrained codes, which consist of
$q$-ary sequences in which $T$ pre-determined reference symbols appear at least
once. In particular, it will be shown that for $q\le 3$ the $2$-constrained
codes are optimal Pearson codes, while for $q\ge 4$ these codes are not
optimal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00296</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00296</id><created>2015-09-01</created><authors><author><keyname>Oh</keyname><forenames>Tae-Hyun</forenames></author><author><keyname>Matsushita</keyname><forenames>Yasuyuki</forenames></author><author><keyname>Tai</keyname><forenames>Yu-Wing</forenames></author><author><keyname>Kweon</keyname><forenames>In So</forenames></author></authors><title>Fast Randomized Singular Value Thresholding for Nuclear Norm
  Minimization</title><categories>cs.CV</categories><comments>Appeared in CVPR 2015, and submitted to TPAMI. Source code is
  available on http://thoh.kaist.ac.kr</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rank minimization can be boiled down to tractable surrogate problems, such as
Nuclear Norm Minimization (NNM) and Weighted NNM (WNNM). The problems related
to NNM (or WNNM) can be solved iteratively by applying a closed-form proximal
operator, called Singular Value Thresholding (SVT) (or Weighted SVT), but they
suffer from high computational cost of computing Singular Value Decomposition
(SVD) at each iteration. We propose a fast and accurate approximation method
for SVT, that we call fast randomized SVT (FRSVT), where we avoid direct
computation of SVD. The key idea is to extract an approximate basis for the
range of a matrix from its compressed matrix. Given the basis, we compute the
partial singular values of the original matrix from a small factored matrix. In
addition, by adopting a range propagation technique, our method further speeds
up the extraction of approximate basis at each iteration. Our theoretical
analysis shows the relationship between the approximation bound of SVD and its
effect to NNM via SVT. Along with the analysis, our empirical results
quantitatively and qualitatively show that our approximation rarely harms the
convergence of the host algorithms. We assess the efficiency and accuracy of
our method on various vision problems, e.g., subspace clustering, weather
artifact removal, and simultaneous multi-image alignment and rectification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00309</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00309</id><created>2015-09-01</created><updated>2015-10-09</updated><authors><author><keyname>Calvin</keyname><forenames>Justus A.</forenames></author><author><keyname>Lewis</keyname><forenames>Cannada A.</forenames></author><author><keyname>Valeev</keyname><forenames>Edward F.</forenames></author></authors><title>Scalable Task-Based Algorithm for Multiplication of Block-Rank-Sparse
  Matrices</title><categories>cs.DC</categories><comments>8 pages, 6 figures, accepted to IA3 2015. arXiv admin note: text
  overlap with arXiv:1504.05046</comments><doi>10.1145/2833179.2833186</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A task-based formulation of Scalable Universal Matrix Multiplication
Algorithm (SUMMA), a popular algorithm for matrix multiplication (MM), is
applied to the multiplication of hierarchy-free, rank-structured matrices that
appear in the domain of quantum chemistry (QC). The novel features of our
formulation are: (1) concurrent scheduling of multiple SUMMA iterations, and
(2) fine-grained task-based composition. These features make it tolerant of the
load imbalance due to the irregular matrix structure and eliminate all
artifactual sources of global synchronization.Scalability of iterative
computation of square-root inverse of block-rank-sparse QC matrices is
demonstrated; for full-rank (dense) matrices the performance of our SUMMA
formulation usually exceeds that of the state-of-the-art dense MM
implementations (ScaLAPACK and Cyclops Tensor Framework).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00313</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00313</id><created>2015-09-01</created><authors><author><keyname>C.</keyname><forenames>Amit Kumar K.</forenames></author><author><keyname>Delannay</keyname><forenames>Damien</forenames></author><author><keyname>De Vleeschouwer</keyname><forenames>Christophe</forenames></author></authors><title>Iterative hypothesis testing for multi-object tracking in presence of
  features with variable reliability</title><categories>cs.CV</categories><comments>21 pages, 8 figures, submitted to CVIU: Special Issue on Visual
  Tracking</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper assumes prior detections of multiple targets at each time instant,
and uses a graph-based approach to connect those detections across time, based
on their position and appearance estimates. In contrast to most earlier works
in the field, our framework has been designed to exploit the appearance
features, even when they are only sporadically available, or affected by a
non-stationary noise, along the sequence of detections. This is done by
implementing an iterative hypothesis testing strategy to progressively
aggregate the detections into short trajectories, named tracklets.
Specifically, each iteration considers a node, named key-node, and investigates
how to link this key-node with other nodes in its neighborhood, under the
assumption that the target appearance is defined by the key-node appearance
estimate. This is done through shortest path computation in a temporal
neighborhood of the key-node. The approach is conservative in that it only
aggregates the shortest paths that are sufficiently better compared to
alternative paths. It is also multi-scale in that the size of the investigated
neighborhood is increased proportionally to the number of detections already
aggregated into the key-node. The multi-scale nature of the process and the
progressive relaxation of its conservativeness makes it both computationally
efficient and effective.
  Experimental validations are performed extensively on a toy example, a 15
minutes long multi-view basketball dataset, and other monocular pedestrian
datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00321</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00321</id><created>2015-09-01</created><updated>2015-10-19</updated><authors><author><keyname>O'Rourke</keyname><forenames>Joseph</forenames></author></authors><title>Spiral Unfoldings of Convex Polyhedra</title><categories>cs.CG cs.DM</categories><comments>22 pages, 36 figures, 4 references. Version 2 added a conjecture in a
  final section</comments><msc-class>52B10</msc-class><acm-class>F.2.2; I.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The notion of a spiral unfolding of a convex polyhedron, resulting by
flattening a special type of Hamiltonian cut-path, is explored. The Platonic
and Archimedian solids all have nonoverlapping spiral unfoldings, although
among generic polyhedra, overlap is more the rule than the exception. The
structure of spiral unfoldings is investigated, primarily by analyzing one
particular class, the polyhedra of revolution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00334</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00334</id><created>2015-09-01</created><authors><author><keyname>Lostanlen</keyname><forenames>Vincent</forenames></author><author><keyname>Mallat</keyname><forenames>St&#xe9;phane</forenames></author></authors><title>Transform\'ee en scattering sur la spirale temps-chroma-octave</title><categories>cs.SD</categories><comments>in French, 4 pages, 3 figures, presented at GRETSI 2015 in Lyon,
  France</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a scattering representation for the analysis and classification
of sounds. It is locally translation-invariant, stable to deformations in time
and frequency, and has the ability to capture harmonic structures. The
scattering representation can be interpreted as a convolutional neural network
which cascades a wavelet transform in time and along a harmonic spiral. We
study its application for the analysis of the deformations of the source-filter
model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00335</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00335</id><created>2015-09-01</created><authors><author><keyname>Janson</keyname><forenames>Thomas</forenames></author><author><keyname>Schindelhauer</keyname><forenames>Christian</forenames></author></authors><title>Receiving Pseudorandom PSK</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pseudorandom PSK [1] enables parallel communication on the same carrier
frequency and at the same time. We propose different signal processing methods
to receive data modulated with pseudorandom PSK. This includes correlation with
the carrier frequency which can be applied to signals in the kHz to MHz range
and signal processing in the intermediate frequency where the correlation with
the carrier frequency is performed analogous in the RF front end. We analyze
the computation complexity for signal processing with the parameters of symbol
length $T$ and number of repetitions of each symbol $K$ with pseudorandom PSK
and show that the number of operations for each sampling point is
$\Theta\left(K\right)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00337</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00337</id><created>2015-09-01</created><authors><author><keyname>Hoefer</keyname><forenames>Martin</forenames></author><author><keyname>Kesselheim</keyname><forenames>Thomas</forenames></author><author><keyname>Kodric</keyname><forenames>Bojana</forenames></author></authors><title>Smoothness for Simultaneous Composition of Mechanisms with Admission</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study social welfare of learning outcomes in mechanisms with admission. In
our repeated game there are $n$ bidders and $m$ mechanisms, and in each round
each mechanism is available for each bidder only with a certain probability.
Our scenario is an elementary case of simple mechanism design with incomplete
information, where availabilities are bidder types. It captures natural
applications in online markets with limited supply and can be used to model
access of unreliable channels in wireless networks.
  If mechanisms satisfy a smoothness guarantee, existing results show that
learning outcomes recover a significant fraction of the optimal social welfare.
These approaches, however, have serious drawbacks in terms of plausibility and
computational complexity. Also, the guarantees apply only when availabilities
are stochastically independent among bidders.
  In contrast, we propose an alternative approach where each bidder uses a
single no-regret learning algorithm and applies it in all rounds. This results
in what we call availability-oblivious coarse correlated equilibria. It
exponentially decreases the learning burden, simplifies implementation (e.g.,
as a method for channel access in wireless devices), and thereby addresses some
of the concerns about Bayes-Nash equilibria and learning outcomes in Bayesian
settings. Our main results are general composition theorems for smooth
mechanisms when valuation functions of bidders are lattice-submodular. They
rely on an interesting connection to the notion of correlation gap of
submodular functions over product lattices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00338</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00338</id><created>2015-09-01</created><updated>2016-01-02</updated><authors><author><keyname>Liu</keyname><forenames>Dantong</forenames></author><author><keyname>Wang</keyname><forenames>Lifeng</forenames></author><author><keyname>Chen</keyname><forenames>Yue</forenames></author><author><keyname>Elkashlan</keyname><forenames>Maged</forenames></author><author><keyname>Wong</keyname><forenames>Kai-Kit</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author><author><keyname>Hanzo</keyname><forenames>Lajos</forenames></author></authors><title>User Association in 5G Networks: A Survey and an Outlook</title><categories>cs.NI</categories><comments>26 pages; accepted to appear in IEEE Communications Surveys and
  Tutorials</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The fifth generation (5G) mobile networks are envisioned to support the
deluge of data traffic with reduced energy consumption and improved quality of
service (QoS) provision. To this end, the key enabling technologies, such as
heterogeneous networks (HetNets), massive multiple-input multiple-output (MIMO)
and millimeter wave (mmWave) techniques, are identified to bring 5G to
fruition. Regardless of the technology adopted, a user association mechanism is
needed to determine whether a user is associated with a particular base station
(BS) before the data transmission commences. User association plays a pivotal
role in enhancing the load balancing, the spectrum efficiency and the energy
efficiency of networks. The emerging 5G networks introduce numerous challenges
and opportunities for the design of sophisticated user association mechanisms.
Hence, substantial research efforts are dedicated to the issues of user
association in HetNets, massive MIMO networks, mmWave networks and energy
harvesting networks. We introduce a taxonomy as a framework for systematically
studying the existing user association algorithms. Based on the proposed
taxonomy, we then proceed to present an extensive overview of the
state-of-the-art in user association conceived for HetNets, massive MIMO,
mmWave and energy harvesting networks. Finally, we summarize the challenges as
well as opportunities of user association in 5G and provide design guidelines
and potential solutions for sophisticated user association mechanisms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00374</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00374</id><created>2015-09-01</created><updated>2016-01-25</updated><authors><author><keyname>Wang</keyname><forenames>Kezhi</forenames></author><author><keyname>Yang</keyname><forenames>Kun</forenames></author><author><keyname>Magurawalage</keyname><forenames>Chathura Sarathchandra</forenames></author></authors><title>Joint Energy Minimization and Resource Allocation in C-RAN with Mobile
  Cloud</title><categories>cs.NI</categories><comments>Accepted by IEEE Transactions on Cloud Computing on 23 Jan, 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud radio access network (C-RAN) has emerged as a potential candidate of
the next generation access network technology to address the increasing mobile
traffic, while mobile cloud computing (MCC) offers a prospective solution to
the resource-limited mobile user in executing computation intensive tasks.
Taking full advantages of above two cloud-based techniques, C-RAN with MCC are
presented in this paper to enhance both performance and energy efficiencies. In
particular, this paper studies the joint energy minimization and resource
allocation in C-RAN with MCC under the time constraints of the given tasks. We
first review the energy and time model of the computation and communication.
Then, we formulate the joint energy minimization into a non-convex optimization
with the constraints of task executing time, transmitting power, computation
capacity and fronthaul data rates. This non-convex optimization is then
reformulated into an equivalent convex problem based on weighted minimum mean
square error (WMMSE). The iterative algorithm is finally given to deal with the
joint resource allocation in C-RAN with mobile cloud. Simulation results
confirm that the proposed energy minimization and resource allocation solution
can improve the system performance and save energy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00378</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00378</id><created>2015-09-01</created><authors><author><keyname>Brier</keyname><forenames>Eric</forenames></author><author><keyname>Coron</keyname><forenames>Jean-S&#xe9;bastien</forenames></author><author><keyname>G&#xe9;raud</keyname><forenames>R&#xe9;mi</forenames></author><author><keyname>Maimut</keyname><forenames>Diana</forenames></author><author><keyname>Naccache</keyname><forenames>David</forenames></author></authors><title>A Number-Theoretic Error-Correcting Code</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we describe a new error-correcting code (ECC) inspired by the
Naccache-Stern cryptosystem. While by far less efficient than Turbo codes, the
proposed ECC happens to be more efficient than some established ECCs for
certain sets of parameters. The new ECC adds an appendix to the message. The
appendix is the modular product of small primes representing the message bits.
The receiver recomputes the product and detects transmission errors using
modular division and lattice reduction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00379</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00379</id><created>2015-09-01</created><authors><author><keyname>Barth</keyname><forenames>Lukas</forenames></author><author><keyname>Gemsa</keyname><forenames>Andreas</forenames></author><author><keyname>Niedermann</keyname><forenames>Benjamin</forenames></author><author><keyname>N&#xf6;llenburg</keyname><forenames>Martin</forenames></author></authors><title>On the Readability of Boundary Labeling</title><categories>cs.CG</categories><comments>Full version of GD 2015 paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Boundary labeling deals with annotating features in images such that labels
are placed outside of the image and are connected by curves (so-called leaders)
to the corresponding features. While boundary labeling has been extensively
investigated from an algorithmic perspective, the research on its readability
has been neglected. In this paper we present the first formal user study on the
readability of boundary labeling. We consider the four most studied leader
types with respect to their performance, i.e., whether and how fast a viewer
can assign a feature to its label and vice versa. We give a detailed analysis
of the results regarding the readability of the four models and discuss their
aesthetic qualities based on the users' preference judgments and interviews.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00388</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00388</id><created>2015-09-01</created><authors><author><keyname>Brandenburg</keyname><forenames>Franz J.</forenames></author><author><keyname>Didimo</keyname><forenames>Walter</forenames></author><author><keyname>Evans</keyname><forenames>William S.</forenames></author><author><keyname>Kindermann</keyname><forenames>Philipp</forenames></author><author><keyname>Liotta</keyname><forenames>Giuseppe</forenames></author><author><keyname>Montecchiani</keyname><forenames>Fabrizio</forenames></author></authors><title>Recognizing and Drawing IC-planar Graphs</title><categories>cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  IC-planar graphs are those graphs that admit a drawing where no two crossed
edges share an end-vertex and each edge is crossed at most once. They are a
proper subfamily of the 1-planar graphs. Given an embedded IC-planar graph $G$
with $n$ vertices, we present an $O(n)$-time algorithm that computes a
straight-line drawing of $G$ in quadratic area, and an $O(n^3)$-time algorithm
that computes a straight-line drawing of $G$ with right-angle crossings in
exponential area. Both these area requirements are worst-case optimal. We also
show that it is NP-complete to test IC-planarity both in the general case and
in the case in which a rotation system is fixed for the input graph.
Furthermore, we describe a polynomial-time algorithm to test whether a set of
matching edges can be added to a triangulated planar graph such that the
resulting graph is IC-planar.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00392</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00392</id><created>2015-09-01</created><authors><author><keyname>Gupta</keyname><forenames>Manish</forenames></author></authors><title>Cascade Markov Decision Processes: Theory and Applications</title><categories>cs.SY</categories><msc-class>60J20, 90C40</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the optimal control of time varying continuous time
Markov chains whose transition rates are themselves Markov processes. In one
set of problems the solution of an ordinary differential equation is shown to
determine the optimal performance and feedback controls, while some other cases
are shown to lead to singular optimal control problems which are more difficult
to solve. Solution techniques are demonstrated using examples from finance to
behavioral decision making.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00395</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00395</id><created>2015-09-01</created><authors><author><keyname>Deng</keyname><forenames>Sijia</forenames></author><author><keyname>Samimi</keyname><forenames>Mathew K.</forenames></author><author><keyname>Rappaport</keyname><forenames>Theodore S.</forenames></author></authors><title>28 GHz and 73 GHz Millimeter-Wave Indoor Propagation Measurements and
  Path Loss Models</title><categories>cs.IT math.IT</categories><comments>7 pages, 9 figures, 2015 IEEE International Conference on
  Communications (ICC), ICC Workshops</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents 28 GHz and 73 GHz millimeter- wave propagation
measurements performed in a typical office environment using a 400
Megachip-per-second broadband sliding correlator channel sounder and highly
directional steerable 15 dBi (30 degrees beamwidth) and 20 dBi (15 degrees
beamwidth) horn antennas. Power delay profiles were acquired for 48
transmitter-receiver location combinations over distances ranging from 3.9 m to
45.9 m with maximum transmit powers of 24 dBm and 12.3 dBm at 28 GHz and 73
GHz, respectively. Directional and omnidirectional path loss models and RMS
delay spread statistics are presented for line-of-sight and non-line-of-sight
environments for both co- and cross-polarized antenna configurations. The LOS
omnidirectional path loss exponents were 1.1 and 1.3 at 28 GHz and 73 GHz, and
2.7 and 3.2 in NLOS at 28 GHz and 73 GHz, respectively, for
vertically-polarized antennas. The mean directional RMS delay spreads were 18.4
ns and 13.3 ns, with maximum values of 193 ns and 288 ns at 28 GHz and 73 GHz,
respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00399</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00399</id><created>2015-09-01</created><authors><author><keyname>Steinmetz</keyname><forenames>Erik</forenames></author><author><keyname>Wildemeersch</keyname><forenames>Matthias</forenames></author><author><keyname>Quek</keyname><forenames>Tony Q. S.</forenames></author><author><keyname>Wymeersch</keyname><forenames>Henk</forenames></author></authors><title>Reception Probabilities in 5G Vehicular Communications close to
  Intersections</title><categories>cs.SY cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vehicular networks allow vehicles to constantly share information with each
other and their surrounding, and are expected to be an integral part in future
intelligent transportation system (ITS). However, the diversity of ITS
applications in combination with the extremely stringent demands in particular
posed by safety critical applications makes the design of vehicular
communication a challenging task. 5G device-to-device communication as well as
802.11p are promising to address this challenge. In order to guide and validate
the design process, analytical expressions of key performance metrics such as
outage probability an throughput are necessary. In this paper, we focus on the
intersection scenario, and present a general procedure to analytically
determine the success probability of a selected link as well as system-wide
throughput. We provide an overview of the salient properties of vehicular
communication systems near intersections and show how the procedure can be used
to model signal propagation conditions typical to different environments of
practical relevance, for instance rural and urban scenarios. The results
indicate that the procedure is sufficiently general and flexible to deal with a
variety of scenarios, and can thus serve as a useful design tool for
communication system engineers, complementing simulations and experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00406</identifier>
 <datestamp>2015-11-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00406</id><created>2015-09-01</created><authors><author><keyname>G&#xf3;mez-Garde&#xf1;es</keyname><forenames>Jes&#xfa;s</forenames></author><author><keyname>De Domenico</keyname><forenames>Manlio</forenames></author><author><keyname>Guti&#xe9;rrez</keyname><forenames>Gerardo</forenames></author><author><keyname>Arenas</keyname><forenames>Alex</forenames></author><author><keyname>G&#xf3;mez</keyname><forenames>Sergio</forenames></author></authors><title>Layer-layer competition in multiplex complex networks</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>11 pages, 3 figures. To appear in Philosophical Transactions of the
  Royal Society A</comments><journal-ref>Philosophical Transactions of the Royal Society A 373 (2015)
  20150117</journal-ref><doi>10.1098/rsta.2015.0117</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The coexistence of multiple types of interactions within social,
technological and biological networks has moved the focus of the physics of
complex systems towards a multiplex description of the interactions between
their constituents. This novel approach has unveiled that the multiplex nature
of complex systems has strong influence in the emergence of collective states
and their critical properties. Here we address an important issue that is
intrinsic to the coexistence of multiple means of interactions within a
network: their competition. To this aim, we study a two-layer multiplex in
which the activity of users can be localized in each of the layer or shared
between them, favoring that neighboring nodes within a layer focus their
activity on the same layer. This framework mimics the coexistence and
competition of multiple communication channels, in a way that the prevalence of
a particular communication platform emerges as a result of the localization of
users activity in one single interaction layer. Our results indicate that there
is a transition from localization (use of a preferred layer) to delocalization
(combined usage of both layers) and that the prevalence of a particular layer
(in the localized state) depends on their structural properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00408</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00408</id><created>2015-09-01</created><authors><author><keyname>Bookatz</keyname><forenames>Adam D.</forenames></author><author><keyname>Roetteler</keyname><forenames>Martin</forenames></author><author><keyname>Wocjan</keyname><forenames>Pawel</forenames></author></authors><title>Improved bounded-strength decoupling schemes for local Hamiltonians</title><categories>quant-ph cs.ET</categories><comments>15 pages</comments><report-no>MIT-CTP /4705</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the task of switching off the Hamiltonian of a system by removing
all internal and system-environment couplings. We propose dynamical decoupling
schemes, that use only bounded-strength controls, for quantum many-body systems
with local system Hamiltonians and local environmental couplings. To do so, we
introduce the combinatorial concept of balanced-cycle orthogonal arrays (BOAs)
and show how to construct them from classical error-correcting codes. The
derived decoupling schemes may be useful as a primitive for more complex
schemes, e.g., for Hamiltonian simulation. For the case of $n$ qubits and a
$2$-local Hamiltonian, the length of the resulting decoupling scheme scales as
$O(n \log n)$, improving over the previously best-known schemes that scaled
quadratically with $n$. More generally, using balanced-cycle orthogonal arrays
constructed from families of BCH codes, we show that bounded-strength
decoupling for any $\ell$-local Hamiltonian, where $\ell \geq 2$, can be
achieved using decoupling schemes of length at most $O(n^{\ell-1} \log n)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00413</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00413</id><created>2015-09-01</created><authors><author><keyname>Desai</keyname><forenames>Aditya</forenames></author><author><keyname>Gulwani</keyname><forenames>Sumit</forenames></author><author><keyname>Hingorani</keyname><forenames>Vineet</forenames></author><author><keyname>Jain</keyname><forenames>Nidhi</forenames></author><author><keyname>Karkare</keyname><forenames>Amey</forenames></author><author><keyname>Marron</keyname><forenames>Mark</forenames></author><author><keyname>R</keyname><forenames>Sailesh</forenames></author><author><keyname>Roy</keyname><forenames>Subhajit</forenames></author></authors><title>Program Synthesis using Natural Language</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Interacting with computers is a ubiquitous activity for millions of people.
Repetitive or specialized tasks often require creation of small, often one-off,
programs. End-users struggle with learning and using the myriad of
domain-specific languages (DSLs) to effectively accomplish these tasks.
  We present a general framework for constructing program synthesizers that
take natural language (NL) inputs and produce expressions in a target DSL. The
framework takes as input a DSL definition and training data consisting of
NL/DSL pairs. From these it constructs a synthesizer by learning optimal
weights and classifiers (using NLP features) that rank the outputs of a
keyword-programming based translation. We applied our framework to three
domains: repetitive text editing, an intelligent tutoring system, and flight
information queries. On 1200+ English descriptions, the respective synthesizers
rank the desired program as the top-1 and top-3 for 80% and 90% descriptions
respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00436</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00436</id><created>2015-09-01</created><authors><author><keyname>Rappaport</keyname><forenames>Theodore S.</forenames></author><author><keyname>Deng</keyname><forenames>Sijia</forenames></author></authors><title>73 GHz Wideband Millimeter-Wave Foliage and Ground Reflection
  Measurements and Models</title><categories>cs.IT math.IT</categories><comments>6 pages, 4 figures, 2015 IEEE International Conference on
  Communications (ICC), ICC Workshops</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents 73 GHz wideband outdoor foliage and ground reflection
measurements. Propagation measurements were made with a 400 Megachip-per-second
sliding correlator channel sounder, with rotatable 27 dBi (7 degrees half-
power beamwidth) horn antennas at both the transmitter and receiver, to study
foliage-induced scattering and de-polarization effects, to assist in developing
future wireless systems that will use adaptive array antennas. Signal
attenuation through foliage was measured to be 0.4 dB/m for both co- and
cross-polarized antenna configurations. Measured ground reflection coefficients
for dirt and gravel ranged from 0.02 to 0.34, for incident angles ranging from
60 degrees to 81 degrees (with respect to the normal incidence of the surface).
These data are useful for link budget design and site-specific (ray-tracing)
models for future millimeter-wave communication systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00442</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00442</id><created>2015-09-01</created><authors><author><keyname>Kostitsyna</keyname><forenames>Irina</forenames></author><author><keyname>N&#xf6;llenburg</keyname><forenames>Martin</forenames></author><author><keyname>Polishchuk</keyname><forenames>Valentin</forenames></author><author><keyname>Schulz</keyname><forenames>Andr&#xe9;</forenames></author><author><keyname>Strash</keyname><forenames>Darren</forenames></author></authors><title>On Minimizing Crossings in Storyline Visualizations</title><categories>cs.DS cs.CG</categories><comments>6 pages, 4 figures. To appear at the 23rd International Symposium on
  Graph Drawing and Network Visualization (GD 2015)</comments><acm-class>F.2.2; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a storyline visualization, we visualize a collection of interacting
characters (e.g., in a movie, play, etc.) by $x$-monotone curves that converge
for each interaction, and diverge otherwise. Given a storyline with $n$
characters, we show tight lower and upper bounds on the number of crossings
required in any storyline visualization for a restricted case. In particular,
we show that if (1) each meeting consists of exactly two characters and (2) the
meetings can be modeled as a tree, then we can always find a storyline
visualization with $O(n\log n)$ crossings. Furthermore, we show that there
exist storylines in this restricted case that require $\Omega(n\log n)$
crossings. Lastly, we show that, in the general case, minimizing the number of
crossings in a storyline visualization is fixed-parameter tractable, when
parameterized on the number of characters $k$. Our algorithm runs in time
$O(k!^2k\log k + k!^2m)$, where $m$ is the number of meetings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00453</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00453</id><created>2015-09-01</created><updated>2015-12-04</updated><authors><author><keyname>Galli</keyname><forenames>Stefano</forenames></author><author><keyname>Kerpez</keyname><forenames>Kenneth</forenames></author><author><keyname>Mariotte</keyname><forenames>Hubert</forenames></author><author><keyname>Moulin</keyname><forenames>Fabienne</forenames></author></authors><title>PLC-to-DSL Interference: Statistical Model and Impact on DSL</title><categories>cs.IT math.IT</categories><comments>12 pages (2-column), 21 figures, published in IEEE ISPLC 2015
  conference (winner of the IEEE ISPLC Best Paper Award), submitted to IEEE
  JSAC special issue</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Newly available standards for broadband access using Digital Subscriber Lines
(DSL) have a high degree of spectrum overlap with home networking technologies
using broadband Power Line Communications (BB-PLC) and this overlap leads to
Electromagnetic Compatibility issues that may cause performance degradation in
DSL systems. This paper studies the characteristics of measured PLC-to-DSL
interference and presents novel results on its statistical characterization.
The frequency-dependent couplings between power line cables and twisted-pairs
are estimated from measurements and a statistical model based on a mixture of
two truncated Gaussian distributions is set forth. The proposed statistical
model allows the accurate evaluation of the impact of BB-PLC interference on
various DSL technologies, in terms of both average and worst-case impacts on
data rate. This paper further provides an extensive assessment of the impact of
PLC-to-DSL interference at various loop lengths and for multiple profiles of
Very-high rate Digital Subscriber Lines (VDSL2), Vectored VDSL2 (V-VDSL2), and
G.fast. The results of this paper confirm that the impact of PLC interference
varies with loop length and whether vectoring is used or not. Furthermore, the
average impact is found to be generally small but worst-case couplings can lead
to substantial degradation of DSL.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00459</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00459</id><created>2015-09-01</created><authors><author><keyname>Kondor</keyname><forenames>D&#xe1;niel</forenames></author><author><keyname>Thebault</keyname><forenames>Pierrick</forenames></author><author><keyname>Grauwin</keyname><forenames>Sebastian</forenames></author><author><keyname>G&#xf3;dor</keyname><forenames>Istv&#xe1;n</forenames></author><author><keyname>Moritz</keyname><forenames>Simon</forenames></author><author><keyname>Sobolevsky</keyname><forenames>Stanislav</forenames></author><author><keyname>Ratti</keyname><forenames>Carlo</forenames></author></authors><title>Visualizing signatures of human activity in cities across the globe</title><categories>cs.SI</categories><comments>Landscape Architecture Frontiers, July 2015 issue</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The availability of big data on human activity is currently changing the way
we look at our surroundings. With the high penetration of mobile phones, nearly
everyone is already carrying a high-precision sensor providing an opportunity
to monitor and analyze the dynamics of human movement on unprecedented scales.
In this article, we present a technique and visualization tool which uses
aggregated activity measures of mobile networks to gain information about human
activity shaping the structure of the cities. Based on ten months of mobile
network data, activity patterns can be compared through time and space to
unravel the &quot;city's pulse&quot; as seen through the specific signatures of different
locations. Furthermore, the tool allows classifying the neighborhoods into
functional clusters based on the timeline of human activity, providing valuable
insights on the actual land use patterns within the city. This way, the
approach and the tool provide new ways of looking at the city structure from
historical perspective and potentially also in real-time based on dynamic
up-to-date records of human behavior. The online tool presents results for four
global cities: New York, London, Hong Kong and Los Angeles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00464</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00464</id><created>2015-09-01</created><authors><author><keyname>Toni</keyname><forenames>Laura</forenames></author><author><keyname>Cheung</keyname><forenames>Gene</forenames></author><author><keyname>Frossard</keyname><forenames>Pascal</forenames></author></authors><title>In-Network View Synthesis for Interactive Multiview Video Systems</title><categories>cs.MM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To enable Interactive multiview video systems with a minimum view-switching
delay, multiple camera views are sent to the users, which are used as reference
images to synthesize additional virtual views via depth-image-based rendering.
In practice, bandwidth constraints may however restrict the number of reference
views sent to clients per time unit, which may in turn limit the quality of the
synthesized viewpoints. We argue that the reference view selection should
ideally be performed close to the users, and we study the problem of in-network
reference view synthesis such that the navigation quality is maximized at the
clients. We consider a distributed cloud network architecture where data stored
in a main cloud is delivered to end users with the help of cloudlets, i.e.,
resource-rich proxies close to the users. In order to satisfy last-hop
bandwidth constraints from the cloudlet to the users, a cloudlet re-samples
viewpoints of the 3D scene into a discrete set of views (combination of
received camera views and virtual views synthesized) to be used as reference
for the synthesis of additional virtual views at the client. This in-network
synthesis leads to better viewpoint sampling given a bandwidth constraint
compared to simple selection of camera views, but it may however carry a
distortion penalty in the cloudlet-synthesized reference views. We therefore
cast a new reference view selection problem where the best subset of views is
defined as the one minimizing the distortion over a view navigation window
defined by the user under some transmission bandwidth constraints. We show that
the view selection problem is NP-hard, and propose an effective polynomial time
algorithm using dynamic programming to solve the optimization problem.
Simulation results finally confirm the performance gain offered by virtual view
synthesis in the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00498</identifier>
 <datestamp>2015-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00498</id><created>2015-09-01</created><authors><author><keyname>Hong</keyname><forenames>Dezhi</forenames></author><author><keyname>Ortiz</keyname><forenames>Jorge</forenames></author><author><keyname>Bhattacharya</keyname><forenames>Arka</forenames></author><author><keyname>Whitehouse</keyname><forenames>Kamin</forenames></author></authors><title>Sensor-Type Classification in Buildings</title><categories>cs.LG</categories><acm-class>C.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many sensors/meters are deployed in commercial buildings to monitor and
optimize their performance. However, because sensor metadata is inconsistent
across buildings, software-based solutions are tightly coupled to the sensor
metadata conventions (i.e. schemas and naming) for each building. Running the
same software across buildings requires significant integration effort.
  Metadata normalization is critical for scaling the deployment process and
allows us to decouple building-specific conventions from the code written for
building applications. It also allows us to deal with missing metadata. One
important aspect of normalization is to differentiate sensors by the typeof
phenomena being observed. In this paper, we propose a general, simple, yet
effective classification scheme to differentiate sensors in buildings by type.
We perform ensemble learning on data collected from over 2000 sensor streams in
two buildings. Our approach is able to achieve more than 92% accuracy for
classification within buildings and more than 82% accuracy for across
buildings. We also introduce a method for identifying potential misclassified
streams. This is important because it allows us to identify opportunities to
attain more input from experts -- input that could help improve classification
accuracy when ground truth is unavailable. We show that by adjusting a
threshold value we are able to identify at least 30% of the misclassified
instances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00504</identifier>
 <datestamp>2015-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00504</id><created>2015-08-28</created><authors><author><keyname>Gadepally</keyname><forenames>Vijay</forenames></author><author><keyname>Kepner</keyname><forenames>Jeremy</forenames></author></authors><title>Using a Power Law Distribution to describe Big Data</title><categories>cs.SI</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The gap between data production and user ability to access, compute and
produce meaningful results calls for tools that address the challenges
associated with big data volume, velocity and variety. One of the key hurdles
is the inability to methodically remove expected or uninteresting elements from
large data sets. This difficulty often wastes valuable researcher and
computational time by expending resources on uninteresting parts of data.
Social sensors, or sensors which produce data based on human activity, such as
Wikipedia, Twitter, and Facebook have an underlying structure which can be
thought of as having a Power Law distribution. Such a distribution implies that
few nodes generate large amounts of data. In this article, we propose a
technique to take an arbitrary dataset and compute a power law distributed
background model that bases its parameters on observed statistics. This model
can be used to determine the suitability of using a power law or automatically
identify high degree nodes for filtering and can be scaled to work with big
data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00506</identifier>
 <datestamp>2015-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00506</id><created>2015-09-01</created><authors><author><keyname>Ali</keyname><forenames>Ahmed O. D.</forenames></author><author><keyname>Yetis</keyname><forenames>Cenk M.</forenames></author><author><keyname>Torlak</keyname><forenames>Murat</forenames></author></authors><title>Second-Order Statistics of MIMO Rayleigh Interference Channels: Theory,
  Applications, and Analysis</title><categories>cs.IT math.IT</categories><comments>30 pages, 9 figures, submitted to IEEE Trans. on Comm</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While first-order channel statistics, such as bit-error rate (BER) and outage
probability, play an important role in the design of wireless communication
systems, they provide information only on the static behavior of fading
channels. On the other hand, second-order statistics, such as level crossing
rate (LCR) and average outage duration (AOD), capture the correlation
properties of fading channels, hence, are used in system design notably in
packet-based transmission systems. In this paper, exact closed-form expressions
are derived for the LCR and AOD of the signal at a receiver where maximal-ratio
combining (MRC) is deployed over flat Rayleigh fading channels in the presence
of additive white Gaussian noise (AWGN) and co-channel interferers with unequal
transmitting powers and unequal speeds. Moreover, in order to gain insight on
the LCR behavior, a simplified approximate expression for the LCR is presented.
As an application of LCR in system designs, the packet error rate (PER) is
evaluated through finite state Markov chain (FSMC) model. Finally, as another
application again by using the FSMC model, the optimum packet length to
maximize the throughput of the system with stop-and-wait automatic repeat
request (SW-ARQ) protocol is derived. Simulation results validating the
presented expressions are provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00509</identifier>
 <datestamp>2015-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00509</id><created>2015-09-01</created><authors><author><keyname>Savas</keyname><forenames>S. Sedef</forenames></author><author><keyname>Tornatore</keyname><forenames>Massimo</forenames></author><author><keyname>Habib</keyname><forenames>M. Farhan</forenames></author><author><keyname>Chowdhury</keyname><forenames>Pulak</forenames></author><author><keyname>Mukherjee</keyname><forenames>Biswanath</forenames></author></authors><title>Disaster-Resilient Control Plane Design and Mapping in Software-Defined
  Networks</title><categories>cs.NI</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Communication networks, such as core optical networks, heavily depend on
their physical infrastructure, and hence they are vulnerable to man-made
disasters, such as Electromagnetic Pulse (EMP) or Weapons of Mass Destruction
(WMD) attacks, as well as to natural disasters. Large-scale disasters may cause
huge data loss and connectivity disruption in these networks. As our dependence
on network services increases, the need for novel survivability methods to
mitigate the effects of disasters on communication networks becomes a major
concern. Software-Defined Networking (SDN), by centralizing control logic and
separating it from physical equipment, facilitates network programmability and
opens up new ways to design disaster-resilient networks. On the other hand, to
fully exploit the potential of SDN, along with data-plane survivability, we
also need to design the control plane to be resilient enough to survive network
failures caused by disasters. Several distributed SDN controller architectures
have been proposed to mitigate the risks of overload and failure, but they are
optimized for limited faults without addressing the extent of large-scale
disaster failures. For disaster resiliency of the control plane, we propose to
design it as a virtual network, which can be solved using Virtual Network
Mapping techniques. We select appropriate mapping of the controllers over the
physical network such that the connectivity among the controllers
(controller-to-controller) and between the switches to the controllers
(switch-to-controllers) is not compromised by physical infrastructure failures
caused by disasters. We formally model this disaster-aware control-plane design
and mapping problem, and demonstrate a significant reduction in the disruption
of controller-to-controller and switch-to-controller communication channels
using our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00511</identifier>
 <datestamp>2015-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00511</id><created>2015-09-01</created><authors><author><keyname>Yang</keyname><forenames>Xitong</forenames></author><author><keyname>Li</keyname><forenames>Yuncheng</forenames></author><author><keyname>Luo</keyname><forenames>Jiebo</forenames></author></authors><title>Pinterest Board Recommendation for Twitter Users</title><categories>cs.SI cs.MM</categories><acm-class>H.3.5</acm-class><doi>10.1145/2733373.2806375</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pinboard on Pinterest is an emerging media to engage online social media
users, on which users post online images for specific topics. Regardless of its
significance, there is little previous work specifically to facilitate
information discovery based on pinboards. This paper proposes a novel pinboard
recommendation system for Twitter users. In order to associate contents from
the two social media platforms, we propose to use MultiLabel classification to
map Twitter user followees to pinboard topics and visual diversification to
recommend pinboards given user interested topics. A preliminary experiment on a
dataset with 2000 users validated our proposed system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00514</identifier>
 <datestamp>2015-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00514</id><created>2015-09-01</created><updated>2015-09-03</updated><authors><author><keyname>Xu</keyname><forenames>Aolin</forenames></author><author><keyname>Raginsky</keyname><forenames>Maxim</forenames></author></authors><title>Information-theoretic lower bounds for distributed function computation</title><categories>cs.IT math.IT</categories><comments>52 pages, 14 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We derive information-theoretic lower bounds on the minimum time required by
any scheme for distributed function computation over a network of
point-to-point channels with finite capacity to achieve a given accuracy with a
given probability. The main contributions include: 1) A lower bound on
conditional mutual information via so-called small ball probabilities, which
captures the influence on the computation time of the joint distribution of the
initial observations at the nodes, the structure of the function, and the
accuracy requirement. For linear functions, the small ball probability can be
expressed in terms of L\'evy concentration functions of sums of independent
random variables, for which tight estimates are available that lead to strict
improvements over existing results. 2) An upper bound on conditional mutual
information via strong data processing inequalities, which complements and
strengthens the cutset-capacity upper bounds from the literature. For discrete
observations, it can lead to much tighter lower bounds on computation time with
respect to the confidence of the computation results. 3) A multi-cutset
analysis that quantifies the dissipation of information as it flows across a
succession of cutsets in the network. This analysis is based on reducing a
general network to a bidirected chain, and the results highlight the dependence
of the computation time on the diameter of the network, a fundamental parameter
that is missing from most of the existing results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00519</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00519</id><created>2015-09-01</created><updated>2016-01-11</updated><authors><author><keyname>Burda</keyname><forenames>Yuri</forenames></author><author><keyname>Grosse</keyname><forenames>Roger</forenames></author><author><keyname>Salakhutdinov</keyname><forenames>Ruslan</forenames></author></authors><title>Importance Weighted Autoencoders</title><categories>cs.LG stat.ML</categories><comments>Submitted to ICLR 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The variational autoencoder (VAE; Kingma, Welling (2014)) is a recently
proposed generative model pairing a top-down generative network with a
bottom-up recognition network which approximates posterior inference. It
typically makes strong assumptions about posterior inference, for instance that
the posterior distribution is approximately factorial, and that its parameters
can be approximated with nonlinear regression from the observations. As we show
empirically, the VAE objective can lead to overly simplified representations
which fail to use the network's entire modeling capacity. We present the
importance weighted autoencoder (IWAE), a generative model with the same
architecture as the VAE, but which uses a strictly tighter log-likelihood lower
bound derived from importance weighting. In the IWAE, the recognition network
uses multiple samples to approximate the posterior, giving it increased
flexibility to model complex posteriors which do not fit the VAE modeling
assumptions. We show empirically that IWAEs learn richer latent space
representations than VAEs, leading to improved test log-likelihood on density
estimation benchmarks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00520</identifier>
 <datestamp>2015-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00520</id><created>2015-09-01</created><authors><author><keyname>Zhang</keyname><forenames>Jiazi</forenames></author><author><keyname>Sankar</keyname><forenames>Lalitha</forenames></author></authors><title>Implication of Unobservable State-and-topology Cyber-physical Attacks</title><categories>cs.SY</categories><comments>8 pages, 5 figures, submitted to IEEE TRANSACTIONS ON SMART GRID</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the physical consequences of a class of unobservable
state-and-topology cyber-physical attacks in which both state and topology data
for a sub-network of the network are changed by an attacker to mask a physical
attack. The problem is formulated as a two-stage optimization problem which
aims to cause overload in a line of the network with limited attack resources.
It is shown that unobservable state-and-topology cyber-physical attacks as
studied in this paper can make the system operation more vulnerable to line
outages and failures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00524</identifier>
 <datestamp>2015-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00524</id><created>2015-09-01</created><authors><author><keyname>Miller</keyname><forenames>Joseph S.</forenames></author><author><keyname>Rute</keyname><forenames>Jason</forenames></author></authors><title>Energy randomness</title><categories>math.LO cs.CC</categories><msc-class>03D32 (Primary), 68Q30, 31C15 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy randomness is a notion of partial randomness introduced by
Diamondstone and Kjos-Hanssen to characterize the sequences that can be
elements of a Martin-L\&quot;of random closed set (in the sense of Barmpalias,
Brodhead, Cenzer, Dashti, and Weber). It has also been applied by Allen,
Bienvenu, and Slaman to the characterization of the possible zero times of a
Martin-L\&quot;of random Brownian motion. In this paper, we show that $X \in
2^\omega$ is $s$-energy random if and only if $\sum_{n\in\omega} 2^{sn -
KM(X\upharpoonright n)} &lt; \infty$, providing a characterization of energy
randomness via a priori complexity $KM$. This is related to a question of
Allen, Bienvenu, and Slaman.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00533</identifier>
 <datestamp>2015-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00533</id><created>2015-09-01</created><authors><author><keyname>Wisdom</keyname><forenames>Scott</forenames></author><author><keyname>Powers</keyname><forenames>Thomas</forenames></author><author><keyname>Atlas</keyname><forenames>Les</forenames></author><author><keyname>Pitton</keyname><forenames>James</forenames></author></authors><title>Enhancement and Recognition of Reverberant and Noisy Speech by Extending
  Its Coherence</title><categories>cs.SD cs.CL stat.AP</categories><comments>22 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most speech enhancement algorithms make use of the short-time Fourier
transform (STFT), which is a simple and flexible time-frequency decomposition
that estimates the short-time spectrum of a signal. However, the duration of
short STFT frames are inherently limited by the nonstationarity of speech
signals. The main contribution of this paper is a demonstration of speech
enhancement and automatic speech recognition in the presence of reverberation
and noise by extending the length of analysis windows. We accomplish this
extension by performing enhancement in the short-time fan-chirp transform
(STFChT) domain, an overcomplete time-frequency representation that is coherent
with speech signals over longer analysis window durations than the STFT. This
extended coherence is gained by using a linear model of fundamental frequency
variation of voiced speech signals. Our approach centers around using a
single-channel minimum mean-square error log-spectral amplitude (MMSE-LSA)
estimator proposed by Habets, which scales coefficients in a time-frequency
domain to suppress noise and reverberation. In the case of multiple
microphones, we preprocess the data with either a minimum variance
distortionless response (MVDR) beamformer, or a delay-and-sum beamformer (DSB).
We evaluate our algorithm on both speech enhancement and recognition tasks for
the REVERB challenge dataset. Compared to the same processing done in the STFT
domain, our approach achieves significant improvement in terms of objective
enhancement metrics (including PESQ---the ITU-T standard measurement for speech
quality). In terms of automatic speech recognition (ASR) performance as
measured by word error rate (WER), our experiments indicate that the STFT with
a long window is more effective for ASR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00536</identifier>
 <datestamp>2015-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00536</id><created>2015-09-01</created><authors><author><keyname>Wakaiki</keyname><forenames>Masashi</forenames></author><author><keyname>Yamamoto</keyname><forenames>Yutaka</forenames></author></authors><title>Stabilization of continuous-time switched linear systems with quantized
  output feedback</title><categories>cs.SY</categories><comments>This journal-version paper is based on the conference paper
  (arXiv:1403.4670)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the problem of stabilizing continuous-time switched
linear systems with quantized output feedback. We assume that the observer and
the control gain are given for each mode. Also, the plant mode is known to the
controller and the quantizer. Extending the result in the non-switched case, we
develop an update rule of the quantizer to achieve asymptotic stability of the
closed-loop system under the average dwell-time assumption. To avoid quantizer
saturation, we adjust the quantizer at every switching time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00539</identifier>
 <datestamp>2015-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00539</id><created>2015-09-01</created><authors><author><keyname>Ouyang</keyname><forenames>Wenzhuo</forenames></author><author><keyname>Bai</keyname><forenames>Jingwen</forenames></author><author><keyname>Sabharwal</keyname><forenames>Ashutosh</forenames></author></authors><title>Leveraging One-hop Information in Massive MIMO Full-Duplex Wireless
  Systems</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE/ACM Transactions on Networking</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a single-cell massive MIMO full-duplex wireless communication
system, where the base-station (BS) is equipped with a large number of
antennas. We consider the setup where the single-antenna mobile users operate
in half- duplex, while each antenna at the BS is capable of full-duplex
transmissions, i.e., it can transmit and receive simultaneously using the same
frequency spectrum. The fundamental challenge in this system is intra-cell
inter-node interference, generated by the transmissions of uplink users to the
receptions at the downlink users. The key operational challenge is estimating
and aggregating inter-mobile channel estimates, which can potentially overwhelm
any gains from full-duplex operation.
  In this work, we propose a scalable and distributed scheme to optimally
manage the inter-node interference by utilizing a &quot;one- hop information
architecture&quot;. In this architecture, the BS only needs to know the
signal-to-interference-plus-noise ratio (SINR) from the downlink users. Each
uplink user needs its own SINR, along with a weighted signal-plus-noise metric
from its one-hop neighboring downlink users, which are the downlink users that
it interferes with. The proposed one-hop information architecture does not
require any network devices to comprehensively gather the vast inter-node
interference channel knowledge, and hence significantly reduces the overhead.
Based on the one-hop information architecture, we design a distributed power
control algorithm and implement such architecture using overheard feedback
information. We show that, in typical asymptotic regimes with many users and
antennas, the proposed distributed power control scheme improves the overall
network utility and reduces the transmission power of the uplink users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00540</identifier>
 <datestamp>2015-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00540</id><created>2015-09-01</created><authors><author><keyname>Wakaiki</keyname><forenames>Masashi</forenames></author><author><keyname>Yamamoto</keyname><forenames>Yutaka</forenames></author></authors><title>Stability analysis of sampled-data switched systems with quantization</title><categories>cs.SY</categories><comments>This journal-version paper is based on the conference paper
  (arXiv:1403.4691)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a stability analysis method for sampled-data switched linear
systems with finite-level static quantizers. In the closed-loop system,
information on the active mode of the plant is transmitted to the controller
only at each sampling time. This limitation of switching information leads to a
mode mismatch between the plant and the controller, and the system may become
unstable. A mode mismatch also makes it difficult to find an attractor set to
which the state trajectory converges. A switching condition for stability is
characterized by the total time when the modes of the plant and the controller
are different. Under the condition, we derive an ultimate bound on the state
trajectories by using a common Lyapunov function computed from a randomized
algorithm. The switching condition can be reduced to a dwell-time condition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00552</identifier>
 <datestamp>2015-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00552</id><created>2015-09-01</created><updated>2015-11-23</updated><authors><author><keyname>Shuai</keyname><forenames>Bing</forenames></author><author><keyname>Zuo</keyname><forenames>Zhen</forenames></author><author><keyname>Wang</keyname><forenames>Gang</forenames></author><author><keyname>Wang</keyname><forenames>Bing</forenames></author></authors><title>DAG-Recurrent Neural Networks For Scene Labeling</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In image labeling, local representations for image units are usually
generated from their surrounding image patches, thus long-range contextual
information is not effectively encoded. In this paper, we introduce recurrent
neural networks (RNNs) to address this issue. Specifically, directed acyclic
graph RNNs (DAG-RNNs) are proposed to process DAG-structured images, which
enables the network to model long-range semantic dependencies among image
units. Our DAG-RNNs are capable of tremendously enhancing the discriminative
power of local representations, which significantly benefits the local
classification. Meanwhile, we propose a novel class weighting function that
attends to rare classes, which phenomenally boosts the recognition accuracy for
non-frequent classes. Integrating with convolution and deconvolution layers,
our DAG-RNNs achieve new state-of-the-art results on the challenging SiftFlow,
CamVid and Barcelona benchmarks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00554</identifier>
 <datestamp>2015-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00554</id><created>2015-09-01</created><authors><author><keyname>Luo</keyname><forenames>Tony T.</forenames></author></authors><title>Achieving Energy Efficiency for Altruistic DISH: Three Properties</title><categories>cs.NI</categories><comments>6 pages, 3 figures. arXiv admin note: substantial text overlap with
  arXiv:1411.6521</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In an altruistic DISH protocol, additional nodes called &quot;altruists&quot; are
deployed in a multi-channel ad hoc network to achieve energy efficiency while
still maintaining the original throughput-delay performance. The responsibility
of altruists is to constantly monitor the control channel and awaken other
(normal) nodes when necessary (to perform data transmissions). Altruists never
sleep while other nodes sleep as far as possible. This technical report proves
three properties related to this cooperative protocol. The first is the
conditions for forming an unsafe pair (UP) in an undirected graph. The second
is the necessary and sufficient conditions for full cooperation coverage to
achieve the void of multi-channel coordination (MCC) problems. The last is the
NP-hardness of determining the minimum number and locations of altruistic nodes
to achieve full cooperation coverage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00556</identifier>
 <datestamp>2015-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00556</id><created>2015-09-02</created><authors><author><keyname>Xu</keyname><forenames>Elvis H. W.</forenames></author><author><keyname>Hui</keyname><forenames>P. M.</forenames></author></authors><title>Efficient Detection of Communities with Significant Overlaps in
  Networks: Partial Community Merger Algorithm</title><categories>cs.SI physics.soc-ph</categories><comments>9 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The identification of communities in large-scale networks is a challenging
task to existing searching schemes when the communities overlap significantly
among their members, as often the case in large-scale social networks. The
strong overlaps render many algorithms invalid. We propose a detection scheme
based on properly merging the partial communities revealed by the ego networks
of each vertex. The general principle, merger criteria, and post-processing
procedures are discussed. This partial community merger algorithm (PCMA) is
tested on two modern benchmark models. It shows a linear time complexity and it
performs accurately and efficiently when compared with two widely used
algorithms. PCMA is then applied to a huge social network and millions of
communities are identified. A detected community can be visualized with all its
members as well as the number of different communities that each member belongs
to. The multiple memberships of a vertex, in turn, illustrates the significant
overlaps between communities that calls for the need of a novel and efficient
algorithm such as PCMA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00557</identifier>
 <datestamp>2015-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00557</id><created>2015-09-02</created><authors><author><keyname>Louni</keyname><forenames>Alireza</forenames></author><author><keyname>Santhanakrishnan</keyname><forenames>Anand</forenames></author><author><keyname>Subbalakshmi</keyname><forenames>K. P.</forenames></author></authors><title>Identification of Source of Rumors in Social Networks with Incomplete
  Information</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rumor source identification in large social networks has received significant
attention lately. Most recent works deal with the scale of the problem by
observing a subset of the nodes in the network, called sensors, to estimate the
source. This paper addresses the problem of locating the source of a rumor in
large social networks where some of these sensor nodes have failed. We estimate
the missing information about the sensors using doubly non-negative (DN) matrix
completion and compressed sensing techniques. This is then used to identify the
actual source by using a maximum likelihood estimator we developed earlier, on
a large data set from Sina Weibo. Results indicate that the estimation
techniques result in almost as good a performance of the ML estimator as for
the network for which complete information is available. To the best of our
knowledge, this is the first research work on source identification with
incomplete information in social networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00558</identifier>
 <datestamp>2015-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00558</id><created>2015-09-02</created><authors><author><keyname>Peng</keyname><forenames>Xi</forenames></author><author><keyname>Shen</keyname><forenames>Juei-Chin</forenames></author><author><keyname>Zhang</keyname><forenames>Jun</forenames></author><author><keyname>Letaief</keyname><forenames>Khaled B.</forenames></author></authors><title>Backhaul-Aware Caching Placement for Wireless Networks</title><categories>cs.IT cs.NI math.IT</categories><comments>6 pages, 3 figures, accepted to IEEE Globecom, San Diego, CA, Dec.
  2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As the capacity demand of mobile applications keeps increasing, the backhaul
network is becoming a bottleneck to support high quality of experience (QoE) in
next-generation wireless networks. Content caching at base stations (BSs) is a
promising approach to alleviate the backhaul burden and reduce user-perceived
latency. In this paper, we consider a wireless caching network where all the
BSs are connected to a central controller via backhaul links. In such a
network, users can obtain the required data from candidate BSs if the data are
pre-cached. Otherwise, the user data need to be first retrieved from the
central controller to local BSs, which introduces extra delay over the
backhaul. In order to reduce the download delay, the caching placement strategy
needs to be optimized. We formulate such a design problem as the minimization
of the average download delay over user requests, subject to the caching
capacity constraint of each BS. Different from existing works, our model takes
BS cooperation in the radio access into consideration and is fully aware of the
propagation delay on the backhaul links. The design problem is a mixed integer
programming problem and is highly complicated, and thus we relax the problem
and propose a low-complexity algorithm. Simulation results will show that the
proposed algorithm can effectively determine the near-optimal caching placement
and provide significant performance gains over conventional caching placement
strategies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00562</identifier>
 <datestamp>2015-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00562</id><created>2015-09-02</created><authors><author><keyname>Fukumoto</keyname><forenames>Hiroyuki</forenames></author><author><keyname>Hayashi</keyname><forenames>Kazunori</forenames></author></authors><title>Overlap Frequency Domain Equalization for Faster-than-Nyquist Signaling</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This letter proposes the Faster-than-Nyquist signaling (FTNS) using overlap
frequency domain equalization (FDE), which compensates the inter-symbol
interference (ISI) due to band limiting filters of the FTNS at the transmitter
and the receiver as well as the frequency selective fading channel. Since
overlap FDE does not require any guard interval (GI) at the transmitter such as
cyclic prefix (CP), higher spectral efficiency can be achieved compared to FTNS
scheme using the conventional FDE. In the proposed method, the equalizer weight
is derived based on minimum mean square error (MMSE) criterion taking the
colored noise due to the receiving filter into consideration. Moreover, we also
give an approximated FDE weight in order to reduce the computational
complexity. The performance of the proposed scheme is demonstrated via computer
simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00568</identifier>
 <datestamp>2015-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00568</id><created>2015-09-02</created><authors><author><keyname>Fire</keyname><forenames>Michael</forenames></author><author><keyname>Schler</keyname><forenames>Jonathan</forenames></author></authors><title>Exploring Online Ad Images Using a Deep Convolutional Neural Network
  Approach</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online advertising is a huge, rapidly growing advertising market in today's
world. One common form of online advertising is using image ads. A decision is
made (often in real time) every time a user sees an ad, and the advertiser is
eager to determine the best ad to display. Consequently, many algorithms have
been developed that calculate the optimal ad to show to the current user at the
present time. Typically, these algorithms focus on variations of the ad,
optimizing among different properties such as background color, image size, or
set of images. However, there is a more fundamental layer. Our study looks at
new qualities of ads that can be determined before an ad is shown (rather than
online optimization) and defines which ads are most likely to be successful.
  We present a set of novel algorithms that utilize deep-learning image
processing, machine learning, and graph theory to investigate online
advertising and to construct prediction models which can foresee an image ad's
success. We evaluated our algorithms on a dataset with over 260,000 ad images,
as well as a smaller dataset specifically related to the automotive industry,
and we succeeded in constructing regression models for ad image click rate
prediction. The obtained results emphasize the great potential of using
deep-learning algorithms to effectively and efficiently analyze image ads and
to create better and more innovative online ads. Moreover, the algorithms
presented in this paper can help predict ad success and can be applied to
analyze other large-scale image corpora.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00584</identifier>
 <datestamp>2015-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00584</id><created>2015-09-02</created><authors><author><keyname>B&#xe1;tfai</keyname><forenames>Norbert</forenames></author></authors><title>Turing's Imitation Game has been Improved</title><categories>cs.AI</categories><msc-class>68T42</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using the recently introduced universal computing model, called orchestrated
machine, that represents computations in a dissipative environment, we consider
a new kind of interpretation of Turing's Imitation Game. In addition we raise
the question whether the intelligence may show fractal properties. Then we
sketch a vision of what robotic cars are going to do in the future. Finally we
give the specification of an artificial life game based on the concept of
orchestrated machines. The purpose of this paper is to start the search for
possible relationships between these different topics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00594</identifier>
 <datestamp>2015-10-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00594</id><created>2015-09-02</created><authors><author><keyname>Gao</keyname><forenames>Jian</forenames></author><author><keyname>Zhou</keyname><forenames>Tao</forenames></author></authors><title>Evaluating user reputation in online rating systems via an iterative
  group-based ranking method</title><categories>cs.IR cs.SI physics.data-an</categories><comments>12 pages, 9 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reputation is a valuable asset in online social lives and it has drawn
increased attention. How to evaluate user reputation in online rating systems
is especially significant due to the existence of spamming attacks. To address
this issue, so far, a variety of methods have been proposed, including
network-based methods, quality-based methods and group-based ranking method. In
this paper, we propose an iterative group-based ranking (IGR) method by
introducing an iterative reputation-allocation process into the original
group-based ranking (GR) method. More specifically, users with higher
reputation have higher weights in dominating the corresponding group sizes. The
reputation of users and the corresponding group sizes are iteratively updated
until they become stable. Results on two real data sets suggest that the
proposed IGR method has better performance and its robustness is considerably
improved comparing with the original GR method. Our work highlights the
positive role of users' grouping behavior towards a better reputation
evaluation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00595</identifier>
 <datestamp>2015-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00595</id><created>2015-09-02</created><authors><author><keyname>Gorjestani</keyname><forenames>Mahdi</forenames></author><author><keyname>Shadkam</keyname><forenames>Elham</forenames></author><author><keyname>Parvizi</keyname><forenames>Mehdi</forenames></author><author><keyname>Aminzadegan</keyname><forenames>Sajedeh</forenames></author></authors><title>A hybrid COA-DEA method for solving multi-objective problems</title><categories>math.OC cs.NE</categories><doi>10.5121/ijcsa.2015.5405</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Cuckoo optimization algorithm (COA) is developed for solving
single-objective problems and it cannot be used for solving multi-objective
problems. So the multi-objective cuckoo optimization algorithm based on data
envelopment analysis (DEA) is developed in this paper and it can gain the
efficient Pareto frontiers. This algorithm is presented by the CCR model of DEA
and the output-oriented approach of it. The selection criterion is higher
efficiency for next iteration of the proposed hybrid method. So the profit
function of the COA is replaced by the efficiency value that is obtained from
DEA. This algorithm is compared with other methods using some test problems.
The results shows using COA and DEA approach for solving multi-objective
problems increases the speed and the accuracy of the generated solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00600</identifier>
 <datestamp>2015-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00600</id><created>2015-09-02</created><updated>2015-12-16</updated><authors><author><keyname>Lertkultanon</keyname><forenames>Puttichai</forenames></author><author><keyname>Pham</keyname><forenames>Quang-Cuong</forenames></author></authors><title>A Single-Query Manipulation Planner</title><categories>cs.RO</categories><comments>8 pages, 7 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In manipulation tasks, a robot interacts with movable object(s). The
configuration space in manipulation planning is thus the Cartesian product of
the configuration space of the robot with those of the movable objects. It is
the complex structure of such a &quot;Composite Configuration Space&quot; that makes
manipulation planning particularly challenging. Previous works approximate the
connectivity of the Composite Configuration Space by means of discretization or
by creating random roadmaps. Such approaches involve an extensive
pre-processing phase, which furthermore has to be re-done each time the
environment changes. In this paper, we propose a high-level Grasp-Placement
Table similar to that proposed by Tournassoud et al. (1987), but which does not
require any discretization or heavy pre-processing. The table captures the
potential connectivity of the Composite Configuration Space while being
specific only to the movable object: in particular, it does not require to be
re-computed when the environment changes. During the query phase, the table is
used to guide a tree-based planner that explores the space systematically. Our
simulations and experiments show that the proposed method enables improvements
in both running time and trajectory quality as compared to existing approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00602</identifier>
 <datestamp>2015-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00602</id><created>2015-09-02</created><authors><author><keyname>Laqrichi</keyname><forenames>S</forenames></author><author><keyname>Gourc</keyname><forenames>Didier</forenames></author><author><keyname>Marmier</keyname><forenames>Fran&#xe7;ois</forenames></author></authors><title>Toward an effort estimation model for software projects integrating risk</title><categories>cs.SE stat.CO</categories><comments>in 22nd International Conference on Production Research, 2013,
  Iguassu Falls, Brazil. 2013</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  According to a study of The Standish Group International, 44% of software
projects cost more and last longer than expected. More accurate the effort
estimation is; the better the enterprise gets organized and the more the
software project respects the commitments on budget, time and quality.
Enhancing the accuracy of effort estimation remains an ongoing challenge to
software professionals. Several factors can influence the accuracy of effort
estimation, namely the immaterial aspect of information system projects, new
technologies and the lack of return on experience. However, the most important
factor of cost and delay increase is software risks. A software risk is an
uncertain event with a negative consequence on the software project. In this
article, we propose a methodology to take into account risk exposure analysis
in the effort estimation model. In the literature, this issue is little
addressed and few approaches are investigated. In this research work, we first
present an overview of these approaches and their limits. Then, we propose an
effort estimation model that improves the accuracy of estimation by integrating
software risks. We finally apply this model to a case study and compare its
results to the results of a classic model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00608</identifier>
 <datestamp>2015-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00608</id><created>2015-09-02</created><authors><author><keyname>Lomuscio</keyname><forenames>Alessio</forenames></author><author><keyname>Michaliszyn</keyname><forenames>Jakub</forenames></author></authors><title>Model Checking Epistemic Halpern-Shoham Logic Extended with Regular
  Expressions</title><categories>cs.LO</categories><comments>19 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Epistemic Halpern-Shoham logic (EHS) is a temporal-epistemic logic that
combines the interval operators of the Halpern-Shoham logic with epistemic
modalities. The semantics of EHS is based on interpreted systems whose
labelling function is defined on the endpoints of intervals. We show that this
definition can be generalised by allowing the labelling function to be based on
the whole interval by means of regular expressions. We prove that all the
positive results known for EHS, notably the attractive complexity of its model
checking problem for some of its fragments, still hold for its generalisation.
We also propose the new logic EHSre which operates on standard Kripke
structures and has expressive power equivalent to that of EHS with regular
expressions. We compare the expressive power of EHSre with standard temporal
logics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00622</identifier>
 <datestamp>2015-10-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00622</id><created>2015-09-02</created><updated>2015-10-12</updated><authors><author><keyname>Freydenberger</keyname><forenames>Dominik D.</forenames></author><author><keyname>Gawrychowski</keyname><forenames>Pawel</forenames></author><author><keyname>Karhum&#xe4;ki</keyname><forenames>Juhani</forenames></author><author><keyname>Manea</keyname><forenames>Florin</forenames></author><author><keyname>Rytter</keyname><forenames>Wojciech</forenames></author></authors><title>Testing k-binomial equivalence</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two words $w_1$ and $w_2$ are said to be $k$-binomial equivalent if every
non-empty word $x$ of length at most $k$ over the alphabet of $w_1$ and $w_2$
appears as a scattered factor of $w_1$ exactly as many times as it appears as a
scattered factor of $w_2$. We give two different polynomial-time algorithms
testing the $k$-binomial equivalence of two words. The first one is
deterministic (but the degree of the corresponding polynomial is too high) and
the second one is randomised (it is more direct and more efficient). These are
the first known algorithms for the problem which run in polynomial time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00630</identifier>
 <datestamp>2015-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00630</id><created>2015-09-02</created><updated>2015-11-18</updated><authors><author><keyname>Vu</keyname><forenames>Ky</forenames></author><author><keyname>Poirion</keyname><forenames>Pierre-Louis</forenames></author><author><keyname>Liberti</keyname><forenames>Leo</forenames></author></authors><title>Gaussian random projections for Euclidean membership problems</title><categories>math.OC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss the application of random projections to the fundamental problem
of deciding whether a given point in a Euclidean space belongs to a given set.
We show that, under a number of different assumptions, the feasibility and
infeasibility of this problem are preserved with high probability when the
problem data is projected to a lower dimensional space. Our results are
applicable to any algorithmic setting which needs to solve Euclidean membership
problems in a high-dimensional space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00641</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00641</id><created>2015-09-02</created><authors><author><keyname>Li</keyname><forenames>Gang</forenames></author><author><keyname>Wang</keyname><forenames>Tao</forenames></author><author><keyname>Ye</keyname><forenames>Ming-Yong</forenames></author><author><keyname>Song</keyname><forenames>and He-Shan</forenames></author></authors><title>Weak measurement combined with quantum delayed-choice experiment and
  implementation in optomechanical system</title><categories>quant-ph cond-mat.quant-gas cs.IT math.IT</categories><doi>10.1140/epjd/e2015-60342-x</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Weak measurement [1,19] combined with quantum delayed-choice experiment that
use quantum beam splitter instead of the beam splitter give rise to a
surprising amplification effect, i.e., counterintuitive negative amplification
effect. We show that this effect is caused by the wave and particle behaviours
of the system to be and can't be explained by a semiclassical wave theory, due
to the entanglement of the system and the ancilla in quantum beam splitter. The
amplification mechanism about wave-particle duality in quantum mechanics lead
us to a scheme for implementation of weak measurement in optomechanical system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00643</identifier>
 <datestamp>2015-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00643</id><created>2015-09-02</created><authors><author><keyname>Gadyatskaya</keyname><forenames>Olga</forenames></author></authors><title>How to Generate Security Cameras: Towards Defence Generation for
  Socio-Technical Systems</title><categories>cs.CR</categories><comments>GraMSec 2015, 16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently security researchers have started to look into automated generation
of attack trees from socio-technical system models. The obvious next step in
this trend of automated risk analysis is automating the selection of security
controls to treat the detected threats. However, the existing socio-technical
models are too abstract to represent all security controls recommended by
practitioners and standards. In this paper we propose an attack-defence model,
consisting of a set of attack-defence bundles, to be generated and maintained
with the socio-technical model. The attack-defence bundles can be used to
synthesise attack-defence trees directly from the model to offer basic
attack-defence analysis, but also they can be used to select and maintain the
security controls that cannot be handled by the model itself.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00645</identifier>
 <datestamp>2015-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00645</id><created>2015-09-02</created><authors><author><keyname>Mandloi</keyname><forenames>Manish</forenames></author><author><keyname>Hussain</keyname><forenames>Mohammed Azahar</forenames></author><author><keyname>Bhatia</keyname><forenames>Vimal</forenames></author></authors><title>Improved Multiple Feedback Successive Interference Cancellation
  Algorithm for Near-Optimal MIMO Detection</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, we propose an improved multiple feedback successive
interference cancellation (IMF-SIC) algorithm for symbol vector detection in
multiple-input multiple-output (MIMO) spatial multiplexing systems. The
multiple feedback (MF) strategy in successive interference cancellation (SIC)
is based on the concept of shadow area constraint (SAC) where, if the decision
falls in the shadow region multiple neighboring constellation points will be
used in the decision feedback loop followed by the conventional SIC. The best
candidate symbol from multiple neighboring symbols is selected using the
maximum likelihood (ML) criteria. However, while deciding the best symbol from
multiple neighboring symbols, the SAC condition may occur in subsequent layers
which results in inaccurate decision. In order to overcome this limitation, in
the proposed algorithm, SAC criteria is checked recursively for each layer.
This results in successful mitigation of error propagation thus significantly
improving the bit error rate (BER) performance. Further, we also propose an
ordered IMF-SIC (OIMF-SIC) where we use log likelihood ratio (LLR) based
dynamic ordering of the detection sequence. In OIMF-SIC, we use the term
dynamic ordering in the sense that the detection order is updated after every
successful decision. Simulation results show that the proposed algorithms
outperform the existing detectors such as conventional SIC and MF-SIC in terms
of BER, and achieves a near ML performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00649</identifier>
 <datestamp>2015-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00649</id><created>2015-09-02</created><authors><author><keyname>Blanqui</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames><affiliation>DEDUCTEAM</affiliation></author></authors><title>Termination of rewrite relations on $\lambda$-terms based on Girard's
  notion of reducibility</title><categories>cs.LO math.LO</categories><proxy>ccsd</proxy><doi>10.1016/j.tcs.2015.07.045</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we show how to extend the notion of reducibility introduced by
Girard for proving the termination of $\beta$-reduction in the polymorphic
$\lambda$-calculus, to prove the termination of various kinds of rewrite
relations on $\lambda$-terms, including rewriting modulo some equational theory
and rewriting with matching modulo $\beta$$\eta$, by using the notion of
computability closure. This provides a powerful termination criterion for
various higher-order rewriting frameworks, including Klop's Combinatory
Reductions Systems with simple types and Nipkow's Higher-order Rewrite Systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00651</identifier>
 <datestamp>2015-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00651</id><created>2015-09-02</created><updated>2015-09-02</updated><authors><author><keyname>Luo</keyname><forenames>Changzhi</forenames></author><author><keyname>Ni</keyname><forenames>Bingbing</forenames></author><author><keyname>Yuan</keyname><forenames>Jun</forenames></author><author><keyname>Wang</keyname><forenames>Jianfeng</forenames></author><author><keyname>Yan</keyname><forenames>Shuicheng</forenames></author><author><keyname>Wang</keyname><forenames>Meng</forenames></author></authors><title>Manipulated Object Proposal: A Discriminative Object Extraction and
  Feature Fusion Framework for First-Person Daily Activity Recognition</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Detecting and recognizing objects interacting with humans lie in the center
of first-person (egocentric) daily activity recognition. However, due to noisy
camera motion and frequent changes in viewpoint and scale, most of the previous
egocentric action recognition methods fail to capture and model highly
discriminative object features. In this work, we propose a novel pipeline for
first-person daily activity recognition, aiming at more discriminative object
feature representation and object-motion feature fusion. Our object feature
extraction and representation pipeline is inspired by the recent success of
object hypotheses and deep convolutional neural network based detection
frameworks. Our key contribution is a simple yet effective manipulated object
proposal generation scheme. This scheme leverages motion cues such as motion
boundary and motion magnitude (in contrast, camera motion is usually considered
as &quot;noise&quot; for most previous methods) to generate a more compact and
discriminative set of object proposals, which are more closely related to the
objects which are being manipulated. Then, we learn more discriminative object
detectors from these manipulated object proposals based on region-based
convolutional neural network (R-CNN). Meanwhile, we develop a network based
feature fusion scheme which better combines object and motion features. We show
in experiments that the proposed framework significantly outperforms the
state-of-the-art recognition performance on a challenging first-person daily
activity benchmark.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00666</identifier>
 <datestamp>2016-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00666</id><created>2015-09-02</created><updated>2016-02-18</updated><authors><author><keyname>Beklemishev</keyname><forenames>Lev D.</forenames></author></authors><title>A note on strictly positive logics and word rewriting systems</title><categories>math.LO cs.LO</categories><comments>9 pages</comments><msc-class>03D03, 03B45, 03B47</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We establish a natural translation from word rewriting systems to strictly
positive polymodal logics. Thereby, the latter can be considered as a
generalization of the former. As a corollary we obtain examples of undecidable
strictly positive normal modal logics. The translation has its counterpart on
the level of proofs: we formulate a natural deep inference proof system for
strictly positive logics generalizing derivations in word rewriting systems. We
also formulate some open questions related to the theory of modal companions of
superintuitionistic logics that was initiated by L.L. Maximova and V.V.
Rybakov.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00669</identifier>
 <datestamp>2015-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00669</id><created>2015-09-02</created><authors><author><keyname>Atkins</keyname><forenames>Ross</forenames></author><author><keyname>McDiarmid</keyname><forenames>Colin</forenames></author></authors><title>Extremal Distances for Subtree Transfer Operations in Binary Trees</title><categories>math.CO cs.DS</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Three standard subtree transfer operations for binary trees, used in
particular for phylogenetic trees, are: tree bisection and reconnection
($TBR$), subtree prune and regraft ($SPR$) and rooted subtree prune and regraft
($rSPR$). For a pair of leaf-labelled binary trees with $n$ leaves, the maximum
number of such moves required to transform one into the other is
$n-\Theta(\sqrt{n})$, extending a result of Ding, Grunewald and Humphries. We
show that if the pair is chosen uniformly at random, then the expected number
of moves required to transfer one into the other is $n-\Theta(n^{2/3})$. These
results may be phrased in terms of agreement forests: we also give extensions
for more than two binary trees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00670</identifier>
 <datestamp>2015-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00670</id><created>2015-09-02</created><authors><author><keyname>Doran</keyname><forenames>Derek</forenames></author><author><keyname>Yelne</keyname><forenames>Samir</forenames></author><author><keyname>Massari</keyname><forenames>Luisa</forenames></author><author><keyname>Calzarossa</keyname><forenames>Maria-Carla</forenames></author><author><keyname>Jackson</keyname><forenames>LaTrelle</forenames></author><author><keyname>Moriarty</keyname><forenames>Glen</forenames></author></authors><title>Stay Awhile and Listen: User Interactions in a Crowdsourced Platform
  Offering Emotional Support</title><categories>cs.SI</categories><doi>10.1145/2808797.2809311</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Internet and online-based social systems are rising as the dominant mode of
communication in society. However, the public or semi-private environment under
which most online communications operate under do not make them suitable
channels for speaking with others about personal or emotional problems. This
has led to the emergence of online platforms for emotional support offering
free, anonymous, and confidential conversations with live listeners. Yet very
little is known about the way these platforms are utilized, and if their
features and design foster strong user engagement. This paper explores the
utilization and the interaction features of hundreds of thousands of users on 7
Cups of Tea, a leading online platform offering online emotional support. It
dissects the level of activity of hundreds of thousands of users, the patterns
by which they engage in conversation with each other, and uses machine learning
methods to find factors promoting engagement. The study may be the first to
measure activities and interactions in a large-scale online social system that
fosters peer-to-peer emotional support.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00684</identifier>
 <datestamp>2015-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00684</id><created>2015-09-02</created><authors><author><keyname>Angelini</keyname><forenames>Patrizio</forenames></author><author><keyname>Da Lozzo</keyname><forenames>Giordano</forenames></author><author><keyname>Di Bartolomeo</keyname><forenames>Marco</forenames></author><author><keyname>Di Donato</keyname><forenames>Valentino</forenames></author><author><keyname>Patrignani</keyname><forenames>Maurizio</forenames></author><author><keyname>Roselli</keyname><forenames>Vincenzo</forenames></author><author><keyname>Tollis</keyname><forenames>Ioannis G.</forenames></author></authors><title>L-Drawings of Directed Graphs</title><categories>cs.DS cs.CG</categories><comments>11 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce L-drawings, a novel paradigm for representing directed graphs
aiming at combining the readability features of orthogonal drawings with the
expressive power of matrix representations. In an L-drawing, vertices have
exclusive $x$- and $y$-coordinates and edges consist of two segments, one
exiting the source vertically and one entering the destination horizontally.
  We study the problem of computing L-drawings using minimum ink. We prove its
NP-completeness and provide a heuristics based on a polynomial-time algorithm
that adds a vertex to a drawing using the minimum additional ink. We performed
an experimental analysis of the heuristics which confirms its effectiveness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00685</identifier>
 <datestamp>2015-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00685</id><created>2015-09-02</created><updated>2015-09-03</updated><authors><author><keyname>Rush</keyname><forenames>Alexander M.</forenames></author><author><keyname>Chopra</keyname><forenames>Sumit</forenames></author><author><keyname>Weston</keyname><forenames>Jason</forenames></author></authors><title>A Neural Attention Model for Abstractive Sentence Summarization</title><categories>cs.CL cs.AI</categories><comments>Proceedings of EMNLP 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Summarization based on text extraction is inherently limited, but
generation-style abstractive methods have proven challenging to build. In this
work, we propose a fully data-driven approach to abstractive sentence
summarization. Our method utilizes a local attention-based model that generates
each word of the summary conditioned on the input sentence. While the model is
structurally simple, it can easily be trained end-to-end and scales to a large
amount of training data. The model shows significant performance gains on the
DUC-2004 shared task compared with several strong baselines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00689</identifier>
 <datestamp>2015-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00689</id><created>2015-09-02</created><authors><author><keyname>Borbely</keyname><forenames>Rebecca Schuller</forenames></author></authors><title>On Normalized Compression Distance and Large Malware</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Normalized Compression Distance (NCD) is a popular tool that uses compression
algorithms to cluster and classify data in a wide range of applications.
Existing discussions of NCD's theoretical merit rely on certain theoretical
properties of compression algorithms. However, we demonstrate that many popular
compression algorithms don't seem to satisfy these theoretical properties. We
explore the relationship between some of these properties and file size,
demonstrating that this theoretical problem is actually a practical problem for
classifying malware with large file sizes, and we then introduce some variants
of NCD that mitigate this problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00690</identifier>
 <datestamp>2015-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00690</id><created>2015-09-01</created><authors><author><keyname>Ansari</keyname><forenames>Zahid</forenames></author><author><keyname>Azeem</keyname><forenames>M. F.</forenames></author><author><keyname>Babu</keyname><forenames>A. Vinaya</forenames></author><author><keyname>Ahmed</keyname><forenames>Waseem</forenames></author></authors><title>A Fuzzy Approach for Feature Evaluation and Dimensionality Reduction to
  Improve the Quality of Web Usage Mining Results</title><categories>cs.DB cs.AI cs.IR</categories><journal-ref>International Journal on Advanced Science Engineering and
  Information Technology, pp. 67-73 Vol. 2 No. 6, 2012. (ISSN: 2088-5334,
  INSIGHT Publishers, Indonesia)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Web Usage Mining is the application of data mining techniques to web usage
log repositories in order to discover the usage patterns that can be used to
analyze the users navigational behavior. During the preprocessing stage, raw
web log data is transformed into a set of user profiles. Each user profile
captures a set of URLs representing a user session. Clustering can be applied
to this sessionized data in order to capture similar interests and trends among
users navigational patterns. Since the sessionized data may contain thousands
of user sessions and each user session may consist of hundreds of URL accesses,
dimensionality reduction is achieved by eliminating the low support URLs. Very
small sessions are also removed in order to filter out the noise from the data.
But direct elimination of low support URLs and small sized sessions may results
in loss of a significant amount of information especially when the count of low
support URLs and small sessions is large. We propose a fuzzy solution to deal
with this problem by assigning weights to URLs and user sessions based on a
fuzzy membership function. After assigning the weights we apply a Fuzzy c-Mean
Clustering algorithm to discover the clusters of user profiles. In this paper,
we describe our fuzzy set theoretic approach to perform feature selection (or
dimensionality reduction) and session weight assignment. Finally we compare our
soft computing based approach of dimensionality reduction with the traditional
approach of direct elimination of small sessions and low support count URLs.
Our results show that fuzzy feature evaluation and dimensionality reduction
results in better performance and validity indices for the discovered clusters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00692</identifier>
 <datestamp>2015-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00692</id><created>2015-09-01</created><authors><author><keyname>Ansari</keyname><forenames>Zahid</forenames></author><author><keyname>Ahmed</keyname><forenames>Waseem</forenames></author><author><keyname>Azeem</keyname><forenames>M. F.</forenames></author><author><keyname>Babu</keyname><forenames>A. Vinaya</forenames></author></authors><title>Discovery of Web Usage Profiles Using Various Clustering Techniques</title><categories>cs.DB cs.IR cs.LG</categories><comments>arXiv admin note: substantial text overlap with arXiv:1507.03340</comments><journal-ref>International Journal of Computer Information Systems, pp. 18-27
  Vol. 1, No. 3, July 2011. (ISSN 2229-5208, Silicon Valley Publishers, United
  Kingdom)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The explosive growth of World Wide Web (WWW) has necessitated the development
of Web personalization systems in order to understand the user preferences to
dynamically serve customized content to individual users. To reveal information
about user preferences from Web usage data, Web Usage Mining (WUM) techniques
are extensively being applied to the Web log data. Clustering techniques are
widely used in WUM to capture similar interests and trends among users
accessing a Web site. Clustering aims to divide a data set into groups or
clusters where inter-cluster similarities are minimized while the intra cluster
similarities are maximized. This paper reviews four of the popularly used
clustering techniques: k-Means, k-Medoids, Leader and DBSCAN. These techniques
are implemented and tested against the Web user navigational data. Performance
and validity results of each technique are presented and compared.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00693</identifier>
 <datestamp>2015-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00693</id><created>2015-09-01</created><authors><author><keyname>Ansari</keyname><forenames>Zahid</forenames></author><author><keyname>Azeem</keyname><forenames>Mohammad Fazle</forenames></author><author><keyname>Babu</keyname><forenames>A. Vinaya</forenames></author><author><keyname>Ahmed</keyname><forenames>Waseem</forenames></author></authors><title>A Fuzzy Clustering Based Approach for Mining Usage Profiles from Web Log
  Data</title><categories>cs.DB cs.IR</categories><journal-ref>International Journal of Computer Science and Information
  Security, pp. 70-79 Vol. 9, No. 6, June 2011. (ISSN 1947-5500, IJCSIS
  Publications, United State)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The World Wide Web continues to grow at an amazing rate in both the size and
complexity of Web sites and is well on its way to being the main reservoir of
information and data. Due to this increase in growth and complexity of WWW, web
site publishers are facing increasing difficulty in attracting and retaining
users. To design popular and attractive websites publishers must understand
their users needs. Therefore analyzing users behaviour is an important part of
web page design. Web Usage Mining (WUM) is the application of data mining
techniques to web usage log repositories in order to discover the usage
patterns that can be used to analyze the users navigational behavior. WUM
contains three main steps: preprocessing, knowledge extraction and results
analysis. The goal of the preprocessing stage in Web usage mining is to
transform the raw web log data into a set of user profiles. Each such profile
captures a sequence or a set of URLs representing a user session.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00705</identifier>
 <datestamp>2015-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00705</id><created>2015-09-02</created><authors><author><keyname>Sashikanth</keyname><forenames>Dinesh Balaji</forenames></author></authors><title>Analysis of Communication Pattern with Scammers in Enron Corpus</title><categories>cs.CL</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is an exploratory analysis into fraud detection taking Enron email
corpus as the case study. The paper posits conclusions like strict servitude
and unquestionable faith among employees as breeding grounds for sham among
higher executives. We also try to infer on the nature of communication between
fraudulent employees and between non- fraudulent-fraudulent employees
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00714</identifier>
 <datestamp>2015-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00714</id><created>2015-09-02</created><authors><author><keyname>Chandra</keyname><forenames>Nitish</forenames></author><author><keyname>Khare</keyname><forenames>Kedar</forenames></author></authors><title>Dictionary based Approach to Edge Detection</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Edge detection is a very essential part of image processing, as quality and
accuracy of detection determines the success of further processing. We have
developed a new self learning technique for edge detection using dictionary
comprised of eigenfilters constructed using features of the input image. The
dictionary based method eliminates the need of pre or post processing of the
image and accounts for noise, blurriness, class of image and variation of
illumination during the detection process itself. Since, this method depends on
the characteristics of the image, the new technique can detect edges more
accurately and capture greater detail than existing algorithms such as Sobel,
Prewitt Laplacian of Gaussian, Canny method etc which use generic filters and
operators. We have demonstrated its application on various classes of images
such as text, face, barcodes, traffic and cell images. An application of this
technique to cell counting in a microscopic image is also presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00715</identifier>
 <datestamp>2015-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00715</id><created>2015-09-02</created><authors><author><keyname>Dalai</keyname><forenames>Marco</forenames></author><author><keyname>Winter</keyname><forenames>Andreas</forenames></author></authors><title>Constant Compositions in the Sphere Packing Bound for Classical-Quantum
  Channels</title><categories>cs.IT math.IT quant-ph</categories><comments>Extended version of arXiv:1401.6039v2</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The sphere packing bound, in the form given by Shannon, Gallager and
Berlekamp, was recently extended to classical-quantum channels, and it was
shown that this creates a natural setting for combining probabilistic
approaches with some combinatorial ones such as the Lov\'asz theta function. In
this paper, we extend the study to the case of constant composition codes. We
first extend the sphere packing bound for classical-quantum channels to this
case, and we then show that the obtained result is related to a variation of
the Lov\'asz theta function studied by Marton. We then propose a further
extension to the case of varying channels and codewords with a constant
conditional composition given a particular sequence. This extension is then
applied to auxiliary channels to deduce a bound which can be interpreted as an
extension of the Elias bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00720</identifier>
 <datestamp>2015-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00720</id><created>2015-09-02</created><authors><author><keyname>Klemz</keyname><forenames>Boris</forenames></author><author><keyname>N&#xf6;llenburg</keyname><forenames>Martin</forenames></author><author><keyname>Prutkin</keyname><forenames>Roman</forenames></author></authors><title>Recognizing Weighted Disk Contact Graphs</title><categories>cs.CG</categories><comments>24 pages, 21 figures, extended version of a paper to appear at the
  International Symposium on Graph Drawing and Network Visualization (GD) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Disk contact representations realize graphs by mapping vertices bijectively
to interior-disjoint disks in the plane such that two disks touch each other if
and only if the corresponding vertices are adjacent in the graph. Deciding
whether a vertex-weighted planar graph can be realized such that the disks'
radii coincide with the vertex weights is known to be NP-hard. In this work, we
reduce the gap between hardness and tractability by analyzing the problem for
special graph classes. We show that it remains NP-hard for outerplanar graphs
with unit weights and for stars with arbitrary weights, strengthening the
previous hardness results. On the positive side, we present constructive
linear-time recognition algorithms for caterpillars with unit weights and for
embedded stars with arbitrary weights.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00721</identifier>
 <datestamp>2015-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00721</id><created>2015-09-02</created><authors><author><keyname>Shchurov</keyname><forenames>Andrey A.</forenames></author></authors><title>A Multilayer Model of Computer Networks</title><categories>cs.NI</categories><comments>5 pages, 4 figures. ISSN:2231-2803</comments><journal-ref>International Journal of Computer Trends and Technology (IJCTT)
  V26(1):12-16, August 2015</journal-ref><doi>10.14445/22312803/IJCTT-V26P103</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The fundamental concept of applying the system methodology to network
analysis declares that network architecture should take into account services
and applications which this network provides and supports. This work introduces
a formal model of computer networks on the basis of the hierarchical multilayer
networks. In turn, individual layers are represented as multiplex networks. The
concept of layered networks provides conditions of top-down consistency of the
model. Next, we determined the necessary set of layers for network architecture
representation with regard to applying the system methodology to network
analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00727</identifier>
 <datestamp>2015-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00727</id><created>2015-09-02</created><authors><author><keyname>Anderson</keyname><forenames>Joseph</forenames></author><author><keyname>Goyal</keyname><forenames>Navin</forenames></author><author><keyname>Nandi</keyname><forenames>Anupama</forenames></author><author><keyname>Rademacher</keyname><forenames>Luis</forenames></author></authors><title>Heavy-tailed Independent Component Analysis</title><categories>cs.LG math.ST stat.CO stat.ML stat.TH</categories><comments>30 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Independent component analysis (ICA) is the problem of efficiently recovering
a matrix $A \in \mathbb{R}^{n\times n}$ from i.i.d. observations of $X=AS$
where $S \in \mathbb{R}^n$ is a random vector with mutually independent
coordinates. This problem has been intensively studied, but all existing
efficient algorithms with provable guarantees require that the coordinates
$S_i$ have finite fourth moments. We consider the heavy-tailed ICA problem
where we do not make this assumption, about the second moment. This problem
also has received considerable attention in the applied literature. In the
present work, we first give a provably efficient algorithm that works under the
assumption that for constant $\gamma &gt; 0$, each $S_i$ has finite
$(1+\gamma)$-moment, thus substantially weakening the moment requirement
condition for the ICA problem to be solvable. We then give an algorithm that
works under the assumption that matrix $A$ has orthogonal columns but requires
no moment assumptions. Our techniques draw ideas from convex geometry and
exploit standard properties of the multivariate spherical Gaussian distribution
in a novel way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00728</identifier>
 <datestamp>2015-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00728</id><created>2015-09-02</created><authors><author><keyname>Thunberg</keyname><forenames>Johan</forenames></author><author><keyname>Bernard</keyname><forenames>Florian</forenames></author><author><keyname>Goncalves</keyname><forenames>Jorge</forenames></author></authors><title>On Transitive Consistency for Linear Invertible Transformations between
  Euclidean Coordinate Systems</title><categories>math.OC cs.CV cs.MA cs.NA stat.ML</categories><comments>25 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Transitive consistency is an intrinsic property for collections of linear
invertible transformations between Euclidean coordinate frames. In practice,
when the transformations are estimated from data, this property is lacking.
This work addresses the problem of synchronizing transformations that are not
transitively consistent. Once the transformations have been synchronized, they
satisfy the transitive consistency condition - a transformation from frame $A$
to frame $C$ is equal to the composite transformation of first transforming A
to B and then transforming B to C. The coordinate frames correspond to nodes in
a graph and the transformations correspond to edges in the same graph. Two
direct or centralized synchronization methods are presented for different graph
topologies; the first one for quasi-strongly connected graphs, and the second
one for connected graphs. As an extension of the second method, an iterative
Gauss-Newton method is presented, which is later adapted to the case of affine
and Euclidean transformations. Two distributed synchronization methods are also
presented for orthogonal matrices, which can be seen as distributed versions of
the two direct or centralized methods; they are similar in nature to standard
consensus protocols used for distributed averaging. When the transformations
are orthogonal matrices, a bound on the optimality gap can be computed.
Simulations show that the gap is almost right, even for noise large in
magnitude. This work also contributes on a theoretical level by providing
linear algebraic relationships for transitively consistent transformations. One
of the benefits of the proposed methods is their simplicity - basic linear
algebraic methods are used, e.g., the Singular Value Decomposition (SVD). For a
wide range of parameter settings, the methods are numerically validated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00731</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00731</id><created>2015-09-02</created><updated>2016-02-19</updated><authors><author><keyname>Sanguinetti</keyname><forenames>Luca</forenames></author><author><keyname>Couillet</keyname><forenames>Romain</forenames></author><author><keyname>Debbah</keyname><forenames>Merouane</forenames></author></authors><title>Large System Analysis of Base Station Cooperation for Power Minimization</title><categories>cs.IT math.IT</categories><comments>32 pages, 6 figures, submitted to IEEE Trans. Wireless Commun. A
  preliminary version of this paper was presented at the IEEE Global
  Communication Conference, San Diego, USA, Dec. 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work focuses on a large-scale multi-cell multi-user MIMO system in which
$L$ base stations (BSs) of $N$ antennas each communicate with $K$
single-antenna user equipments. We consider the design of the linear precoder
that minimizes the total power consumption while ensuring target user rates.
Three configurations with different degrees of cooperation among BSs are
considered: the coordinated beamforming scheme (only channel state information
is shared among BSs), the coordinated multipoint MIMO processing technology or
network MIMO (channel state and data cooperation), and a single cell
beamforming scheme (only local channel state information is used for
beamforming while channel state cooperation is needed for power allocation).
The analysis is conducted assuming that $N$ and $K$ grow large with a non
trivial ratio $K/N$ and imperfect channel state information (modeled by the
generic Gauss-Markov formulation form) is available at the BSs. Tools of random
matrix theory are used to compute, in explicit form, deterministic
approximations for: (i) the parameters of the optimal precoder; (ii) the powers
needed to ensure target rates; and (iii) the total transmit power. These
results are instrumental to get further insight into the structure of the
optimal precoders and also to reduce the implementation complexity in
large-scale networks. Numerical results are used to validate the asymptotic
analysis in the finite system regime and to make comparisons among the
different configurations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00737</identifier>
 <datestamp>2015-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00737</id><created>2015-09-02</created><authors><author><keyname>Pickem</keyname><forenames>Daniel</forenames></author><author><keyname>Egerstedt</keyname><forenames>Magnus</forenames></author><author><keyname>Shamma</keyname><forenames>Jeff S.</forenames></author></authors><title>A Game-theoretic Formulation of the Homogeneous Self-Reconfiguration
  Problem</title><categories>cs.MA cs.GT cs.SY</categories><comments>8 pages, 5 figures, 2 algorithms</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we formulate the homogeneous two- and three-dimensional
self-reconfiguration problem over discrete grids as a constrained potential
game. We develop a game-theoretic learning algorithm based on the
Metropolis-Hastings algorithm that solves the self-reconfiguration problem in a
globally optimal fashion. Both a centralized and a fully distributed algorithm
are presented and we show that the only stochastically stable state is the
potential function maximizer, i.e. the desired target configuration. These
algorithms compute transition probabilities in such a way that even though each
agent acts in a self-interested way, the overall collective goal of
self-reconfiguration is achieved. Simulation results confirm the feasibility of
our approach and show convergence to desired target configurations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00757</identifier>
 <datestamp>2015-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00757</id><created>2015-09-02</created><authors><author><keyname>Golovach</keyname><forenames>Petr A.</forenames></author><author><keyname>Requil&#xe9;</keyname><forenames>Cl&#xe9;ment</forenames></author><author><keyname>Thilikos</keyname><forenames>Dimitrios M.</forenames></author></authors><title>Variants of Plane Diameter Completion</title><categories>cs.DS math.CO</categories><comments>Accepted in IPEC 2015</comments><msc-class>68R10, 05C85, 05C10</msc-class><acm-class>G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The {\sc Plane Diameter Completion} problem asks, given a plane graph $G$ and
a positive integer $d$, if it is a spanning subgraph of a plane graph $H$ that
has diameter at most $d$. We examine two variants of this problem where the
input comes with another parameter $k$. In the first variant, called BPDC, $k$
upper bounds the total number of edges to be added and in the second, called
BFPDC, $k$ upper bounds the number of additional edges per face. We prove that
both problems are {\sf NP}-complete, the first even for 3-connected graphs of
face-degree at most 4 and the second even when $k=1$ on 3-connected graphs of
face-degree at most 5. In this paper we give parameterized algorithms for both
problems that run in $O(n^{3})+2^{2^{O((kd)^2\log d)}}\cdot n$ steps.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00764</identifier>
 <datestamp>2015-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00764</id><created>2015-09-02</created><authors><author><keyname>Lamm</keyname><forenames>Sebastian</forenames></author><author><keyname>Sanders</keyname><forenames>Peter</forenames></author><author><keyname>Schulz</keyname><forenames>Christian</forenames></author><author><keyname>Strash</keyname><forenames>Darren</forenames></author><author><keyname>Werneck</keyname><forenames>Renato F.</forenames></author></authors><title>Finding Near-Optimal Independent Sets at Scale</title><categories>cs.DS cs.NE cs.SI</categories><comments>17 pages, 1 figure, 8 tables. arXiv admin note: text overlap with
  arXiv:1502.01687</comments><acm-class>F.2.2; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The independent set problem is NP-hard and particularly difficult to solve in
large sparse graphs. In this work, we develop an advanced evolutionary
algorithm, which incorporates kernelization techniques to compute large
independent sets in huge sparse networks. A recent exact algorithm has shown
that large networks can be solved exactly by employing a branch-and-reduce
technique that recursively kernelizes the graph and performs branching.
However, one major drawback of their algorithm is that, for huge graphs,
branching still can take exponential time. To avoid this problem, we
recursively choose vertices that are likely to be in a large independent set
(using an evolutionary approach), then further kernelize the graph. We show
that identifying and removing vertices likely to be in large independent sets
opens up the reduction space---which not only speeds up the computation of
large independent sets drastically, but also enables us to compute high-quality
independent sets on much larger instances than previously reported in the
literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00773</identifier>
 <datestamp>2015-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00773</id><created>2015-09-02</created><authors><author><keyname>Balliu</keyname><forenames>Alkida</forenames></author><author><keyname>Olivetti</keyname><forenames>Dennis</forenames></author><author><keyname>Babaoglu</keyname><forenames>Ozalp</forenames></author><author><keyname>Marzolla</keyname><forenames>Moreno</forenames></author><author><keyname>S&#xee;rbu</keyname><forenames>Alina</forenames></author></authors><title>A Big Data Analyzer for Large Trace Logs</title><categories>cs.DC</categories><comments>26 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current generation of Internet-based services are typically hosted on large
data centers that take the form of warehouse-size structures housing tens of
thousands of servers. Continued availability of a modern data center is the
result of a complex orchestration among many internal and external actors
including computing hardware, multiple layers of intricate software, networking
and storage devices, electrical power and cooling plants. During the course of
their operation, many of these components produce large amounts of data in the
form of event and error logs that are essential not only for identifying and
resolving problems but also for improving data center efficiency and
management. Most of these activities would benefit significantly from data
analytics techniques to exploit hidden statistical patterns and correlations
that may be present in the data. The sheer volume of data to be analyzed makes
uncovering these correlations and patterns a challenging task. This paper
presents BiDAl, a prototype Java tool for log-data analysis that incorporates
several Big Data technologies in order to simplify the task of extracting
information from data traces produced by large clusters and server farms. BiDAl
provides the user with several analysis languages (SQL, R and Hadoop MapReduce)
and storage backends (HDFS and SQLite) that can be freely mixed and matched so
that a custom tool for a specific task can be easily constructed. BiDAl has a
modular architecture so that it can be extended with other backends and
analysis languages in the future. In this paper we present the design of BiDAl
and describe our experience using it to analyze publicly-available traces from
Google data clusters, with the goal of building a realistic model of a complex
data center.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00777</identifier>
 <datestamp>2015-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00777</id><created>2015-09-02</created><authors><author><keyname>Kim</keyname><forenames>Kwang-Ki K.</forenames></author></authors><title>A Note on the Convexity of $\log \det ( I + KX^{-1} )$ and its
  Constrained Optimization Representation</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This note provides another proof for the {\em convexity} ({\em strict
convexity}) of $\log \det ( I + KX^{-1} )$ over the positive definite cone for
any given positive semidefinite matrix $K \succeq 0$ (positive definite matrix
$K \succ 0$) and the {\em strictly convexity} of $\log \det (K + X^{-1})$ over
the positive definite cone for any given $K \succeq 0$. Equivalent optimization
representation with linear matrix inequalities (LMIs) for the functions $\log
\det ( I + KX^{-1} )$ and $\log \det (K + X^{-1})$ are presented. Their
optimization representations with LMI constraints can be particularly useful
for some related synthetic design problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00779</identifier>
 <datestamp>2015-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00779</id><created>2015-09-02</created><authors><author><keyname>Kalamkar</keyname><forenames>Sanket S.</forenames></author><author><keyname>Banerjee</keyname><forenames>Adrish</forenames></author></authors><title>Interference-Assisted Wireless Energy Harvesting in Cognitive Relay
  Network with Multiple Primary Transceivers</title><categories>cs.IT cs.NI math.IT</categories><comments>6 pages, 5 figures, To be presented at IEEE GLOBECOM 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a spectrum sharing scenario, where a secondary network coexists
with a primary network of multiple transceivers. The secondary network consists
of an energy-constrained decode-and-forward secondary relay which assists the
communication between a secondary transmitter and a destination in the presence
of the interference from multiple primary transmitters. The secondary relay
harvests energy from the received radio-frequency signals, which include the
information signal from the secondary transmitter and the primary interference.
The harvested energy is then used to decode the secondary information and
forward it to the secondary destination. At the relay, we adopt a time
switching policy due to its simplicity that switches between the energy
harvesting and information decoding over time. Specifically, we derive a
closed-form expression for the secondary outage probability under the primary
outage constraint and the peak power constraint at both secondary transmitter
and relay. In addition, we investigate the effect of the number of primary
transceivers on the optimal energy harvesting duration that minimizes the
secondary outage probability. By utilizing the primary interference as a useful
energy source in the energy harvesting phase, the secondary network achieves a
better outage performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00789</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00789</id><created>2015-09-02</created><updated>2016-02-18</updated><authors><author><keyname>Hayes</keyname><forenames>Jamie</forenames></author><author><keyname>Danezis</keyname><forenames>George</forenames></author></authors><title>k-fingerprinting: a Robust Scalable Website Fingerprinting Technique</title><categories>cs.CR</categories><comments>17 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Website fingerprinting enables an attacker to infer which web page a client
is browsing through encrypted or anonymized network connections. We present a
new website fingerprinting technique based on random decision forests and
evaluate performance over standard web pages as well as Tor hidden services, on
a larger scale than previous works. Our technique, k-fingerprinting, performs
better than current state-of-the-art attacks even against website
fingerprinting defenses, and we show that it is possible to launch a website
fingerprinting attack in the face of a large amount of noisy data. We can
correctly determine which of 30 monitored hidden services a client is visiting
with 85% true positive rate (TPR), a false positive rate (FPR) as low as 0.02%,
from a world size of 100,000 unmonitored web pages. We further show that error
rates vary widely between web resources, and thus some patterns of use will be
predictably more vulnerable to attack than others.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00816</identifier>
 <datestamp>2015-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00816</id><created>2015-09-02</created><authors><author><keyname>Jayasuriya</keyname><forenames>Suren</forenames></author><author><keyname>Pediredla</keyname><forenames>Adithya</forenames></author><author><keyname>Sivaramakrishnan</keyname><forenames>Sriram</forenames></author><author><keyname>Molnar</keyname><forenames>Alyosha</forenames></author><author><keyname>Veeraraghavan</keyname><forenames>Ashok</forenames></author></authors><title>Depth Fields: Extending Light Field Techniques to Time-of-Flight Imaging</title><categories>cs.CV</categories><comments>9 pages, 8 figures, Accepted to 3DV 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A variety of techniques such as light field, structured illumination, and
time-of-flight (TOF) are commonly used for depth acquisition in consumer
imaging, robotics and many other applications. Unfortunately, each technique
suffers from its individual limitations preventing robust depth sensing. In
this paper, we explore the strengths and weaknesses of combining light field
and time-of-flight imaging, particularly the feasibility of an on-chip
implementation as a single hybrid depth sensor. We refer to this combination as
depth field imaging. Depth fields combine light field advantages such as
synthetic aperture refocusing with TOF imaging advantages such as high depth
resolution and coded signal processing to resolve multipath interference. We
show applications including synthesizing virtual apertures for TOF imaging,
improved depth mapping through partial and scattering occluders, and single
frequency TOF phase unwrapping. Utilizing space, angle, and temporal coding,
depth fields can improve depth sensing in the wild and generate new insights
into the dimensions of light's plenoptic function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00818</identifier>
 <datestamp>2015-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00818</id><created>2015-09-02</created><authors><author><keyname>Bannister</keyname><forenames>Michael J.</forenames></author><author><keyname>Brown</keyname><forenames>David A.</forenames></author><author><keyname>Eppstein</keyname><forenames>David</forenames></author></authors><title>Confluent Orthogonal Drawings of Syntax Diagrams</title><categories>cs.OH cs.FL</categories><comments>GD 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a pipeline for generating syntax diagrams (also called railroad
diagrams) from context free grammars. Syntax diagrams are a graphical
representation of a context free language, which we formalize abstractly as a
set of mutually recursive nondeterministic finite automata and draw by
combining elements from the confluent drawing, layered drawing, and smooth
orthogonal drawing styles. Within our pipeline we introduce several heuristics
that modify the grammar but preserve the language, improving the aesthetics of
the final drawing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00824</identifier>
 <datestamp>2015-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00824</id><created>2015-09-02</created><authors><author><keyname>Bandeira</keyname><forenames>Afonso S.</forenames></author></authors><title>A note on Probably Certifiably Correct algorithms</title><categories>math.OC cs.DS cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many optimization problems of interest are known to be intractable, and while
there are often heuristics that are known to work on typical instances, it is
usually not easy to determine a posteriori whether the optimal solution was
found. In this short note, we discuss algorithms that not only solve the
problem on typical instances, but also provide a posteriori certificates of
optimality, probably certifiably correct (PCC) algorithms. As an illustrative
example, we present a fast PCC algorithm for minimum bisection under the
stochastic block model and briefly discuss other examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00825</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00825</id><created>2015-09-02</created><updated>2015-09-10</updated><authors><author><keyname>Cruz</keyname><forenames>Rafael M. O.</forenames></author><author><keyname>Sabourin</keyname><forenames>Robert</forenames></author><author><keyname>Cavalcanti</keyname><forenames>George D. C.</forenames></author></authors><title>A DEEP analysis of the META-DES framework for dynamic selection of
  ensemble of classifiers</title><categories>cs.LG stat.ML</categories><comments>47 Pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dynamic ensemble selection (DES) techniques work by estimating the level of
competence of each classifier from a pool of classifiers. Only the most
competent ones are selected to classify a given test sample. Hence, the key
issue in DES is the criterion used to estimate the level of competence of the
classifiers in predicting the label of a given test sample. In order to perform
a more robust ensemble selection, we proposed the META-DES framework using
meta-learning, where multiple criteria are encoded as meta-features and are
passed down to a meta-classifier that is trained to estimate the competence
level of a given classifier. In this technical report, we present a
step-by-step analysis of each phase of the framework during training and test.
We show how each set of meta-features is extracted as well as their impact on
the estimation of the competence level of the base classifier. Moreover, an
analysis of the impact of several factors in the system performance, such as
the number of classifiers in the pool, the use of different linear base
classifiers, as well as the size of the validation data. We show that using the
dynamic selection of linear classifiers through the META-DES framework, we can
solve complex non-linear classification problems where other combination
techniques such as AdaBoost cannot.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00832</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00832</id><created>2015-09-02</created><authors><author><keyname>Ye</keyname><forenames>Chuang</forenames></author><author><keyname>Ozcan</keyname><forenames>Gozde</forenames></author><author><keyname>Gursoy</keyname><forenames>M. Cenk</forenames></author><author><keyname>Velipasalar</keyname><forenames>Senem</forenames></author></authors><title>Multimedia Transmission over Cognitive Radio Channels under Sensing
  Uncertainty</title><categories>cs.IT math.IT</categories><comments>To appear in the IEEE Transactions on Signal Processing</comments><doi>10.1109/TSP.2015.2478747</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the performance of hierarchical modulation-based
multimedia transmission in cognitive radio (CR) systems with imperfect channel
sensing results under constraints on both transmit and interference power
levels. Unequal error protection (UEP) of data transmission using hierarchical
quadrature amplitude modulation (HQAM) is considered in which high priority
(HP) data is protected more than low priority (LP) data. In this setting,
closed-form bit error rate (BER) expressions for HP data and LP data are
derived in Nakagami-$m$ fading channels in the presence of sensing errors.
Subsequently, the optimal power control that minimizes weighted sum of average
BERs of HP bits and LP bits or its upper bound subject to peak/average transmit
power and average interference power constraints is derived and a
low-complexity power control algorithm is proposed. Power levels are determined
in three different scenarios, depending on the availability of perfect channel
side information (CSI) of the transmission and interference links, statistical
CSI of both links, or perfect CSI of the transmission link and imperfect CSI of
the interference link. The impact of imperfect channel sensing decisions on the
error rate performance of cognitive transmissions is also evaluated. In
addition, tradeoffs between the number of retransmissions, the severity of
fading, and peak signal-to-noise ratio (PSNR) quality are analyzed numerically.
Moreover, performance comparisons of multimedia transmission with conventional
quadrature amplitude modulation (QAM) and HQAM, and the proposed power control
strategies are carried out in terms of the received data quality and number of
retransmissions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00835</identifier>
 <datestamp>2015-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00835</id><created>2015-09-02</created><authors><author><keyname>Klawitter</keyname><forenames>Jonathan</forenames></author><author><keyname>N&#xf6;llenburg</keyname><forenames>Martin</forenames></author><author><keyname>Ueckerdt</keyname><forenames>Torsten</forenames></author></authors><title>Combinatorial Properties of Triangle-Free Rectangle Arrangements and the
  Squarability Problem</title><categories>cs.CG</categories><comments>15 pages, 13 figures, extended version of a paper to appear at the
  International Symposium on Graph Drawing and Network Visualization (GD) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider arrangements of axis-aligned rectangles in the plane. A geometric
arrangement specifies the coordinates of all rectangles, while a combinatorial
arrangement specifies only the respective intersection type in which each pair
of rectangles intersects. First, we investigate combinatorial contact
arrangements, i.e., arrangements of interior-disjoint rectangles, with a
triangle-free intersection graph. We show that such rectangle arrangements are
in bijection with the 4-orientations of an underlying planar multigraph and
prove that there is a corresponding geometric rectangle contact arrangement.
Moreover, we prove that every triangle-free planar graph is the contact graph
of such an arrangement. Secondly, we introduce the question whether a given
rectangle arrangement has a combinatorially equivalent square arrangement. In
addition to some necessary conditions and counterexamples, we show that
rectangle arrangements pierced by a horizontal line are squarable under certain
sufficient conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00836</identifier>
 <datestamp>2015-09-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00836</id><created>2015-09-02</created><authors><author><keyname>Ozel</keyname><forenames>Omur</forenames></author><author><keyname>Ulukus</keyname><forenames>Sennur</forenames></author><author><keyname>Grover</keyname><forenames>Pulkit</forenames></author></authors><title>Energy Harvesting Transmitters that Heat Up: Throughput Maximization
  under Temperature Constraints</title><categories>cs.IT cs.NI math.IT</categories><comments>Submitted to IEEE Transactions on Wireless Communications, August
  2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by damage due to heating in sensor operation, we consider the
throughput optimal offline data scheduling problem in an energy harvesting
transmitter such that the resulting temperature increase remains below a
critical level. We model the temperature dynamics of the transmitter as a
linear system and determine the optimal transmit power policy under such
temperature constraints as well as energy harvesting constraints over an AWGN
channel. We first derive the structural properties of the solution for the
general case with multiple energy arrivals. We show that the optimal power
policy is piecewise monotone decreasing with possible jumps at the energy
harvesting instants. We derive analytical expressions for the optimal solution
in the single energy arrival case. We show that, in the single energy arrival
case, the optimal power is monotone decreasing, the resulting temperature is
monotone increasing, and both remain constant after the temperature hits the
critical level. We then generalize the solution for the multiple energy arrival
case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00838</identifier>
 <datestamp>2016-01-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00838</id><created>2015-09-02</created><updated>2016-01-08</updated><authors><author><keyname>Mei</keyname><forenames>Hongyuan</forenames></author><author><keyname>Bansal</keyname><forenames>Mohit</forenames></author><author><keyname>Walter</keyname><forenames>Matthew R.</forenames></author></authors><title>What to talk about and how? Selective Generation using LSTMs with
  Coarse-to-Fine Alignment</title><categories>cs.CL cs.AI cs.LG cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an end-to-end, domain-independent neural encoder-aligner-decoder
model for selective generation, i.e., the joint task of content selection and
surface realization. Our model first encodes a full set of over-determined
database event records via an LSTM-based recurrent neural network, then
utilizes a novel coarse-to-fine aligner to identify the small subset of salient
records to talk about, and finally employs a decoder to generate free-form
descriptions of the aligned, selected records. Our model achieves the best
selection and generation results reported to-date (with 59% relative
improvement in generation) on the benchmark WeatherGov dataset, despite using
no specialized features or linguistic resources. Using an improved k-nearest
neighbor beam filter helps further. We also perform a series of ablations and
visualizations to elucidate the contributions of our key model components.
Lastly, we evaluate the generalizability of our model on the RoboCup dataset,
and get results that are competitive with or better than the state-of-the-art,
despite being severely data-starved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00844</identifier>
 <datestamp>2015-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00844</id><created>2015-09-02</created><authors><author><keyname>Marchal</keyname><forenames>Olivier</forenames></author></authors><title>Locks and keys: How fast can you open several locks with too many keys?</title><categories>math.HO cs.CR math.PR</categories><comments>12 pages, 6 figures. To be read essentially for fun</comments><msc-class>97K50, 00A69</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This short note is the result of a French &quot;Hippocampe internship&quot; that aims
at introducing the world of research to young undergraduate French students.
The problem studied is the following: imagine yourself locked in a cage barred
with $n$ different locks. You are given a keyring with $N \geq n$ keys
containing the $n$ keys that open the locks. In average, how many trials are
required to open all locks and get out? The article studies $3$ different
strategies and compare them. Implementation of the strategies are also proposed
as illustrations of the theoretical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00864</identifier>
 <datestamp>2015-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00864</id><created>2015-09-02</created><authors><author><keyname>Sorenson</keyname><forenames>Jonathan P.</forenames></author><author><keyname>Webster</keyname><forenames>Jonathan</forenames></author></authors><title>Strong Pseudoprimes to Twelve Prime Bases</title><categories>math.NT cs.DS cs.MS</categories><msc-class>Primary 11Y16, 11Y16, Secondary 11A41, 68W40, 68W10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $\psi_m$ be the smallest strong pseudoprime to the first $m$ prime bases.
This value is known for $1 \leq m \leq 11$. We extend this by finding
$\psi_{12}$ and $\psi_{13}$. We also present an algorithm to find all integers
$n\le B$ that are strong pseudoprimes to the first $m$ prime bases; with a
reasonable heuristic assumption we can show that it takes at most
$B^{2/3+o(1)}$ time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00885</identifier>
 <datestamp>2015-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00885</id><created>2015-09-02</created><authors><author><keyname>Vaidyanathan</keyname><forenames>Kaushik</forenames></author></authors><title>Exploiting Challenges of Sub-20 nm CMOS for Affordable Technology
  Scaling</title><categories>cs.ET</categories><comments>CMU PhD Thesis</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For the past four decades, cost and features have driven CMOS scaling. Severe
lithography and material limitations seen below the 20 nm node, however, are
challenging the fundamental premise of affordable CMOS scaling. Just continuing
to co-optimize leaf cell circuit and layout designs with process technology
does not enable us to exploit the challenges of a sub-20 nm CMOS. For
affordable scaling it is imperative to work past sub-20 nm technology
impediments while exploiting its features. To this end, we propose to broaden
the scope of design technology co-optimization (DTCO) to be more holistic by
including micro-architecture design and CAD, along with circuits, layout and
process technology. Applying such holistic DTCO to the most significant block
in a system-on-chip (SoC), embedded memory, we can synthesize smarter and
efficient embedded memory blocks that are customized to application needs.
  To evaluate the efficacy of the proposed holistic DTCO process, we designed,
fabricated and tested several design experiments in a state-of-the-art IBM
14SOI process. DTCOed leaf cells, standard cells and SRAM bitcells were robust
during testing, but failed to meet node to node area scaling requirements.
Holistic DTCO, when applied to a widely used parallel access SRAM sub-block,
consumed 25% less area with a 50% better performance per watt compared to a
traditional implementation using compiled SRAM blocks and standard cells. To
extend the benefits of holistic DTCO to other embedded memory intensive
sub-blocks in SoCs, we developed a readily customizable smart memory synthesis
framework (SMSF). We believe that such an approach is important to establish an
affordable path for sub-20 nm scaling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00888</identifier>
 <datestamp>2015-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00888</id><created>2015-09-02</created><authors><author><keyname>Chen</keyname><forenames>Pengwen</forenames></author><author><keyname>Fannjiang</keyname><forenames>Albert</forenames></author></authors><title>Fourier Phase Retrieval with a Single Mask by Douglas-Rachford Algorithm</title><categories>math.NA cs.NA physics.data-an physics.optics</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Douglas-Rachford (DR) algorithm is analyzed for Fourier phase retrieval with
a single random phase mask. Local, geometric convergence to a unique fixed
point is proved with numerical demonstration of global convergence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00909</identifier>
 <datestamp>2015-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00909</id><created>2015-09-02</created><authors><author><keyname>Ekbia</keyname><forenames>Hamid</forenames></author><author><keyname>Mattioli</keyname><forenames>Michael</forenames></author><author><keyname>Kouper</keyname><forenames>Inna</forenames></author><author><keyname>Arave</keyname><forenames>G.</forenames></author><author><keyname>Ghazinejad</keyname><forenames>Ali</forenames></author><author><keyname>Bowman</keyname><forenames>Timothy</forenames></author><author><keyname>Suri</keyname><forenames>Venkata Ratandeep</forenames></author><author><keyname>Tsou</keyname><forenames>Andrew</forenames></author><author><keyname>Weingart</keyname><forenames>Scott</forenames></author><author><keyname>Sugimoto</keyname><forenames>Cassidy R.</forenames></author></authors><title>Big data, bigger dilemmas: A critical review</title><categories>cs.CY</categories><comments>in Journal of the Association for Information Science &amp; Technology
  (2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recent interest in Big Data has generated a broad range of new academic,
corporate, and policy practices along with an evolving debate amongst its
proponents, detractors, and skeptics. While the practices draw on a common set
of tools, techniques, and technologies, most contributions to the debate come
either from a particular disciplinary perspective or with an eye on a
domain-specific issue. A close examination of these contributions reveals a set
of common problematics that arise in various guises in different places. It
also demonstrates the need for a critical synthesis of the conceptual and
practical dilemmas surrounding Big Data. The purpose of this article is to
provide such a synthesis by drawing on relevant writings in the sciences,
humanities, policy, and trade literature. In bringing these diverse literatures
together, we aim to shed light on the common underlying issues that concern and
affect all of these areas. By contextualizing the phenomenon of Big Data within
larger socio-economic developments, we also seek to provide a broader
understanding of its drivers, barriers, and challenges. This approach allows us
to identify attributes of Big Data that need to receive more
attention--autonomy, opacity, and generativity, disparity, and
futurity--leading to questions and ideas for moving beyond dilemmas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00910</identifier>
 <datestamp>2015-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00910</id><created>2015-09-02</created><authors><author><keyname>Aji</keyname><forenames>Ablimit</forenames></author><author><keyname>Hoang</keyname><forenames>Vo</forenames></author><author><keyname>Wang</keyname><forenames>Fusheng</forenames></author></authors><title>Effective Spatial Data Partitioning for Scalable Query Processing</title><categories>cs.DB</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Recently, MapReduce based spatial query systems have emerged as a cost
effective and scalable solution to large scale spatial data processing and
analytics. MapReduce based systems achieve massive scalability by partitioning
the data and running query tasks on those partitions in parallel. Therefore,
effective data partitioning is critical for task parallelization, load
balancing, and directly affects system performance. However, several pitfalls
of spatial data partitioning make this task particularly challenging. First,
data skew is very common in spatial applications. To achieve best query
performance, data skew need to be reduced. Second, spatial partitioning
approaches generate boundary objects that cross multiple partitions, and add
extra query processing overhead. Consequently, boundary objects need to be
minimized. Third, the high computational complexity of spatial partitioning
algorithms combined with massive amounts of data require an efficient approach
for partitioning to achieve overall fast query response. In this paper, we
provide a systematic evaluation of multiple spatial partitioning methods with a
set of different partitioning strategies, and study their implications on the
performance of MapReduce based spatial queries. We also study sampling based
partitioning methods and their impact on queries, and propose several MapReduce
based high performance spatial partitioning methods. The main objective of our
work is to provide a comprehensive guidance for optimal spatial data
partitioning to support scalable and fast spatial data processing in massively
parallel data processing frameworks such as MapReduce. The algorithms developed
in this work are open source and can be easily integrated into different high
performance spatial data processing systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00913</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00913</id><created>2015-09-02</created><updated>2015-09-29</updated><authors><author><keyname>Simpson</keyname><forenames>Andrew J. R.</forenames></author></authors><title>On-the-Fly Learning in a Perpetual Learning Machine</title><categories>cs.LG</categories><msc-class>68Txx</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite the promise of brain-inspired machine learning, deep neural networks
(DNN) have frustratingly failed to bridge the deceptively large gap between
learning and memory. Here, we introduce a Perpetual Learning Machine; a new
type of DNN that is capable of brain-like dynamic 'on the fly' learning because
it exists in a self-supervised state of Perpetual Stochastic Gradient Descent.
Thus, we provide the means to unify learning and memory within a machine
learning framework. We also explore the elegant duality of abstraction and
synthesis: the Yin and Yang of deep learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00916</identifier>
 <datestamp>2015-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00916</id><created>2015-09-02</created><authors><author><keyname>Lee</keyname><forenames>Euiwoong</forenames></author><author><keyname>Schmidt</keyname><forenames>Melanie</forenames></author><author><keyname>Wright</keyname><forenames>John</forenames></author></authors><title>Improved and Simplified Inapproximability for k-means</title><categories>cs.CG</categories><comments>6 pages</comments><acm-class>F.2.2; I.5.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The k-means problem consists of finding k centers in the d-dimensional
Euclidean space that minimize the sum of the squared distances of all points in
an input set P to their closest respective center. Awasthi et. al. recently
showed that there exists a constant c &gt; 1 such that it is NP-hard to
approximate the k-means objective within a factor of c. We establish that the
constant c is at least 1.0013.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00926</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00926</id><created>2015-09-02</created><updated>2015-09-09</updated><authors><author><keyname>Skliar</keyname><forenames>Osvaldo</forenames></author><author><keyname>Monge</keyname><forenames>Ricardo E.</forenames></author><author><keyname>Gapper</keyname><forenames>Sherry</forenames></author></authors><title>Using Inclusion Diagrams as an Alternative to Venn Diagrams to Determine
  the Validity of Categorical Syllogisms</title><categories>cs.LO math.LO</categories><comments>29 pages</comments><msc-class>03B05, 03B10, 97E30, 00A66</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inclusion diagrams are introduced as an alternative to using Venn diagrams to
determine the validity of categorical syllogisms, and are used here for the
analysis of diverse categorical syllogisms. As a preliminary example of a
possible generalization of the use of inclusion diagrams, consideration is
given also to an argument that includes more than two premises and more than
three terms, the classic major, middle and minor terms in categorical
syllogisms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00930</identifier>
 <datestamp>2015-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00930</id><created>2015-09-02</created><authors><author><keyname>Oono</keyname><forenames>Kenta</forenames></author><author><keyname>Yoshida</keyname><forenames>Yuichi</forenames></author></authors><title>Testing Properties of Functions on Finite Groups</title><categories>cs.DS</categories><comments>Accepted to Random Structures and Algorithms</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study testing properties of functions on finite groups. First we consider
functions of the form $f:G \to \mathbb{C}$, where $G$ is a finite group. We
show that conjugate invariance, homomorphism, and the property of being
proportional to an irreducible character is testable with a constant number of
queries to $f$, where a character is a crucial notion in representation theory.
Our proof relies on representation theory and harmonic analysis on finite
groups. Next we consider functions of the form $f: G \to M_d(\mathbb{C})$,
where $d$ is a fixed constant and $M_d(\mathbb{C})$ is the family of $d$ by $d$
matrices with each element in $\mathbb{C}$. For a function $g:G \to
M_d(\mathbb{C})$, we show that the unitary isomorphism to $g$ is testable with
a constant number of queries to $f$, where we say that $f$ and $g$ are unitary
isomorphic if there exists a unitary matrix $U$ such that $f(x) = Ug(x)U^{-1}$
for any $x \in G$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00935</identifier>
 <datestamp>2015-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00935</id><created>2015-09-02</created><authors><author><keyname>Zhang</keyname><forenames>Meng</forenames></author><author><keyname>Liu</keyname><forenames>Yuan</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author></authors><title>Secrecy Wireless Information and Power Transfer in OFDMA Systems</title><categories>cs.IT math.IT</categories><comments>To appear in Globecom 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider simultaneous wireless information and power
transfer (SWIPT) in orthogonal frequency division multiple access (OFDMA)
systems with the coexistence of information receivers (IRs) and energy
receivers (ERs). The IRs are served with best-effort secrecy data and the ERs
harvest energy with minimum required harvested power. To enhance physical-layer
security and yet satisfy energy harvesting requirements, we introduce a new
frequency-domain artificial noise based approach. We study the optimal resource
allocation for the weighted sum secrecy rate maximization via transmit power
and subcarrier allocation. The considered problem is non-convex, while we
propose an efficient algorithm for solving it based on Lagrange duality method.
Simulation results illustrate the effectiveness of the proposed algorithm as
compared against other heuristic schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00938</identifier>
 <datestamp>2015-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00938</id><created>2015-09-02</created><authors><author><keyname>Zhang</keyname><forenames>Meng</forenames></author><author><keyname>Liu</keyname><forenames>Yuan</forenames></author></authors><title>Joint Secure Beamforming for Cognitive Radio Networks with Untrusted
  Secondary Users</title><categories>cs.IT math.IT</categories><comments>To appear in Globecom 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider simultaneous wireless information and power
transfer (SWIPT) in orthogonal frequency division multiple access (OFDMA)
systems with the coexistence of information receivers (IRs) and energy
receivers (ERs). The IRs are served with best-effort secrecy data and the ERs
harvest energy with minimum required harvested power. To enhance physical-layer
security and yet satisfy energy harvesting requirements, we introduce a new
frequency-domain artificial noise based approach. We study the optimal resource
allocation for the weighted sum secrecy rate maximization via transmit power
and subcarrier allocation. The considered problem is non-convex, while we
propose an efficient algorithm for solving it based on Lagrange duality method.
Simulation results illustrate the effectiveness of the proposed algorithm as
compared against other heuristic schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00940</identifier>
 <datestamp>2015-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00940</id><created>2015-09-03</created><updated>2015-11-13</updated><authors><author><keyname>Lu</keyname><forenames>Xiao</forenames></author><author><keyname>Wang</keyname><forenames>Ping</forenames></author><author><keyname>Niyato</keyname><forenames>Dusit</forenames></author><author><keyname>Kim</keyname><forenames>Dong In</forenames></author><author><keyname>Han</keyname><forenames>Zhu</forenames></author></authors><title>Wireless Charging Technologies: Fundamentals, Standards, and Network
  Applications</title><categories>cs.NI</categories><comments>to appear in IEEE Communications Surveys and Tutorial</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless charging is a technology of transmitting power through an air gap to
electrical devices for the purpose of energy replenishment. The recent progress
in wireless charging techniques and development of commercial products have
provided a promising alternative way to address the energy bottleneck of
conventionally portable battery-powered devices. However, the incorporation of
wireless charging into the existing wireless communication systems also brings
along a series of challenging issues with regard to implementation, scheduling,
and power management. In this article, we present a comprehensive overview of
wireless charging techniques, the developments in technical standards, and
their recent advances in network applications. In particular, with regard to
network applications, we review the mobile charger dispatch strategies, static
charger scheduling strategies and wireless charger deployment strategies.
Additionally, we discuss open issues and challenges in implementing wireless
charging technologies. Finally, we envision some practical future network
applications of wireless charging.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00947</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00947</id><created>2015-09-03</created><updated>2015-10-14</updated><authors><author><keyname>Rasekhi</keyname><forenames>Jalil</forenames></author></authors><title>Motion planning using shortest path</title><categories>cs.RO</categories><comments>The paper has been withdrawn due to a crucial sign error in equation
  3,4</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a new method for path planning to a point for robot
in environment with obstacles. The resulting algorithm is implemented as a
simple variation of Dijkstra's algorithm. By adding a constraint to the
shortest-path, the algorithm is able to exclude all the paths between two
points that violate the constraint.This algorithm provides the robot the
possibility to move from the initial position to the final position (target)
when we have enough samples in the domain. In this case the robot follows a
smooth path that does not fall in to the obstacles. Our method is simpler than
the previous proposals in the literature and performs comparably to the best
methods, both on simulated and some real datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00948</identifier>
 <datestamp>2015-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00948</id><created>2015-09-03</created><authors><author><keyname>Bezzo</keyname><forenames>Nicola</forenames></author><author><keyname>Hecker</keyname><forenames>Joshua P.</forenames></author><author><keyname>Stolleis</keyname><forenames>Karl</forenames></author><author><keyname>Moses</keyname><forenames>Melanie E.</forenames></author><author><keyname>Fierro</keyname><forenames>Rafael</forenames></author></authors><title>Exploiting Heterogeneous Robotic Systems in Cooperative Missions</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider the problem of coordinating robotic systems with
different kinematics, sensing and vision capabilities to achieve certain
mission goals. An approach that makes use of a heterogeneous team of agents has
several advantages when cost, integration of capabilities, or large search
areas need to be considered. A heterogeneous team allows for the robots to
become &quot;specialized&quot;, accomplish sub-goals more effectively, and thus increase
the overall mission efficiency. Two main scenarios are considered in this work.
In the first case study we exploit mobility to implement a power control
algorithm that increases the Signal to Interference plus Noise Ratio (SINR)
among certain members of the network. We create realistic sensing fields and
manipulation by using the geometric properties of the sensor field-of-view and
the manipulability metric, respectively. The control strategy for each agent of
the heterogeneous system is governed by an artificial physics law that
considers the different kinematics of the agents and the environment, in a
decentralized fashion. Through simulation results we show that the network is
able to stay connected at all times and covers the environment well. The second
scenario studied in this paper is the biologically-inspired coordination of
heterogeneous physical robotic systems. A team of ground rovers, designed to
emulate desert seed-harvester ants, explore an experimental area using
behaviors fine-tuned in simulation by a genetic algorithm. Our robots
coordinate with a base station and collect clusters of resources scattered
within the experimental space. We demonstrate experimentally that through
coordination with an aerial vehicle, our ant-like ground robots are able to
collect resources two times faster than without the use of heterogeneous
coordination.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00961</identifier>
 <datestamp>2015-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00961</id><created>2015-09-03</created><authors><author><keyname>Pilson</keyname><forenames>Christopher S.</forenames></author><author><keyname>McElroy</keyname><forenames>James C.</forenames></author></authors><title>A Typology of Authentication Systems</title><categories>cs.CR</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Authentication systems are designed to give the right person access to an
organization's information system and to restrict it from the wrong person.
Such systems are designed by IT professionals to protect an organization's
assets (e.g., the organization's network, database, or other information). Too
often, such systems are designed around technical specifications without regard
for the end user. We argue that doing so may actually compromise a system's
security. This paper examines authentication systems from both the point of
view of the organization and that of the user.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00962</identifier>
 <datestamp>2015-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00962</id><created>2015-09-03</created><authors><author><keyname>Wang</keyname><forenames>Runchun</forenames></author><author><keyname>Thakur</keyname><forenames>Chetan Singh</forenames></author><author><keyname>Hamilton</keyname><forenames>Tara Julia</forenames></author><author><keyname>Tapson</keyname><forenames>Jonathan</forenames></author><author><keyname>van Schaik</keyname><forenames>Andre</forenames></author></authors><title>A compact aVLSI conductance-based silicon neuron</title><categories>cs.NE</categories><comments>BioCAS-2015</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  We present an analogue Very Large Scale Integration (aVLSI) implementation
that uses first-order lowpass filters to implement a conductance-based silicon
neuron for high-speed neuromorphic systems. The aVLSI neuron consists of a soma
(cell body) and a single synapse, which is capable of linearly summing both the
excitatory and inhibitory postsynaptic potentials (EPSP and IPSP) generated by
the spikes arriving from different sources. Rather than biasing the silicon
neuron with different parameters for different spiking patterns, as is
typically done, we provide digital control signals, generated by an FPGA, to
the silicon neuron to obtain different spiking behaviours. The proposed neuron
is only ~26.5 um2 in the IBM 130nm process and thus can be integrated at very
high density. Circuit simulations show that this neuron can emulate different
spiking behaviours observed in biological neurons.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00963</identifier>
 <datestamp>2015-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00963</id><created>2015-09-03</created><authors><author><keyname>K&#xfc;&#xe7;&#xfc;k</keyname><forenames>Dilek</forenames></author><author><keyname>K&#xfc;&#xe7;&#xfc;k</keyname><forenames>Do&#x11f;an</forenames></author></authors><title>On TimeML-Compliant Temporal Expression Extraction in Turkish</title><categories>cs.CL</categories><comments>7 pages, 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is commonly acknowledged that temporal expression extractors are important
components of larger natural language processing systems like information
retrieval and question answering systems. Extraction and normalization of
temporal expressions in Turkish has not been given attention so far except the
extraction of some date and time expressions within the course of named entity
recognition. As TimeML is the current standard of temporal expression and event
annotation in natural language texts, in this paper, we present an analysis of
temporal expressions in Turkish based on the related TimeML classification
(i.e., date, time, duration, and set expressions). We have created a lexicon
for Turkish temporal expressions and devised considerably wide-coverage
patterns using the lexical classes as the building blocks. We believe that the
proposed patterns, together with convenient normalization rules, can be readily
used by prospective temporal expression extraction tools for Turkish.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00967</identifier>
 <datestamp>2015-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00967</id><created>2015-09-03</created><authors><author><keyname>Xu</keyname><forenames>Ying</forenames></author><author><keyname>Thakur</keyname><forenames>Chetan Singh</forenames></author><author><keyname>Hamilton</keyname><forenames>Tara Julia</forenames></author><author><keyname>Tapson</keyname><forenames>Jonathan</forenames></author><author><keyname>Wang</keyname><forenames>Runchun</forenames></author><author><keyname>van Schaik</keyname><forenames>Andre</forenames></author></authors><title>A Reconfigurable Mixed-signal Implementation of a Neuromorphic ADC</title><categories>cs.NE</categories><comments>BioCAS-2015</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  We present a neuromorphic Analogue-to-Digital Converter (ADC), which uses
integrate-and-fire (I&amp;F) neurons as the encoders of the analogue signal, with
modulated inhibitions to decohere the neuronal spikes trains. The architecture
consists of an analogue chip and a control module. The analogue chip comprises
two scan chains and a twodimensional integrate-and-fire neuronal array.
Individual neurons are accessed via the chains one by one without any encoder
decoder or arbiter. The control module is implemented on an FPGA (Field
Programmable Gate Array), which sends scan enable signals to the scan chains
and controls the inhibition for individual neurons. Since the control module is
implemented on an FPGA, it can be easily reconfigured. Additionally, we propose
a pulse width modulation methodology for the lateral inhibition, which makes
use of different pulse widths indicating different strengths of inhibition for
each individual neuron to decohere neuronal spikes. Software simulations in
this paper tested the robustness of the proposed ADC architecture to fixed
random noise. A circuit simulation using ten neurons shows the performance and
the feasibility of the architecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00976</identifier>
 <datestamp>2016-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00976</id><created>2015-09-03</created><updated>2016-02-10</updated><authors><author><keyname>AlAmmouri</keyname><forenames>Ahmad</forenames></author><author><keyname>ElSawy</keyname><forenames>Hesham</forenames></author><author><keyname>Amin</keyname><forenames>Osama</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author></authors><title>In-Band $\alpha$-Duplex Scheme for Cellular Networks: A Stochastic
  Geometry Approach</title><categories>cs.IT math.IT stat.AP</categories><comments>Submitted to IEEE TWC</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In-band full-duplex (FD) communications have been optimistically promoted to
improve the spectrum utilization and efficiency. However, the penetration of FD
communications to the cellular networks domain is challenging due to the
imposed uplink/downlink interference. This paper presents a tractable
framework, based on stochastic geometry, to study FD communications in cellular
networks. Particularly, we assess the FD communications effect on the network
performance and quantify the associated gains. The study proves the
vulnerability of the uplink to the downlink interference and shows that the
improved FD rate gains harvested in the downlink (up to $97\%$) comes at the
expense of a significant degradation in the uplink rate (up to $94\%$).
Therefore, we propose a novel fine-grained duplexing scheme, denoted as
$\alpha$-duplex scheme, which allows a partial overlap between the uplink and
the downlink frequency bands. We derive the required conditions to harvest rate
gains from the $\alpha$-duplex scheme and show its superiority to both the FD
and half-duplex (HD) schemes. In particular, we show that the $\alpha$-duplex
scheme provides a simultaneous improvement of $28\%$ for the downlink rate and
$56\%$ for the uplink rate. Finally, we show that the amount of the overlap can
be optimized based on the network design objective.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00977</identifier>
 <datestamp>2015-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00977</id><created>2015-09-03</created><authors><author><keyname>&#xdc;&#xe7;&#xfc;nc&#xfc;</keyname><forenames>Ali Bulut</forenames></author><author><keyname>Y&#x131;lmaz</keyname><forenames>Ali &#xd6;zg&#xfc;r</forenames></author></authors><title>Pulse Shaping Methods for OQAM/OFDM and WCP-COQAM</title><categories>cs.IT math.IT</categories><comments>5 pages, 1 figure, submitted to IEEE signal processing letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  GFDM is a new modulation format whose advantages compared to OFDM reportedly
make it a preferable modulation format for 5G. However, the non-orthogonal
nature of GFDM with matched filtering (MF) receiver for pulses with good
time-frequency localization is one of its disadvantages, leading to the
proposal of WCP-COQAM, employing offset quadrature amplitude modulation (OQAM).
In this paper, we prove that a pulse satisfying orthogonality conditions for
OQAM-OFDM will also satisfy orthogonality with WCP-COQAM, thus the pulse design
methods developed for OQAM-OFDM can also be used with WCP-COQAM. This statement
is also verified by the simulation based results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00981</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00981</id><created>2015-09-03</created><updated>2015-09-08</updated><authors><author><keyname>Khan</keyname><forenames>Shujaat</forenames></author><author><keyname>Ibrahim</keyname><forenames>Muhammad Sohail</forenames></author><author><keyname>Khan</keyname><forenames>Kafeel Ahmed</forenames></author><author><keyname>Ebrahim</keyname><forenames>Mansoor</forenames></author></authors><title>Security Analysis of Secure Force Algorithm for Wireless Sensor Networks</title><categories>cs.CR</categories><comments>in Asian Journal of Engineering Science and Technology 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Wireless Sensor Networks, the sensor nodes are battery powered small
devices designed for long battery life. These devices also lack in terms of
processing capability and memory. In order to provide high confidentiality to
these resource constrained network nodes, a suitable security algorithm is
needed to be deployed that can establish a balance between security level and
processing overhead. The objective of this research work is to perform a
security analysis and performance evaluation of recently proposed Secure Force
algorithm. This paper shows the comparison of Secure Force 64, 128, and 192 bit
architecture on the basis of avalanche effect (key sensitivity), entropy change
analysis, image histogram, and computational time. Moreover, based on the
evaluation results, the paper also suggests the possible solutions for the
weaknesses of the SF algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00993</identifier>
 <datestamp>2015-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00993</id><created>2015-09-03</created><authors><author><keyname>Hekrdla</keyname><forenames>Miroslav</forenames></author><author><keyname>Matera</keyname><forenames>Andrea</forenames></author><author><keyname>Wang</keyname><forenames>Weiyang</forenames></author><author><keyname>Wei</keyname><forenames>Dong</forenames></author><author><keyname>Spagnolini</keyname><forenames>Umberto</forenames></author></authors><title>Ordered Tomlinson-Harashima Precoding in G.fast Downstream</title><categories>cs.IT cs.NI math.IT</categories><comments>7 pages, 11 figures, Accepted at the 2015 IEEE Globecom 2015,
  Selected Areas in Communications: Access Networks and Systems, 6-10 December,
  2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  G.fast is an upcoming next generation DSL standard envisioned to use
bandwidth up to 212 MHz. Far-end crosstalk (FEXT) at these frequencies greatly
overcomes direct links. Its cancellation based on non-linear
Tomlinson-Harashima Precoding (THP) proved to show significant advantage over
standard linear precoding. This paper proposes a novel THP structure in which
ordering of successive interference pre-cancellation can be optimized for
downstream with non-cooperating receivers. The optimized scheme is compared to
existing THP structure denoted as equal-rate THP which is widely adopted in
wireless downlink. Structure and performance of both methods differ
significantly favoring the proposed scheme. The ordering that maximizes the
minimum rate (max-min fairness) for each tone of the discrete multi-tone
modulation is the familiar V-BLAST ordering. However, V-BLAST does not lead to
the global maximum when applied independently on each tone. The proposed novel
Dynamic Ordering (DO) strategy takes into account asymmetric channel statistics
to yield the highest minimum aggregated rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00996</identifier>
 <datestamp>2015-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00996</id><created>2015-09-03</created><authors><author><keyname>Accattoli</keyname><forenames>Beniamino</forenames></author><author><keyname>Barenbaum</keyname><forenames>Pablo</forenames></author><author><keyname>Mazza</keyname><forenames>Damiano</forenames></author></authors><title>A Strong Distillery</title><categories>cs.PL cs.LO</categories><comments>Accepted at APLAS 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Abstract machines for the strong evaluation of lambda-terms (that is, under
abstractions) are a mostly neglected topic, despite their use in the
implementation of proof assistants and higher-order logic programming
languages. This paper introduces a machine for the simplest form of strong
evaluation, leftmost-outermost (call-by-name) evaluation to normal form,
proving it correct, complete, and bounding its overhead. Such a machine, deemed
Strong Milner Abstract Machine, is a variant of the KAM computing normal forms
and using just one global environment. Its properties are studied via a special
form of decoding, called a distillation, into the Linear Substitution Calculus,
neatly reformulating the machine as a standard micro-step strategy for explicit
substitutions, namely linear leftmost-outermost reduction, i.e., the extension
to normal form of linear head reduction. Additionally, the overhead of the
machine is shown to be linear both in the number of steps and in the size of
the initial term, validating its design. The study highlights two distinguished
features of strong machines, namely backtracking phases and their interactions
with abstractions and environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.00998</identifier>
 <datestamp>2015-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.00998</id><created>2015-09-03</created><authors><author><keyname>Yu</keyname><forenames>Zhaofei</forenames></author><author><keyname>Chen</keyname><forenames>Feng</forenames></author><author><keyname>Dong</keyname><forenames>Jianwu</forenames></author><author><keyname>Dai</keyname><forenames>Qionghai</forenames></author></authors><title>Sampling-based Causal Inference in Cue Combination and its Neural
  Implementation</title><categories>cs.NE q-bio.NC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Causal inference in cue combination is to decide whether the cues have a
single cause or multiple causes. Although the Bayesian causal inference model
explains the problem of causal inference in cue combination successfully, how
causal inference in cue combination could be implemented by neural circuits, is
unclear. The existing method based on calculating log posterior ratio with
variable elimination has the problem of being unrealistic and task-specific. In
this paper, we take advantages of the special structure of the Bayesian causal
inference model and propose a hierarchical inference algorithm based on
importance sampling. A simple neural circuit is designed to implement the
proposed inference algorithm. Theoretical analyses and experimental results
demonstrate that our algorithm converges to the accurate value as the sample
size goes to infinite. Moreover, the neural circuit we design can be easily
generalized to implement inference for other problems, such as the
multi-stimuli cause inference and the same-different judgment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01004</identifier>
 <datestamp>2015-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01004</id><created>2015-09-03</created><updated>2015-10-06</updated><authors><author><keyname>Kondo</keyname><forenames>Yohei</forenames></author><author><keyname>Hayashi</keyname><forenames>Kohei</forenames></author><author><keyname>Maeda</keyname><forenames>Shin-ichi</forenames></author></authors><title>Bayesian Masking: Sparse Bayesian Estimation with Weaker Shrinkage Bias</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A common strategy for sparse linear regression is to introduce
regularization, which eliminates irrelevant features by letting the
corresponding weights be zeros. However, regularization often shrinks the
estimator for relevant features, which leads to incorrect feature selection.
Motivated by the above-mentioned issue, we propose Bayesian masking (BM), a
sparse estimation method which imposes no regularization on the weights. The
key concept of BM is to introduce binary latent variables that randomly mask
features. Estimating the masking rates determines the relevance of the features
automatically. We derive a variational Bayesian inference algorithm that
maximizes the lower bound of the factorized information criterion (FIC), which
is a recently developed asymptotic criterion for evaluating the marginal
log-likelihood. In addition, we propose reparametrization to accelerate the
convergence of the derived algorithm. Finally, we show that BM outperforms
Lasso and automatic relevance determination (ARD) in terms of the
sparsity-shrinkage trade-off.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01007</identifier>
 <datestamp>2016-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01007</id><created>2015-09-03</created><updated>2016-03-08</updated><authors><author><keyname>Osborne</keyname><forenames>Dominique</forenames></author><author><keyname>Narayan</keyname><forenames>Shashi</forenames></author><author><keyname>Cohen</keyname><forenames>Shay B.</forenames></author></authors><title>Encoding Prior Knowledge with Eigenword Embeddings</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Canonical correlation analysis (CCA) is a method for reducing the dimension
of data represented using two views. It has been previously used to derive word
embeddings, where one view indicates a word, and the other view indicates its
context. We describe a way to incorporate prior knowledge into CCA, give a
theoretical justification for it, and test it by deriving word embeddings and
evaluating them on a myriad of datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01013</identifier>
 <datestamp>2015-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01013</id><created>2015-09-03</created><authors><author><keyname>Dvorak</keyname><forenames>Zdenek</forenames></author><author><keyname>Kral</keyname><forenames>Daniel</forenames></author><author><keyname>Thomas</keyname><forenames>Robin</forenames></author></authors><title>Three-coloring triangle-free graphs on surfaces VI. 3-colorability of
  quadrangulations</title><categories>math.CO cs.DM</categories><comments>28 pages, no figures</comments><msc-class>05C85 (Primary), 05C15 (Secondary)</msc-class><acm-class>F.2.2; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a linear-time algorithm to decide 3-colorability (and find a
3-coloring, if it exists) of quadrangulations of a fixed surface. The algorithm
also allows to prescribe the coloring for a bounded number of vertices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01014</identifier>
 <datestamp>2015-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01014</id><created>2015-09-03</created><authors><author><keyname>Iwata</keyname><forenames>Yoichi</forenames></author><author><keyname>Yoshida</keyname><forenames>Yuichi</forenames></author></authors><title>On the Equivalence among Problems of Bounded Width</title><categories>cs.DS cs.CC</categories><comments>accepted to ESA 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce a methodology, called decomposition-based
reductions, for showing the equivalence among various problems of
bounded-width.
  First, we show that the following are equivalent for any $\alpha &gt; 0$:
  * SAT can be solved in $O^*(2^{\alpha \mathrm{tw}})$ time,
  * 3-SAT can be solved in $O^*(2^{\alpha \mathrm{tw}})$ time,
  * Max 2-SAT can be solved in $O^*(2^{\alpha \mathrm{tw}})$ time,
  * Independent Set can be solved in $O^*(2^{\alpha \mathrm{tw}})$ time, and
  * Independent Set can be solved in $O^*(2^{\alpha \mathrm{cw}})$ time, where
tw and cw are the tree-width and clique-width of the instance, respectively.
  Then, we introduce a new parameterized complexity class EPNL, which includes
Set Cover and Directed Hamiltonicity, and show that SAT, 3-SAT, Max 2-SAT, and
Independent Set parameterized by path-width are EPNL-complete. This implies
that if one of these EPNL-complete problems can be solved in $O^*(c^k)$ time,
then any problem in EPNL can be solved in $O^*(c^k)$ time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01018</identifier>
 <datestamp>2015-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01018</id><created>2015-09-03</created><authors><author><keyname>Kosolobov</keyname><forenames>Dmitry</forenames></author></authors><title>Finding the Leftmost Critical Factorization on Unordered Alphabet</title><categories>cs.DS</categories><comments>12 pages, 13 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a linear time and space algorithm computing the leftmost critical
factorization of a given string on an unordered alphabet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01023</identifier>
 <datestamp>2015-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01023</id><created>2015-09-03</created><authors><author><keyname>Adeyanju</keyname><forenames>Ibrahim</forenames></author></authors><title>Generating Weather Forecast Texts with Case Based Reasoning</title><categories>cs.AI cs.CL</categories><comments>6 pages</comments><journal-ref>International Journal of Computer Applications 45(10) (2012) 35-40</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several techniques have been used to generate weather forecast texts. In this
paper, case based reasoning (CBR) is proposed for weather forecast text
generation because similar weather conditions occur over time and should have
similar forecast texts. CBR-METEO, a system for generating weather forecast
texts was developed using a generic framework (jCOLIBRI) which provides modules
for the standard components of the CBR architecture. The advantage in a CBR
approach is that systems can be built in minimal time with far less human
effort after initial consultation with experts. The approach depends heavily on
the goodness of the retrieval and revision components of the CBR process. We
evaluated CBRMETEO with NIST, an automated metric which has been shown to
correlate well with human judgements for this domain. The system shows
comparable performance with other NLG systems that perform the same task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01038</identifier>
 <datestamp>2015-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01038</id><created>2015-09-03</created><authors><author><keyname>Argyriou</keyname><forenames>Antonios</forenames></author></authors><title>Multi-Source Cooperative Communication with Opportunistic Interference
  Cancelling Relays</title><categories>cs.NI cs.IT math.IT</categories><comments>in IEEE Transactions on Communications, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a multi-user cooperative protocol for wireless
networks. Two sources transmit simultaneously their information blocks and
relays employ opportunistically successive interference cancellation (SIC) in
an effort to decode them. An adaptive decode/amplify-and-forward scheme is
applied at the relays to the decoded blocks or their sufficient statistic if
decoding fails. The main feature of the protocol is that SIC is exploited in a
network since more opportunities arise for each block to be decoded as the
number of used relays NRU is increased. This feature leads to benefits in terms
of diversity and multiplexing gains that are proven with the help of an
analytical outage model and a diversity-multiplexing tradeoff (DMT) analysis.
The performance improvements are achieved without any network synchronization
and coordination. In the final part of this work the closed-form outage
probability model is used by a novel approach for offline pre-selection of the
NRU relays, that have the best SIC performance, from a larger number of NR
nodes. The analytical results are corroborated with extensive simulations,
while the protocol is compared with orthogonal and multi-user protocols
reported in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01040</identifier>
 <datestamp>2015-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01040</id><created>2015-09-03</created><authors><author><keyname>Adeyanju</keyname><forenames>Ibrahim</forenames></author></authors><title>Building a Truly Distributed Constraint Solver with JADE</title><categories>cs.AI cs.DC</categories><comments>7 pages</comments><journal-ref>International Journal of Computer Applications (IJCA) 46 (8)
  (2012) 5-7</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Real life problems such as scheduling meeting between people at different
locations can be modelled as distributed Constraint Satisfaction Problems
(CSPs). Suitable and satisfactory solutions can then be found using constraint
satisfaction algorithms which can be exhaustive (backtracking) or otherwise
(local search). However, most research in this area tested their algorithms by
simulation on a single PC with a single program entry point. The main
contribution of our work is the design and implementation of a truly
distributed constraint solver based on a local search algorithm using Java
Agent DEvelopment framework (JADE) to enable communication between agents on
different machines. Particularly, we discuss design and implementation issues
related to truly distributed constraint solver which might not be critical when
simulated on a single machine. Evaluation results indicate that our truly
distributed constraint solver works well within the observed limitations when
tested with various distributed CSPs. Our application can also incorporate any
constraint solving algorithm with little modifications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01047</identifier>
 <datestamp>2015-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01047</id><created>2015-09-03</created><authors><author><keyname>Aubel</keyname><forenames>C&#xe9;line</forenames></author><author><keyname>Stotz</keyname><forenames>David</forenames></author><author><keyname>B&#xf6;lcskei</keyname><forenames>Helmut</forenames></author></authors><title>A Theory of Super-Resolution from Short-Time Fourier Transform
  Measurements</title><categories>cs.IT math.IT</categories><comments>63 pages, submitted to the Journal of Fourier Analysis and
  Applications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  While spike trains are obviously not band-limited, the theory of
super-resolution tells us that perfect recovery of unknown spike locations and
weights from low-pass Fourier transform measurements is possible provided that
the minimum spacing, $\Delta$, between spikes is not too small. Specifically,
for a measurement cutoff frequency of $f_c$, Donoho [2] showed that exact
recovery is possible if the spikes (on $\mathbb{R}$) lie on a lattice and
$\Delta &gt; 1/f_c$, but does not specify a corresponding recovery method.
Cand$\text{\`e}$s and Fernandez-Granda [3] provide a convex programming method
for the recovery of periodic spike trains, i.e., spike trains on the torus
$\mathbb{T}$, which provably succeeds if $\Delta &gt; 2/f_c$, and does not need
the spikes within the fundamental period to lie on a lattice. In this paper, we
develop a theory of super-resolution from short-time Fourier transform (STFT)
measurements. Specifically, we present a recovery method, similar in spirit to
the one in [3] for pure Fourier measurements, that provably succeeds if $\Delta
&gt; 1/f_c$. This factor-of-two improvement in the recovery threshold is also
observed in numerical results. Our theory is based on a measure-theoretic
formulation of the recovery problem, which leads to considerable generality in
the sense of the results being grid-free and applying to spike trains on both
$\mathbb{R}$ and $\mathbb{T}$. The case of spike trains on $\mathbb{R}$ comes
with significant technical challenges and requires, inter alia, the development
of new results on the STFT of measures. For the recovery of spike trains on
$\mathbb{T}$ we prove that the correct solution can be obtained by solving a
sequence of finite-dimensional convex programming problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01053</identifier>
 <datestamp>2015-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01053</id><created>2015-09-03</created><authors><author><keyname>Probst</keyname><forenames>Malte</forenames></author><author><keyname>Rothlauf</keyname><forenames>Franz</forenames></author></authors><title>Training a Restricted Boltzmann Machine for Classification by Labeling
  Model Samples</title><categories>cs.LG</categories><report-no>Technical Report 04/2012</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an alternative method for training a classification model. Using
the MNIST set of handwritten digits and Restricted Boltzmann Machines, it is
possible to reach a classification performance competitive to semi-supervised
learning if we first train a model in an unsupervised fashion on unlabeled data
only, and then manually add labels to model samples instead of training data
samples with the help of a GUI. This approach can benefit from the fact that
model samples can be presented to the human labeler in a video-like fashion,
resulting in a higher number of labeled examples. Also, after some initial
training, hard-to-classify examples can be distinguished from easy ones
automatically, saving manual work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01066</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01066</id><created>2015-09-03</created><updated>2016-02-16</updated><authors><author><keyname>Berkenkamp</keyname><forenames>Felix</forenames></author><author><keyname>Schoellig</keyname><forenames>Angela P.</forenames></author><author><keyname>Krause</keyname><forenames>Andreas</forenames></author></authors><title>Safe Controller Optimization for Quadrotors with Gaussian Processes</title><categories>cs.RO</categories><comments>IEEE International Conference on Robotics and Automation, 2016. 6
  pages, 4 figures. A video of the experiments can be found at
  http://tiny.cc/icra16_video . A Python implementation of the algorithm is
  available at https://github.com/befelix/SafeOpt</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the most fundamental problems when designing controllers for dynamic
systems is the tuning of the controller parameters. Typically, a model of the
system is used to obtain an initial controller, but ultimately the controller
parameters must be tuned manually on the real system to achieve the best
performance. To avoid this manual tuning step, methods from machine learning,
such as Bayesian optimization, have been used. However, as these methods
evaluate different controller parameters on the real system, safety-critical
system failures may happen. In this paper, we overcome this problem by
applying, for the first time, a recently developed safe optimization algorithm,
SafeOpt, to the problem of automatic controller parameter tuning. Given an
initial, low-performance controller, SafeOpt automatically optimizes the
parameters of a control law while guaranteeing safety. It models the underlying
performance measure as a Gaussian process and only explores new controller
parameters whose performance lies above a safe performance threshold with high
probability. Experimental results on a quadrotor vehicle indicate that the
proposed method enables fast, automatic, and safe optimization of controller
parameters without human intervention.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01074</identifier>
 <datestamp>2015-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01074</id><created>2015-09-03</created><authors><author><keyname>Mohamed</keyname><forenames>Ahmed Nabil</forenames></author></authors><title>A Novice Guide towards Human Motion Analysis and Understanding</title><categories>cs.CV</categories><comments>35 Pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Human motion analysis and understanding has been, and is still, the focus of
attention of many disciplines which is considered an obvious indicator of the
wide and massive importance of the subject. The purpose of this article is to
shed some light on this very important subject, so it can be a good insight for
a novice computer vision researcher in this field by providing him/her with a
wealth of knowledge about the subject covering many directions. There are two
main contributions of this article. The first one investigates various aspects
of some disciplines (e.g., arts, philosophy, psychology, and neuroscience) that
are interested in the subject and review some of their contributions stressing
on those that can be useful for computer vision researchers. Moreover, many
examples are illustrated to indicate the benefits of integrating concepts and
results among different disciplines. The second contribution is concerned with
the subject from the computer vision aspect where we discuss the following
issues. First, we explore many demanding and promising applications to reveal
the wide and massive importance of the field. Second, we list various types of
sensors that may be used for acquiring various data. Third, we review different
taxonomies used for classifying motions. Fourth, we review various processes
involved in motion analysis. Fifth, we exhibit how different surveys are
structured. Sixth, we examine many of the most cited and recent reviews in the
field that have been published during the past two decades to reveal various
approaches used for implementing different stages of the problem and refer to
various algorithms and their suitability for different situations. Moreover, we
provide a long list of public datasets and discuss briefly some examples of
these datasets. Finally, we provide a general discussion of the subject from
the aspect of computer vision.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01075</identifier>
 <datestamp>2015-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01075</id><created>2015-09-03</created><authors><author><keyname>Lopez-Ramos</keyname><forenames>Juan Antonio</forenames></author><author><keyname>Rosenthal</keyname><forenames>Joachim</forenames></author><author><keyname>Schipani</keyname><forenames>Davide</forenames></author><author><keyname>Schnyder</keyname><forenames>Reto</forenames></author></authors><title>Group key management based on semigroup actions</title><categories>cs.IT cs.CR math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we provide a suite of protocols for group key management based
on general semigroup actions. Construction of the key is made in a distributed
and collaborative way. We provide security proofs against passive attacks and
suitable examples that may enhance both the security level and communication
overheads of previous existing protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01081</identifier>
 <datestamp>2015-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01081</id><created>2015-09-03</created><authors><author><keyname>Schnyder</keyname><forenames>Reto</forenames></author><author><keyname>Lopez-Ramos</keyname><forenames>Juan Antonio</forenames></author><author><keyname>Rosenthal</keyname><forenames>Joachim</forenames></author><author><keyname>Schipani</keyname><forenames>Davide</forenames></author></authors><title>An Active Attack on a Multiparty Key Exchange Protocol</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The multiparty key exchange introduced in Steiner et al.\@ and presented in
more general form by the authors is known to be secure against passive attacks.
In this paper, an active attack is presented assuming malicious control of the
communications of the last two users for the duration of only the key exchange.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01094</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01094</id><created>2015-09-03</created><updated>2015-09-04</updated><authors><author><keyname>P&#xe9;rez</keyname><forenames>Miguel Rodr&#xed;guez</forenames></author><author><keyname>Alonso</keyname><forenames>Sergio Herrer&#xed;a</forenames></author><author><keyname>Veiga</keyname><forenames>Manuel Fern&#xe1;ndez</forenames></author><author><keyname>Garc&#xed;a</keyname><forenames>C&#xe1;ndido L&#xf3;pez</forenames></author></authors><title>An Ant Colonization Routing Algorithm to Minimize Network Power
  Consumption</title><categories>cs.NI</categories><comments>Accepted version of the manuscript. 12 pages</comments><journal-ref>Journal of Network and Computer Applications Volume 58, December
  2015, Pages 217--226</journal-ref><doi>10.1016/j.jnca.2015.08.011</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rising energy consumption of IT infrastructure concerns have spurred the
development of more power efficient networking equipment and algorithms. When
\emph{old} equipment just drew an almost constant amount of power regardless of
the traffic load, there were some efforts to minimize the total energy usage by
modifying routing decisions to aggregate traffic in a minimal set of links,
creating the opportunity to power off some unused equipment during low traffic
periods. New equipment, with power profile functions depending on the offered
load, presents new challenges for optimal routing. The goal now is not just to
power some links down, but to aggregate and/or spread the traffic so that
devices operate in their sweet spot in regards to network usage. In this paper
we present an algorithm that, making use of the ant colonization algorithm,
computes, in a decentralized manner, the routing tables so as to minimize
global energy consumption. Moreover, the resulting algorithm is also able to
track changes in the offered load and react to them in real time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01095</identifier>
 <datestamp>2015-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01095</id><created>2015-09-03</created><authors><author><keyname>Savage</keyname><forenames>Saiph</forenames></author><author><keyname>Monroy-Hernandez</keyname><forenames>Andres</forenames></author><author><keyname>Bhattacharjee</keyname><forenames>Kasturi</forenames></author><author><keyname>Hollerer</keyname><forenames>Tobias</forenames></author></authors><title>Tag Me Maybe: Perceptions of Public Targeted Sharing on Facebook</title><categories>cs.SI cs.CY</categories><comments>5 pages, one figure, Hypertext 2016</comments><acm-class>H.5.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social network sites allow users to publicly tag people in their posts. These
tagged posts allow users to share to both the general public and a targeted
audience, dynamically assembled via notifications that alert the people
mentioned. We investigate people's perceptions of this mixed sharing mode
through a qualitative study with 120 participants. We found that individuals
like this sharing modality as they believe it strengthens their relationships.
Individuals also report using tags to have more control of Facebook's ranking
algorithm, and to expose one another to novel information and people. This work
helps us understand people's complex relationships with the algorithms that
mediate their interactions with each another. We conclude by discussing the
design implications of these findings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01103</identifier>
 <datestamp>2015-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01103</id><created>2015-09-03</created><updated>2015-09-08</updated><authors><author><keyname>Li</keyname><forenames>Jun</forenames></author><author><keyname>Wen</keyname><forenames>Miaowen</forenames></author><author><keyname>Yan</keyname><forenames>Yier</forenames></author><author><keyname>Song</keyname><forenames>Sangseob</forenames></author><author><keyname>Lee</keyname><forenames>Moon Ho</forenames></author></authors><title>Differential Spatial Modulation with Gray Coded Antenna Activation Order</title><categories>cs.IT math.IT math.PR</categories><comments>This paper has been withdrawn by the author due to a error in
  equation 2</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Differential spatial modulation (DSM) was recently proposed to overcome the
challenge of channel estimation in spatial modulation (SM). In this letter, we
propose a gray code order of antenna index permutations for DSM. To facilitate
the implementation, the well-known Trotter-Johnson ranking and unranking
algorithms are adopted, which results in similar computational complexity to
the existing DSM that uses the lexicographic order. The coding gain achieved by
the proposed gray code order over the existing lexicographic order is also
analyzed and verified via simulations, which reveals a maximum of about 1.2dB
for the case of four transmit antennas. Based on the gray coding framework, we
further propose a diversity-enhancing scheme named intersected gray (I-gray)
code order for DSM, where the permutations of active antenna indices are
selected directly from the odd (even) positions of the full permutations in the
gray code order. From analysis and simulations, it is shown that the I-gray
code order can harvests an additional diversity order at the expense of only
one information bit loss for each transmission with respect to the gray code
order.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01116</identifier>
 <datestamp>2015-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01116</id><created>2015-09-03</created><authors><author><keyname>Martino</keyname><forenames>Giovanni Da San</forenames></author><author><keyname>Navarin</keyname><forenames>Nicol&#xf2;</forenames></author><author><keyname>Sperduti</keyname><forenames>Alessandro</forenames></author></authors><title>A tree-based kernel for graphs with continuous attributes</title><categories>cs.LG</categories><comments>Paper submitted to IEEE Transactions on Neural Networks and Learning
  Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The availability of graph data with node attributes that can be either
discrete or real-valued is constantly increasing. While existing kernel methods
are effective techniques for dealing with graphs having discrete node labels,
their adaptation to non-discrete or continuous node attributes has been
limited, mainly for computational issues. Recently, a few kernels especially
tailored for this domain, have been proposed. In order to alleviate the
computational problems, the size of the feature space of such kernels tend to
be smaller than the ones of the kernels for discrete node attributes. However,
such choice might have a negative impact on the predictive performance. In this
paper, we propose a graph kernel for complex and continuous nodes' attributes,
whose features are tree structures extracted from specific graph visits.
Experimental results obtained on real-world datasets show that the
(approximated version of the) proposed kernel is comparable with current
state-of-the-art kernels in terms of classification accuracy while requiring
shorter running times.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01117</identifier>
 <datestamp>2015-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01117</id><created>2015-08-31</created><authors><author><keyname>Helmling</keyname><forenames>Michael</forenames></author></authors><title>Introduction to Mathematical Programming-Based Error-Correction Decoding</title><categories>cs.IT math.IT</categories><comments>LaTeX sources maintained here: https://github.com/supermihi/lpdintro</comments><msc-class>94B35</msc-class><acm-class>E.4</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Decoding error-correctiong codes by methods of mathematical optimization,
most importantly linear programming, has become an important alternative
approach to both algebraic and iterative decoding methods since its
introduction by Feldman et al. At first celebrated mainly for its analytical
powers, real-world applications of LP decoding are now within reach thanks to
most recent research. This document gives an elaborate introduction into both
mathematical optimization and coding theory as well as a review of the
contributions by which these two areas have found common ground.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01122</identifier>
 <datestamp>2015-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01122</id><created>2015-09-03</created><authors><author><keyname>Mendes</keyname><forenames>Caio C&#xe9;sar Teodoro</forenames></author><author><keyname>Fr&#xe9;mont</keyname><forenames>Vincent</forenames></author><author><keyname>Wolf</keyname><forenames>Denis Fernando</forenames></author></authors><title>Vision-Based Road Detection using Contextual Blocks</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Road detection is a fundamental task in autonomous navigation systems. In
this paper, we consider the case of monocular road detection, where images are
segmented into road and non-road regions. Our starting point is the well-known
machine learning approach, in which a classifier is trained to distinguish road
and non-road regions based on hand-labeled images. We proceed by introducing
the use of &quot;contextual blocks&quot; as an efficient way of providing contextual
information to the classifier. Overall, the proposed methodology, including its
image feature selection and classifier, was conceived with computational cost
in mind, leaving room for optimized implementations. Regarding experiments, we
perform a sensible evaluation of each phase and feature subset that composes
our system. The results show a great benefit from using contextual blocks and
demonstrate their computational efficiency. Finally, we submit our results to
the KITTI road detection benchmark achieving scores comparable with state of
the art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01123</identifier>
 <datestamp>2015-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01123</id><created>2015-09-03</created><authors><author><keyname>Shang</keyname><forenames>Yilun</forenames></author></authors><title>A Combinatorial Necessary and Sufficient Condition for Cluster Consensus</title><categories>cs.SY math.OC</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this technical note, cluster consensus of discrete-time linear multi-agent
systems is investigated. A set of stochastic matrices $\mathcal{P}$ is said to
be a cluster consensus set if the system achieves cluster consensus for any
initial state and any sequence of matrices taken from $\mathcal{P}$. By
introducing a cluster ergodicity coefficient, we present an equivalence
relation between a range of characterization of cluster consensus set under
some mild conditions including the widely adopted inter-cluster common
influence. We obtain a combinatorial necessary and sufficient condition for a
compact set $\mathcal{P}$ to be a cluster consensus set. This combinatorial
condition is an extension of the avoiding set condition for global consensus,
and can be easily checked by an elementary routine. As a byproduct, our result
unveils that the cluster-spanning trees condition is not only sufficient but
necessary in some sense for cluster consensus problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01126</identifier>
 <datestamp>2015-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01126</id><created>2015-09-03</created><authors><author><keyname>Potluri</keyname><forenames>Pushpa Sree</forenames></author></authors><title>Training of CC4 Neural Network with Spread Unary Coding</title><categories>cs.NE</categories><comments>8 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper adapts the corner classification algorithm (CC4) to train the
neural networks using spread unary inputs. This is an important problem as
spread unary appears to be at the basis of data representation in biological
learning. The modified CC4 algorithm is tested using the pattern classification
experiment and the results are found to be good. Specifically, we show that the
number of misclassified points is not particularly sensitive to the chosen
radius of generalization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01149</identifier>
 <datestamp>2015-10-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01149</id><created>2015-09-03</created><updated>2015-10-28</updated><authors><author><keyname>Williams</keyname><forenames>Grady</forenames></author><author><keyname>Aldrich</keyname><forenames>Andrew</forenames></author><author><keyname>Theodorou</keyname><forenames>Evangelos</forenames></author></authors><title>Model Predictive Path Integral Control using Covariance Variable
  Importance Sampling</title><categories>cs.SY cs.DC cs.RO</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we develop a Model Predictive Path Integral (MPPI) control
algorithm based on a generalized importance sampling scheme and perform
parallel optimization via sampling using a Graphics Processing Unit (GPU). The
proposed generalized importance sampling scheme allows for changes in the drift
and diffusion terms of stochastic diffusion processes and plays a significant
role in the performance of the model predictive control algorithm. We compare
the proposed algorithm in simulation with a model predictive control version of
differential dynamic programming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01168</identifier>
 <datestamp>2015-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01168</id><created>2015-09-03</created><authors><author><keyname>Damianou</keyname><forenames>Andreas</forenames></author><author><keyname>Lawrence</keyname><forenames>Neil D.</forenames></author></authors><title>Semi-described and semi-supervised learning with Gaussian processes</title><categories>stat.ML cs.AI cs.LG math.PR</categories><comments>Published in the proceedings for Uncertainty in Artificial
  Intelligence (UAI), 2015</comments><msc-class>60G15, 58E30</msc-class><acm-class>G.3; I.2.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Propagating input uncertainty through non-linear Gaussian process (GP)
mappings is intractable. This hinders the task of training GPs using uncertain
and partially observed inputs. In this paper we refer to this task as
&quot;semi-described learning&quot;. We then introduce a GP framework that solves both,
the semi-described and the semi-supervised learning problems (where missing
values occur in the outputs). Auto-regressive state space simulation is also
recognised as a special case of semi-described learning. To achieve our goal we
develop variational methods for handling semi-described inputs in GPs, and
couple them with algorithms that allow for imputing the missing values while
treating the uncertainty in a principled, Bayesian manner. Extensive
experiments on simulated and real-world data study the problems of iterative
forecasting and regression/classification with missing values. The results
suggest that the principled propagation of uncertainty stemming from our
framework can significantly improve performance in these tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01173</identifier>
 <datestamp>2015-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01173</id><created>2015-09-03</created><authors><author><keyname>Zhang</keyname><forenames>Yuan</forenames></author><author><keyname>Levina</keyname><forenames>Elizaveta</forenames></author><author><keyname>Zhu</keyname><forenames>Ji</forenames></author></authors><title>Community Detection in Networks with Node Features</title><categories>stat.ML cs.SI physics.soc-ph</categories><comments>16 pages, 5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many methods have been proposed for community detection in networks, but most
of them do not take into account additional information on the nodes that is
often available in practice. In this paper, we propose a new joint community
detection criterion that uses both the network edge information and the node
features to detect community structures. One advantage our method has over
existing joint detection approaches is the flexibility of learning the impact
of different features which may differ across communities. Another advantage is
the flexibility of choosing the amount of influence the feature information has
on communities. The method is asymptotically consistent under the block model
with additional assumptions on the feature distributions, and performs well on
simulated and real networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01183</identifier>
 <datestamp>2015-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01183</id><created>2015-09-03</created><authors><author><keyname>Fan</keyname><forenames>Miao</forenames></author><author><keyname>Zhou</keyname><forenames>Qiang</forenames></author><author><keyname>Zheng</keyname><forenames>Thomas Fang</forenames></author><author><keyname>Grishman</keyname><forenames>Ralph</forenames></author></authors><title>Parallel Knowledge Embedding with MapReduce on a Multi-core Processor</title><categories>cs.DC cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article firstly attempts to explore parallel algorithms of learning
distributed representations for both entities and relations in large-scale
knowledge repositories with {\it MapReduce} programming model on a multi-core
processor. We accelerate the training progress of a canonical knowledge
embedding method, i.e. {\it translating embedding} ({\bf TransE}) model, by
dividing a whole knowledge repository into several balanced subsets, and
feeding each subset into an individual core where local embeddings can
concurrently run updating during the {\it Map} phase. However, it usually
suffers from inconsistent low-dimensional vector representations of the same
key, which are collected from different {\it Map} workers, and further leads to
conflicts when conducting {\it Reduce} to merge the various vectors associated
with the same key. Therefore, we try several strategies to acquire the merged
embeddings which may not only retain the performance of {\it entity inference},
{\it relation prediction}, and even {\it triplet classification} evaluated by
the single-thread {\bf TransE} on several well-known knowledge bases such as
Freebase and NELL, but also scale up the learning speed along with the number
of cores within a processor. So far, the empirical studies show that we could
achieve comparable results as the single-thread {\bf TransE} performs by the
{\it stochastic gradient descend} (SGD) algorithm, as well as increase the
training speed multiple times via adapting the {\it batch gradient descend}
(BGD) algorithm for {\it MapReduce} paradigm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01186</identifier>
 <datestamp>2015-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01186</id><created>2015-09-03</created><authors><author><keyname>Sun</keyname><forenames>Wei</forenames></author><author><keyname>Theodorou</keyname><forenames>Evangelos</forenames></author><author><keyname>Tsiotras</keyname><forenames>Panagiotis</forenames></author></authors><title>Model Based Reinforcement Learning with Final Time Horizon Optimization</title><categories>cs.SY</categories><comments>9 pages, 5 figures, NIPS2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present one of the first algorithms on model based reinforcement learning
and trajectory optimization with free final time horizon. Grounded on the
optimal control theory and Dynamic Programming, we derive a set of backward
differential equations that propagate the value function and provide the
optimal control policy and the optimal time horizon. The resulting policy
generalizes previous results in model based trajectory optimization. Our
analysis shows that the proposed algorithm recovers the theoretical optimal
solution on linear low dimensional problem. Finally we provide application
results on nonlinear systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01187</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01187</id><created>2015-09-03</created><updated>2016-02-13</updated><authors><author><keyname>Mozaffari</keyname><forenames>Mohammad</forenames></author><author><keyname>Saad</keyname><forenames>Walid</forenames></author><author><keyname>Bennis</keyname><forenames>Mehdi</forenames></author><author><keyname>Debbah</keyname><forenames>Merouane</forenames></author></authors><title>Unmanned Aerial Vehicle with Underlaid Device-to-Device Communications:
  Performance and Tradeoffs</title><categories>cs.IT cs.NI math.IT</categories><comments>accepted in the IEEE Transactions on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the deployment of an unmanned aerial vehicle (UAV) as a flying
base station used to provide on the fly wireless communications to a given
geographical area is analyzed. In particular, the co-existence between the UAV,
that is transmitting data in the downlink, and an underlaid device-todevice
(D2D) communication network is considered. For this model, a tractable
analytical framework for the coverage and rate analysis is derived. Two
scenarios are considered: a static UAV and a mobile UAV. In the first scenario,
the average coverage probability and the system sum-rate for the users in the
area are derived as a function of the UAV altitude and the number of D2D users.
In the second scenario, using the disk covering problem, the minimum number of
stop points that the UAV needs to visit in order to completely cover the area
is computed. Furthermore, considering multiple retransmissions for the UAV and
D2D users, the overall outage probability of the D2D users is derived.
Simulation and analytical results show that, depending on the density of D2D
users, optimal values for the UAV altitude exist for which the system sum-rate
and the coverage probability are maximized. Moreover, our results also show
that, by enabling the UAV to intelligently move over the target area, the total
required transmit power of UAV while covering the entire area, is minimized.
Finally, in order to provide a full coverage for the area of interest, the
tradeoff between the coverage and delay, in terms of the number of stop points,
is discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01190</identifier>
 <datestamp>2015-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01190</id><created>2015-09-03</created><authors><author><keyname>Sanders</keyname><forenames>Peter</forenames></author><author><keyname>Schulz</keyname><forenames>Christian</forenames></author></authors><title>Advanced Multilevel Node Separator Algorithms</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A node separator of a graph is a subset S of the nodes such that removing S
and its incident edges divides the graph into two disconnected components of
about equal size. In this work, we introduce novel algorithms to find small
node separators in large graphs. With focus on solution quality, we introduce
novel flow-based local search algorithms which are integrated in a multilevel
framework. In addition, we transfer techniques successfully used in the graph
partitioning field. This includes the usage of edge ratings tailored to our
problem to guide the graph coarsening algorithm as well as highly localized
local search and iterated multilevel cycles to improve solution quality even
further. Experiments indicate that flow-based local search algorithms on its
own in a multilevel framework are already highly competitive in terms of
separator quality. Adding additional local search algorithms further improves
solution quality. Our strongest configuration almost always outperforms
competing systems while on average computing 10% and 62% smaller separators
than Metis and Scotch, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01199</identifier>
 <datestamp>2015-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01199</id><created>2015-08-25</created><authors><author><keyname>Legara</keyname><forenames>Erika Fille</forenames></author><author><keyname>Monterola</keyname><forenames>Christopher</forenames></author></authors><title>Inferring Passenger Type from Commuter Eigentravel Matrices</title><categories>physics.soc-ph cs.CY physics.data-an stat.AP stat.ML</categories><comments>14 pages, 7 figures. Preprint submitted to Elsevier and is currently
  under review. An earlier version of this work (contributed as an extended
  abstract) has been accepted for presentation at the 2015 Conference on
  Complex Systems in Phoenix, Arizona, USA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A sufficient knowledge of the demographics of a commuting public is essential
in formulating and implementing more targeted transportation policies, as
commuters exhibit different ways of traveling. With the advent of the Automated
Fare Collection system (AFC), probing the travel patterns of commuters has
become less invasive and more accessible. Consequently, numerous transport
studies related to human mobility have shown that these observed patterns allow
one to pair individuals with locations and/or activities at certain times of
the day. However, classifying commuters using their travel signatures is yet to
be thoroughly examined.
  Here, we contribute to the literature by demonstrating a procedure to
characterize passenger types (Adult, Child/Student, and Senior Citizen) based
on their three-month travel patterns taken from a smart fare card system. We
first establish a method to construct distinct commuter matrices, which we
refer to as eigentravel matrices, that capture the characteristic travel
routines of individuals. From the eigentravel matrices, we build classification
models that predict the type of passengers traveling. Among the models
explored, the gradient boosting method (GBM) gives the best prediction accuracy
at 76%, which is 84% better than the minimum model accuracy (41%) required
vis-\`a-vis the proportional chance criterion. In addition, we find that travel
features generated during weekdays have greater predictive power than those on
weekends. This work should not only be useful for transport planners, but for
market researchers as well. With the awareness of which commuter types are
traveling, ads, service announcements, and surveys, among others, can be made
more targeted spatiotemporally. Finally, our framework should be effective in
creating synthetic populations for use in real-world simulations that involve a
metropolitan's public transport system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01205</identifier>
 <datestamp>2015-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01205</id><created>2015-09-03</created><authors><author><keyname>Torrieri</keyname><forenames>Don</forenames></author><author><keyname>Talarico</keyname><forenames>Salvatore</forenames></author><author><keyname>Valenti</keyname><forenames>Matthew C.</forenames></author></authors><title>Performance Comparisons of Geographic Routing Protocols in Mobile Ad Hoc
  Networks</title><categories>cs.IT cs.NI math.IT</categories><comments>11 pages, 13 figures, to appear on IEEE Transactions On
  Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Geographic routing protocols greatly reduce the requirements of topology
storage and provide flexibility in the accommodation of the dynamic behavior of
mobile ad hoc networks. This paper presents performance evaluations and
comparisons of two geographic routing protocols and the popular AODV protocol.
The tradeoffs among the average path reliabilities, average conditional delays,
average conditional numbers of hops, and area spectral efficiencies and the
effects of various parameters are illustrated for finite ad hoc networks with
randomly placed mobiles. This paper uses a dual method of closed-form analysis
and simple simulation that is applicable to most routing protocols and provides
a much more realistic performance evaluation than has previously been possible.
Some features included in the new analysis are shadowing, exclusion and guard
zones, distance-dependent fading, and interference correlation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01208</identifier>
 <datestamp>2015-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01208</id><created>2015-09-03</created><updated>2015-10-02</updated><authors><author><keyname>Kuang</keyname><forenames>Da</forenames></author><author><keyname>Drake</keyname><forenames>Barry</forenames></author><author><keyname>Park</keyname><forenames>Haesun</forenames></author></authors><title>Fast Clustering and Topic Modeling Based on Rank-2 Nonnegative Matrix
  Factorization</title><categories>cs.LG cs.IR cs.NA</categories><comments>This paper has been withdrawn by the author to clarify the authorship</comments><acm-class>F.2.1; H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The importance of unsupervised clustering and topic modeling is well
recognized with ever-increasing volumes of text data. In this paper, we propose
a fast method for hierarchical clustering and topic modeling called HierNMF2.
Our method is based on fast Rank-2 nonnegative matrix factorization (NMF) that
performs binary clustering and an efficient node splitting rule. Further
utilizing the final leaf nodes generated in HierNMF2 and the idea of
nonnegative least squares fitting, we propose a new clustering/topic modeling
method called FlatNMF2 that recovers a flat clustering/topic modeling result in
a very simple yet significantly more effective way than any other existing
methods. We implement highly optimized open source software in C++ for both
HierNMF2 and FlatNMF2 for hierarchical and partitional clustering/topic
modeling of document data sets.
  Substantial experimental tests are presented that illustrate significant
improvements both in computational time as well as quality of solutions. We
compare our methods to other clustering methods including K-means, standard
NMF, and CLUTO, and also topic modeling methods including latent Dirichlet
allocation (LDA) and recently proposed algorithms for NMF with separability
constraints. Overall, we present efficient tools for analyzing large-scale data
sets, and techniques that can be generalized to many other data analytics
problem domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01220</identifier>
 <datestamp>2015-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01220</id><created>2015-08-22</created><authors><author><keyname>Ben-Ezra</keyname><forenames>Moshe</forenames></author></authors><title>Light Efficient Flutter Shutter</title><categories>cs.GR cs.CV</categories><comments>This documnet and the code listing in it are submitted under the
  permissive MIT License in hope it will be useful. In case anyone is
  interesting in 2012 date confirmation - the documnet was notarized at MIT on
  5 Dec 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Flutter shutter is a technique in which the exposure is chopped into segments
and light is only integrated part of the time. By carefully selecting the
chopping sequence it is possible to better condition the data for
reconstruction problems such as motion deblurring, focal sweeping, and
compressed sensing. The partial exposure trades better conditioning for less
energy. In problems such as motion deblurring the available energy is what
caused the problem in the first place (as strong illumination allows short
exposure thus eliminates motion blur). It is still beneficial because the
benefit from the better conditioning outweighs the cost in energy.
  This documents is focused on light efficient flutter shutter that provides
better conditioning and better energy utilization than conventional flutter
shutter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01221</identifier>
 <datestamp>2015-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01221</id><created>2015-09-03</created><updated>2015-10-02</updated><authors><author><keyname>Crochemore</keyname><forenames>Maxime</forenames></author><author><keyname>Kolpakov</keyname><forenames>Roman</forenames></author><author><keyname>Kucherov</keyname><forenames>Gregory</forenames></author></authors><title>Optimal searching of gapped repeats in a word</title><categories>cs.FL</categories><comments>27 pages. arXiv admin note: text overlap with arXiv:1309.4055</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Following (Kolpakov et al., 2013; Gawrychowski and Manea, 2015), we continue
the study of {\em $\alpha$-gapped repeats} in strings, defined as factors $uvu$
with $|uv|\leq \alpha |u|$. Our main result is the $O(\alpha n)$ bound on the
number of {\em maximal} $\alpha$-gapped repeats in a string of length $n$,
previously proved to be $O(\alpha^2 n)$ in (Kolpakov et al., 2013). For a
closely related notion of maximal $\delta$-subrepetition (maximal factors of
exponent between $1+\delta$ and $2$), our result implies the $O(n/\delta)$
bound on their number, which improves the bound of (Kolpakov et al., 2010) by a
$\log n$ factor.
  We also prove an algorithmic time bound $O(\alpha n+S)$ ($S$ size of the
output) for computing all maximal $\alpha$-gapped repeats. Our solution,
inspired by (Gawrychowski and Manea, 2015), is different from the recently
published proof by (Tanimura et al., 2015) of the same bound. Together with our
bound on $S$, this implies an $O(\alpha n)$-time algorithm for computing all
maximal $\alpha$-gapped repeats.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01226</identifier>
 <datestamp>2015-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01226</id><created>2015-09-02</created><authors><author><keyname>AbdollahRamezani</keyname><forenames>Sajjad</forenames></author><author><keyname>Arik</keyname><forenames>Kamalodin</forenames></author><author><keyname>Khavasi</keyname><forenames>Amin</forenames></author><author><keyname>Kavehvash</keyname><forenames>Zahra</forenames></author></authors><title>Analog Computing Using Graphene-based Metalines</title><categories>cs.ET cond-mat.mes-hall physics.optics</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the new concept of &quot;metalines&quot; for manipulating the amplitude
and phase profile of an incident wave locally and independently. Thanks to the
highly confined graphene plasmons, a transmit-array of graphene-based metalines
is used to realize analog computing on an ultra-compact, integrable and planar
platform. By employing the general concepts of spatial Fourier transformation,
a well-designed structure of such meta-transmit-array combined with graded
index lenses can perform two mathematical operations; i.e. differentiation and
integration, with high efficiency. The presented configuration is about 60
times shorter than the recent structure proposed by Silva et al.(Science, 2014,
343, 160-163); moreover, our simulated output responses are in more agreement
with the desired analytic results. These findings may lead to remarkable
achievements in light-based plasmonic signal processors at nanoscale instead of
their bulky conventional dielectric lens-based counterparts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01229</identifier>
 <datestamp>2015-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01229</id><created>2015-09-03</created><authors><author><keyname>Opper</keyname><forenames>Manfred</forenames></author><author><keyname>&#xc7;akmak</keyname><forenames>Burak</forenames></author><author><keyname>Winther</keyname><forenames>Ole</forenames></author></authors><title>A Theory of Solving TAP Equations for Ising Models with General
  Invariant Random Matrices</title><categories>cond-mat.dis-nn cs.IT math.IT</categories><comments>26 pages, 6 Figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of solving TAP mean field equations by iteration for
Ising model with coupling matrices that are drawn at random from general
invariant ensembles. We develop an analysis of iterative algorithms using a
dynamical functional approach that in the thermodynamic limit yields an
effective dynamics of a single variable trajectory. Our main novel contribution
is the expression for the implicit memory term of the dynamics for general
invariant ensembles. By subtracting these terms, that depend on magnetizations
at previous time steps, the implicit memory terms cancel making the iteration
dependent on a Gaussian distributed field only. The TAP magnetizations are
stable fixed points if an AT stability criterion is fulfilled. We illustrate
our method explicitly for coupling matrices drawn from the random orthogonal
ensemble.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01240</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01240</id><created>2015-09-03</created><updated>2016-02-07</updated><authors><author><keyname>Hardt</keyname><forenames>Moritz</forenames></author><author><keyname>Recht</keyname><forenames>Benjamin</forenames></author><author><keyname>Singer</keyname><forenames>Yoram</forenames></author></authors><title>Train faster, generalize better: Stability of stochastic gradient
  descent</title><categories>cs.LG math.OC stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that parametric models trained by a stochastic gradient method (SGM)
with few iterations have vanishing generalization error. We prove our results
by arguing that SGM is algorithmically stable in the sense of Bousquet and
Elisseeff. Our analysis only employs elementary tools from convex and
continuous optimization. We derive stability bounds for both convex and
non-convex optimization under standard Lipschitz and smoothness assumptions.
  Applying our results to the convex case, we provide new insights for why
multiple epochs of stochastic gradient methods generalize well in practice. In
the non-convex case, we give a new interpretation of common practices in neural
networks, and formally show that popular techniques for training large deep
models are indeed stability-promoting. Our findings conceptually underscore the
importance of reducing training time beyond its obvious benefit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01270</identifier>
 <datestamp>2015-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01270</id><created>2015-09-03</created><authors><author><keyname>Farhidzadeh</keyname><forenames>Hamidreza</forenames></author></authors><title>Machine Learning Methods to Analyze Arabidopsis Thaliana Plant Root
  Growth</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the challenging problems in biology is to classify plants based on
their reaction on genetic mutation. Arabidopsis Thaliana is a plant that is so
interesting, because its genetic structure has some similarities with that of
human beings. Biologists classify the type of this plant to mutated and not
mutated (wild) types. Phenotypic analysis of these types is a time-consuming
and costly effort by individuals. In this paper, we propose a modified feature
extraction step by using velocity and acceleration of root growth. In the
second step, for plant classification, we employed different Support Vector
Machine (SVM) kernels and two hybrid systems of neural networks. Gated Negative
Correlation Learning (GNCL) and Mixture of Negatively Correlated Experts (MNCE)
are two ensemble methods based on complementary feature of classical
classifiers; Mixture of Expert (ME) and Negative Correlation Learning (NCL).
The hybrid systems conserve of advantages and decrease the effects of
disadvantages of NCL and ME. Our Experimental shows that MNCE and GNCL improve
the efficiency of classical classifiers, however, some SVM kernels function has
better performance than classifiers based on neural network ensemble method.
Moreover, kernels consume less time to obtain a classification rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01271</identifier>
 <datestamp>2015-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01271</id><created>2015-09-03</created><authors><author><keyname>Farhidzadeh</keyname><forenames>Hamidreza</forenames></author></authors><title>Probabilistic Neural Network Training for Semi-Supervised Classifiers</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose another version of help-training approach by
employing a Probabilistic Neural Network (PNN) that improves the performance of
the main discriminative classifier in the semi-supervised strategy. We
introduce the PNN-training algorithm and use it for training the support vector
machine (SVM) with a few numbers of labeled data and a large number of
unlabeled data. We try to find the best labels for unlabeled data and then use
SVM to enhance the classification rate. We test our method on two famous
benchmarks and show the efficiency of our method in comparison with pervious
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01277</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01277</id><created>2015-09-03</created><updated>2016-02-20</updated><authors><author><keyname>Rennie</keyname><forenames>Colin</forenames></author><author><keyname>Shome</keyname><forenames>Rahul</forenames></author><author><keyname>Bekris</keyname><forenames>Kostas E.</forenames></author><author><keyname>De Souza</keyname><forenames>Alberto F.</forenames></author></authors><title>A Dataset for Improved RGBD-based Object Detection and Pose Estimation
  for Warehouse Pick-and-Place</title><categories>cs.CV cs.RO</categories><comments>To appear in RA-L</comments><msc-class>68T40, 68T45, 68T01</msc-class><acm-class>I.2.10; I.2.9; I.4.8; I.5.2; I.4.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An important logistics application of robotics involves manipulators that
pick-and-place objects placed in warehouse shelves. A critical aspect of this
task corre- sponds to detecting the pose of a known object in the shelf using
visual data. Solving this problem can be assisted by the use of an RGB-D
sensor, which also provides depth information beyond visual data. Nevertheless,
it remains a challenging problem since multiple issues need to be addressed,
such as low illumination inside shelves, clutter, texture-less and reflective
objects as well as the limitations of depth sensors. This paper provides a new
rich data set for advancing the state-of-the-art in RGBD- based 3D object pose
estimation, which is focused on the challenges that arise when solving
warehouse pick- and-place tasks. The publicly available data set includes
thousands of images and corresponding ground truth data for the objects used
during the first Amazon Picking Challenge at different poses and clutter
conditions. Each image is accompanied with ground truth information to assist
in the evaluation of algorithms for object detection. To show the utility of
the data set, a recent algorithm for RGBD-based pose estimation is evaluated in
this paper. Based on the measured performance of the algorithm on the data set,
various modifications and improvements are applied to increase the accuracy of
detection. These steps can be easily applied to a variety of different
methodologies for object pose detection and improve performance in the domain
of warehouse pick-and-place.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01287</identifier>
 <datestamp>2015-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01287</id><created>2015-09-03</created><authors><author><keyname>Condessa</keyname><forenames>Filipe</forenames></author><author><keyname>Bioucas-Dias</keyname><forenames>Jos&#xe9;</forenames></author><author><keyname>Castro</keyname><forenames>Carlos</forenames></author><author><keyname>Ozolek</keyname><forenames>John</forenames></author><author><keyname>Kova&#x10d;evi&#x107;</keyname><forenames>Jelena</forenames></author></authors><title>Image Classification with Rejection using Contextual Information</title><categories>cs.CV</categories><comments>21 pages, 8 figures</comments><msc-class>68T10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new supervised algorithm for image classification with
rejection using multiscale contextual information. Rejection is desired in
image-classification applications that require a robust classifier but not the
classification of the entire image. The proposed algorithm combines local and
multiscale contextual information with rejection, improving the classification
performance. As a probabilistic model for classification, we adopt a
multinomial logistic regression. The concept of rejection with contextual
information is implemented by modeling the classification problem as an energy
minimization problem over a graph representing local and multiscale
similarities of the image. The rejection is introduced through an energy data
term associated with the classification risk and the contextual information
through an energy smoothness term associated with the local and multiscale
similarities within the image. We illustrate the proposed method on the
classification of images of H&amp;E-stained teratoma tissues.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01288</identifier>
 <datestamp>2015-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01288</id><created>2015-09-03</created><authors><author><keyname>Zimmermann</keyname><forenames>Max</forenames></author><author><keyname>Ntoutsi</keyname><forenames>Eirini</forenames></author><author><keyname>Spiliopoulou</keyname><forenames>Myra</forenames></author></authors><title>Incremental Active Opinion Learning Over a Stream of Opinionated
  Documents</title><categories>cs.IR cs.CL cs.LG</categories><comments>10 pages, 14 figures, conference: WISDOM (KDD'15)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Applications that learn from opinionated documents, like tweets or product
reviews, face two challenges. First, the opinionated documents constitute an
evolving stream, where both the author's attitude and the vocabulary itself may
change. Second, labels of documents are scarce and labels of words are
unreliable, because the sentiment of a word depends on the (unknown) context in
the author's mind. Most of the research on mining over opinionated streams
focuses on the first aspect of the problem, whereas for the second a continuous
supply of labels from the stream is assumed. Such an assumption though is
utopian as the stream is infinite and the labeling cost is prohibitive. To this
end, we investigate the potential of active stream learning algorithms that ask
for labels on demand. Our proposed ACOSTREAM 1 approach works with limited
labels: it uses an initial seed of labeled documents, occasionally requests
additional labels for documents from the human expert and incrementally adapts
to the underlying stream while exploiting the available labeled documents. In
its core, ACOSTREAM consists of a MNB classifier coupled with &quot;sampling&quot;
strategies for requesting class labels for new unlabeled documents. In the
experiments, we evaluate the classifier performance over time by varying: (a)
the class distribution of the opinionated stream, while assuming that the set
of the words in the vocabulary is fixed but their polarities may change with
the class distribution; and (b) the number of unknown words arriving at each
moment, while the class polarity may also change. Our results show that active
learning on a stream of opinionated documents, delivers good performance while
requiring a small selection of labels
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01301</identifier>
 <datestamp>2015-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01301</id><created>2015-09-03</created><authors><author><keyname>Bang-Jensen</keyname><forenames>Joergen</forenames></author><author><keyname>Huang</keyname><forenames>J.</forenames></author><author><keyname>Zhu</keyname><forenames>Xuding</forenames></author></authors><title>Completing orientations of partially oriented graphs</title><categories>cs.DM</categories><comments>17 pages, 3 figures</comments><msc-class>05C20, 05C62</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We initiate a general study of what we call orientation completion problems.
For a fixed class C of oriented graphs, the orientation completion problem asks
whether a given partially oriented graph P can be completed to an oriented
graph in C by orienting the (non-oriented) edges in P. Orien- tation completion
problems commonly generalize several existing problems including recognition of
certain classes of graphs and digraphs as well as extending representations of
certain geometrically representable graphs. We study orientation completion
problems for various classes of oriented graphs, including k-arc- strong
oriented graphs, k-strong oriented graphs, quasi-transitive oriented graphs,
local tournament, acyclic local tournaments, locally transitive tournaments,
locally transitive local tournaments, in- tournaments, and oriented graphs
which have directed cycle factors. We show that the orientation completion
problem for each of these classes is either polynomial time solvable or
NP-complete. We also show that some of the NP-complete problems become
polynomial time solvable when the input oriented graphs satisfy certain extra
conditions. Our results imply that the representation extension problems for
proper interval graphs and for proper circular arc graphs are polynomial time
solvable, which generalize a previous result.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01302</identifier>
 <datestamp>2015-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01302</id><created>2015-09-03</created><authors><author><keyname>Kim</keyname><forenames>Kwang-Ki K.</forenames></author><author><keyname>Braatz</keyname><forenames>Richard D.</forenames></author></authors><title>Stability Analysis of Discrete-time Lure Systems with Slope-restricted
  Odd Monotonic Nonlinearities</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many nonlinear dynamical systems can be written as Lure systems, which are
described by a linear time-invariant system interconnected with a diagonal
static sector-bounded nonlinearity. Sufficient conditions are derived for the
global asymptotic stability analysis of discrete-time Lure systems in which the
nonlinearities have restricted slope and/or are odd, which is the usual case in
real applications. A Lure-Postnikov-type Lyapunov function is proposed that is
used to derive sufficient analysis conditions in terms of linear matrix
inequalities (LMIs). The derived stability critera are provably less
conservative than criteria published in the literature, with numerical examples
indicating that conservatism can be reduced by orders of magnitude.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01307</identifier>
 <datestamp>2015-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01307</id><created>2015-09-03</created><authors><author><keyname>Gorzny</keyname><forenames>Jan</forenames></author><author><keyname>Huang</keyname><forenames>Jing</forenames></author></authors><title>End-vertices of LBFS of (AT-free) bigraphs</title><categories>cs.DM</categories><comments>12 pages, 4 figures</comments><msc-class>05C85</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lexicographic Breadth First Search (LBFS) is one of fundamental graph search
algorithms that has numerous applications, including recognition of graph
classes, computation of graph parameters, and detection of certain graph
structures. The well-known result of Rose, Tarjan and Lueker on the
end-vertices of LBFS of chordal graphs has tempted researchers to study the
end-vertices of LBFS of various classes of graphs, including chordal graphs,
split graphs, interval graphs, and asteroidal triple-free (AT-free) graphs. In
this paper we study the end-vertices of LBFS of bipartite graphs. We show that
deciding whether a vertex of a bipartite graph is the end-vertex of an LBFS is
an NP-complete problem. In contrast we characterize the end-vertices of LBFS of
AT-free bipartite graphs. Our characterization implies that the problem of
deciding whether a vertex of an AT-free bipartite graph is the end-vertex of an
LBFS is solvable in polynomial time. Key words: Lexicographic breadth first
search, end-vertex, bipartite graphs, AT-free, proper interval bigraph,
characterization, algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01310</identifier>
 <datestamp>2015-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01310</id><created>2015-09-03</created><authors><author><keyname>Lu</keyname><forenames>Qian</forenames></author><author><keyname>Xu</keyname><forenames>Chunshan</forenames></author><author><keyname>Liu</keyname><forenames>Haitao</forenames></author></authors><title>The influence of Chunking on Dependency Crossing and Distance</title><categories>cs.CL</categories><comments>6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper hypothesizes that chunking plays important role in reducing
dependency distance and dependency crossings. Computer simulations, when
compared with natural languages,show that chunking reduces mean dependency
distance (MDD) of a linear sequence of nodes (constrained by continuity or
projectivity) to that of natural languages. More interestingly, chunking alone
brings about less dependency crossings as well, though having failed to reduce
them, to such rarity as found in human languages. These results suggest that
chunking may play a vital role in the minimization of dependency distance, and
a somewhat contributing role in the rarity of dependency crossing. In addition,
the results point to a possibility that the rarity of dependency crossings is
not a mere side-effect of minimization of dependency distance, but a linguistic
phenomenon with its own motivations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01313</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01313</id><created>2015-09-03</created><updated>2015-12-28</updated><authors><author><keyname>Zazo</keyname><forenames>Santiago</forenames></author><author><keyname>Macua</keyname><forenames>Sergio Valcarcel</forenames></author><author><keyname>S&#xe1;nchez-Fern&#xe1;ndez</keyname><forenames>Matilde</forenames></author><author><keyname>Zazo</keyname><forenames>Javier</forenames></author></authors><title>Dynamic Potential Games in Communications: Fundamentals and Applications</title><categories>cs.SY cs.GT math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a noncooperative dynamic game, multiple agents operating in a changing
environment aim to optimize their utilities over an infinite time horizon.
Time-varying environments allow to model more realistic scenarios (e.g., mobile
devices equipped with batteries, wireless communications over a fading channel,
etc.). However, solving a dynamic game is a difficult task that requires
dealing with multiple coupled optimal control problems. We focus our analysis
on a class of problems, named \textit{dynamic potential games}, whose solution
can be found through a single multivariate optimal control problem. Our
analysis generalizes previous studies by considering that the set of
environment's states and the set of players' actions are constrained, as it is
required by most of the applications. And the theoretical results are the
natural extension of the analysis for static potential games. We apply the
analysis and provide numerical methods to solve four key example problems, with
different features each: energy demand control in a smart-grid network, network
flow optimization in which the relays have bounded link capacity and limited
battery life, uplink multiple access communication with users that have to
optimize the use of their batteries, and two optimal scheduling games with
nonstationary channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01314</identifier>
 <datestamp>2015-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01314</id><created>2015-09-03</created><authors><author><keyname>Bax</keyname><forenames>Eric</forenames></author><author><keyname>Li</keyname><forenames>James</forenames></author><author><keyname>Wen</keyname><forenames>Zheng</forenames></author></authors><title>Exponential Weight Functions for Quasi-Proportional Auctions</title><categories>cs.GT</categories><comments>16 pages, 16 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In quasi-proportional auctions, the allocation is shared among bidders in
proportion to their weighted bids. The auctioneer selects a bid weight
function, and bidders know the weight function when they bid. In this note, we
analyze how weight functions that are exponential in the bid affect bidder
behavior. We show that exponential weight functions have a pure-strategy Nash
equilibrium, we characterize bids at an equilibrium, and we compare it to an
equilibrium for power weight functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01323</identifier>
 <datestamp>2015-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01323</id><created>2015-09-03</created><authors><author><keyname>Hong</keyname><forenames>Xia</forenames></author><author><keyname>Chen</keyname><forenames>Sheng</forenames></author><author><keyname>Guo</keyname><forenames>Yi</forenames></author><author><keyname>Gao</keyname><forenames>Junbin</forenames></author></authors><title>l1-norm Penalized Orthogonal Forward Regression</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A l1-norm penalized orthogonal forward regression (l1-POFR) algorithm is
proposed based on the concept of leaveone- out mean square error (LOOMSE).
Firstly, a new l1-norm penalized cost function is defined in the constructed
orthogonal space, and each orthogonal basis is associated with an individually
tunable regularization parameter. Secondly, due to orthogonal computation, the
LOOMSE can be analytically computed without actually splitting the data set,
and moreover a closed form of the optimal regularization parameter in terms of
minimal LOOMSE is derived. Thirdly, a lower bound for regularization parameters
is proposed, which can be used for robust LOOMSE estimation by adaptively
detecting and removing regressors to an inactive set so that the computational
cost of the algorithm is significantly reduced. Illustrative examples are
included to demonstrate the effectiveness of this new l1-POFR approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01324</identifier>
 <datestamp>2015-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01324</id><created>2015-09-03</created><authors><author><keyname>Huang</keyname><forenames>Kun</forenames></author><author><keyname>Parampalli</keyname><forenames>Udaya</forenames></author><author><keyname>Xian</keyname><forenames>Ming</forenames></author></authors><title>Security Concerns in Minimum Storage Cooperative Regenerating Codes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Here, we revisit the problem of exploring the secrecy capacity of minimum
storage cooperative regenerating (MSCR) codes under the
$\{l_1,l_2\}$-eavesdropper model, where the eavesdropper can observe the data
stored on $l_1$ nodes and the repair traffic of an additional $l_2$ nodes.
Compared to minimum storage regenerating (MSR) codes which support only single
node repairs, MSCR codes allow efficient simultaneous repairs of multiple
failed nodes, referred to as a \emph{repair group}. However, the repair data
sent from a helper node to another failed node may vary with different repair
groups or the sets of helper nodes, which would inevitably leak more data
information to the eavesdropper and even render the storage system unable to
maintain any data secrecy.
  In this paper, we introduce and study a special category of MSCR codes,
termed &quot;\emph{stable}&quot; MSCR codes, where the repair data from any one helper
node to any one failed node is required to be independent of the repair group
or the set of helper nodes. Our main contributions include: 1. Demonstrating
that two existing MSCR codes inherently are not stable and thus have poor
secrecy capacity, 2. Converting one existing MSCR code to a stable one, which
offers better secrecy capacity when compared to the original one, 3. Employing
information theoretic analysis to characterize the secrecy capacity of stable
MSCR codes in certain situations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01329</identifier>
 <datestamp>2015-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01329</id><created>2015-09-03</created><authors><author><keyname>Zhu</keyname><forenames>Yan</forenames></author><author><keyname>Tian</keyname><forenames>Yuandong</forenames></author><author><keyname>Mexatas</keyname><forenames>Dimitris</forenames></author><author><keyname>Doll&#xe1;r</keyname><forenames>Piotr</forenames></author></authors><title>Semantic Amodal Segmentation</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Common visual recognition tasks such as classification, object detection, and
semantic segmentation are rapidly reaching maturity, and given the recent rate
of progress, it is not unreasonable to conjecture that techniques for many of
these problems will approach human levels of performance in the next few years.
In this paper we look to the future: what is the next frontier in visual
recognition?
  We offer one possible answer to this question. We propose a detailed image
annotation that captures information beyond the visible pixels and requires
complex reasoning about full scene structure. Specifically, we create an amodal
segmentation of each image: the full extent of each region is marked, not just
the visible pixels. Annotators outline and name all salient regions in the
image and specify a partial depth order. The result is a rich scene structure,
including visible and occluded portions of each region, figure-ground edge
information, semantic labels, and object overlap.
  To date, we have labeled 500 images in the BSDS dataset with at least five
annotators per image. Critically, the resulting full scene annotation is
surprisingly consistent between annotators. For example, for edge detection our
annotations have substantially higher human consistency than the original BSDS
edges while providing a greater challenge for existing algorithms. We are
currently annotating ~5000 images from the MS COCO dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01330</identifier>
 <datestamp>2015-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01330</id><created>2015-09-03</created><authors><author><keyname>Mseddi</keyname><forenames>Amina</forenames></author><author><keyname>Salahuddin</keyname><forenames>Mohammad Ali</forenames></author><author><keyname>Zhani</keyname><forenames>Mohamed Faten</forenames></author><author><keyname>Elbiaze</keyname><forenames>Halima</forenames></author><author><keyname>Glitho</keyname><forenames>Roch H.</forenames></author></authors><title>On Optimizing Replica Migration in Distributed Cloud Storage Systems</title><categories>cs.NI cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the wide adoption of large-scale Internet services and big data, the
cloud has become the ideal environment to satisfy the ever-growing storage
demand, thanks to its seemingly limitless capacity, high availability and
faster access time. In this context, data replication has been touted as the
ultimate solution to improve data availability and reduce access time. However,
replica placement systems usually need to migrate and create a large number of
data replicas over time between and within data centers, incurring a large
overhead in terms of network load and availability. In this paper, we propose
CRANE, an effiCient Replica migrAtion scheme for distributed cloud Storage
systEms. CRANE complements any replica placement algorithm by efficiently
managing replica creation in geo-distributed infrastructures by (1) minimizing
the time needed to copy the data to the new replica location, (2) avoiding
network congestion, and (3) ensuring a minimal availability of the data. Our
results show that, compared to swift (the OpenStack project for managing data
storage), CRANE is able to minimize up to 30% of the replica creation time and
25% of inter-data center network traffic, while ensuring the minimum required
availability of the data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01331</identifier>
 <datestamp>2015-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01331</id><created>2015-09-03</created><authors><author><keyname>Kune</keyname><forenames>Raghavendra</forenames></author><author><keyname>Konugurthi</keyname><forenames>Pramodkumar</forenames></author><author><keyname>Agarwal</keyname><forenames>Arun</forenames></author><author><keyname>Chillarige</keyname><forenames>Raghavendra Rao</forenames></author><author><keyname>Buyya</keyname><forenames>Rajkumar</forenames></author></authors><title>The Anatomy of Big Data Computing</title><categories>cs.DC</categories><comments>33 pages, 11 figures, 5 tables. Appears in Software: Practice and
  Experience (SPE), Wiley Press, 2015</comments><acm-class>C.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Advances in information technology and its widespread growth in several areas
of business, engineering, medical and scientific studies are resulting in
information/data explosion. Knowledge discovery and decision making from such
rapidly growing voluminous data is a challenging task in terms of data
organization and processing, which is an emerging trend known as Big Data
Computing; a new paradigm which combines large scale compute, new data
intensive techniques and mathematical models to build data analytics. Big Data
computing demands a huge storage and computing for data curation and processing
that could be delivered from on-premise or clouds infrastructures. This paper
discusses the evolution of Big Data computing, differences between traditional
data warehousing and Big Data, taxonomy of Big Data computing and underpinning
technologies, integrated platform of Big Data and Clouds known as Big Data
Clouds, layered architecture and components of Big Data Cloud and finally
discusses open technical challenges and future directions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01332</identifier>
 <datestamp>2015-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01332</id><created>2015-09-03</created><authors><author><keyname>Natarajan</keyname><forenames>Lakshmi</forenames></author><author><keyname>Hong</keyname><forenames>Yi</forenames></author><author><keyname>Viterbo</keyname><forenames>Emanuele</forenames></author></authors><title>Lattice Codes achieve the Capacity of Gaussian Broadcast Channels with
  Coded Side Information</title><categories>cs.IT math.IT</categories><comments>21 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lattices possess elegant mathematical properties which have been previously
used in the literature to show that structured codes can be efficient in a
variety of communication scenarios, including coding for the additive white
Gaussian noise (AWGN) channel, dirty-paper channel, Wyner-Ziv coding, coding
for relay networks and so forth. Following the approach introduced by Erez and
Zamir, we show that lattice codes are optimal for the family of Gaussian
broadcast channels where the source transmits a set of common messages to all
receivers and each receiver has 'coded side information', i.e., prior
information in the form of linear combinations of the messages. This channel
model, which is an instance of the Gaussian version of index coding, is
motivated by applications to multi-terminal networks where the nodes may have
access to coded versions of the messages from previous signal hops or through
orthogonal channels. The known results on the capacity of this channel are
based on random Gaussian codebooks. The structured coding scheme proposed in
this paper utilizes Construction A lattices designed over prime finite fields,
and 'algebraic binning' at the decoders to expurgate the channel code and
obtain good lattice subcodes, for every possible set of linear combinations
available as side information. As a corollary, we show that lattice codes based
on Construction A can achieve the capacity of single-user AWGN channels with
the size 'p' of the prime field growing as a function of the code length 'n' as
'n^beta', for any fixed 'beta&gt;0', which is the slowest yet reported in the
literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01337</identifier>
 <datestamp>2015-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01337</id><created>2015-09-03</created><authors><author><keyname>Hou</keyname><forenames>Mingzhe</forenames></author><author><keyname>Deng</keyname><forenames>Zongquan</forenames></author><author><keyname>Duan</keyname><forenames>Guangren</forenames></author></authors><title>Adaptive Control of Uncertain Pure-feedback Nonlinear Systems</title><categories>cs.SY</categories><comments>Submitted to Automatica</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel adaptive control approach is proposed to solve the globally
asymptotic state regulation problem for unknown pure-feedback nonlinear systems
in the pseudo-affine form. The pure-feedback nonlinear system under
consideration is with nonlinearly parameterised uncertainties and unknown
functions as its control coefficients. Based on the parameter separation
technique, a backstepping controller is designed by adopting the adaptive high
gain idea. The proposed controller is capable of guaranteeing, for any initial
system condition, boundedness of all signals in the closed-loop system and
globally asymptotic regulation of the state. An example is employed to
demonstrate the effectiveness of the proposed control method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01338</identifier>
 <datestamp>2015-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01338</id><created>2015-09-04</created><authors><author><keyname>Soman</keyname><forenames>Sumit</forenames></author><author><keyname>Srivastava</keyname><forenames>Siddharth</forenames></author><author><keyname>Srivastava</keyname><forenames>Saurabh</forenames></author><author><keyname>Rajput</keyname><forenames>Nitendra</forenames></author></authors><title>Brain Computer Interfaces for Mobile Apps: State-of-the-art and Future
  Directions</title><categories>cs.HC</categories><comments>Reprint from Proceedings of the 9th International Conference on
  Interfaces and Human Computer Interaction (http://ihci-conf.org/), 8 pages</comments><msc-class>68T35, 68U35</msc-class><acm-class>H.5.2; H.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent times, there have been significant advancements in utilizing the
sensing capabilities of mobile devices for developing applications. The primary
objective has been to enhance the way a user interacts with the application by
making it effortless and convenient. This paper explores the capabilities of
using Brain Computer Interfaces (BCI), an evolving subset of Human Computer
Interaction (HCI) paradigms, to control mobile devices. We present a
comprehensive survey of the state-of-the-art in this area, discussing the
challenges and limitations in using BCI for mobile applications. Further we
propose possible modalities that in future can benefit with BCI applications.
This paper consolidates research directions being pursued in this domain, and
draws conclusions on feasibility and benefits of using BCI systems effectively
augmented to the mobile application development domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01343</identifier>
 <datestamp>2015-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01343</id><created>2015-09-04</created><authors><author><keyname>Abbasnejad</keyname><forenames>Iman</forenames></author><author><keyname>Sridharan</keyname><forenames>Sridha</forenames></author><author><keyname>Denman</keyname><forenames>Simon</forenames></author><author><keyname>Fookes</keyname><forenames>Clinton</forenames></author><author><keyname>Lucey</keyname><forenames>Simon</forenames></author></authors><title>Learning Temporal Alignment Uncertainty for Efficient Event Detection</title><categories>cs.CV</categories><comments>Appeared in DICTA 2015, 8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we tackle the problem of efficient video event detection. We
argue that linear detection functions should be preferred in this regard due to
their scalability and efficiency during estimation and evaluation. A popular
approach in this regard is to represent a sequence using a bag of words (BOW)
representation due to its: (i) fixed dimensionality irrespective of the
sequence length, and (ii) its ability to compactly model the statistics in the
sequence. A drawback to the BOW representation, however, is the intrinsic
destruction of the temporal ordering information. In this paper we propose a
new representation that leverages the uncertainty in relative temporal
alignments between pairs of sequences while not destroying temporal ordering.
Our representation, like BOW, is of a fixed dimensionality making it easily
integrated with a linear detection function. Extensive experiments on CK+,
6DMG, and UvA-NEMO databases show significant performance improvements across
both isolated and continuous event detection tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01346</identifier>
 <datestamp>2015-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01346</id><created>2015-09-04</created><authors><author><keyname>Zaidi</keyname><forenames>Nayyar A.</forenames></author><author><keyname>Webb</keyname><forenames>Geoffrey I.</forenames></author><author><keyname>Carman</keyname><forenames>Mark J.</forenames></author><author><keyname>Petitjean</keyname><forenames>Francois</forenames></author></authors><title>Deep Broad Learning - Big Models for Big Data</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep learning has demonstrated the power of detailed modeling of complex
high-order (multivariate) interactions in data. For some learning tasks there
is power in learning models that are not only Deep but also Broad. By Broad, we
mean models that incorporate evidence from large numbers of features. This is
of especial value in applications where many different features and
combinations of features all carry small amounts of information about the
class. The most accurate models will integrate all that information. In this
paper, we propose an algorithm for Deep Broad Learning called DBL. The proposed
algorithm has a tunable parameter $n$, that specifies the depth of the model.
It provides straightforward paths towards out-of-core learning for large data.
We demonstrate that DBL learns models from large quantities of data with
accuracy that is highly competitive with the state-of-the-art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01347</identifier>
 <datestamp>2015-11-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01347</id><created>2015-09-04</created><updated>2015-11-04</updated><authors><author><keyname>Denis</keyname><forenames>Christophe</forenames><affiliation>CMLA</affiliation></author><author><keyname>Castro</keyname><forenames>Pablo De Oliveira</forenames><affiliation>UVSQ</affiliation></author><author><keyname>Petit</keyname><forenames>Eric</forenames><affiliation>UVSQ</affiliation></author></authors><title>Verificarlo: checking floating point accuracy through Monte Carlo
  Arithmetic</title><categories>cs.MS cs.NA</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Numerical accuracy of floating point computation is a well studied topic
which has not made its way to the end-user in scientific computing. Yet, it has
become a critical issue with the recent requirements for code modernization to
harness new highly parallel hardware and perform higher resolution computation.
To democratize numerical accuracy analysis, it is important to propose tools
and methodologies to study large use cases in a reliable and automatic way. In
this paper, we propose verificarlo, an extension to the LLVM compiler to
automatically use Monte Carlo Arithmetic in a transparent way for the end-user.
It supports all the major languages including C, C++, and Fortran. Unlike
source-to-source approaches, our implementation captures the influence of
compiler optimizations on the numerical accuracy. We illustrate how Monte Carlo
Arithmetic using the verificarlo tool outperforms the existing approaches on
various use cases and is a step toward automatic numerical analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01349</identifier>
 <datestamp>2015-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01349</id><created>2015-09-04</created><authors><author><keyname>Avrachenkov</keyname><forenames>Konstantin</forenames><affiliation>MAESTRO</affiliation></author><author><keyname>Borkar</keyname><forenames>Vivek</forenames></author><author><keyname>Saboo</keyname><forenames>Krishnakant</forenames></author></authors><title>Parallel and Distributed Approaches for Graph Based Semi-supervised
  Learning</title><categories>cs.LG</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two approaches for graph based semi-supervised learning are proposed. The
firstapproach is based on iteration of an affine map. A key element of the
affine map iteration is sparsematrix-vector multiplication, which has several
very efficient parallel implementations. The secondapproach belongs to the
class of Markov Chain Monte Carlo (MCMC) algorithms. It is based onsampling of
nodes by performing a random walk on the graph. The latter approach is
distributedby its nature and can be easily implemented on several processors or
over the network. Boththeoretical and practical evaluations are provided. It is
found that the nodes are classified intotheir class with very small error. The
sampling algorithm's ability to track new incoming nodesand to classify them is
also demonstrated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01352</identifier>
 <datestamp>2015-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01352</id><created>2015-09-04</created><authors><author><keyname>Mitra</keyname><forenames>Rangeet</forenames></author><author><keyname>Bhatia</keyname><forenames>Vimal</forenames></author></authors><title>Diffusion-KLMS Algorithm and its Performance Analysis for Non-Linear
  Distributed Networks</title><categories>cs.LG cs.DC cs.IT cs.SY math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a distributed network environment, the diffusion-least mean squares (LMS)
algorithm gives faster convergence than the original LMS algorithm. It has also
been observed that, the diffusion-LMS generally outperforms other distributed
LMS algorithms like spatial LMS and incremental LMS. However, both the original
LMS and diffusion-LMS are not applicable in non-linear environments where data
may not be linearly separable. A variant of LMS called kernel-LMS (KLMS) has
been proposed in the literature for such non-linearities. In this paper, we
propose kernelised version of diffusion-LMS for non-linear distributed
environments. Simulations show that the proposed approach has superior
convergence as compared to algorithms of the same genre. We also introduce a
technique to predict the transient and steady-state behaviour of the proposed
algorithm. The techniques proposed in this work (or algorithms of same genre)
can be easily extended to distributed parameter estimation applications like
cooperative spectrum sensing and massive multiple input multiple output (MIMO)
receiver design which are potential components for 5G communication systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01353</identifier>
 <datestamp>2015-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01353</id><created>2015-09-04</created><authors><author><keyname>Wang</keyname><forenames>Zhe</forenames></author><author><keyname>Duan</keyname><forenames>Lingjie</forenames></author><author><keyname>Zhang</keyname><forenames>Rui</forenames></author></authors><title>Adaptively Directional Wireless Power Transfer for Large-scale Sensor
  Networks</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE J. Sel. Areas in Commun. A preliminary version of
  this paper will be presented at the IEEE Global Communication Conference, San
  Diego, USA, Dec. 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless power transfer (WPT) prolongs the lifetime of wireless sensor
network by providing sustainable power supply to the distributed sensor nodes
(SNs) via electromagnetic waves. To improve the energy transfer efficiency in a
large WPT system, this paper proposes an adaptively directional WPT (AD-WPT)
scheme, where the power beacons (PBs) adapt the energy beamforming strategy to
SNs' locations by concentrating the transmit power on the nearby SNs within the
efficient charging radius. With the aid of stochastic geometry, we derive the
closed-form expressions of the distribution metrics of the aggregate received
power at a typical SN and further approximate the complementary cumulative
distribution function using Gamma distribution with second-order moment
matching. To design the charging radius for the optimal AD-WPT operation, we
exploit the tradeoff between the power intensity of the energy beams and the
number of SNs to be charged. Depending on different SN task requirements, the
optimal AD-WPT can maximize the average received power or the active
probability of the SNs, respectively. It is shown that both the maximized
average received power and the maximized sensor active probability increase
with the increased deployment density and transmit power of the PBs, and
decrease with the increased density of the SNs and the energy beamwidth.
Finally, we show that the optimal AD-WPT can significantly improve the energy
transfer efficiency compared with the traditional omnidirectional WPT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01354</identifier>
 <datestamp>2015-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01354</id><created>2015-09-04</created><authors><author><keyname>Guo</keyname><forenames>Jinma</forenames></author><author><keyname>Li</keyname><forenames>Jianmin</forenames></author></authors><title>CNN Based Hashing for Image Retrieval</title><categories>cs.CV cs.LG</categories><comments>16 pages, 6 figures</comments><acm-class>I.2.6; H.3.1</acm-class><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  Along with data on the web increasing dramatically, hashing is becoming more
and more popular as a method of approximate nearest neighbor search. Previous
supervised hashing methods utilized similarity/dissimilarity matrix to get
semantic information. But the matrix is not easy to construct for a new
dataset. Rather than to reconstruct the matrix, we proposed a straightforward
CNN-based hashing method, i.e. binarilizing the activations of a fully
connected layer with threshold 0 and taking the binary result as hash codes.
This method achieved the best performance on CIFAR-10 and was comparable with
the state-of-the-art on MNIST. And our experiments on CIFAR-10 suggested that
the signs of activations may carry more information than the relative values of
activations between samples, and that the co-adaption between feature extractor
and hash functions is important for hashing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01360</identifier>
 <datestamp>2015-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01360</id><created>2015-09-04</created><authors><author><keyname>Nassif</keyname><forenames>Roula</forenames></author><author><keyname>Richard</keyname><forenames>C&#xe9;dric</forenames></author><author><keyname>Ferrari</keyname><forenames>Andr&#xe9;</forenames></author><author><keyname>Sayed</keyname><forenames>Ali H.</forenames></author></authors><title>Proximal Multitask Learning over Networks with Sparsity-inducing
  Coregularization</title><categories>cs.SY cs.MA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we consider multitask learning problems where clusters of nodes
are interested in estimating their own parameter vector. Cooperation among
clusters is beneficial when the optimal models of adjacent clusters have a good
number of similar entries. We propose a fully distributed algorithm for solving
this problem. The approach relies on minimizing a global mean-square error
criterion regularized by non-differentiable terms to promote cooperation among
neighboring clusters. A general diffusion forward-backward splitting strategy
is introduced. Then, it is specialized to the case of sparsity promoting
regularizers. A closed-form expression for the proximal operator of a weighted
sum of $\ell_1$-norms is derived to achieve higher efficiency. We also provide
conditions on the step-sizes that ensure convergence of the algorithm in the
mean and mean-square error sense. Simulations are conducted to illustrate the
effectiveness of the strategy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01371</identifier>
 <datestamp>2015-12-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01371</id><created>2015-09-04</created><updated>2015-12-14</updated><authors><author><keyname>Yang</keyname><forenames>Shudi</forenames></author><author><keyname>Yao</keyname><forenames>Zheng-An</forenames></author></authors><title>Complete Weight Enumerators of a Family of Three-Weight Linear Codes</title><categories>cs.IT math.IT</categories><comments>13 pages. arXiv admin note: text overlap with arXiv:1505.06326</comments><msc-class>94B15, 11T71</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linear codes have been an interesting topic in both theory and practice for
many years. In this paper, for an odd prime $p$, we present the explicit
complete weight enumerator of a family of $p$-ary linear codes constructed with
defining set. The weight enumerator is an mmediate result of the complete
weight enumerator, which shows that the codes proposed in this paper are
three-weight linear codes. Additionally, all nonzero codewords are minimal and
thus they are suitable for secret sharing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01377</identifier>
 <datestamp>2015-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01377</id><created>2015-09-04</created><authors><author><keyname>Joroughi</keyname><forenames>Vahid</forenames></author><author><keyname>V&#xe1;zquez</keyname><forenames>Miguel &#xc1;ngel</forenames></author><author><keyname>P&#xe9;rez-Neira</keyname><forenames>Ana I.</forenames></author></authors><title>Generalized Multicast Multibeam Precoding for Satellite Communications</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with the problem of precoding in multibeam satellite
systems. In contrast to general multiuser multiple-input-multiple-output (MIMO)
cellular schemes, multibeam satellite architectures suffer from different
challenges. First, satellite communications standards embed more than one user
in each frame in order to increase the channel coding gain. This leads to the
different so-called multigroup multicast model, whose optimization requires
computationally complex operations. Second, when the data traffic is generated
by several Earth stations (gateways), the precoding matrix must be
distributively computed and attain additional payload restrictions. Third,
since the feedback channel is adverse (large delay and quantization errors),
the precoding must be able to deal with such uncertainties. In order to solve
the aforementioned problems, we propose a two-stage precoding design in order
to both limit the multibeam interference and to enhance the intra-beam minimum
user signal power (i.e. the one that dictates the rate allocation per beam). A
robust version of the proposed precoder based on a first perturbation model is
presented. This mechanism behaves well when the channel state information is
corrupted. Furthermore, we propose a per beam user grouping mechanism together
with its robust version in order to increase the precoding gain. Finally, a
method for dealing with the multiple gateway architecture is presented, which
offers high throughputs with a low inter-gateway communication. The conceived
designs are evaluated in a close-to-real beam pattern and the latest broadband
communication standard for satellite communications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01379</identifier>
 <datestamp>2015-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01379</id><created>2015-09-04</created><authors><author><keyname>Balubaid</keyname><forenames>Mohammed A.</forenames></author><author><keyname>Manzoor</keyname><forenames>Umar</forenames></author></authors><title>Ontology Based SMS Controller for Smart Phones</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Text analysis includes lexical analysis of the text and has been widely
studied and used in diverse applications. In the last decade, researchers have
proposed many efficient solutions to analyze / classify large text dataset,
however, analysis / classification of short text is still a challenge because
1) the data is very sparse 2) It contains noise words and 3) It is difficult to
understand the syntactical structure of the text. Short Messaging Service (SMS)
is a text messaging service for mobile/smart phone and this service is
frequently used by all mobile users. Because of the popularity of SMS service,
marketing companies nowadays are also using this service for direct marketing
also known as SMS marketing.In this paper, we have proposed Ontology based SMS
Controller which analyze the text message and classify it using ontology
aslegitimate or spam. The proposed system has been tested on different
scenarios and experimental results shows that the proposed solution is
effective both in terms of efficiency and time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01380</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01380</id><created>2015-09-04</created><updated>2015-09-11</updated><authors><author><keyname>Fong</keyname><forenames>Silas L.</forenames></author><author><keyname>Tan</keyname><forenames>Vincent Y. F.</forenames></author></authors><title>A Proof of the Strong Converse Theorem for Gaussian Broadcast Channels
  via the Gaussian Poincar\'{e} Inequality</title><categories>cs.IT math.IT</categories><comments>17 pages. Submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that 2-user Gaussian broadcast channels admit the strong converse.
This implies that every sequence of block codes with an asymptotic average
error probability smaller than one is such that all the limit points of the
sequence of rate pairs must lie within the capacity region derived by Cover and
Bergmans. The main mathematical tool required for our analysis is a logarithmic
Sobolev inequality known as the Gaussian Poincar\'e inequality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01386</identifier>
 <datestamp>2015-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01386</id><created>2015-09-04</created><authors><author><keyname>Ahmed</keyname><forenames>Jawwad</forenames></author><author><keyname>Johnsson</keyname><forenames>Andreas</forenames></author><author><keyname>Yanggratoke</keyname><forenames>Rerngvit</forenames></author><author><keyname>Ardelius</keyname><forenames>John</forenames></author><author><keyname>Flinta</keyname><forenames>Christofer</forenames></author><author><keyname>Stadler</keyname><forenames>Rolf</forenames></author></authors><title>Predicting SLA Violations in Real Time using Online Machine Learning</title><categories>cs.NI cs.LG cs.SE stat.ML</categories><comments>8 pages, 5 figures, 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Detecting faults and SLA violations in a timely manner is critical for
telecom providers, in order to avoid loss in business, revenue and reputation.
At the same time predicting SLA violations for user services in telecom
environments is difficult, due to time-varying user demands and infrastructure
load conditions.
  In this paper, we propose a service-agnostic online learning approach,
whereby the behavior of the system is learned on the fly, in order to predict
client-side SLA violations. The approach uses device-level metrics, which are
collected in a streaming fashion on the server side.
  Our results show that the approach can produce highly accurate predictions
(&gt;90% classification accuracy and &lt; 10% false alarm rate) in scenarios where
SLA violations are predicted for a video-on-demand service under changing load
patterns. The paper also highlight the limitations of traditional offline
learning methods, which perform significantly worse in many of the considered
scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01396</identifier>
 <datestamp>2015-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01396</id><created>2015-09-04</created><authors><author><keyname>Ferretti</keyname><forenames>Stefano</forenames></author><author><keyname>Ghini</keyname><forenames>Vittorio</forenames></author><author><keyname>Panzieri</keyname><forenames>Fabio</forenames></author></authors><title>A Survey on Handover Management in Mobility Architectures</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work presents a comprehensive and structured taxonomy of available
techniques for managing the handover process in mobility architectures.
Representative works from the existing literature have been divided into
appropriate categories, based on their ability to support horizontal handovers,
vertical handovers and multihoming. We describe approaches designed to work on
the current Internet (i.e. IPv4-based networks), as well as those that have
been devised for the &quot;future&quot; Internet (e.g. IPv6-based networks and
extensions). Quantitative measures and qualitative indicators are also
presented and used to evaluate and compare the examined approaches. This
critical review provides some valuable guidelines and suggestions for designing
and developing mobility architectures, including some practical expedients
(e.g. those required in the current Internet environment), aimed to cope with
the presence of NAT/firewalls and to provide support to legacy systems and
several communication protocols working at the application layer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01404</identifier>
 <datestamp>2015-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01404</id><created>2015-09-04</created><authors><author><keyname>Vandaele</keyname><forenames>Arnaud</forenames></author><author><keyname>Gillis</keyname><forenames>Nicolas</forenames></author><author><keyname>Lei</keyname><forenames>Qi</forenames></author><author><keyname>Zhong</keyname><forenames>Kai</forenames></author><author><keyname>Dhillon</keyname><forenames>Inderjit</forenames></author></authors><title>Coordinate Descent Methods for Symmetric Nonnegative Matrix
  Factorization</title><categories>cs.NA cs.CV cs.LG math.OC stat.ML</categories><comments>25 pages, 7 figures, 6 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a symmetric nonnegative matrix $A$, symmetric nonnegative matrix
factorization (symNMF) is the problem of finding a nonnegative matrix $H$,
usually with much fewer columns than $A$, such that $A \approx HH^T$. SymNMF
can be used for data analysis and in particular for various clustering tasks.
In this paper, we propose simple and very efficient coordinate descent schemes
to solve this problem, and that can handle large and sparse input matrices. The
effectiveness of our methods is illustrated on synthetic and real-world data
sets, and we show that they perform favorably compared to recent
state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01421</identifier>
 <datestamp>2015-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01421</id><created>2015-09-04</created><authors><author><keyname>Novara</keyname><forenames>Carlo</forenames></author></authors><title>Polynomial model inversion control: numerical tests and applications</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A novel control design approach for general nonlinear systems is described in
this paper. The approach is based on the identification of a polynomial model
of the system to control and on the on-line inversion of this model. Extensive
simulations are carried out to test the numerical efficiency of the approach.
Numerical examples of applicative interest are presented, concerned with
control of the Duffing oscillator, control of a robot manipulator and insulin
regulation in a type 1 diabetic patient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01423</identifier>
 <datestamp>2015-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01423</id><created>2015-09-04</created><authors><author><keyname>Ho</keyname><forenames>H. W.</forenames></author><author><keyname>De Wagter</keyname><forenames>C.</forenames></author><author><keyname>Remes</keyname><forenames>B. D. W.</forenames></author><author><keyname>de Croon</keyname><forenames>G. C. H. E.</forenames></author></authors><title>Optical-Flow based Self-Supervised Learning of Obstacle Appearance
  applied to MAV Landing</title><categories>cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Monocular optical flow has been widely used to detect obstacles in Micro Air
Vehicles (MAVs) during visual navigation. However, this approach requires
significant movement, which reduces the efficiency of navigation and may even
introduce risks in narrow spaces. In this paper, we introduce a novel setup of
self-supervised learning (SSL), in which optical flow cues serve as a scaffold
to learn the visual appearance of obstacles in the environment. We apply it to
a landing task, in which initially `surface roughness' is estimated from the
optical flow field in order to detect obstacles. Subsequently, a linear
regression function is learned that maps appearance features represented by
texton distributions to the roughness estimate. After learning, the MAV can
detect obstacles by just analyzing a still image. This allows the MAV to search
for a landing spot without moving. We first demonstrate this principle to work
with offline tests involving images captured from an on-board camera, and then
demonstrate the principle in flight. Although surface roughness is a property
of the entire flow field in the global image, the appearance learning even
allows for the pixel-wise segmentation of obstacles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01425</identifier>
 <datestamp>2015-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01425</id><created>2015-09-04</created><authors><author><keyname>Sun</keyname><forenames>Yan</forenames></author><author><keyname>Ng</keyname><forenames>Derrick Wing Kwan</forenames></author><author><keyname>Zhu</keyname><forenames>Jun</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author></authors><title>Multi-Objective Optimization for Robust Power Efficient and Secure
  Full-Duplex Wireless Communication Systems</title><categories>cs.IT math.IT</categories><comments>Submitted for possible journal publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the power efficient resource allocation
algorithm design for secure multiuser wireless communication systems employing
a full-duplex (FD) base station (BS) for serving multiple half-duplex (HD)
downlink (DL) and uplink (UL) users simultaneously. We propose a
multi-objective optimization framework to study two conflicting yet desirable
design objectives, i.e., total DL transmit power minimization and total UL
transmit power minimization. To this end, the weighed Tchebycheff method is
adopted to formulate the resource allocation algorithm design as a
multi-objective optimization problem (MOOP). The considered MOOP takes into
account the quality-of-service (QoS) requirements of all legitimate users for
guaranteeing secure DL and UL transmission in the presence of potential
eavesdroppers. Thereby, secure UL transmission is enabled by the FD BS and
would not be possible with an HD BS. The imperfectness of the channel state
information of the eavesdropping channels and the inter-user interference
channels is incorporated for robust resource allocation algorithm design.
Although the considered MOOP is non-convex, we solve it optimally by
semidefinite programming (SDP) relaxation. Simulation results not only unveil
the trade-off between the total DL transmit power and the total UL transmit
power, but also confirm the robustness of the proposed algorithm against
potential eavesdroppers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01469</identifier>
 <datestamp>2015-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01469</id><created>2015-09-04</created><authors><author><keyname>Guo</keyname><forenames>Ruiqi</forenames></author><author><keyname>Kumar</keyname><forenames>Sanjiv</forenames></author><author><keyname>Choromanski</keyname><forenames>Krzysztof</forenames></author><author><keyname>Simcha</keyname><forenames>David</forenames></author></authors><title>Quantization based Fast Inner Product Search</title><categories>cs.AI cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a quantization based approach for fast approximate Maximum Inner
Product Search (MIPS). Each database vector is quantized in multiple subspaces
via a set of codebooks, learned directly by minimizing the inner product
quantization error. Then, the inner product of a query to a database vector is
approximated as the sum of inner products with the subspace quantizers.
Different from recently proposed LSH approaches to MIPS, the database vectors
and queries do not need to be augmented in a higher dimensional feature space.
We also provide a theoretical analysis of the proposed approach, consisting of
the concentration results under mild assumptions. Furthermore, if a small
sample of example queries is given at the training time, we propose a modified
codebook learning procedure which further improves the accuracy. Experimental
results on a variety of datasets including those arising from deep neural
networks show that the proposed approach significantly outperforms the existing
state-of-the-art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01476</identifier>
 <datestamp>2015-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01476</id><created>2015-09-03</created><authors><author><keyname>Mariani</keyname><forenames>Manuel Sebastian</forenames></author><author><keyname>Medo</keyname><forenames>Matus</forenames></author><author><keyname>Zhang</keyname><forenames>Yi-Cheng</forenames></author></authors><title>Ranking nodes in growing networks: When PageRank fails</title><categories>physics.soc-ph cs.IR cs.SI</categories><comments>Article + Supplementary Information</comments><journal-ref>Scientific Reports 5, 16181 (2015)</journal-ref><doi>10.1038/srep16181</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  PageRank is arguably the most popular ranking algorithm which is being
applied in real systems ranging from information to biological and
infrastructure networks. Despite its outstanding popularity and broad use in
different areas of science, the relation between the algorithm's efficacy and
properties of the network on which it acts has not yet been fully understood.
We study here PageRank's performance on a network model supported by real data,
and show that realistic temporal effects make PageRank fail in individuating
the most valuable nodes for a broad range of model parameters. Results on real
data are in qualitative agreement with our model-based findings. This failure
of PageRank reveals that the static approach to information filtering is
inappropriate for a broad class of growing systems, and suggest that
time-dependent algorithms that are based on the temporal linking patterns of
these systems are needed to better rank the nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01477</identifier>
 <datestamp>2015-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01477</id><created>2015-09-04</created><authors><author><keyname>Medo</keyname><forenames>Matus</forenames></author><author><keyname>Mariani</keyname><forenames>Manuel S.</forenames></author><author><keyname>Zeng</keyname><forenames>An</forenames></author><author><keyname>Zhang</keyname><forenames>Yi-Cheng</forenames></author></authors><title>Identification and modeling of discoverers in online social systems</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>main paper (11 pages, 5 figures) together with supporting information
  (8 pages, 6 figures)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The dynamics of individuals is of essential importance for understanding the
evolution of social systems. Most existing models assume that individuals in
diverse systems, ranging from social networks to e-commerce, all tend to what
is already popular. We develop an analytical time-aware framework which shows
that when individuals make choices -- which item to buy, for example -- in
online social systems, a small fraction of them is consistently successful in
discovering popular items long before they actually become popular. We argue
that these users, whom we refer to as discoverers, are fundamentally different
from the previously known opinion leaders, influentials, and innovators. We use
the proposed framework to demonstrate that discoverers are present in a wide
range of systems. Once identified, they can be used to predict the future
success of items. We propose a network model which reproduces the discovery
patterns observed in the real data. Furthermore, data produced by the model
pose a fundamental challenge to classical ranking algorithms which neglect the
time of link creation and thus fail to discriminate between discoverers and
ordinary users in the data. Our results open the door to qualitative and
quantitative study of fine temporal patterns in social systems and have
far-reaching implications for network modeling and algorithm design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01481</identifier>
 <datestamp>2015-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01481</id><created>2015-09-04</created><authors><author><keyname>Pods</keyname><forenames>Jurgis</forenames></author></authors><title>A Comparison of Computational Models for the Extracellular Potential of
  Neurons</title><categories>q-bio.NC cs.CE cs.DC</categories><comments>12 pages (incl. references), 4 figures (color)</comments><msc-class>92C20</msc-class><acm-class>I.6.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The extracellular space has an ambiguous role in neuroscience. It is present
in every physiologically relevant system and often used as a measurement site
in experimental recordings, but it has received subordinate attention compared
to the intracellular domain. In computational modeling, it is often regarded as
a passive, homogeneous resistive medium with a constant conductivity, which
greatly simplifies the computation of extracellular potentials. However, recent
studies have shown that local ionic diffusion and capacitive effects of
electrically active membranes can have a substantial impact on the
extracellular potential. These effects can not be described by traditional
models, and they have been subject to theoretical and experimental analyses. We
strive to give an overview over recent progress in modeling the extracellular
space with special regard towards the concentration and potential dynamics on
different temporal and spatial scales. Three models with distinct assumptions
and levels of detail are compared both theoretically and by means of numerical
simulations: the classical volume conductor (VC) model, which is most
frequently used in form of the line source approximation (LSA); the very
detailed, but computationally intensive Poisson-Nernst-Planck model of
electrodiffusion (PNP); and an intermediate one called the electroneutral model
(EN). The results clearly show that there is no one model for all applications,
as they show significantly different responses especially close to neuronal
membranes. Finally, we list some common use cases for model simulations and
give recommendations on which model to use in each situation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01502</identifier>
 <datestamp>2015-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01502</id><created>2015-09-04</created><authors><author><keyname>Rodriguez</keyname><forenames>Nathaniel</forenames></author><author><keyname>Bollen</keyname><forenames>Johan</forenames></author><author><keyname>Ahn</keyname><forenames>Yong-Yeol</forenames></author></authors><title>Collective dynamics of belief evolution under cognitive coherence and
  social conformity</title><categories>physics.soc-ph cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Human history has been marked by social instability and conflict, often
driven by the irreconcilability of opposing sets of beliefs, ideologies, and
religious dogmas. The dynamics of belief systems has been studied mainly from
two distinct perspectives, namely how cognitive biases lead to individual
belief rigidity and how social influence leads to social conformity. Here we
propose a unifying framework that connects cognitive and social forces together
in order to study the dynamics of societal belief evolution. Each individual is
endowed with a network of interacting beliefs that evolves through interaction
with other individuals in a social network. The adoption of beliefs is affected
by both internal coherence and social conformity. Our framework explains how
social instabilities can arise in otherwise homogeneous populations, how small
numbers of zealots with highly coherent beliefs can overturn societal
consensus, and how belief rigidity protects fringe groups and cults against
invasion from mainstream beliefs, allowing them to persist and even thrive in
larger societies. Our results suggest that strong consensus may be insufficient
to guarantee social stability, that the cognitive coherence of belief-systems
is vital in determining their ability to spread, and that coherent
belief-systems may pose a serious problem for resolving social polarization,
due to their ability to prevent consensus even under high levels of social
exposure. We therefore argue that the inclusion of cognitive factors into a
social model is crucial in providing a more complete picture of collective
human dynamics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01504</identifier>
 <datestamp>2015-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01504</id><created>2015-09-04</created><authors><author><keyname>Khadir</keyname><forenames>Omar</forenames></author></authors><title>Insecure primitive elements in an ElGamal signature protocol</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider the classical ElGamal digital signature scheme based on the modular
relation $\alpha^m\equiv y^r\, r^s\ [p]$. In this work, we prove that if we can
compute a natural integer $i$ such that $\alpha^i\ mod\ p$ is smooth and
divides $p-1$, then it is possible to sign any given document without knowing
the secret key. Therefore we extend and reinforce Bleichenbacher's attack
presented at Eurocrypt'96.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01506</identifier>
 <datestamp>2015-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01506</id><created>2015-09-04</created><authors><author><keyname>Memeti</keyname><forenames>Suejb</forenames></author><author><keyname>Pllana</keyname><forenames>Sabri</forenames></author></authors><title>Analyzing large-scale DNA Sequences on Multi-core Architectures</title><categories>cs.DC</categories><comments>The 18th IEEE International Conference on Computational Science and
  Engineering (CSE 2015), Porto, Portugal, 20 - 23 October 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rapid analysis of DNA sequences is important in preventing the evolution of
different viruses and bacteria during an early phase, early diagnosis of
genetic predispositions to certain diseases (cancer, cardiovascular diseases),
and in DNA forensics. However, real-world DNA sequences may comprise several
Gigabytes and the process of DNA analysis demands adequate computational
resources to be completed within a reasonable time. In this paper we present a
scalable approach for parallel DNA analysis that is based on Finite Automata,
and which is suitable for analyzing very large DNA segments. We evaluate our
approach for real-world DNA segments of mouse (2.7GB), cat (2.4GB), dog
(2.4GB), chicken (1GB), human (3.2GB) and turkey (0.2GB). Experimental results
on a dual-socket shared-memory system with 24 physical cores show speed-ups of
up to 17.6x. Our approach is up to 3x faster than a pattern-based parallel
approach that uses the RE2 library.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01509</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01509</id><created>2015-09-04</created><updated>2016-01-25</updated><authors><author><keyname>Gebru</keyname><forenames>Israel D.</forenames></author><author><keyname>Alameda-Pineda</keyname><forenames>Xavier</forenames></author><author><keyname>Forbes</keyname><forenames>Florence</forenames></author><author><keyname>Horaud</keyname><forenames>Radu</forenames></author></authors><title>EM Algorithms for Weighted-Data Clustering with Application to
  Audio-Visual Scene Analysis</title><categories>cs.CV cs.LG stat.ML</categories><comments>14 pages, 4 figures, 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data clustering has received a lot of attention and numerous methods,
algorithms and software packages are available. Among these techniques,
parametric finite-mixture models play a central role due to their interesting
mathematical properties and to the existence of maximum-likelihood estimators
based on expectation-maximization (EM). In this paper we propose a new mixture
model that associates a weight with each observed point. We introduce the
weighted-data Gaussian mixture and we derive two EM algorithms. The first one
considers a fixed weight for each observation. The second one treats each
weight as a random variable following a gamma distribution. We propose a model
selection method based on a minimum message length criterion, provide a weight
initialization strategy, and validate the proposed algorithms by comparing them
with several state of the art parametric and non-parametric clustering
techniques. We also demonstrate the effectiveness and robustness of the
proposed clustering technique in the presence of heterogeneous data, namely
audio-visual scene analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01514</identifier>
 <datestamp>2015-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01514</id><created>2015-09-04</created><authors><author><keyname>Knyazev</keyname><forenames>Andrew</forenames></author><author><keyname>Malyshev</keyname><forenames>Alexander</forenames></author></authors><title>Conjugate Gradient Acceleration of Non-Linear Smoothing Filters</title><categories>cs.CV</categories><comments>5 pages, 5 figures, IEEE Conference GlobalSIP 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The most efficient signal edge-preserving smoothing filters, e.g., for
denoising, are non-linear. Thus, their acceleration is challenging and is often
performed in practice by tuning filter parameters, such as by increasing the
width of the local smoothing neighborhood, resulting in more aggressive
smoothing of a single sweep at the cost of increased edge blurring. We propose
an alternative technology, accelerating the original filters without tuning, by
running them through a special conjugate gradient method, not affecting their
quality. The filter non-linearity is dealt with by careful freezing and
restarting. Our initial numerical experiments on toy one-dimensional signals
demonstrate 20x acceleration of the classical bilateral filter and 3-5x
acceleration of the recently developed guided filter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01520</identifier>
 <datestamp>2015-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01520</id><created>2015-09-04</created><authors><author><keyname>Ba</keyname><forenames>Sileye</forenames></author><author><keyname>Alameda-Pineda</keyname><forenames>Xavier</forenames></author><author><keyname>Xompero</keyname><forenames>Alessio</forenames></author><author><keyname>Horaud</keyname><forenames>Radu</forenames></author></authors><title>An On-line Variational Bayesian Model for Multi-Person Tracking from
  Cluttered Scenes</title><categories>cs.CV stat.ML</categories><comments>17 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Object tracking is an ubiquitous problem that appears in many applications
such as remote sensing, audio processing, computer vision, human-machine
interfaces, human-robot interaction, etc. Although thorougly investigated in
computer vision, tracking a time-varying number of persons remains a
challenging open problem. In this paper, we propose an on-line variational
Bayesian model for multi-person tracking from cluttered visual observations
provided by person detectors. The paper has the following contributions: A
Bayesian framework for tracking an unknown and varying number of persons, a
variational expectation-maximization (VEM) algorithm with closed-form
expressions for the posterior distributions and model parameter estimation, A
method capable of exploiting observations from multiple detectors, thus
enabling multimodal fusion, and built-in object-birth and object-visibility
processes that allow to handle person appearance and disappearance. The method
is evaluated on standard datasets, which shows competitive and encouraging
results with respect to state of the art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01531</identifier>
 <datestamp>2015-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01531</id><created>2015-09-04</created><authors><author><keyname>Mathias</keyname><forenames>Joel</forenames></author><author><keyname>Kaddah</keyname><forenames>Rim</forenames></author><author><keyname>Bu&#x161;i&#x107;</keyname><forenames>Ana</forenames></author><author><keyname>Meyn</keyname><forenames>Sean</forenames></author></authors><title>Smart Fridge / Dumb Grid? Demand Dispatch for the Power Grid of 2020</title><categories>cs.SY math.OC</categories><msc-class>93E20, 47N70</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In discussions at the 2015 HICSS meeting, it was argued that loads can
provide most of the ancillary services required today and in the future.
Through load-level and grid-level control design, high-quality ancillary
service for the grid is obtained without impacting quality of service delivered
to the consumer. This approach to grid regulation is called demand dispatch:
loads are providing service continuously and automatically, without consumer
interference.
  In this paper we ask, what intelligence is required at the grid-level? In
particular, does the grid-operator require more than one-way communication to
the loads? Our main conclusion: risk is not great in lower frequency ranges,
e.g., PJM's RegA or BPA's balancing reserves. In particular, ancillary services
from refrigerators and pool-pumps can be obtained successfully with only
one-way communication. This requires intelligence at the loads, and much less
intelligence at the grid level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01546</identifier>
 <datestamp>2015-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01546</id><created>2015-09-04</created><authors><author><keyname>Hofmeyr</keyname><forenames>David P.</forenames></author><author><keyname>Pavlidis</keyname><forenames>Nicos G.</forenames></author><author><keyname>Eckley</keyname><forenames>Idris A.</forenames></author></authors><title>Minimum Spectral Connectivity Projection Pursuit for Unsupervised
  Classification</title><categories>stat.ML cs.LG</categories><msc-class>62H30</msc-class><acm-class>I.5.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of determining the optimal univariate subspace for
maximising the separability of a binary partition of unlabeled data, as
measured by spectral graph theory. This is achieved by ?nding projections which
minimise the second eigenvalue of the Laplacian matrices of the projected data,
which corresponds to a non-convex, non-smooth optimisation problem. We show
that the optimal projection based on spectral connectivity converges to the
vector normal to the maximum margin hyperplane through the data, as the scaling
parameter is reduced to zero. This establishes a connection between
connectivity as measured by spectral graph theory and maximal Euclidean
separation. It also allows us to apply our methodology to the problem of ?nding
large margin linear separators. The computational cost associated with each
eigen-problem is quadratic in the number of data. To mitigate this problem, we
propose an approximation method using microclusters with provable approximation
error bounds. We evaluate the performance of the proposed method on simulated
and publicly available data sets and ?nd that it compares favourably with
existing methods for projection pursuit and dimension reduction for
unsupervised data partitioning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01549</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01549</id><created>2015-09-04</created><updated>2015-09-14</updated><authors><author><keyname>Lai</keyname><forenames>Matthew</forenames></author></authors><title>Giraffe: Using Deep Reinforcement Learning to Play Chess</title><categories>cs.AI cs.LG cs.NE</categories><comments>MSc Dissertation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This report presents Giraffe, a chess engine that uses self-play to discover
all its domain-specific knowledge, with minimal hand-crafted knowledge given by
the programmer. Unlike previous attempts using machine learning only to perform
parameter-tuning on hand-crafted evaluation functions, Giraffe's learning
system also performs automatic feature extraction and pattern recognition. The
trained evaluation function performs comparably to the evaluation functions of
state-of-the-art chess engines - all of which containing thousands of lines of
carefully hand-crafted pattern recognizers, tuned over many years by both
computer chess experts and human chess masters. Giraffe is the most successful
attempt thus far at using end-to-end machine learning to play chess.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01552</identifier>
 <datestamp>2015-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01552</id><created>2015-09-04</created><authors><author><keyname>Ponikwar</keyname><forenames>Christoph</forenames></author><author><keyname>Hof</keyname><forenames>Hans-Joachim</forenames></author></authors><title>Overview on Security Approaches in Intelligent Transportation Systems</title><categories>cs.CR cs.NI</categories><comments>The Ninth International Conference on Emerging Security Information,
  Systems and Technologies - SECURWARE 2015, Venice, Italy, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Major standardization bodies developed and designed systems that should be
used in vehicular ad-hoc networks. The Institute of Electrical and Electronics
Engineers (IEEE) in America designed the wireless access in vehicular
environments (WAVE) system. The European Telecommunications Standards Institute
(ETSI) did come up with the &quot;ITS-G5&quot; system. Those Vehicular Ad-hoc Networks
(VANETs) are the basis for Intelligent Transportation Systems (ITSs). They aim
to efficiently communicate and provide benefits to people, ranging from
improved safety to convenience. But different design and architectural choices
lead to different network properties, especially security properties that are
fundamentally depending on the networks architecture. To be able to compare
different security architectures, different proposed approaches need to be
discussed. One problem in current research is the missing focus on different
approaches for trust establishment in VANETs. Therefore, this paper surveys
different security issues and solutions in VANETs and we furthermore categorize
these solutions into three basic trust defining architectures: centralized,
decentralized and hybrid. These categories represent how trust is build in a
system, i.e., in a centralized, decentralized way or even by combining both
opposing approaches to a hybrid solution, which aims to inherit the benefits of
both worlds. This survey defines those categories and finds that hybrid
approaches are underrepresented in current research efforts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01553</identifier>
 <datestamp>2015-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01553</id><created>2015-09-04</created><authors><author><keyname>Vilisov</keyname><forenames>Valery</forenames></author></authors><title>Research of the Robot's Learning Effectiveness in the Changing
  Environment</title><categories>cs.RO</categories><comments>International Scientific and Technological Conference Extreme
  Robotics, October 1-2, 2014, Saint-Petersburg, Russia</comments><doi>10.13140/RG.2.1.3343.7926</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The object of the research is the adaptive algorithms that are used by the
operator when educating the robotic systems. Operator, being the target-setting
subject, is interested in the goal that robotic systems, being the conductor of
his targets (criteria), would provide a maximum effectiveness of these targets'
(criteria's) achievement. Thus, the adaptive algorithms provide the adequate
reflection of the operator's goals, found in the robotic systems' actions. This
work considers potential possibilities of such target adaption of the robotic
systems used for the class of the allocation problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01569</identifier>
 <datestamp>2015-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01569</id><created>2015-09-04</created><authors><author><keyname>Vilisov</keyname><forenames>Valery</forenames></author></authors><title>Learning Mobile Robot Based on Adaptive Controlled Markov Chains</title><categories>cs.RO</categories><comments>International Scientific and Technological Conference Extreme
  Robotics, October 8-9, 2015, Saint-Petersburg, Russia</comments><doi>10.13140/RG.2.1.4462.0001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Herein we suggest a mobile robot-training algorithm that is based on the
preference approximation of the decision taker who controls the robot, which in
its turn is managed by the Markov chain. Setup of the model parameters is made
on the basis of the data referring to the situations and decisions involving
the decision taker. The model that adapts to the decision taker's preferences
can be set up either a priori, during the process of the robot's normal
operation, or during specially planned testing sessions. Basing on the
simulation modelling data of the robot's operation process and on the decision
taker's robot control we have set up the model parameters thus illustrating
both working capacity of all algorithm components and adaptation effectiveness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01572</identifier>
 <datestamp>2015-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01572</id><created>2015-09-04</created><authors><author><keyname>Kreienbuehl</keyname><forenames>Andreas</forenames></author><author><keyname>Benedusi</keyname><forenames>Pietro</forenames></author><author><keyname>Ruprecht</keyname><forenames>Daniel</forenames></author><author><keyname>Krause</keyname><forenames>Rolf</forenames></author></authors><title>Time parallel gravitational collapse simulation</title><categories>gr-qc cs.CE cs.DC cs.PF</categories><comments>17 pages, 9 figures, 1 listing, and 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article demonstrates the applicability of the parallel-in-time method
Parareal to the numerical solution of the Einstein gravity equations for the
spherical collapse of a massless scalar field. To account for the shrinking of
the spatial domain in time, a tailored load balancing scheme is proposed and
compared to load balancing based on number of time steps alone. The performance
of Parareal is studied for both the sub-critical and black hole case; our
experiments show that Parareal generates substantial speedup and, in the
super-critical regime, can also reproduce the black hole mass scaling law.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01576</identifier>
 <datestamp>2015-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01576</id><created>2015-09-04</created><authors><author><keyname>Renault</keyname><forenames>Gabriel</forenames></author></authors><title>Invertibility modulo dead-ending no-P-universes</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In normal version of combinatorial game theory, all games are invertible,
whereas only the empty game is invertible in mis\`ere version. For this reason,
several restricted universes were earlier considered for their study, in which
more games are invertible. We here study combinatorial games in mis\`ere
version, in particular universes where no player would like to pass their turn
In these universes, we prove that having one extra condition makes all games
become invertible. We then focus our attention on a specific quotient, called
Q_Z, and show that all sums of universes whose quotient is Q_Z also have Q_Z as
their quotient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01596</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01596</id><created>2015-08-26</created><authors><author><keyname>Khalili</keyname><forenames>Shahrouz</forenames></author><author><keyname>Simeone</keyname><forenames>Osvaldo</forenames></author></authors><title>Inter-Layer Per-Mobile Optimization of Cloud Mobile Computing: A
  Message-Passing Approach</title><categories>cs.DC cs.NI</categories><comments>submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud mobile computing enables the offloading of computation-intensive
applications from a mobile device to a cloud processor via a wireless
interface. In light of the strong interplay between offloading decisions at the
application layer and physical-layer parameters, which determine the energy and
latency associated with the mobile-cloud communication, this paper investigates
the inter-layer optimization of fine-grained task offloading across both
layers. In prior art, this problem was formulated, under a serial
implementation of processing and communication, as a mixed integer program,
entailing a complexity that is exponential in the number of tasks. In this
work, instead, algorithmic solutions are proposed that leverage the structure
of the call graphs of typical applications by means of message passing on the
call graph, under both serial and parallel implementations of processing and
communication. For call trees, the proposed solutions have a linear complexity
in the number of tasks, and efficient extensions are presented for more general
call graphs that include &quot;map&quot; and &quot;reduce&quot;-type tasks. Moreover, the proposed
schemes are optimal for the serial implementation, and provide principled
heuristics for the parallel implementation. Extensive numerical results yield
insights into the impact of inter-layer optimization and on the comparison of
the two implementations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01599</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01599</id><created>2015-09-04</created><updated>2015-09-11</updated><authors><author><keyname>Bhatia</keyname><forenames>Parminder</forenames></author><author><keyname>Ji</keyname><forenames>Yangfeng</forenames></author><author><keyname>Eisenstein</keyname><forenames>Jacob</forenames></author></authors><title>Better Document-level Sentiment Analysis from RST Discourse Parsing</title><categories>cs.CL cs.AI</categories><comments>Published at Empirical Methods in Natural Language Processing (EMNLP
  2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Discourse structure is the hidden link between surface features and
document-level properties, such as sentiment polarity. We show that the
discourse analyses produced by Rhetorical Structure Theory (RST) parsers can
improve document-level sentiment analysis, via composition of local information
up the discourse tree. First, we show that reweighting discourse units
according to their position in a dependency representation of the rhetorical
structure can yield substantial improvements on lexicon-based sentiment
analysis. Next, we present a recursive neural network over the RST structure,
which offers significant improvements over classification-based methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01600</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01600</id><created>2015-09-04</created><updated>2015-09-24</updated><authors><author><keyname>Razavi</keyname><forenames>Alireza</forenames></author><author><keyname>Valkama</keyname><forenames>Mikko</forenames></author><author><keyname>Lohan</keyname><forenames>Elena-Simona</forenames></author></authors><title>K-Means Fingerprint Clustering for Low-Complexity Floor Estimation in
  Indoor Mobile Localization</title><categories>cs.IT math.IT</categories><comments>Accepted to IEEE Globecom 2015, Workshop on Localization and
  Tracking: Indoors, Outdoors and Emerging Networks</comments><doi>10.1109/GLOCOMW.2015.7414026</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Indoor localization in multi-floor buildings is an important research
problem. Finding the correct floor, in a fast and efficient manner, in a
shopping mall or an unknown university building can save the users' search time
and can enable a myriad of Location Based Services in the future. One of the
most widely spread techniques for floor estimation in multi-floor buildings is
the fingerprinting-based localization using Received Signal Strength (RSS)
measurements coming from indoor networks, such as WLAN and BLE. The clear
advantage of RSS-based floor estimation is its ease of implementation on a
multitude of mobile devices at the Application Programming Interface (API)
level, because RSS values are directly accessible through API interface.
However, the downside of a fingerprinting approach, especially for large-scale
floor estimation and positioning solutions, is their need to store and transmit
a huge amount of fingerprinting data. The problem becomes more severe when the
localization is intended to be done on mobile devices which have limited
memory, power, and computational resources. An alternative floor estimation
method, which has lower complexity and is faster than the fingerprinting is the
Weighted Centroid Localization (WCL) method. The trade-off is however paid in
terms of a lower accuracy than the one obtained with traditional fingerprinting
with Nearest Neighbour (NN) estimates. In this paper a novel K-means-based
method for floor estimation via fingerprint clustering of WiFi and various
other positioning sensor outputs is introduced. Our method achieves a floor
estimation accuracy close to the one with NN fingerprinting, while
significantly improves the complexity and the speed of the floor detection
algorithm. The decrease in the database size is achieved through storing and
transmitting only the cluster heads (CH's) and their corresponding floor
labels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01602</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01602</id><created>2015-09-04</created><authors><author><keyname>Bogun</keyname><forenames>Ivan</forenames></author><author><keyname>Angelova</keyname><forenames>Anelia</forenames></author><author><keyname>Jaitly</keyname><forenames>Navdeep</forenames></author></authors><title>Object Recognition from Short Videos for Robotic Perception</title><categories>cs.CV</categories><comments>7 pages, 6 figures, 3 tables</comments><acm-class>I.5.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Deep neural networks have become the primary learning technique for object
recognition. Videos, unlike still images, are temporally coherent which makes
the application of deep networks non-trivial. Here, we investigate how motion
can aid object recognition in short videos. Our approach is based on Long
Short-Term Memory (LSTM) deep networks. Unlike previous applications of LSTMs,
we implement each gate as a convolution. We show that convolutional-based LSTM
models are capable of learning motion dependencies and are able to improve the
recognition accuracy when more frames in a sequence are available. We evaluate
our approach on the Washington RGBD Object dataset and on the Washington RGBD
Scenes dataset. Our approach outperforms deep nets applied to still images and
sets a new state-of-the-art in this domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01608</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01608</id><created>2015-09-04</created><authors><author><keyname>Agreste</keyname><forenames>Santa</forenames></author><author><keyname>Catanese</keyname><forenames>Salvatore</forenames></author><author><keyname>De Meo</keyname><forenames>Pasquale</forenames></author><author><keyname>Ferrara</keyname><forenames>Emilio</forenames></author><author><keyname>Fiumara</keyname><forenames>Giacomo</forenames></author></authors><title>Network Structure and Resilience of Mafia Syndicates</title><categories>cs.SI cs.CY physics.soc-ph</categories><comments>22 pages, 10 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present the results of the study of Sicilian Mafia
organization by using Social Network Analysis. The study investigates the
network structure of a Mafia organization, describing its evolution and
highlighting its plasticity to interventions targeting membership and its
resilience to disruption caused by police operations. We analyze two different
datasets about Mafia gangs built by examining different digital trails and
judicial documents spanning a period of ten years: the former dataset includes
the phone contacts among suspected individuals, the latter is constituted by
the relationships among individuals actively involved in various criminal
offenses. Our report illustrates the limits of traditional investigation
methods like tapping: criminals high up in the organization hierarchy do not
occupy the most central positions in the criminal network, and oftentimes do
not appear in the reconstructed criminal network at all. However, we also
suggest possible strategies of intervention, as we show that although criminal
networks (i.e., the network encoding mobsters and crime relationships) are
extremely resilient to different kind of attacks, contact networks (i.e., the
network reporting suspects and reciprocated phone calls) are much more
vulnerable and their analysis can yield extremely valuable insights.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01618</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01618</id><created>2015-09-04</created><authors><author><keyname>Li</keyname><forenames>Chengtao</forenames></author><author><keyname>Jegelka</keyname><forenames>Stefanie</forenames></author><author><keyname>Sra</keyname><forenames>Suvrit</forenames></author></authors><title>Efficient Sampling for k-Determinantal Point Processes</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Determinantal Point Processes (DPPs) provide probabilistic models over
discrete sets of items that help model repulsion and diversity. Applicability
of DPPs to large sets of data is, however, hindered by the expensive matrix
operations involved, especially when sampling. We therefore propose a new
efficient approximate two-stage sampling algorithm for discrete k-DPPs. As
opposed to previous approximations, our algorithm aims at minimizing the
variational distance to the original distribution. Experiments indicate that
the resulting sampling algorithm works well on large data and yields more
accurate samples than previous approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01624</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01624</id><created>2015-09-04</created><authors><author><keyname>Tian</keyname><forenames>Dong</forenames></author><author><keyname>Mansour</keyname><forenames>Hassan</forenames></author><author><keyname>Knyazev</keyname><forenames>Andrew</forenames></author><author><keyname>Vetro</keyname><forenames>Anthony</forenames></author></authors><title>Chebyshev and Conjugate Gradient Filters for Graph Image Denoising</title><categories>cs.CV</categories><comments>6 pages, 6 figures, accepted to 2014 IEEE International Conference on
  Multimedia and Expo Workshops (ICMEW)</comments><report-no>MERL TR2014-062</report-no><journal-ref>Multimedia and Expo Workshops (ICMEW), 2014 IEEE International
  Conference on, vol., no., pp.1-6, 14-18 July 2014</journal-ref><doi>10.1109/ICMEW.2014.6890711</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In 3D image/video acquisition, different views are often captured with
varying noise levels across the views. In this paper, we propose a graph-based
image enhancement technique that uses a higher quality view to enhance a
degraded view. A depth map is utilized as auxiliary information to match the
perspectives of the two views. Our method performs graph-based filtering of the
noisy image by directly computing a projection of the image to be filtered onto
a lower dimensional Krylov subspace of the graph Laplacian. We discuss two
graph spectral denoising methods: first using Chebyshev polynomials, and second
using iterations of the conjugate gradient algorithm. Our framework generalizes
previously known polynomial graph filters, and we demonstrate through numerical
simulations that our proposed technique produces subjectively cleaner images
with about 1-3 dB improvement in PSNR over existing polynomial graph filters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01626</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01626</id><created>2015-09-04</created><updated>2015-09-10</updated><authors><author><keyname>Zhang</keyname><forenames>Xiang</forenames></author><author><keyname>Zhao</keyname><forenames>Junbo</forenames></author><author><keyname>LeCun</keyname><forenames>Yann</forenames></author></authors><title>Character-level Convolutional Networks for Text Classification</title><categories>cs.LG cs.CL</categories><comments>An early version of this work entitled &quot;Text Understanding from
  Scratch&quot; was posted in Feb 2015 as arXiv:1502.01710. The present paper has
  considerably more experimental results and a rewritten introduction. Advances
  in Neural Information Processing Systems 28 (NIPS 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article offers an empirical exploration on the use of character-level
convolutional networks (ConvNets) for text classification. We constructed
several large-scale datasets to show that character-level convolutional
networks could achieve state-of-the-art or competitive results. Comparisons are
offered against traditional models such as bag of words, n-grams and their
TFIDF variants, and deep learning models such as word-based ConvNets and
recurrent neural networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01630</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01630</id><created>2015-09-04</created><authors><author><keyname>Mordechai</keyname><forenames>Yael</forenames></author></authors><title>Optimization and Reoptimization in Scheduling Problems</title><categories>cs.DS</categories><comments>Thesis work. arXiv admin note: text overlap with arXiv:1311.4021 by
  other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Parallel machine scheduling has been extensively studied in the past decades,
with applications ranging from production planning to job processing in large
computing clusters. In this work we study some of these fundamental
optimization problems, as well as their parameterized and reoptimization
variants.
  We first present improved bounds for job scheduling on unrelated parallel
machines, with the objective of minimizing the latest completion time
(makespan) of the schedule. We consider the subclass of fully-feasible
instances, in which the processing time of each job, on any machine, does not
exceed the minimum makespan. The problem is known to be hard to approximate
within factor 4/3 already in this subclass. Although fully-feasible instances
are hard to identify, we give a polynomial time algorithm that yields for such
instances a schedule whose makespan is better than twice the optimal, the best
known ratio for general instances. Moreover, we show that our result is robust
under small violations of feasibility constraints.
  We further study the power of parameterization. We show that makespan
minimization on unrelated machines admits a parameterized approximation scheme,
where the parameter used is the number of processing times that are large
relative to the latest completion time of the schedule. We also present an FPT
algorithm for the graph-balancing problem, which corresponds to the instances
of the restricted assignment problem where each job can be processed on at most
2 machines.
  Finally, motivated by practical scenarios, we initiate the study of
reoptimization in job scheduling on identical and uniform machines, with the
objective of minimizing the makespan. We develop reapproximation algorithms
that yield in both models the best possible approximation ratio of
$(1+\epsilon)$, for any $\epsilon &gt;0$, with respect to the minimum makespan.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01644</identifier>
 <datestamp>2015-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01644</id><created>2015-09-04</created><updated>2015-11-26</updated><authors><author><keyname>Masson</keyname><forenames>Warwick</forenames></author><author><keyname>Ranchod</keyname><forenames>Pravesh</forenames></author><author><keyname>Konidaris</keyname><forenames>George</forenames></author></authors><title>Reinforcement Learning with Parameterized Actions</title><categories>cs.AI cs.LG</categories><comments>Accepted for AAAI 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a model-free algorithm for learning in Markov decision processes
with parameterized actions-discrete actions with continuous parameters. At each
step the agent must select both which action to use and which parameters to use
with that action. We introduce the Q-PAMDP algorithm for learning in these
domains, show that it converges to a local optimum, and compare it to direct
policy search in the goal-scoring and Platform domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01649</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01649</id><created>2015-09-04</created><authors><author><keyname>Garnaga</keyname><forenames>Valerii</forenames></author></authors><title>Using of Neuro-Indexes</title><categories>cs.IR</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  The article describes a new data structure called neuro-index. It is an
alternative to well-known file indexes. The neuro-index is fundamentally
different because it stores weight coefficients in neural network. It is not a
reference type like &quot;keyword-position in a file&quot;.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01653</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01653</id><created>2015-09-04</created><authors><author><keyname>Khan</keyname><forenames>Talha Ahmed</forenames></author><author><keyname>Alkhateeb</keyname><forenames>Ahmed</forenames></author><author><keyname>Heath</keyname><forenames>Robert W.</forenames><suffix>Jr</suffix></author></authors><title>Millimeter Wave Energy Harvesting</title><categories>cs.IT math.IT</categories><comments>30 pages, 10 figures, submitted to IEEE Transactions on Wireless
  Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The millimeter wave (mmWave) band, which is a prime candidate for 5G cellular
networks, seems attractive for wireless energy harvesting. This is because it
will feature large antenna arrays as well as extremely dense basestation
deployments. The viability of mmWave for energy harvesting though is unclear,
due to the differences in propagation characteristics such as extreme
sensitivity to building blockages. This paper considers a scenario where
low-power devices extract energy and/or information from the incident mmWave
signals. Leveraging tools from stochastic geometry, closed-form expressions are
derived to characterize the energy coverage probability, the average harvested
power, and the overall (energy-and-information) coverage probability at a
typical wireless-powered device in terms of important parameters, such as the
cellular network density, the antenna geometry parameters, and the channel
parameters. Numerical results reveal several network and device level design
insights. For example, at the basestations, the antenna geometry parameters
such as beamwidth can be optimized to maximize the network-wide energy coverage
for a given user population. At the device level, the overall performance can
be substantially improved by optimally splitting the received signal for energy
and information extraction, and by deploying multi-antenna arrays. For the
latter, an efficient low-power multi-antenna mmWave receiver architecture is
proposed for simultaneous energy and information transfer. Overall, simulation
results suggest that mmWave energy harvesting generally provides a substantial
performance gain over lower frequency solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01654</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01654</id><created>2015-09-04</created><authors><author><keyname>Lin</keyname><forenames>Yuewei</forenames></author><author><keyname>Ezzeldeen</keyname><forenames>Kareem</forenames></author><author><keyname>Zhou</keyname><forenames>Youjie</forenames></author><author><keyname>Fan</keyname><forenames>Xiaochuan</forenames></author><author><keyname>Yu</keyname><forenames>Hongkai</forenames></author><author><keyname>Qian</keyname><forenames>Hui</forenames></author><author><keyname>Wang</keyname><forenames>Song</forenames></author></authors><title>Co-interest Person Detection from Multiple Wearable Camera Videos</title><categories>cs.CV</categories><comments>ICCV 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wearable cameras, such as Google Glass and Go Pro, enable video data
collection over larger areas and from different views. In this paper, we tackle
a new problem of locating the co-interest person (CIP), i.e., the one who draws
attention from most camera wearers, from temporally synchronized videos taken
by multiple wearable cameras. Our basic idea is to exploit the motion patterns
of people and use them to correlate the persons across different videos,
instead of performing appearance-based matching as in traditional video
co-segmentation/localization. This way, we can identify CIP even if a group of
people with similar appearance are present in the view. More specifically, we
detect a set of persons on each frame as the candidates of the CIP and then
build a Conditional Random Field (CRF) model to select the one with consistent
motion patterns in different videos and high spacial-temporal consistency in
each video. We collect three sets of wearable-camera videos for testing the
proposed algorithm. All the involved people have similar appearances in the
collected videos and the experiments demonstrate the effectiveness of the
proposed algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01655</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01655</id><created>2015-09-04</created><authors><author><keyname>Mozaffari</keyname><forenames>Mohammad</forenames></author><author><keyname>Saad</keyname><forenames>Walid</forenames></author><author><keyname>Bennis</keyname><forenames>Mehdi</forenames></author><author><keyname>Debbah</keyname><forenames>Merouane</forenames></author></authors><title>Drone Small Cells in the Clouds: Design, Deployment and Performance
  Analysis</title><categories>cs.IT cs.NI math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The use of drone small cells (DSCs) which are aerial wireless base stations
that can be mounted on flying devices such as unmanned aerial vehicles (UAVs),
is emerging as an effective technique for providing wireless services to ground
users in a variety of scenarios. The efficient deployment of such DSCs while
optimizing the covered area is one of the key design challenges. In this paper,
considering the low altitude platform (LAP), the downlink coverage performance
of DSCs is investigated. The optimal DSC altitude which leads to a maximum
ground coverage and minimum required transmit power for a single DSC is
derived. Furthermore, the problem of providing a maximum coverage for a certain
geographical area using two DSCs is investigated in two scenarios; interference
free and full interference between DSCs. The impact of the distance between
DSCs on the coverage area is studied and the optimal distance between DSCs
resulting in maximum coverage is derived. Numerical results verify our
analytical results on the existence of optimal DSCs altitude/separation
distance and provide insights on the optimal deployment of DSCs to supplement
wireless network coverage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01659</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01659</id><created>2015-09-04</created><authors><author><keyname>Aghajanyan</keyname><forenames>Armen</forenames></author></authors><title>Gravitational Clustering</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The downfall of many supervised learning algorithms, such as neural networks,
is the inherent need for a large amount of training data. Although there is a
lot of buzz about big data, there is still the problem of doing classification
from a small dataset. Other methods such as support vector machines, although
capable of dealing with few samples, are inherently binary classifiers, and are
in need of learning strategies such as One vs All in the case of
multi-classification. In the presence of a large number of classes this can
become problematic. In this paper we present, a novel approach to supervised
learning through the method of clustering. Unlike traditional methods such as
K-Means, Gravitational Clustering does not require the initial number of
clusters, and automatically builds the clusters, individual samples can be
arbitrarily weighted and it requires only few samples while staying resilient
to over-fitting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01660</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01660</id><created>2015-09-04</created><authors><author><keyname>Peng</keyname><forenames>Yu</forenames></author><author><keyname>Wang</keyname><forenames>Shuling</forenames></author><author><keyname>Zhan</keyname><forenames>Naijun</forenames></author><author><keyname>Zhang</keyname><forenames>Lijun</forenames></author></authors><title>Extending Hybrid CSP with Probability and Stochasticity</title><categories>cs.LO cs.SY</categories><comments>The conference version of this paper is accepted by SETTA 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Probabilistic and stochastic behavior are omnipresent in computer controlled
systems, in particular, so-called safety-critical hybrid systems, because of
fundamental properties of nature, uncertain environments, or simplifications to
overcome complexity. Tightly intertwining discrete, continuous and stochastic
dynamics complicates modelling, analysis and verification of stochastic hybrid
systems (SHSs). In the literature, this issue has been extensively
investigated, but unfortunately it still remains challenging as no promising
general solutions are available yet. In this paper, we give our effort by
proposing a general compositional approach for modelling and verification of
SHSs. First, we extend Hybrid CSP (HCSP), a very expressive and process
algebra-like formal modeling language for hybrid systems, by introducing
probability and stochasticity to model SHSs, which is called stochastic HCSP
(SHCSP). To this end, ordinary differential equations (ODEs) are generalized by
stochastic differential equations (SDEs) and non-deterministic choice is
replaced by probabilistic choice. Then, we extend Hybrid Hoare Logic (HHL) to
specify and reason about SHCSP processes. We demonstrate our approach by an
example from real-world.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01662</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01662</id><created>2015-09-05</created><authors><author><keyname>Pilson</keyname><forenames>Christopher S.</forenames></author></authors><title>Tightly-Held and Ephemeral Psychometrics: Password and Passphrase
  Authentication Utilizing User-Supplied Constructs of Self</title><categories>cs.HC</categories><comments>17 pages</comments><acm-class>H.1.1; H.1.2; K.6.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This research investigates the role of passwords and passphrases as valid
authentication methodologies. Specifically, this research dispels earlier work
that ignores information-theoretic lessons learned from cognitive and social
psychology and psycholinguistics, and extends and enriches the current password
security model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01675</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01675</id><created>2015-09-05</created><authors><author><keyname>Bonamy</keyname><forenames>Marthe</forenames></author><author><keyname>Kowalik</keyname><forenames>&#x141;ukasz</forenames></author><author><keyname>Pilipczuk</keyname><forenames>Micha&#x142;</forenames></author><author><keyname>Soca&#x142;a</keyname><forenames>Arkadiusz</forenames></author></authors><title>Linear kernels for outbranching problems in sparse digraphs</title><categories>cs.DS</categories><comments>Extended abstract accepted for IPEC'15, 27 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the $k$-Leaf Out-Branching and $k$-Internal Out-Branching problems we are
given a directed graph $D$ with a designated root $r$ and a nonnegative integer
$k$. The question is to determine the existence of an outbranching rooted at
$r$ that has at least $k$ leaves, or at least $k$ internal vertices,
respectively. Both these problems were intensively studied from the points of
view of parameterized complexity and kernelization, and in particular for both
of them kernels with $O(k^2)$ vertices are known on general graphs. In this
work we show that $k$-Leaf Out-Branching admits a kernel with $O(k)$ vertices
on $\mathcal{H}$-minor-free graphs, for any fixed family of graphs
$\mathcal{H}$, whereas $k$-Internal Out-Branching admits a kernel with $O(k)$
vertices on any graph class of bounded expansion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01676</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01676</id><created>2015-09-05</created><authors><author><keyname>P&#xe9;rez</keyname><forenames>Miguel Rodr&#xed;guez</forenames></author><author><keyname>Veiga</keyname><forenames>Manuel Fern&#xe1;ndez</forenames></author><author><keyname>Alonso</keyname><forenames>Sergio Herrer&#xed;a</forenames></author><author><keyname>Hmila</keyname><forenames>Mariem</forenames></author><author><keyname>Garc&#xed;a</keyname><forenames>C&#xe1;ndido L&#xf3;pez</forenames></author></authors><title>Optimum Traffic Allocation in Bundled Energy Efficient Ethernet Links</title><categories>cs.NI</categories><doi>10.1109/JSYST.2015.2466086</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The energy demands of Ethernet links have been an active focus of research in
the recent years. This work has enabled a new generation of Energy Efficient
Ethernet (EEE) interfaces able to adapt their power consumption to the actual
traffic demands, thus yielding significant energy savings. With the energy
consumption of single network connections being a solved problem, in this paper
we focus on the energy demands of link aggregates that are commonly used to
increase the capacity of a network connection. We build on known energy models
of single EEE links to derive the energy demands of the whole aggregate as a
function on how the traffic load is spread among its powered links. We then
provide a practical method to share the load that minimizes overall energy
consumption with controlled packet delay, and prove that it is valid for a wide
range of EEE links. Finally, we validate our method with both synthetic and
real traffic traces captured in Internet backbones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01682</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01682</id><created>2015-09-05</created><authors><author><keyname>Sousa</keyname><forenames>Felipe R. M.</forenames></author><author><keyname>Cordeiro</keyname><forenames>Lucas C.</forenames></author><author><keyname>Filho</keyname><forenames>Eddie B. de Lima</forenames></author></authors><title>Bounded Model Checking of C++ Programs Based on the Qt Framework
  (extended version)</title><categories>cs.LO cs.SE</categories><comments>extended version of paper published at GCCE'15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The software development process for embedded systems is getting faster and
faster, which generally incurs an increase in the associated complexity. As a
consequence, consumer electronics companies usually invest a lot of resources
in fast and automatic verification processes, in order to create robust systems
and reduce product recall rates. Because of that, the present paper proposes a
simplified version of the Qt framework, which is integrated into the Efficient
SMT-Based Bounded Model Checking tool to verify actual applications that use
the mentioned framework. The method proposed in this paper presents a success
rate of 94.45%, for the developed test suite.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01683</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01683</id><created>2015-09-05</created><authors><author><keyname>Benedikt</keyname><forenames>Michael</forenames></author><author><keyname>Bourhis</keyname><forenames>Pierre</forenames></author><author><keyname>Cate</keyname><forenames>Balder ten</forenames></author><author><keyname>Puppis</keyname><forenames>Gabriele</forenames></author></authors><title>Querying Visible and Invisible Tables in the Presence of Integrity
  Constraints</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a wide-ranging study of the scenario where a subset of the tables
in a relational schema are visible to a user (that is, their complete contents
are known) while the remaining tables are invisible. The schema also has a set
of integrity constraints, which may relate the visible tables to invisible ones
but also may constrain both the visible and invisible instances. We want to
determine whether information about a user query can be inferred using only the
visible information and the constraints. We consider whether positive
information about the query can be inferred, and also whether negative
information (the query does not hold) can be inferred. We further consider both
the instance-level version of the problem (the visible table extensions are
given) and the schema-level version, where we want to know whether information
can be leaked in some instance of the schema. Our instance-level results
classify the complexity of these problems, both as a function of all inputs,
and in the size of the instance alone. Our schema-level results exhibit an
unusual dividing line between decidable and undecidable cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01692</identifier>
 <datestamp>2016-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01692</id><created>2015-09-05</created><updated>2016-02-17</updated><authors><author><keyname>Vylomova</keyname><forenames>Ekaterina</forenames></author><author><keyname>Rimell</keyname><forenames>Laura</forenames></author><author><keyname>Cohn</keyname><forenames>Trevor</forenames></author><author><keyname>Baldwin</keyname><forenames>Timothy</forenames></author></authors><title>Take and Took, Gaggle and Goose, Book and Read: Evaluating the Utility
  of Vector Differences for Lexical Relation Learning</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent work on word embeddings has shown that simple vector subtraction over
pre-trained embeddings is surprisingly effective at capturing different lexical
relations, despite lacking explicit supervision. Prior work has evaluated this
intriguing result using a word analogy prediction formulation and hand-selected
relations, but the generality of the finding over a broader range of lexical
relation types and different learning settings has not been evaluated. In this
paper, we carry out such an evaluation in two learning settings: (1) spectral
clustering to induce word relations, and (2) supervised learning to classify
vector differences into relation types. We find that word embeddings capture a
surprising amount of information, and that, under suitable supervised training,
vector subtraction generalises well to a broad range of relations, including
over unseen lexical items.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01693</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01693</id><created>2015-09-05</created><authors><author><keyname>Abuella</keyname><forenames>Mohamed</forenames><affiliation>Southern Illinois University</affiliation></author><author><keyname>Hatziadoniu</keyname><forenames>Constantine J.</forenames><affiliation>Southern Illinois University</affiliation></author></authors><title>The Economic Dispatch for Integrated Wind Power Systems Using Particle
  Swarm Optimization</title><categories>cs.CE</categories><comments>This paper is a partial work of M.S.Thesis in Electrical and Computer
  Engineering at Southern Illinois University Carbondale</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The economic dispatch of wind power units is quite different from that in
conventional thermal units, since the adopted model should take into
consideration the intermittency nature of wind speed as well. Therefore, this
paper uses a model that takes into account the aforementioned consideration in
addition to whether the utility owns wind turbines or not. The economic
dispatch is solved by using one of the modern optimization algorithms: the
particle swarm optimization algorithm. A 6-bus system is used and it includes
wind-powered generators besides to thermal generators. The thorough analysis of
the results is also provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01698</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01698</id><created>2015-09-05</created><updated>2015-09-27</updated><authors><author><keyname>&#x15e;im&#x15f;ekli</keyname><forenames>Umut</forenames></author><author><keyname>Koptagel</keyname><forenames>Hazal</forenames></author><author><keyname>&#xd6;ztoprak</keyname><forenames>Figen</forenames></author><author><keyname>Birbil</keyname><forenames>&#x15e;. &#x130;lker</forenames></author><author><keyname>Cemgil</keyname><forenames>Ali Taylan</forenames></author></authors><title>HAMSI: Distributed Incremental Optimization Algorithm Using Quadratic
  Approximations for Partially Separable Problems</title><categories>stat.ML cs.LG</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose HAMSI, a provably convergent incremental algorithm for solving
large-scale partially separable optimization problems that frequently emerge in
machine learning and inferential statistics. The algorithm is based on a local
quadratic approximation and hence allows incorporating a second order curvature
information to speed-up the convergence. Furthermore, HAMSI needs almost no
tuning, and it is scalable as well as easily parallelizable. In large-scale
simulation studies with the MovieLens datasets, we illustrate that the method
is superior to a state-of-the-art distributed stochastic gradient descent
method in terms of convergence behavior. This performance gain comes at the
expense of using memory that scales only linearly with the total size of the
optimization variables. We conclude that HAMSI may be considered as a viable
alternative in many scenarios, where first order methods based on variants of
stochastic gradient descent are applicable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01703</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01703</id><created>2015-09-05</created><authors><author><keyname>Bajovic</keyname><forenames>Dragana</forenames></author><author><keyname>Jakovetic</keyname><forenames>Dusan</forenames></author><author><keyname>Krejic</keyname><forenames>Natasa</forenames></author><author><keyname>Jerinkic</keyname><forenames>Natasa Krklec</forenames></author></authors><title>Newton-like method with diagonal correction for distributed optimization</title><categories>cs.IT math.IT math.OC</categories><comments>submitted to a journal; authors' order is alphabetical</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider distributed optimization problems where networked nodes
cooperatively minimize the sum of their locally known convex costs. A popular
class of methods to solve these problems are the distributed gradient methods,
which are attractive due to their inexpensive iterations, but have a drawback
of slow convergence rates. This motivates the incorporation of second-order
information in the distributed methods, but this task is challenging: although
the Hessians which arise in the algorithm design respect the sparsity of the
network, their inverses are dense, hence rendering distributed implementations
difficult. We overcome this challenge and propose a class of distributed
Newton-like methods, which we refer to as Distributed Quasi Newton (DQN). The
DQN family approximates the Hessian inverse by: 1) splitting the Hessian into
its diagonal and off-diagonal part, 2) inverting the diagonal part, and 3)
approximating the inverse of the off-diagonal part through a weighted linear
function. The approximation is parameterized by the tuning variables which
correspond to different splittings of the Hessian and by different weightings
of the off-diagonal Hessian part. Specific choices of the tuning variables give
rise to different variants of the proposed general DQN method -- dubbed DQN-0,
DQN-1 and DQN-2 -- which mutually trade-off communication and computational
costs for convergence. Simulations illustrate that the proposed DQN methods
compare favorably with existing alternatives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01706</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01706</id><created>2015-09-05</created><authors><author><keyname>Zhang</keyname><forenames>Jing</forenames></author><author><keyname>Paschalidis</keyname><forenames>Ioannis Ch.</forenames></author></authors><title>An Improved Composite Hypothesis Test for Markov Models with
  Applications in Network Anomaly Detection</title><categories>cs.IT cs.SY math.IT</categories><comments>6 pages, 6 figures; final version for CDC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent work has proposed the use of a composite hypothesis Hoeffding test for
statistical anomaly detection. Setting an appropriate threshold for the test
given a desired false alarm probability involves approximating the false alarm
probability. To that end, a large deviations asymptotic is typically used
which, however, often results in an inaccurate setting of the threshold,
especially for relatively small sample sizes. This, in turn, results in an
anomaly detection test that does not control well for false alarms. In this
paper, we develop a tighter approximation using the Central Limit Theorem (CLT)
under Markovian assumptions. We apply our result to a network anomaly detection
application and demonstrate its advantages over earlier work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01709</identifier>
 <datestamp>2015-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01709</id><created>2015-09-05</created><updated>2015-11-18</updated><authors><author><keyname>Mitra</keyname><forenames>Peetak</forenames></author><author><keyname>Gudibande</keyname><forenames>Niranjan</forenames></author><author><keyname>Iyer</keyname><forenames>Kannan</forenames></author><author><keyname>Eldho</keyname><forenames>TI</forenames></author></authors><title>Algorithm for estimating swirl angles in multi-intake hydraulic sumps</title><categories>cs.CE</categories><comments>The paper has been withdrawn by the author (Peetak Mitra) since it
  was felt the paper was sketchy in its concepts</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The paper has been withdrawn effective November 18, 2015.
  Hydraulic Pump sumps are designed to provide a swirl free flow to the pump.
The degree of swirl is measured in physical model tests using a swirl meter and
a quantity known as swirl angle is generally measured. The present paper
presents a novel method to compute the bulk swirl angle using the local
velocity field obtained from computational fluid dynamics data. The basis for
the present method is the conservation of angular momentum conservation. By
carrying out both numerical and experimental studies the novel swirl angle
calculation method is validated. Further the effect of vortex suppression
devices in reducing the swirl angle is also demonstrated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01710</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01710</id><created>2015-09-05</created><authors><author><keyname>Jiang</keyname><forenames>Wenhao</forenames></author><author><keyname>Nie</keyname><forenames>Feiping</forenames></author><author><keyname>Chung</keyname><forenames>Fu-lai Korris</forenames></author><author><keyname>Huang</keyname><forenames>Heng</forenames></author></authors><title>Algorithm and Theoretical Analysis for Domain Adaptation Feature
  Learning with Linear Classifiers</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Domain adaptation problem arises in a variety of applications where the
training set (\textit{source} domain) and testing set (\textit{target} domain)
follow different distributions. The difficulty of such learning problem lies in
how to bridge the gap between the source distribution and target distribution.
In this paper, we give an formal analysis of feature learning algorithms for
domain adaptation with linear classifiers. Our analysis shows that in order to
achieve good adaptation performance, the second moments of source domain
distribution and target domain distribution should be similar. Based on such a
result, a new linear feature learning algorithm for domain adaptation is
designed and proposed. Furthermore, the new algorithm is extended to have
multiple layers, resulting in becoming another linear feature learning
algorithm. The newly introduced method is effective for the domain adaptation
tasks on Amazon review dataset and spam dataset from ECML/PKDD 2006 discovery
challenge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01719</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01719</id><created>2015-09-05</created><authors><author><keyname>Lin</keyname><forenames>Yuewei</forenames></author><author><keyname>Chen</keyname><forenames>Jing</forenames></author><author><keyname>Cao</keyname><forenames>Yu</forenames></author><author><keyname>Zhou</keyname><forenames>Youjie</forenames></author><author><keyname>Zhang</keyname><forenames>Lingfeng</forenames></author><author><keyname>Tang</keyname><forenames>Yuan Yan</forenames></author><author><keyname>Wang</keyname><forenames>Song</forenames></author></authors><title>Unsupervised Cross-Domain Recognition by Identifying Compact Joint
  Subspaces</title><categories>cs.CV</categories><comments>ICIP 2015 Top 10% paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a new method to solve the cross-domain recognition
problem. Different from the traditional domain adaption methods which rely on a
global domain shift for all classes between source and target domain, the
proposed method is more flexible to capture individual class variations across
domains. By adopting a natural and widely used assumption -- &quot;the data samples
from the same class should lay on a low-dimensional subspace, even if they come
from different domains&quot;, the proposed method circumvents the limitation of the
global domain shift, and solves the cross-domain recognition by finding the
compact joint subspaces of source and target domain. Specifically, given
labeled samples in source domain, we construct subspaces for each of the
classes. Then we construct subspaces in the target domain, called anchor
subspaces, by collecting unlabeled samples that are close to each other and
highly likely all fall into the same class. The corresponding class label is
then assigned by minimizing a cost function which reflects the overlap and
topological structure consistency between subspaces across source and target
domains, and within anchor subspaces, respectively.We further combine the
anchor subspaces to corresponding source subspaces to construct the compact
joint subspaces. Subsequently, one-vs-rest SVM classifiers are trained in the
compact joint subspaces and applied to unlabeled data in the target domain. We
evaluate the proposed method on two widely used datasets: object recognition
dataset for computer vision tasks, and sentiment classification dataset for
natural language processing tasks. Comparison results demonstrate that the
proposed method outperforms the comparison methods on both datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01722</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01722</id><created>2015-09-05</created><authors><author><keyname>Ferrer-i-Cancho</keyname><forenames>Ramon</forenames></author></authors><title>A commentary on &quot;The now-or-never bottleneck: a fundamental constraint
  on language&quot;, by Christiansen and Chater (2015)</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a recent article, Christiansen and Chater (2015) present a fundamental
constraint on language, i.e. a now-or-never bottleneck that arises from our
fleeting memory, and explore its implications, e.g., chunk-and-pass processing,
outlining a framework that promises to unify different areas of research. Here
we explore additional support for this constraint and suggest further
connections from quantitative linguistics and information theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01727</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01727</id><created>2015-09-05</created><authors><author><keyname>Sandomirskiy</keyname><forenames>Fedor</forenames></author></authors><title>On repeated zero-sum games with incomplete information and
  asymptotically bounded values</title><categories>cs.GT math.OC</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider repeated zero-sum games with incomplete information on the side
of Player2 with the total payoff given by the non-normalized sum of stage
gains. In the classical examples he value $V_N$ of such $N$-stage game is of
the order of $N$ or $\sqrt{N}$ as $N\to \infty$.
  Our aim is to present a general framework for another asymptotic behavior of
the value $V_N$ observed for the discrete version of the financial market model
introduced by De Meyer and Saley. For this game Domansky and independently De
Meyer with Marino found that $V_N$ remains bounded as $N\to\infty$ and
converges to the limiting value. This game is almost-fair, i.e. if Player1
forgets his private information the value becomes zero.
  We describe a class of almost-fair games having bounded values in terms of an
easy-checkable property of the auxiliary non-revealing game. We call this
property the trigger property and it says that there exists an optimal strategy
of Player2 that is piecewise-constant as a function of a prior distribution
$p$. Discrete market models have the trigger property. We show that for
non-trigger almost-fair games with additional non-degeneracy condition $V_N$ is
of the order of $\sqrt{N}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01740</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01740</id><created>2015-09-05</created><updated>2015-10-15</updated><authors><author><keyname>Garland</keyname><forenames>Joshua</forenames></author><author><keyname>James</keyname><forenames>Ryan G.</forenames></author><author><keyname>Bradley</keyname><forenames>Elizabeth</forenames></author></authors><title>A new method for choosing parameters in delay reconstruction-based
  forecast strategies</title><categories>cs.IT math.DS math.IT physics.data-an</categories><comments>15 pages, 14 figures, 1 table</comments><journal-ref>Physical Review E 93 (2) 022221, 2016</journal-ref><doi>10.1103/PhysRevE.93.022221</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Delay-coordinate reconstruction is a proven modeling strategy for building
effective forecasts of nonlinear time series. The first step in this process is
the estimation of good values for two parameters, the time delay and the
embedding dimension. Many heuristics and strategies have been proposed in the
literature for estimating these values. Few, if any, of these methods were
developed with forecasting in mind, however, and their results are not optimal
for that purpose. Even so, these heuristics---intended for other
applications---are routinely used when building delay coordinate
reconstruction-based forecast models. In this paper, we propose a new strategy
for choosing optimal parameter values for forecast methods that are based on
delay-coordinate reconstructions. The basic calculation involves maximizing the
shared information between each delay vector and the future state of the
system. We illustrate the effectiveness of this method on several synthetic and
experimental systems, showing that this metric can be calculated quickly and
reliably from a relatively short time series, and that it provides a direct
indication of how well a near-neighbor based forecasting method will work on a
given delay reconstruction of that time series. This allows a practitioner to
choose reconstruction parameters that avoid any pathologies, regardless of the
underlying mechanism, and maximize the predictive information contained in the
reconstruction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01746</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01746</id><created>2015-09-05</created><updated>2016-02-25</updated><authors><author><keyname>Erickson</keyname><forenames>Alejandro</forenames></author><author><keyname>Kiasari</keyname><forenames>Abbas Eslami</forenames></author><author><keyname>Navaridas</keyname><forenames>Javier</forenames></author><author><keyname>Stewart</keyname><forenames>Iain A.</forenames></author></authors><title>An Optimal Single-Path Routing Algorithm in the Datacenter Network
  DPillar</title><categories>cs.DC</categories><comments>Submitted to a Journal. A version of this paper appeared at the 9th
  Annual International Conference on Combinatorial Optimization and
  Applications (COCOA)</comments><acm-class>B.4.3; C.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  DPillar has recently been proposed as a server-centric datacenter network and
is combinatorially related to (but distinct from) the well-known wrapped
butterfly network. We explain the relationship between DPillar and the wrapped
butterfly network before proving that the underlying graph of DPillar is a
Cayley graph; hence, the datacenter network DPillar is node-symmetric. We use
this symmetry property to establish a single-path routing algorithm for DPillar
that computes a shortest path and has time complexity $O(k)$, where $k$
parameterizes the dimension of DPillar (we refer to the number of ports in its
switches as $n$). Our analysis also enables us to calculate the diameter of
DPillar exactly. Moreover, our algorithm is trivial to implement, being
essentially a conditional clause of numeric tests, and improves significantly
upon a routing algorithm earlier employed for DPillar. Furthermore, we provide
empirical data in order to demonstrate this improvement. In particular, we
empirically show that our routing algorithm improves the average length of
paths found, the aggregate bottleneck throughput, and the communication
latency. A secondary, yet important, effect of our work is that it emphasises
that datacenter networks are amenable to a closer combinatorial scrutiny that
can significantly improve their computational efficiency and performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01747</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01747</id><created>2015-09-05</created><authors><author><keyname>Erickson</keyname><forenames>Alejandro</forenames></author><author><keyname>Kiasari</keyname><forenames>Abbas Eslami</forenames></author><author><keyname>Navaridas</keyname><forenames>Javier</forenames></author><author><keyname>Stewart</keyname><forenames>Iain A.</forenames></author></authors><title>Routing Algorithms for Recursively-Defined Data Centre Networks</title><categories>cs.DC</categories><comments>Appeared at the 13th IEEE International Symposium on Parallel and
  Distributed Processing with Applications (IEEE ISPA-15)</comments><acm-class>C.2.1; B.4.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The server-centric data centre network architecture can accommodate a wide
variety of network topologies. Newly proposed topologies in this arena often
require several rounds of analysis and experimentation in order that they might
achieve their full potential as data centre networks. We propose a family of
novel routing algorithms on two well-known data centre networks of this type,
(Generalized) DCell and FiConn, using techniques that can be applied more
generally to the class of networks we call completely connected
recursively-defined networks. In doing so, we develop a classification of all
possible routes from server-node to server-node on these networks, called
general routes of order $t$, and find that for certain topologies of interest,
our routing algorithms efficiently produce paths that are up to 16% shorter
than the best previously known algorithms, and are comparable to shortest
paths. In addition to finding shorter paths, we show evidence that our
algorithms also have good load-balancing properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01756</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01756</id><created>2015-09-05</created><authors><author><keyname>Li</keyname><forenames>Xueru</forenames></author><author><keyname>Bj&#xf6;rnson</keyname><forenames>Emil</forenames></author><author><keyname>Larsson</keyname><forenames>Erik G.</forenames></author><author><keyname>Zhou</keyname><forenames>Shidong</forenames></author><author><keyname>Wang</keyname><forenames>Jing</forenames></author></authors><title>A Multi-cell MMSE Detector for Massive MIMO Systems and New Large System
  Analysis</title><categories>cs.IT math.IT</categories><comments>6 pages, 3 figures, accepted by Globecom 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a new multi-cell MMSE detector is proposed for massive MIMO
systems. Let $K$ and $B$ denote the number of users in each cell and the number
of available pilot sequences in the network, respectively, with $B = \beta K$,
where $\beta \ge 1 $ is called the pilot reuse factor. The novelty of the
multi-cell MMSE detector is that it utilizes all $B$ channel directions that
can be estimated locally at a base station, so that intra-cell interference,
parts of the inter-cell interference and the noise can all be actively
suppressed, while conventional detectors only use the $K$ intra-cell channels.
Furthermore, in the large-system limit, a deterministic equivalent expression
of the uplink SINR for the proposed multi-cell MMSE is derived. The expression
is easy to compute and accounts for power control for the pilot and payload,
imperfect channel estimation and arbitrary pilot allocation. Numerical results
show that significant sum spectral efficiency gains can be obtained by the
multi-cell MMSE over the conventional single-cell MMSE and the recent
multi-cell ZF, and the gains become more significant as $\beta$ and/or $K$
increases. Furthermore, the deterministic equivalent is shown to be very
accurate even for relatively small system dimensions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01758</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01758</id><created>2015-09-05</created><authors><author><keyname>Li</keyname><forenames>Xueru</forenames></author><author><keyname>Bj&#xf6;rnson</keyname><forenames>Emil</forenames></author><author><keyname>Larsson</keyname><forenames>Erik G.</forenames></author><author><keyname>Zhou</keyname><forenames>Shidong</forenames></author><author><keyname>Wang</keyname><forenames>Jing</forenames></author></authors><title>A Multi-cell MMSE Precoder for Massive MIMO Systems and New Large System
  Analysis</title><categories>cs.IT math.IT</categories><comments>6 pages, 4 figures, accepted by Globecom 2015. arXiv admin note: text
  overlap with arXiv:1509.01756</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a new multi-cell MMSE precoder is proposed for massive MIMO
systems. We consider a multi-cell network where each cell has $K$ users and $B$
orthogonal pilot sequences are available, with $B = \beta K$ and $\beta \ge 1$
being the pilot reuse factor over the network. In comparison with conventional
single-cell precoding which only uses the $K$ intra-cell channel estimates, the
proposed multi-cell MMSE precoder utilizes all $B$ channel directions that can
be estimated locally at a base station, so that the transmission is designed
spatially to suppress both parts of the inter-cell and intra-cell interference.
To evaluate the performance, a large-scale approximation of the downlink SINR
for the proposed multi-cell MMSE precoder is derived and the approximation is
tight in the large-system limit. Power control for the pilot and payload,
imperfect channel estimation and arbitrary pilot allocation are accounted for
in our precoder. Numerical results show that the proposed multi-cell MMSE
precoder achieves a significant sum spectral efficiency gain over the classical
single-cell MMSE precoder and the gain increases as $K$ or $\beta$ grows.
Compared with the recent M-ZF precoder, whose performance degrades drastically
for a large $K$, our M-MMSE can always guarantee a high and stable performance.
Moreover, the large-scale approximation is easy to compute and shown to be
accurate even for small system dimensions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01763</identifier>
 <datestamp>2016-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01763</id><created>2015-09-05</created><updated>2016-02-03</updated><authors><author><keyname>Zhang</keyname><forenames>Yihua</forenames></author><author><keyname>Blanton</keyname><forenames>Marina</forenames></author><author><keyname>Almashaqbeh</keyname><forenames>Ghada</forenames></author></authors><title>Implementing Support for Pointers to Private Data in a General-Purpose
  Secure Multi-Party Compiler</title><categories>cs.CR cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent compilers allow a general-purpose program (written in a conventional
programming language) that handles private data to be translated into secure
distributed implementation of the corresponding functionality. The resulting
program is then guaranteed to provably protect private data using secure
multi-party computation techniques. The goals of such compilers are generality,
usability, and efficiency, but the complete set of features of a modern
programming language has not been supported to date by the existing compilers.
In particular, recent compilers PICCO and the two-party ANSI C compiler strive
to translate any C program into its secure multi-party implementation, but
currently lack support for pointers and dynamic memory allocation, which are
important components of many C programs. In this work, we mitigate the
limitation and add support for pointers to private data and consequently
dynamic memory allocation to the PICCO compiler, enabling it to handle a more
diverse set of programs over private data. Because doing so opens up a new
design space, we investigate the use of pointers to private data (with known as
well as private locations stored in them) in programs and report our findings.
Besides dynamic memory allocation, we examine other important topics associated
with common pointer use such as reference by pointer/address, casting, and
building various data structures in the context of secure multi-party
computation. This results in enabling the compiler to automatically translate a
user program that uses pointers to private data into its distributed
implementation that provably protects private data throughout the computation.
We empirically evaluate the constructions and report on performance of
representative programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01770</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01770</id><created>2015-09-06</created><authors><author><keyname>Wimalawarne</keyname><forenames>Kishan</forenames></author><author><keyname>Tomioka</keyname><forenames>Ryota</forenames></author><author><keyname>Sugiyama</keyname><forenames>Masashi</forenames></author></authors><title>Theoretical and Experimental Analyses of Tensor-Based Regression and
  Classification</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We theoretically and experimentally investigate tensor-based regression and
classification. Our focus is regularization with various tensor norms,
including the overlapped trace norm, the latent trace norm, and the scaled
latent trace norm. We first give dual optimization methods using the
alternating direction method of multipliers, which is computationally efficient
when the number of training samples is moderate. We then theoretically derive
an excess risk bound for each tensor norm and clarify their behavior. Finally,
we perform extensive experiments using simulated and real data and demonstrate
the superiority of tensor-based learning methods over vector- and matrix-based
learning methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01771</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01771</id><created>2015-09-06</created><updated>2015-09-07</updated><authors><author><keyname>Fuentes-Pineda</keyname><forenames>Gibran</forenames></author><author><keyname>Meza-Ruiz</keyname><forenames>Ivan Vladimir</forenames></author></authors><title>Sampled Weighted Min-Hashing for Large-Scale Topic Mining</title><categories>cs.LG cs.CL cs.IR</categories><comments>10 pages, Proceedings of the Mexican Conference on Pattern
  Recognition 2015</comments><doi>10.1007/978-3-319-19264-2_20</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present Sampled Weighted Min-Hashing (SWMH), a randomized approach to
automatically mine topics from large-scale corpora. SWMH generates multiple
random partitions of the corpus vocabulary based on term co-occurrence and
agglomerates highly overlapping inter-partition cells to produce the mined
topics. While other approaches define a topic as a probabilistic distribution
over a vocabulary, SWMH topics are ordered subsets of such vocabulary.
Interestingly, the topics mined by SWMH underlie themes from the corpus at
different levels of granularity. We extensively evaluate the meaningfulness of
the mined topics both qualitatively and quantitatively on the NIPS (1.7 K
documents), 20 Newsgroups (20 K), Reuters (800 K) and Wikipedia (4 M) corpora.
Additionally, we compare the quality of SWMH with Online LDA topics for
document representation in classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01774</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01774</id><created>2015-09-06</created><updated>2016-02-11</updated><authors><author><keyname>Kaushik</keyname><forenames>Ankit</forenames></author><author><keyname>Sharma</keyname><forenames>Shree Krishna</forenames></author><author><keyname>Chatzinotas</keyname><forenames>Symeon</forenames></author><author><keyname>Ottersten</keyname><forenames>Bj&#xf6;rn</forenames></author><author><keyname>Jondral</keyname><forenames>Friedrich K.</forenames></author></authors><title>Sensing-Throughput Tradeoff for Interweave Cognitive Radio System: A
  Deployment-Centric Viewpoint</title><categories>cs.IT math.IT</categories><comments>13 pages, 10 figures, Accepted to be published in IEEE Transactions
  on Wireless Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Secondary access to the licensed spectrum is viable only if interference is
avoided at the primary system. In this regard, different paradigms have been
conceptualized in the existing literature. Of these, Interweave Systems (ISs)
that employ spectrum sensing have been widely investigated. Baseline models
investigated in the literature characterize the performance of IS in terms of a
sensing-throughput tradeoff, however, this characterization assumes the
knowledge of the involved channels at the secondary transmitter, which is
unavailable in practice. Motivated by this fact, we establish a novel approach
that incorporates channel estimation in the system model, and consequently
investigate the impact of imperfect channel estimation on the performance of
the IS. More particularly, the variation induced in the detection probability
affects the detector's performance at the secondary transmitter, which may
result in severe interference at the primary users. In this view, we propose to
employ average and outage constraints on the detection probability, in order to
capture the performance of the IS. Our analysis reveals that with an
appropriate choice of the estimation time determined by the proposed model, the
degradation in performance of the IS can be effectively controlled, and
subsequently the achievable secondary throughput can be significantly enhanced.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01787</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01787</id><created>2015-09-06</created><authors><author><keyname>Hlin&#x11b;n&#xfd;</keyname><forenames>Petr</forenames></author><author><keyname>Salazar</keyname><forenames>Gelasio</forenames></author></authors><title>On Hardness of the Joint Crossing Number</title><categories>cs.DM math.CO</categories><msc-class>05C10, 68R10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Joint Crossing Number problem asks for a simultaneous embedding of two
disjoint graphs into one surface such that the number of edge crossings
(between the two graphs) is minimized. It was introduced by Negami in 2001 in
connection with diagonal flips in triangulations of surfaces, and subsequently
investigated in a general form for small-genus surfaces. We prove that all of
the commonly considered variants of this problem are NP-hard already in the
orientable surface of genus 6, by a reduction from a special variant of the
anchored crossing number problem of Cabello and Mohar.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01788</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01788</id><created>2015-09-06</created><authors><author><keyname>Hasnat</keyname><forenames>Md. Abul</forenames></author><author><keyname>Alata</keyname><forenames>Olivier</forenames></author><author><keyname>Tr&#xe9;meau</keyname><forenames>Alain</forenames></author></authors><title>Joint Color-Spatial-Directional clustering and Region Merging (JCSD-RM)
  for unsupervised RGB-D image segmentation</title><categories>cs.CV</categories><comments>submitted to the IEEE Transactions on Pattern Analysis and Machine
  Intelligence (TPAMI)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent advances in depth imaging sensors provide easy access to the
synchronized depth with color, called RGB-D image. In this paper, we propose an
unsupervised method for indoor RGB-D image segmentation and analysis. We
consider a statistical image generation model based on the color and geometry
of the scene. Our method consists of a joint color-spatial-directional
clustering method followed by a statistical planar region merging method. We
evaluate our method on the NYU depth database and compare it with existing
unsupervised RGB-D segmentation methods. Results show that, it is comparable
with the state of the art methods and it needs less computation time. Moreover,
it opens interesting perspectives to fuse color and geometry in an unsupervised
manner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01804</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01804</id><created>2015-09-06</created><authors><author><keyname>Petersen</keyname><forenames>Alexander Michael</forenames></author></authors><title>Quantifying the impact of weak, strong, and super ties in scientific
  careers</title><categories>physics.soc-ph cs.DL physics.data-an stat.AP</categories><comments>13 pages, 5 figures, 1 Table</comments><journal-ref>Proceedings of the National Academy of Sciences 112, E4671-E4680
  (2015)</journal-ref><doi>10.1073/pnas.1501444112</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scientists are frequently faced with the important decision to start or
terminate a creative partnership. This process can be influenced by strategic
motivations, as early career researchers are pursuers, whereas senior
researchers are typically attractors, of new collaborative opportunities.
Focusing on the longitudinal aspects of scientific collaboration, we analyzed
473 collaboration profiles using an ego-centric perspective which accounts for
researcher-specific characteristics and provides insight into a range of
topics, from career achievement and sustainability to team dynamics and
efficiency. From more than 166,000 collaboration records, we quantify the
frequency distributions of collaboration duration and tie-strength, showing
that collaboration networks are dominated by weak ties characterized by high
turnover rates. We use analytic extreme-value thresholds to identify a new
class of indispensable `super ties', the strongest of which commonly exhibit
&gt;50% publication overlap with the central scientist. The prevalence of super
ties suggests that they arise from career strategies based upon cost, risk, and
reward sharing and complementary skill matching. We then use a combination of
descriptive and panel regression methods to compare the subset of publications
coauthored with a super tie to the subset without one, controlling for
pertinent features such as career age, prestige, team size, and prior group
experience. We find that super ties contribute to above-average productivity
and a 17% citation increase per publication, thus identifying these
partnerships - the analog of life partners - as a major factor in science
career development.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01806</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01806</id><created>2015-09-06</created><authors><author><keyname>Weinberger</keyname><forenames>Nir</forenames></author><author><keyname>Merhav</keyname><forenames>Neri</forenames></author></authors><title>Channel Detection in Coded Communication</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of block-coded communication, where in each block,
the channel law belongs to one of two disjoint sets. The decoder is aimed to
decode only messages that have undergone a channel from one of the sets, and
thus has to detect the set which contains the prevailing channel. We begin with
the simplified case where each of the sets is a singleton. For any given code,
we derive the optimum detection/decoding rule in the sense of the best
trade-off among the probabilities of decoding error, false alarm, and
misdetection, and also introduce sub-optimal detection/decoding rules which are
simpler to implement. Then, various achievable bounds on the error exponents
are derived, including the exact single-letter characterization of the random
coding exponents for the optimal detector/decoder. We then extend the random
coding analysis to general sets of channels, and show that there exists a
universal detector/decoder which performs asymptotically as well as the optimal
detector/decoder, when tuned to detect a channel from a specific pair of
channels. The case of a pair of binary symmetric channels is discussed in
detail.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01815</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01815</id><created>2015-09-06</created><authors><author><keyname>Vilisov</keyname><forenames>Valery</forenames></author></authors><title>Research: Analysis of Transport Model that Approximates Decision Taker's
  Preferences</title><categories>cs.LG cs.AI math.OC stat.AP</categories><doi>10.13140/RG.2.1.5085.6166</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Paper provides a method for solving the reverse Monge-Kantorovich transport
problem (TP). It allows to accumulate positive decision-taking experience made
by decision-taker in situations that can be presented in the form of TP. The
initial data for the solution of the inverse TP is the information on orders,
inventories and effective decisions take by decision-taker. The result of
solving the inverse TP contains evaluations of the TPs payoff matrix elements.
It can be used in new situations to select the solution corresponding to the
preferences of the decision-taker. The method allows to gain decision-taker
experience, so it can be used by others. The method allows to build the model
of decision-taker preferences in a specific application area. The model can be
updated regularly to ensure its relevance and adequacy to the decision-taker
system of preferences. This model is adaptive to the current preferences of the
decision taker.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01817</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01817</id><created>2015-09-06</created><authors><author><keyname>Pandey</keyname><forenames>Gaurav</forenames></author><author><keyname>Dukkipati</keyname><forenames>Ambedkar</forenames></author></authors><title>Hierarchical Completely Random Measures for Mixed Membership Modelling</title><categories>math.ST cs.LG stat.TH</categories><comments>14 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main aim of this paper is to establish the applicability of a broad class
of random measures, that includes the gamma process, for mixed membership
modelling. We use completely random measures~(CRM) and hierarchical CRM to
define a prior for Poisson processes. We derive the marginal distribution of
the resultant point process, when the underlying CRM is marginalized out. Using
well known properties unique to Poisson processes, we were able to derive an
exact approach for instantiating a Poisson process with a hierarchical CRM
prior. Furthermore, we derive Gibbs sampling strategies for hierarchical CRM
models based on Chinese restaurant franchise sampling scheme. As an example, we
present the sum of generalized gamma process (SGGP), and show its application
in topic-modelling. We show that one can determine the power-law behaviour of
the topics and words in a Bayesian fashion, by defining a prior on the
parameters of SGGP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01822</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01822</id><created>2015-09-06</created><authors><author><keyname>Khina</keyname><forenames>Anatoly</forenames></author><author><keyname>Kochman</keyname><forenames>Yuval</forenames></author><author><keyname>Khisti</keyname><forenames>Ashish</forenames></author></authors><title>The MIMO Wiretap Channel Decomposed</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of sending a secret message over the Gaussian multiple-input
multiple-output (MIMO) wiretap channel is studied. While the capacity of this
channel is known, it is not clear how to construct optimal coding schemes that
achieve this capacity. In this work we show how to use linear operations along
with successive interference cancellation in order to reduce the problem to
that of designing optimal codes for the single-antenna additive-noise Gaussian
wiretap channel. Much like popular communication techniques in the absence of
an eavesdropper, the data is carried over parallel streams. The derivation of
the schemes is based upon joint triangularization of channel matrices. We find
that the same techniques can be used to re-derive capacity expressions for the
MIMO wiretap channel in a way that is simple and closely connected to a
transmission scheme. This technique allows to extend the previously proven
strong security for scalar Gaussian channels to the MIMO case. We further
consider the problem of transmitting confidential messages over a two-user
broadcast MIMO channel. For that problem, we find that derivation of both the
capacity and a transmission scheme is a direct corollary of the analysis we
applied to the MIMO wiretap channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01844</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01844</id><created>2015-09-06</created><authors><author><keyname>Filtser</keyname><forenames>Arnold</forenames></author><author><keyname>Krauthgamer</keyname><forenames>Robert</forenames></author></authors><title>Sparsification of Two-Variable Valued CSPs</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A valued constraint satisfaction problem (VCSP) instance $(V,\Pi,w)$ is a set
of variables $V$ with a set of constraints $\Pi$ weighted by $w$. Given a VCSP
instance, we are interested in a re-weighted sub-instance $(V,\Pi'\subset
\Pi,w')$ such that preserves the value of the given instance (under every
assignment to the variables) within factor $1\pm\epsilon$. A well-studied
special case is cut sparsification in graphs, which has found various
applications.
  We show that a VCSP instance consisting of a single boolean predicate
$P(x,y)$ (e.g., for cut, $P=\mbox{XOR}$) can be sparsified into
$O(|V|/\epsilon^2)$ constraints if and only if the number of inputs that
satisfy $P$ is anything but one (i.e., $|P^{-1}(1)| \neq 1$). Furthermore, this
sparsity bound is tight unless $P$ is a relatively trivial predicate. We
conclude that also systems of 2SAT (or 2LIN) constraints can be sparsified.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01846</identifier>
 <datestamp>2016-02-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01846</id><created>2015-09-06</created><updated>2016-02-01</updated><authors><author><keyname>Pan</keyname><forenames>Yunpeng</forenames></author><author><keyname>Theodorou</keyname><forenames>Evangelos A.</forenames></author><author><keyname>Kontitsis</keyname><forenames>Michail</forenames></author></authors><title>Sample Efficient Path Integral Control under Uncertainty</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a data-driven optimal control framework that can be viewed as a
generalization of the path integral (PI) control approach. We find iterative
feedback control laws without parameterization based on probabilistic
representation of learned dynamics model. The proposed algorithm operates in a
forward-backward manner which differentiate from other PI-related methods that
perform forward sampling to find optimal controls. Our method uses
significantly less samples to find optimal controls compared to other
approaches within the PI control family that relies on extensive sampling from
given dynamics models or trials on physical systems in model-free fashions. In
addition, the learned controllers can be generalized to new tasks without
re-sampling based on the compositionality theory for the linearly-solvable
optimal control framework. We provide experimental results on three different
systems and comparisons with state-of-the-art model-based methods to
demonstrate the efficiency and generalizability of the proposed framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01851</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01851</id><created>2015-09-06</created><authors><author><keyname>Balduzzi</keyname><forenames>David</forenames></author></authors><title>Deep Online Convex Optimization by Putting Forecaster to Sleep</title><categories>cs.LG cs.GT cs.NE</categories><comments>20 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Methods from convex optimization such as accelerated gradient descent are
widely used as building blocks for deep learning algorithms. However, the
reasons for their empirical success are unclear, since neural networks are not
convex and standard guarantees do not apply. This paper develops the first
rigorous link between online convex optimization and error backpropagation on
convolutional networks. The first step is to introduce circadian games, a mild
generalization of convex games with similar convergence properties. The main
result is that error backpropagation on a convolutional network is equivalent
to playing out a circadian game. It follows immediately that the waking-regret
of players in the game (the units in the neural network) controls the overall
rate of convergence of the network. Finally, we explore some implications of
the results: (i) we describe the representations learned by a neural network
game-theoretically; (ii) propose a learning setting at the level of individual
units that can be plugged into deep architectures; and (iii) propose a new
approach to adaptive model selection by applying bandit algorithms to choose
which players to wake on each round.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01852</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01852</id><created>2015-09-06</created><authors><author><keyname>Rossi</keyname><forenames>Giovanni</forenames></author></authors><title>Weighted paths between partitions</title><categories>cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Developing from a concern in bioinformatics, this paper analyses alternative
metrics between partitions. From both theoretical and applicative perspectives,
a seemingly most appropriate distance between any two partitions is HD, which
counts the number of atoms finer than either one but not both. While faithfully
reproducing the traditional Hamming distance between subsets, HD is very
sensible and easily computable through scalar products between Boolean vectors.
Also, it properly deals with complements and axiomatically resembles the
(entropy-based) variation of information VI distance. Entire families of
metrics (including both HD and VI) obtain as minimal paths in the weighted
graph given by the Hasse diagram: submodular weighting functions yield
path-based distances visiting the join (of any two partitions), whereas
supermodularity leads to visit the meet. This also provides an exact (rather
than heuristic) approach to the consensus partition (combinatorial
optimization) problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01857</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01857</id><created>2015-09-06</created><authors><author><keyname>Rastuti</keyname></author><author><keyname>Abdillah</keyname><forenames>Leon Andretti</forenames></author><author><keyname>Agustini</keyname><forenames>Eka Puji</forenames></author></authors><title>Sistem Informasi Geografis Potensi Wilayah Kabupaten Banyuasin Berbasis
  Web</title><categories>cs.CY</categories><comments>6 pages, presented at the Student Colloquium Sistem Informasi &amp;
  Teknik Informatika (SC-SITI2015), Palembang, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Development of information technology in the field of spatial data processing
has helped many digital mapping. In the present study, the authors will empower
geographic information systems (GIS) for geographic data processing potential
Banyuasin district. The potential of the region are agriculture, farming, and
industry. The method used to develop the GIS is the waterfall approach models.
After conducting a series of activities starting from the analysis,
requirements, system design, coding, testing, then obtained an information
systems that can provide information about the geographical spread of the
potential of web-based Banyuasin district with the help of ArcGIS. This system
was built by using the waterfall model. The result is a GIS that provide
location information potential of the region.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01858</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01858</id><created>2015-09-06</created><authors><author><keyname>Nakkiran</keyname><forenames>Preetum</forenames></author><author><keyname>Rashmi</keyname><forenames>K. V.</forenames></author><author><keyname>Ramchandran</keyname><forenames>Kannan</forenames></author></authors><title>Optimal Systematic Distributed Storage Codes with Fast Encoding</title><categories>cs.IT cs.DC cs.NI math.IT</categories><comments>16 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Erasure codes are being increasingly used in distributed-storage systems in
place of data-replication, since they provide the same level of reliability
with much lower storage overhead. We consider the problem of constructing
explicit erasure codes for distributed storage with the following desirable
properties motivated by practice: (i) Maximum-Distance-Separable (MDS): to
provide maximal reliability at minimum storage overhead, (ii) Optimal
repair-bandwidth: to minimize the amount of data needed to be transferred to
repair a failed node from remaining ones, (iii) Flexibility in repair: to allow
maximal flexibility in selecting subset of nodes to use for repair, which
includes not requiring that all surviving nodes be used for repair, (iv)
Systematic Form: to ensure that the original data exists in uncoded form, and
(v) Fast encoding: to minimize the cost of generating encoded data (enabled by
a sparse generator matrix).
  This paper presents the first explicit code construction which theoretically
guarantees all the five desired properties simultaneously. Our construction
builds on a powerful class of codes called Product-Matrix (PM) codes. PM codes
satisfy properties (i)-(iii), and either (iv) or (v), but not both
simultaneously. Indeed, native PM codes have inherent structure that leads to
sparsity, but this structure is destroyed when the codes are made systematic.
We first present an analytical framework for understanding the interaction
between the design of PM codes and the systematic property. Using this
framework, we provide an explicit code construction that simultaneously
achieves all the above desired properties. We also present general ways of
transforming existing storage and repair optimal codes to enable fast encoding
through sparsity. In practice, such sparse codes result in encoding speedup by
a factor of about 4 for typical parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01860</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01860</id><created>2015-09-06</created><authors><author><keyname>Hautphenne</keyname><forenames>Sophie</forenames></author><author><keyname>Krings</keyname><forenames>Gautier</forenames></author><author><keyname>Delvenne</keyname><forenames>Jean-Charles</forenames></author><author><keyname>Blondel</keyname><forenames>Vincent D.</forenames></author></authors><title>Sensitivity analysis of a branching process evolving on a network with
  application in epidemiology</title><categories>physics.soc-ph cs.SI physics.data-an</categories><comments>17 pages (30 with SI), Journal of Complex Networks, Feb 2015</comments><doi>10.1093/comnet/cnv001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We perform an analytical sensitivity analysis for a model of a
continuous-time branching process evolving on a fixed network. This allows us
to determine the relative importance of the model parameters to the growth of
the population on the network. We then apply our results to the early stages of
an influenza-like epidemic spreading among a set of cities connected by air
routes in the United States. We also consider vaccination and analyze the
sensitivity of the total size of the epidemic with respect to the fraction of
vaccinated people. Our analysis shows that the epidemic growth is more
sensitive with respect to transmission rates within cities than travel rates
between cities. More generally, we highlight the fact that branching processes
offer a powerful stochastic modeling tool with analytical formulas for
sensitivity which are easy to use in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01864</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01864</id><created>2015-09-06</created><authors><author><keyname>Su</keyname><forenames>Lili</forenames></author><author><keyname>Vaidya</keyname><forenames>Nitin</forenames></author></authors><title>Fault-Tolerant Multi-Agent Optimization: Part III</title><categories>cs.DC math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study fault-tolerant distributed optimization of a sum of convex (cost)
functions with real-valued scalar input/output in the presence of crash faults
or Byzantine faults. In particular, the goal is to optimize a global cost
function $\frac{1}{n}\sum_{i\in \mathcal{V}} h_i(x)$, where $\mathcal{V}=\{1,
\ldots, n\}$ is the collection of agents, and $h_i(x)$ is agent $i$'s local
cost function, which is initially known only to agent $i$. Since the above
global cost function cannot be optimized exactly in presence of crash faults or
Byzantine faults, we define two weaker versions of the problem for crash faults
and Byzantine faults, respectively.
  When some agents may crash, the goal for the weaker problem is to generate an
output that is an optimum of a function formed as $$C(\sum_{i\in \mathcal{N}}
h_i(x)+\sum_{i\in \mathcal{F}} \alpha_i h_i(x)),$$ where $\mathcal{N}$ is the
set of non-faulty agents, $\mathcal{F}$ is the set of faulty agents (crashed
agents), $0\le \alpha_i\le 1$ for each $i\in \mathcal{F}$ and $C$ is a
normalization constant such that $C(|\mathcal{N}|+\sum_{i\in \mathcal{F}}
\alpha_i)=1$. We present an iterative algorithm in which each agent only needs
to perform local computation, and send one message per iteration.
  When some agents may be Byzantine, the system cannot take full advantage of
the data kept by non-faulty agents. The goal for the associated weaker problem
is to generate an output that is an optimum of a function formed as
$$\sum_{i\in \mathcal{N}}\alpha_i h_i(x),$$ such that $\alpha_i\geq 0$ for each
$i\in \mathcal{N}$ and $\sum_{i\in \mathcal{N}}\alpha_i=1$. We present an
iterative algorithm, where only local computation is needed and only one
message per agent is sent in each iteration, that ensures that at least
$|\mathcal{N}|-f$ agents have weights ($\alpha_i$'s) that are lower bounded by
$\frac{1}{2(|\mathcal{N}|-f)}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01865</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01865</id><created>2015-09-06</created><authors><author><keyname>Olieman</keyname><forenames>Alex</forenames></author><author><keyname>Kamps</keyname><forenames>Jaap</forenames></author><author><keyname>Marx</keyname><forenames>Maarten</forenames></author><author><keyname>Nusselder</keyname><forenames>Arjan</forenames></author></authors><title>A Hybrid Approach to Domain-Specific Entity Linking</title><categories>cs.IR cs.CL</categories><comments>SEM'15</comments><acm-class>H.3.1</acm-class><journal-ref>Proc. Posters and Demos track of 11th Int. Conf. on Semantic
  Systems (2015) 55-58</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The current state-of-the-art Entity Linking (EL) systems are geared towards
corpora that are as heterogeneous as the Web, and therefore perform
sub-optimally on domain-specific corpora. A key open problem is how to
construct effective EL systems for specific domains, as knowledge of the local
context should in principle increase, rather than decrease, effectiveness. In
this paper we propose the hybrid use of simple specialist linkers in
combination with an existing generalist system to address this problem. Our
main findings are the following. First, we construct a new reusable benchmark
for EL on a corpus of domain-specific conversations. Second, we test the
performance of a range of approaches under the same conditions, and show that
specialist linkers obtain high precision in isolation, and high recall when
combined with generalist linkers. Hence, we can effectively exploit local
context and get the best of both worlds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01866</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01866</id><created>2015-09-06</created><authors><author><keyname>Taylor</keyname><forenames>Richard</forenames></author></authors><title>$O(n^{2/5})$ Approximation of the Quadratic Knapsack Problem</title><categories>cs.DS cs.CC cs.DM math.CO</categories><comments>6 pages one figure</comments><msc-class>68W25</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For any given $\epsilon&gt;0$ we provide an algorithm for the Quadratic Knapsack
Problem that has an approximation ratio of $O(n^{2/5+\epsilon})$ and a run time
of $O(n^{1/\epsilon})$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01867</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01867</id><created>2015-09-06</created><authors><author><keyname>Cs&#xf3;ka</keyname><forenames>Endre</forenames></author><author><keyname>De&#xe1;k</keyname><forenames>Attila</forenames></author></authors><title>A macro placer algorithm for chip design</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is a set of rectangular macros with given dimensions, and there are
wires connecting some pairs (or sets) of them. We have a placement area where
these macros should be placed without overlaps in order to minimize the total
length of wires. We present a heuristic algorithm which utilizes a special data
structure for representing two dimensional stepfunctions. This results in fast
integral computation and function modification over rectangles. Our heuristics,
especially our data structure for two-dimensional functions, may be useful in
other applications, as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01872</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01872</id><created>2015-09-06</created><authors><author><keyname>Pak</keyname><forenames>Burak</forenames></author><author><keyname>Verbeke</keyname><forenames>Johan</forenames></author></authors><title>Design Studio 2.0: Augmenting Reflective Architectural Design Learning</title><categories>cs.HC cs.SI</categories><journal-ref>Journal of Information Technology in Construction, 17, 502-519
  (2012)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Web 2.0 is beyond a jargon describing technological transformation: it refers
to new strategies, tools and techniques that encourage and augment informed,
creative and social inter(actions). When considered in an educational context,
Web 2.0 provides various opportunities for enhanced integration and for
improving the learning processes in information-rich collaborative disciplines
such as urban planning and architectural design. The dialogue between the
design students and studio teachers can be mediated in various ways by creating
novel learning spaces using Web 2.0-based social software and information
aggregation services, and brought to a level where the Web 2.0 environment
supports, augments and enriches the reflective learning processes. We propose
to call this new setting Design Studio 2.0. We suggest that Design Studio 2.0
can provide numerous opportunities which are not fully or easily available in a
conventional design studio setting. In this context, we will introduce a
web-based geographic virtual environment model (GEO-VEM) and discuss how we
reconfigured and rescaled this model with the objective of supporting an
international urban design studio by encouraging students to make a
collaborative and location-based analysis of a project site (the
Brussels-Charleroi Canal). Pursuing the discussion further, we will present our
experiences and observations of this design studio including web use
statistics, and the results of student attitude surveys. In conclusion, we will
reflect the difficulties and challenges of using the GEO-VEM in the Design
Studio in a blended learning context and develop future prospects. As a result,
we will introduce a set of key criteria for the development and implementation
of an effective e-learning environment as a sustainable platform for supporting
the Design Studio 2.0.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01874</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01874</id><created>2015-09-06</created><authors><author><keyname>Pak</keyname><forenames>Burak</forenames></author><author><keyname>Verbeke</keyname><forenames>Johan</forenames></author></authors><title>Geoweb 2.0 for Participatory Urban Design: Affordances and Critical
  Success Factors</title><categories>cs.SI cs.HC</categories><journal-ref>International Journal of Architectural Computing 12(3) 283-305
  (2014)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we discuss the affordances of open-source Geoweb 2.0 platforms
to support the participatory design of urban projects in real-world
practices.We first introduce the two open-source platforms used in our study
for testing purposes. Then, based on evidence from five different field studies
we identify five affordances of these platforms: conversations on alternative
urban projects, citizen consultation, design empowerment, design studio
learning and design research. We elaborate on these in detail and identify a
key set of success factors for the facilitation of better practices in the
future.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01876</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01876</id><created>2015-09-06</created><authors><author><keyname>Pak</keyname><forenames>Burak</forenames></author><author><keyname>Verbeke</keyname><forenames>Johan</forenames></author></authors><title>Redesigning the urban design studio: Two learning experiments</title><categories>cs.HC cs.SI</categories><journal-ref>Journal of Learning Design 3 (2) 1-13 2013</journal-ref><doi>10.5204/jld.v6i3.160</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main aim of this paper is to discuss how the combination of Web 2.0,
social media and geographic technologies can provide opportunities for learning
and new forms of participation in an urban design studio. This discussion is
mainly based on our recent findings from two experimental urban design studio
setups as well as former research and literature studies. In brief, the web
platform enabled us to extend the learning that took place in the design studio
beyond the studio hours, to represent the design information in novel ways and
allocate multiple communication forms. We found that the student activity in
the introduced web platform was related to their progress up to a certain
extent. Moreover, the students perceived the platform as a convenient medium
and addressed it as a valuable resource for learning. This study should be
conceived as a continuation of a series of our Design Studio 2.0 experiments
which involve the exploitation of opportunities provided by novel
socio-geographic information and communication technologies for the improvement
of the design learning processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01880</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01880</id><created>2015-09-06</created><authors><author><keyname>Sarker</keyname><forenames>Md. Abdul Latif</forenames></author><author><keyname>Lee</keyname><forenames>Moon Ho</forenames></author></authors><title>Transmit Antenna Correlation in Spatial Multiplexing Systems</title><categories>cs.IT math.IT</categories><comments>4</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this letter we present a transmit antenna correlation in spatial
multiplexing systems with channel state information at the transmitter (CSIT)
of a multiple-input multiple-output (MIMO) correlated channel. Indeed, high
correlation degrades system capacity and performance. Most related work has
thus far considered in the statistical channel model such as Kronecker model
with an approximate Toeplitz form of the transmitter or receiver side
correlation for measuring the Power Azimuth Spectral (PAS), but channel
capacity has not been well studied. The Toeplitz assumption of the correlation
matrix restricts the actual configuration of MIMO systems to a linear structure
for a uniform linear array (ULA). Therefore, we only propose an optimal
circulant assumption of a novel transmit correlation matrix in this letter to
significantly reduce arithmetic complexity and spectral norms. We use this
result to maximize ergodic capacity and minimize average bit error rate (BER)
in spatial multiplexing (SM) systems with a zero-forcing (ZF) linear receiver.
Simulation results show that the proposed method can achieve significant
performance enhancement versus conventional equal power transmission
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01881</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01881</id><created>2015-09-06</created><authors><author><keyname>Costa</keyname><forenames>Camila F.</forenames></author><author><keyname>Nascimento</keyname><forenames>Mario A.</forenames></author><author><keyname>Macedo</keyname><forenames>Jose A. F.</forenames></author><author><keyname>Theodoridis</keyname><forenames>Yannis</forenames></author><author><keyname>Pelekis</keyname><forenames>Nikos</forenames></author><author><keyname>Machado</keyname><forenames>Javam</forenames></author></authors><title>Optimal Time-dependent Sequenced Route Queries in Road Networks</title><categories>cs.DB cs.SI</categories><comments>10 pages, 12 figures To be published as a short paper in the 23rd ACM
  SIGSPATIAL</comments><acm-class>H.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present an algorithm for optimal processing of
time-dependent sequenced route queries in road networks, i.e., given a road
network where the travel time over an edge is time-dependent and a given
ordered list of categories of interest, we find the fastest route between an
origin and destination that passes through a sequence of points of interest
belonging to each of the specified categories of interest. For instance,
considering a city road network at a given departure time, one can find the
fastest route between one's work and his/her home, passing through a bank, a
supermarket and a restaurant, in this order. The main contribution of our work
is the consideration of the time dependency of the network, a realistic
characteristic of urban road networks, which has not been considered previously
when addressing the optimal sequenced route query. Our approach uses the A*
search paradigm that is equipped with an admissible heuristic function, thus
guaranteed to yield the optimal solution, along with a pruning scheme for
further reducing the search space. In order to compare our proposal we extended
a previously proposed solution aimed at non-time dependent sequenced route
queries, enabling it to deal with the time-dependency. Our experiments using
real and synthetic data sets have shown our proposed solution to be up to two
orders of magnitude faster than the temporally extended previous solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01886</identifier>
 <datestamp>2015-10-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01886</id><created>2015-09-06</created><updated>2015-10-03</updated><authors><author><keyname>Mao</keyname><forenames>Yuyi</forenames></author><author><keyname>Zhang</keyname><forenames>Jun</forenames></author><author><keyname>Letaief</keyname><forenames>Khaled B.</forenames></author></authors><title>A Lyapunov Optimization Approach for Green Cellular Networks with Hybrid
  Energy Supplies</title><categories>cs.IT math.IT</categories><comments>15 pages, 8 figures, to appear in IEEE Journal on Selected Areas in
  Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Powering cellular networks with renewable energy sources via energy
harvesting (EH) has recently been proposed as a promising solution for green
networking. However, with intermittent and random energy arrivals, it is
challenging to provide satisfactory quality of service (QoS) in EH networks. To
enjoy the greenness brought by EH while overcoming the instability of the
renewable energy sources, hybrid energy supply (HES) networks that are powered
by both EH and the electric grid have emerged as a new paradigm for green
communications. In this paper, we will propose new design methodologies for HES
green cellular networks with the help of Lyapunov optimization techniques. The
network service cost, which addresses both the grid energy consumption and
achievable QoS, is adopted as the performance metric, and it is optimized via
base station assignment and power control (BAPC). Our main contribution is a
low-complexity online algorithm to minimize the long-term average network
service cost, namely, the Lyapunov optimization-based BAPC (LBAPC) algorithm.
One main advantage of this algorithm is that the decisions depend only on the
instantaneous side information without requiring distribution information of
channels and EH processes. To determine the network operation, we only need to
solve a deterministic per-time slot problem, for which an efficient inner-outer
optimization algorithm is proposed. Moreover, the proposed algorithm is shown
to be asymptotically optimal via rigorous analysis. Finally, sample simulation
results are presented to verify the theoretical analysis as well as validate
the effectiveness of the proposed algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01898</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01898</id><created>2015-09-07</created><updated>2015-09-09</updated><authors><author><keyname>Petersen</keyname><forenames>Ian R.</forenames></author><author><keyname>Huntington</keyname><forenames>Elanor H.</forenames></author></authors><title>A Possible Implementation of a Direct Coupling Coherent Quantum Observer</title><categories>quant-ph cs.SY math.OC</categories><comments>A reduced version of this paper has been accepted to appear in the
  2015 Australian Control Conference. arXiv admin note: text overlap with
  arXiv:1408.0399</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the problem of implementing a previously proposed direct
coupling quantum observer for a closed linear quantum system. This observer is
shown to be able to estimate some but not all of the plant variables in a time
averaged sense. The paper proposes a possible experimental implementation of
the observer plant system using a non-degenerate parametric amplifier.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01899</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01899</id><created>2015-09-07</created><updated>2015-09-10</updated><authors><author><keyname>Liu</keyname><forenames>Quan</forenames></author><author><keyname>Guo</keyname><forenames>Wu</forenames></author><author><keyname>Ling</keyname><forenames>Zhen-Hua</forenames></author></authors><title>Integrate Document Ranking Information into Confidence Measure
  Calculation for Spoken Term Detection</title><categories>cs.CL</categories><comments>4 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes an algorithm to improve the calculation of confidence
measure for spoken term detection (STD). Given an input query term, the
algorithm first calculates a measurement named document ranking weight for each
document in the speech database to reflect its relevance with the query term by
summing all the confidence measures of the hypothesized term occurrences in
this document. The confidence measure of each term occurrence is then
re-estimated through linear interpolation with the calculated document ranking
weight to improve its reliability by integrating document-level information.
Experiments are conducted on three standard STD tasks for Tamil, Vietnamese and
English respectively. The experimental results all demonstrate that the
proposed algorithm achieves consistent improvements over the state-of-the-art
method for confidence measure calculation. Furthermore, this algorithm is still
effective even if a high accuracy speech recognizer is not available, which
makes it applicable for the languages with limited speech resources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01931</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01931</id><created>2015-09-07</created><authors><author><keyname>Jin</keyname><forenames>Xianglan</forenames></author><author><keyname>Kim</keyname><forenames>Young-Han</forenames></author></authors><title>The Approximate Capacity of the MIMO Relay Channel</title><categories>cs.IT math.IT</categories><comments>8 pages, 5 figures, submitted to the IEEE Transactions on Information
  Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Capacity bounds are studied for the multiple-antenna complex Gaussian relay
channel with t1 transmitting antennas at the sender, r2 receiving and t2
transmitting antennas at the relay, and r3 receiving antennas at the receiver.
It is shown that the partial decode-forward coding scheme achieves within
min(t1,r2) bits from the cutset bound and at least one half of the cutset
bound, establishing a good approximate expression of the capacity. A similar
additive gap of min(t1 + t2, r3) + r2 bits is shown to be achieved by the
compress-forward coding scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01932</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01932</id><created>2015-09-07</created><authors><author><keyname>Ackerman</keyname><forenames>Eyal</forenames></author></authors><title>On topological graphs with at most four crossings per edge</title><categories>math.CO cs.CG</categories><comments>41 pages, 29 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that if a graph $G$ with $n \geq 3$ vertices can be drawn in the
plane such that each of its edges is involved in at most four crossings, then
$G$ has at most $6n-12$ edges. This settles a conjecture of Pach,
Radoi\v{c}i\'{c}, Tardos, and T\'oth, and yields a better bound for the famous
Crossing Lemma: The crossing number, $\mbox{cr}(G)$, of a (not too sparse)
graph $G$ with $n$ vertices and $m$ edges is at least $c\frac{m^3}{n^2}$, where
$c &gt; 1/29$. This bound is known to be tight, apart from the constant $c$ for
which the previous best lower bound was $1/31.1$. As another corollary we
obtain some progress on the Albertson conjecture: Albertson conjectured that if
the chromatic number of a graph $G$ is $r$, then $\mbox{cr}(G) \geq
\mbox{cr}(K_r)$. This was verified by Albertson, Cranston, and Fox for $r \leq
12$, and for $r \leq 16$ by Bar\'at and T\'oth. Our results imply that
Albertson conjecture holds for $r \leq 18$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01938</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01938</id><created>2015-09-07</created><authors><author><keyname>Kirchhoff</keyname><forenames>Katrin</forenames></author><author><keyname>Zhao</keyname><forenames>Bing</forenames></author><author><keyname>Wang</keyname><forenames>Wen</forenames></author></authors><title>Exploiting Out-of-Domain Data Sources for Dialectal Arabic Statistical
  Machine Translation</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Statistical machine translation for dialectal Arabic is characterized by a
lack of data since data acquisition involves the transcription and translation
of spoken language. In this study we develop techniques for extracting parallel
data for one particular dialect of Arabic (Iraqi Arabic) from out-of-domain
corpora in different dialects of Arabic or in Modern Standard Arabic. We
compare two different data selection strategies (cross-entropy based and
submodular selection) and demonstrate that a very small but highly targeted
amount of found data can improve the performance of a baseline machine
translation system. We furthermore report on preliminary experiments on using
automatically translated speech data as additional training data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01942</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01942</id><created>2015-09-07</created><authors><author><keyname>Mamandipoor</keyname><forenames>Babak</forenames></author><author><keyname>Ramasamy</keyname><forenames>Dinesh</forenames></author><author><keyname>Madhow</keyname><forenames>Upamanyu</forenames></author></authors><title>Newtonized Orthogonal Matching Pursuit: Frequency Estimation over the
  Continuum</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Signal Processing (TSP)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a fast sequential algorithm for the fundamental problem of
estimating continuous-valued frequencies and amplitudes using samples of a
noisy mixture of sinusoids. The algorithm is a natural generalization of
Orthogonal Matching Pursuit (OMP) to the continuum using Newton refinements,
and hence is termed Newtonized OMP (NOMP). Each iteration consists of two
phases: detection of a new sinusoid, and sequential Newton refinements of the
parameters of already detected sinusoids. The refinements play a critical role
in two ways: (1) sidestepping the potential basis mismatch from discretizing a
continuous parameter space, (2) providing feedback for locally refining
parameters estimated in previous iterations. We characterize convergence, and
provide a Constant False Alarm Rate (CFAR) based termination criterion. By
benchmarking against the Cramer Rao Bound, we show that NOMP achieves
near-optimal performance under a variety of conditions. The algorithm
significantly outperforms classical algorithms such as MUSIC in terms of
estimation accuracy, and yields comparable (slightly better) accuracy at much
lower computational cost relative to more recent atomic norm Soft Thresholding
(AST) and Lasso algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01947</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01947</id><created>2015-09-07</created><authors><author><keyname>Kuehne</keyname><forenames>Hilde</forenames></author><author><keyname>Serre</keyname><forenames>Thomas</forenames></author></authors><title>Towards a generative approach to activity recognition and segmentation</title><categories>cs.CV</categories><comments>8 pages, 4 figures. arXiv admin note: text overlap with
  arXiv:1508.06073</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As research on action recognition matures, the focus is gradually shifting
away from categorizing manually-segmented clips into basic action units to
parsing and understanding long action sequences that make up human daily
activities. There is a long history of applying structured models for the
analysis of temporal sequences. Yet, while they seem like an obvious choice for
the recognition of human activities, they have not quite reached a level of
maturity in vision which is comparable to that speech recognition. With the
widespread availability of large video datasets, combined with recent progress
in the development of compact feature representations, the time seems ripe to
revisit structured generative approaches. We propose an end-to-end generative
framework which uses reduced Fisher Vectors (FVs) in conjunction with
structured temporal models for the segmentation and recognition of video
sequences. It shows that the overall generative properties of FVs make them
especially suitable for a combination with generative models like Gaussian
Mixtures. The proposed approach is extensively evaluated on a variety of action
recognition datasets ranging from human cooking activities to animal behavioral
analysis. It shows that the architecture, despite its simplicity, it is able to
outperform complex state-of-the-art approaches on all larger datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01951</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01951</id><created>2015-09-07</created><authors><author><keyname>Katole</keyname><forenames>Atul Laxman</forenames></author><author><keyname>Yellapragada</keyname><forenames>Krishna Prasad</forenames></author><author><keyname>Bedi</keyname><forenames>Amish Kumar</forenames></author><author><keyname>Kalra</keyname><forenames>Sehaj Singh</forenames></author><author><keyname>Chaitanya</keyname><forenames>Mynepalli Siva</forenames></author></authors><title>Hierarchical Deep Learning Architecture For 10K Objects Classification</title><categories>cs.CV cs.LG cs.NE</categories><comments>As appeared in proceedings for CS &amp; IT 2015 - Second International
  Conference on Computer Science &amp; Engineering (CSEN 2015)</comments><journal-ref>Computer Science &amp; Information Technology (CS &amp; IT) (2015) 77-93</journal-ref><doi>10.5121/csit.2015.51408</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Evolution of visual object recognition architectures based on Convolutional
Neural Networks &amp; Convolutional Deep Belief Networks paradigms has
revolutionized artificial Vision Science. These architectures extract &amp; learn
the real world hierarchical visual features utilizing supervised &amp; unsupervised
learning approaches respectively. Both the approaches yet cannot scale up
realistically to provide recognition for a very large number of objects as high
as 10K. We propose a two level hierarchical deep learning architecture inspired
by divide &amp; conquer principle that decomposes the large scale recognition
architecture into root &amp; leaf level model architectures. Each of the root &amp;
leaf level models is trained exclusively to provide superior results than
possible by any 1-level deep learning architecture prevalent today. The
proposed architecture classifies objects in two steps. In the first step the
root level model classifies the object in a high level category. In the second
step, the leaf level recognition model for the recognized high level category
is selected among all the leaf models. This leaf level model is presented with
the same input object image which classifies it in a specific category. Also we
propose a blend of leaf level models trained with either supervised or
unsupervised learning approaches. Unsupervised learning is suitable whenever
labelled data is scarce for the specific leaf level models. Currently the
training of leaf level models is in progress; where we have trained 25 out of
the total 47 leaf level models as of now. We have trained the leaf models with
the best case top-5 error rate of 3.2% on the validation data set for the
particular leaf models. Also we demonstrate that the validation error of the
leaf level models saturates towards the above mentioned accuracy as the number
of epochs are increased to more than sixty.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01957</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01957</id><created>2015-09-07</created><authors><author><keyname>Gribovskiy</keyname><forenames>A.</forenames></author><author><keyname>Mondada</keyname><forenames>F.</forenames></author><author><keyname>Deneubourg</keyname><forenames>J. L.</forenames></author><author><keyname>Cazenille</keyname><forenames>L.</forenames></author><author><keyname>Bredeche</keyname><forenames>N.</forenames></author><author><keyname>Halloy</keyname><forenames>J.</forenames></author></authors><title>Automated Analysis of Behavioural Variability and Filial Imprinting of
  Chicks (G. gallus), using Autonomous Robots</title><categories>q-bio.QM cs.LG cs.RO physics.bio-ph</categories><comments>17 pages, 17 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inter-individual variability has various impacts in animal social behaviour.
This implies that not only collective behaviours have to be studied but also
the behavioural variability of each member composing the groups. To understand
those effects on group behaviour, we develop a quantitative methodology based
on automated ethograms and autonomous robots to study the inter-individual
variability among social animals. We choose chicks of \textit{Gallus gallus
domesticus} as a classic social animal model system for their suitability in
laboratory and controlled experimentation. Moreover, even domesticated chicken
present social structures implying forms or leadership and filial imprinting.
We develop an imprinting methodology on autonomous robots to study individual
and social behaviour of free moving animals. This allows to quantify the
behaviours of large number of animals. We develop an automated experimental
methodology that allows to make relatively fast controlled experiments and
efficient data analysis. Our analysis are based on high-throughput data
allowing a fine quantification of individual behavioural traits. We quantify
the efficiency of various state-of-the-art algorithms to automate data analysis
and produce automated ethograms. We show that the use of robots allows to
provide controlled and quantified stimuli to the animals in absence of human
intervention. We quantify the individual behaviour of 205 chicks obtained from
hatching after synchronized fecundation. Our results show a high variability of
individual behaviours and of imprinting quality and success. Three classes of
chicks are observed with various level of imprinting. Our study shows that the
concomitant use of autonomous robots and automated ethograms allows detailed
and quantitative analysis of behavioural patterns of animals in controlled
laboratory experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01978</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01978</id><created>2015-09-07</created><authors><author><keyname>Brodic</keyname><forenames>Darko</forenames></author><author><keyname>Amelio</keyname><forenames>Alessia</forenames></author><author><keyname>Milivojevic</keyname><forenames>Zoran N.</forenames></author></authors><title>An Approach to the Analysis of the South Slavic Medieval Labels Using
  Image Texture</title><categories>cs.CV cs.AI cs.CL</categories><comments>15 pages, 9 figures, 3rd Workshop on Recognition and Action for Scene
  Understanding (REACTS 2015)</comments><acm-class>I.4; I.2.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper presents a new script classification method for the discrimination
of the South Slavic medieval labels. It consists in the textural analysis of
the script types. In the first step, each letter is coded by the equivalent
script type, which is defined by its typographical features. Obtained coded
text is subjected to the run-length statistical analysis and to the adjacent
local binary pattern analysis in order to extract the features. The result
shows a diversity between the extracted features of the scripts, which makes
the feature classification more effective. It is the basis for the
classification process of the script identification by using an extension of a
state-of-the-art approach for document clustering. The proposed method is
evaluated on an example of hand-engraved in stone and hand-printed in paper
labels in old Cyrillic, angular and round Glagolitic. Experiments demonstrate
very positive results, which prove the effectiveness of the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01981</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01981</id><created>2015-09-07</created><authors><author><keyname>Garc&#xed;a-Marco</keyname><forenames>Ignacio</forenames></author><author><keyname>Knauer</keyname><forenames>Kolja</forenames></author></authors><title>Drawing graphs with vertices and edges in convex position</title><categories>math.CO cs.CG cs.DM</categories><comments>12 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A graph has strong convex dimension $2$, if it admits a straight-line drawing
in the plane such that its vertices are in convex position and the midpoints of
its edges are also in convex position. Halman, Onn, and Rothblum conjectured
that graphs of strong convex dimension $2$ are planar and therefore have at
most $3n-6$ edges. We prove that all such graphs have at most $2n-3$ edges
while on the other hand we present a class of non-planar graphs of strong
convex dimension $2$. We also give lower bounds on the maximum number of edges
a graph of strong convex dimension $2$ can have and discuss variants of this
graph class. We apply our results to questions about large convexly independent
sets in Minkowski sums of planar point sets, that have been of interest in
recent years.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01987</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01987</id><created>2015-09-07</created><authors><author><keyname>H&#xe4;lsig</keyname><forenames>Tim</forenames></author><author><keyname>Lankl</keyname><forenames>Berthold</forenames></author></authors><title>Array Size Reduction for High-Rank LOS MIMO ULAs</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Wireless Communications Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose an extended LOS MIMO channel model, which considers
an additional phase shifting term in the transmission path, and which provides
the potential to improve channel conditioning significantly. We show that this
phase shifting can, for example, be achieved by adding a dielectric material
between the transmitting and receiving antennas, where the phase shift is
dependent on the distance the waves travel in the medium. Using that distance
as a design parameter we demonstrate that the optimal spacing between antenna
elements of uniform linear arrays, achieving full spatial multiplexing, can be
reduced compared with the well-known spacing criterion from previous
investigations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01988</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01988</id><created>2015-09-07</created><authors><author><keyname>Kanade</keyname><forenames>Varun</forenames></author><author><keyname>Leonardos</keyname><forenames>Nikos</forenames></author><author><keyname>Magniez</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames></author></authors><title>Stable Matching with Evolving Preferences</title><categories>cs.GT cs.DS</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of stable matching with dynamic preference lists. At
each time step, the preference list of some player may change by swapping
random adjacent members. The goal of a central agency (algorithm) is to
maintain an approximately stable matching (in terms of number of blocking
pairs) at all times. The changes in the preference lists are not reported to
the algorithm, but must instead be probed explicitly by the algorithm. We
design an algorithm that in expectation and with high probability maintains a
matching that has at most $O((log (n))^2)$ blocking pairs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01990</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01990</id><created>2015-09-07</created><authors><author><keyname>von Looz</keyname><forenames>Moritz</forenames></author><author><keyname>Meyerhenke</keyname><forenames>Henning</forenames></author></authors><title>Querying Probabilistic Neighborhoods in Spatial Data Sets Efficiently</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  $\newcommand{\dist}{\operatorname{dist}}$ In this paper we define the notion
of a probabilistic neighborhood in spatial data: Let a set $P$ of $n$ points in
$\mathbb{R}^d$, a query point $q \in \mathbb{R}^d$, a distance metric $\dist$,
and a monotonically decreasing function $f : \mathbb{R}^+ \rightarrow [0,1]$ be
given. Then a point $p \in P$ belongs to the probabilistic neighborhood $N(q,
f)$ of $q$ with respect to $f$ with probability $f(\dist(p,q))$. We envision
applications in facility location, sensor networks, and other scenarios where a
connection between two entities becomes less likely with increasing distance. A
straightforward query algorithm would determine a probabilistic neighborhood in
$\Theta(n\cdot d)$ time by probing each point in $P$.
  To answer the query in sublinear time for the planar case, we augment a
quadtree suitably and design a corresponding query algorithm. Our theoretical
analysis shows that -- for certain distributions of planar $P$ -- our algorithm
answers a query in $O((|N(q,f)| + \sqrt{n})\log n)$ time with high probability
(whp). This matches up to a logarithmic factor the cost induced by
quadtree-based algorithms for deterministic queries and is asymptotically
faster than the straightforward approach whenever $|N(q,f)| \in o(n / \log n)$.
  As practical proofs of concept we use two applications, one in the Euclidean
and one in the hyperbolic plane. In particular, our results yield the first
generator for random hyperbolic graphs with arbitrary temperatures in
subquadratic time. Moreover, our experimental data show the usefulness of our
algorithm even if the point distribution is unknown or not uniform: The running
time savings over the pairwise probing approach constitute at least one order
of magnitude already for a modest number of points and queries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.01998</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.01998</id><created>2015-09-07</created><authors><author><keyname>Wei</keyname><forenames>Shari Lim</forenames></author><author><keyname>Vasilaki</keyname><forenames>Eleni</forenames></author><author><keyname>Khiat</keyname><forenames>Ali</forenames></author><author><keyname>Salaoru</keyname><forenames>Iulia</forenames></author><author><keyname>Berdan</keyname><forenames>Radu</forenames></author><author><keyname>Prodromakis</keyname><forenames>Themistoklis</forenames></author></authors><title>Emulating long-term synaptic dynamics with memristive devices</title><categories>cs.ET</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The potential of memristive devices is often seeing in implementing
neuromorphic architectures for achieving brain-like computation. However, the
designing procedures do not allow for extended manipulation of the material,
unlike CMOS technology, the properties of the memristive material should be
harnessed in the context of such computation, under the view that biological
synapses are memristors. Here we demonstrate that single solid-state TiO2
memristors can exhibit associative plasticity phenomena observed in biological
cortical synapses, and are captured by a phenomenological plasticity model
called triplet rule. This rule comprises of a spike-timing dependent plasticity
regime and a classical hebbian associative regime, and is compatible with a
large amount of electrophysiology data. Via a set of experiments with our
artificial, memristive, synapses we show that, contrary to conventional uses of
solid-state memory, the co-existence of field- and thermally-driven switching
mechanisms that could render bipolar and/or unipolar programming modes is a
salient feature for capturing long-term potentiation and depression synaptic
dynamics. We further demonstrate that the non-linear accumulating nature of
memristors promotes long-term potentiating or depressing memory transitions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02006</identifier>
 <datestamp>2015-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02006</id><created>2015-09-07</created><authors><author><keyname>Lee</keyname><forenames>Seungyoung</forenames></author></authors><title>Characteristics of Preferentially Attached Network Grown from Small
  World</title><categories>physics.soc-ph cs.SI</categories><comments>13 pages, 6 figures, submitted for the journal publication</comments><journal-ref>J. Korean Phys. Soc. 67 (2015) No. 9, 1703-1707</journal-ref><doi>10.3938/jkps.67.1703</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a model for a preferentially attached network which has grown
from a small world network. Here, the average path length and the clustering
coefficient are estimated, and the topological properties of modeled networks
are compared as the initial conditions are changed. As a result, it is shown
that the topological properties of the initial network remain even after the
network growth. However, the vulnerability of each to preferentially attached
nodes being added is not the same. It is found that the average path length
rapidly decreases as the ratio of preferentially attached nodes increases and
that the characteristics of the initial network can be easily disappeared. On
the other hand, the clustering coefficient of the initial network slowly
decreases with the ratio of preferentially attached nodes and its clustering
characteristic remains much longer.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02010</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02010</id><created>2015-09-07</created><updated>2015-09-26</updated><authors><author><keyname>Olieman</keyname><forenames>Alex</forenames></author><author><keyname>Kamps</keyname><forenames>Jaap</forenames></author><author><keyname>Claros</keyname><forenames>Rosa Merino</forenames></author></authors><title>LocLinkVis: A Geographic Information Retrieval-Based System for
  Large-Scale Exploratory Search</title><categories>cs.IR</categories><comments>SEM'15</comments><journal-ref>Proc. Posters and Demos Track of 11th Int. Conf. on Semantic
  Systems (2015) 30-33</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present LocLinkVis (Locate-Link-Visualize); a system which
supports exploratory information access to a document collection based on
geo-referencing and visualization. It uses a gazetteer which contains
representations of places ranging from countries to buildings, and that is used
to recognize toponyms, disambiguate them into places, and to visualize the
resulting spatial footprints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02012</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02012</id><created>2015-09-07</created><authors><author><keyname>De Giacomo</keyname><forenames>Giuseppe</forenames><affiliation>Sapienza University of Rome, Italy</affiliation></author><author><keyname>Lesp&#xe9;rance</keyname><forenames>Yves</forenames><affiliation>York University, Toronto, ON, Canada</affiliation></author><author><keyname>Patrizi</keyname><forenames>Fabio</forenames><affiliation>Free University of Bozen-Bolzano, Italy</affiliation></author></authors><title>Bounded Situation Calculus Action Theories</title><categories>cs.AI</categories><comments>51 pages</comments><acm-class>I.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate bounded action theories in the situation
calculus. A bounded action theory is one which entails that, in every
situation, the number of object tuples in the extension of fluents is bounded
by a given constant, although such extensions are in general different across
the infinitely many situations. We argue that such theories are common in
applications, either because facts do not persist indefinitely or because the
agent eventually forgets some facts, as new ones are learnt. We discuss various
classes of bounded action theories. Then we show that verification of a
powerful first-order variant of the mu-calculus is decidable for such theories.
Notably, this variant supports a controlled form of quantification across
situations. We also show that through verification, we can actually check
whether an arbitrary action theory maintains boundedness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02023</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02023</id><created>2015-09-07</created><updated>2016-02-29</updated><authors><author><keyname>Deligkas</keyname><forenames>Argyrios</forenames></author><author><keyname>Fearnley</keyname><forenames>John</forenames></author><author><keyname>Spirakis</keyname><forenames>Paul</forenames></author></authors><title>Lipschitz Continuity and Approximate Equilibria</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study games with continuous action spaces and non-linear
payoff functions. Our key insight is that Lipschitz continuity of the payoff
function allows us to provide algorithms for finding approximate equilibria in
these games. We begin by studying Lipschitz games, which encompass, for
example, all concave games with Lipschitz continuous payoff functions. We
provide an efficient algorithm for computing approximate equilibria in these
games. Then we turn our attention to penalty games, which encompass biased
games and games in which players take risk into account. Here we show that if
the penalty function is Lipschitz continuous, then we can provide a
quasi-polynomial time approximation scheme. Finally, we study distance biased
games, where we present simple strongly polynomial time algorithms for finding
best responses in $L_1$, $L_2^2$, and $L_\infty$ biased games, and then use
these algorithms to provide strongly polynomial algorithms that find $2/3$,
$5/7$, and $2/3$ approximations for these norms, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02027</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02027</id><created>2015-09-07</created><authors><author><keyname>Hu</keyname><forenames>Wenrui</forenames></author><author><keyname>Tao</keyname><forenames>Dacheng</forenames></author><author><keyname>Zhang</keyname><forenames>Wensheng</forenames></author><author><keyname>Xie</keyname><forenames>Yuan</forenames></author><author><keyname>Yang</keyname><forenames>Yehui</forenames></author></authors><title>A New Low-Rank Tensor Model for Video Completion</title><categories>cs.CV</categories><comments>8 pages, 11 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a new low-rank tensor model based on the circulant
algebra, namely, twist tensor nuclear norm or t-TNN for short. The twist tensor
denotes a 3-way tensor representation to laterally store 2D data slices in
order. On one hand, t-TNN convexly relaxes the tensor multi-rank of the twist
tensor in the Fourier domain, which allows an efficient computation using FFT.
On the other, t-TNN is equal to the nuclear norm of block circulant
matricization of the twist tensor in the original domain, which extends the
traditional matrix nuclear norm in a block circulant way. We test the t-TNN
model on a video completion application that aims to fill missing values and
the experiment results validate its effectiveness, especially when dealing with
video recorded by a non-stationary panning camera. The block circulant
matricization of the twist tensor can be transformed into a circulant block
representation with nuclear norm invariance. This representation, after
transformation, exploits the horizontal translation relationship between the
frames in a video, and endows the t-TNN model with a more powerful ability to
reconstruct panning videos than the existing state-of-the-art low-rank models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02030</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02030</id><created>2015-09-07</created><authors><author><keyname>Tagarelli</keyname><forenames>Andrea</forenames></author><author><keyname>Interdonato</keyname><forenames>Roberto</forenames></author></authors><title>Time-aware Analysis and Ranking of Lurkers in Social Networks</title><categories>cs.SI physics.soc-ph</categories><comments>23 pages, 9 figures, 7 tables</comments><journal-ref>Social Network Analysis and Mining, Vol 5, Issue 1, December 2015</journal-ref><doi>10.1007/s13278-015-0276-y</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mining the silent members of an online community, also called lurkers, has
been recognized as an important problem that accompanies the extensive use of
online social networks (OSNs). Existing solutions to the ranking of lurkers can
aid understanding the lurking behaviors in an OSN. However, they are limited to
use only structural properties of the static network graph, thus ignoring any
relevant information concerning the time dimension. Our goal in this work is to
push forward research in lurker mining in a twofold manner: (i) to provide an
in-depth analysis of temporal aspects that aims to unveil the behavior of
lurkers and their relations with other users, and (ii) to enhance existing
methods for ranking lurkers by integrating different time-aware properties
concerning information-production and information-consumption actions. Network
analysis and ranking evaluation performed on Flickr, FriendFeed and Instagram
networks allowed us to draw interesting remarks on both the understanding of
lurking dynamics and on transient and cumulative scenarios of time-aware
ranking.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02032</identifier>
 <datestamp>2015-10-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02032</id><created>2015-09-07</created><updated>2015-10-15</updated><authors><author><keyname>Ramos</keyname><forenames>Marcus V. M.</forenames></author><author><keyname>de Queiroz</keyname><forenames>Ruy J. G. B.</forenames></author></authors><title>Formalization of simplification for context-free grammars</title><categories>cs.FL</categories><comments>LSFA 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Context-free grammar simplification is a subject of high importance in
computer language processing technology as well as in formal language theory.
This paper presents a formalization, using the Coq proof assistant, of the fact
that general context-free grammars generate languages that can be also
generated by simpler and equivalent context-free grammars. Namely, useless
symbol elimination, inaccessible symbol elimination, unit rules elimination and
empty rules elimination operations were described and proven correct with
respect to the preservation of the language generated by the original grammar.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02046</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02046</id><created>2015-09-07</created><authors><author><keyname>Wu</keyname><forenames>Yuanxin</forenames></author><author><keyname>Shi</keyname><forenames>Wei</forenames></author></authors><title>On Calibration of Three-axis Magnetometer</title><categories>cs.RO</categories><comments>to appear in IEEE Sensors Journal</comments><doi>10.1109/JSEN.2015.2459767</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Magnetometer has received wide applications in attitude determination and
scientific measurements. Calibration is an important step for any practical
magnetometer use. The most popular three-axis magnetometer calibration methods
are attitude-independent and have been founded on an approximate maximum
likelihood (ML) estimation with a quartic subjective function, derived from the
fact that the magnitude of the calibrated measurements should be constant in a
homogeneous magnetic field. This paper highlights the shortcomings of those
popular methods and proposes to use the quadratic optimal ML estimation instead
for magnetometer calibration. Simulation and test results show that the optimal
ML calibration is superior to the approximate ML methods for magnetometer
calibration in both accuracy and stability, especially for those situations
without sufficient attitude excitation. The significant benefits deserve the
moderately increased computation burden. The main conclusion obtained in the
context of magnetometer in this paper is potentially applicable to various
kinds of three-axis sensors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02054</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02054</id><created>2015-09-07</created><authors><author><keyname>Pan</keyname><forenames>Xianfei</forenames></author><author><keyname>Wu</keyname><forenames>Yuanxin</forenames></author></authors><title>Underwater Doppler Navigation with Self-calibration</title><categories>cs.RO cs.SY</categories><comments>To appear in Journal of Navigation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Precise autonomous navigation remains a substantial challenge to all
underwater platforms. Inertial Measurement Units (IMU) and Doppler Velocity
Logs (DVL) have complementary characteristics and are promising sensors that
could enable fully autonomous underwater navigation in unexplored areas without
relying on additional external Global Positioning System (GPS) or acoustic
beacons. This paper addresses the combined IMU/DVL navigation system from the
viewpoint of observability. We show by analysis that under moderate conditions
the combined system is observable. Specifically, the DVL parameters, including
the scale factor and misalignment angles, can be calibrated in-situ without
using external GPS or acoustic beacon sensors. Simulation results using a
practical estimator validate the analytic conclusions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02058</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02058</id><created>2015-09-07</created><authors><author><keyname>Costero</keyname><forenames>Luis</forenames></author><author><keyname>Igual</keyname><forenames>Francisco D.</forenames></author><author><keyname>Olcoz</keyname><forenames>Katzalin</forenames></author><author><keyname>Quintana-Ort&#xed;</keyname><forenames>Enrique S.</forenames></author></authors><title>Revisiting Conventional Task Schedulers to Exploit Asymmetry in ARM
  big.LITTLE Architectures for Dense Linear Algebra</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dealing with asymmetry in the architecture opens a plethora of questions from
the perspective of scheduling task-parallel applications, and there exist early
attempts to address this problem via ad-hoc strategies embedded into a runtime
framework. In this paper we take a different path, which consists in addressing
the complexity of the problem at the library level, via a few asymmetry-aware
fundamental kernels, hiding the architecture heterogeneity from the task
scheduler. For the specific domain of dense linear algebra, we show that this
is not only possible but delivers much higher performance than a naive approach
based on an asymmetry-oblivious scheduler. Furthermore, this solution also
outperforms an ad-hoc asymmetry-aware scheduler furnished with sophisticated
scheduling techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02060</identifier>
 <datestamp>2015-12-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02060</id><created>2015-09-07</created><authors><author><keyname>Hampson</keyname><forenames>Christopher</forenames></author><author><keyname>Kikot</keyname><forenames>Stanislav</forenames></author><author><keyname>Kurucz</keyname><forenames>Agi</forenames></author></authors><title>The decision problem of modal product logics with a diagonal, and faulty
  counter machines</title><categories>cs.LO</categories><doi>10.1007/s11225-015-9647-7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the propositional modal (and algebraic) treatment of two-variable
first-order logic equality is modelled by a `diagonal' constant, interpreted in
square products of universal frames as the identity (also known as the
`diagonal') relation. Here we study the decision problem of products of two
arbitrary modal logics equipped with such a diagonal. As the presence or
absence of equality in two-variable first-order logic does not influence the
complexity of its satisfiability problem, one might expect that adding a
diagonal to product logics in general is similarly harmless. We show that this
is far from being the case, and there can be quite a big jump in complexity,
even from decidable to the highly undecidable. Our undecidable logics can also
be viewed as new fragments of first- order logic where adding equality changes
a decidable fragment to undecidable. We prove our results by a novel
application of counter machine problems. While our formalism apparently cannot
force reliable counter machine computations directly, the presence of a unique
diagonal in the models makes it possible to encode both lossy and
insertion-error computations, for the same sequence of instructions. We show
that, given such a pair of faulty computations, it is then possible to
reconstruct a reliable run from them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02073</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02073</id><created>2015-09-02</created><authors><author><keyname>Otieno</keyname><forenames>Okal Christopher</forenames></author></authors><title>Managing &amp; Analyzing Large Volumes of Dynamic &amp; Diverse Data</title><categories>cs.CY</categories><comments>6 Pages; IJCSIS August 2015 ID 31071538</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  This study reviews the topic of big data management in the 21st-century.
There are various developments that have facilitated the extensive use of that
form of data in different organizations. The most prominent beneficiaries are
internet businesses and big companies that used vast volumes of data even
before the computational era. The research looks at the definitions of big data
and the factors that influence its access and use for different persons around
the globe. Most people consider the internet as the most significant source of
this data and more specifically on cloud computing and social networking
platforms. It requires sufficient and adequate management procedures to achieve
the efficient use of the big data. The study revisits some of the conventional
methods that companies use to attain this. There are different challenges such
as cost and security that limit the use of big data. Despite these problems,
there are various benefits that everyone can exploit by implementing it, and
they are the focus for most enterprises.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02074</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02074</id><created>2015-09-07</created><authors><author><keyname>Ghorbel</keyname><forenames>Asma</forenames></author><author><keyname>Kobayashi</keyname><forenames>Mari</forenames></author><author><keyname>Yang</keyname><forenames>Sheng</forenames></author></authors><title>Cache-Enabled Broadcast Packet Erasure Channels with State Feedback</title><categories>cs.IT math.IT</categories><comments>8 pages, 4 figures, to be presented at the 53rd Annual Allerton
  Conference on Communication, Control, and Computing, IL, USA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a cache-enabled K-user broadcast erasure packet channel in which
a server with a library of N files wishes to deliver a requested file to each
user who is equipped with a cache of a finite memory M. Assuming that the
transmitter has state feedback and user caches can be filled during off-peak
hours reliably by decentralized cache placement, we characterize the optimal
rate region as a function of the memory size, the erasure probability. The
proposed delivery scheme, based on the scheme proposed by Gatzianas et al.,
exploits the receiver side information established during the placement phase.
Our results enable us to quantify the net benefits of decentralized coded
caching in the presence of erasure. The role of state feedback is found useful
especially when the erasure probability is large and/or the normalized memory
size is small.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02094</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02094</id><created>2015-09-07</created><authors><author><keyname>Park</keyname><forenames>Hyun Soo</forenames></author><author><keyname>Niu</keyname><forenames>Yedong</forenames></author><author><keyname>Shi</keyname><forenames>Jianbo</forenames></author></authors><title>Future Localization from an Egocentric Depth Image</title><categories>cs.CV</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a method for future localization: to predict a set of
plausible trajectories of ego-motion given a depth image. We predict paths
avoiding obstacles, between objects, even paths turning around a corner into
space behind objects. As a byproduct of the predicted trajectories of
ego-motion, we discover in the image the empty space occluded by foreground
objects. We use no image based features such as semantic labeling/segmentation
or object detection/recognition for this algorithm. Inspired by proxemics, we
represent the space around a person using an EgoSpace map, akin to an
illustrated tourist map, that measures a likelihood of occlusion at the
egocentric coordinate system. A future trajectory of ego-motion is modeled by a
linear combination of compact trajectory bases allowing us to constrain the
predicted trajectory. We learn the relationship between the EgoSpace map and
trajectory from the EgoMotion dataset providing in-situ measurements of the
future trajectory. A cost function that takes into account partial occlusion
due to foreground objects is minimized to predict a trajectory. This cost
function generates a trajectory that passes through the occluded space, which
allows us to discover the empty space behind the foreground objects. We
quantitatively evaluate our method to show predictive validity and apply to
various real world scenes including walking, shopping, and social interactions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02099</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02099</id><created>2015-09-07</created><authors><author><keyname>Lamothe</keyname><forenames>Jacques</forenames><affiliation>Mines Albi-Carmaux</affiliation></author><author><keyname>Marmier</keyname><forenames>Fran&#xe7;ois</forenames><affiliation>Mines Albi-Carmaux</affiliation></author><author><keyname>Dupuy</keyname><forenames>Matthieu</forenames><affiliation>Mines Albi-Carmaux</affiliation></author><author><keyname>Gaborit</keyname><forenames>Paul</forenames><affiliation>Mines Albi-Carmaux</affiliation></author><author><keyname>Dupont</keyname><forenames>Lionel</forenames><affiliation>Mines Albi-Carmaux</affiliation></author></authors><title>Scheduling rules to minimize total tardiness in a parallel machine
  problem with setup and calendar constraints</title><categories>cs.DC</categories><proxy>ccsd</proxy><journal-ref>Computers and Operations Research, 2012, 39 (6), pp.1236-1244.</journal-ref><doi>10.1016/j.cor.2010.07.007</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quality control lead times are one of most significant causes of loss of time
in the pharmaceutical and cosmetics industries. This is partly due to the
organization of laboratories that feature parallel multipurpose machines for
chromatographic analyses. The testing process requires long setup times and
operators are needed to launch the process. The various controls are
non-preemptive and are characterized by a release date, a due date and
available routings. These quality processes lead to significant delays, and we
therefore evaluate the total tardiness criterion. Previous heuristics were
defined for the total tardiness criterion, parallel machines, and setup such as
ATC (Apparent Tardiness Cost) and ATCS (ATC with setups). We propose new rules
and a simulated annealing procedure in order to minimize total tardiness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02122</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02122</id><created>2015-09-07</created><authors><author><keyname>Royer</keyname><forenames>Loic A.</forenames></author><author><keyname>Richmond</keyname><forenames>David L.</forenames></author><author><keyname>Rother</keyname><forenames>Carsten</forenames></author><author><keyname>Andres</keyname><forenames>Bjoern</forenames></author><author><keyname>Kainmueller</keyname><forenames>Dagmar</forenames></author></authors><title>Convexity Shape Constraints for Image Segmentation</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Segmenting an image into multiple components is a central task in computer
vision. In many practical scenarios, prior knowledge about plausible components
is available. Incorporating such prior knowledge into models and algorithms for
image segmentation is highly desirable, yet can be non-trivial. In this work,
we introduce a new approach that allows, for the first time, to constrain some
or all components of a segmentation to have convex shapes. Specifically, we
extend the Minimum Cost Multicut Problem by a class of constraints that enforce
convexity. To solve instances of this APX-hard integer linear program to
optimality, we separate the proposed constraints in the branch-and-cut loop of
a state-of-the-art ILP solver. Results on natural and biological images
demonstrate the effectiveness of the approach as well as its advantage over the
state-of-the-art heuristic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02130</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02130</id><created>2015-09-07</created><authors><author><keyname>Quattoni</keyname><forenames>Ariadna</forenames></author><author><keyname>Ramisa</keyname><forenames>Arnau</forenames></author><author><keyname>Madhyastha</keyname><forenames>Pranava Swaroop</forenames></author><author><keyname>Simo-Serra</keyname><forenames>Edgar</forenames></author><author><keyname>Moreno-Noguer</keyname><forenames>Francesc</forenames></author></authors><title>Structured Prediction with Output Embeddings for Semantic Image
  Annotation</title><categories>cs.CV</categories><comments>11 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the task of annotating images with semantic tuples. Solving this
problem requires an algorithm which is able to deal with hundreds of classes
for each argument of the tuple. In such contexts, data sparsity becomes a key
challenge, as there will be a large number of classes for which only a few
examples are available. We propose handling this by incorporating feature
representations of both the inputs (images) and outputs (argument classes) into
a factorized log-linear model, and exploiting the flexibility of scoring
functions based on bilinear forms. Experiments show that integrating feature
representations of the outputs in the structured prediction model leads to
better overall predictions. We also conclude that the best output
representation is specific for each type of argument.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02135</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02135</id><created>2015-09-07</created><updated>2015-09-14</updated><authors><author><keyname>Lawson</keyname><forenames>Gary</forenames></author><author><keyname>Sundriyal</keyname><forenames>Vaibhav</forenames></author><author><keyname>Sosonkina</keyname><forenames>Masha</forenames></author><author><keyname>Shen</keyname><forenames>Yuzhong</forenames></author></authors><title>Experimentation Procedure for Offloaded Mini-Apps Executed on Cluster
  Architectures with Xeon Phi Accelerators</title><categories>cs.DC</categories><comments>Complete, but references are a mess</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A heterogeneous cluster architecture is complex. It contains hundreds, or
thousands of devices connected by a tiered communication system in order to
solve a problem. As a heterogeneous system, these devices will have varying
performance capabilities. To better understand the interactions which occur
between the various devices during execution, an experimentation procedure has
been devised to capture, store, and analyze important and meaningful data. The
procedure consists of various tools, techniques, and methods for capturing
relevant timing, power, and performance data for a typical execution. This
procedure currently applies to architectures with Intel Xeon processors and
Intel Xeon Phi accelerators. It has been applied to the Co-Design Molecular
Dynamics mini-app, courtesy of the ExMatEx team. This work aims to provide
end-users with a strategy for investigating codes executed on heterogeneous
cluster architectures with Xeon Phi accelerators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02140</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02140</id><created>2015-09-07</created><authors><author><keyname>Milani</keyname><forenames>Alessia</forenames></author><author><keyname>Mosteiro</keyname><forenames>Miguel A.</forenames></author></authors><title>A Faster Counting Protocol for Anonymous Dynamic Networks</title><categories>cs.DC cs.DS</categories><msc-class>68W15</msc-class><acm-class>F.2.0</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of counting the number of nodes in a slotted-time
communication network, under the challenging assumption that nodes do not have
identifiers and the network topology changes frequently. That is, for each time
slot links among nodes can change arbitrarily provided that the network is
always connected. Tolerating dynamic topologies is crucial in face of mobility
and unreliable communication whereas, even if identifiers are available, it
might be convenient to ignore them in massive networks with changing topology.
Counting is a fundamental task in distributed computing since knowing the size
of the system often facilitates the design of solutions for more complex
problems. Currently, the best upper bound proved on the running time to compute
the exact network size is double-exponential. However, only linear complexity
lower bounds are known, leaving open the question of whether efficient Counting
protocols for Anonymous Dynamic Networks exist or not. In this paper we make a
significant step towards answering this question by presenting a distributed
Counting protocol for Anonymous Dynamic Networks which has exponential time
complexity. Our algorithm ensures that eventually every node knows the exact
size of the system and stops executing the algorithm. Previous Counting
protocols have either double-exponential time complexity, or they are
exponential but do not terminate, or terminate but do not provide running-time
guarantees, or guarantee only an exponential upper bound on the network size.
Other protocols are heuristic and do not guarantee the correct count.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02151</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02151</id><created>2015-09-07</created><updated>2015-09-08</updated><authors><author><keyname>Ritchie</keyname><forenames>Daniel</forenames></author><author><keyname>Stuhlm&#xfc;ller</keyname><forenames>Andreas</forenames></author><author><keyname>Goodman</keyname><forenames>Noah D.</forenames></author></authors><title>C3: Lightweight Incrementalized MCMC for Probabilistic Programs using
  Continuations and Callsite Caching</title><categories>cs.AI cs.PL</categories><comments>Fix typo in author name</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lightweight, source-to-source transformation approaches to implementing MCMC
for probabilistic programming languages are popular for their simplicity,
support of existing deterministic code, and ability to execute on existing fast
runtimes. However, they are also slow, requiring a complete re-execution of the
program on every Metropolis Hastings proposal. We present a new extension to
the lightweight approach, C3, which enables efficient, incrementalized
re-execution of MH proposals. C3 is based on two core ideas: transforming
probabilistic programs into continuation passing style (CPS), and caching the
results of function calls. We show that on several common models, C3 reduces
proposal runtime by 20-100x, in some cases reducing runtime complexity from
linear in model size to constant. We also demonstrate nearly an order of
magnitude speedup on a complex inverse procedural modeling application.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02152</identifier>
 <datestamp>2015-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02152</id><created>2015-09-07</created><updated>2015-09-15</updated><authors><author><keyname>Nie</keyname><forenames>Ding</forenames></author><author><keyname>Hochwald</keyname><forenames>Bertrand M.</forenames></author></authors><title>Bandwidth Analysis of Multiport Radio-Frequency Systems</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Antennas and Propagation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When multiple radio-frequency sources are connected to multiple loads through
a passive multiport matching network, perfect power transfer to the loads
across all frequencies is generally impossible. We provide an analysis of
bandwidth over which power transfer is possible. Our principal tools include
broadband multiport matching upper bounds, presented herein, on the integral
over all frequency of the logarithm of a suitably defined power loss ratio. In
general, the larger the integral, the wider the bandwidth over which power
transfer can be accomplished. We apply these bounds in several ways: We show
how the number of sources and loads, and the coupling between loads, affect
achievable bandwidth. We analyze the bandwidth of networks constrained to have
certain architectures. We characterize systems whose bandwidths scale as the
ratio between the numbers of loads and sources. Numerical examples are also
presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02153</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02153</id><created>2015-09-07</created><authors><author><keyname>Si</keyname><forenames>Wei</forenames></author><author><keyname>Starobinski</keyname><forenames>David</forenames></author><author><keyname>Laifenfeld</keyname><forenames>Moshe</forenames></author></authors><title>Hybrid-BCP: A Robust Load Balancing and Routing Protocol for Intra-Car
  Wired/Wireless Networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the emergence of connected and autonomous vehicles, sensors are
increasingly deployed within cars to support new functionalities. Traffic
generated by these sensors congest traditional intra-car networks, such as CAN
buses. Furthermore, the large amount of wires needed to connect sensors makes
it harder to design cars in a modular way. To alleviate these limitations, we
propose, simulate, and implement a hybrid wired/wireless architecture, in which
each node is connected to either a wired interface or a wireless interface or
both. Specifically, we propose a new protocol, called Hybrid-Backpressure
Collection Protocol (Hybrid-BCP), to efficiently collect data from sensors in
intra-car networks. Hybrid-BCP is backward-compatible with the CAN bus
technology, and builds on the BCP protocol, designed for wireless sensor
networks. Hybrid-BCP achieves high throughput and shows resilience to dynamic
network conditions, including adversarial interferences. Our testbed
implementation, based on CAN and ZigBee transceivers, demonstrates the load
balancing and routing functionalities of Hybrid-BCP and its resilience to DoS
attacks. We further provide simulation results, obtained with the ns-3
simulator and based on real intra-car RSSI traces, that compare between the
performance of Hybrid-BCP and a tree-based collection protocol. Notably, the
simulations show that Hybrid-BCP can achieve the same performance as the
tree-based protocol while reducing the radio transmission power by a factor of
10.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02154</identifier>
 <datestamp>2015-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02154</id><created>2015-09-07</created><authors><author><keyname>Garcia-Saura</keyname><forenames>Carlos</forenames></author></authors><title>Self-calibration of a differential wheeled robot using only a gyroscope
  and a distance sensor</title><categories>cs.RO</categories><comments>MSc thesis supervised by Prof. Andrew J. Davison at Imperial College
  London</comments><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Research in mobile robotics often demands platforms that have an adequate
balance between cost and reliability. In the case of terrestrial robots, one of
the available options is the GNBot, an open-hardware project intended for the
evaluation of swarm search strategies. The lack of basic odometry sensors such
as wheel encoders had so far difficulted the implementation of an accurate
high-level controller in this platform. Thus, the aim of this thesis is to
improve motion control in the GNBot by incorporating a gyroscope whilst
maintaining the requisite of no wheel encoders. Among the problems that have
been tackled are: accurate in-place rotations, minimal drift during linear
motions, and arc-performing functionality. Additionally, the resulting
controller is calibrated autonomously by using both the gyroscope module and
the infrared rangefinder on board each robot, greatly simplifying the
calibration of large swarms. The report first explains the design decisions
that were made in order to implement the self-calibration routine, and then
evaluates the performance of the new motion controller by means of off-line
video tracking. The motion accuracy of the new controller is also compared with
the previously existing solution in an odor search experiment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02157</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02157</id><created>2015-09-07</created><authors><author><keyname>Yang</keyname><forenames>Yao</forenames></author></authors><title>Detecting Potential Instabilities of Numerical Algorithms</title><categories>cs.NA</categories><comments>This paper has a different perspectives about stability analysis
  axioms (forward stability, backward stability and mixed stability axioms as
  in Demmel, Kahan and Parlett's papers and teaching at Berkeley for numerical
  algorithms</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It has been the standard teaching of today that backward stability analysis
is taught as absolute, just as in Newtonian physics time is taught absolute
time. We will prove it is not true in general. It depends on algorithms. We
will prove that forward and mixed stability anlaysis are absolutely invalid
stability analysis in the sense that they have absolutely wrong reference
points for detecting huge element growth of any algoritms(if any), even an
&quot;ideal&quot; or &quot;desirable&quot; backward stability analysis is not so &quot;ideal&quot; or
&quot;desirable&quot; in general. Any of forward stable, backward stable and mixed stable
algorihms as in Demmel, Kahan , Parlett and other's papers and text books, see
Demmel(6) and Higham(8)may not be really stable at all because they may fail to
detect and expose any potential instabilities of the algorithm in corresponding
stability analysis. Therefore, it is impossible to prove an algorithm is stable
according to the standard teachin of today, just as it is impossible to prove a
mathematical equuation(Maxwell's) is a law of physics according to the standard
teaching in Newtonian physics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02182</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02182</id><created>2015-09-07</created><authors><author><keyname>Schaefer</keyname><forenames>Rafael F.</forenames></author><author><keyname>Loyka</keyname><forenames>Sergey</forenames></author></authors><title>The Secrecy Capacity of Compound Gaussian MIMO Wiretap Channels</title><categories>cs.IT math.IT</categories><comments>accepted for publication in IEEE Trans. Inf. Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Strong secrecy capacity of compound wiretap channels is studied. The known
lower bounds for the secrecy capacity of compound finite-state memoryless
channels under discrete alphabets are extended to arbitrary uncertainty sets
and continuous alphabets under the strong secrecy criterion. The conditions
under which these bounds are tight are given. Under the saddle-point condition,
the compound secrecy capacity is shown to be equal to that of the worst-case
channel. Based on this, the compound Gaussian MIMO wiretap channel is studied
under the spectral norm constraint and without the degradedness assumption.
First, it is assumed that only the eavesdropper channel is unknown, but is
known to have a bounded spectral norm (maximum channel gain). The compound
secrecy capacity is established in a closed form and the optimal signaling is
identified: the compound capacity equals the worst-case channel capacity thus
establishing the saddle-point property; the optimal signaling is Gaussian and
on the eigenvectors of the legitimate channel and the worst-case eavesdropper
is isotropic. The eigenmode power allocation somewhat resembles the standard
water-filling but is not identical to it. More general uncertainty sets are
considered and the existence of a maximum element is shown to be sufficient for
a saddle-point to exist, so that signaling on the worst-case channel achieves
the compound capacity of the whole class of channels. The case of
rank-constrained eavesdropper is considered and the respective compound secrecy
capacity is established. Subsequently, the case of additive uncertainty in the
legitimate channel, in addition to the unknown eavesdropper channel, is
studied. Its compound secrecy capacity and the optimal signaling are
established in a closed-form as well, revealing the same saddle-point property.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02190</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02190</id><created>2015-09-07</created><updated>2015-10-15</updated><authors><author><keyname>Sekeh</keyname><forenames>Salimeh Yasaei</forenames></author></authors><title>Extended inequalities for weighted Renyi entropy involving generalized
  Gaussian densities</title><categories>cs.IT math.IT math.PR</categories><comments>17 pages</comments><msc-class>60A10, 60B05, 60C05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper the author analyses the weighted Renyi entropy in order to
derive several inequalities in weighted case. Furthermore, using the proposed
notions $\alpha$-th generalized derivation and ($\alpha$; p)-th weighted Fisher
information, extended versions of the moment-entropy, Fisher information and
Cramer-Rao inequalities in terms of generalized Gaussian densities are given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02195</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02195</id><created>2015-09-07</created><authors><author><keyname>Exarchos</keyname><forenames>Ioannis</forenames></author><author><keyname>Theodorou</keyname><forenames>Evangelos A.</forenames></author></authors><title>Learning Optimal Control via Forward and Backward Stochastic
  Differential Equations</title><categories>cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a novel sampling-based numerical scheme designed to
solve a certain class of stochastic optimal control problems, utilizing forward
and backward stochastic differential equations (FBSDEs). By means of a
nonlinear version of the famous Feynman-Kac lemma, we obtain a probabilistic
representation of the solution to the Hamilton-Jacobi-Bellman equation,
expressed in form of a system of decoupled FBSDEs. This system of FBSDEs can
then be simulated employing linear regression techniques. To enhance the
efficiency of the proposed scheme when treating more complex nonlinear systems,
we then derive an iterative modification based on Girsanov's theorem on the
change of measure, which features importance sampling. The modified scheme is
capable of learning the optimal control without requiring an initial guess. We
present simulations that validate the algorithm by means of a direct comparison
to a closed form LQR solution, and demonstrate its efficiency in treating
nonlinear dynamics as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02200</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02200</id><created>2015-09-07</created><updated>2015-09-18</updated><authors><author><keyname>Vaezi</keyname><forenames>Mojtaba</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>The Capacity Region of the One-Sided Gaussian Interference Channel</title><categories>cs.IT math.IT</categories><comments>This paper has been withdrawn due to a flaw in the proof of the outer
  bound</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The capacity region of the one-sided Gaussian interference channel is
established in the weak interference regime. To characterize this region, a new
representation of the Han-Kobayashi inner bound for the one-sided Gaussian
interference channel is first given. Next, a new outer bound on the capacity
region of this channel is introduced which is tight in the weak interference
regime. This is the first capacity region for any variant of the interference
channel in the weak interference regime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02207</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02207</id><created>2015-09-07</created><authors><author><keyname>Carlsen</keyname><forenames>Fredrik Nyg&#xe5;rd</forenames></author></authors><title>Personalized Search</title><categories>cs.IR cs.DL</categories><report-no>CERN-IT-Note-2015-007</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As the volume of electronically available information grows, relevant items
become harder to find. This work presents an approach to personalizing search
results in scientific publication databases. This work focuses on re-ranking
search results from existing search engines like Solr or ElasticSearch. This
work also includes the development of Obelix, a new recommendation system used
to re-rank search results. The project was proposed and performed at CERN,
using the scientific publications available on the CERN Document Server (CDS).
This work experiments with re-ranking using offline and online evaluation of
users and documents in CDS. The experiments conclude that the personalized
search result outperform both latest first and word similarity in terms of
click position in the search result for global search in CDS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02208</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02208</id><created>2015-09-07</created><authors><author><keyname>Chung</keyname><forenames>Cheng-Tao</forenames></author><author><keyname>Chan</keyname><forenames>Chun-an</forenames></author><author><keyname>Lee</keyname><forenames>Lin-shan</forenames></author></authors><title>Unsupervised Discovery of Linguistic Structure Including Two-level
  Acoustic Patterns Using Three Cascaded Stages of Iterative Optimization</title><categories>cs.CL</categories><comments>Accepted by ICASSP 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Techniques for unsupervised discovery of acoustic patterns are getting
increasingly attractive, because huge quantities of speech data are becoming
available but manual annotations remain hard to acquire. In this paper, we
propose an approach for unsupervised discovery of linguistic structure for the
target spoken language given raw speech data. This linguistic structure
includes two-level (subword-like and word-like) acoustic patterns, the lexicon
of word-like patterns in terms of subword-like patterns and the N-gram language
model based on word-like patterns. All patterns, models, and parameters can be
automatically learned from the unlabelled speech corpus. This is achieved by an
initialization step followed by three cascaded stages for acoustic, linguistic,
and lexical iterative optimization. The lexicon of word-like patterns defines
allowed consecutive sequence of HMMs for subword-like patterns. In each
iteration, model training and decoding produces updated labels from which the
lexicon and HMMs can be further updated. In this way, model parameters and
decoded labels are respectively optimized in each iteration, and the knowledge
about the linguistic structure is learned gradually layer after layer. The
proposed approach was tested in preliminary experiments on a corpus of Mandarin
broadcast news, including a task of spoken term detection with performance
compared to a parallel test using models trained in a supervised way. Results
show that the proposed system not only yields reasonable performance on its
own, but is also complimentary to existing large vocabulary ASR systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02213</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02213</id><created>2015-09-07</created><authors><author><keyname>Chung</keyname><forenames>Cheng-Tao</forenames></author><author><keyname>Chan</keyname><forenames>Chun-an</forenames></author><author><keyname>Lee</keyname><forenames>Lin-shan</forenames></author></authors><title>Unsupervised Spoken Term Detection with Spoken Queries by Multi-level
  Acoustic Patterns with Varying Model Granularity</title><categories>cs.CL</categories><comments>Accepted by ICASSP 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new approach for unsupervised Spoken Term Detection
with spoken queries using multiple sets of acoustic patterns automatically
discovered from the target corpus. The different pattern HMM
configurations(number of states per model, number of distinct models, number of
Gaussians per state)form a three-dimensional model granularity space. Different
sets of acoustic patterns automatically discovered on different points properly
distributed over this three-dimensional space are complementary to one another,
thus can jointly capture the characteristics of the spoken terms. By
representing the spoken content and spoken query as sequences of acoustic
patterns, a series of approaches for matching the pattern index sequences while
considering the signal variations are developed. In this way, not only the
on-line computation load can be reduced, but the signal distributions caused by
different speakers and acoustic conditions can be reasonably taken care of. The
results indicate that this approach significantly outperformed the unsupervised
feature-based DTW baseline by 16.16\% in mean average precision on the TIMIT
corpus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02217</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02217</id><created>2015-09-07</created><authors><author><keyname>Chung</keyname><forenames>Cheng-Tao</forenames></author><author><keyname>Hsu</keyname><forenames>Wei-Ning</forenames></author><author><keyname>Lee</keyname><forenames>Cheng-Yi</forenames></author><author><keyname>Lee</keyname><forenames>Lin-Shan</forenames></author></authors><title>Enhancing Automatically Discovered Multi-level Acoustic Patterns
  Considering Context Consistency With Applications in Spoken Term Detection</title><categories>cs.CL</categories><comments>Accepted by ICASSP 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel approach for enhancing the multiple sets of
acoustic patterns automatically discovered from a given corpus. In a previous
work it was proposed that different HMM configurations (number of states per
model, number of distinct models) for the acoustic patterns form a
two-dimensional space. Multiple sets of acoustic patterns automatically
discovered with the HMM configurations properly located on different points
over this two-dimensional space were shown to be complementary to one another,
jointly capturing the characteristics of the given corpus. By representing the
given corpus as sequences of acoustic patterns on different HMM sets, the
pattern indices in these sequences can be relabeled considering the context
consistency across the different sequences. Good improvements were observed in
preliminary experiments of pattern spoken term detection (STD) performed on
both TIMIT and Mandarin Broadcast News with such enhanced patterns.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02218</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02218</id><created>2015-09-07</created><authors><author><keyname>Yoshida</keyname><forenames>Mitsuo</forenames></author><author><keyname>Arase</keyname><forenames>Yuki</forenames></author><author><keyname>Tsunoda</keyname><forenames>Takaaki</forenames></author><author><keyname>Yamamoto</keyname><forenames>Mikio</forenames></author></authors><title>Wikipedia Page View Reflects Web Search Trend</title><categories>cs.SI cs.IR</categories><comments>2 pages, 4 figures, The 2015 ACM Web Science Conference (WebSci15)</comments><acm-class>H.3.3; H.3.5</acm-class><doi>10.1145/2786451.2786495</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The frequency of a web search keyword generally reflects the degree of public
interest in a particular subject matter. Search logs are therefore useful
resources for trend analysis. However, access to search logs is typically
restricted to search engine providers. In this paper, we investigate whether
search frequency can be estimated from a different resource such as Wikipedia
page views of open data. We found frequently searched keywords to have
remarkably high correlations with Wikipedia page views. This suggests that
Wikipedia page views can be an effective tool for determining popular global
web search trends.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02223</identifier>
 <datestamp>2016-01-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02223</id><created>2015-09-07</created><updated>2016-01-26</updated><authors><author><keyname>Gorokh</keyname><forenames>Artur</forenames></author><author><keyname>Korolev</keyname><forenames>Yury</forenames></author><author><keyname>Valkonen</keyname><forenames>Tuomo</forenames></author></authors><title>Diffusion tensor imaging with deterministic error bounds</title><categories>cs.CV cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Errors in the data and the forward operator of an inverse problem can be
handily modelled using partial order in Banach lattices. We present some
existing results of the theory of regularisation in this novel framework, where
errors are represented as bounds by means of the appropriate partial order.
  We apply the theory to Diffusion Tensor Imaging, where correct noise
modelling is challenging: it involves the Rician distribution and the nonlinear
Stejskal-Tanner equation. Linearisation of the latter in the statistical
framework would complicate the noise model even further. We avoid this using
the error bounds approach, which preserves simple error structure under
monotone transformations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02235</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02235</id><created>2015-09-07</created><authors><author><keyname>Malakhov</keyname><forenames>Anton</forenames></author></authors><title>Per-bucket concurrent rehashing algorithms</title><categories>cs.DS</categories><comments>The author is one of core developers of Intel Threading Building
  Blocks (TBB) and the algorithm discussed in the paper is implemented as
  tbb::concurrent_hash_map since TBB version 2.2. The paper compares it with
  tbb::concurrent_hash_map implementation available in TBB 2.1. This paper was
  written in 2011 for the last of 3 attempts to be accepted for a conference
  (PPoPP10, PPoPP11, and SPAA11)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a generic algorithm for concurrent resizing and
on-demand per-bucket rehashing for an extensible hash table. In contrast to
known lock-based hash table algorithms, the proposed algorithm separates the
resizing and rehashing stages so that they neither invalidate existing buckets
nor block any concurrent operations. Instead, the rehashing work is deferred
and split across subsequent operations with the table. The rehashing operation
uses bucket-level synchronization only and therefore allows a race condition
between lookup and moving operations running in different threads. Instead of
using explicit synchronization, the algorithm detects the race condition and
restarts the lookup operation. In comparison with other lock-based algorithms,
the proposed algorithm reduces high-level synchronization on the hot path,
improving performance, concurrency, and scalability of the table. The response
time of the operations is also more predictable. The algorithm is compatible
with cache friendly data layouts for buckets and does not depend on any memory
reclamation techniques thus potentially achieving additional performance gain
with corresponding implementations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02238</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02238</id><created>2015-09-07</created><authors><author><keyname>Li</keyname><forenames>Fangfang</forenames></author><author><keyname>Zhao</keyname><forenames>Yanchang</forenames></author><author><keyname>Felsche</keyname><forenames>Klaus</forenames></author><author><keyname>Xu</keyname><forenames>Guandong</forenames></author><author><keyname>Cao</keyname><forenames>Longbing</forenames></author></authors><title>Coupling Analysis Between Twitter and Call Centre</title><categories>cs.SI</categories><comments>19 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social media has been contributing many research areas such as data mining,
recommender systems, time series analysis, etc. However, there are not many
successful applications regarding social media in government agencies. In fact,
lots of governments have social media accounts such as twitter and facebook.
More and more customers are likely to communicate with governments on social
media, causing massive external social media data for governments. This
external data would be bene?ficial for analysing behaviours and real needs of
the customers. Besides this, most governments also have a call centre to help
customers solve their problems. It is not diffi?cult to imagine that the
enquiries on external social media and internal call centre may have some
coupling relationships. The couplings could be helpful for studying customers'
intent and allocating government's limited resources for better service. In
this paper, we mainly focus on analysing the coupling relations between
internal call centre and external public media using time series analysis
methods for Australia Department of Immigration and Border Protec-tion. The
discovered couplings demonstrate that call centre and public media indeed have
correlations, which are signi?cant for understanding customers' behaviours.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02256</identifier>
 <datestamp>2016-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02256</id><created>2015-09-08</created><updated>2015-12-30</updated><authors><author><keyname>Zadeh</keyname><forenames>Reza Bosagh</forenames></author><author><keyname>Meng</keyname><forenames>Xiangrui</forenames></author><author><keyname>Staple</keyname><forenames>Aaron</forenames></author><author><keyname>Yavuz</keyname><forenames>Burak</forenames></author><author><keyname>Pu</keyname><forenames>Li</forenames></author><author><keyname>Venkataraman</keyname><forenames>Shivaram</forenames></author><author><keyname>Sparks</keyname><forenames>Evan</forenames></author><author><keyname>Ulanov</keyname><forenames>Alexander</forenames></author><author><keyname>Zaharia</keyname><forenames>Matei</forenames></author></authors><title>Matrix Computations and Optimization in Apache Spark</title><categories>cs.DC</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We describe matrix computations available in the cluster programming
framework, Apache Spark. Out of the box, Spark provides abstractions and
implementations for distributed matrices and optimization routines using these
matrices. When translating single-node algorithms to run on a distributed
cluster, we observe that often a simple idea is enough: separating matrix
operations from vector operations and shipping the matrix operations to be ran
on the cluster, while keeping vector operations local to the driver. In the
case of the Singular Value Decomposition, by taking this idea to an extreme, we
are able to exploit the computational power of a cluster, while running code
written decades ago for a single core. Another example is our Spark port of the
popular TFOCS optimization package, originally built for MATLAB, which allows
for solving Linear programs as well as a variety of other convex programs. We
conclude with a comprehensive set of benchmarks for hardware accelerated matrix
computations from the JVM, which is interesting in its own right, as many
cluster programming frameworks use the JVM. The contributions described in this
paper are already merged into Apache Spark and available on Spark installations
by default, and commercially supported by a slew of companies which provide
further services.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02267</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02267</id><created>2015-09-08</created><authors><author><keyname>Nishimura</keyname><forenames>Yuki</forenames></author></authors><title>Stabilization by Unbounded-Variation Noises</title><categories>cs.SY math.OC</categories><comments>22 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we claim the availability of deterministic noises for
stabilization of the origins of dynamical systems, provided that the noises
have unbounded variations. To achieve the result, we first consider the system
representations based on rough path analysis; then, we provide the notion of
asymptotic stability in roughness to analyze the stability for the systems. In
the procedure, we also confirm that the system representations include
stochastic differential equations; we also found that asymptotic stability in
roughness is the same property as uniform almost sure asymptotic stability
provided by Bardi and Cesaroni. After the discussion, we confirm that there is
a case that deterministic noises are capable of making the origin become
asymptotically stable in roughness while stochastic noises do not achieve the
same stabilization results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02268</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02268</id><created>2015-09-08</created><authors><author><keyname>Kung</keyname><forenames>Yi-Hsuan</forenames></author><author><keyname>Lee</keyname><forenames>Taeho</forenames></author><author><keyname>Tseng</keyname><forenames>Po-Ning</forenames></author><author><keyname>Hsiao</keyname><forenames>Hsu-Chun</forenames></author><author><keyname>Kim</keyname><forenames>Tiffany Hyun-Jin</forenames></author><author><keyname>Lee</keyname><forenames>Soo Bum</forenames></author><author><keyname>Lin</keyname><forenames>Yue-Hsun</forenames></author><author><keyname>Perrig</keyname><forenames>Adrian</forenames></author></authors><title>A Practical System for Guaranteed Access in the Presence of DDoS Attacks
  and Flash Crowds</title><categories>cs.CR</categories><comments>16 pages, a full technical report for 'A Practical System for
  Guaranteed Access in the Presence of DDoS Attacks and Flash Crowds' in IEEE
  International Conference on Network Protocols, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the growing incidents of flash crowds and sophisticated DDoS attacks
mimicking benign traffic, it becomes challenging to protect Internet-based
services solely by differentiating attack traffic from legitimate traffic.
While fair-sharing schemes are commonly suggested as a defense when
differentiation is difficult, they alone may suffer from highly variable or
even unbounded waiting times. We propose RainCheck Filter (RCF), a lightweight
primitive that guarantees bounded waiting time for clients despite server
flooding without keeping per-client state on the server. RCF achieves strong
waiting time guarantees by prioritizing clients based on how long the clients
have waited-as if the server maintained a queue in which the clients lined up
waiting for service. To avoid keeping state for every incoming client request,
the server sends to the client a raincheck, a timestamped cryptographic token
that not only informs the client to retry later but also serves as a proof of
the client's priority level within the virtual queue. We prove that every
client complying with RCF can access the server in bounded time, even under a
flash crowd incident or a DDoS attack. Our large-scale simulations confirm that
RCF provides a small and predictable maximum waiting time while existing
schemes cannot. To demonstrate its deployability, we implement RCF as a Python
module such that web developers can protect a critical server resource by
adding only three lines of code.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02289</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02289</id><created>2015-09-08</created><authors><author><keyname>Kolassa</keyname><forenames>Carsten</forenames></author><author><keyname>Rendel</keyname><forenames>Holger</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>Evaluation of Variability Concepts for Simulink in the Automotive Domain</title><categories>cs.SE</categories><comments>10 pages, 7 figures, 6 tables, Proceedings of 48th Hawaii
  International Conference on System Sciences (HICSS), pp. 5373-5382, Kauai,
  Hawaii, USA, IEEE Computer Society, 2015</comments><acm-class>D.2; K.6.3; D.2.1; D.2.2</acm-class><journal-ref>Proceedings of 48th Hawaii International Conference on System
  Sciences (HICSS), pp. 5373-5382, Kauai, Hawaii, USA, IEEE Computer Society,
  2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modeling variability in Matlab/Simulink becomes more and more important. We
took the two variability modeling concepts already included in Matlab/Simulink
and our own one and evaluated them to find out which one is suited best for
modeling variability in the automotive domain. We conducted a controlled
experiment with developers at Volkswagen AG to decide which concept is
preferred by developers and if their preference aligns with measurable
performance factors. We found out that all existing concepts are viable
approaches and that the delta approach is both the preferred concept as well as
the objectively most efficient one, which makes Delta-Simulink a good solution
to model variability in the automotive domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02291</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02291</id><created>2015-09-08</created><authors><author><keyname>Roth</keyname><forenames>Alexander</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>Towards Product Lining Model-Driven Development Code Generators</title><categories>cs.SE</categories><comments>6 pages, 1 figure, Proceedings of the 3rd International Conference on
  Model-Driven Engineering and Software Development, pp. 539-545, Angers,
  France, SciTePress, 2015</comments><journal-ref>Proceedings of the 3rd International Conference on Model-Driven
  Engineering and Software Development, pp. 539-545, Angers, France,
  SciTePress, 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A code generator systematically transforms compact models to detailed code.
Today, code generation is regarded as an integral part of model-driven
development (MDD). Despite its relevance, the development of code generators is
an inherently complex task and common methodologies and architectures are
lacking. Additionally, reuse and extension of existing code generators only
exist on individual parts. A systematic development and reuse based on a code
generator product line is still in its infancy. Thus, the aim of this paper is
to identify the mechanism necessary for a code generator product line by (a)
analyzing the common product line development approach and (b) mapping those to
a code generator specific infrastructure. As a first step towards realizing a
code generator product line infrastructure, we present a component-based
implementation approach based on ideas of variability-aware module systems and
point out further research challenges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02293</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02293</id><created>2015-09-08</created><authors><author><keyname>Nazari</keyname><forenames>Pedram Mir Seyed</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>Using Software Categories for the Development of Generative Software</title><categories>cs.SE</categories><comments>6 pages, 7 figures, Proceedings of the 3rd International Conference
  on Model-Driven Engineering and Software Development, pp. 498-503, Angers,
  France, SciTePress, 2015</comments><journal-ref>Proceedings of the 3rd International Conference on Model-Driven
  Engineering and Software Development, pp. 498-503, Angers, France,
  SciTePress, 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In model-driven development (MDD) software emerges by systematically
transforming abstract models to concrete source code. Ideally, performing those
transformations is to a large extent the task of code generators. One approach
for developing a new code generator is to write a reference implementation and
separate it into handwritten and generatable code. Typically, the generator
developer manually performs this separation a process that is often
time-consuming, labor-intensive, difficult to maintain and may produce more
code than necessary. Software categories provide a way for separating code into
designated parts with defined dependencies, for example, &quot;Business Logic&quot; code
that may not directly use &quot;Technical&quot; code. This paper presents an approach
that uses the concept of software categories to semi-automatically determine
candidates for generated code. The main idea is to iteratively derive the
categories for uncategorized code from the dependencies of categorized code.
The candidates for generated or handwritten code finally are code parts
belonging to specific (previously defined) categories. This approach helps the
generator developer in finding candidates for generated code more easily and
systematically than searching by hand and is a step towards tool-supported
development of generative software.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02297</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02297</id><created>2015-09-08</created><authors><author><keyname>Nguyen</keyname><forenames>Phan-Minh</forenames></author><author><keyname>Armand</keyname><forenames>Marc A.</forenames></author></authors><title>On Capacity Formulation with Stationary Inputs and Application to a
  Bit-Patterned Media Recording Channel Model</title><categories>cs.IT math.IT</categories><comments>25 pages, 10 figures, to appear in IEEE Transactions on Information
  Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this correspondence, we illustrate among other things the use of the
stationarity property of the set of capacity-achieving inputs in capacity
calculations. In particular, as a case study, we consider a bit-patterned media
recording channel model and formulate new lower and upper bounds on its
capacity that yield improvements over existing results. Inspired by the
observation that the new bounds are tight at low noise levels, we also
characterize the capacity of this model as a series expansion in the low-noise
regime.
  The key to these results is the realization of stationarity in the
supremizing input set in the capacity formula. While the property is prevalent
in capacity formulations in the ergodic-theoretic literature, we show that this
realization is possible in the Shannon-theoretic framework where a channel is
defined as a sequence of finite-dimensional conditional probabilities, by
defining a new class of consistent stationary and ergodic channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02301</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02301</id><created>2015-09-08</created><updated>2016-01-29</updated><authors><author><keyname>Ganea</keyname><forenames>Octavian-Eugen</forenames></author><author><keyname>Ganea</keyname><forenames>Marina</forenames></author><author><keyname>Lucchi</keyname><forenames>Aurelien</forenames></author><author><keyname>Eickhoff</keyname><forenames>Carsten</forenames></author><author><keyname>Hofmann</keyname><forenames>Thomas</forenames></author></authors><title>Probabilistic Bag-Of-Hyperlinks Model for Entity Linking</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many fundamental problems in natural language processing rely on determining
what entities appear in a given text. Commonly referenced as entity linking,
this step is a fundamental component of many NLP tasks such as text
understanding, automatic summarization, semantic search or machine translation.
Name ambiguity, word polysemy, context dependencies and a heavy-tailed
distribution of entities contribute to the complexity of this problem.
  We here propose a probabilistic approach that makes use of an effective
graphical model to perform collective entity disambiguation. Input mentions
(i.e.,~linkable token spans) are disambiguated jointly across an entire
document by combining a document-level prior of entity co-occurrences with
local information captured from mentions and their surrounding context. The
model is based on simple sufficient statistics extracted from data, thus
relying on few parameters to be learned.
  Our method does not require extensive feature engineering, nor an expensive
training procedure. We use loopy belief propagation to perform approximate
inference. The low complexity of our model makes this step sufficiently fast
for real-time usage. We demonstrate the accuracy of our approach on a wide
range of benchmark datasets, showing that it matches, and in many cases
outperforms, existing state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02302</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02302</id><created>2015-09-08</created><authors><author><keyname>Wan</keyname><forenames>Weiwei</forenames></author><author><keyname>Harada</keyname><forenames>Kensuke</forenames></author></authors><title>Developing and Comparing Single-arm and Dual-arm Regrasp</title><categories>cs.RO</categories><doi>10.1109/LRA.2016.2517147</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of this paper is to develop efficient regrasp algorithms for
single-arm and dual-arm regrasp and compares the performance of single-arm and
dual-arm regrasp by running the two algorithms thousands of times. We focus on
pick-and-place regrasp which reorients an object from one placement to another
by using a sequence of pick-ups and place-downs. After analyzing the simulation
results, we find dual-arm regrasp is not necessarily better than single-arm
regrasp: Dual-arm regrasp is flexible. When the two hands can grasp the object
with good clearance, dual-arm regrasp is better and has higher successful rate
than single-arm regrasp. However, dual-arm regrasp suffers from geometric
constraints caused by the two arms. When the grasps overlap, dual-arm regrasp
is bad. Developers need to sample grasps with high density to reduce
overlapping. This leads to exploded combinatorics in previous methods, but is
possible with the algorithms presented in this paper. Following the results,
practitioners may choose single-arm or dual-arm robots by considering the
object shapes and grasps. Meanwhile, they can reduce overlapping and implement
practical dual-arm regrasp by using the presented algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02308</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02308</id><created>2015-09-08</created><authors><author><keyname>Mei</keyname><forenames>Xinxin</forenames></author><author><keyname>Chu</keyname><forenames>Xiaowen</forenames></author></authors><title>Dissecting GPU Memory Hierarchy through Microbenchmarking</title><categories>cs.AR cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Memory access efficiency is a key factor in fully utilizing the computational
power of graphics processing units (GPUs). However, many details of the GPU
memory hierarchy are not released by GPU vendors. In this paper, we propose a
novel fine-grained microbenchmarking approach and apply it to three generations
of NVIDIA GPUs, namely Fermi, Kepler and Maxwell, to expose the previously
unknown characteristics of their memory hierarchies. Specifically, we
investigate the structures of different GPU cache systems, such as the data
cache, the texture cache and the translation look-aside buffer (TLB). We also
investigate the throughput and access latency of GPU global memory and shared
memory. Our microbenchmark results offer a better understanding of the
mysterious GPU memory hierarchy, which will facilitate the software
optimization and modelling of GPU architectures. To the best of our knowledge,
this is the first study to reveal the cache properties of Kepler and Maxwell
GPUs, and the superiority of Maxwell in shared memory performance under bank
conflict.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02314</identifier>
 <datestamp>2016-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02314</id><created>2015-09-08</created><updated>2016-03-07</updated><authors><author><keyname>Zhao</keyname><forenames>Shenjian</forenames></author><author><keyname>Xie</keyname><forenames>Cong</forenames></author><author><keyname>Zhang</keyname><forenames>Zhihua</forenames></author></authors><title>A Scalable and Extensible Framework for Superposition-Structured Models</title><categories>cs.NA math.OC stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many learning tasks, structural models usually lead to better
interpretability and higher generalization performance. In recent years,
however, the simple structural models such as lasso are frequently proved to be
insufficient. Accordingly, there has been a lot of work on
&quot;superposition-structured&quot; models where multiple structural constraints are
imposed. To efficiently solve these &quot;superposition-structured&quot; statistical
models, we develop a framework based on a proximal Newton-type method.
Employing the smoothed conic dual approach with the LBFGS updating formula, we
propose a scalable and extensible proximal quasi-Newton (SEP-QN) framework.
Empirical analysis on various datasets shows that our framework is potentially
powerful, and achieves super-linear convergence rate for optimizing some
popular &quot;superposition-structured&quot; statistical models such as the fused sparse
group lasso.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02317</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02317</id><created>2015-09-08</created><authors><author><keyname>Gomez</keyname><forenames>Lluis</forenames></author><author><keyname>Karatzas</keyname><forenames>Dimosthenis</forenames></author></authors><title>Object Proposals for Text Extraction in the Wild</title><categories>cs.CV</categories><comments>13th International Conference on Document Analysis and Recognition
  (ICDAR 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Object Proposals is a recent computer vision technique receiving increasing
interest from the research community. Its main objective is to generate a
relatively small set of bounding box proposals that are most likely to contain
objects of interest. The use of Object Proposals techniques in the scene text
understanding field is innovative. Motivated by the success of powerful while
expensive techniques to recognize words in a holistic way, Object Proposals
techniques emerge as an alternative to the traditional text detectors.
  In this paper we study to what extent the existing generic Object Proposals
methods may be useful for scene text understanding. Also, we propose a new
Object Proposals algorithm that is specifically designed for text and compare
it with other generic methods in the state of the art. Experiments show that
our proposal is superior in its ability of producing good quality word
proposals in an efficient way. The source code of our method is made publicly
available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02320</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02320</id><created>2015-09-08</created><authors><author><keyname>Qi</keyname><forenames>Xianbiao</forenames></author><author><keyname>Zhao</keyname><forenames>Guoying</forenames></author><author><keyname>Chen</keyname><forenames>Jie</forenames></author><author><keyname>Pietik&#xe4;inen</keyname><forenames>Matti</forenames></author></authors><title>HEp-2 Cell Classification: The Role of Gaussian Scale Space Theory as A
  Pre-processing Approach</title><categories>cs.CV</categories><comments>9 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  \textit{Indirect Immunofluorescence Imaging of Human Epithelial Type 2}
(HEp-2) cells is an effective way to identify the presence of Anti-Nuclear
Antibody (ANA). Most existing works on HEp-2 cell classification mainly focus
on feature extraction, feature encoding and classifier design. Very few efforts
have been devoted to study the importance of the pre-processing techniques. In
this paper, we analyze the importance of the pre-processing, and investigate
the role of Gaussian Scale Space (GSS) theory as a pre-processing approach for
the HEp-2 cell classification task. We validate the GSS pre-processing under
the Local Binary Pattern (LBP) and the Bag-of-Words (BoW) frameworks. Under the
BoW framework, the introduced pre-processing approach, using only one Local
Orientation Adaptive Descriptor (LOAD), achieved superior performance on the
Executable Thematic on Pattern Recognition Techniques for Indirect
Immunofluorescence (ET-PRT-IIF) image analysis. Our system, using only one
feature, outperformed the winner of the ICPR 2014 contest that combined four
types of features. Meanwhile, the proposed pre-processing method is not
restricted to this work; it can be generalized to many existing works.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02322</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02322</id><created>2015-09-08</created><authors><author><keyname>Gu&#xe9;don</keyname><forenames>Olivier</forenames></author><author><keyname>Litvak</keyname><forenames>Alexander E.</forenames></author><author><keyname>Pajor</keyname><forenames>Alain</forenames></author><author><keyname>Tomczak-Jaegermann</keyname><forenames>Nicole</forenames></author></authors><title>On the interval of fluctuation of the singular values of random matrices</title><categories>math.PR cs.IT math.FA math.IT</categories><comments>To appear in J. Eur. Math. Soc</comments><msc-class>60B20, 46B06, 15B52, 46B09, 60D05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $A$ be a matrix whose columns $X_1,\dots, X_N$ are independent random
vectors in $\mathbb{R}^n$. Assume that the tails of the 1-dimensional marginals
decay as $\mathbb{P}(|\langle X_i, a\rangle|\geq t)\leq t^{-p}$ uniformly in
$a\in S^{n-1}$ and $i\leq N$. Then for $p&gt;4$ we prove that with high
probability $A/{\sqrt{n}}$ has the Restricted Isometry Property (RIP) provided
that Euclidean norms $|X_i|$ are concentrated around $\sqrt{n}$. We also show
that the covariance matrix is well approximated by the empirical covariance
matrix and establish corresponding quantitative estimates on the rate of
convergence in terms of the ratio $n/N$. Moreover, we obtain sharp bounds for
both problems when the decay is of the type $ \exp({-t^{\alpha}})$ with $\alpha
\in (0,2]$, extending the known case $\alpha\in[1, 2]$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02325</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02325</id><created>2015-09-08</created><authors><author><keyname>Georgiou</keyname><forenames>Orestis</forenames></author><author><keyname>Wang</keyname><forenames>Shanshan</forenames></author><author><keyname>Bocus</keyname><forenames>Mohammud Z.</forenames></author><author><keyname>Dettmann</keyname><forenames>Carl P.</forenames></author><author><keyname>Coon</keyname><forenames>Justin P.</forenames></author></authors><title>Directional antennas improve the link-connectivity of interference
  limited ad hoc networks</title><categories>cs.NI cs.IT math.IT</categories><comments>6 pages, 7 figures, conference proceedings of PIMRC'2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study wireless ad hoc networks in the absence of any channel contention or
transmit power control and ask how antenna directivity affects network
connectivity in the interference limited regime. We answer this question by
deriving closed-form expressions for the outage probability, capacity and mean
node degree of the network using tools from stochastic geometry. These novel
results provide valuable insights for the design of future ad hoc networks.
Significantly, our results suggest that the more directional the interfering
transmitters are, the less detrimental are the effects of interference to
individual links. We validate our analytical results through computer
simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02333</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02333</id><created>2015-09-08</created><authors><author><keyname>Peters</keyname><forenames>Dominik</forenames></author></authors><title>$\Sigma_2^p$-complete Problems on Hedonic Games</title><categories>cs.GT</categories><comments>7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hedonic games provide a general model of coalition formation, in which a set
of agents is partitioned into coalitions, with each agent having preferences
over which other players are in her coalition. We prove that with additively
separable preferences, it is $\Sigma_2^p$-complete to decide whether a core- or
strict-core-stable partition exists, extending a result of Woeginger (2013).
Our result holds even if valuations are symmetric and non-zero only for a
constant number of other agents. We also establish $\Sigma_2^p$-completeness of
deciding non-emptiness of the strict core for hedonic games with dichotomous
preferences. Of more general interest, we prove that TRUE-$\exists\forall$-3DNF
remains $\Sigma_2^p$-hard even if no variable occurs more than three times.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02335</identifier>
 <datestamp>2016-02-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02335</id><created>2015-09-08</created><updated>2016-02-11</updated><authors><author><keyname>Wu</keyname><forenames>Yongpeng</forenames></author><author><keyname>Wen</keyname><forenames>Chao-Kai</forenames></author><author><keyname>Ng</keyname><forenames>Derrick Wing Kwan</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author><author><keyname>Lozano</keyname><forenames>Angel</forenames></author></authors><title>Low-Complexity MIMO Precoding with Discrete Signals and Statistical CSI</title><categories>cs.IT math.IT</categories><comments>Accepted by ICC 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the design of multiple-input multiple-output
single-user precoders for finite-alphabet signals under the premise of
statistical channel-state information at the transmitter. Based on an
asymptotic expression for the mutual information of channels exhibiting antenna
correlations, we propose a low-complexity iterative algorithm that radically
reduces the computational load of existing approaches by orders of magnitude
with only minimal losses in performance. The savings increase with the number
of transmit antennas and with the cardinality of the signal alphabet, making it
possible to supports values thereof that were unwieldy in existing solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02337</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02337</id><created>2015-09-08</created><authors><author><keyname>Arieli</keyname><forenames>Itai</forenames></author><author><keyname>Babichenko</keyname><forenames>Yakov</forenames></author></authors><title>Random Extensive Form Games and its Application to Bargaining</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider two-player random extensive form games where the payoffs at the
leaves are independently drawn uniformly at random from a given feasible set C.
We study the asymptotic distribution of the subgame perfect equilibrium outcome
for binary-trees with increasing depth in various random (or deterministic)
assignments of players to nodes. We characterize the assignments under which
the asymptotic distribution concentrates around a point. Our analysis provides
a natural way to derive from the asymptotic distribution a novel solution
concept for two-player bargaining problems with a solid strategic
justification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02348</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02348</id><created>2015-09-08</created><authors><author><keyname>Lauer</keyname><forenames>Fabien</forenames><affiliation>ABC</affiliation></author></authors><title>On the complexity of piecewise affine system identification</title><categories>stat.ML cs.CC</categories><comments>Automatica, International Federation of Automatic Control, 2015</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper provides results regarding the computational complexity of hybrid
system identification. More precisely, we focus on the estimation of piecewise
affine (PWA) maps from input-output data and analyze the complexity of
computing a global minimizer of the error. Previous work showed that a global
solution could be obtained for continuous PWA maps with a worst-case complexity
exponential in the number of data. In this paper, we show how global optimality
can be reached for a slightly more general class of possibly discontinuous PWA
maps with a complexity only polynomial in the number of data, however with an
exponential complexity with respect to the data dimension. This result is
obtained via an analysis of the intrinsic classification subproblem of
associating the data points to the different modes. In addition, we prove that
the problem is NP-hard, and thus that the exponential complexity in the
dimension is a natural expectation for any exact algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02351</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02351</id><created>2015-09-08</created><authors><author><keyname>Z&#xf6;chmann</keyname><forenames>Erich</forenames></author><author><keyname>Schwarz</keyname><forenames>Stefan</forenames></author><author><keyname>Pratschner</keyname><forenames>Stefan</forenames></author><author><keyname>Nagel</keyname><forenames>Lukas</forenames></author><author><keyname>Lerch</keyname><forenames>Martin</forenames></author><author><keyname>Rupp</keyname><forenames>Markus</forenames></author></authors><title>Exploring the Physical Layer Frontiers of Cellular Uplink - The Vienna
  LTE-A Simulator</title><categories>cs.IT math.IT</categories><comments>submitted to Eurasip Journal on Wireless Communications and
  Networking on 07-Sep-2015, Manuscript ID: JWCN-D-15-00369</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Communication systems in practice are subject to many technical/technological
constraints and restrictions. MIMO processing in current wireless
communications, as an example, mostly employs codebook based pre-coding to save
computational complexity at the transmitters and receivers. In such cases,
closed form expressions for capacity or bit-error probability are often
unattainable; effects of realistic signal processing algorithms on the
performance of practical communication systems rather have to be studied in
simulation environments. The Vienna {LTE-A} Uplink Simulator is a 3GPP {LTE-A}
standard compliant link level simulator that is publicly available under an
academic use license, facilitating reproducible evaluations of signal
processing algorithms and transceiver designs in wireless communications. This
paper reviews research results that have been obtained by means of the Vienna
LTE-A Uplink Simulator, highlights the effects of Single Carrier Frequency
Division Multiplexing (as the distinguishing feature to LTE-A downlink),
extends known link adaptation concepts to uplink transmission, shows the
implications of the uplink pilot pattern for gathering Channel State
Information at the receiver and completes with possible future research
directions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02366</identifier>
 <datestamp>2015-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02366</id><created>2015-09-08</created><authors><author><keyname>Xu</keyname><forenames>Xiangru</forenames></author><author><keyname>Ozay</keyname><forenames>Necmiye</forenames></author><author><keyname>Gupta</keyname><forenames>Vijay</forenames></author></authors><title>Passivity Degradation In Discrete Control Implementations: An
  Approximate Bisimulation Approach</title><categories>math.OC cs.SY</categories><comments>This is an extended version of our IEEE CDC 2015 paper to appear in
  Japan</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present some preliminary results for compositional analysis
of heterogeneous systems containing both discrete state models and continuous
systems using consistent notions of dissipativity and passivity. We study the
following problem: given a physical plant model and a continuous feedback
controller designed using traditional control techniques, how is the
closed-loop passivity affected when the continuous controller is replaced by a
discrete (i.e., symbolic) implementation within this framework? Specifically,
we give quantitative results on performance degradation when the discrete
control implementation is approximately bisimilar to the continuous controller,
and based on them, we provide conditions that guarantee the boundedness
property of the closed-loop system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02374</identifier>
 <datestamp>2016-01-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02374</id><created>2015-09-08</created><updated>2016-01-04</updated><authors><author><keyname>Montanaro</keyname><forenames>Ashley</forenames></author></authors><title>Quantum walk speedup of backtracking algorithms</title><categories>quant-ph cs.DS</categories><comments>23 pages; v2: minor changes to presentation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a general method to obtain quantum speedups of classical
algorithms which are based on the technique of backtracking, a standard
approach for solving constraint satisfaction problems (CSPs). Backtracking
algorithms explore a tree whose vertices are partial solutions to a CSP in an
attempt to find a complete solution. Assume there is a classical backtracking
algorithm which finds a solution to a CSP on n variables, or outputs that none
exists, and whose corresponding tree contains T vertices, each vertex
corresponding to a test of a partial solution. Then we show that there is a
bounded-error quantum algorithm which completes the same task using O(sqrt(T)
n^(3/2) log n) tests. In particular, this quantum algorithm can be used to
speed up the DPLL algorithm, which is the basis of many of the most efficient
SAT solvers used in practice. The quantum algorithm is based on the use of a
quantum walk algorithm of Belovs to search in the backtracking tree. We also
discuss how, for certain distributions on the inputs, the algorithm can lead to
an exponential reduction in expected runtime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02380</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02380</id><created>2015-09-08</created><authors><author><keyname>Compagnoni</keyname><forenames>Marco</forenames></author><author><keyname>Canclini</keyname><forenames>Antonio</forenames></author><author><keyname>Bestagini</keyname><forenames>Paolo</forenames></author><author><keyname>Antonacci</keyname><forenames>Fabio</forenames></author><author><keyname>Sarti</keyname><forenames>Augusto</forenames></author><author><keyname>Tubaro</keyname><forenames>Stefano</forenames></author></authors><title>TDOA denoising for acoustic source localization</title><categories>cs.SD stat.ME</categories><comments>16 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this manuscript, we formulate the problem of denoising Time Differences of
Arrival (TDOAs) in the TDOA space, i.e. the Euclidean space spanned by TDOA
measurements. The method consists of pre-processing the TDOAs with the purpose
of reducing the measurement noise. The complete set of TDOAs (i.e., TDOAs
computed at all microphone pairs) is known to form a redundant set, which lies
on a linear subspace in the TDOA space. Noise, however, prevents TDOAs from
lying exactly on this subspace. We therefore show that TDOA denoising can be
seen as a projection operation that suppresses the component of the noise that
is orthogonal to that linear subspace. We then generalize the projection
operator also to the cases where the set of TDOAs is incomplete. We
analytically show that this operator improves the localization accuracy, and we
further confirm that via simulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02383</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02383</id><created>2015-09-08</created><authors><author><keyname>Carvalho</keyname><forenames>J. Frederico</forenames></author><author><keyname>Pequito</keyname><forenames>Sergio</forenames></author><author><keyname>Aguiar</keyname><forenames>A. Pedro</forenames></author><author><keyname>Kar</keyname><forenames>Soummya</forenames></author><author><keyname>Pappas</keyname><forenames>George J.</forenames></author></authors><title>Static Output Feedback: On Essential Feasible Information Patterns</title><categories>math.OC cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, for linear time-invariant plants, where a collection of
possible inputs and outputs are known a priori, we address the problem of
determining the communication between outputs and inputs, i.e., information
patterns, such that desired control objectives of the closed-loop system (for
instance, stabilizability) through static output feedback may be ensured.
  We address this problem in the structural system theoretic context. To this
end, given a specified structural pattern (locations of zeros/non-zeros) of the
plant matrices, we introduce the concept of essential information patterns,
i.e., communication patterns between outputs and inputs that satisfy the
following conditions: (i) ensure arbitrary spectrum assignment of the
closed-loop system, using static output feedback constrained to the information
pattern, for almost all possible plant instances with the specified structural
pattern; and (ii) any communication failure precludes the resulting information
pattern from attaining the pole placement objective in (i).
  Subsequently, we study the problem of determining essential information
patterns. First, we provide several necessary and sufficient conditions to
verify whether a specified information pattern is essential or not. Further, we
show that such conditions can be verified by resorting to algorithms with
polynomial complexity (in the dimensions of the state, input and output).
Although such verification can be performed efficiently, it is shown that the
problem of determining essential information patterns is in general NP-hard.
The main results of the paper are illustrated through examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02384</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02384</id><created>2015-09-08</created><authors><author><keyname>Kramer</keyname><forenames>Arthur</forenames></author><author><keyname>Subramanian</keyname><forenames>Anand</forenames></author></authors><title>A unified heuristic and an annotated bibliography for a large class of
  earliness-tardiness scheduling problems</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work proposes a unified heuristic algorithm for a large class of
earliness-tardiness (E-T) scheduling problems. We consider single/parallel
machine E-T problems that may or may not consider some additional features such
as idle time, setup times and release dates. In addition, we also consider
those problems whose objective is to minimize either the total (average)
weighted completion time or the total (average) weighted flow time, which arise
as particular cases when the due dates of all jobs are either set to zero or to
their associated release dates, respectively. The developed local search based
metaheuristic framework is quite simple, but at the same time relies on
sophisticated procedures for efficiently performing local search according to
the characteristics of the problem. We present efficient move evaluation
approaches for some parallel machine problems that generalize the existing ones
for single machine problems. The algorithm was tested in hundreds of instances
of several E-T problems and particular cases. The results obtained show that
our unified heuristic is capable of producing high quality solutions when
compared to the best ones available in the literature that were obtained by
specific methods. Moreover, we provide an extensive annotated bibliography on
the problems related to those considered in this work, where we not only
indicate the approach(es) used in each publication, but we also point out the
characteristics of the problem(s) considered. Beyond that, we classify the
existing methods in different categories so as to have a better idea of the
popularity of each type of solution procedure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02400</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02400</id><created>2015-08-21</created><authors><author><keyname>Abouzar</keyname><forenames>Pooyan</forenames></author><author><keyname>Michelson</keyname><forenames>David G.</forenames></author><author><keyname>Hamdi</keyname><forenames>Maziyar</forenames></author></authors><title>RSSI-Based Distributed Self-Localization for Wireless Sensor Networks
  used in Precision Agriculture</title><categories>cs.DC</categories><comments>23 pages, 11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Node localization algorithms that can be easily integrated into deployed
wireless sensor networks (WSNs) and which run seamlessly with proprietary lower
layer communication protocols running on off-the-shelf modules can help
operators of large farms and orchards avoid the difficulty, cost and/or time
involved with manual or satellite-based node localization techniques. Even
though the state-of-the-art node localization algorithms can achieve low error
rates using distributed techniques such as belief propagation (BP), they are
not well suited to WSNs deployed for precision agriculture applications with
large number of nodes, few number of landmarks and lack real time update
capability. The algorithm proposed here is designed for applications such as
pest control and irrigation in large farms and orchards where greater power
efficiency and scalability are required but location accuracy requirements are
less demanding. Our algorithm uses received signal strength indicator (RSSI)
values to estimate the distribution of distance between nodes then updates the
location probability mass function (pmf) of nodes in a distributed manner. At
every time step, the most recently communicated path loss samples and location
prior pmf received from neighbouring nodes is sufficient for nodes with unknown
location to update their location pmf. This renders the algorithm recursive,
hence results in lower computational complexity at each time step. We propose a
particular realization of the method in which only one node multicasts at each
time step and neighbouring nodes update their location pmf conditioned on all
communicated samples over previous time steps. This is highly compatible with
realistic WSN deployments, e.g., ZigBee which are based upon the ad hoc
on-demand distance vector (AODV) where nodes flood route request (RREQ) and
route reply (RREP) packets in the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02405</identifier>
 <datestamp>2015-12-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02405</id><created>2015-09-08</created><updated>2015-12-19</updated><authors><author><keyname>Gupta</keyname><forenames>Vipul</forenames></author><author><keyname>Sah</keyname><forenames>Abhay Kumar</forenames></author><author><keyname>Chaturvedi</keyname><forenames>A. K.</forenames></author></authors><title>Iterative Matrix Inversion Based Low Complexity Detection in
  Large/Massive MIMO Systems</title><categories>cs.IT math.IT</categories><comments>6 pages, 7 figures, submitted to Proc. of the IEEE International
  Conference on Communications (ICC) Workshop on 5G RAN Design 2016, Kuala
  Lumpur, Malaysia</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linear detectors such as zero forcing (ZF) or minimum mean square error
(MMSE) are imperative for large/massive MIMO systems for both the downlink and
uplink scenarios. However these linear detectors require matrix inversion which
is computationally expensive for such huge systems. In this paper, we assert
that calculating an exact inverse is not necessary to find the ZF/MMSE solution
and an approximate inverse would yield a similar performance. This is possible
if the quantized solution calculated using the approximate inverse is same as
the one calculated using the exact inverse. We quantify the amount of
approximation that can be tolerated for this to happen. Motivated by this, we
propose to use the existing iterative methods for obtaining low complexity
approximate inverses. We show that, after a sufficient number of iterations,
the inverse using iterative methods can provide a similar error performance. In
addition, we also show that the advantage of using an approximate inverse is
not limited to linear detectors but can be extended to non linear detectors
such as sphere decoders (SD). An approximate inverse can be used for any SD
that requires matrix inversion. We prove that application of approximate
inverse leads to a smaller radius, which in turn reduces the search space
leading to reduction in complexity. Numerical results corroborate our claim
that using approximate matrix inversion reduces decoding complexity in
large/massive MIMO systems with no loss in error performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02409</identifier>
 <datestamp>2015-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02409</id><created>2015-09-08</created><authors><author><keyname>Doulaty</keyname><forenames>Mortaza</forenames></author><author><keyname>Saz</keyname><forenames>Oscar</forenames></author><author><keyname>Hain</keyname><forenames>Thomas</forenames></author></authors><title>Data-selective Transfer Learning for Multi-Domain Speech Recognition</title><categories>cs.LG cs.CL cs.SD</categories><journal-ref>16th Interspeech.Proc. (2015) 2897-2901</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Negative transfer in training of acoustic models for automatic speech
recognition has been reported in several contexts such as domain change or
speaker characteristics. This paper proposes a novel technique to overcome
negative transfer by efficient selection of speech data for acoustic model
training. Here data is chosen on relevance for a specific target. A submodular
function based on likelihood ratios is used to determine how acoustically
similar each training utterance is to a target test set. The approach is
evaluated on a wide-domain data set, covering speech from radio and TV
broadcasts, telephone conversations, meetings, lectures and read speech.
Experiments demonstrate that the proposed technique both finds relevant data
and limits negative transfer. Results on a 6--hour test set show a relative
improvement of 4% with data selection over using all data in PLP based models,
and 2% with DNN features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02412</identifier>
 <datestamp>2015-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02412</id><created>2015-09-08</created><authors><author><keyname>Doulaty</keyname><forenames>Mortaza</forenames></author><author><keyname>Saz</keyname><forenames>Oscar</forenames></author><author><keyname>Hain</keyname><forenames>Thomas</forenames></author></authors><title>Unsupervised Domain Discovery using Latent Dirichlet Allocation for
  Acoustic Modelling in Speech Recognition</title><categories>cs.CL</categories><journal-ref>16th Interspeech.Proc. (2015) 3640-3644, Dresden, Germany</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Speech recognition systems are often highly domain dependent, a fact widely
reported in the literature. However the concept of domain is complex and not
bound to clear criteria. Hence it is often not evident if data should be
considered to be out-of-domain. While both acoustic and language models can be
domain specific, work in this paper concentrates on acoustic modelling. We
present a novel method to perform unsupervised discovery of domains using
Latent Dirichlet Allocation (LDA) modelling. Here a set of hidden domains is
assumed to exist in the data, whereby each audio segment can be considered to
be a weighted mixture of domain properties. The classification of audio
segments into domains allows the creation of domain specific acoustic models
for automatic speech recognition. Experiments are conducted on a dataset of
diverse speech data covering speech from radio and TV broadcasts, telephone
conversations, meetings, lectures and read speech, with a joint training set of
60 hours and a test set of 6 hours. Maximum A Posteriori (MAP) adaptation to
LDA based domains was shown to yield relative Word Error Rate (WER)
improvements of up to 16% relative, compared to pooled training, and up to 10%,
compared with models adapted with human-labelled prior domain knowledge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02413</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02413</id><created>2015-08-28</created><authors><author><keyname>Huang</keyname><forenames>Yanping</forenames></author></authors><title>Learning Efficient Representations for Reinforcement Learning</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Markov decision processes (MDPs) are a well studied framework for solving
sequential decision making problems under uncertainty. Exact methods for
solving MDPs based on dynamic programming such as policy iteration and value
iteration are effective on small problems. In problems with a large discrete
state space or with continuous state spaces, a compact representation is
essential for providing an efficient approximation solutions to MDPs. Commonly
used approximation algorithms involving constructing basis functions for
projecting the value function onto a low dimensional subspace, and building a
factored or hierarchical graphical model to decompose the transition and reward
functions. However, hand-coding a good compact representation for a given
reinforcement learning (RL) task can be quite difficult and time consuming.
Recent approaches have attempted to automatically discover efficient
representations for RL.
  In this thesis proposal, we discuss the problems of automatically
constructing structured kernel for kernel based RL, a popular approach to
learning non-parametric approximations for value function. We explore a space
of kernel structures which are built compositionally from base kernels using a
context-free grammar. We examine a greedy algorithm for searching over the
structure space. To demonstrate how the learned structure can represent and
approximate the original RL problem in terms of compactness and efficiency, we
plan to evaluate our method on a synthetic problem and compare it to other RL
baselines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02417</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02417</id><created>2015-09-08</created><authors><author><keyname>Garcia-Saura</keyname><forenames>Carlos</forenames></author></authors><title>Central Pattern Generators for the control of robotic systems</title><categories>cs.RO cs.NE</categories><comments>Report supervised by Prof. Murray Shanahan at Imperial College London</comments><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Bio-inspired control of motion is an active field of research with many
applications in real world tasks. In the case of robotic systems that need to
exhibit oscillatory behaviour (i.e. locomotion of snake-type or legged robots),
Central Pattern Generators (CPGs) are among the most versatile solutions. These
controllers are often based on loosely-coupled oscillators similar to those
found in the neural circuits of many animal species, and can be more robust to
uncertainty (i.e. external perturbations) than traditional control approaches.
This project provides an overview of the state-of-the-art in the field of CPGs,
and in particular their applications within robotic systems. The project also
tackles the implementation of a CPG-based controller in a small 3D-printed
hexapod.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02421</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02421</id><created>2015-09-08</created><authors><author><keyname>Trias</keyname><forenames>Antonio</forenames></author></authors><title>Fundamentals of the Holomorphic Embedding Load-Flow Method</title><categories>cs.SY math.AG math.CV</categories><comments>17 pages, 1 figure</comments><msc-class>14H50, 14H81, 30B10, 30B40, 30B70, 30E10, 94C99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Holomorphic Embedding Load-Flow Method (HELM) was recently introduced as
a novel technique to constructively solve the power-flow equations in power
grids, based on advanced complex analysis. In this paper, the theoretical
foundations of the method are established in detail. Starting from a
fundamental projective invariance of the power-flow equations, it is shown how
to devise holomorphicity-preserving embeddings that ultimately allow regarding
the power-flow problem as essentially a study in algebraic curves.
Complementing this algebraic-geometric viewpoint, which lays the foundation of
the method, it is shown how to apply standard analytic techniques (power
series) for practical computation. Stahl's theorem on the maximality of the
analytic continuation provided by Pad\'e approximants then ensures the
completeness of the method. On the other hand, it is shown how to extend the
method to accommodate smooth controls, such as the ubiquitous
generator-controlled PV bus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02424</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02424</id><created>2015-09-08</created><authors><author><keyname>Bredereck</keyname><forenames>Robert</forenames></author><author><keyname>Chen</keyname><forenames>Jiehua</forenames></author><author><keyname>Niedermeier</keyname><forenames>Rolf</forenames></author><author><keyname>Walsh</keyname><forenames>Toby</forenames></author></authors><title>Parliamentary Voting Procedures: Agenda Control, Manipulation, and
  Uncertainty</title><categories>cs.GT cs.MA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study computational problems for two popular parliamentary voting
procedures: the amendment procedure and the successive procedure. While finding
successful manipulations or agenda controls is tractable for both procedures,
our real-world experimental results indicate that most elections cannot be
manipulated by a few voters and agenda control is typically impossible. If the
voter preferences are incomplete, then finding which alternatives can possibly
win is NP-hard for both procedures. Whilst deciding if an alternative
necessarily wins is coNP-hard for the amendment procedure, it is
polynomial-time solvable for the successive one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02427</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02427</id><created>2015-09-08</created><authors><author><keyname>Tan</keyname><forenames>Jin</forenames></author><author><keyname>Ma</keyname><forenames>Yanting</forenames></author><author><keyname>Rueda</keyname><forenames>Hoover</forenames></author><author><keyname>Baron</keyname><forenames>Dror</forenames></author><author><keyname>Arce</keyname><forenames>Gonzalo</forenames></author></authors><title>Approximate Message Passing in Coded Aperture Snapshot Spectral Imaging</title><categories>cs.IT math.IT</categories><comments>to appear in Globalsip 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a compressive hyperspectral imaging reconstruction problem, where
three-dimensional spatio-spectral information about a scene is sensed by a
coded aperture snapshot spectral imager (CASSI). The approximate message
passing (AMP) framework is utilized to reconstruct hyperspectral images from
CASSI measurements, and an adaptive Wiener filter is employed as a
three-dimensional image denoiser within AMP. We call our algorithm
&quot;AMP-3D-Wiener.&quot; The simulation results show that AMP-3D-Wiener outperforms
existing widely-used algorithms such as gradient projection for sparse
reconstruction (GPSR) and two-step iterative shrinkage/thresholding (TwIST)
given the same amount of runtime. Moreover, in contrast to GPSR and TwIST,
AMP-3D-Wiener need not tune any parameters, which simplifies the reconstruction
process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02437</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02437</id><created>2015-09-08</created><authors><author><keyname>Soni</keyname><forenames>Rishabh</forenames></author><author><keyname>Mathai</keyname><forenames>K. James</forenames></author></authors><title>Improved Twitter Sentiment Prediction through Cluster-then-Predict Model</title><categories>cs.IR cs.CL cs.LG cs.SI</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the past decade humans have experienced exponential growth in the use of
online resources, in particular social media and microblogging websites such as
Facebook, Twitter, YouTube and also mobile applications such as WhatsApp, Line,
etc. Many companies have identified these resources as a rich mine of marketing
knowledge. This knowledge provides valuable feedback which allows them to
further develop the next generation of their product. In this paper, sentiment
analysis of a product is performed by extracting tweets about that product and
classifying the tweets showing it as positive and negative sentiment. The
authors propose a hybrid approach which combines unsupervised learning in the
form of K-means clustering to cluster the tweets and then performing supervised
learning methods such as Decision Trees and Support Vector Machines for
classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02439</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02439</id><created>2015-09-08</created><authors><author><keyname>Laurent</keyname><forenames>Nicolas</forenames></author><author><keyname>Mens</keyname><forenames>Kim</forenames></author></authors><title>Parsing Expression Grammars Made Practical</title><categories>cs.PL</categories><comments>To appear in SLE 2015</comments><acm-class>D.3.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Parsing Expression Grammars (PEGs) define languages by specifying
recursive-descent parser that recognises them. The PEG formalism exhibits
desirable properties, such as closure under composition, built-in
disambiguation, unification of syntactic and lexical concerns, and closely
matching programmer intuition. Unfortunately, state of the art PEG parsers
struggle with left-recursive grammar rules, which are not supported by the
original definition of the formalism and can lead to infinite recursion under
naive implementations. Likewise, support for associativity and explicit
precedence is spotty. To remedy these issues, we introduce Autumn, a general
purpose PEG library that supports left-recursion, left and right associativity
and precedence rules, and does so efficiently. Furthermore, we identify infix
and postfix operators as a major source of inefficiency in left-recursive PEG
parsers and show how to tackle this problem. We also explore the extensibility
of the PEG paradigm by showing how one can easily introduce new parsing
operators and how our parser accommodates custom memoization and error handling
strategies. We compare our parser to both state of the art and battle-tested
PEG and CFG parsers, such as Rats!, Parboiled and ANTLR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02441</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02441</id><created>2015-09-04</created><authors><author><keyname>Tripathi</keyname><forenames>Subarna</forenames></author><author><keyname>Belongie</keyname><forenames>Serge</forenames></author><author><keyname>Hwang</keyname><forenames>Youngbae</forenames></author><author><keyname>Nguyen</keyname><forenames>Truong</forenames></author></authors><title>Semantic Video Segmentation : Exploring Inference Efficiency</title><categories>cs.CV</categories><comments>To appear in proc of ISOCC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explore the efficiency of the CRF inference beyond image level semantic
segmentation and perform joint inference in video frames. The key idea is to
combine best of two worlds: semantic co-labeling and more expressive models.
Our formulation enables us to perform inference over ten thousand images within
seconds and makes the system amenable to perform video semantic segmentation
most effectively. On CamVid dataset, with TextonBoost unaries, our proposed
method achieves up to 8% improvement in accuracy over individual semantic image
segmentation without additional time overhead. The source code is available at
https://github.com/subtri/video_inference
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02447</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02447</id><created>2015-09-08</created><authors><author><keyname>Yu</keyname><forenames>Adams Wei</forenames></author><author><keyname>Ma</keyname><forenames>Wanli</forenames></author><author><keyname>Yu</keyname><forenames>Yaoliang</forenames></author><author><keyname>Carbonell</keyname><forenames>Jaime G.</forenames></author><author><keyname>Sra</keyname><forenames>Suvrit</forenames></author></authors><title>Efficient Structured Matrix Rank Minimization</title><categories>cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of finding structured low-rank matrices using nuclear
norm regularization where the structure is encoded by a linear map. In contrast
to most known approaches for linearly structured rank minimization, we do not
(a) use the full SVD, nor (b) resort to augmented Lagrangian techniques, nor
(c) solve linear systems per iteration. Instead, we formulate the problem
differently so that it is amenable to a generalized conditional gradient
method, which results in a practical improvement with low per iteration
computational cost. Numerical results show that our approach significantly
outperforms state-of-the-art competitors in terms of running time, while
effectively recovering low rank solutions in stochastic system realization and
spectral compressed sensing problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02458</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02458</id><created>2015-09-08</created><authors><author><keyname>Chung</keyname><forenames>Yeounoh</forenames></author><author><keyname>Park</keyname><forenames>Chang-yong</forenames></author><author><keyname>Kim</keyname><forenames>Noo-ri</forenames></author><author><keyname>Cho</keyname><forenames>Hana</forenames></author><author><keyname>Yoon</keyname><forenames>Taebok</forenames></author><author><keyname>Lee</keyname><forenames>Hunjoo</forenames></author><author><keyname>Lee</keyname><forenames>Jee-Hyong</forenames></author></authors><title>A Behavior Analysis-Based Game Bot Detection Approach Considering
  Various Play Styles</title><categories>cs.LG cs.AI</categories><journal-ref>ETRI Journal 35.6 (2013): 1058-1067</journal-ref><doi>10.4218/etrij.13.2013.0049</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An approach for game bot detection in MMORPGs is proposed based on the
analysis of game playing behavior. Since MMORPGs are large scale games, users
can play in various ways. This variety in playing behavior makes it hard to
detect game bots based on play behaviors. In order to cope with this problem,
the proposed approach observes game playing behaviors of users and groups them
by their behavioral similarities. Then, it develops a local bot detection model
for each player group. Since the locally optimized models can more accurately
detect game bots within each player group, the combination of those models
brings about overall improvement. For a practical purpose of reducing the
workloads of the game servers in service, the game data is collected at a low
resolution in time. Behavioral features are selected and developed to
accurately detect game bots with the low resolution data, considering common
aspects of MMORPG playing. Through the experiment with the real data from a
game currently in service, it is shown that the proposed local model approach
yields more accurate results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02459</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02459</id><created>2015-09-08</created><authors><author><keyname>Oltean</keyname><forenames>Mihai</forenames></author><author><keyname>Dumitrescu</keyname><forenames>D.</forenames></author></authors><title>Evolving TSP heuristics using Multi Expression Programming</title><categories>cs.AI cs.NE</categories><comments>International Conference on Computational Sciences, ICCS'04, 6-9
  June, Krakow, Poland, Edited by M. Bubak, G.van Albada, P. Sloot, and J.
  Dongarra, Vol II, pp. 670-673, Springer-Verlag, Berlin, 2004. Source code
  available for download at:
  http://www.cs.ubbcluj.ro/~moltean/evolve_heuristics.html</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi Expression Programming (MEP) is an evolutionary technique that may be
used for solving computationally difficult problems. MEP uses a linear solution
representation. Each MEP individual is a string encoding complex expressions
(computer programs). A MEP individual may encode multiple solutions of the
current problem. In this paper MEP is used for evolving a Traveling Salesman
Problem (TSP) heuristic for graphs satisfying triangle inequality. Evolved MEP
heuristic is compared with Nearest Neighbor Heuristic (NN) and Minimum Spanning
Tree Heuristic (MST) on some difficult problems in TSPLIB. For most of the
considered problems the evolved MEP heuristic outperforms NN and MST. The
obtained algorithm was tested against some problems in TSPLIB. The results
emphasizes that evolved MEP heuristic is a powerful tool for solving difficult
TSP instances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02464</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02464</id><created>2015-09-08</created><updated>2016-01-23</updated><authors><author><keyname>Rahman</keyname><forenames>Muntasir Raihan</forenames></author><author><keyname>Tseng</keyname><forenames>Lewis</forenames></author><author><keyname>Nguyen</keyname><forenames>Son</forenames></author><author><keyname>Gupta</keyname><forenames>Indranil</forenames></author><author><keyname>Vaidya</keyname><forenames>Nitin</forenames></author></authors><title>Characterizing and Adapting the Consistency-Latency Tradeoff in
  Distributed Key-value Stores</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The CAP theorem is a fundamental result that applies to distributed storage
systems. In this paper, we first present and prove two CAP-like impossibility
theorems. To state these theorems, we present probabilistic models to
characterize the three important elements of the CAP theorem: consistency (C),
availability or latency (A), and partition tolerance (P). The theorems show the
un-achievable envelope, i.e., which combinations of the parameters of the three
models make them impossible to achieve together. Next, we present the design of
a class of systems called PCAP that perform close to the envelope described by
our theorems. In addition, these systems allow applications running on a single
data-center to specify either a latency SLA or a consistency SLA. The PCAP
systems automatically adapt, in real-time and under changing network
conditions, to meet the SLA while optimizing the other C/A metric. We
incorporate PCAP into two popular key-value stores -- Apache Cassandra and
Riak. Our experiments with these two deployments, under realistic workloads,
reveal that the PCAP system satisfactorily meets SLAs, and performs close to
the achievable envelope. We also extend PCAP from a single data-center to
multiple geo-distributed data-centers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02465</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02465</id><created>2015-09-08</created><authors><author><keyname>Gadde</keyname><forenames>Akshay</forenames></author><author><keyname>Knyazev</keyname><forenames>Andrew</forenames></author><author><keyname>Tian</keyname><forenames>Dong</forenames></author><author><keyname>Mansour</keyname><forenames>Hassan</forenames></author></authors><title>Guided Signal Reconstruction with Application to Image Magnification</title><categories>cs.IT cs.NA math.IT</categories><comments>5 pages, 6 figures; Accepted to IEEE GlobalSIP 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of reconstructing a signal from its projection on a
subspace. The proposed signal reconstruction algorithms utilize a guiding
subspace that represents desired properties of reconstructed signals. We show
that optimal reconstructed signals belong to a convex bounded set, called the
&quot;reconstruction&quot; set. We also develop iterative algorithms, based on conjugate
gradient methods, to approximate optimal reconstructions with low memory and
computational costs. The effectiveness of the proposed approach is demonstrated
for image magnification, where the reconstructed image quality is shown to
exceed that of consistent and generalized reconstruction schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02468</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02468</id><created>2015-09-08</created><authors><author><keyname>Knyazev</keyname><forenames>Andrew</forenames></author><author><keyname>Malyshev</keyname><forenames>Alexander</forenames></author></authors><title>Accelerated graph-based spectral polynomial filters</title><categories>cs.CV</categories><comments>6 pages, 6 figures. Accepted to the 2015 IEEE International Workshop
  on Machine Learning for Signal Processing</comments><report-no>MERL-TR2015-106</report-no><journal-ref>Machine Learning for Signal Processing (MLSP), 2015 IEEE 25th
  International Workshop on , pp.1-6, 17-20 Sept. 2015</journal-ref><doi>10.1109/MLSP.2015.7324315</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph-based spectral denoising is a low-pass filtering using the
eigendecomposition of the graph Laplacian matrix of a noisy signal. Polynomial
filtering avoids costly computation of the eigendecomposition by projections
onto suitable Krylov subspaces. Polynomial filters can be based, e.g., on the
bilateral and guided filters. We propose constructing accelerated polynomial
filters by running flexible Krylov subspace based linear and eigenvalue solvers
such as the Block Locally Optimal Preconditioned Conjugate Gradient (LOBPCG)
method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02470</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02470</id><created>2015-09-08</created><authors><author><keyname>Luo</keyname><forenames>Jianwei</forenames></author><author><keyname>Li</keyname><forenames>Jianguo</forenames></author><author><keyname>Wang</keyname><forenames>Jun</forenames></author><author><keyname>Jiang</keyname><forenames>Zhiguo</forenames></author><author><keyname>Chen</keyname><forenames>Yurong</forenames></author></authors><title>Deep Attributes from Context-Aware Regional Neural Codes</title><categories>cs.CV cs.LG cs.NE</categories><comments>10 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, many researches employ middle-layer output of convolutional neural
network models (CNN) as features for different visual recognition tasks.
Although promising results have been achieved in some empirical studies, such
type of representations still suffer from the well-known issue of semantic gap.
This paper proposes so-called deep attribute framework to alleviate this issue
from three aspects. First, we introduce object region proposals as intermedia
to represent target images, and extract features from region proposals. Second,
we study aggregating features from different CNN layers for all region
proposals. The aggregation yields a holistic yet compact representation of
input images. Results show that cross-region max-pooling of soft-max layer
output outperform all other layers. As soft-max layer directly corresponds to
semantic concepts, this representation is named &quot;deep attributes&quot;. Third, we
observe that only a small portion of generated regions by object proposals
algorithm are correlated to classification target. Therefore, we introduce
context-aware region refining algorithm to pick out contextual regions and
build context-aware classifiers.
  We apply the proposed deep attributes framework for various vision tasks.
Extensive experiments are conducted on standard benchmarks for three visual
recognition tasks, i.e., image classification, fine-grained recognition and
visual instance retrieval. Results show that deep attribute approaches achieve
state-of-the-art results, and outperforms existing peer methods with a
significant margin, even though some benchmarks have little overlap of concepts
with the pre-trained CNN models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02471</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02471</id><created>2015-09-08</created><authors><author><keyname>Rocha</keyname><forenames>Herbert</forenames></author><author><keyname>Ismail</keyname><forenames>Hussama</forenames></author><author><keyname>Cordeiro</keyname><forenames>Lucas</forenames></author><author><keyname>Barreto</keyname><forenames>Raimundo</forenames></author></authors><title>Model Checking Embedded C Software using k-Induction and Invariants
  (extended version)</title><categories>cs.LO cs.SE</categories><comments>extended version of paper published at SBESC'15. arXiv admin note:
  substantial text overlap with arXiv:1502.02327</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a proof by induction algorithm, which combines k-induction with
invariants to model check embedded C software with bounded and unbounded loops.
The k-induction algorithm consists of three cases: in the base case, we aim to
find a counterexample with up to k loop unwindings; in the forward condition,
we check whether loops have been fully unrolled and that the safety property P
holds in all states reachable within k unwindings; and in the inductive step,
we check that whenever P holds for k unwindings, it also holds after the next
unwinding of the system. For each step of the k-induction algorithm, we infer
invariants using affine constraints (i.e., polyhedral) to specify pre- and
post-conditions. Experimental results show that our approach can handle a wide
variety of safety properties in typical embedded software applications from
telecommunications, control systems, and medical devices; we demonstrate an
improvement of the induction algorithm effectiveness if compared to other
approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02475</identifier>
 <datestamp>2016-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02475</id><created>2015-09-08</created><updated>2016-01-07</updated><authors><author><keyname>Ackerman</keyname><forenames>Eyal</forenames></author><author><keyname>Keszegh</keyname><forenames>Bal&#xe1;zs</forenames></author><author><keyname>Vizer</keyname><forenames>Mate</forenames></author></authors><title>On the size of planarly connected crossing graphs</title><categories>math.CO cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A pair of independent and crossing edges in a drawing of a graph is planarly
connected if there is a crossing-free edge that connects endpoints of the
crossed edges. A graph is a planarly connected crossing (PCC) graph, if it
admits a drawing in which every pair of independent and crossing edges is
planarly connected. We prove that a PCC graph with $n$ vertices has $O(n)$
edges.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02479</identifier>
 <datestamp>2015-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02479</id><created>2015-09-08</created><updated>2015-10-22</updated><authors><author><keyname>Letouzey</keyname><forenames>Pierre</forenames><affiliation>PPS, PI.R2</affiliation></author></authors><title>Hofstadter's problem for curious readers</title><categories>cs.LO math.HO</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This document summarizes the proofs made during a Coq development inSummer
2015. This development investigates the function G introduced by Hofstadter in
his famous &quot;G{\&quot;o}del, Escher, Bach&quot; book as well as a related infinite tree.
The left/right flipped variantof this G tree has also been studied here,
following Hofstadter's &quot;problem for the curious reader&quot;.The initial G function
is refered as sequence A005206 in OEIS, while the flipped version is the
sequence A123070.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02485</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02485</id><created>2015-08-28</created><authors><author><keyname>Campos</keyname><forenames>Victor</forenames></author><author><keyname>Corr&#xea;a</keyname><forenames>Ricardo C.</forenames></author><author><keyname>Donne</keyname><forenames>Diego Delle</forenames></author><author><keyname>Marenco</keyname><forenames>Javier</forenames></author><author><keyname>Wagler</keyname><forenames>Annegret</forenames></author></authors><title>Polyhedral studies of vertex coloring problems: The asymmetric
  representatives formulation</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite the fact that some vertex coloring problems are polynomially solvable
on certain graph classes, most of these problems are not &quot;under control&quot; from a
polyhedral point of view. The equivalence between \emph{optimization} and
\emph{polyhedral separation} suggests that, for these problems, there must
exist formulations admitting some elegant characterization for the polytopes
associated to them. Therefore, it is interesting to study known formulations
for vertex coloring with the goal of finding such characterizations. In this
work we study the asymmetric representatives formulation and we show that the
corresponding coloring polytope, for a given graph $G$, can be interpreted as
the stable set polytope of another graph obtained from $G$. This result allows
us to derive complete characterizations for the corresponding coloring polytope
for some families of graphs, based on known complete characterizations for the
stable set polytope.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02487</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02487</id><created>2015-09-08</created><updated>2015-09-09</updated><authors><author><keyname>Mahmoody</keyname><forenames>Ahmad</forenames></author><author><keyname>Kornaropoulos</keyname><forenames>Evgenios M.</forenames></author><author><keyname>Upfal</keyname><forenames>Eli</forenames></author></authors><title>Optimizing Static and Adaptive Probing Schedules for Rapid Event
  Detection</title><categories>cs.DS cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We formulate and study a fundamental search and detection problem, Schedule
Optimization, motivated by a variety of real-world applications, ranging from
monitoring content changes on the web, social networks, and user activities to
detecting failure on large systems with many individual machines.
  We consider a large system consists of many nodes, where each node has its
own rate of generating new events, or items. A monitoring application can probe
a small number of nodes at each step, and our goal is to compute a probing
schedule that minimizes the expected number of undiscovered items at the
system, or equivalently, minimizes the expected time to discover a new item in
the system.
  We study the Schedule Optimization problem both for deterministic and
randomized memoryless algorithms. We provide lower bounds on the cost of an
optimal schedule and construct close to optimal schedules with rigorous
mathematical guarantees. Finally, we present an adaptive algorithm that starts
with no prior information on the system and converges to the optimal memoryless
algorithms by adapting to observed data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02490</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02490</id><created>2015-09-08</created><authors><author><keyname>Alves</keyname><forenames>Erickson H. da S.</forenames></author><author><keyname>Cordeiro</keyname><forenames>Lucas C.</forenames></author><author><keyname>Filho</keyname><forenames>Eddie B. de Lima</forenames></author></authors><title>Fault Localization in Multi-Threaded C Programs using Bounded Model
  Checking (extended version)</title><categories>cs.LO cs.SE</categories><comments>extended version of paper published at SBESC'15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software debugging is a very time-consuming process, which is even worse for
multi-threaded programs, due to the non-deterministic behavior of
thread-scheduling algorithms. However, the debugging time may be greatly
reduced, if automatic methods are used for localizing faults. In this study, a
new method for fault localization, in multi-threaded C programs, is proposed.
It transforms a multi-threaded program into a corresponding sequential one and
then uses a fault-diagnosis method suitable for this type of program, in order
to localize faults. The code transformation is implemented with rules and
context switch information from counterexamples, which are typically generated
by bounded model checkers. Experimental results show that the proposed method
is effective, in such a way that sequential fault-localization methods can be
extended to multi-threaded programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02491</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02491</id><created>2015-09-08</created><authors><author><keyname>Knyazev</keyname><forenames>Andrew</forenames></author></authors><title>Edge-enhancing Filters with Negative Weights</title><categories>cs.CV cs.IT math.CO math.IT</categories><comments>5 pages; 6 figures. Accepted to IEEE GlobalSIP 2015 conference</comments><msc-class>68U10, 05C85</msc-class><acm-class>I.4.3; I.4.6; I.5.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In [DOI:10.1109/ICMEW.2014.6890711], a graph-based denoising is performed by
projecting the noisy image to a lower dimensional Krylov subspace of the graph
Laplacian, constructed using nonnegative weights determined by distances
between image data corresponding to image pixels. We~extend the construction of
the graph Laplacian to the case, where some graph weights can be negative.
Removing the positivity constraint provides a more accurate inference of a
graph model behind the data, and thus can improve quality of filters for
graph-based signal processing, e.g., denoising, compared to the standard
construction, without affecting the costs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02492</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02492</id><created>2015-09-08</created><authors><author><keyname>Trindade</keyname><forenames>Alessandro</forenames></author><author><keyname>Ismail</keyname><forenames>Hussama</forenames></author><author><keyname>Cordeiro</keyname><forenames>Lucas</forenames></author></authors><title>Applying Multi-Core Model Checking to Hardware-Software Partitioning in
  Embedded Systems (extended version)</title><categories>cs.LO cs.SE</categories><comments>extended version of paper published at SBESC'15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an alternative approach to solve the hardware (HW) and software
(SW) partitioning problem, which uses Bounded Model Checking (BMC) based on
Satisfiability Modulo Theories (SMT) in conjunction with a multi-core support
using Open Multi-Processing. The multi-core SMT-based BMC approach allows
initializing many verification instances based on processors cores numbers
available to the model checker. Each instance checks for a different optimum
value until the optimization problem is satisfied. The goal is to show that
multi-core model-checking techniques can be effective, in particular cases, to
find the optimal solution of the HW-SW partitioning problem using an SMT-based
BMC approach. We compare the experimental results of our proposed approach with
Integer Linear Programming and the Genetic Algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02503</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02503</id><created>2015-09-08</created><authors><author><keyname>Landsberg</keyname><forenames>J. M.</forenames></author></authors><title>An introduction to geometric complexity theory</title><categories>math.AG cs.CC math.DG math.RT</categories><comments>Draft of article to appear in the Newsletter of the European
  Mathematical Society. 9 pages in original, arXiv version has extra spaces due
  to arXiv processing</comments><msc-class>68Q15 (20G05)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  I survey methods from differential geometry, algebraic geometry and
representation theory relevant for the permanent v. determinant problem from
computer science, an algebraic analog of the P v. NP problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02512</identifier>
 <datestamp>2015-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02512</id><created>2015-09-08</created><authors><author><keyname>Amoh</keyname><forenames>Justice</forenames></author><author><keyname>Odame</keyname><forenames>Kofi</forenames></author></authors><title>DeepCough: A Deep Convolutional Neural Network in A Wearable Cough
  Detection System</title><categories>cs.NE cs.LG</categories><comments>BioCAS-2015</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  In this paper, we present a system that employs a wearable acoustic sensor
and a deep convolutional neural network for detecting coughs. We evaluate the
performance of our system on 14 healthy volunteers and compare it to that of
other cough detection systems that have been reported in the literature.
Experimental results show that our system achieves a classification sensitivity
of 95.1% and a specificity of 99.5%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02533</identifier>
 <datestamp>2015-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02533</id><created>2015-09-08</created><authors><author><keyname>Mavroforakis</keyname><forenames>Charalampos</forenames></author><author><keyname>Mathioudakis</keyname><forenames>Michael</forenames></author><author><keyname>Gionis</keyname><forenames>Aristides</forenames></author></authors><title>Absorbing random-walk centrality: Theory and algorithms</title><categories>cs.SI cs.DS</categories><comments>11 pages, 11 figures, short paper to appear at ICDM 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a new notion of graph centrality based on absorbing random walks.
Given a graph $G=(V,E)$ and a set of query nodes $Q\subseteq V$, we aim to
identify the $k$ most central nodes in $G$ with respect to $Q$. Specifically,
we consider central nodes to be absorbing for random walks that start at the
query nodes $Q$. The goal is to find the set of $k$ central nodes that
minimizes the expected length of a random walk until absorption. The proposed
measure, which we call $k$ absorbing random-walk centrality, favors diverse
sets, as it is beneficial to place the $k$ absorbing nodes in different parts
of the graph so as to &quot;intercept&quot; random walks that start from different query
nodes.
  Although similar problem definitions have been considered in the literature,
e.g., in information-retrieval settings where the goal is to diversify
web-search results, in this paper we study the problem formally and prove some
of its properties. We show that the problem is NP-hard, while the objective
function is monotone and supermodular, implying that a greedy algorithm
provides solutions with an approximation guarantee. On the other hand, the
greedy algorithm involves expensive matrix operations that make it prohibitive
to employ on large datasets. To confront this challenge, we develop more
efficient algorithms based on spectral clustering and on personalized PageRank.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02534</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02534</id><created>2015-09-08</created><authors><author><keyname>Lv</keyname><forenames>Tiejun</forenames></author><author><keyname>Gao</keyname><forenames>Hui</forenames></author><author><keyname>Li</keyname><forenames>Xiaopeng</forenames></author><author><keyname>Yang</keyname><forenames>Shaoshi</forenames></author><author><keyname>Hanzo</keyname><forenames>Lajos</forenames></author></authors><title>Space-Time Hierarchical-Graph Based Cooperative Localization in Wireless
  Sensor Networks</title><categories>cs.IT math.IT</categories><comments>14 pages, 15 figures, 4 tables, accepted to appear on IEEE
  Transactions on Signal Processing, Sept. 2015</comments><journal-ref>IEEE Transactions on Signal Processing, vol. 64, no. 2, pp.
  322-334, Jan. 2016</journal-ref><doi>10.1109/TSP.2015.2480038</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It has been shown that cooperative localization is capable of improving both
the positioning accuracy and coverage in scenarios where the global positioning
system (GPS) has a poor performance. However, due to its potentially excessive
computational complexity, at the time of writing the application of cooperative
localization remains limited in practice. In this paper, we address the
efficient cooperative positioning problem in wireless sensor networks. A
space-time hierarchical-graph based scheme exhibiting fast convergence is
proposed for localizing the agent nodes. In contrast to conventional methods,
agent nodes are divided into different layers with the aid of the space-time
hierarchical-model and their positions are estimated gradually. In particular,
an information propagation rule is conceived upon considering the quality of
positional information. According to the rule, the information always
propagates from the upper layers to a certain lower layer and the message
passing process is further optimized at each layer. Hence, the potential error
propagation can be mitigated. Additionally, both position estimation and
position broadcasting are carried out by the sensor nodes. Furthermore, a
sensor activation mechanism is conceived, which is capable of significantly
reducing both the energy consumption and the network traffic overhead incurred
by the localization process. The analytical and numerical results provided
demonstrate the superiority of our space-time hierarchical-graph based
cooperative localization scheme over the benchmarking schemes considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02557</identifier>
 <datestamp>2015-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02557</id><created>2015-09-08</created><authors><author><keyname>Dison</keyname><forenames>W.</forenames></author><author><keyname>Einstein</keyname><forenames>E.</forenames></author><author><keyname>Riley</keyname><forenames>T. R.</forenames></author></authors><title>Taming the hydra: the word problem and extreme integer compression</title><categories>math.GR cs.CC</categories><comments>63 pages, 1 figure</comments><msc-class>20F10, 20F65, 68W32, 68Q17</msc-class><acm-class>F.2.2; E.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a finitely presented group, the word problem asks for an algorithm which
declares whether or not words on the generators represent the identity. The
Dehn function is a complexity measure of a direct attack on the word problem by
applying the defining relations. Dison &amp; Riley showed that a &quot;hydra phenomenon&quot;
gives rise to novel groups with extremely fast growing (Ackermannian) Dehn
functions. Here we show that nevertheless, there are efficient (polynomial
time) solutions to the word problems of these groups. Our main innovation is a
means of computing efficiently with enormous integers which are represented in
compressed forms by strings of Ackermann functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02563</identifier>
 <datestamp>2015-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02563</id><created>2015-09-08</created><authors><author><keyname>Verdonschot</keyname><forenames>Sander</forenames></author></authors><title>Flips and Spanners</title><categories>cs.CG</categories><comments>151 pages, PhD thesis. This thesis contains chapters based on
  arXiv:1212.0570, arXiv:1409.6397, arXiv:1206.0303, arXiv:1110.6473, and
  arXiv:1310.1166</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this thesis, we study two different graph problems.
  The first problem revolves around geometric spanners. Here, we have a set of
points in the plane and we want to connect them with straight line segments,
such that there is a path between each pair of points that does not make a
large detour. If we achieve this, the resulting graph is called a spanner. We
focus our attention on $\Theta$-graphs, which are constructed by connecting
each point with its nearest neighbour in a fixed number of cones. Although this
construction is very straight-forward, it has proven challenging to fully
determine the properties of the resulting graphs. We show that if the
construction uses 5 cones, the resulting graphs are still spanners. This was
the only number of cones for which this question remained unanswered. We also
present a routing strategy on the half-$\Theta_6$-graph, a variant of the graph
with 6 cones. We show that our routing strategy finds a path whose length is at
most a constant factor from the straight-line distance between the endpoints.
Moreover, we show that this routing strategy is optimal.
  In the second part, we turn our attention to flips in triangulations. A flip
is a simple operation that transforms one triangulation into another. It turns
out that with enough flips, we can transform any triangulation into any other.
But how many flips is enough? We present an improved upper bound of $5.2n -
33.6$ on the maximum flip distance between any pair of triangulations with n
vertices. Along the way, we prove matching lower bounds on each step in the
current algorithm, including a tight bound of $(3n - 9)/5$ flips needed to make
a triangulation 4-connected. In addition, we prove tight $\Theta(n \log n)$
bounds on the number of flips required in several settings where the edges have
unique labels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02574</identifier>
 <datestamp>2015-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02574</id><created>2015-09-08</created><authors><author><keyname>Roberto</keyname><forenames>Elizabeth</forenames></author></authors><title>Spatial Boundaries and the Local Context of Residential Segregation</title><categories>physics.soc-ph cs.IT math.IT stat.ME</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spatial boundaries are a defining feature of a city's social and spatial
organization. Rivers, highways, and train tracks create excess distance between
nearby locations and often mark social separation -- they become dividing lines
that are well known to residents. Qualitative studies are rich with insight
about the local significance of boundaries, but they have been largely ignored
in the quantitative segregation literature. I advance existing scholarship by
integrating spatial boundaries into the way we measure residential segregation
for city populations. I introduce a new method that measures the proximity of
residential locations and the reach of local environments around each location
using road distance. This is more realistic than straight line (&quot;as the crow
flies&quot;) distance, because it captures the connectivity of roads and the
distance imposed by physical boundaries. I measure segregation using the
Divergence Index, which evaluates how surprising the composition of each local
environment is given the overall population. I use this new approach to examine
how spatial boundaries structure patterns of racial and ethnic residential
segregation in U.S. cities. Results reveal the salience of city boundaries in
structuring segregation patterns in Detroit, and how the presence of physical
boundaries affects the composition of local environments in Manhattan. Further,
the experience of segregation is highly unequal within cities such as St.
Louis. Some residents live in completely segregated environments, while others
live in areas that are a microcosm of the city's diverse population. This
research bridges qualitative insight on the local experience of segregation and
how we measure segregation for city populations. My emphasis on spatial
boundaries and local context reframes our understanding of segregation, and
offers deeper insight into even the most studied U.S. cities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02576</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02576</id><created>2015-09-08</created><updated>2015-09-10</updated><authors><author><keyname>Fan</keyname><forenames>Chenglin</forenames></author><author><keyname>Zhu</keyname><forenames>Binhai</forenames></author></authors><title>Complexity and Algorithms for the Discrete Fr\'echet Distance Upper
  Bound with Imprecise Input</title><categories>cs.CG</categories><comments>15 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of computing the upper bound of the discrete Fr\'{e}chet
distance for imprecise input, and prove that the problem is NP-hard. This
solves an open problem posed in 2010 by Ahn \emph{et al}. If shortcuts are
allowed, we show that the upper bound of the discrete Fr\'{e}chet distance with
shortcuts for imprecise input can be computed in polynomial time and we present
several efficient algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02584</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02584</id><created>2015-09-08</created><updated>2015-10-17</updated><authors><author><keyname>Saulpaugh</keyname><forenames>Evan</forenames></author></authors><title>XCRUSH: A Family of ARX Block Ciphers</title><categories>cs.CR</categories><comments>14 pages, 2 figures. Includes test vectors and C99 reference
  implementation</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The XCRUSH family of non-Feistel, ARX block ciphers is designed to make
efficient use of modern 64-bit general-purpose processors using a small number
of encryption rounds which are simple to implement in software. The avalanche
function, which applies one data-dependent, key-dependent rotation per 64-bit
word of plaintext per round, allows XCRUSH to achieve a visibly diffuse 256-bit
ciphertext block after only two rounds. Designed for speed in software, 3-round
XCRUSH is measured at ~7.3 cycles/byte single-threaded on an Intel Haswell
processor. A pseudorandom number generator, constructed using the avalanche
function, serves as a key scheduling algorithm. No security claims are made in
this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02587</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02587</id><created>2015-09-08</created><updated>2015-09-12</updated><authors><author><keyname>Yousefi</keyname><forenames>Bardia</forenames></author><author><keyname>Loo</keyname><forenames>C. K.</forenames></author></authors><title>A Dual Fast and Slow Feature Interaction in Biologically Inspired Visual
  Recognition of Human Action</title><categories>cs.CV</categories><comments>This paper has been withdrawn by the author due to a mistake in file</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computational neuroscience studies that have examined human visual system
through functional magnetic resonance imaging (fMRI) have identified a model
where the mammalian brain pursues two distinct pathways (for recognition of
biological movement tasks). In the brain, dorsal stream analyzes the
information of motion (optical flow), which is the fast features, and ventral
stream (form pathway) analyzes form information (through active basis model
based incremental slow feature analysis ) as slow features. The proposed
approach suggests the motion perception of the human visual system composes of
fast and slow feature interactions that identifies biological movements. Form
features in the visual system biologically follows the application of active
basis model with incremental slow feature analysis for the extraction of the
slowest form features of human objects movements in the ventral stream.
Applying incremental slow feature analysis provides an opportunity to use the
action prototypes. To extract the slowest features episodic observation is
required but the fast features updates the processing of motion information in
every frames. Experimental results have shown promising accuracy for the
proposed model and good performance with two datasets (KTH and Weizmann).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02596</identifier>
 <datestamp>2015-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02596</id><created>2015-09-08</created><authors><author><keyname>Wang</keyname><forenames>Yu</forenames></author><author><keyname>Yuan</keyname><forenames>Jianbo</forenames></author><author><keyname>Luo</keyname><forenames>Jiebo</forenames></author></authors><title>To Love or to Loathe: How is the World Reacting to China's Rise?</title><categories>cs.CY</categories><comments>8 pages, 10 figures, 5 tables, ICDM'15 workshop proceedings</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  China has experienced a spectacular economic growth in recent decades. Its
economy grew more than 48 times from 1980 to 2013. How are the other countries
reacting to China's rise? Do they see it as an economic opportunity or a
security threat? In this paper, we answer this question by analyzing online
news reports about China published in Australia, France, Germany, Japan,
Russia, South Korea, the UK and the US. More specifically, we first analyze the
frequency with which China has appeared in news headlines, which is a measure
of China's influence in the world. Second, we build a Naive Bayes classifier to
study the evolving nature of the news reports, i.e., whether they are economic
or political. We then evaluate the friendliness of the news coverage based on
sentiment analysis. Empirical results indicate that there has been increasing
news coverage of China in all the countries under study. We also find that the
emphasis of the reports is generally shifting towards China's economy. Here
Japan and South Korea are exceptions: they are reporting more on Chinese
politics. In terms of global sentiment, the picture is quite gloomy. With the
exception of Australia and, to some extent, France, all the other countries
under examination are becoming less positive towards China.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02597</identifier>
 <datestamp>2016-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02597</id><created>2015-09-08</created><updated>2016-02-19</updated><authors><author><keyname>Chang</keyname><forenames>Tsung-Hui</forenames></author><author><keyname>Hong</keyname><forenames>Mingyi</forenames></author><author><keyname>Liao</keyname><forenames>Wei-Cheng</forenames></author><author><keyname>Wang</keyname><forenames>Xiangfeng</forenames></author></authors><title>Asynchronous Distributed ADMM for Large-Scale Optimization- Part I:
  Algorithm and Convergence Analysis</title><categories>cs.DC cs.LG cs.SY</categories><comments>37 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Aiming at solving large-scale learning problems, this paper studies
distributed optimization methods based on the alternating direction method of
multipliers (ADMM). By formulating the learning problem as a consensus problem,
the ADMM can be used to solve the consensus problem in a fully parallel fashion
over a computer network with a star topology. However, traditional synchronized
computation does not scale well with the problem size, as the speed of the
algorithm is limited by the slowest workers. This is particularly true in a
heterogeneous network where the computing nodes experience different
computation and communication delays. In this paper, we propose an asynchronous
distributed ADMM (AD-AMM) which can effectively improve the time efficiency of
distributed optimization. Our main interest lies in analyzing the convergence
conditions of the AD-ADMM, under the popular partially asynchronous model,
which is defined based on a maximum tolerable delay of the network.
Specifically, by considering general and possibly non-convex cost functions, we
show that the AD-ADMM is guaranteed to converge to the set of
Karush-Kuhn-Tucker (KKT) points as long as the algorithm parameters are chosen
appropriately according to the network delay. We further illustrate that the
asynchrony of the ADMM has to be handled with care, as slightly modifying the
implementation of the AD-ADMM can jeopardize the algorithm convergence, even
under a standard convex setting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02601</identifier>
 <datestamp>2015-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02601</id><created>2015-09-08</created><authors><author><keyname>Alegr&#xed;a-Galicia</keyname><forenames>Carlos</forenames></author><author><keyname>Orden</keyname><forenames>David</forenames></author><author><keyname>Seara</keyname><forenames>Carlos</forenames></author><author><keyname>Urrutia</keyname><forenames>Jorge</forenames></author></authors><title>On the $O_\beta$-hull of a planar point set</title><categories>cs.CG</categories><acm-class>I.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the $O_\beta$-hull of a planar point set, a generalization of the
Orthogonal Convex Hull where the coordinate axes form an angle $\beta$. Given a
set $P$ of $n$ points in the plane, we show how to maintain the $O_\beta$-hull
of $P$ while $\beta$ runs from $0$ to $\pi$ in $O(n \log n)$ time and $O(n)$
space. With the same complexity, we also find the values of $\beta$ that
maximize the area and the perimeter of the $O_\beta$-hull and, furthermore, we
find the value of $\beta$ achieving the best fitting of the point set $P$ with
a two-joint chain of alternate interior angle $\beta$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02604</identifier>
 <datestamp>2015-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02604</id><created>2015-09-08</created><authors><author><keyname>Chang</keyname><forenames>Tsung-Hui</forenames></author><author><keyname>Liao</keyname><forenames>Wei-Cheng</forenames></author><author><keyname>Hong</keyname><forenames>Mingyi</forenames></author><author><keyname>Wang</keyname><forenames>Xiangfeng</forenames></author></authors><title>Asynchronous Distributed ADMM for Large-Scale Optimization- Part II:
  Linear Convergence Analysis and Numerical Performance</title><categories>cs.DC cs.LG cs.SY</categories><comments>submitted for publication, 28 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The alternating direction method of multipliers (ADMM) has been recognized as
a versatile approach for solving modern large-scale machine learning and signal
processing problems efficiently. When the data size and/or the problem
dimension is large, a distributed version of ADMM can be used, which is capable
of distributing the computation load and the data set to a network of computing
nodes. Unfortunately, a direct synchronous implementation of such algorithm
does not scale well with the problem size, as the algorithm speed is limited by
the slowest computing nodes. To address this issue, in a companion paper, we
have proposed an asynchronous distributed ADMM (AD-ADMM) and studied its
worst-case convergence conditions. In this paper, we further the study by
characterizing the conditions under which the AD-ADMM achieves linear
convergence. Our conditions as well as the resulting linear rates reveal the
impact that various algorithm parameters, network delay and network size have
on the algorithm performance. To demonstrate the superior time efficiency of
the proposed AD-ADMM, we test the AD-ADMM on a high-performance computer
cluster by solving a large-scale logistic regression problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02612</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02612</id><created>2015-09-08</created><updated>2015-12-09</updated><authors><author><keyname>Lenstra</keyname><forenames>H. W.</forenames><suffix>Jr.</suffix></author><author><keyname>Silverberg</keyname><forenames>A.</forenames></author></authors><title>Roots of unity in orders</title><categories>math.AC cs.CR math.NT</categories><comments>To appear in Foundations of Computational Mathematics</comments><msc-class>16H15 (primary), 11R54, 13A99 (secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give deterministic polynomial-time algorithms that, given an order,
compute the primitive idempotents and determine a set of generators for the
group of roots of unity in the order. Also, we show that the discrete logarithm
problem in the group of roots of unity can be solved in polynomial time. As an
auxiliary result, we solve the discrete logarithm problem for certain unit
groups in finite rings. Our techniques, which are taken from commutative
algebra, may have further potential in the context of cryptology and computer
algebra.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02618</identifier>
 <datestamp>2015-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02618</id><created>2015-09-08</created><authors><author><keyname>Huang</keyname><forenames>Shan</forenames></author><author><keyname>Sun</keyname><forenames>Hong</forenames></author><author><keyname>Zhang</keyname><forenames>Haijian</forenames></author><author><keyname>Yu</keyname><forenames>Lei</forenames></author></authors><title>ESPRIT-based frequency estimation of multiple sinusoids with
  double-channel sub-Nyquist sampling</title><categories>cs.IT math.IT</categories><comments>3 pages, 4 figures. arXiv admin note: text overlap with
  arXiv:1508.05723</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In some applications of frequency estimation, it is challenging to sample at
as high as Nyquist rate due to hardware limitations. Based on conventional
ESPRIT, a new method for frequency estimation with double-channel sub-Nyquist
sampling is proposed. When the two undersampled multiples are relatively prime
integers, partial samples are selected to estimate the frequency components
contained in the signals via ESPRIT. The simulations show that the proposed
method holds almost the same accuracy with regular sampling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02620</identifier>
 <datestamp>2015-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02620</id><created>2015-09-08</created><authors><author><keyname>Dixit</keyname><forenames>Dharmendra</forenames></author><author><keyname>Sahu</keyname><forenames>P. R.</forenames></author></authors><title>Performance of QAM Schemes with Dual-Hop DF Relaying Systems over Mixed
  $\eta$-$\mu$ and $\kappa$-$\mu$ Fading Channels</title><categories>cs.IT math.IT</categories><comments>25</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Performance of quadrature amplitude modulation (QAM) schemes is analyzed with
dual-hop decode-and-forward (DF) relaying systems over mixed $\eta$-$\mu$ and
$\kappa$-$\mu$ fading channels. Closed-form expressions are obtained for the
average symbol error rate (ASER) for general order rectangular QAM and cross
QAM schemes using moment generating function based approach. Derived
expressions are in the form of Lauricella's $(F_D^{(n)}(\cdot),
\Phi_1^{(n)}(\cdot))$ hypergeometric functions which can be numerically
evaluated using either integral or series representation. The obtained ASER
expressions include other mixed fading channel cases addressed in the
literature as special cases such as mixed Hoyt, and Rice fading, mixed
Nakagami-$m$, and Rice fading. We further obtain a simple expression for the
asymptotic ASER, which is useful to determine a factor governing the system
performance at high SNRs, i.e., the diversity order. Additionally, we analyze
the optimal power allocation, which provides a practical design rule to
optimally distribute the total transmission power between the source and the
relay to minimize the ASER. Extensive numerical and computer simulation results
are presented that confirm the accuracy of presented mathematical analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02626</identifier>
 <datestamp>2015-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02626</id><created>2015-09-08</created><authors><author><keyname>Huang</keyname><forenames>Yu-Chih</forenames></author></authors><title>Lattice Index Codes from Algebraic Number Fields</title><categories>cs.IT math.IT</categories><comments>22 pages, 13 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Broadcasting $K$ independent messages to multiple users where each user
demands all the messages and has a subset of the messages as side information
is studied. Recently, Natarajan, Hong, and Viterbo proposed a novel
broadcasting strategy called lattice index coding which uses lattices
constructed over some principal ideal domains (PIDs) for transmission and
showed that this scheme provides uniform side information gains. In this paper,
we generalize this strategy to general rings of algebraic integers of number
fields which may not be PIDs. Upper and lower bounds on the side information
gains for the proposed scheme constructed over some interesting classes of
number fields are provided and are shown to coincide asymptotically in message
rates. This generalization substantially enlarges the design space and
partially includes the scheme by Natarajan, Hong, and Viterbo as a special
case. Perhaps more importantly, in addition to side information gains, the
proposed lattice index codes benefit from diversity gains inherent in
constellations carved from number fields when used over Rayleigh fading
channel. Some interesting examples are also provided for which the proposed
scheme allows all the messages to be from the same field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02627</identifier>
 <datestamp>2015-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02627</id><created>2015-09-08</created><authors><author><keyname>Alegr&#xed;a-Galicia</keyname><forenames>Carlos</forenames></author><author><keyname>Gardu&#xf1;o</keyname><forenames>Tzolkin</forenames></author><author><keyname>Seara</keyname><forenames>Carlos</forenames></author><author><keyname>Rosas-Navarrete</keyname><forenames>Areli</forenames></author><author><keyname>Urrutia</keyname><forenames>Jorge</forenames></author></authors><title>Rectilinear convex hull with minimum area</title><categories>cs.CG</categories><acm-class>I.3.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $P$ be a planar set of $n$ points in general position. We consider the
problem of computing an orientation of the plane for which the Rectilinear
Convex Hull of $P$ has minimum area. Bae et al. (Computational Geometry: Theory
and Applications, Vol. 42, 2009) solved the problem in quadratic time and
linear space. We describe an algorithm that reduces this time complexity to
$\Theta(n \log n)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02628</identifier>
 <datestamp>2015-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02628</id><created>2015-09-08</created><authors><author><keyname>Huang</keyname><forenames>Shan</forenames></author><author><keyname>Sun</keyname><forenames>Hong</forenames></author><author><keyname>Yu</keyname><forenames>Lei</forenames></author><author><keyname>Zhang</keyname><forenames>Haijian</forenames></author></authors><title>A Class of Deterministic Sensing Matrices and Their Application in
  Harmonic Detection</title><categories>cs.IT math.IT</categories><comments>12 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a class of deterministic sensing matrices are constructed by
selecting rows from Fourier matrices. These matrices have better performance in
sparse recovery than random partial Fourier matrices. The coherence and
restricted isometry property of these matrices are given to evaluate their
capacity as compressive sensing matrices. In general, compressed sensing
requires random sampling in data acquisition, which is difficult to implement
in hardware. By using these sensing matrices in harmonic detection, a
deterministic sampling method is provided. The frequencies and amplitudes of
the harmonic components are estimated from under-sampled data. The simulations
show that this under-sampled method is feasible and valid in noisy
environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02629</identifier>
 <datestamp>2015-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02629</id><created>2015-09-08</created><authors><author><keyname>Techakesari</keyname><forenames>O.</forenames></author><author><keyname>Nurdin</keyname><forenames>H. I.</forenames></author></authors><title>Error Bounds on Finite-Dimensional Approximations of Input-Output Open
  Quantum Systems</title><categories>quant-ph cs.SY math-ph math.MP</categories><comments>19 pages. Condensed version to appear in Proceedings of the 54th IEEE
  Conference on Decision and Control (CDC) (Osaka, Japan, December 15-18, 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many physical systems of interest that are encountered in practice are
input-output open quantum systems described by quantum stochastic differential
equations and defined on an infinite-dimensional underlying Hilbert space. Most
commonly, these systems involve coupling to a quantum harmonic oscillator as a
system component. This paper is concerned with the error in the
finite-dimensional approximation of input-output open quantum systems defined
on an infinite-dimensional underlying Hilbert space. We present explicit error
bounds between the time evolution of the state of a class of
infinite-dimensional quantum systems and its approximation on a
finite-dimensional subspace of the original, when both are initialized in the
latter subspace. Applications to some physical examples drawn from the
literature are provided to illustrate our results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02630</identifier>
 <datestamp>2015-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02630</id><created>2015-09-09</created><authors><author><keyname>Bhalshankar</keyname><forenames>Satish</forenames></author><author><keyname>Gulve</keyname><forenames>Avinash K.</forenames></author></authors><title>Audio Steganography: LSB Technique Using a Pyramid Structure and Range
  of Bytes</title><categories>cs.MM</categories><comments>12 page, 16 Figures</comments><journal-ref>International Journal of Advanced Computer Research (IJACR),
  Volume-5, Issue-20, September-2015 ,pp.233-248</journal-ref><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The demand for keeping the information secure and confidential simultaneously
has been progressively increasing. Among various techniques- Audio
Steganography, a technique of embedding information transparently in a digital
media thereby restricting the access to such information has been prominently
developed. Imperceptibility, robustness, and payload or hiding capacity are the
main character for it. In earlier, LSB techniques increased payload capacity
would hamper robustness as well as imperceptibility of the cover media and vice
versa. The proposed technique overcomes the problem. It provides relatively
good improvement in the payload capacity by dividing the bytes of cover media
into ranges to hide the bits of secret message appropriately. As well as due to
the use of ranges of bytes the robustness of cover media has maintained and
imperceptibility preserved by using a pyramid structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02633</identifier>
 <datestamp>2015-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02633</id><created>2015-09-09</created><authors><author><keyname>Cheng</keyname><forenames>Hei Victor</forenames></author><author><keyname>Bj&#xf6;rnson</keyname><forenames>Emil</forenames></author><author><keyname>Larsson</keyname><forenames>Erik G.</forenames></author></authors><title>Uplink Pilot and Data Power Control for Single Cell Massive MIMO Systems
  with MRC</title><categories>cs.IT math.IT math.OC</categories><comments>5 pages, 3 figures, presented in ISWCS 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the jointly optimal pilot and data power allocation in
single cell uplink massive MIMO systems. A closed form solution for the optimal
length of the training interval is derived. Using the spectral efficiency (SE)
as performance metric and setting a total energy budget per co- herence
interval the power control is formulated as optimization problems for two
different objective functions: the minimum SE among the users and the sum SE.
The optimal power control policy is found for the case of maximizing the
minimum SE by converting it to a geometric program (GP). Since maximizing the
sum SE is an NP-hard problem, an efficient algorithm is developed for finding
KKT (local maximum) points. Simulation results show the advantage of optimizing
the power control over both pilot and data power, as compared to heuristic
power control policies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02634</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02634</id><created>2015-09-09</created><updated>2015-09-24</updated><authors><author><keyname>Liu</keyname><forenames>Ziwei</forenames></author><author><keyname>Li</keyname><forenames>Xiaoxiao</forenames></author><author><keyname>Luo</keyname><forenames>Ping</forenames></author><author><keyname>Loy</keyname><forenames>Chen Change</forenames></author><author><keyname>Tang</keyname><forenames>Xiaoou</forenames></author></authors><title>Semantic Image Segmentation via Deep Parsing Network</title><categories>cs.CV</categories><comments>To appear in International Conference on Computer Vision (ICCV) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses semantic image segmentation by incorporating rich
information into Markov Random Field (MRF), including high-order relations and
mixture of label contexts. Unlike previous works that optimized MRFs using
iterative algorithm, we solve MRF by proposing a Convolutional Neural Network
(CNN), namely Deep Parsing Network (DPN), which enables deterministic
end-to-end computation in a single forward pass. Specifically, DPN extends a
contemporary CNN architecture to model unary terms and additional layers are
carefully devised to approximate the mean field algorithm (MF) for pairwise
terms. It has several appealing properties. First, different from the recent
works that combined CNN and MRF, where many iterations of MF were required for
each training image during back-propagation, DPN is able to achieve high
performance by approximating one iteration of MF. Second, DPN represents
various types of pairwise terms, making many existing works as its special
cases. Third, DPN makes MF easier to be parallelized and speeded up in
Graphical Processing Unit (GPU). DPN is thoroughly evaluated on the PASCAL VOC
2012 dataset, where a single DPN model yields a new state-of-the-art
segmentation accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02636</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02636</id><created>2015-09-09</created><updated>2015-09-09</updated><authors><author><keyname>Liang</keyname><forenames>Xiaodan</forenames></author><author><keyname>Wei</keyname><forenames>Yunchao</forenames></author><author><keyname>Shen</keyname><forenames>Xiaohui</forenames></author><author><keyname>Yang</keyname><forenames>Jianchao</forenames></author><author><keyname>Lin</keyname><forenames>Liang</forenames></author><author><keyname>Yan</keyname><forenames>Shuicheng</forenames></author></authors><title>Proposal-free Network for Instance-level Object Segmentation</title><categories>cs.CV</categories><comments>14 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Instance-level object segmentation is an important yet under-explored task.
The few existing studies are almost all based on region proposal methods to
extract candidate segments and then utilize object classification to produce
final results. Nonetheless, generating accurate region proposals itself is
quite challenging. In this work, we propose a Proposal-Free Network (PFN ) to
address the instance-level object segmentation problem, which outputs the
instance numbers of different categories and the pixel-level information on 1)
the coordinates of the instance bounding box each pixel belongs to, and 2) the
confidences of different categories for each pixel, based on pixel-to-pixel
deep convolutional neural network. All the outputs together, by using any
off-the-shelf clustering method for simple post-processing, can naturally
generate the ultimate instance-level object segmentation results. The whole PFN
can be easily trained in an end-to-end way without the requirement of a
proposal generation stage. Extensive evaluations on the challenging PASCAL VOC
2012 semantic segmentation benchmark demonstrate that the proposed PFN solution
well beats the state-of-the-arts for instance-level object segmentation. In
particular, the $AP^r$ over 20 classes at 0.5 IoU reaches 58.7% by PFN,
significantly higher than 43.8% and 46.3% by the state-of-the-art algorithms,
SDS [9] and [16], respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02640</identifier>
 <datestamp>2015-10-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02640</id><created>2015-09-09</created><updated>2015-10-02</updated><authors><author><keyname>Garc&#xed;a-Recuero</keyname><forenames>&#xc1;lvaro</forenames></author></authors><title>On the energy efficiency of client-centric data consistency management
  under random read/write access to Big Data with Apache HBase</title><categories>cs.DC cs.PF</categories><comments>Energy and Performance Evaluation of Apache HBase</comments><acm-class>H.3.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The total estimated energy bill for data centers in 2010 was \$11.5 billion,
and experts estimate that the energy cost of a typical data center doubles
every five years. On the other hand, storage advancements have started to lag
behind computational developments, therein becoming a bottleneck for the
ongoing data growth which already approaches Exascale levels. We investigate
the relationship among data throughput and energy footprint on a large storage
cluster, with the goal of formalizing it as a metric that reflects the trading
among consistency and energy. Employing a client-centric consistency approach,
and while honouring ACID properties of the chosen columnar store for the case
study (Apache HBase), we present the factors involved in the energy consumption
of the system as well as lessons learned to underpin further design of
energy-efficient cluster scale storage systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02648</identifier>
 <datestamp>2015-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02648</id><created>2015-09-09</created><authors><author><keyname>Xiang</keyname><forenames>Chengdi</forenames></author><author><keyname>Petersen</keyname><forenames>Ian R.</forenames></author><author><keyname>Dong</keyname><forenames>Daoyi</forenames></author></authors><title>Coherent Robust H-Infinity Control of Uncertain Linear Quantum
  Stochastic Systems</title><categories>cs.SY</categories><comments>7 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers a class of uncertain linear quantum systems subject to
uncertain perturbations in the system Hamiltonian. We present a method to
design a coherent robust H-infinity controller so that the closed loop system
is robustly stable and achieves a prescribed level of disturbance attenuation
with all the admissible uncertainties. An illustrative example shows that for
the given system, the method presented in this paper has improved performance
over the existing quantum H-infinity control results without considering
uncertainty.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02649</identifier>
 <datestamp>2015-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02649</id><created>2015-09-09</created><authors><author><keyname>Ji</keyname><forenames>Pan</forenames></author><author><keyname>Salzmann</keyname><forenames>Mathieu</forenames></author><author><keyname>Li</keyname><forenames>Hongdong</forenames></author></authors><title>Shape Interaction Matrix Revisited and Robustified: Efficient Subspace
  Clustering with Corrupted and Incomplete Data</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Shape Interaction Matrix (SIM) is one of the earliest approaches to
performing subspace clustering (i.e., separating points drawn from a union of
subspaces). In this paper, we revisit the SIM and reveal its connections to
several recent subspace clustering methods. Our analysis lets us derive a
simple, yet effective algorithm to robustify the SIM and make it applicable to
realistic scenarios where the data is corrupted by noise. We justify our method
by intuitive examples and the matrix perturbation theory. We then show how this
approach can be extended to handle missing data, thus yielding an efficient and
general subspace clustering algorithm. We demonstrate the benefits of our
approach over state-of-the-art subspace clustering methods on several
challenging motion segmentation and face clustering problems, where the data
includes corrupted and missing measurements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02654</identifier>
 <datestamp>2015-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02654</id><created>2015-09-09</created><authors><author><keyname>Berger</keyname><forenames>Christian</forenames></author><author><keyname>Block</keyname><forenames>Delf</forenames></author><author><keyname>Heeren</keyname><forenames>S&#xf6;nke</forenames></author><author><keyname>Hons</keyname><forenames>Christian</forenames></author><author><keyname>K&#xfc;hnel</keyname><forenames>Stefan</forenames></author><author><keyname>Leschke</keyname><forenames>Andr&#xe9;</forenames></author><author><keyname>Plotnikov</keyname><forenames>Dimitri</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author></authors><title>Simulations on Consumer Tests: Systematic Evaluation of Tolerance Ranges
  by Model-Based Generation of Simulation Scenarios</title><categories>cs.SE</categories><comments>15 pages, 6 figures, Fahrerassistenzsysteme und Integrierte
  Sicherheit, VDI Berichte 2014, pp. 403-418</comments><journal-ref>Fahrerassistenzsysteme und Integrierte Sicherheit, VDI Berichte
  2014, pp. 403-418</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Context: Since 2014 several modern cars were rated regarding the performances
of their active safety systems at the European New Car Assessment Programme
(EuroNCAP). Nowadays, consumer tests play a significant role for the OEM's
series development with worldwide perspective, because a top rating is needed
to underline the worthiness of active safety features from the customers' point
of view. Furthermore, EuroNCAP already published their roadmap 2020 in which
they outline further extensions in today's testing and rating procedures that
will aggravate the current requirements addressed to those systems. Especially
Autonomous Emergency Braking/Forward Collision Warning systems (AEB/FCW) are
going to face a broader field of application as pedestrian detection or two-way
traffic scenarios. Objective: This work focuses on the systematic generation of
test scenarios concentrating on specific parameters that can vary within
certain tolerance ranges like the lateral position of the vehicle-under-test
(VUT) and its test velocity for example. It is of high interest to examine the
effect of the tolerance ranges on the braking points in different test cases
representing different trajectories and velocities because they will influence
significantly a later scoring during the assessments and thus the safety
abilities of the regarding car. Method: We present a formal model using a graph
to represent the allowed variances based on the relevant points in time. Now,
varying velocities of the VUT will be added to the model while the vehicle is
approaching a target vehicle. The derived trajectories were used as test cases
for a simulation environment. Selecting interesting test cases and processing
them with the simulation environment, the influence on the system's performance
of different test parameters will be investigated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02655</identifier>
 <datestamp>2015-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02655</id><created>2015-09-09</created><authors><author><keyname>Chen</keyname><forenames>Xiang</forenames></author><author><keyname>Chen</keyname><forenames>Wei</forenames></author></authors><title>Delay-Optimal Buffer-Aware Probabilistic Scheduling with Adaptive
  Transmission</title><categories>cs.IT math.IT</categories><comments>6 pages, 4 figures, accepted by IEEE ICCC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cross-layer scheduling is a promising way to improve Quality of Service (QoS)
given a power constraint. In this paper, we investigate the system with random
data arrival and adaptive transmission. Probabilistic scheduling strategies
aware of the buffer state are applied to generalize conventional deterministic
scheduling. Based on this, the average delay and power consumption are analysed
by Markov reward process. The optimal delay-power tradeoff curve is the Pareto
frontier of the feasible delay-power region. It is proved that the optimal
delay-power tradeoff is piecewise-linear, whose vertices are obtained by
deterministic strategies. Moreover, the corresponding strategies of the optimal
tradeoff curve are threshold-based, hence can be obtained by a proposed
effective algorithm. On the other hand, we formulate a linear programming to
minimize the average delay given a fixed power constraint. By varying the power
constraint, the optimal delay-power tradeoff curve can also be obtained. It is
demonstrated that the algorithm result and the optimization result match each
other, and are further validated by Monte-Carlo simulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02663</identifier>
 <datestamp>2015-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02663</id><created>2015-09-09</created><authors><author><keyname>Li</keyname><forenames>Chun-Wei</forenames></author><author><keyname>Chen</keyname><forenames>Kuo-Ming</forenames></author><author><keyname>Fu</keyname><forenames>Po-Chun</forenames></author><author><keyname>Chen</keyname><forenames>Wei-Ning</forenames></author><author><keyname>Lin</keyname><forenames>Che</forenames></author></authors><title>Energy-Efficient Deterministic Adaptive Beamforming Algorithms for
  Distributed Sensor/Relay Networks</title><categories>cs.IT math.IT</categories><comments>Submitted for possible journal publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this study, energy-efficient deterministic adaptive beamforming algorithms
are proposed for distributed sensor/relay networks. Specifically, DBSA, D-QESA,
D-QESA-E, and a hybrid algorithm, hybrid-QESA, that combines the benefits of
both deterministic and random adaptive beamforming algorithms, are proposed.
Rigorous convergence analyses are provided for all our proposed algorithms and
convergence to the global optimal solution is shown for all our proposed
algorithms. Through extensive numerical simulations, we demonstrate that
superior performance is achieved by our proposed DBSA and D-QESA over random
adaptive beamforming algorithms for static channels. Surprisingly, D-QESA is
also more robust against random node removal than random adaptive beamforming
algorithms. For time-varying channels, hybrid-QESA indeed achieves the best
performance since it combines the benefits of both types of adaptive
beamforming algorithms. In summary, our proposed deterministic algorithms
demonstrate superior performance both in terms of convergence time and
robustness against channel and network uncertainties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02664</identifier>
 <datestamp>2015-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02664</id><created>2015-09-09</created><authors><author><keyname>Khalili</keyname><forenames>Azam</forenames></author><author><keyname>Rastegarnia</keyname><forenames>Amir</forenames></author></authors><title>Performance Analysis of Incremental LMS over Flat Fading Channels</title><categories>cs.IT cs.DC cs.SY math.IT</categories><comments>7 Figures, 10 pages. arXiv admin note: text overlap with
  arXiv:1508.02108</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the effect of fading in the communication channels between sensor
nodes on the performance of the incremental least mean square (ILMS) algorithm,
and derive steady state performance metrics, including the mean-square
deviation (MSD), excess mean-square error (EMSE) and meansquare error (MSE). We
obtain conditions for mean convergence of the ILMS algorithm, and show that in
the presence of fading channels, the ILMS algorithm is asymptotically biased.
Furthermore, the dynamic range for mean stability depends only on the mean
channel gain, and under simplifying technical assumptions, we show that the
MSD, EMSE and MSE are non-decreasing functions of the channel gain variances,
with mean-square convergence to the steady states possible only if the channel
gain variances are limited. We derive sufficient conditions to ensure
mean-square convergence, and verify our results through simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02668</identifier>
 <datestamp>2015-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02668</id><created>2015-09-09</created><authors><author><keyname>Kundu</keyname><forenames>Atreyee</forenames></author><author><keyname>Chatterjee</keyname><forenames>Debasish</forenames></author></authors><title>A graph theoretic approach to input-to-state stability of switched
  systems</title><categories>cs.SY</categories><comments>14 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article deals with input-to-state stability (ISS) of discrete-time
switched systems. Given a family of nonlinear systems with exogenous inputs, we
present a class of switching signals under which the resulting switched system
is ISS. We allow non-ISS systems in the family and our analysis involves
graph-theoretic arguments. A weighted digraph is associated to the switched
system, and a switching signal is expressed as an infinite walk on this
digraph, both in a natural way. Our class of stabilizing switching signals
(infinite walks) is periodic in nature and affords simple algorithmic
construction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02673</identifier>
 <datestamp>2015-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02673</id><created>2015-09-09</created><authors><author><keyname>Oltean</keyname><forenames>Mihai</forenames></author><author><keyname>Muntean</keyname><forenames>Oana</forenames></author></authors><title>Solving NP-complete problems with delayed signals: an overview of
  current research directions</title><categories>cs.ET cs.CC</categories><comments>in Proceedings of 1st international workshop on Optical
  SuperComputing, LNCS 5172, pp. 115-128, Springer-Verlag, 2008, more info at:
  http://www.cs.ubbcluj.ro/~moltean/optical</comments><doi>10.1007/978-3-540-85673-3_10</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we summarize the existing principles for building
unconventional computing devices that involve delayed signals for encoding
solutions to NP-complete problems. We are interested in the following aspects:
the properties of the signal, the operations performed within the devices, some
components required for the physical implementation, precision required for
correctly reading the solution and the decrease in the signal's strength. Six
problems have been solved so far by using the above enumerated principles:
Hamiltonian path, travelling salesman, bounded and unbounded subset sum,
Diophantine equations and exact cover. For the hardware implementation several
types of signals can be used: light, electric power, sound, electro-magnetic
etc.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02683</identifier>
 <datestamp>2015-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02683</id><created>2015-09-09</created><authors><author><keyname>van der Zanden</keyname><forenames>Tom C.</forenames></author></authors><title>Parameterized Complexity of Graph Constraint Logic</title><categories>cs.CC</categories><acm-class>F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph constraint logic is a framework introduced by Hearn and Demaine, which
provides several problems that are often a convenient starting point for
reductions. We study the parameterized complexity of Constraint Graph
Satisfiability and both bounded and unbounded versions of Nondeterministic
Constraint Logic (NCL) with respect to solution length, treewidth and maximum
degree of the underlying constraint graph as parameters. As a main result we
show that restricted NCL remains PSPACE-complete on graphs of bounded
bandwidth, strengthening Hearn and Demaine's framework. This allows us to
improve upon existing results obtained by reduction from NCL. We show that
reconfiguration versions of several classical graph problems (including
independent set, feedback vertex set and dominating set) are PSPACE-complete on
planar graphs of bounded bandwidth and that Rush Hour, generalized to $k\times
n$ boards, is PSPACE-complete even when $k$ is at most a constant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02709</identifier>
 <datestamp>2015-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02709</id><created>2015-09-09</created><authors><author><keyname>Everitt</keyname><forenames>Tom</forenames></author><author><keyname>Hutter</keyname><forenames>Marcus</forenames></author></authors><title>A Topological Approach to Meta-heuristics: Analytical Results on the BFS
  vs. DFS Algorithm Selection Problem</title><categories>cs.AI</categories><comments>Main results published in 28th Australian Joint Conference on
  Artificial Intelligence, 2015</comments><acm-class>I.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Search is a central problem in artificial intelligence, and BFS and DFS the
two most fundamental ways to search. In this report we derive results for
average BFS and DFS runtime: For tree search, we employ a probabilistic model
of goal distribution; for graph search, the analysis depends on an additional
statistic of path redundancy and average branching factor. As an application,
we use the results on two concrete grammar problems. The runtime estimates can
be used to select the faster out of BFS and DFS for a given problem, and may
form the basis for further analysis of more advanced search methods. Finally,
we verify our results experimentally; the analytical approximations come
surprisingly close to empirical reality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02712</identifier>
 <datestamp>2015-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02712</id><created>2015-09-09</created><authors><author><keyname>Deng</keyname><forenames>Yansha</forenames></author><author><keyname>Wang</keyname><forenames>Lifeng</forenames></author><author><keyname>Wong</keyname><forenames>Kai-Kit</forenames></author><author><keyname>Nallanathan</keyname><forenames>Arumugam</forenames></author><author><keyname>Elkashlan</keyname><forenames>Maged</forenames></author></authors><title>Safeguarding Massive MIMO Aided HetNets Using Physical Layer Security</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper exploits the potential of physical layer security in massive
multiple-input multiple-output (MIMO) aided two-tier heterogeneous networks
(HetNets). We focus on the downlink secure transmission in the presence of
multiple eavesdroppers. We first address the impact of massive MIMO on the
maximum receive power based user association. We then derive the tractable
upper bound expressions for the secrecy outage probability of a HetNets user.We
show that the implementation of massive MIMO significantly improves the secrecy
performance, which indicates that physical layer security could be a promising
solution for safeguarding massive MIMO HetNets. Furthermore, we show that the
secrecy outage probability of HetNets user first degrades and then improves
with increasing the density of PBSs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02730</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02730</id><created>2015-09-09</created><authors><author><keyname>Mitra</keyname><forenames>Rangeet</forenames></author><author><keyname>Bhatia</keyname><forenames>Vimal</forenames></author></authors><title>Finite Dictionary Variants of the Diffusion KLMS Algorithm</title><categories>cs.SY cs.DC cs.IT cs.LG math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The diffusion based distributed learning approaches have been found to be a
viable solution for learning over linearly separable datasets over a network.
However, approaches till date are suitable for linearly separable datasets and
need to be extended to scenarios in which we need to learn a non-linearity. In
such scenarios, the recently proposed diffusion kernel least mean squares
(KLMS) has been found to be performing better than diffusion least mean squares
(LMS). The drawback of diffusion KLMS is that it requires infinite storage for
observations (also called dictionary). This paper formulates the diffusion KLMS
in a fixed budget setting such that the storage requirement is curtailed while
maintaining appreciable performance in terms of convergence. Simulations have
been carried out to validate the two newly proposed algorithms named as
quantised diffusion KLMS (QDKLMS) and fixed budget diffusion KLMS (FBDKLMS)
against KLMS, which indicate that both the proposed algorithms deliver better
performance as compared to the KLMS while reducing the dictionary size storage
requirement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02739</identifier>
 <datestamp>2015-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02739</id><created>2015-09-09</created><authors><author><keyname>Singh</keyname><forenames>Jaspreet</forenames></author><author><keyname>Fernando</keyname><forenames>Zeon Trevor</forenames></author><author><keyname>Chawla</keyname><forenames>Saniya</forenames></author></authors><title>LearnWeb-OER: Improving Accessibility of Open Educational Resources</title><categories>cs.HC cs.CY</categories><comments>LinkedUp Vici Challenge, International Semantic Web Conference, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In addition to user-generated content, Open Educational Resources are
increasingly made available on the Web by several institutions and
organizations with the aim of being re-used. Nevertheless, it is still
difficult for users to find appropriate resources for specific learning
scenarios among the vast amount offered on the Web. Our goal is to give users
the opportunity to search for authentic resources from the Web and reuse them
in a learning context. The LearnWeb-OER platform enhances collaborative
searching and sharing of educational resources providing specific means and
facilities for education. In the following, we provide a description of the
functionalities that support users in collaboratively collecting, selecting,
annotating and discussing search results and learning resources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02748</identifier>
 <datestamp>2015-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02748</id><created>2015-09-09</created><authors><author><keyname>Blackburn</keyname><forenames>Simon R.</forenames></author></authors><title>Maximum likelihood decoding for multilevel channels with gain and offset
  mismatch</title><categories>cs.IT math.IT</categories><comments>17 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  K.A.S. Immink and J.H. Weber recently defined and studied a channel with both
gain and offset mismatch, modelling the behaviour of charge-leakage in flash
memory. They proposed a decoding measure for this channel based on minimising
Pearson distance (a notion from cluster analysis). The paper derives a formula
for maximum likelihood decoding for this channel, and also defines and
justifies a notion of minimum distance of a code in this context.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02762</identifier>
 <datestamp>2015-12-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02762</id><created>2015-09-09</created><authors><author><keyname>Lehrenfeld</keyname><forenames>Christoph</forenames></author></authors><title>High order unfitted finite element methods on level set domains using
  isoparametric mappings</title><categories>math.NA cs.CE</categories><comments>18 pages, 8 figures</comments><msc-class>65N30, 65N85</msc-class><acm-class>G.1.8</acm-class><doi>10.1016/j.cma.2015.12.005</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new class of unfitted finite element methods with high order
accurate numerical integration over curved surfaces and volumes which are only
implicitly defined by level set functions. An unfitted finite element method
which is suitable for the case of piecewise planar interfaces is combined with
a parametric mapping of the underlying mesh resulting in an isoparametric
unfitted finite element method. The parametric mapping is constructed in a way
such that the quality of the piecewise planar interface reconstruction is
significantly improved allowing for high order accurate computations of
(unfitted) domain and surface integrals. This approach is new. We present the
method, discuss implementational aspects and present numerical examples which
demonstrate the quality and potential of this method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02763</identifier>
 <datestamp>2015-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02763</id><created>2015-09-09</created><authors><author><keyname>Stanislav</keyname><forenames>Aranovskiy</forenames></author><author><keyname>Alexey</keyname><forenames>Bobtsov</forenames></author><author><keyname>Romeo</keyname><forenames>Ortega</forenames></author><author><keyname>Anton</keyname><forenames>Pyrkin</forenames></author></authors><title>Performance Enhancement of Parameter Estimators via Dynamic Regressor
  Extension and Mixing</title><categories>cs.SY</categories><comments>The paper is submitted to IEEE Transactions on Automatic Control</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new way to design parameter estimators with enhanced performance is
proposed in the paper. The procedure consists of two stages, first, the
generation of new regression forms via the application of a dynamic operator to
the original regression. Second, a suitable mix of these new regressors to
obtain the final desired regression form. For classical linear regression forms
the procedure yields a new parameter estimator whose convergence is established
without the usual requirement of regressor persistency of excitation. The
technique is also applied to nonlinear regressions with &quot;partially&quot; monotonic
parameter dependence---giving rise again to estimators with enhanced
performance. Simulation results illustrate the advantages of the proposed
procedure in both scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02768</identifier>
 <datestamp>2015-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02768</id><created>2015-09-09</created><authors><author><keyname>Wang</keyname><forenames>Lei</forenames></author><author><keyname>Cai</keyname><forenames>Yueming</forenames></author><author><keyname>Zou</keyname><forenames>Yulong</forenames></author><author><keyname>Yang</keyname><forenames>Weiwei</forenames></author><author><keyname>Hanzo</keyname><forenames>Lajos</forenames></author></authors><title>Joint Relay and Jammer Selection Improves the Physical Layer Security in
  the Face of CSI Feedback Delays</title><categories>cs.IT math.IT</categories><comments>15 pages, IEEE Transactions on Vehicular Technology, Sept. 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We enhance the physical-layer security (PLS) of amplify-and-forward relaying
networks with the aid of joint relay and jammer selection (JRJS), despite the
deliterious effect of channel state information (CSI) feedback delays.
Furthermore, we conceive a new outage-based characterization approach for the
JRJS scheme. The traditional best relay selection (TBRS) is also considered as
a benchmark. We first derive closed-form expressions of both the connection
outage probability (COP) and of the secrecy outage probability (SOP) for both
the TBRS and JRJS schemes. Then, a reliable-and-secure connection probability
(RSCP) is defined and analyzed for characterizing the effect of the correlation
between the COP and SOP introduced by the corporate source-relay link. The
reliability-security ratio (RSR) is introduced for characterizing the
relationship between the reliability and security through the asymptotic
analysis. Moreover, the concept of effective secrecy throughput is defined as
the product of the secrecy rate and of the RSCP for the sake of characterizing
the overall efficiency of the system, as determined by the transmit SNR,
secrecy codeword rate and the power sharing ratio between the relay and jammer.
The impact of the direct source-eavesdropper link and additional performance
comparisons with respect to other related selection schemes are further
included. Our numerical results show that the JRJS scheme outperforms the TBRS
method both in terms of the RSCP as well as in terms of its effective secrecy
throughput, but it is more sensitive to the feedback delays. Increasing the
transmit SNR will not always improve the overall throughput. Moreover, the RSR
results demonstrate that upon reducing the CSI feedback delays, the reliability
improves more substantially than the security degrades, implying an overall
improvement in terms of the security-reliability tradeoff.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02783</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02783</id><created>2015-09-09</created><updated>2015-09-19</updated><authors><author><keyname>Huerta</keyname><forenames>Juan M.</forenames></author><author><keyname>Childs</keyname><forenames>Clancy</forenames></author></authors><title>Accelerating News Integration in Automatic Knowledge Extraction
  Ecosystems: an API-first Outlook</title><categories>cs.CY cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Leveraging Application Programming Interfaces (APIs) has been widely
acknowledged as a valuable approach to software and system design that have
promoted the acceleration of products and services development by allowing the
decoupling of interface design from service implementation details. Many
organizations in the news and journalism industry have adopted and promoted
this API oriented approach. In the first part of this paper, we provide a
survey of the most significant recent work around traditional news and
journalistic open APIs and how these have been influenced by and impacted the
news product landscape. In the second part of the paper, we identify two
disruptive technology trends that we believe will impact the role and value of
news/journalism products in the future: API-first development methodologies,
and the increased role of news-supported automatic knowledge extraction and
analytic services. We anticipate that these two driving forces will create a
new wave of adoption, open collaboration, standardization and overall progress
in news content adoption in knowledge platforms. We provide a brief overview of
our experience in this area at Dow Jones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02796</identifier>
 <datestamp>2015-10-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02796</id><created>2015-09-09</created><authors><author><keyname>K&#xf6;ster</keyname><forenames>Johannes</forenames></author></authors><title>Rust-Bio - a fast and safe bioinformatics library</title><categories>cs.MS</categories><doi>10.1093/bioinformatics/btv573</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present Rust-Bio, the first general purpose bioinformatics library for the
innovative Rust programming language. Rust-Bio leverages the unique combination
of speed, memory safety and high-level syntax offered by Rust to provide a fast
and safe set of bioinformatics algorithms and data structures with a focus on
sequence analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02805</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02805</id><created>2015-09-09</created><updated>2016-03-04</updated><authors><author><keyname>Qiu</keyname><forenames>Teng</forenames></author><author><keyname>Li</keyname><forenames>Yongjie</forenames></author></authors><title>Clustering by Hierarchical Nearest Neighbor Descent (H-NND)</title><categories>stat.ML cs.CV cs.LG stat.ME</categories><comments>19 pages, 9 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Previously in 2014, we proposed the Nearest Descent (ND) method, capable of
generating an efficient Graph, called the in-tree (IT). Due to some beautiful
and effective features, this IT structure proves well suited for data
clustering. Although there exist some redundant edges in IT, they usually have
salient features and thus it is not hard to remove them.
  Subsequently, in order to prevent the seemingly redundant edges from
occurring, we proposed the Nearest Neighbor Descent (NND) by adding the
&quot;Neighborhood&quot; constraint on ND. Consequently, clusters automatically emerged,
without the additional requirement of removing the redundant edges. However,
NND proved still not perfect, since it brought in a new yet worse problem, the
&quot;over-partitioning&quot; problem.
  Now, in this paper, we propose a method, called the Hierarchical Nearest
Neighbor Descent (H-NND), which overcomes the over-partitioning problem of NND
via using the hierarchical strategy. Specifically, H-NND uses ND to effectively
merge the over-segmented sub-graphs or clusters that NND produces. Like ND,
H-NND also generates the IT structure, in which the redundant edges once again
appear. This seemingly comes back to the situation that ND faces. However,
compared with ND, the redundant edges in the IT structure generated by H-NND
generally become more salient, thus being much easier and more reliable to be
identified even by the simplest edge-removing method which takes the edge
length as the only measure. In other words, the IT structure constructed by
H-NND becomes more fitted for data clustering. We prove this on several
clustering datasets of varying shapes, dimensions and attributes. Besides,
compared with ND, H-NND generally takes less computation time to construct the
IT data structure for the input data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02807</identifier>
 <datestamp>2015-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02807</id><created>2015-09-09</created><authors><author><keyname>Stamate</keyname><forenames>Cosmin</forenames></author><author><keyname>Magoulas</keyname><forenames>George D.</forenames></author><author><keyname>Thomas</keyname><forenames>Michael S. C.</forenames></author></authors><title>Transfer learning approach for financial applications</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Artificial neural networks learn how to solve new problems through a
computationally intense and time consuming process. One way to reduce the
amount of time required is to inject preexisting knowledge into the network. To
make use of past knowledge, we can take advantage of techniques that transfer
the knowledge learned from one task, and reuse it on another (sometimes
unrelated) task. In this paper we propose a novel selective breeding technique
that extends the transfer learning with behavioural genetics approach proposed
by Kohli, Magoulas and Thomas (2013), and evaluate its performance on financial
data. Numerical evidence demonstrates the credibility of the new approach. We
provide insights on the operation of transfer learning and highlight the
benefits of using behavioural principles and selective breeding when tackling a
set of diverse financial applications problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02822</identifier>
 <datestamp>2015-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02822</id><created>2015-09-09</created><authors><author><keyname>Fu</keyname><forenames>Gang</forenames></author><author><keyname>Bolton</keyname><forenames>Evan</forenames></author><author><keyname>Rosinach</keyname><forenames>N&#xfa;ria Queralt</forenames></author><author><keyname>Furlong</keyname><forenames>Laura I.</forenames></author><author><keyname>Nguyen</keyname><forenames>Vinh</forenames></author><author><keyname>Sheth</keyname><forenames>Amit</forenames></author><author><keyname>Bodenreider</keyname><forenames>Olivier</forenames></author><author><keyname>Dumontier</keyname><forenames>Michel</forenames></author></authors><title>Exposing Provenance Metadata Using Different RDF Models</title><categories>cs.DB cs.PF</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A standard model for exposing structured provenance metadata of scientific
assertions on the Semantic Web would increase interoperability,
discoverability, reliability, as well as reproducibility for scientific
discourse and evidence-based knowledge discovery. Several Resource Description
Framework (RDF) models have been proposed to track provenance. However,
provenance metadata may not only be verbose, but also significantly redundant.
Therefore, an appropriate RDF provenance model should be efficient for
publishing, querying, and reasoning over Linked Data. In the present work, we
have collected millions of pairwise relations between chemicals, genes, and
diseases from multiple data sources, and demonstrated the extent of redundancy
of provenance information in the life science domain. We also evaluated the
suitability of several RDF provenance models for this crowdsourced data set,
including the N-ary model, the Singleton Property model, and the
Nanopublication model. We examined query performance against three commonly
used large RDF stores, including Virtuoso, Stardog, and Blazegraph. Our
experiments demonstrate that query performance depends on both RDF store as
well as the RDF provenance model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02826</identifier>
 <datestamp>2015-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02826</id><created>2015-09-09</created><authors><author><keyname>Matthes</keyname><forenames>Ralph</forenames><affiliation>IRIT - CNRS &amp; Univ. of Toulouse</affiliation></author><author><keyname>Mio</keyname><forenames>Matteo</forenames><affiliation>CNRS &amp; ENS Lyon</affiliation></author></authors><title>Proceedings Tenth International Workshop on Fixed Points in Computer
  Science</title><categories>cs.LO</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 191, 2015</journal-ref><doi>10.4204/EPTCS.191</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains the proceedings of the Tenth International Workshop on
Fixed Points in Computer Science (FICS 2015) which took place on September 11th
and 12th, 2015 in Berlin, Germany, as a satellite event of the conference
Computer Science Logic (CSL 2015).
  Fixed points play a fundamental role in several areas of computer science.
They are used to justify (co)recursive definitions and associated reasoning
techniques. The construction and properties of fixed points have been
investigated in many different settings such as: design and implementation of
programming languages, logics, verification, databases. The aim of this
workshop is to provide a forum for researchers to present their results to
those members of the computer science and logic communities who study or apply
the theory of fixed points.
  Each of the 11 contributed papers of this volume were evaluated by three or
four reviewers. Some of the papers were re-reviewed after revision.
  Additionally, this volume contains the abstracts of the FICS 2015 invited
talks given by Bartek Klin and James Worrell.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02830</identifier>
 <datestamp>2015-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02830</id><created>2015-09-09</created><authors><author><keyname>Kerrison</keyname><forenames>Steve</forenames></author><author><keyname>Eder</keyname><forenames>Kerstin</forenames></author></authors><title>Modeling and visualizing networked multi-core embedded software energy
  consumption</title><categories>cs.DC cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this report we present a network-level multi-core energy model and a
software development process workflow that allows software developers to
estimate the energy consumption of multi-core embedded programs. This work
focuses on a high performance, cache-less and timing predictable embedded
processor architecture, XS1. Prior modelling work is improved to increase
accuracy, then extended to be parametric with respect to voltage and frequency
scaling (VFS) and then integrated into a larger scale model of a network of
interconnected cores. The modelling is supported by enhancements to an open
source instruction set simulator to provide the first network timing aware
simulations of the target architecture. Simulation based modelling techniques
are combined with methods of results presentation to demonstrate how such work
can be integrated into a software developer's workflow, enabling the developer
to make informed, energy aware coding decisions. A set of single-,
multi-threaded and multi-core benchmarks are used to exercise and evaluate the
models and provide use case examples for how results can be presented and
interpreted. The models all yield accuracy within an average +/-5 % error
margin.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02840</identifier>
 <datestamp>2015-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02840</id><created>2015-09-09</created><authors><author><keyname>Knyazev</keyname><forenames>Andrew</forenames></author><author><keyname>Zhu</keyname><forenames>Peizhen</forenames></author><author><keyname>Di Cairano</keyname><forenames>Stefano</forenames></author></authors><title>Explicit model predictive control accuracy analysis</title><categories>cs.SY math.NA math.OC</categories><comments>6 pages, 7 figures. Accepted to IEEE CDC 2015</comments><msc-class>34H05, 65G50, 93C05</msc-class><acm-class>J.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Model Predictive Control (MPC) can efficiently control constrained systems in
real-time applications. MPC feedback law for a linear system with linear
inequality constraints can be explicitly computed off-line, which results in an
off-line partition of the state space into non-overlapped convex regions, with
affine control laws associated to each region of the partition. An actual
implementation of this explicit MPC in low cost micro-controllers requires the
data to be &quot;quantized&quot;, i.e. represented with a small number of memory bits. An
aggressive quantization decreases the number of bits and the controller
manufacturing costs, and may increase the speed of the controller, but reduces
accuracy of the control input computation. We derive upper bounds for the
absolute error in the control depending on the number of quantization bits and
system parameters. The bounds can be used to determine how many quantization
bits are needed in order to guarantee a specific level of accuracy in the
control input.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02841</identifier>
 <datestamp>2015-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02841</id><created>2015-09-09</created><authors><author><keyname>Georgiadis</keyname><forenames>Loukas</forenames></author><author><keyname>Italiano</keyname><forenames>Giuseppe F.</forenames></author><author><keyname>Papadopoulos</keyname><forenames>Charis</forenames></author><author><keyname>Parotsidis</keyname><forenames>Nikos</forenames></author></authors><title>Approximating the Smallest Spanning Subgraph for 2-Edge-Connectivity in
  Directed Graphs</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $G$ be a strongly connected directed graph. We consider the following
three problems, where we wish to compute the smallest strongly connected
spanning subgraph of $G$ that maintains respectively: the $2$-edge-connected
blocks of $G$ (\textsf{2EC-B}); the $2$-edge-connected components of $G$
(\textsf{2EC-C}); both the $2$-edge-connected blocks and the $2$-edge-connected
components of $G$ (\textsf{2EC-B-C}). All three problems are NP-hard, and thus
we are interested in efficient approximation algorithms. For \textsf{2EC-C} we
can obtain a $3/2$-approximation by combining previously known results. For
\textsf{2EC-B} and \textsf{2EC-B-C}, we present new $4$-approximation
algorithms that run in linear time. We also propose various heuristics to
improve the size of the computed subgraphs in practice, and conduct a thorough
experimental study to assess their merits in practical scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02843</identifier>
 <datestamp>2015-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02843</id><created>2015-09-09</created><authors><author><keyname>Greenblatt</keyname><forenames>Jordan</forenames></author></authors><title>Dimensionally Exponential Lower Bounds on the $L^p$ Norms of the
  Spherical Maximal Operator for Cartesian Powers of Finite Trees and Related
  Graphs</title><categories>math.CO cs.DM math.CA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $T$ be a finite tree graph, $T^N$ be the Cartesian power graph of $T$,
and $d^N$ be the graph distance metric on $T^N$. Also let \[ \mathbb S_r^N(x)
:= \{v \in T^N: d^N(x,v) = r\} \] be the sphere of radius $r$ centered at $x$
and $M$ be the spherical maximal averaging operator on $T^N$ given by \[ Mf(x)
:= \sup_{\substack{r \geq 0 \\ \mathbb S_r^N(x) \neq \emptyset}}
\frac{1}{|\mathbb S_r^N(x)|} |\sum_{\mathbb S_r^N(x)} f(y)|. \] We will show
that for any fixed $1 \leq p \leq \infty$, the $L^p$ operator norm of $M$, i.e.
\[ \|M\|_p := \sup_{\|f\|_p = 1} \|Mf\|_p, \] grows exponentially in the
dimension $N$. In particular, if $r$ is the probability that a random vertex of
$T$ is a leaf, then $\|M\|_p \geq r^{-N/p}$, although this is not a sharp
bound.
  This exponential growth phenomenon extends to a class of graphs strictly
larger than trees, which we will call \emph{global antipode graphs}. This
growth result stands in contrast to the work of Greenblatt, Harrow, Kolla,
Krause, and Schulman that proved that the spherical maximal $L^p$ bounds (for
$p &gt; 1$) are dimension-independent for finite cliques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02848</identifier>
 <datestamp>2015-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02848</id><created>2015-09-09</created><authors><author><keyname>Knyazev</keyname><forenames>Andrew</forenames></author><author><keyname>Malyshev</keyname><forenames>Alexander</forenames></author></authors><title>Continuation model predictive control on smooth manifolds</title><categories>math.OC cs.SY</categories><comments>6 pages, 8 figures. Accepted to the 16th IFAC Workshop on Control
  Applications of Optimization (CAO'2015), Garmisch-Partenkirchen, Germany,
  October 6--9, 2015</comments><msc-class>93B40, 49M15, 65L12</msc-class><journal-ref>IFAC-PapersOnLine, Volume 48, Issue 25, 2015, Pages 126-131, ISSN
  2405-8963</journal-ref><doi>10.1016/j.ifacol.2015.11.071</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Model predictive control (MPC) anticipates future events to take appropriate
control actions. Nonlinear MPC (NMPC) describes systems with nonlinear models
and/or constraints. Continuation MPC, suggested by T.~Ohtsuka in 2004, uses
Krylov-Newton iterations. Continuation MPC is suitable for nonlinear problems
and has been recently adopted for minimum time problems. We extend the
continuation MPC approach to a case where the state is implicitly constrained
to a smooth manifold. We propose an algorithm for on-line controller
implementation and demonstrate its numerical effectiveness for a test problem
on a hemisphere.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02849</identifier>
 <datestamp>2015-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02849</id><created>2015-09-09</created><authors><author><keyname>Fanti</keyname><forenames>Giulia</forenames></author><author><keyname>Kairouz</keyname><forenames>Peter</forenames></author><author><keyname>Oh</keyname><forenames>Sewoong</forenames></author><author><keyname>Ramchandran</keyname><forenames>Kannan</forenames></author><author><keyname>Viswanath</keyname><forenames>Pramod</forenames></author></authors><title>Hiding the Rumor Source</title><categories>cs.CR cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Anonymous messaging platforms, such as Secret, Yik Yak, and Whisper, have
emerged as important social media tools for sharing one's thoughts without the
fear of being judged by friends, family, or the public. Further, anonymous
platforms like these are important in nations with authoritarian governments,
where the right to free expression and sometimes the personal safety of the
author of the message depend on anonymity. Whether for fear of judgment or
personal endangerment, it is sometimes crucial to keep anonymous the identity
of the user who initially posted a sensitive message. In this paper, we
consider a global adversary who wishes to identify the author of a message; it
observes either a snapshot of the spread of a message at a certain time or
sampled timestamp metadata (or both). Recent advances in rumor source detection
show that existing messaging protocols are vulnerable against such an
adversary. We introduce a novel messaging protocol, which we call adaptive
diffusion, and show that under the snapshot adversarial model, it spreads the
messages fast and achieves a perfect obfuscation of the source when the
underlying contact network is an infinite regular tree. That is, all users with
the message are nearly equally likely to have been the origin of the message.
Under the timestamp-based adversary, we show that it achieves optimal
obfuscation asymptotically in the degree of the underlying regular tree.
Experiments on a sampled Facebook network demonstrate that adaptive diffusion
effectively hides the location of the source under both adversarial models,
even when the graph is finite, irregular and has cycles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02852</identifier>
 <datestamp>2015-12-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02852</id><created>2015-09-09</created><authors><author><keyname>Knyazev</keyname><forenames>Andrew</forenames></author><author><keyname>Malyshev</keyname><forenames>Alexander</forenames></author></authors><title>Efficient particle continuation model predictive control</title><categories>math.OC cs.SY</categories><comments>5 pages, 6 figures. Accepted to the 16th IFAC Workshop on Control
  Applications of Optimization (CAO'2015), Garmisch-Partenkirchen, Germany,
  October 6--9, 2015</comments><msc-class>93B40, 93C30</msc-class><journal-ref>IFAC-PapersOnLine, Volume 48, Issue 25, 2015, Pages 287-291, ISSN
  2405-8963</journal-ref><doi>10.1016/j.ifacol.2015.11.102</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Continuation model predictive control (MPC), introduced by T. Ohtsuka in
2004, uses Krylov-Newton approaches to solve MPC optimization and is suitable
for nonlinear and minimum time problems. We suggest particle continuation MPC
in the case, where the system dynamics or constraints can discretely change
on-line. We propose an algorithm for on-line controller implementation of
continuation MPC for ensembles of predictions corresponding to various
anticipated changes and demonstrate its numerical effectiveness for a test
minimum time problem arriving to a destination. Simultaneous on-line particle
computation of ensembles of controls, for several dynamically changing system
dynamics, allows choosing the optimal destination on-line and adapt it as
needed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02861</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02861</id><created>2015-09-09</created><authors><author><keyname>Knyazev</keyname><forenames>Andrew</forenames></author><author><keyname>Malyshev</keyname><forenames>Alexander</forenames></author></authors><title>Preconditioning for continuation model predictive control</title><categories>math.OC cs.SY</categories><comments>6 pages, 8 figures. Accepted to the 5th IFAC Conference on Nonlinear
  Model Predictive Control (NMPC'15), Seville, Spain, September 17-20, 2015</comments><report-no>MERL-TR2015-112</report-no><msc-class>49J15, 93B40, 65F08</msc-class><journal-ref>IFAC PapersOnLine Volume 48, Issue 23, 2015, Pages 191-196</journal-ref><doi>10.1016/j.ifacol.2015.11.282</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Model predictive control (MPC) anticipates future events to take appropriate
control actions. Nonlinear MPC (NMPC) deals with nonlinear models and/or
constraints. A Continuation/GMRES Method for NMPC, suggested by T. Ohtsuka in
2004, uses the GMRES iterative algorithm to solve a forward difference
approximation $Ax=b$ of the original NMPC equations on every time step. We have
previously proposed accelerating the GMRES and MINRES convergence by
preconditioning the coefficient matrix $A$. We now suggest simplifying the
construction of the preconditioner, by approximately solving a forward
recursion for the state and a backward recursion for the costate, or simply
reusing previously computed solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02876</identifier>
 <datestamp>2015-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02876</id><created>2015-09-09</created><authors><author><keyname>K</keyname><forenames>Harish</forenames></author><author><keyname>R</keyname><forenames>Varadhan</forenames></author><author><keyname>M</keyname><forenames>Anurag R</forenames></author><author><keyname>S</keyname><forenames>Harmanpreet</forenames></author></authors><title>Low Cost Swarm Based Diligent Cargo Transit System</title><categories>cs.RO</categories><comments>6 pages, 9 figures, 1 block diagram</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The goal of this paper is to present the design and development of a low cost
cargo transit system which can be adapted in developing countries like India
where there is abundant and cheap human labour which makes the process of
automation in any industry a challenge to innovators. The need of the hour is
an automation system that can diligently transfer cargo from one place to
another and minimize human intervention in the cargo transit industry.
Therefore, a solution is being proposed which could effectively bring down
human labour and the resources needed to implement them. The reduction in human
labour and resources is achieved by the use of low cost components and very
limited modification of the surroundings and the existing vehicles themselves.
The operation of the cargo transit system has been verified and the relevant
results are presented. An economical and robust cargo transit system is
designed and implemented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02877</identifier>
 <datestamp>2016-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02877</id><created>2015-09-09</created><updated>2016-02-04</updated><authors><author><keyname>Zhao</keyname><forenames>Yingbo</forenames></author><author><keyname>Cort&#xe9;s</keyname><forenames>Jorge</forenames></author></authors><title>Gramian-based reachability metrics for bilinear networks</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies Gramian-based reachability metrics for bilinear control
systems. In the context of complex networks, bilinear systems capture scenarios
where an actuator not only can affect the state of a node but also
interconnections among nodes. Under the assumption that the input's infinity
norm is bounded by some function of the network dynamic matrices, we derive a
Gramian-based lower bound on the minimum input energy required to steer the
state from the origin to any reachable target state. This result motivates our
study of various objects associated to the reachability Gramian to quantify the
ease of controllability of the bilinear network: the minimum eigenvalue
(worst-case minimum input energy to reach a state), the trace (average minimum
input energy to reach a state), and its determinant (volume of the ellipsoid
containing the reachable states using control inputs with no more than unit
energy). We establish an increasing returns property of the reachability
Gramian as a function of the actuators, which in turn allows us to derive a
general lower bound on the reachability metrics in terms of the aggregate
contribution of the individual actuators. We conclude by examining the effect
on the worst-case minimum input energy of the addition of bilinear inputs to
difficult-to-control linear symmetric networks. We show that the bilinear
networks resulting from the addition of either inputs at a finite number of
interconnections or at all self loops with weight vanishing with the network
scale remain difficult-to-control. Various examples illustrate our results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02897</identifier>
 <datestamp>2015-09-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02897</id><created>2015-09-09</created><authors><author><keyname>Andoni</keyname><forenames>Alexandr</forenames></author><author><keyname>Indyk</keyname><forenames>Piotr</forenames></author><author><keyname>Laarhoven</keyname><forenames>Thijs</forenames></author><author><keyname>Razenshteyn</keyname><forenames>Ilya</forenames></author><author><keyname>Schmidt</keyname><forenames>Ludwig</forenames></author></authors><title>Practical and Optimal LSH for Angular Distance</title><categories>cs.DS cs.CG cs.IR</categories><comments>22 pages, an extended abstract is to appear in the proceedings of the
  29th Annual Conference on Neural Information Processing Systems (NIPS 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show the existence of a Locality-Sensitive Hashing (LSH) family for the
angular distance that yields an approximate Near Neighbor Search algorithm with
the asymptotically optimal running time exponent. Unlike earlier algorithms
with this property (e.g., Spherical LSH [Andoni, Indyk, Nguyen, Razenshteyn
2014], [Andoni, Razenshteyn 2015]), our algorithm is also practical, improving
upon the well-studied hyperplane LSH [Charikar, 2002] in practice. We also
introduce a multiprobe version of this algorithm, and conduct experimental
evaluation on real and synthetic data sets.
  We complement the above positive results with a fine-grained lower bound for
the quality of any LSH family for angular distance. Our lower bound implies
that the above LSH family exhibits a trade-off between evaluation time and
quality that is close to optimal for a natural class of LSH functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02900</identifier>
 <datestamp>2016-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02900</id><created>2015-09-09</created><updated>2016-01-28</updated><authors><author><keyname>Franke</keyname><forenames>Beate</forenames></author><author><keyname>Plante</keyname><forenames>Jean-Fran&#xe7;ois</forenames></author><author><keyname>Roscher</keyname><forenames>Ribana</forenames></author><author><keyname>Lee</keyname><forenames>Annie</forenames></author><author><keyname>Smyth</keyname><forenames>Cathal</forenames></author><author><keyname>Hatefi</keyname><forenames>Armin</forenames></author><author><keyname>Chen</keyname><forenames>Fuqi</forenames></author><author><keyname>Gil</keyname><forenames>Einat</forenames></author><author><keyname>Schwing</keyname><forenames>Alexander</forenames></author><author><keyname>Selvitella</keyname><forenames>Alessandro</forenames></author><author><keyname>Hoffman</keyname><forenames>Michael M.</forenames></author><author><keyname>Grosse</keyname><forenames>Roger</forenames></author><author><keyname>Hendricks</keyname><forenames>Dieter</forenames></author><author><keyname>Reid</keyname><forenames>Nancy</forenames></author></authors><title>Statistical Inference, Learning and Models in Big Data</title><categories>stat.ML cs.LG</categories><comments>Thematic Program on Statistical Inference, Learning, and Models for
  Big Data, Fields Institute; 23 pages, 2 figures</comments><msc-class>62-07</msc-class><acm-class>I.2.6; I.2.3; I.5.1; G.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The need for new methods to deal with big data is a common theme in most
scientific fields, although its definition tends to vary with the context.
Statistical ideas are an essential part of this, and as a partial response, a
thematic program on statistical inference, learning, and models in big data was
held in 2015 in Canada, under the general direction of the Canadian Statistical
Sciences Institute, with major funding from, and most activities located at,
the Fields Institute for Research in Mathematical Sciences. This paper gives an
overview of the topics covered, describing challenges and strategies that seem
common to many different areas of application, and including some examples of
applications to make these challenges and strategies more concrete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02908</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02908</id><created>2015-09-05</created><updated>2015-09-24</updated><authors><author><keyname>Bowen</keyname><forenames>Jonathan P.</forenames></author></authors><title>Provably Correct Systems: Community, connections, and citations</title><categories>cs.DL cs.LO</categories><comments>15 pages, 4 figures. Presented at Festschrift for Prof. Dr
  Ernst-R\&quot;udiger Olderog, University of Oldenburg, Germany, 8-9 September 2015</comments><acm-class>D.2.4; D.2.8; F.4.1; I.2.4; J.4; K.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The original European ESPRIT ProCoS I and II projects on Provably Correct
Systems} took place around a quarter of a century ago. Since then the legacy of
the initiative has spawned many researchers with careers in formal methods. One
of the leaders on the ProCoS projects was Ernst-R\&quot;udiger Olderog. This paper
charts the influence of the ProCoS projects and the subsequent ProCoS-WG
Working Group, using Prof. Dr Olderog as an example. The community of
researchers surrounding an initiative such as ProCoS is considered in the
context of the social science concept of a Community of Practice (CoP) and the
collaborations undertaken through coauthorship of and citations to
publications. Consideration of citation metrics is also included.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02911</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02911</id><created>2015-09-09</created><authors><author><keyname>Gharaibeh</keyname><forenames>Ammar</forenames></author><author><keyname>Khreishah</keyname><forenames>Abdallah</forenames></author><author><keyname>Ji</keyname><forenames>Bo</forenames></author><author><keyname>Ayyash</keyname><forenames>Moussa</forenames></author></authors><title>A provably efficient online collaborative caching algorithm for
  multicell-coordinated systems</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Caching at the base stations brings the contents closer to the users, reduces
the traffic through the backhaul links, and reduces the delay experienced by
the cellular users. The cellular network operator may charge the content
providers for caching their contents. Moreover, content providers may lose
their users if the users are not getting their desired quality of service, such
as maximum tolerable delay in Video on Demand services. In this paper, we study
the collaborative caching problem for a multicell-coordinated system from the
point of view of minimizing the total cost paid by the content providers. We
formulate the problem as an Integer Linear Program and prove its
NP-completeness. We also provide an online caching algorithm that does not
require any knowledge about the contents popularities. We prove that the online
algorithm achieves a competitive ratio of $\mathcal{O}(\log(n))$, and we show
that the best competitive ratio that any online algorithm can achieve is
$\Omega(\frac{\log(n)}{\log\log(n)})$. Therefore, our proposed caching
algorithm is provably efficient. Through simulations, we show that our online
algorithm performs very close to the optimal offline collaborative scheme, and
can outperform it when contents popularities are not properly estimated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02944</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02944</id><created>2015-09-09</created><authors><author><keyname>Eaton</keyname><forenames>Edward</forenames></author><author><keyname>Song</keyname><forenames>Fang</forenames></author></authors><title>Making Existential-Unforgeable Signatures Strongly Unforgeable in the
  Quantum Random-Oracle Model</title><categories>quant-ph cs.CR</categories><comments>15 pages, to appear in Proceedings TQC 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Strongly unforgeable signature schemes provide a more stringent security
guarantee than the standard existential unforgeability. It requires that not
only forging a signature on a new message is hard, it is infeasible as well to
produce a new signature on a message for which the adversary has seen valid
signatures before. Strongly unforgeable signatures are useful both in practice
and as a building block in many cryptographic constructions.
  This work investigates a generic transformation that compiles any
existential-unforgeable scheme into a strongly unforgeable one, which was
proposed by Teranishi et al. and was proven in the classical random-oracle
model. Our main contribution is showing that the transformation also works
against quantum adversaries in the quantum random-oracle model. We develop
proof techniques such as adaptively programming a quantum random-oracle in a
new setting, which could be of independent interest. Applying the
transformation to an existential-unforgeable signature scheme due to Cash et
al., which can be shown to be quantum-secure assuming certain lattice problems
are hard for quantum computers, we get an efficient quantum-secure strongly
unforgeable signature scheme in the quantum random-oracle model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02953</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02953</id><created>2015-09-09</created><updated>2015-09-21</updated><authors><author><keyname>Vaezi</keyname><forenames>Mojtaba</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>The Capacity of Mixed and One-Sided Gaussian Interference Channels</title><categories>cs.IT math.IT</categories><comments>This paper has been withdrawn due to a flaw in the proof of the outer
  bound</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper adds to the understanding of the capacity region of the Gaussian
interference channel. To this end, the capacity region of the one-sided
Gaussian interference channel is first fully characterized. This is
accomplished by introducing a new representation of the Han-Kobayashi region
and providing a new outer bound on the capacity region of this channel which is
tight both in the weak and strong interference regimes. In light of this
capacity result, the capacity region of the degraded Gaussian interference
channel is established immediately. Next, by combining the capacity regions of
the one-sided interference channels in the weak and strong interference
regimes, new outer bounds on the capacity region of the interference channel
are introduced, in various interference regimes. It is then proved that the
outer bound corresponding to the mixed interference regime, in which one of the
receivers is subject to strong interference while the other one suffers from
weak interference, is tight in a broad range of this regime. The new capacity
results, on the whole, confirm the optimality of decoding part of the
interference and treating the rest as noise. The optimum amount to be decoded
varies from 0 to 100% of the interfering signal, depending on the relative
importance of the users' rates (i.e., the ratio of weights in the weighted
sum-rate), their transmission powers, and the gain of the weak interference
link. Optimal values are found explicitly, based on the above parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02954</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02954</id><created>2015-09-09</created><authors><author><keyname>Wang</keyname><forenames>Joseph</forenames></author><author><keyname>Trapeznikov</keyname><forenames>Kirill</forenames></author><author><keyname>Saligrama</keyname><forenames>Venkatesh</forenames></author></authors><title>Sensor Selection by Linear Programming</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We learn sensor trees from training data to minimize sensor acquisition costs
during test time. Our system adaptively selects sensors at each stage if
necessary to make a confident classification. We pose the problem as empirical
risk minimization over the choice of trees and node decision rules. We
decompose the problem, which is known to be intractable, into combinatorial
(tree structures) and continuous parts (node decision rules) and propose to
solve them separately. Using training data we greedily solve for the
combinatorial tree structures and for the continuous part, which is a
non-convex multilinear objective function, we derive convex surrogate loss
functions that are piecewise linear. The resulting problem can be cast as a
linear program and has the advantage of guaranteed convergence, global
optimality, repeatability and computational efficiency. We show that our
proposed approach outperforms the state-of-art on a number of benchmark
datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02955</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02955</id><created>2015-09-09</created><authors><author><keyname>Jaggard</keyname><forenames>Aaron D.</forenames></author><author><keyname>Lutz</keyname><forenames>Neil</forenames></author><author><keyname>Schapira</keyname><forenames>Michael</forenames></author><author><keyname>Wright</keyname><forenames>Rebecca N.</forenames></author></authors><title>Dynamics at the Boundary of Game Theory and Distributed Computing</title><categories>cs.GT</categories><comments>This article includes significantly revised and extended material
  from the conference papers arXiv:0910.1585 and arXiv:1403.5791</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We use ideas from distributed computing and game theory to study dynamic and
decentralized environments in which computational nodes, or decision makers,
interact strategically and with limited information. In such environments,
which arise in many real-world settings, the participants act as both economic
and computational entities. We exhibit a general non-convergence result for a
broad class of dynamics in asynchronous settings. We consider implications of
our result across a wide variety of interesting and timely applications:
circuit design, social networks, Internet routing, and congestion control. We
also study the computational and communication complexity of testing the
convergence of asynchronous dynamics, as well as the effects of limited
asynchrony. For uncoupled game dynamics, in which preferences are private
inputs, we give new bounds on the recall necessary for self stabilization to an
equilibrium. Our work opens a new avenue for research at the intersection of
distributed computing and game theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02956</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02956</id><created>2015-09-09</created><authors><author><keyname>Boshkovska</keyname><forenames>Elena</forenames></author><author><keyname>Ng</keyname><forenames>Derrick Wing Kwan</forenames></author><author><keyname>Zlatanov</keyname><forenames>Nikola</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author></authors><title>Practical Non-linear Energy Harvesting Model and Resource Allocation for
  SWIPT Systems</title><categories>cs.IT math.IT</categories><comments>Accepted for publication, IEEE Communications Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this letter, we propose a practical non-linear energy harvesting model and
design a resource allocation algorithm for simultaneous wireless information
and power transfer (SWIPT) systems. The algorithm design is formulated as a
non-convex optimization problem for the maximization of the total harvested
power at energy harvesting receivers subject to minimum required
signal-to-interference-plus-noise ratios (SINRs) at multiple information
receivers. We transform the considered non-convex objective function from
sum-of-ratios form into an equivalent objective function in subtractive form,
which enables the derivation of an efficient iterative resource allocation
algorithm. In each iteration, a rank-constrained semidefinite program (SDP) is
solved optimally by SDP relaxation. Numerical results unveil a substantial
performance gain that can be achieved if the resource allocation design is
based on the proposed non-linear energy harvesting model instead of the
traditional linear model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02962</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02962</id><created>2015-09-09</created><authors><author><keyname>Stuhlm&#xfc;ller</keyname><forenames>Andreas</forenames></author><author><keyname>Hawkins</keyname><forenames>Robert X. D.</forenames></author><author><keyname>Siddharth</keyname><forenames>N.</forenames></author><author><keyname>Goodman</keyname><forenames>Noah D.</forenames></author></authors><title>Coarse-to-Fine Sequential Monte Carlo for Probabilistic Programs</title><categories>cs.AI stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many practical techniques for probabilistic inference require a sequence of
distributions that interpolate between a tractable distribution and an
intractable distribution of interest. Usually, the sequences used are simple,
e.g., based on geometric averages between distributions. When models are
expressed as probabilistic programs, the models themselves are highly
structured objects that can be used to derive annealing sequences that are more
sensitive to domain structure. We propose an algorithm for transforming
probabilistic programs to coarse-to-fine programs which have the same marginal
distribution as the original programs, but generate the data at increasing
levels of detail, from coarse to fine. We apply this algorithm to an Ising
model, its depth-from-disparity variation, and a factorial hidden Markov model.
We show preliminary evidence that the use of coarse-to-fine models can make
existing generic inference algorithms more efficient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02970</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02970</id><created>2015-09-09</created><authors><author><keyname>Koniusz</keyname><forenames>Piotr</forenames></author><author><keyname>Cherian</keyname><forenames>Anoop</forenames></author></authors><title>Dictionary Learning and Sparse Coding for Third-order Super-symmetric
  Tensors</title><categories>cs.CV</categories><comments>13 pages, NIPS</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Super-symmetric tensors - a higher-order extension of scatter matrices - are
becoming increasingly popular in machine learning and computer vision for
modelling data statistics, co-occurrences, or even as visual descriptors.
However, the size of these tensors are exponential in the data dimensionality,
which is a significant concern. In this paper, we study third-order
super-symmetric tensor descriptors in the context of dictionary learning and
sparse coding. Our goal is to approximate these tensors as sparse conic
combinations of atoms from a learned dictionary, where each atom is a symmetric
positive semi-definite matrix. Apart from the significant benefits to tensor
compression that this framework provides, our experiments demonstrate that the
sparse coefficients produced by the scheme lead to better aggregation of
high-dimensional data, and showcases superior performance on two common
computer vision tasks compared to the state-of-the-art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02971</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02971</id><created>2015-09-09</created><updated>2016-02-29</updated><authors><author><keyname>Lillicrap</keyname><forenames>Timothy P.</forenames></author><author><keyname>Hunt</keyname><forenames>Jonathan J.</forenames></author><author><keyname>Pritzel</keyname><forenames>Alexander</forenames></author><author><keyname>Heess</keyname><forenames>Nicolas</forenames></author><author><keyname>Erez</keyname><forenames>Tom</forenames></author><author><keyname>Tassa</keyname><forenames>Yuval</forenames></author><author><keyname>Silver</keyname><forenames>David</forenames></author><author><keyname>Wierstra</keyname><forenames>Daan</forenames></author></authors><title>Continuous control with deep reinforcement learning</title><categories>cs.LG stat.ML</categories><comments>10 pages + supplementary</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We adapt the ideas underlying the success of Deep Q-Learning to the
continuous action domain. We present an actor-critic, model-free algorithm
based on the deterministic policy gradient that can operate over continuous
action spaces. Using the same learning algorithm, network architecture and
hyper-parameters, our algorithm robustly solves more than 20 simulated physics
tasks, including classic problems such as cartpole swing-up, dexterous
manipulation, legged locomotion and car driving. Our algorithm is able to find
policies whose performance is competitive with those found by a planning
algorithm with full access to the dynamics of the domain and its derivatives.
We further demonstrate that for many of the tasks the algorithm can learn
policies end-to-end: directly from raw pixel inputs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02972</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02972</id><created>2015-09-09</created><authors><author><keyname>Lichtman</keyname><forenames>Jared D.</forenames></author></authors><title>On the Multidimensional Stable Marriage Problem</title><categories>cs.GT cs.DM cs.DS</categories><comments>8 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a problem definition of the stable marriage problem for a general
number of parties $p$ under a natural preference scheme in which each person
has simple lists for the other parties. We extend the notion of stability in a
natural way and present so called elemental and compound algorithms to generate
matchings for a problem instance. We demonstrate the stability of matchings
generated by both algorithms, as well as show that the former runs in $O(pn^2)$
time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02975</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02975</id><created>2015-09-09</created><authors><author><keyname>Huntsman</keyname><forenames>Steve</forenames></author><author><keyname>Rezaee</keyname><forenames>Arman</forenames></author></authors><title>De Bruijn entropy and string similarity</title><categories>cs.DM cs.IT math.CO math.IT</categories><comments>Extended version of a paper presented at WORDS 2015; MATLAB source
  code and scripts for reproducing results are included</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the notion of de Bruijn entropy of an Eulerian quiver and show
how the corresponding relative entropy can be applied to practical string
similarity problems. This approach explicitly links the combinatorial and
information-theoretical properties of words and its performance is superior to
edit distances in many respects and competitive in most others. The
computational complexity of our current implementation is parametrically
tunable between linear and cubic, and we outline how an optimized linear
algebra subroutine can reduce the cubic complexity to approximately linear.
Numerous examples are provided, including a realistic application to molecular
phylogenetics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02981</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02981</id><created>2015-09-09</created><updated>2015-10-29</updated><authors><author><keyname>Song</keyname><forenames>Yangbo</forenames></author></authors><title>Social Learning with Coordination Motives</title><categories>cs.SI cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The theoretical study of social learning typically assumes that each agent's
action affects only her own payoff. In this paper, I present a model in which
agents' actions directly affect the payoffs of other agents. On a discrete time
line, there is a community containing a random number of agents in each period.
Before each agent needs to take an action, the community receives a private
signal about the underlying state of the world and may observe some past
actions in previous communities. An agent's payoff is higher if her action
matches the state or if more agents take the same action as hers. I analyze two
observation structures: exogenous observation and costly strategic observation.
In both cases, coordination motives enhance social learning in the sense that
agents take the correct action with significantly higher probability when the
community size is greater than a threshold. In particular, this probability
reaches one (asymptotic learning) with unbounded private beliefs and can be
arbitrarily close to one with bounded private beliefs. I then discuss the issue
of multiple equilibria and use risk dominance as a criterion for equilibrium
selection. I find that in the selected equilibria, the community size has no
effect on learning under exogenous observation, facilitates learning under
endogenous observation and unbounded private beliefs, and either helps or
hinders learning under endogenous observation and bounded private beliefs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02984</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02984</id><created>2015-09-09</created><authors><author><keyname>Andika</keyname></author><author><keyname>Abdillah</keyname><forenames>Leon Andretti</forenames></author><author><keyname>Ariandi</keyname><forenames>Muhammad</forenames></author></authors><title>Sistem Informasi Geografis Ruang Terbuka Hijau Kawasan Perkotaan (RTHKP)
  Palembang</title><categories>cs.CY</categories><comments>6 pages, Student Colloquium Sistem Informasi &amp; Teknik Informatika
  (SC-SITI). Andika, et al., &quot;Sistem Informasi Geografis Ruang Terbuka Hijau
  Kawasan Perkotaan (RTHKP) Palembang,&quot; presented at the Student Colloquium
  Sistem Informasi &amp; Teknik Informatika (SC-SITI) 2015, Palembang, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Geographic Information System (GIS) is a computer-based system used to store
and manipulate geographic information. In this study, GIS is used to obtain
information about &quot;open green space of urban areas&quot; (RTHKP). Office of street
lighting and Cemetery Palembang is one agency that regulates the green open
spaces but not using media such as websites that support the community to get
information about the open green space of urban areas in the city of Palembang,
so it was apparent from the author will build a system RTKHP with geographic
information system development methodology Rational Unified Process (RUP), the
programming language PHP and uses a MySQL database. With the GIS open green
space of urban areas that will be made later can help facilitate the public to
get information related to RTHKP and assist the Department of street lighting
and landscaping burial in managing and providing related information RTHKP in
Palembang so that delivery of information to be more effective.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02986</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02986</id><created>2015-09-09</created><authors><author><keyname>Adam</keyname><forenames>Gina C.</forenames></author><author><keyname>Hoskins</keyname><forenames>Brian D.</forenames></author><author><keyname>Prezioso</keyname><forenames>Mirko</forenames></author><author><keyname>Strukov</keyname><forenames>Dmitri B.</forenames></author></authors><title>Three-Dimensional Stateful Material Implication Logic</title><categories>cs.ET cond-mat.mes-hall</categories><comments>24 pages, 13 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Monolithic three-dimensional integration of memory and logic circuits could
dramatically improve performance and energy efficiency of computing systems.
Some conventional and emerging memories are suitable for vertical integration,
including highly scalable metal-oxide resistive switching devices (memristors),
yet integration of logic circuits proves to be much more challenging. Here we
demonstrate memory and logic functionality in a monolithic three-dimensional
circuit by adapting recently proposed memristor-based stateful material
implication logic. Though such logic has been already implemented with a
variety of memory devices, prohibitively large device variability in the most
prospective memristor-based circuits has limited experimental demonstrations to
simple gates and just a few cycles of operations. By developing a
low-temperature, low-variability fabrication process, and modifying the
original circuit to increase its robustness to device imperfections, we
experimentally show, for the first time, reliable multi-cycle multi-gate
material implication logic operation within a three-dimensional stack of
monolithically integrated memristors. The direct data manipulation in three
dimensions enables extremely compact and high-throughput logic-in-memory
computing and, remarkably, presents a viable solution for the Feynman grand
challenge of implementing an 8-bit adder at the nanoscale.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02992</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02992</id><created>2015-09-09</created><authors><author><keyname>Ackerman</keyname><forenames>Nathanael L.</forenames></author><author><keyname>Freer</keyname><forenames>Cameron E.</forenames></author><author><keyname>Roy</keyname><forenames>Daniel M.</forenames></author></authors><title>On computability and disintegration</title><categories>math.LO cs.LO math.PR math.ST stat.TH</categories><comments>29 pages</comments><msc-class>Primary: 03F60, 28A50, Secondary: 68Q17, 60A05, 62A01, 65C50, 68Q87</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the disintegration operator on a complete separable metric space
along a projection map, restricted to measures for which the disintegration is
unique and continuous, is strongly Weihrauch equivalent to the limit operator
Lim. When a measure does not have a unique continuous disintegration, we may
still obtain a disintegration when some basis of continuity sets has the Vitali
covering property with respect to the measure; the disintegration, however, may
depend on the choice of sets. We show that, when the basis is computable, the
resulting disintegration is strongly Weihrauch reducible to Lim, and further
exhibit a single distribution realizing this upper bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.02995</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.02995</id><created>2015-09-09</created><authors><author><keyname>Dai</keyname><forenames>Wei</forenames></author><author><keyname>Cheung</keyname><forenames>Gene</forenames></author><author><keyname>Cheung</keyname><forenames>Ngai-Man</forenames></author><author><keyname>Ortega</keyname><forenames>Antonio</forenames></author><author><keyname>Au</keyname><forenames>Oscar C.</forenames></author></authors><title>Merge Frame Design for Video Stream Switching using Piecewise Constant
  Functions</title><categories>cs.MM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ability to efficiently switch from one pre-encoded video stream to
another (e.g., for bitrate adaptation or view switching) is important for many
interactive streaming applications. Recently, stream-switching mechanisms based
on distributed source coding (DSC) have been proposed. In order to reduce the
overall transmission rate, these approaches provide a &quot;merge&quot; mechanism, where
information is sent to the decoder such that the exact same frame can be
reconstructed given that any one of a known set of side information (SI) frames
is available at the decoder (e.g., each SI frame may correspond to a different
stream from which we are switching). However, the use of bit-plane coding and
channel coding in many DSC approaches leads to complex coding and decoding. In
this paper, we propose an alternative approach for merging multiple SI frames,
using a piecewise constant (PWC) function as the merge operator. In our
approach, for each block to be reconstructed, a series of parameters of these
PWC merge functions are transmitted in order to guarantee identical
reconstruction given the known side information blocks. We consider two
different scenarios. In the first case, a target frame is first given, and then
merge parameters are chosen so that this frame can be reconstructed exactly at
the decoder. In contrast, in the second scenario, the reconstructed frame and
merge parameters are jointly optimized to meet a rate-distortion criteria.
Experiments show that for both scenarios, our proposed merge techniques can
outperform both a recent approach based on DSC and the SP-frame approach in
H.264, in terms of compression efficiency and decoder complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03000</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03000</id><created>2015-09-09</created><authors><author><keyname>Pradhan</keyname><forenames>Chandan</forenames></author><author><keyname>Murthy</keyname><forenames>Garimella Rama</forenames></author></authors><title>Full-Duplex Transceiver for Future Cellular Network: A Smart Antenna
  Approach</title><categories>cs.NI cs.IT math.IT</categories><comments>arXiv admin note: text overlap with arXiv:1506.02132</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a transceiver architecture for full-duplex (FD)
eNodeB (eNB) and FD user equipment (UE) transceiver. For FD
communication,.i.e., simultaneous in-band uplink and downlink operation, same
subcarriers can be allocated to UE in both uplink and downlink. Hence, contrary
to traditional LTE, we propose using single-carrier frequency division multiple
accesses (SC-FDMA) for downlink along with the conventional method of using it
for uplink. The use of multiple antennas at eNB and singular value
decomposition (SVD) in the downlink allows multiple users (MU) to operate on
the same set of ubcarriers. In the uplink, successive interference cancellation
with optimal ordering (SSIC-OO) algorithm is used to decouple signals of UEs
operating in the same set of subcarriers. A smart antenna approach is adopted
which prevents interference, in downlink of a UE, from uplink signals of other
UEs sharing same subcarriers. The approach includes using multiple antennas at
UEs to form directed beams towards eNode and nulls towards other UEs. The
proposed architecture results in significant improvement of the overall
spectrum efficiency per cell of the cellular network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03001</identifier>
 <datestamp>2015-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03001</id><created>2015-09-09</created><updated>2015-10-14</updated><authors><author><keyname>Kang</keyname><forenames>Byeongkeun</forenames></author><author><keyname>Tripathi</keyname><forenames>Subarna</forenames></author><author><keyname>Nguyen</keyname><forenames>Truong Q.</forenames></author></authors><title>Real-time Sign Language Fingerspelling Recognition using Convolutional
  Neural Networks from Depth map</title><categories>cs.CV</categories><comments>2015 3rd IAPR Asian Conference on Pattern Recognition (ACPR)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sign language recognition is important for natural and convenient
communication between deaf community and hearing majority. We take the highly
efficient initial step of automatic fingerspelling recognition system using
convolutional neural networks (CNNs) from depth maps. In this work, we consider
relatively larger number of classes compared with the previous literature. We
train CNNs for the classification of 31 alphabets and numbers using a subset of
collected depth data from multiple subjects. While using different learning
configurations, such as hyper-parameter selection with and without validation,
we achieve 99.99% accuracy for observed signers and 83.58% to 85.49% accuracy
for new signers. The result shows that accuracy improves as we include more
data from different subjects during training. The processing time is 3 ms for
the prediction of a single image. To the best of our knowledge, the system
achieves the highest accuracy and speed. The trained model and dataset is
available on our repository.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03002</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03002</id><created>2015-09-10</created><authors><author><keyname>Zhao</keyname><forenames>Dawei</forenames></author><author><keyname>Wang</keyname><forenames>Lianhai</forenames></author><author><keyname>Wang</keyname><forenames>Zhen</forenames></author></authors><title>The robustness of multiplex networks under layer node-based attack</title><categories>cs.SI cs.SY physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  From transportation networks to complex infrastructures, and to social and
economic networks, a large variety of systems can be described in terms of
multiplex networks formed by a set of nodes interacting through different
network layers. Network robustness, as one of the most successful application
areas of complex networks, has also attracted great interest in both
theoretical and empirical researches. However, the vast majority of existing
researches mainly focus on the robustness of single-layer networks an
interdependent networks, how multiplex networks respond to potential attack is
still short of further exploration. Here we study the robustness of multiplex
networks under two attack strategies: layer node-based random attack and layer
node-based targeted attack. A theoretical analysis framework is proposed to
calculate the critical threshold and the size of giant component of multiplex
networks when a fraction of layer nodes are removed randomly or intentionally.
Via numerous simulations, it is unveiled that the theoretical method can
accurately predict the threshold and the size of giant component, irrespective
of attack strategies. Moreover, we also compare the robustness of multiplex
networks under multiplex node-based attack and layer node-based attack, and
find that layer node-based attack makes multiplex networks more vulnerable,
regardless of average degree and underlying topology. Our finding may shed new
light on the protection of multiplex networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03005</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03005</id><created>2015-09-10</created><authors><author><keyname>Balduzzi</keyname><forenames>David</forenames></author><author><keyname>Ghifary</keyname><forenames>Muhammad</forenames></author></authors><title>Compatible Value Gradients for Reinforcement Learning of Continuous Deep
  Policies</title><categories>cs.LG cs.AI cs.NE stat.ML</categories><comments>27 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes GProp, a deep reinforcement learning algorithm for
continuous policies with compatible function approximation. The algorithm is
based on two innovations. Firstly, we present a temporal-difference based
method for learning the gradient of the value-function. Secondly, we present
the deviator-actor-critic (DAC) model, which comprises three neural networks
that estimate the value function, its gradient, and determine the actor's
policy respectively. We evaluate GProp on two challenging tasks: a contextual
bandit problem constructed from nonparametric regression datasets that is
designed to probe the ability of reinforcement learning algorithms to
accurately estimate gradients; and the octopus arm, a challenging reinforcement
learning benchmark. GProp is competitive with fully supervised methods on the
bandit task and achieves the best performance to date on the octopus arm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03013</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03013</id><created>2015-09-10</created><authors><author><keyname>Charalambidis</keyname><forenames>Angelos</forenames></author><author><keyname>Rondogiannis</keyname><forenames>Panos</forenames></author><author><keyname>Symeonidou</keyname><forenames>Ioanna</forenames></author></authors><title>Equivalence of two Fixed-Point Semantics for Definitional Higher-Order
  Logic Programs</title><categories>cs.LO cs.PL</categories><comments>In Proceedings FICS 2015, arXiv:1509.02826</comments><proxy>EPTCS</proxy><acm-class>D.1.6;F.3.2;F.4.1</acm-class><journal-ref>EPTCS 191, 2015, pp. 18-32</journal-ref><doi>10.4204/EPTCS.191.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Two distinct research approaches have been proposed for assigning a purely
extensional semantics to higher-order logic programming. The former approach
uses classical domain theoretic tools while the latter builds on a fixed-point
construction defined on a syntactic instantiation of the source program. The
relationships between these two approaches had not been investigated until now.
In this paper we demonstrate that for a very broad class of programs, namely
the class of definitional programs introduced by W. W. Wadge, the two
approaches coincide (with respect to ground atoms that involve symbols of the
program). On the other hand, we argue that if existential higher-order
variables are allowed to appear in the bodies of program rules, the two
approaches are in general different. The results of the paper contribute to a
better understanding of the semantics of higher-order logic programming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03014</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03014</id><created>2015-09-10</created><authors><author><keyname>Eguchi</keyname><forenames>Naohi</forenames></author></authors><title>Formalizing Termination Proofs under Polynomial Quasi-interpretations</title><categories>cs.LO cs.CC cs.PL</categories><comments>In Proceedings FICS 2015, arXiv:1509.02826</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 191, 2015, pp. 33-47</journal-ref><doi>10.4204/EPTCS.191.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Usual termination proofs for a functional program require to check all the
possible reduction paths. Due to an exponential gap between the height and size
of such the reduction tree, no naive formalization of termination proofs yields
a connection to the polynomial complexity of the given program. We solve this
problem employing the notion of minimal function graph, a set of pairs of a
term and its normal form, which is defined as the least fixed point of a
monotone operator. We show that termination proofs for programs reducing under
lexicographic path orders (LPOs for short) and polynomially quasi-interpretable
can be optimally performed in a weak fragment of Peano arithmetic. This yields
an alternative proof of the fact that every function computed by an
LPO-terminating, polynomially quasi-interpretable program is computable in
polynomial space. The formalization is indeed optimal since every
polynomial-space computable function can be computed by such a program. The
crucial observation is that inductive definitions of minimal function graphs
under LPO-terminating programs can be approximated with transfinite induction
along LPOs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03015</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03015</id><created>2015-09-10</created><authors><author><keyname>&#xc9;sik</keyname><forenames>Zolt&#xe1;n</forenames><affiliation>University of Szeged</affiliation></author><author><keyname>Fahrenberg</keyname><forenames>Uli</forenames><affiliation>Inria Rennes</affiliation></author><author><keyname>Legay</keyname><forenames>Axel</forenames><affiliation>Inria Rennes</affiliation></author></authors><title>*-Continuous Kleene $\omega$-Algebras for Energy Problems</title><categories>cs.LO cs.FL</categories><comments>In Proceedings FICS 2015, arXiv:1509.02826</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 191, 2015, pp. 48-59</journal-ref><doi>10.4204/EPTCS.191.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy problems are important in the formal analysis of embedded or
autonomous systems. Using recent results on star-continuous Kleene
omega-algebras, we show here that energy problems can be solved by algebraic
manipulations on the transition matrix of energy automata. To this end, we
prove general results about certain classes of finitely additive functions on
complete lattices which should be of a more general interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03016</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03016</id><created>2015-09-10</created><authors><author><keyname>Gurov</keyname><forenames>Dilian</forenames><affiliation>KTH Royal Institute of Technology, Stockholm, Sweden</affiliation></author><author><keyname>Markov</keyname><forenames>Minko</forenames><affiliation>St. Kliment Ohridski University of Sofia, Sofia, Bulgaria</affiliation></author></authors><title>Self-Correlation and Maximum Independence in Finite Relations</title><categories>cs.DM cs.DB</categories><comments>In Proceedings FICS 2015, arXiv:1509.02826</comments><proxy>EPTCS</proxy><acm-class>G.2.3; H.2.1</acm-class><journal-ref>EPTCS 191, 2015, pp. 60-74</journal-ref><doi>10.4204/EPTCS.191.7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider relations with no order on their attributes as in Database
Theory. An independent partition of the set of attributes S of a finite
relation R is any partition X of S such that the join of the projections of R
over the elements of X yields R. Identifying independent partitions has many
applications and corresponds conceptually to revealing orthogonality between
sets of dimensions in multidimensional point spaces. A subset of S is termed
self-correlated if there is a value of each of its attributes such that no
tuple of R contains all those values. This paper uncovers a connection between
independence and self-correlation, showing that the maximum independent
partition is the least fixed point of a certain inflationary transformer alpha
that operates on the finite lattice of partitions of S. alpha is defined via
the minimal self-correlated subsets of S. We use some additional properties of
alpha to show the said fixed point is still the limit of the standard
approximation sequence, just as in Kleene's well-known fixed point theorem for
continuous functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03017</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03017</id><created>2015-09-10</created><authors><author><keyname>Hansen</keyname><forenames>Helle Hvid</forenames><affiliation>Delft University of Technology, Delft, The Netherlands</affiliation></author><author><keyname>Kupke</keyname><forenames>Clemens</forenames><affiliation>University of Strathclyde, Glasgow, United Kingdom</affiliation></author></authors><title>Weak Completeness of Coalgebraic Dynamic Logics</title><categories>cs.LO</categories><comments>In Proceedings FICS 2015, arXiv:1509.02826</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 191, 2015, pp. 90-104</journal-ref><doi>10.4204/EPTCS.191.9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a coalgebraic generalisation of Fischer and Ladner's Propositional
Dynamic Logic (PDL) and Parikh's Game Logic (GL). In earlier work, we proved a
generic strong completeness result for coalgebraic dynamic logics without
iteration. The coalgebraic semantics of such programs is given by a monad T,
and modalities are interpreted via a predicate lifting &#xce;&#xbb; whose transpose is a
monad morphism from T to the neighbourhood monad. In this paper, we show that
if the monad T carries a complete semilattice structure, then we can define an
iteration construct, and suitable notions of diamond-likeness and box-likeness
of predicate-liftings which allows for the definition of an axiomatisation
parametric in T, &#xce;&#xbb; and a chosen set of pointwise program operations. As our
main result, we show that if the pointwise operations are &quot;negation-free&quot; and
Kleisli composition left-distributes over the induced join on Kleisli arrows,
then this axiomatisation is weakly complete with respect to the class of
standard models. As special instances, we recover the weak completeness of PDL
and of dual-free Game Logic. As a modest new result we obtain completeness for
dual-free GL extended with intersection (demonic choice) of games.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03018</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03018</id><created>2015-09-10</created><authors><author><keyname>Lange</keyname><forenames>Martin</forenames><affiliation>University of Kassel</affiliation></author></authors><title>The Arity Hierarchy in the Polyadic $\mu$-Calculus</title><categories>cs.LO</categories><comments>In Proceedings FICS 2015, arXiv:1509.02826</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 191, 2015, pp. 105-116</journal-ref><doi>10.4204/EPTCS.191.10</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The polyadic mu-calculus is a modal fixpoint logic whose formulas define
relations of nodes rather than just sets in labelled transition systems. It can
express exactly the polynomial-time computable and bisimulation-invariant
queries on finite graphs. In this paper we show a hierarchy result with respect
to expressive power inside the polyadic mu-calculus: for every level of
fixpoint alternation, greater arity of relations gives rise to higher
expressive power. The proof uses a diagonalisation argument.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03019</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03019</id><created>2015-09-10</created><authors><author><keyname>Lehtinen</keyname><forenames>Karoliina</forenames></author></authors><title>Disjunctive form and the modal $\mu$ alternation hierarchy</title><categories>cs.LO</categories><comments>In Proceedings FICS 2015, arXiv:1509.02826</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 191, 2015, pp. 117-131</journal-ref><doi>10.4204/EPTCS.191.11</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies the relationship between disjunctive form, a syntactic
normal form for the modal mu calculus, and the alternation hierarchy. First it
shows that all disjunctive formulas which have equivalent tableau have the same
syntactic alternation depth. However, tableau equivalence only preserves
alternation depth for the disjunctive fragment: there are disjunctive formulas
with arbitrarily high alternation depth that are tableau equivalent to
alternation-free non-disjunctive formulas. Conversely, there are
non-disjunctive formulas of arbitrarily high alternation depth that are tableau
equivalent to disjunctive formulas without alternations. This answers
negatively the so far open question of whether disjunctive form preserves
alternation depth. The classes of formulas studied here illustrate a previously
undocumented type of avoidable syntactic complexity which may contribute to our
understanding of why deciding the alternation hierarchy is still an open
problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03020</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03020</id><created>2015-09-10</created><authors><author><keyname>Lozes</keyname><forenames>Etienne</forenames><affiliation>ENS Cachan, CNRS</affiliation></author></authors><title>A Type-Directed Negation Elimination</title><categories>cs.LO cs.PL</categories><comments>In Proceedings FICS 2015, arXiv:1509.02826</comments><proxy>EPTCS</proxy><acm-class>F4.1</acm-class><journal-ref>EPTCS 191, 2015, pp. 132-142</journal-ref><doi>10.4204/EPTCS.191.12</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the modal mu-calculus, a formula is well-formed if each recursive variable
occurs underneath an even number of negations. By means of De Morgan's laws, it
is easy to transform any well-formed formula into an equivalent formula without
negations &#xe2;&#x80;&#x93; its negation normal form. Moreover, if the formula is of size n,
its negation normal form of is of the same size O(n). The full modal
mu-calculus and the negation normal form fragment are thus equally expressive
and concise.
  In this paper we extend this result to the higher-order modal fixed point
logic (HFL), an extension of the modal mu-calculus with higher-order recursive
predicate transformers. We present a procedure that converts a formula into an
equivalent formula without negations of quadratic size in the worst case and of
linear size when the number of variables of the formula is fixed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03021</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03021</id><created>2015-09-10</created><authors><author><keyname>Torrini</keyname><forenames>Paolo</forenames><affiliation>KU Leuven</affiliation></author><author><keyname>Schrijvers</keyname><forenames>Tom</forenames><affiliation>KU Leuven</affiliation></author></authors><title>Reasoning about modular datatypes with Mendler induction</title><categories>cs.LO</categories><comments>In Proceedings FICS 2015, arXiv:1509.02826</comments><proxy>EPTCS</proxy><acm-class>I.2.3; F.4.1</acm-class><journal-ref>EPTCS 191, 2015, pp. 143-157</journal-ref><doi>10.4204/EPTCS.191.13</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In functional programming, datatypes a la carte provide a convenient modular
representation of recursive datatypes, based on their initial algebra
semantics. Unfortunately it is highly challenging to implement this technique
in proof assistants that are based on type theory, like Coq. The reason is that
it involves type definitions, such as those of type-level fixpoint operators,
that are not strictly positive. The known work-around of impredicative
encodings is problematic, insofar as it impedes conventional inductive
reasoning. Weak induction principles can be used instead, but they considerably
complicate proofs.
  This paper proposes a novel and simpler technique to reason inductively about
impredicative encodings, based on Mendler-style induction. This technique
involves dispensing with dependent induction, ensuring that datatypes can be
lifted to predicates and relying on relational formulations. A case study on
proving subject reduction for structural operational semantics illustrates that
the approach enables modular proofs, and that these proofs are essentially
similar to conventional ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03025</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03025</id><created>2015-09-10</created><authors><author><keyname>Chen</keyname><forenames>Yudong</forenames></author><author><keyname>Wainwright</keyname><forenames>Martin J.</forenames></author></authors><title>Fast low-rank estimation by projected gradient descent: General
  statistical and algorithmic guarantees</title><categories>math.ST cs.LG stat.ML stat.TH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optimization problems with rank constraints arise in many applications,
including matrix regression, structured PCA, matrix completion and matrix
decomposition problems. An attractive heuristic for solving such problems is to
factorize the low-rank matrix, and to run projected gradient descent on the
nonconvex factorized optimization problem. The goal of this problem is to
provide a general theoretical framework for understanding when such methods
work well, and to characterize the nature of the resulting fixed point. We
provide a simple set of conditions under which projected gradient descent, when
given a suitable initialization, converges geometrically to a statistically
useful solution. Our results are applicable even when the initial solution is
outside any region of local convexity, and even when the problem is globally
concave. Working in a non-asymptotic framework, we show that our conditions are
satisfied for a wide range of concrete models, including matrix regression,
structured PCA, matrix completion with real and quantized observations, matrix
decomposition, and graph clustering problems. Simulation results show excellent
agreement with the theoretical predictions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03026</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03026</id><created>2015-09-10</created><authors><author><keyname>Kim</keyname><forenames>Joy</forenames></author><author><keyname>Monroy-Hernandez</keyname><forenames>Andres</forenames></author></authors><title>Storia: Summarizing Social Media Content based on Narrative Theory using
  Crowdsourcing</title><categories>cs.HC cs.CY</categories><acm-class>H.5.3</acm-class><doi>10.1145/2818048.2820072</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  People from all over the world use social media to share thoughts and
opinions about events, and understanding what people say through these channels
has been of increasing interest to researchers, journalists, and marketers
alike. However, while automatically generated summaries enable people to
consume large amounts of data efficiently, they do not provide the context
needed for a viewer to fully understand an event. Narrative structure can
provide templates for the order and manner in which this data is presented to
create stories that are oriented around narrative elements rather than
summaries made up of facts. In this paper, we use narrative theory as a
framework for identifying the links between social media content. To do this,
we designed crowdsourcing tasks to generate summaries of events based on
commonly used narrative templates. In a controlled study, for certain types of
events, people were more emotionally engaged with stories created with
narrative structure and were also more likely to recommend them to others
compared to summaries created without narrative structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03044</identifier>
 <datestamp>2015-11-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03044</id><created>2015-09-10</created><updated>2015-11-19</updated><authors><author><keyname>Li</keyname><forenames>Xiujun</forenames></author><author><keyname>Li</keyname><forenames>Lihong</forenames></author><author><keyname>Gao</keyname><forenames>Jianfeng</forenames></author><author><keyname>He</keyname><forenames>Xiaodong</forenames></author><author><keyname>Chen</keyname><forenames>Jianshu</forenames></author><author><keyname>Deng</keyname><forenames>Li</forenames></author><author><keyname>He</keyname><forenames>Ji</forenames></author></authors><title>Recurrent Reinforcement Learning: A Hybrid Approach</title><categories>cs.LG cs.AI cs.SY</categories><comments>11 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Successful applications of reinforcement learning in real-world problems
often require dealing with partially observable states. It is in general very
challenging to construct and infer hidden states as they often depend on the
agent's entire interaction history and may require substantial domain
knowledge. In this work, we investigate a deep-learning approach to learning
the representation of states in partially observable tasks, with minimal prior
knowledge of the domain. In particular, we propose a new family of hybrid
models that combines the strength of both supervised learning (SL) and
reinforcement learning (RL), trained in a joint fashion: The SL component can
be a recurrent neural networks (RNN) or its long short-term memory (LSTM)
version, which is equipped with the desired property of being able to capture
long-term dependency on history, thus providing an effective way of learning
the representation of hidden states. The RL component is a deep Q-network (DQN)
that learns to optimize the control for maximizing long-term rewards. Extensive
experiments in a direct mailing campaign problem demonstrate the effectiveness
and advantages of the proposed approach, which performs the best among a set of
previous state-of-the-art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03045</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03045</id><created>2015-09-10</created><authors><author><keyname>Wienhofen</keyname><forenames>Leendert</forenames></author><author><keyname>Mathisen</keyname><forenames>Bj&#xf8;rn Magnus</forenames></author><author><keyname>Roman</keyname><forenames>Dumitru</forenames></author></authors><title>Empirical Big Data Research: A Systematic Literature Mapping</title><categories>cs.DL cs.CY cs.DB</categories><comments>18 pages paper, 32 pages in total including references. Submitted to
  Elsevier Information Systems</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Background: Big Data is a relatively new field of research and technology,
and literature reports a wide variety of concepts labeled with Big Data. The
maturity of a research field can be measured in the number of publications
containing empirical results. In this paper we present the current status of
empirical research in Big Data. Method: We employed a systematic mapping method
with which we mapped the collected research according to the labels Variety,
Volume and Velocity. In addition, we addressed the application areas of Big
Data. Results: We found that 151 of the assessed 1778 contributions contain a
form of empirical result and can be mapped to one or more of the 3 V's and 59
address an application area. Conclusions: The share of publications containing
empirical results is well below the average compared to computer science
research as a whole. In order to mature the research on Big Data, we recommend
applying empirical methods to strengthen the confidence in the reported
results. Based on our trend analysis we consider Variety to be the most
promising uncharted area in Big Data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03046</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03046</id><created>2015-09-10</created><authors><author><keyname>Karpinski</keyname><forenames>Marek</forenames></author><author><keyname>Mark&#xf3;</keyname><forenames>Roland</forenames></author></authors><title>Explicit Bounds for Nondeterministically Testable Hypergraph Parameters</title><categories>cs.DS math.CO</categories><comments>26 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this note we give a new effective proof method for the equivalence of the
notions of testability and nondeterministic testability for uniform hypergraph
parameters. We provide the first effective upper bound on the sample complexity
of any nondeterministically testable $r$-uniform hypergraph parameter as a
function of the sample complexity of its witness parameter for arbitrary $r$.
The dependence is of the form of an exponential tower function with the height
linear in $r$. Our argument depends crucially on the new upper bounds for the
$r$-cut norm of sampled $r$-uniform hypergraphs. We employ also our approach
for some other restricted classes of hypergraph parameters, and present some
applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03048</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03048</id><created>2015-09-10</created><authors><author><keyname>Krajicek</keyname><forenames>Jan</forenames></author></authors><title>Consistency of circuit evaluation, extended resolution and total NP
  search problems</title><categories>math.LO cs.LO</categories><comments>Preliminary version 10.September 2015</comments><msc-class>03F20 (Primary), 68Q15 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider sets $\Gamma(n,s,k)$ of narrow clauses expressing that no
definition of a size $s$ circuit with $n$ inputs is refutable in resolution R
in $k$ steps. We show that every CNF shortly refutable in Extended R, ER, can
be easily reduced to an instance of $\Gamma(0,s,k)$ (with $s,k$ depending on
the size of the ER-refutation) and, in particular, that $\Gamma(0,s,k)$ when
interpreted as a relativized NP search problem is complete among all such
problems provably total in bounded arithmetic theory $V^1_1$.
  We use the ideas of implicit proofs to define from $\Gamma(0,s,k)$ a
non-relativized NP search problem $i\Gamma$ and we show that it is complete
among all such problems provably total in bounded arithmetic theory $V^1_2$.
The reductions are definable in $S^1_2$.
  We indicate how similar results can be proved for some other propositional
proof systems and bounded arithmetic theories and how the construction can be
used to define specific random unsatisfiable formulas, and we formulate two
open problems about them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03057</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03057</id><created>2015-09-10</created><authors><author><keyname>Yamakami</keyname><forenames>Tomoyuki</forenames></author></authors><title>The World of Combinatorial Fuzzy Problems and the Efficiency of Fuzzy
  Approximation Algorithms</title><categories>cs.AI cs.CC</categories><comments>A4, 10pt, 10 pages. This extended abstract already appeared in the
  Proceedings of the Joint 7th International Conference on Soft Computing and
  Intelligent Systems (SCIS 2014) and 15th International Symposium on Advanced
  Intelligent Systems (ISIS 2014), December 3-6, 2014, Institute of Electrical
  and Electronics Engineers (IEEE), pp. 29-35, 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We re-examine a practical aspect of combinatorial fuzzy problems of various
types, including search, counting, optimization, and decision problems. We are
focused only on those fuzzy problems that take series of fuzzy input objects
and produce fuzzy values. To solve such problems efficiently, we design fast
fuzzy algorithms, which are modeled by polynomial-time deterministic fuzzy
Turing machines equipped with read-only auxiliary tapes and write-only output
tapes and also modeled by polynomial-size fuzzy circuits composed of fuzzy
gates. We also introduce fuzzy proof verification systems to model the
fuzzification of nondeterminism. Those models help us identify four complexity
classes: Fuzzy-FPA of fuzzy functions, Fuzzy-PA and Fuzzy-NPA of fuzzy decision
problems, and Fuzzy-NPAO of fuzzy optimization problems. Based on a relative
approximation scheme targeting fuzzy membership degree, we formulate two
notions of &quot;reducibility&quot; in order to compare the computational complexity of
two fuzzy problems. These reducibility notions make it possible to locate the
most difficult fuzzy problems in Fuzzy-NPA and in Fuzzy-NPAO.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03067</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03067</id><created>2015-09-10</created><authors><author><keyname>Dede</keyname><forenames>Jens</forenames></author><author><keyname>Kuladinithi</keyname><forenames>Koojana</forenames></author><author><keyname>F&#xf6;rster</keyname><forenames>Anna</forenames></author><author><keyname>Nannen</keyname><forenames>Okko</forenames></author><author><keyname>Lehnhoff</keyname><forenames>Sebastian</forenames></author></authors><title>OMNeT++ and mosaik: Enabling Simulation of Smart Grid Communications</title><categories>cs.NI</categories><comments>Published in: A. F\&quot;orster, C. Minkenberg, G. R. Herrera, M. Kirsche
  (Eds.), Proc. of the 2nd OMNeT++ Community Summit, IBM Research - Zurich,
  Switzerland, September 3-4, 2015</comments><report-no>OMNET/2015/02</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a preliminary system architecture of integrating OMNeT++
into the mosaik co-simulation framework. This will enable realistic simulation
of communication network protocols and services for smart grid scenarios and on
the other side, further development of communication protocols for smart grid
applications. Thus, by integrating OMNeT++ and mosaik, both communities will be
able to leverage each others's sophisticated simulation models and expertise.
The main challenges identified are the external management of the OMNeT++
simulation kernel and performance issues when federating various simulators,
including OMNeT++ into the mosaik framework. The purpose of this paper is to
bring these challenges up and to gather relevant experience and expertise from
the OMNeT++ community. We especially encourage collaboration among all OMNeT++
developers and users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03069</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03069</id><created>2015-09-10</created><authors><author><keyname>Udugama</keyname><forenames>Asanga</forenames></author><author><keyname>Kuladinithi</keyname><forenames>Koojana</forenames></author><author><keyname>F&#xf6;rster</keyname><forenames>Anna</forenames></author><author><keyname>G&#xf6;rg</keyname><forenames>Carmelita</forenames></author></authors><title>Federating OMNeT++ Simulations with Testbed Environments</title><categories>cs.NI cs.PF</categories><comments>Published in: A. F\&quot;orster, C. Minkenberg, G. R. Herrera, M. Kirsche
  (Eds.), Proc. of the 2nd OMNeT++ Community Summit, IBM Research - Zurich,
  Switzerland, September 3-4, 2015</comments><report-no>OMNET/2015/03</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We are in the process of developing a system architecture for opportunistic
and information centric communications. This architecture (called Keetchi),
meant for the Internet of Things (IoT) is focussed on enabling applications to
perform distributed and decentralised communications among smart devices. To
realise and evaluate this architecture, we follow a 3-step approach. Our first
approach of evaluation is the development of a testbed with smart devices
(mainly smart phones and tablets) deployed with this architecture including the
applications. The second step is where the architecture is evaluated in large
scale scenarios with the OMNeT++ simulation environment. The third step is
where the OMNeT++ simulation environment is fed with traces of data collected
from experiments done using the testbed. In realising these environments, we
develop the functionality of this architecture as a common code base that is
able to operate in the OMNeT++ environment as well as in the smart devices of
the testbed (e.g., Android, iOS, Contiki, etc.). This paper presents the
details of the &quot;Write once, compile anywhere&quot; (WOCA) code base architecture of
Keetchi.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03075</identifier>
 <datestamp>2016-02-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03075</id><created>2015-09-10</created><updated>2016-02-05</updated><authors><author><keyname>Mouradian</keyname><forenames>Alexandre</forenames></author></authors><title>Modeling Dense Urban Wireless Networks with 3D Stochastic Geometry</title><categories>cs.NI</categories><comments>Submitted to Springer</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the past decade, many works on the modeling of wireless networks using
stochastic geometry have been proposed. Results about probability of coverage,
throughput or mean interference, have been provided for a wide variety of
networks (cellular, ad-hoc, cognitive, sensors, etc). These results notably
allow to tune network protocol parameters. Nevertheless, in their vast
majority, these works assume that the wireless network deployment is flat:
nodes are placed on the Euclidean plane. However, this assumption is disproved
in dense urban environments where many nodes are deployed in high buildings. In
this letter, we derive the exact form of the probability of coverage for the
cases where the interferers form a 3D Poisson Point Process (PPP) and a 3D
Modified Matern Process (MMP), and compare the results with the 2D case. The
main goal of this letter is to show that the 2D model, although being the most
common, can lead either to an optimistic or a pessimistic evaluation of the
probability of coverage depending on the parameters of the model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03085</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03085</id><created>2015-09-10</created><authors><author><keyname>Ben-Yishai</keyname><forenames>Assaf</forenames></author><author><keyname>Shayevitz</keyname><forenames>Ofer</forenames></author></authors><title>Interactive Schemes for the AWGN Channel with Noisy Feedback</title><categories>cs.IT math.IT</categories><comments>Submitted to the IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of communication over an additive white Gaussian noise
(AWGN) channel with an AWGN feedback channel. When the feedback channel is
noiseless, the classic Schalkwijk-Kailath (S-K) scheme is known to achieve
capacity in a simple sequential fashion, while attaining reliability superior
to non-feedback schemes. In this work, we show how simplicity and reliability
can be attained when the feedback is noisy. We introduce a low-complexity
low-delay scheme that operates close to capacity for a fixed bit error
probability (e.g. $10^{-6}$). We further provide an asymptotic construction
admitting an error exponent that can significantly exceed the best possible
non-feedback exponent. Both results hold when the feedback channel is
sufficiently better than the feedforward channel. Our approach is based on the
interpretation of feedback transmission as a side-information problem, and
employs an interactive modulo-lattice solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03091</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03091</id><created>2015-09-10</created><authors><author><keyname>Engle</keyname><forenames>Ryan D. L.</forenames></author><author><keyname>Hodson</keyname><forenames>Douglas D.</forenames></author><author><keyname>Grimaila</keyname><forenames>Michael R.</forenames></author><author><keyname>Mailloux</keyname><forenames>Logan O.</forenames></author><author><keyname>McLaughlin</keyname><forenames>Colin V.</forenames></author><author><keyname>Baumgartner</keyname><forenames>Gerald</forenames></author></authors><title>Modeling Quantum Optical Components, Pulses and Fiber Channels Using
  OMNeT++</title><categories>cs.CR cs.ET quant-ph</categories><comments>Published in: A. F\&quot;orster, C. Minkenberg, G. R. Herrera, M. Kirsche
  (Eds.), Proc. of the 2nd OMNeT++ Community Summit, IBM Research - Zurich,
  Switzerland, September 3-4, 2015</comments><report-no>OMNET/2015/04</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantum Key Distribution (QKD) is an innovative technology which exploits the
laws of quantum mechanics to generate and distribute unconditionally secure
cryptographic keys. While QKD offers the promise of unconditionally secure key
distribution, real world systems are built from non-ideal components which
necessitates the need to model and understand the impact these non-idealities
have on system performance and security. OMNeT++ has been used as a basis to
develop a simulation framework to support this endeavor. This framework,
referred to as &quot;qkdX&quot; extends OMNeT++'s module and message abstractions to
efficiently model optical components, optical pulses, operating protocols and
processes. This paper presents the design of this framework including how
OMNeT++'s abstractions have been utilized to model quantum optical components,
optical pulses, fiber and free space channels. Furthermore, from our toolbox of
created components, we present various notional and real QKD systems, which
have been studied and analyzed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03101</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03101</id><created>2015-09-10</created><authors><author><keyname>Kirsche</keyname><forenames>Michael</forenames></author><author><keyname>Kremmer</keyname><forenames>Roman</forenames></author></authors><title>uIP Support for the Network Simulation Cradle</title><categories>cs.NI cs.PF</categories><comments>Published in: A. F\&quot;orster, C. Minkenberg, G. R. Herrera, M. Kirsche
  (Eds.), Proc. of the 2nd OMNeT++ Community Summit, IBM Research - Zurich,
  Switzerland, September 3-4, 2015</comments><report-no>OMNET/2015/06</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces the ongoing integration of Contiki's uIP stack into the
OMNeT++ port of the Network Simulation Cradle (NSC). The NSC utilizes code from
real world stack implementations and allows for an accurate simulation and
comparison of different TCP/IP stacks. uIP(v6) provides resource-constrained
devices with an RFC-compliant TCP/IP stack and promotes the use of IPv6 in the
vastly growing field of Internet of Things scenarios. This work-in-progress
report discusses our motivation to integrate uIP into the NSC, our chosen
approach and possible use cases for the simulation of uIP in OMNeT++.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03103</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03103</id><created>2015-09-10</created><authors><author><keyname>Al-Rubaye</keyname><forenames>Atheer</forenames></author><author><keyname>Seitz</keyname><forenames>Jochen</forenames></author></authors><title>Dynamic Index NAT as a Mobility Solution in OMNeT++</title><categories>cs.NI cs.PF</categories><comments>Published in: A. F\&quot;orster, C. Minkenberg, G. R. Herrera, M. Kirsche
  (Eds.), Proc. of the 2nd OMNeT++ Community Summit, IBM Research - Zurich,
  Switzerland, September 3-4, 2015</comments><report-no>OMNET/2015/07</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobility in wireless networks causes a major issue from the IP-addressing
perspective. When a Mobile Node (MN) moves to another subnet, it will probably
get assigned a new IP address. This causes a routing problem since the MN will
not be reachable with its previous IP address known to the other communication
party. Real time applications might suffer from connection drops, which is
recognized as inconvenience in the currently used service, unless some solution
is provided. An approach to maintain session continuity while traversing
heterogeneous networks of different subnet addresses is proposed. Here, a
cross-layer module is implemented in OMNeT++ with NAT functionality to provide
a seamless handover. A proof of concept is also shown with analogy to the
Mobile IPv6 protocol provided in INET.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03105</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03105</id><created>2015-09-10</created><authors><author><keyname>Scussel</keyname><forenames>Artur Austregesilo</forenames></author><author><keyname>Panholzer</keyname><forenames>Georg</forenames></author><author><keyname>Brandauer</keyname><forenames>Christof</forenames></author><author><keyname>von T&#xfc;llenburg</keyname><forenames>Ferdinand</forenames></author></authors><title>Improvements in OMNeT++/INET Real-Time Scheduler for Emulation Mode</title><categories>cs.NI</categories><comments>Published in: A. F\&quot;orster, C. Minkenberg, G. R. Herrera, M. Kirsche
  (Eds.), Proc. of the 2nd OMNeT++ Community Summit, IBM Research - Zurich,
  Switzerland, September 3-4, 2015</comments><report-no>OMNET/2015/08</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper the performance of INET's emulation mode is evaluated. In
particular, the focus of the study is on the precision of the delay emulation.
It is shown, that this precision is low in INET 2.6 (respectively a later
version provided in the integration branch that fixed the crashes of 2.6 in
emulation mode). Two errors in the implementation are identified and an
alternative configuration for packet capturing is proposed. The performance
tests are re-run with such a modified version of the real-time scheduler (which
is now included in the recent INET 3.0 release) and it is shown that the
responsiveness of the emulation mode and the precision of delay emulation are
improved significantly. Finally, the negative impact of the modified capturing
configuration is briefly analyzed. Packet loss in the capturing process has
deteriorated but in fact is has already plagued the emulation mode of previous
implementations and this topic clearly demands for further studies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03109</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03109</id><created>2015-09-10</created><authors><author><keyname>Umuhoza</keyname><forenames>Eric</forenames></author></authors><title>Domain-Specific Modeling and Code Generation for Cross-Platform
  Multi-Device Mobile Apps</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, mobile devices constitute the most common computing device. This
new computing model has brought intense competition among hardware and software
providers who are continuously introducing increasingly powerful mobile devices
and innovative OSs into the market. In consequence, cross-platform and
multi-device development has become a priority for software companies that want
to reach the widest possible audience. However, developing an application for
several platforms implies high costs and technical complexity. Currently, there
are several frameworks that allow cross-platform application development.
However, these approaches still require manual programming. My research
proposes to face the challenge of the mobile revolution by exploiting
abstraction, modeling and code generation, in the spirit of the modern paradigm
of Model Driven Engineering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03111</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03111</id><created>2015-09-10</created><authors><author><keyname>Weinrank</keyname><forenames>Felix</forenames></author><author><keyname>T&#xfc;xen</keyname><forenames>Michael</forenames></author><author><keyname>Rathgeb</keyname><forenames>Erwin P.</forenames></author></authors><title>Integration of RTMFP in the OMNeT++ Simulation Environment</title><categories>cs.NI cs.PF</categories><comments>Published in: A. F\&quot;orster, C. Minkenberg, G. R. Herrera, M. Kirsche
  (Eds.), Proc. of the 2nd OMNeT++ Community Summit, IBM Research - Zurich,
  Switzerland, September 3-4, 2015</comments><report-no>OMNET/2015/10</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces the new Real-Time Media Flow Protocol (RTMFP)
simulation model for the INET framework for the OMNeT++ simulation environment.
RTMFP is a message orientated protocol with a focus on real time peer-to-peer
communication. After Adobe Inc. released the specifications, we were able to
implement the protocol in INET and compare its performance to the similar
Stream Control Transmission Protocol (SCTP) with a focus on congestion control
and flow control mechanisms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03118</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03118</id><created>2015-09-10</created><authors><author><keyname>Hofmann</keyname><forenames>Johannes</forenames></author><author><keyname>Eitzinger</keyname><forenames>Jan</forenames></author><author><keyname>Fey</keyname><forenames>Dietmar</forenames></author></authors><title>Execution-Cache-Memory Performance Model: Introduction and Validation</title><categories>cs.DC cs.PF</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This report serves two purposes: To introduce and validate the
Execution-Cache-Memory (ECM) performance model and to provide a thorough
analysis of current Intel processor architectures with a special emphasis on
Intel Xeon Haswell-EP. The ECM model is a simple analytical performance model
which focuses on basic architectural resources. The architectural analysis and
model predictions are showcased and validated using a set of elementary
microbenchmarks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03127</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03127</id><created>2015-09-10</created><authors><author><keyname>R&#xfc;ngeler</keyname><forenames>Irene</forenames></author><author><keyname>T&#xfc;xen</keyname><forenames>Michael</forenames></author></authors><title>Integration of the Packetdrill Testing Tool in INET</title><categories>cs.NI cs.PF</categories><comments>Published in: A. F\&quot;orster, C. Minkenberg, G. R. Herrera, M. Kirsche
  (Eds.), Proc. of the 2nd OMNeT++ Community Summit, IBM Research - Zurich,
  Switzerland, September 3-4, 2015</comments><report-no>OMNET/2015/05</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Google released in 2013 a script-based tool called packetdrill, which allows
to test transport protocols like UDP and TCP on Linux and BSD-based operating
systems. The scripts defining a test-case allow to inject packets to the
implementation under test, perform operations at the API controlling the
transport protocol and verify the sending of packets, all at specified times.
This paper describes a port of packetdrill to the INET framework for the
OMNeT++ simulation environment providing a simple and powerful method of
testing the transport protocols implemented in INET.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03140</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03140</id><created>2015-09-10</created><authors><author><keyname>Rain</keyname><forenames>Andreas</forenames></author><author><keyname>Kaiser</keyname><forenames>Daniel</forenames></author><author><keyname>Waldvogel</keyname><forenames>Marcel</forenames></author></authors><title>Realistic, Extensible DNS and mDNS Models for INET/OMNeT++</title><categories>cs.NI cs.PF</categories><comments>Published in: A. F\&quot;orster, C. Minkenberg, G. R. Herrera, M. Kirsche
  (Eds.), Proc. of the 2nd OMNeT++ Community Summit, IBM Research - Zurich,
  Switzerland, September 3-4, 2015</comments><report-no>OMNET/2015/09</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The domain name system (DNS) is one of the core services in today's network
structures. In local and ad-hoc networks DNS is often enhanced or replaced by
mDNS. As of yet, no simulation models for DNS and mDNS have been developed for
INET/OMNeT++. We introduce DNS and mDNS simulation models for OMNeT++, which
allow researchers to easily prototype and evaluate extensions for these
protocols. In addition, we present models for our own experimental extensions,
namely Stateless DNS and Privacy-Enhanced mDNS, that are based on the
aforementioned models. Using our models we were able to further improve the
efficiency of our protocol extensions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03147</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03147</id><created>2015-09-10</created><updated>2016-02-02</updated><authors><author><keyname>Kivim&#xe4;ki</keyname><forenames>Ilkka</forenames></author><author><keyname>Lebichot</keyname><forenames>Bertrand</forenames></author><author><keyname>Saram&#xe4;ki</keyname><forenames>Jari</forenames></author><author><keyname>Saerens</keyname><forenames>Marco</forenames></author></authors><title>Two betweenness centrality measures based on Randomized Shortest Paths</title><categories>cs.SI cs.DS physics.soc-ph</categories><comments>Minor updates; published in Scientific Reports</comments><journal-ref>Scientific Reports 6, Article number: 19668 (2016)</journal-ref><doi>10.1038/srep19668</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces two new closely related betweenness centrality measures
based on the Randomized Shortest Paths (RSP) framework, which fill a gap
between traditional network centrality measures based on shortest paths and
more recent methods considering random walks or current flows. The framework
defines Boltzmann probability distributions over paths of the network which
focus on the shortest paths, but also take into account longer paths depending
on an inverse temperature parameter. RSP's have previously proven to be useful
in defining distance measures on networks. In this work we study their utility
in quantifying the importance of the nodes of a network. The proposed RSP
betweenness centralities combine, in an optimal way, the ideas of using the
shortest and purely random paths for analysing the roles of network nodes,
avoiding issues involving these two paradigms. We present the derivations of
these measures and how they can be computed in an efficient way. In addition,
we show with real world examples the potential of the RSP betweenness
centralities in identifying interesting nodes of a network that more
traditional methods might fail to notice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03150</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03150</id><created>2015-09-10</created><authors><author><keyname>Wei</keyname><forenames>Yunchao</forenames></author><author><keyname>Liang</keyname><forenames>Xiaodan</forenames></author><author><keyname>Chen</keyname><forenames>Yunpeng</forenames></author><author><keyname>Shen</keyname><forenames>Xiaohui</forenames></author><author><keyname>Cheng</keyname><forenames>Ming-Ming</forenames></author><author><keyname>Zhao</keyname><forenames>Yao</forenames></author><author><keyname>Yan</keyname><forenames>Shuicheng</forenames></author></authors><title>STC: A Simple to Complex Framework for Weakly-supervised Semantic
  Segmentation</title><categories>cs.CV</categories><comments>4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, significant improvement has been made on semantic object
segmentation due to the development of deep convolutional neural networks
(DCNNs). Training such a DCNN usually relies on a large number of images with
pixel-level segmentation masks, and annotating these images is very costly in
terms of both finance and human effort. In this paper, we propose a simple to
complex (STC) framework in which only image-level annotations are utilized to
learn DCNNs for semantic segmentation. Specifically, we first train an initial
segmentation network called Initial-DCNN with the saliency maps of simple
images (i.e., those with a single category of major object(s) and clean
background). These saliency maps can be automatically obtained by existing
bottom-up salient object detection techniques, where no supervision information
is needed. Then, a better network called Enhanced-DCNN is learned with
supervision from the predicted segmentation masks of simple images based on the
Initial-DCNN as well as the image-level annotations. Finally, more pixel-level
segmentation masks of complex images (two or more categories of objects with
cluttered background), which are inferred by using Enhanced-DCNN and
image-level annotations, are utilized as the supervision information to learn
the Powerful-DCNN for semantic segmentation. Our method utilizes $40$K simple
images from Flickr.com and 10K complex images from PASCAL VOC for step-wisely
boosting the segmentation network. Extensive experimental results on PASCAL VOC
2012 segmentation benchmark demonstrate that the proposed STC framework
outperforms the state-of-the-art algorithms for weakly-supervised semantic
segmentation by a large margin (e.g., 10.6% over MIL-ILP-seg [1]).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03161</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03161</id><created>2015-09-10</created><authors><author><keyname>Dokulil</keyname><forenames>Jiri</forenames></author><author><keyname>Benkner</keyname><forenames>Siegfried</forenames></author></authors><title>OCR extensions - local identifiers, labeled GUIDs, file IO, and data
  block partitioning</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present several proposals for extending the Open Community Runtime (OCR)
specification. The extension are identifiers with local validity, which use the
concept of futures to provide OCR implementations more optimization
opportunities, labeled GUIDs with creator functions, which are based on the
local identifiers and allow the developer to create arrays of OCR objects that
are safe from race conditions in case of concurrent creation of objects, a
simple file IO interface, which builds on top of the existing data block
concepts, and finally data block partitioning, which allows better control and
flexibility in situations where multiple tasks want to access disjoint parts of
a data block.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03165</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03165</id><created>2015-09-10</created><authors><author><keyname>Dibbelt</keyname><forenames>Julian</forenames></author><author><keyname>Strasser</keyname><forenames>Ben</forenames></author><author><keyname>Wagner</keyname><forenames>Dorothea</forenames></author></authors><title>Fast Exact Shortest Path and Distance Queries on Road Networks with
  Parametrized Costs</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a scenario for route planning in road networks, where the objective
to be optimized may change between every shortest path query. Since this
invalidates many of the known speedup techniques for road networks that are
based on preprocessing of shortest path structures, we investigate
optimizations exploiting solely the topological structure of networks. We
experimentally evaluate our technique on a large set of real-world road
networks of various data sources. With lightweight preprocessing our technique
answers long-distance queries across continental networks significantly faster
than previous approaches towards the same problem formulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03169</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03169</id><created>2015-09-10</created><authors><author><keyname>Levesque</keyname><forenames>Martin</forenames></author><author><keyname>Tipper</keyname><forenames>David</forenames></author></authors><title>ptp++: A Precision Time Protocol Simulation Model for OMNeT++ / INET</title><categories>cs.NI cs.PF</categories><comments>Published in: A. F\&quot;orster, C. Minkenberg, G. R. Herrera, M. Kirsche
  (Eds.), Proc. of the 2nd OMNeT++ Community Summit, IBM Research - Zurich,
  Switzerland, September 3-4, 2015</comments><report-no>OMNET/2015/11</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Precise time synchronization is expected to play a key role in emerging
distributed and real-time applications such as the smart grid and Internet of
Things (IoT) based applications. The Precision Time Protocol (PTP) is currently
viewed as one of the main synchronization solutions over a packet-switched
network, which supports microsecond synchronization accuracy. In this paper, we
present a PTP simulation model for OMNeT++ INET, which allows to investigate
the synchronization accuracy under different network configurations and
conditions. To show some illustrative simulation results using the developed
module, we investigate on the network load fluctuations and their impacts on
the PTP performance by considering a network with class-based
quality-of-service (QoS) support. The simulation results show that the network
load significantly affects the network delay symmetry, and investigate a new
technique called class probing to improve the PTP accuracy and mitigate the
load fluctuation effects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03176</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03176</id><created>2015-09-10</created><authors><author><keyname>Weston</keyname><forenames>Jeffery</forenames></author><author><keyname>Koski</keyname><forenames>Eric</forenames></author></authors><title>High Frequency Radio Network Simulation Using OMNeT++</title><categories>cs.NI cs.PF</categories><comments>Published in: A. F\&quot;orster, C. Minkenberg, G. R. Herrera, M. Kirsche
  (Eds.), Proc. of the 2nd OMNeT++ Community Summit, IBM Research - Zurich,
  Switzerland, September 3-4, 2015</comments><report-no>OMNET/2015/12</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Harris Corporation has an interest in making HF radios a suitable medium for
wireless information networks using standard Internet protocols. Although HF
radio links have many unique characteristics, HF wireless subnets can be
subject to many of the same traffic flow characteristics and topologies as
existing line-of-sight (LOS) radio networks, giving rise to similar issues
(media access, connectivity, routing) which lend themselves to investigation
through simulation. Accordingly, we have undertaken to develop efficient,
high-fidelity simulations of various aspects of HF radio communications and
networking using the OMNeT++ framework. Essential aspects of these simulations
include HF channel models simulating relevant channel attributes such as Signal
to Noise Ratio, multipath, and Doppler spread; a calibrated physical layer
model reproducing the error statistics (including burst error distributions) of
the MIL-STD-188-110B/C HF modem waveforms, both narrowband (3 kHz) and wideband
(up to 24 kHz) on the simulated HF channels; a model of the NATO STANAG 5066
data link protocol; and integration of these models with the OMNeT++ network
simulation framework and its INET library of Internet protocol models. This
simulation is used to evaluate the impacts of different STANAG 5066
configuration settings on TCP network performance, and to evaluate strategies
for optimizing throughput over HF links using TCP Performance Enhancing Proxy
(PEP) techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03185</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03185</id><created>2015-09-10</created><authors><author><keyname>Simpson</keyname><forenames>Andrew J. R.</forenames></author></authors><title>Use it or Lose it: Selective Memory and Forgetting in a Perpetual
  Learning Machine</title><categories>cs.LG</categories><comments>arXiv admin note: substantial text overlap with arXiv:1509.00913</comments><msc-class>68Txx</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a recent article we described a new type of deep neural network - a
Perpetual Learning Machine (PLM) - which is capable of learning 'on the fly'
like a brain by existing in a state of Perpetual Stochastic Gradient Descent
(PSGD). Here, by simulating the process of practice, we demonstrate both
selective memory and selective forgetting when we introduce statistical recall
biases during PSGD. Frequently recalled memories are remembered, whilst
memories recalled rarely are forgotten. This results in a 'use it or lose it'
stimulus driven memory process that is similar to human memory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03196</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03196</id><created>2015-09-10</created><authors><author><keyname>Chen</keyname><forenames>Yu-Zhong</forenames></author><author><keyname>Wang</keyname><forenames>Lezhi</forenames></author><author><keyname>Wang</keyname><forenames>Wenxu</forenames></author><author><keyname>Lai</keyname><forenames>Ying-Cheng</forenames></author></authors><title>The paradox of controlling complex networks: control inputs versus
  energy requirement</title><categories>cs.SY cs.SI math-ph math.MP physics.soc-ph</categories><comments>37 pages, 16 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the linear controllability framework for
complex networks from a physical point of view. There are three main results.
(1) If one applies control signals as determined from the structural
controllability theory, there is a high probability that the control energy
will diverge. Especially, if a network is deemed controllable using a single
driving signal, then most likely the energy will diverge. (2) The energy
required for control exhibits a power-law scaling behavior. (3) Applying
additional control signals at proper nodes in the network can reduce and
optimize the energy cost. We identify the fundamental structures embedded in
the network, the longest control chains, which determine the control energy and
give rise to the power-scaling behavior. (To our knowledge, this was not
reported in any previous work on control of complex networks.) In addition, the
issue of control precision is addressed. These results are supported by
extensive simulations from model and real networks, physical reasoning, and
mathematical analyses.
  Notes on the submission history of this work: This work started in late 2012.
The phenomena of power-law energy scaling and energy divergence with a single
controller were discovered in 2013. Strategies to reduce and optimize control
energy was articulated and tested in 2013. The senior co-author (YCL) gave
talks about these results at several conferences, including the NETSCI 2014
Satellite entitled &quot;Controlling Complex Networks&quot; on June 2, 2014. The paper
was submitted to PNAS in September 2014 and was turned down. It was revised and
submitted to PRX in early 2015 and was rejected. After that it was revised and
submitted to Nature Communications in May 2015 and again was turned down.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03198</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03198</id><created>2015-06-19</created><authors><author><keyname>Bhamra</keyname><forenames>G. S.</forenames></author><author><keyname>Verma</keyname><forenames>A. K.</forenames></author><author><keyname>Patel</keyname><forenames>R. B.</forenames></author></authors><title>Agent enabled Mining of Distributed Protein Data Banks</title><categories>cs.CE</categories><journal-ref>International Journal in Foundations of Computer Science &amp;
  Technology (IJFCST), Vol.5, No.3, May 2015</journal-ref><doi>10.5121/ijfcst.2015.5303</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mining biological data is an emergent area at the intersection between
bioinformatics and data mining (DM). The intelligent agent based model is a
popular approach in constructing Distributed Data Mining (DDM) systems to
address scalable mining over large scale distributed data. The nature of
associations between different amino acids in proteins has also been a subject
of great anxiety. There is a strong need to develop new models and exploit and
analyze the available distributed biological data sources. In this study, we
have designed and implemented a multi-agent system (MAS) called Agent enriched
Quantitative Association Rules Mining for Amino Acids in distributed Protein
Data Banks (AeQARM-AAPDB). Such globally strong association rules enhance
understanding of protein composition and are desirable for synthesis of
artificial proteins. A real protein data bank is used to validate the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03200</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03200</id><created>2015-06-19</created><authors><author><keyname>Kumar</keyname><forenames>Abhishek</forenames></author><author><keyname>Gupta</keyname><forenames>Suresh Chandra</forenames></author></authors><title>A new Initial Centroid finding Method based on Dissimilarity Tree for
  K-means Algorithm</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cluster analysis is one of the primary data analysis technique in data mining
and K-means is one of the commonly used partitioning clustering algorithm. In
K-means algorithm, resulting set of clusters depend on the choice of initial
centroids. If we can find initial centroids which are coherent with the
arrangement of data, the better set of clusters can be obtained. This paper
proposes a method based on the Dissimilarity Tree to find, the better initial
centroid as well as every bit more accurate cluster with less computational
time. Theory analysis and experimental results indicate that the proposed
method can effectively improve the accuracy of clusters and reduce the
computational complexity of the K-means algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03203</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03203</id><created>2015-09-10</created><authors><author><keyname>Gogineni</keyname><forenames>Vinay Chakravarthi</forenames></author></authors><title>Adaptive Convex Combination of APA and ZA-APA algorithms for Sparse
  System Identification</title><categories>cs.SY cs.IT math.IT</categories><comments>Under communication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In general, one often encounters the systems that have sparse impulse
response, with time varying system sparsity. Conventional adaptive filters
which perform well for identification of non-sparse systems fail to exploit the
system sparsity for improving the performance as the sparsity level increases.
This paper presents a new approach that uses an adaptive convex combination of
Affine Projection Algorithm (APA) and Zero-attracting Affine Projection
Algorithm (ZA-APA)algorithms for identifying the sparse systems, which adapts
dynamically to the sparsity of the system. Thus works well in both sparse and
non-sparse environments and also the usage of affine projection makes it robust
against colored input. It is shown that, for non-sparse systems, the proposed
combination always converges to the APA algorithm, while for semi-sparse
systems, it converges to a solution that produces lesser steady state EMSE than
produced by either of the component filters. For highly sparse systems,
depending on the value of the proportionality constant ($\rho$) in ZA-APA
algorithm, the proposed combined filter may either converge to the ZA-APA based
filter or produce a solution similar to the semi-sparse case i.e.,
outerperforms both the constituent filters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03205</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03205</id><created>2015-09-10</created><updated>2015-12-30</updated><authors><author><keyname>Li</keyname><forenames>Xiaofei</forenames></author><author><keyname>Girin</keyname><forenames>Laurent</forenames></author><author><keyname>Horaud</keyname><forenames>Radu</forenames></author><author><keyname>Gannot</keyname><forenames>Sharon</forenames></author></authors><title>Estimation of the Direct-Path Relative Transfer Function for Supervised
  Sound-Source Localization</title><categories>cs.SD</categories><comments>12 pages, 7 figures, 5 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the problem of binaural localization of a single speech
source in noisy and reverberant environments. The binaural response
corresponding to the direct-path propagation of a single source is a function
of the source direction. In practice, this response is contaminated by noise
and reverberations. The direct-path relative transfer function (DP-RTF) is
defined as the ratio between the direct-path acoustic transfer function of the
two channels. We propose a method to estimate the DP-RTF from the noisy and
reverberant microphone signals in the short-time Fourier transform domain.
First, the convolutive transfer function approximation is adopted to accurately
represent the impulse response of the sensors in the STFT domain. Second, the
DP-RTF is estimated by using the auto- and cross-power spectral densities at
each frequency and over multiple frames. In the presence of stationary noise,
an inter-frame spectral subtraction algorithm is proposed, which enables to
achieve the estimation of noise-free auto- and cross-power spectral densities.
Finally, the estimated DP-RTFs are concatenated across frequencies and used as
a feature vector for the localization of speech. Experiments with both
simulated and real data show that the proposed localization method performs
well, even under severe adverse acoustic conditions, and outperforms
state-of-the-art localization methods under most of the acoustic conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03208</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03208</id><created>2015-07-13</created><authors><author><keyname>Elmadany</keyname><forenames>Abdelrahim A</forenames></author><author><keyname>Abdou</keyname><forenames>Sherif M</forenames></author><author><keyname>Gheith</keyname><forenames>Mervat</forenames></author></authors><title>Towards Understanding Egyptian Arabic Dialogues</title><categories>cs.CL</categories><comments>arXiv admin note: substantial text overlap with arXiv:1505.03081</comments><journal-ref>International Journal of Computer Applications 120(220, PP 7-12,
  June 2015</journal-ref><doi>10.5120/21390-4427</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Labelling of user's utterances to understanding his attends which called
Dialogue Act (DA) classification, it is considered the key player for dialogue
language understanding layer in automatic dialogue systems. In this paper, we
proposed a novel approach to user's utterances labeling for Egyptian
spontaneous dialogues and Instant Messages using Machine Learning (ML) approach
without relying on any special lexicons, cues, or rules. Due to the lack of
Egyptian dialect dialogue corpus, the system evaluated by multi-genre corpus
includes 4725 utterances for three domains, which are collected and annotated
manually from Egyptian call-centers. The system achieves F1 scores of 70. 36%
overall domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03212</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03212</id><created>2015-09-10</created><authors><author><keyname>Chakrabarty</keyname><forenames>Deeparnab</forenames></author><author><keyname>Ene</keyname><forenames>Alina</forenames></author><author><keyname>Krishnaswamy</keyname><forenames>Ravishankar</forenames></author><author><keyname>Panigrahi</keyname><forenames>Debmalya</forenames></author></authors><title>Online Buy-at-Bulk Network Design</title><categories>cs.DS</categories><comments>24 pages, longer version of a FOCS 2015 paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the first non-trivial online algorithms for the non-uniform,
multicommodity buy-at-bulk (MC-BB) network design problem in undirected and
directed graphs. Our competitive ratios qualitatively match the best known
approximation factors for the corresponding offline problems. The main engine
for our results is an online reduction theorem of MC-BB problems to their
single-sink (SS-BB) counterparts. We use the concept of junction-tree solutions
(Chekuri et al., FOCS 2006) that play an important role in solving the offline
versions of the problem via a greedy subroutine -- an inherently offline
procedure. Our main technical contribution is in designing an online algorithm
using only the existence of good junction-trees to reduce an MC-BB instance to
multiple SS-BB sub-instances. Along the way, we also give the first non-trivial
online node-weighted/directed single-sink buy-at-bulk algorithms. In addition
to the new results, our generic reduction also yields new proofs of recent
results for the online node-weighted Steiner forest and online group Steiner
forest problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03214</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03214</id><created>2015-08-15</created><authors><author><keyname>Abbas</keyname><forenames>Hosny A.</forenames></author><author><keyname>Shaheen</keyname><forenames>Samir I.</forenames></author><author><keyname>Amin</keyname><forenames>Mohammed H.</forenames></author></authors><title>Simple, Flexible, and Interoperable SCADA System Based on Agent
  Technology</title><categories>cs.SY</categories><comments>16 pages. arXiv admin note: text overlap with arXiv:1009.5346 by
  other authors without attribution</comments><journal-ref>Intelligent Control and Automation, 2015, 6, 184-199</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  SCADA (Supervisory Control and Data Acquisition) is concerned with gathering
process information from industrial control processes found in utilities such
as power grids, water networks, transportation, manufacturing, etc., to provide
the human operators with the required real-time access to industrial processes
to be monitored and controlled either locally (on-site)or remotely (i.e.,
through Internet). Conventional solutions such as custom SCADA packages, custom
communication protocols, and centralized architectures are no longer
appropriate for engineering this type of systems because of their highly
distribution and their uncertain continuously changing working environments.
Multi-agent systems (MAS) appeared as a new architectural style for engineering
complex and highly dynamic applications such as SCADA systems. In this paper,
we propose an approach for simply developing flexible and interoperable SCADA
systems based on the integration of MAS and OPC process protocol. The proposed
SCADA system has the following advantages: 1) simple (easier to be
implemented); 2) flexible (able to adapt to its environment dynamic changes);
and 3) interoperable (relative to the underlying control systems, which belongs
to diverse of vendors). The applicability of the proposed approach is
demonstrated by a real case study example carried out in a paper mill.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03221</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03221</id><created>2015-08-21</created><authors><author><keyname>Mandal</keyname><forenames>Sudip</forenames></author><author><keyname>Saha</keyname><forenames>Goutam</forenames></author><author><keyname>Pal</keyname><forenames>Rajat K.</forenames></author></authors><title>Recurrent Neural Network Based Modeling of Gene Regulatory Network Using
  Bat Algorithm</title><categories>cs.AI cs.NE</categories><comments>14 pages, 4 figure. arXiv admin note: text overlap with
  arXiv:1004.4170 by other authors</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Correct inference of genetic regulations inside a cell is one of the greatest
challenges in post genomic era for the biologist and researchers. Several
intelligent techniques and models were already proposed to identify the
regulatory relations among genes from the biological database like time series
microarray data. Recurrent Neural Network (RNN) is one of the most popular and
simple approach to model the dynamics as well as to infer correct dependencies
among genes. In this paper, Bat Algorithm (BA) was applied to optimize the
model parameters of RNN model of Gene Regulatory Network (GRN). Initially the
proposed method is tested against small artificial network without any noise
and the efficiency was observed in term of number of iteration, number of
population and BA optimization parameters. The model was also validated in
presence of different level of random noise for the small artificial network
and that proved its ability to infer the correct inferences in presence of
noise like real world dataset. In the next phase of this research, BA based RNN
is applied to real world benchmark time series microarray dataset of E. Coli.
The results shown that it can able to identify the maximum true positive
regulation but also include some false positive regulations. Therefore, BA is
very suitable for identifying biological plausible GRN with the help RNN model
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03242</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03242</id><created>2015-09-10</created><authors><author><keyname>Girdhar</keyname><forenames>Yogesh</forenames></author><author><keyname>Dudek</keyname><forenames>Gregory</forenames></author></authors><title>Gibbs Sampling Strategies for Semantic Perception of Streaming Video
  Data</title><categories>cs.RO cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Topic modeling of streaming sensor data can be used for high level perception
of the environment by a mobile robot. In this paper we compare various Gibbs
sampling strategies for topic modeling of streaming spatiotemporal data, such
as video captured by a mobile robot. Compared to previous work on online topic
modeling, such as o-LDA and incremental LDA, we show that the proposed
technique results in lower online and final perplexity, given the realtime
constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03243</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03243</id><created>2015-06-22</created><authors><author><keyname>Askali</keyname><forenames>M.</forenames></author><author><keyname>Nouh</keyname><forenames>S.</forenames></author><author><keyname>Azouaoui</keyname><forenames>A.</forenames></author><author><keyname>Belkasmi</keyname><forenames>M.</forenames></author></authors><title>Discovery of good double and triple circulant codes using multiple
  impulse method</title><categories>cs.IT math.IT</categories><comments>arXiv admin note: text overlap with arXiv:1303.4375</comments><journal-ref>ISSN: 0975-3273 &amp; E-ISSN: 0975-9085, Volume 5, Issue 1, 2013,
  pp.-141-148</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The construction of optimal linear block error-correcting codes is not an
easy problem, for this, many studies describe methods for generating good error
correcting codes in terms of minimum distance. In a previous work, we have
presented the multiple impulse method (MIM) to evaluate the minimum distance of
linear codes. In this paper we will present an optimization of the MIM method
by genetic algorithms, and we found many new optimal Double and Triple
Circulant Codes (DCC &amp; TCC) with the highest known parameters using the MIM
method as an evaluator of the minimum distance. Two approaches are used in the
exploration of the space of generators; the first is based on genetic
algorithms, however the second is on the random search algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03247</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03247</id><created>2015-09-10</created><authors><author><keyname>Chaudhuri</keyname><forenames>Arindam</forenames></author></authors><title>An Epsilon Hierarchical Fuzzy Twin Support Vector Regression</title><categories>cs.AI</categories><comments>Research work at Samsung Research and Development Institute Delhi</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The research presents epsilon hierarchical fuzzy twin support vector
regression based on epsilon fuzzy twin support vector regression and epsilon
twin support vector regression. Epsilon FTSVR is achieved by incorporating
trapezoidal fuzzy numbers to epsilon TSVR which takes care of uncertainty
existing in forecasting problems. Epsilon FTSVR determines a pair of epsilon
insensitive proximal functions by solving two related quadratic programming
problems. The structural risk minimization principle is implemented by
introducing regularization term in primal problems of epsilon FTSVR. This
yields dual stable positive definite problems which improves regression
performance. Epsilon FTSVR is then reformulated as epsilon HFTSVR consisting of
a set of hierarchical layers each containing epsilon FTSVR. Experimental
results on both synthetic and real datasets reveal that epsilon HFTSVR has
remarkable generalization performance with minimum training time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03248</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03248</id><created>2015-09-10</created><authors><author><keyname>Trigeorgis</keyname><forenames>George</forenames></author><author><keyname>Bousmalis</keyname><forenames>Konstantinos</forenames></author><author><keyname>Zafeiriou</keyname><forenames>Stefanos</forenames></author><author><keyname>Schuller</keyname><forenames>Bjoern W.</forenames></author></authors><title>A deep matrix factorization method for learning attribute
  representations</title><categories>cs.CV cs.LG stat.ML</categories><comments>Submitted to TPAMI (16-Mar-2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Semi-Non-negative Matrix Factorization is a technique that learns a
low-dimensional representation of a dataset that lends itself to a clustering
interpretation. It is possible that the mapping between this new representation
and our original data matrix contains rather complex hierarchical information
with implicit lower-level hidden attributes, that classical one level
clustering methodologies can not interpret. In this work we propose a novel
model, Deep Semi-NMF, that is able to learn such hidden representations that
allow themselves to an interpretation of clustering according to different,
unknown attributes of a given dataset. We also present a semi-supervised
version of the algorithm, named Deep WSF, that allows the use of (partial)
prior information for each of the known attributes of a dataset, that allows
the model to be used on datasets with mixed attribute knowledge. Finally, we
show that our models are able to learn low-dimensional representations that are
better suited for clustering, but also classification, outperforming
Semi-Non-negative Matrix Factorization, but also other state-of-the-art
methodologies variants.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03252</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03252</id><created>2015-09-10</created><updated>2015-09-13</updated><authors><author><keyname>Alshehri</keyname><forenames>Khaled</forenames></author><author><keyname>Liu</keyname><forenames>Ji</forenames></author><author><keyname>Chen</keyname><forenames>Xudong</forenames></author><author><keyname>Ba&#x15f;ar</keyname><forenames>Tamer</forenames></author></authors><title>A Stackelberg Game for Multi-Period Demand Response Management in the
  Smart Grid</title><categories>math.OC cs.GT</categories><comments>Accepted for Proc. 54th IEEE Conference on Decision and Control</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies a multi-period demand response management problem in the
smart grid where multiple utility companies compete among themselves. The
user-utility interactions are modeled by a noncooperative game of a Stackelberg
type where the interactions among the utility companies are captured through a
Nash equilibrium. It is shown that this game has a unique Stackelberg
equilibrium at which the utility companies set prices to maximize their
revenues (within a Nash game) while the users respond accordingly to maximize
their utilities subject to their budget constraints. Closed-form expressions
are provided for the corresponding strategies of the users and the utility
companies. It is shown that the multi- period scheme, compared with the
single-period case, provides more incentives for the users to participate in
the game. A necessary and sufficient condition on the minimum budget needed for
a user to participate is provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03254</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03254</id><created>2015-09-10</created><authors><author><keyname>Gao</keyname><forenames>Yuan</forenames></author><author><keyname>Moreira</keyname><forenames>Nelma</forenames></author><author><keyname>Reis</keyname><forenames>Rog&#xe9;rio</forenames></author><author><keyname>Yu</keyname><forenames>Sheng</forenames></author></authors><title>A Survey on Operational State Complexity</title><categories>cs.FL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Descriptional complexity is the study of the conciseness of the various
models representing formal languages. The state complexity of a regular
language is the size, measured by the number of states of the smallest, either
deterministic or nondeterministic, finite automaton that recognises it.
Operational state complexity is the study of the state complexity of operations
over languages. In this survey, we review the state complexities of individual
regularity preserving language operations on regular and some subregular
languages. Then we revisit the state complexities of the combination of
individual operations. We also review methods of estimation and approximation
of state complexity of more complex combined operations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03257</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03257</id><created>2015-09-10</created><authors><author><keyname>Joswig</keyname><forenames>Michael</forenames></author><author><keyname>Kileel</keyname><forenames>Joe</forenames></author><author><keyname>Sturmfels</keyname><forenames>Bernd</forenames></author><author><keyname>Wagner</keyname><forenames>Andr&#xe9;</forenames></author></authors><title>Rigid Multiview Varieties</title><categories>math.AG cs.CV math.AC</categories><comments>10 pages, 1 figure</comments><msc-class>14M99, 68T45</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The multiview variety from computer vision is generalized to images by $n$
cameras of points linked by a distance constraint. The resulting
five-dimensional variety lives in a product of $2n$ projective planes. We
determine defining polynomial equations, and we explore generalizations of this
variety to scenarios of interest in applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03258</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03258</id><created>2015-09-10</created><updated>2015-09-13</updated><authors><author><keyname>Bubeck</keyname><forenames>S&#xe9;bastien</forenames></author><author><keyname>Ganguly</keyname><forenames>Shirshendu</forenames></author></authors><title>Entropic CLT and phase transition in high-dimensional Wishart matrices</title><categories>math.PR cs.IT math.FA math.IT math.ST stat.TH</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider high dimensional Wishart matrices $\mathbb{X} \mathbb{X}^{\top}$
where the entries of $\mathbb{X} \in {\mathbb{R}^{n \times d}}$ are i.i.d. from
a log-concave distribution. We prove an information theoretic phase transition:
such matrices are close in total variation distance to the corresponding
Gaussian ensemble if and only if $d$ is much larger than $n^3$. Our proof is
entropy-based, making use of the chain rule for relative entropy along with the
recursive structure in the definition of the Wishart ensemble. The proof
crucially relies on the well known relation between Fisher information and
entropy, a variational representation for Fisher information, concentration
bounds for the spectral norm of a random matrix, and certain small ball
probability estimates for log-concave measures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03262</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03262</id><created>2015-09-10</created><authors><author><keyname>Shoukry</keyname><forenames>Yasser</forenames></author><author><keyname>Nuzzo</keyname><forenames>Pierluigi</forenames></author><author><keyname>Bezzo</keyname><forenames>Nicola</forenames></author><author><keyname>Sangiovanni-Vincentelli</keyname><forenames>Alberto L.</forenames></author><author><keyname>Seshia</keyname><forenames>Sanjit A.</forenames></author><author><keyname>Tabuada</keyname><forenames>Paulo</forenames></author></authors><title>A Satisfiability Modulo Theory Approach to Secure State Reconstruction
  in Differentially Flat Systems Under Sensor Attacks</title><categories>math.OC cs.CR cs.IT cs.SY math.IT</categories><comments>arXiv admin note: text overlap with arXiv:1412.4324</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of estimating the state of a differentially flat
system from measurements that may be corrupted by an adversarial attack. In
cyber-physical systems, malicious attacks can directly compromise the system's
sensors or manipulate the communication between sensors and controllers. We
consider attacks that only corrupt a subset of sensor measurements. We show
that the possibility of reconstructing the state under such attacks is
characterized by a suitable generalization of the notion of s-sparse
observability, previously introduced by some of the authors in the linear case.
We also extend our previous work on the use of Satisfiability Modulo Theory
solvers to estimate the state under sensor attacks to the context of
differentially flat systems. The effectiveness of our approach is illustrated
on the problem of controlling a quadrotor under sensor attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03271</identifier>
 <datestamp>2016-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03271</id><created>2015-09-10</created><updated>2016-03-04</updated><authors><author><keyname>Smith</keyname><forenames>Anna</forenames></author><author><keyname>Calder</keyname><forenames>Catherine A.</forenames></author><author><keyname>Browning</keyname><forenames>Christopher R.</forenames></author></authors><title>Empirical Reference Distributions for Networks of Different Size</title><categories>stat.ME cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network analysis has become an increasingly prevalent research tool across a
vast range of scientific fields. Here, we focus on the particular issue of
comparing network statistics, i.e. graph-level measures of network structural
features, across multiple networks that differ in size. Although &quot;normalized&quot;
versions of some network statistics exist, we demonstrate via simulation why
direct comparison of raw and normalized statistics is often inappropriate. We
examine a recent suggestion to normalize network statistics relative to
Erdos-Renyi random graphs and demonstrate via simulation how this is an
improvement over direct comparison, but still sometimes problematic. We propose
a new adjustment method based on a reference distribution constructed as a
mixture model of random graphs which reflect the dependence structure exhibited
in the observed networks. We show that using simple Bernoulli models as mixture
components in this reference distribution can provide adjusted network
statistics that are relatively comparable across different network sizes but
still describe interesting features of networks, and that this can be
accomplished at relatively low computational expense. Finally, we apply this
methodology to a collection of co-location networks derived from the Los
Angeles Family and Neighborhood Survey activity location data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03278</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03278</id><created>2015-09-10</created><authors><author><keyname>Saboori</keyname><forenames>Arash</forenames></author><author><keyname>Hosseini</keyname><forenames>S. Abolfazl</forenames></author></authors><title>A New Method For Digital Watermarking Based on Combination of DCT and
  PCA</title><categories>cs.MM</categories><comments>Telecommunications Forum Telfor (TELFOR), 2014 22nd</comments><doi>10.1109/TELFOR.2014.7034461</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the digital watermarking with DCT method,the watermark is located within a
range of DCT coefficients of the cover image. In this paper to use the
low-frequency band, a new method is proposed by using a combination of the DCT
and PCA transform. The proposed method is compared to other DCT methods, our
method is robust and keeps the quality of cover image, also increases capacity
of the watermarking.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03281</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03281</id><created>2015-09-10</created><authors><author><keyname>Mossel</keyname><forenames>Elchanan</forenames></author><author><keyname>Xu</keyname><forenames>Jiaming</forenames></author></authors><title>Density Evolution in the Degree-correlated Stochastic Block Model</title><categories>stat.ML cs.IT math.IT math.PR</categories><comments>arXiv admin note: text overlap with arXiv:1508.02344</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is a recent surge of interest in identifying the sharp recovery
thresholds for cluster recovery under the stochastic block model. In this
paper, we address the more refined question of how many vertices that will be
misclassified on average. We consider the binary form of the stochastic block
model, where $n$ vertices are partitioned into two clusters with edge
probability $a/n$ within the first cluster, $c/n$ within the second cluster,
and $b/n$ across clusters. Suppose that as $n \to \infty$, $a= b+ \mu \sqrt{ b}
$, $c=b+ \nu \sqrt{ b} $ for two fixed constants $\mu, \nu$, and $b \to \infty$
with $b=n^{o(1)}$. When the cluster sizes are balanced and $\mu \neq \nu$, we
show that the minimum fraction of misclassified vertices on average is given by
$Q(\sqrt{v^*})$, where $Q(x)$ is the Q-function for standard normal, $v^*$ is
the unique fixed point of $v= \frac{(\mu-\nu)^2}{16} + \frac{ (\mu+\nu)^2 }{16}
\mathbb{E}[ \tanh(v+ \sqrt{v} Z)],$ and $Z$ is standard normal. Moreover, the
minimum misclassified fraction on average is attained by a local algorithm,
namely belief propagation, in time linear in the number of edges. Our proof
techniques are based on connecting the cluster recovery problem to tree
reconstruction problems, and analyzing the density evolution of belief
propagation on trees with Gaussian approximations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03284</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03284</id><created>2015-09-03</created><updated>2015-09-13</updated><authors><author><keyname>F&#xf6;rster</keyname><forenames>Anna</forenames></author><author><keyname>Minkenberg</keyname><forenames>Cyriel</forenames></author><author><keyname>Herrera</keyname><forenames>German Rodriguez</forenames></author><author><keyname>Kirsche</keyname><forenames>Michael</forenames></author></authors><title>Proceedings of the 2nd OMNeT++ Community Summit, IBM Research - Zurich,
  Switzerland, September 3-4, 2015</title><categories>cs.PF cs.NI</categories><proxy>Michael Kirsche</proxy><acm-class>I.6; C.2.0; C.4; D.4.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is the Proceedings of the 2nd OMNeT++ Community Summit, which was held
at IBM Research - Zurich, Switzerland on September 3-4, 2015.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03287</identifier>
 <datestamp>2015-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03287</id><created>2015-09-09</created><authors><author><keyname>Oikonomou-Filandras</keyname><forenames>Panagiotis-Agis</forenames></author><author><keyname>Wong</keyname><forenames>Kai-Kit</forenames></author><author><keyname>Zhang</keyname><forenames>Yangyang</forenames></author></authors><title>Grid-Based Belief Propagation for Cooperative Localization</title><categories>cs.NI cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel parametric message representation for belief propagation
(BP) that provides a novel grid-based way to address the cooperative
localization problem in wireless networks. The proposed Grid-BP approach allows
faster calculations than non-parametric representations and works well with
existing grid-based coordinate systems, e.g., NATO's military grid reference
system (MGRS). This overcomes the hidden challenge inherent in all distributed
localization algorithms that require a universally known global reference
system (GCS), even though every node localizes using arbitrary local coordinate
systems (LCSs) for a reference. Simulation results demonstrate that Grid-BP
achieves similar accuracy at much reduced complexity when compared to common
techniques that assume ideal reference
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03295</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03295</id><created>2015-09-09</created><updated>2016-02-27</updated><authors><author><keyname>Ferrer-i-Cancho</keyname><forenames>Ramon</forenames></author><author><keyname>G&#xf3;mez-Rodr&#xed;guez</keyname><forenames>Carlos</forenames></author></authors><title>Liberating language research from dogmas of the 20th century</title><categories>cs.CL cs.SI physics.soc-ph</categories><comments>Minor corrections</comments><journal-ref>Liberating language research from dogmas of the 20th century.
  Glottometrics 33, 33-34 (2016)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A commentary on the article &quot;Large-scale evidence of dependency length
minimization in 37 languages&quot; by Futrell, Mahowald &amp; Gibson (PNAS 2015 112 (33)
10336-10341).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03302</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03302</id><created>2015-09-10</created><authors><author><keyname>Barnes</keyname><forenames>Matt</forenames></author><author><keyname>Miller</keyname><forenames>Kyle</forenames></author><author><keyname>Dubrawski</keyname><forenames>Artur</forenames></author></authors><title>Performance Bounds for Pairwise Entity Resolution</title><categories>stat.ML cs.CY cs.DB cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One significant challenge to scaling entity resolution algorithms to massive
datasets is understanding how performance changes after moving beyond the realm
of small, manually labeled reference datasets. Unlike traditional machine
learning tasks, when an entity resolution algorithm performs well on small
hold-out datasets, there is no guarantee this performance holds on larger
hold-out datasets. We prove simple bounding properties between the performance
of a match function on a small validation set and the performance of a pairwise
entity resolution algorithm on arbitrarily sized datasets. Thus, our approach
enables optimization of pairwise entity resolution algorithms for large
datasets, using a small set of labeled data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03327</identifier>
 <datestamp>2016-01-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03327</id><created>2015-09-08</created><updated>2016-01-15</updated><authors><author><keyname>Nica</keyname><forenames>Mihai</forenames></author></authors><title>Optimal Strategy in &quot;Guess Who?&quot;: Beyond Binary Search</title><categories>math.PR cs.GT math.OC</categories><comments>13 pages, 2 figures. Derivation rewritten from the point of view of
  &quot;Continuous Guess Who?&quot;. To appear in Probability in the Engineering and
  Informational Sciences</comments><msc-class>91A15, 60G40, 62L15, 91A60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  &quot;Guess Who?&quot; is a popular two player game where players ask &quot;Yes&quot;/&quot;No&quot;
questions to search for their opponent's secret identity from a pool of
possible candidates. This is modeled as a simple stochastic game. Using this
model, the optimal strategy is explicitly found. Contrary to popular belief,
performing a binary search is \emph{not} always optimal. Instead, the optimal
strategy for the player who trails is to make certain bold plays in an attempt
catch up. This is discovered by first analyzing a continuous version of the
game where players play indefinitely and the winner is never decided after
finitely many rounds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03335</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03335</id><created>2015-09-10</created><authors><author><keyname>Tan</keyname><forenames>Jianchao</forenames></author><author><keyname>Lien</keyname><forenames>Jyh-Ming</forenames></author><author><keyname>Gingold</keyname><forenames>Yotam</forenames></author></authors><title>Decomposing Digital Paintings into Layers via RGB-space Geometry</title><categories>cs.GR</categories><msc-class>65D18</msc-class><acm-class>I.3.7; I.4.6</acm-class><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  In digital painting software, layers organize paintings. However, layers are
not explicitly represented, transmitted, or published with the final digital
painting. We propose a technique to decompose a digital painting into layers.
In our decomposition, each layer represents a coat of paint of a single paint
color applied with varying opacity throughout the image. Our decomposition is
based on the painting's RGB-space geometry. In RGB-space, a geometric structure
is revealed due to the linear nature of the standard Porter-Duff &quot;over&quot; pixel
compositing operation. The vertices of the convex hull of pixels in RGB-space
suggest paint colors. Users choose the degree of simplification to perform on
the convex hull, as well as a layer order for the colors. We solve a
constrained optimization problem to find maximally translucent, spatially
coherent opacity for each layer, such that the composition of the layers
reproduces the original image. We demonstrate the utility of the resulting
decompositions for re-editing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03339</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03339</id><created>2015-09-10</created><authors><author><keyname>Krebbers</keyname><forenames>Robbert</forenames></author></authors><title>A Formal C Memory Model for Separation Logic</title><categories>cs.LO cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The core of a formal semantics of an imperative programming language is a
memory model that describes the behavior of operations on the memory. Defining
a memory model that matches the description of C in the C11 standard is
challenging because C allows both high-level (by means of typed expressions)
and low-level (by means of bit manipulation) memory accesses. The C11 standard
has restricted the interaction between these two levels to make more effective
compiler optimizations possible, on the expense of making the memory model
complicated.
  We describe a formal memory model of the (non-concurrent part of the) C11
standard that incorporates these restrictions, and at the same time describes
low-level memory operations. This formal memory model includes a rich
permission model to make it usable in separation logic and supports reasoning
about program transformations. The memory model and essential properties of it
have been fully formalized using the Coq proof assistant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03347</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03347</id><created>2015-09-10</created><authors><author><keyname>Haesaert</keyname><forenames>Sofie</forenames></author><author><keyname>Hof</keyname><forenames>Paul M. J. Van den</forenames></author><author><keyname>Abate</keyname><forenames>Alessandro</forenames></author></authors><title>Data-driven and Model-based Verification: a Bayesian Identification
  Approach</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work develops a measurement-driven and model-based formal verification
approach, applicable to systems with partly unknown dynamics. We provide a
principled method, grounded on reachability analysis and on Bayesian inference,
to compute the confidence that a physical system driven by external inputs and
accessed under noisy measurements, verifies a temporal logic property. A case
study is discussed, where we investigate the bounded- and unbounded-time safety
of a partly unknown linear time invariant system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03350</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03350</id><created>2015-09-06</created><authors><author><keyname>Liu</keyname><forenames>Xiwei</forenames></author><author><keyname>Chen</keyname><forenames>Tianping</forenames></author></authors><title>Fixed-time cluster synchronization for complex networks via pinning
  control</title><categories>cs.SY math.OC nlin.AO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the fixed-time cluster synchronization problem for complex
networks via pinning control is discussed. Fixed-time synchronization has been
a hot topic in recent years, which means that the network can achieve
synchronization in finite-time and the settling time is bounded by a constant
for any initial values. To realize the fixed-time cluster synchronization, a
simple distributed protocol by pinning control technique is designed, whose
validity is rigorously proved, and some sufficient criteria for fixed-time
cluster synchronization are also obtained. Especially, when the cluster number
is one, the cluster synchronization becomes the complete synchronization
problem; when the intrinsic dynamics for each node is missed, the fixed-time
cluster synchronization becomes the fixed-time cluster (or complete) consensus
problem; when the network has only one node, the coupling term between nodes
will disappear, and the synchronization problem becomes the simplest
master-slave case, which also includes the stability problem for nonlinear
systems like neural networks. All these cases are also discussed. Finally,
numerical simulations are presented to demonstrate the correctness of obtained
theoretical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03351</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03351</id><created>2015-09-07</created><authors><author><keyname>Cicone</keyname><forenames>Antonio</forenames></author><author><keyname>D'Innocenzo</keyname><forenames>Alessandro</forenames></author><author><keyname>Guglielmi</keyname><forenames>Nicola</forenames></author><author><keyname>Laglia</keyname><forenames>Linda</forenames></author></authors><title>A sub-optimal solution for optimal control of linear systems with
  unmeasurable switching delays</title><categories>cs.SY math.OC</categories><comments>arXiv admin note: text overlap with arXiv:1401.1673</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the optimal control design problem for discrete-time LTI systems
with state feedback, when the actuation signal is subject to unmeasurable
switching propagation delays, due to e.g. the routing in a multi-hop
communication network and/or jitter. In particular, we set up a constrained
optimization problem where the cost function is the worst-case $\mathcal{L}_2$
norm for all admissible switching delays. We first show how to model these
systems as pure switching linear systems, and as main contribution of the paper
we provide an algorithm to compute a sub-optimal solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03353</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03353</id><created>2015-09-10</created><authors><author><keyname>Liu</keyname><forenames>Yu</forenames></author><author><keyname>Simeone</keyname><forenames>Osvaldo</forenames></author><author><keyname>Haimovich</keyname><forenames>Alexander M.</forenames></author><author><keyname>Su</keyname><forenames>Wei</forenames></author></authors><title>Modulation Classification for MIMO-OFDM Signals via Approximate Bayesian
  Inference</title><categories>cs.IT math.IT</categories><comments>This manuscript has been submitted to IEEE Trans. Veh. Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of modulation classification for a multiple-antenna (MIMO) system
employing orthogonal frequency division multiplexing (OFDM) is investigated
under the assumption of unknown frequency-selective fading channels and
signal-to-noise ratio (SNR). The classification problem is formulated as a
Bayesian inference task, and solutions are proposed based on Gibbs sampling and
mean field variational inference. The proposed methods rely on a selection of
the prior distributions that adopts a latent Dirichlet model for the modulation
type and on the Bayesian network formalism. The Gibbs sampling method converges
to the optimal Bayesian solution and, using numerical results, its accuracy is
seen to improve for small sample sizes when switching to the mean field
variational inference technique after a number of iterations. The speed of
convergence is shown to improve via annealing and random restarts. While most
of the literature on modulation classification assume that the channels are
flat fading, that the number of receive antennas is no less than that of
transmit antennas, and that a large number of observed data symbols are
available, the proposed methods perform well under more general conditions.
Finally, the proposed Bayesian methods are demonstrated to improve over
existing non-Bayesian approaches based on independent component analysis and on
prior Bayesian methods based on the `superconstellation' method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03355</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03355</id><created>2015-09-10</created><authors><author><keyname>Zapolsky</keyname><forenames>Samuel</forenames></author><author><keyname>Drumwright</keyname><forenames>Evan</forenames></author></authors><title>Inverse Dynamics with Rigid Contact and Friction</title><categories>cs.RO</categories><comments>Submitted to Springer Autonomous Robots (AURO), 28 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inverse dynamics is used extensively in robotics and biomechanics
applications. In manipulator and legged robots, it can form the basis of an
effective nonlinear control strategy by providing a robot with both accurate
positional tracking and active compliance. In biomechanics applications,
inverse dynamics control can approximately determine the net torques applied at
anatomical joints that correspond to an observed motion.
  In the context of robot control, using inverse dynamics requires knowledge of
all contact forces acting on the robot; accurately perceiving external forces
applied to the robot requires filtering and thus significant time delay. An
alternative approach has been suggested in recent literature: predicting
contact and actuator forces simultaneously under the assumptions of rigid body
dynamics, rigid contact, and friction. Existing such inverse dynamics
approaches have used approximations to the contact models, which permits use of
fast numerical linear algebra algorithms. In contrast, we describe inverse
dynamics methods that are derived only from first principles.
  We assess these inverse dynamics approaches in a control context on a
locomoting quadrupedal robot using both perfectly accurate sensor data from
simulation. The data collected from these experiments gives an upper bound on
the performance of such controllers in situ. For points of comparison, we
assess performance on the same tasks with both error feedback control and
inverse dynamics control with virtual contact force sensors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03356</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03356</id><created>2015-09-10</created><authors><author><keyname>Messai</keyname><forenames>Malek</forenames></author><author><keyname>Giulio</keyname><forenames>Colavolpe</forenames></author><author><keyname>Karine</keyname><forenames>Amis</forenames></author><author><keyname>Frederic</keyname><forenames>Guilloud</forenames></author></authors><title>Binary Continuous Phase Modulations Robust to a Modulation Index
  Mismatch</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider binary continuous phase modulation (CPM) signals used in some
recent low-cost and low-power consumption telecommunications standard. When
these signals are generated through a low-cost transmitter, the real modulation
index can end up being quite different from the nominal value employed at the
receiver and a significant performance degradation is observed, unless proper
techniques for the estimation and compensation are employed. For this reason,
we design new binary schemes with a much higher robustness. They are based on
the concatenation of a suitable precoder with binary input and a ternary CPM
format. The result is a family of CPM formats whose phase state is constrained
to follow a specific evolution. Two of these precoders are considered. We will
discuss many aspects related to these schemes, such as the power spectral
density, the spectral efficiency, simplified detection, the minimum distance,
and the uncoded performance. The adopted precoders do not change the recursive
nature of CPM schemes. So these schemes are still suited for serial
concatenation, through a pseudo-random interleaver, with an outer channel
encoder.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03357</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03357</id><created>2015-09-10</created><authors><author><keyname>Wang</keyname><forenames>Wei</forenames></author><author><keyname>Tang</keyname><forenames>Ming</forenames></author><author><keyname>Shu</keyname><forenames>Panpan</forenames></author><author><keyname>Wang</keyname><forenames>Zhen</forenames></author></authors><title>Dynamics of social contagions with heterogeneous adoption thresholds:
  Crossover phenomena in phase transition</title><categories>physics.soc-ph cs.SI</categories><report-no>New J. Phys.18 013029 (2016)</report-no><doi>10.1088/1367-2630/18/1/013029</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Heterogeneous adoption thresholds exist widely in social contagions, but were
always neglected in previous studies. We first propose a non-Markovian
spreading threshold model with general adoption threshold distribution. In
order to understand the effects of heterogeneous adoption thresholds
quantitatively, an edge-based compartmental theory is developed for the
proposed model. We use a binary spreading threshold model as a specific
example, in which some individuals have a low adoption threshold (i.e.,
activists) while the remaining ones hold a relatively high adoption threshold
(i.e., bigots), to demonstrate that heterogeneous adoption thresholds markedly
affect the final adoption size and phase transition. Interestingly, the
first-order, second-order and hybrid phase transitions can be found in the
system. More importantly, there are two different kinds of crossover phenomena
in phase transition for distinct values of bigots' adoption threshold: a change
from first-order or hybrid phase transition to the second-order phase
transition. The theoretical predictions based on the suggested theory agree
very well with the results of numerical simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03371</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03371</id><created>2015-09-10</created><authors><author><keyname>Tschopp</keyname><forenames>Fabian</forenames></author></authors><title>Efficient Convolutional Neural Networks for Pixelwise Classification on
  Heterogeneous Hardware Systems</title><categories>cs.CV cs.AI</categories><comments>92 pages, project source code available at
  https://github.com/naibaf7/, technical report written at ETH Z\&quot;urich, in
  collaboration with AMD, UZH INI and HHMI Janelia</comments><acm-class>I.2.6; I.5.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work presents and analyzes three convolutional neural network (CNN)
models for efficient pixelwise classification of images. When using
convolutional neural networks to classify single pixels in patches of a whole
image, a lot of redundant computations are carried out when using sliding
window networks. This set of new architectures solve this issue by either
removing redundant computations or using fully convolutional architectures that
inherently predict many pixels at once.
  The implementations of the three models are accessible through a new utility
on top of the Caffe library. The utility provides support for a wide range of
image input and output formats, pre-processing parameters and methods to
equalize the label histogram during training. The Caffe library has been
extended by new layers and a new backend for availability on a wider range of
hardware such as CPUs and GPUs through OpenCL.
  On AMD GPUs, speedups of $54\times$ (SK-Net), $437\times$ (U-Net) and
$320\times$ (USK-Net) have been observed, taking the SK equivalent SW (sliding
window) network as the baseline. The label throughput is up to one megapixel
per second.
  The analyzed neural networks have distinctive characteristics that apply
during training or processing, and not every data set is suitable to every
architecture. The quality of the predictions is assessed on two neural tissue
data sets, of which one is the ISBI 2012 challenge data set. Two different loss
functions, Malis loss and Softmax loss, were used during training.
  The whole pipeline, consisting of models, interface and modified Caffe
library, is available as Open Source software under the working title Project
Greentea.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03374</identifier>
 <datestamp>2015-11-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03374</id><created>2015-09-10</created><updated>2015-10-30</updated><authors><author><keyname>Hajiesmaili</keyname><forenames>Mohammad H.</forenames></author><author><keyname>Talebi</keyname><forenames>Mohammad Sadegh</forenames></author><author><keyname>Khonsari</keyname><forenames>Ahmad</forenames></author></authors><title>Utility-Optimal Dynamic Rate Allocation under Average End-to-End Delay
  Requirements</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  QoS-aware networking applications such as real-time streaming and video
surveillance systems require nearly fixed average end-to-end delay over long
periods to communicate efficiently, although may tolerate some delay variations
in short periods. This variability exhibits complex dynamics that makes rate
control of such applications a formidable task. This paper addresses rate
allocation for heterogeneous QoS-aware applications that preserves the
long-term end-to-end delay constraint while, similar to Dynamic Network Utility
Maximization (DNUM), strives to achieve the maximum network utility aggregated
over a fixed time interval. Since capturing temporal dynamics in QoS
requirements of sources is allowed in our system model, we incorporate a novel
time-coupling constraint in which delay-sensitivity of sources is considered
such that a certain end-to-end average delay for each source over a
pre-specified time interval is satisfied. We propose DA-DNUM algorithm, as a
dual-based solution, which allocates source rates for the next time interval in
a distributed fashion, given the knowledge of network parameters in advance.
Through numerical experiments, we show that DA-DNUM gains higher average link
utilization and a wider range of feasible scenarios in comparison with the
best, to our knowledge, rate control schemes that may guarantee such
constraints on delay.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03388</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03388</id><created>2015-09-11</created><authors><author><keyname>Abeywardena</keyname><forenames>Dinuka</forenames></author><author><keyname>Kodagoda</keyname><forenames>Sarath</forenames></author><author><keyname>Dissanayake</keyname><forenames>Gamini</forenames></author><author><keyname>Munasinghe</keyname><forenames>Rohan</forenames></author></authors><title>Improved State Estimation in Quadrotor MAVs: A Novel Drift-Free Velocity
  Estimator</title><categories>cs.RO</categories><comments>Published in IEEE Robotics &amp; Automation Magazine, (Volume:20 , Issue:
  4 )</comments><journal-ref>IEEE Robotics &amp; Automation Magazine, (Volume:20 , Issue: 4 ) 2013</journal-ref><doi>10.1109/MRA.2012.2225472</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes the synthesis and evaluation of a novel state estimator
for a Quadrotor Micro Aerial Vehicle. Dynamic equations which relate
acceleration, attitude and the aero-dynamic propeller drag are encapsulated in
an extended Kalman filter framework for estimating the velocity and the
attitude of the quadrotor. It is demonstrated that exploiting the relationship
between the body frame accelerations and velocities, due to blade flapping,
enables drift free estimation of lateral and longitudinal components of body
frame translational velocity along with improvements to roll and pitch
components of body attitude estimations. Real world data sets gathered using a
commercial off-the-shelf quadrotor platform, together with ground truth data
from a Vicon system, are used to evaluate the effectiveness of the proposed
algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03389</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03389</id><created>2015-09-11</created><authors><author><keyname>Lang</keyname><forenames>Jerome</forenames></author><author><keyname>Skowron</keyname><forenames>Piotr</forenames></author></authors><title>Multi-Attribute Proportional Representation</title><categories>cs.AI cs.DS cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the following problem in which a given number of items has to be
chosen from a predefined set. Each item is described by a vector of attributes
and for each attribute there is a desired distribution that the selected set
should have. We look for a set that fits as much as possible the desired
distributions on all attributes. Examples of applications include choosing
members of a representative committee, where candidates are described by
attributes such as sex, age and profession, and where we look for a committee
that for each attribute offers a certain representation, i.e., a single
committee that contains a certain number of young and old people, certain
number of men and women, certain number of people with different professions,
etc. With a single attribute the problem collapses to the apportionment problem
for party-list proportional representation systems (in such case the value of
the single attribute would be a political affiliation of a candidate). We study
the properties of the associated subset selection rules, as well as their
computation complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03390</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03390</id><created>2015-09-11</created><authors><author><keyname>Ohlsson</keyname><forenames>Stellan</forenames></author><author><keyname>Sloan</keyname><forenames>Robert H.</forenames></author><author><keyname>Tur&#xe1;n</keyname><forenames>Gy&#xf6;rgy</forenames></author><author><keyname>Urasky</keyname><forenames>Aaron</forenames></author></authors><title>Measuring an Artificial Intelligence System's Performance on a Verbal IQ
  Test For Young Children</title><categories>cs.AI</categories><comments>17 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We administered the Verbal IQ (VIQ) part of the Wechsler Preschool and
Primary Scale of Intelligence (WPPSI-III) to the ConceptNet 4 AI system. The
test questions (e.g., &quot;Why do we shake hands?&quot;) were translated into ConceptNet
4 inputs using a combination of the simple natural language processing tools
that come with ConceptNet together with short Python programs that we wrote.
The question answering used a version of ConceptNet based on spectral methods.
The ConceptNet system scored a WPPSI-III VIQ that is average for a
four-year-old child, but below average for 5 to 7 year-olds. Large variations
among subtests indicate potential areas of improvement. In particular, results
were strongest for the Vocabulary and Similarities subtests, intermediate for
the Information subtest, and lowest for the Comprehension and Word Reasoning
subtests. Comprehension is the subtest most strongly associated with common
sense. The large variations among subtests and ordinary common sense strongly
suggest that the WPPSI-III VIQ results do not show that &quot;ConceptNet has the
verbal abilities a four-year-old.&quot; Rather, children's IQ tests offer one
objective metric for the evaluation and comparison of AI systems. Also, this
work continues previous research on Psychometric AI.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03391</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03391</id><created>2015-09-11</created><authors><author><keyname>Deng</keyname><forenames>Yuxin</forenames></author><author><keyname>Du</keyname><forenames>Wenjie</forenames></author><author><keyname>Gebler</keyname><forenames>Daniel</forenames></author></authors><title>Modal Characterisations of Behavioural Pseudometrics</title><categories>cs.LO</categories><comments>20 pages</comments><msc-class>68Q85</msc-class><acm-class>F.3.2</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  For the model of probabilistic labelled transition systems that allow for the
co-existence of nondeterminism and probabilities, we present two notions of
bisimulation metrics: one is state-based and the other is distribution-based.
We provide a sound and complete modal characterisation for each of them, using
real-valued modal logics based on the Hennessy-Milner logic. The logic for
characterising the state-based metric is much simpler than an earlier logic by
Desharnais et al. as it uses only two non-expansive operators rather than the
general class of non-expansive operators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03411</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03411</id><created>2015-09-11</created><authors><author><keyname>Khanzadi</keyname><forenames>M. Reza</forenames></author><author><keyname>Krishnan</keyname><forenames>Rajet</forenames></author><author><keyname>Eriksson</keyname><forenames>Thomas</forenames></author></authors><title>Receiver Algorithm based on Differential Signaling for SIMO Phase Noise
  Channels with Common and Separate Oscillator Configurations</title><categories>cs.IT math.IT</categories><comments>IEEE GLOBECOM 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a receiver algorithm consisting of differential transmission
and a two-stage detection for a single-input multiple-output (SIMO) phase-noise
channels is studied. Specifically, the phases of the QAM modulated data symbols
are manipulated before transmission in order to make them more immune to the
random rotational effects of phase noise. At the receiver, a two-stage detector
is implemented, which first detects the amplitude of the transmitted symbols
from a nonlinear combination of the received signal amplitudes. Then in the
second stage, the detector performs phase detection. The studied signaling
method does not require transmission of any known symbols that act as pilots.
Furthermore, no phase noise estimator (or a tracker) is needed at the receiver
to compensate the effect of phase noise. This considerably reduces the
complexity of the receiver structure. Moreover, it is observed that the studied
algorithm can be used for the setups where a common local oscillator or
separate independent oscillators drive the radio-frequency circuitries
connected to each antenna. Due to the differential encoding/decoding of the
phase, weighted averaging can be employed at a multi-antenna receiver, allowing
for phase noise suppression to leverage the large number of antennas. Hence, we
observe that the performance improves by increasing the number of antennas,
especially in the separate oscillator case. Further increasing the number of
receive antennas results in a performance error floor, which is a function of
the quality of the oscillator at the transmitter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03413</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03413</id><created>2015-09-11</created><authors><author><keyname>Basu</keyname><forenames>Saikat</forenames></author><author><keyname>Karki</keyname><forenames>Manohar</forenames></author><author><keyname>Ganguly</keyname><forenames>Sangram</forenames></author><author><keyname>DiBiano</keyname><forenames>Robert</forenames></author><author><keyname>Mukhopadhyay</keyname><forenames>Supratik</forenames></author><author><keyname>Nemani</keyname><forenames>Ramakrishna</forenames></author></authors><title>Learning Sparse Feature Representations using Probabilistic Quadtrees
  and Deep Belief Nets</title><categories>cs.CV</categories><comments>Published in the European Symposium on Artificial Neural Networks,
  ESANN 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning sparse feature representations is a useful instrument for solving an
unsupervised learning problem. In this paper, we present three labeled
handwritten digit datasets, collectively called n-MNIST. Then, we propose a
novel framework for the classification of handwritten digits that learns sparse
representations using probabilistic quadtrees and Deep Belief Nets. On the
MNIST and n-MNIST datasets, our framework shows promising results and
significantly outperforms traditional Deep Belief Networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03424</identifier>
 <datestamp>2015-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03424</id><created>2015-09-11</created><updated>2015-11-03</updated><authors><author><keyname>Karpenkov</keyname><forenames>George</forenames></author><author><keyname>Monniaux</keyname><forenames>David</forenames></author><author><keyname>Wendler</keyname><forenames>Philipp</forenames></author></authors><title>Program Analysis with Local Policy Iteration</title><categories>cs.LO cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new algorithm for deriving numerical invariants that combines
the precision of max-policy iteration with the flexibility and scalability of
conventional Kleene iterations. It is defined in the Configurable Program
Analysis (CPA) framework, thus allowing inter-analysis communication.
  It uses adjustable-block encoding in order to traverse loop-free program
sections, possibly containing branching, without introducing extra abstraction.
Our technique operates over any template linear constraint domain, including
the interval and octagon domains; templates can also be derived from the
program source.
  The implementation is evaluated on a set of benchmarks from the Software
Verification Competition (SV-Comp). It competes favorably with state-of-the-art
analyzers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03427</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03427</id><created>2015-09-11</created><authors><author><keyname>Haesaert</keyname><forenames>Sofie</forenames></author><author><keyname>Hof</keyname><forenames>Paul M. J. Van den</forenames></author><author><keyname>Abate</keyname><forenames>Alessandro</forenames></author></authors><title>Observer-based correct-by-design controller synthesis</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Current state-of-the-art correct-by-design controllers are designed for
full-state measurable systems. This work first extends the applicability of
correct-by-design controllers to partially observable LTI systems. Leveraging
2nd order bounds we give a design method that has a quantifiable robustness to
probabilistic disturbances on state transitions and on output measurements. In
a case study from smart buildings we evaluate the new output-based
correct-by-design controller on a physical system with limited sensor
information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03447</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03447</id><created>2015-09-11</created><authors><author><keyname>Brandenburg</keyname><forenames>Franz J.</forenames></author></authors><title>On 4-Map Graphs and 1-Planar Graphs and their Recognition Problem</title><categories>cs.CG</categories><msc-class>68R10, 05C10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We establish a one-to-one correspondence between 1-planar graphs and general
and hole-free 4-map graphs and show that 1-planar graphs can be recognized in
polynomial time if they are crossing-augmented, fully triangulated, and maximal
1-planar, respectively, with a polynomial of degree 120, 3, and 5,
respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03453</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03453</id><created>2015-09-11</created><authors><author><keyname>Verdoliva</keyname><forenames>Luisa</forenames></author><author><keyname>Cozzolino</keyname><forenames>Davide</forenames></author><author><keyname>Poggi</keyname><forenames>Giovanni</forenames></author></authors><title>A reliable order-statistics-based approximate nearest neighbor search
  algorithm</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a new algorithm for fast approximate nearest neighbor search based
on the properties of ordered vectors. Data vectors are classified based on the
index and sign of their largest components, thereby partitioning the space in a
number of cones centered in the origin. The query is itself classified, and the
search starts from the selected cone and proceeds to neighboring ones. Overall,
the proposed algorithm corresponds to locality sensitive hashing in the space
of directions, with hashing based on the order of components. Thanks to the
statistical features emerging through ordering, it deals very well with the
challenging case of unstructured data, and is a valuable building block for
more complex techniques dealing with structured data. Experiments on both
simulated and real-world data prove the proposed algorithm to provide a
state-of-the-art performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03456</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03456</id><created>2015-09-11</created><authors><author><keyname>Harraj</keyname><forenames>Abdeslam El</forenames></author><author><keyname>Raissouni</keyname><forenames>Naoufal</forenames></author></authors><title>OCR accuracy improvement on document images through a novel
  pre-processing approach</title><categories>cs.CV</categories><journal-ref>Signal &amp; Image Processing : An International Journal (SIPIJ)
  Vol.6, No.4, August 2015</journal-ref><doi>10.5121/sipij.2015.6401</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Digital camera and mobile document image acquisition are new trends arising
in the world of Optical Character Recognition and text detection. In some
cases, such process integrates many distortions and produces poorly scanned
text or text-photo images and natural images, leading to an unreliable OCR
digitization. In this paper, we present a novel nonparametric and unsupervised
method to compensate for undesirable document image distortions aiming to
optimally improve OCR accuracy. Our approach relies on a very efficient stack
of document image enhancing techniques to recover deformation of the entire
document image. First, we propose a local brightness and contrast adjustment
method to effectively handle lighting variations and the irregular distribution
of image illumination. Second, we use an optimized greyscale conversion
algorithm to transform our document image to greyscale level. Third, we sharpen
the useful information in the resulting greyscale image using Un-sharp Masking
method. Finally, an optimal global binarization approach is used to prepare the
final document image to OCR recognition. The proposed approach can
significantly improve text detection rate and optical character recognition
accuracy. To demonstrate the efficiency of our approach, an exhaustive
experimentation on a standard dataset is presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03475</identifier>
 <datestamp>2015-10-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03475</id><created>2015-09-11</created><updated>2015-10-23</updated><authors><author><keyname>Cho</keyname><forenames>Minhyung</forenames></author><author><keyname>Dhir</keyname><forenames>Chandra Shekhar</forenames></author><author><keyname>Lee</keyname><forenames>Jaehyung</forenames></author></authors><title>Hessian-free Optimization for Learning Deep Multidimensional Recurrent
  Neural Networks</title><categories>cs.LG cs.NE stat.ML</categories><comments>to appear at NIPS 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multidimensional recurrent neural networks (MDRNNs) have shown a remarkable
performance in the area of speech and handwriting recognition. The performance
of an MDRNN is improved by further increasing its depth, and the difficulty of
learning the deeper network is overcome by using Hessian-free (HF)
optimization. Given that connectionist temporal classification (CTC) is
utilized as an objective of learning an MDRNN for sequence labeling, the
non-convexity of CTC poses a problem when applying HF to the network. As a
solution, a convex approximation of CTC is formulated and its relationship with
the EM algorithm and the Fisher information matrix is discussed. An MDRNN up to
a depth of 15 layers is successfully trained using HF, resulting in an improved
performance for sequence labeling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03476</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03476</id><created>2015-09-11</created><updated>2015-10-14</updated><authors><author><keyname>Barthe</keyname><forenames>Gilles</forenames></author><author><keyname>Espitau</keyname><forenames>Thomas</forenames></author><author><keyname>Gr&#xe9;goire</keyname><forenames>Benjamin</forenames></author><author><keyname>Hsu</keyname><forenames>Justin</forenames></author><author><keyname>Stefanesco</keyname><forenames>L&#xe9;o</forenames></author><author><keyname>Strub</keyname><forenames>Pierre-Yves</forenames></author></authors><title>Relational reasoning via probabilistic coupling</title><categories>cs.LO cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Probabilistic coupling is a powerful tool for analyzing pairs of
probabilistic processes. Roughly, coupling two processes requires finding an
appropriate witness process that models both processes in the same probability
space. Couplings are powerful tools proving properties about the relation
between two processes, include reasoning about convergence of distributions and
stochastic dominance---a probabilistic version of a monotonicity property.
  While the mathematical definition of coupling looks rather complex and
cumbersome to manipulate, we show that the relational program logic pRHL---the
logic underlying the EasyCrypt cryptographic proof assistant---already
internalizes a generalization of probabilistic coupling. With this insight,
constructing couplings is no harder than constructing logical proofs. We
demonstrate how to express and verify classic examples of couplings in pRHL,
and we mechanically verify several couplings in EasyCrypt.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03484</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03484</id><created>2015-09-11</created><updated>2016-01-29</updated><authors><author><keyname>Hu</keyname><forenames>Yanqing</forenames></author><author><keyname>Ji</keyname><forenames>Shenggong</forenames></author><author><keyname>Feng</keyname><forenames>Ling</forenames></author><author><keyname>Havlin</keyname><forenames>Shlomo</forenames></author><author><keyname>Jin</keyname><forenames>Yuliang</forenames></author></authors><title>Optimizing locally the spread of influence in large scale online social
  networks</title><categories>physics.soc-ph cs.CY cs.DS cs.SI</categories><comments>50 pages, 21 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optimizing the spread of influence in online social networks (OSNs) is
important for the design of efficient viral marketing strategies using online
recommendations. It is commonly believed that, spreading is a global process,
whose optimization would require the knowledge of the whole network
information. Here we uncover a characteristic local length scale, called
influence radius, hidden in the global nature of spreading processes. We show
that, any node's influence to the entire OSN can be quantified from its local
network environment within the influence radius, which is significantly smaller
than the whole network diameter. By mapping the problem onto bond percolation,
we give a theoretical explanation for the presence of this short influence
radius, and a framework to quantify individual's influence in real OSNs. We
then propose a scalable optimization algorithm to identify the most influential
spreaders. The time complexity of our algorithm is independent of network size,
and its performance is remarkably close the true optimum. Our method may be
applied to other large scale spreading problems, such as the world-wide
epidemic control.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03486</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03486</id><created>2015-09-11</created><updated>2015-10-27</updated><authors><author><keyname>Ivanov</keyname><forenames>Todor</forenames></author><author><keyname>Izberovic</keyname><forenames>Sead</forenames></author></authors><title>Evaluating Hadoop Clusters with TPCx-HS</title><categories>cs.DC</categories><comments>32 pages</comments><report-no>Technical Report No. 2015-1</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The growing complexity and variety of Big Data platforms makes it both
difficult and time consuming for all system users to properly setup and operate
the systems. Another challenge is to compare the platforms in order to choose
the most appropriate one for a particular application. All these factors
motivate the need for a standardized Big Data benchmark that can help the users
in the process of platform evaluation. Just recently TPCx-HS [1][2] has been
released as the first standardized Big Data benchmark designed to stress test a
Hadoop cluster. The goal of this study is to evaluate and compare how the
network setup influences the performance of a Hadoop cluster. In particular,
experiments were performed using shared and dedicated 1Gbit networks utilized
by the same Cloudera Hadoop Distribution (CDH) cluster setup. The TPCx-HS
benchmark, which is very network intensive, was used to stress test and compare
both cluster setups. All the presented results are obtained by using the
officially available version [1] of the benchmark, but they are not comparable
with the officially reported results and are meant as an experimental
evaluation, not audited by any external organization. As expected the dedicated
1Gbit network setup performed much faster than the shared 1Gbit setup. However,
what was surprising is the negligible price difference between both cluster
setups, which pays off with a multifold performance return.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03488</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03488</id><created>2015-09-11</created><authors><author><keyname>Eckle-Kohler</keyname><forenames>Judith</forenames></author></authors><title>Inferring and evaluating semantic classes of verbs signaling modality</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We infer semantic classes of verbs signaling modality from a purely syntactic
classification of 637 German verbs by applying findings from linguistics about
correspondences between verb meaning and syntax. Our extensive evaluation of
the semantic classification is based on a linking to three other lexical
resources at the word sense level: to the German wordnet GermaNet and to the
English resources VerbNet and FrameNet. This way, we are able to perform a
reproducible semantic characterization of the inferred German classes. We also
perform a corpus-based evaluation revealing that the frequencies of the classes
in corpora of different genres are significantly different. We will make the
resulting bilingual resource of German-English semantically categorized verb
classes publicly available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03500</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03500</id><created>2015-09-11</created><authors><author><keyname>Canu</keyname><forenames>Ma&#xeb;l</forenames><affiliation>LIP6</affiliation></author><author><keyname>Detyniecki</keyname><forenames>Marcin</forenames><affiliation>IBS PAN, LIP6</affiliation></author><author><keyname>Lesot</keyname><forenames>Marie-Jeanne</forenames><affiliation>LIP6</affiliation></author><author><keyname>d'Allonnes</keyname><forenames>Adrien Revault</forenames><affiliation>LIASD</affiliation></author></authors><title>Fast community structure local uncovering by independent vertex-centred
  process</title><categories>cs.SI physics.soc-ph</categories><comments>2015 IEEE/ACM International Conference on Advances in Social Networks
  Analysis and Mining, Aug 2015, Paris, France. Proceedings of the 2015
  IEEE/ACM International Conference on Advances in Social Networks Analysis and
  Mining</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the task of community detection and proposes a local
approach based on a distributed list building, where each vertex broadcasts
basic information that only depends on its degree and that of its neighbours. A
decentralised external process then unveils the community structure. The
relevance of the proposed method is experimentally shown on both artificial and
real data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03502</identifier>
 <datestamp>2015-09-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03502</id><created>2015-09-11</created><updated>2015-09-25</updated><authors><author><keyname>Oh</keyname><forenames>Seong Joon</forenames></author><author><keyname>Benenson</keyname><forenames>Rodrigo</forenames></author><author><keyname>Fritz</keyname><forenames>Mario</forenames></author><author><keyname>Schiele</keyname><forenames>Bernt</forenames></author></authors><title>Person Recognition in Personal Photo Collections</title><categories>cs.CV</categories><comments>Accepted to ICCV 2015, revised</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recognising persons in everyday photos presents major challenges (occluded
faces, different clothing, locations, etc.) for machine vision. We propose a
convnet based person recognition system on which we provide an in-depth
analysis of informativeness of different body cues, impact of training data,
and the common failure modes of the system. In addition, we discuss the
limitations of existing benchmarks and propose more challenging ones. Our
method is simple and is built on open source and open data, yet it improves the
state of the art results on a large dataset of social media photos (PIPA).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03503</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03503</id><created>2015-09-04</created><authors><author><keyname>Winkler</keyname><forenames>Marco</forenames></author></authors><title>NoSPaM Manual - A Tool for Node-Specific Triad Pattern Mining</title><categories>cs.SI cs.CV cs.DS physics.data-an physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The detection of triadic subgraph motifs is a common methodology in
complex-networks research. The procedure usually applied in order to detect
motifs evaluates whether a certain subgraph pattern is overrepresented in a
network as a whole. However, motifs do not necessarily appear frequently in
every region of a graph. For this reason, we recently introduced the framework
of Node-Specific Pattern Mining (NoSPaM). This work is a manual for an
implementation of NoSPaM which can be downloaded from www.mwinkler.eu.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03527</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03527</id><created>2015-09-11</created><authors><author><keyname>Gauthier</keyname><forenames>Thibault</forenames></author><author><keyname>Kaliszyk</keyname><forenames>Cezary</forenames></author></authors><title>Sharing HOL4 and HOL Light proof knowledge</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  New proof assistant developments often involve concepts similar to already
formalized ones. When proving their properties, a human can often take
inspiration from the existing formalized proofs available in other provers or
libraries. In this paper we propose and evaluate a number of methods, which
strengthen proof automation by learning from proof libraries of different
provers. Certain conjectures can be proved directly from the dependencies
induced by similar proofs in the other library. Even if exact correspondences
are not found, learning-reasoning systems can make use of the association
between proved theorems and their characteristics to predict the relevant
premises. Such external help can be further combined with internal advice. We
evaluate the proposed knowledge-sharing methods by reproving the HOL Light and
HOL4 standard libraries. The learning-reasoning system HOL(y)Hammer, whose
single best strategy could automatically find proofs for 30% of the HOL Light
problems, can prove 40% with the knowledge from HOL4.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03530</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03530</id><created>2015-09-11</created><authors><author><keyname>Dega</keyname><forenames>Ravi Kumar Yadav</forenames></author><author><keyname>Ercal</keyname><forenames>Gunes</forenames></author></authors><title>A comparative analysis of progressive multiple sequence alignment
  approaches using UPGMA and neighbor joining based guide trees</title><categories>cs.CE cs.DS</categories><comments>9 Pages</comments><journal-ref>International Journal of Computer Science, Engineering and
  Information Technology (IJCSEIT), Vol. 5,No.3/4, August 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiple sequence alignment is increasingly important to bioinformatics, with
several applications ranging from phylogenetic analyses to domain
identification. There are several ways to perform multiple sequence alignment,
an important way of which is the progressive alignment approach studied in this
work. Progressive alignment involves three steps: find the distance between
each pair of sequences; construct a guide tree based on the distance matrix;
finally based on the guide tree align sequences using the concept of aligned
profiles. Our contribution is in comparing two main methods of guide tree
construction in terms of both efficiency and accuracy of the overall alignment:
UPGMA and Neighbor Join methods. Our experimental results indicate that the
Neighbor Join method is both more efficient in terms of performance and more
accurate in terms of overall cost minimization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03531</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03531</id><created>2015-09-11</created><authors><author><keyname>Egele</keyname><forenames>Manuel</forenames></author><author><keyname>Stringhini</keyname><forenames>Gianluca</forenames></author><author><keyname>Kruegel</keyname><forenames>Christopher</forenames></author><author><keyname>Vigna</keyname><forenames>Giovanni</forenames></author></authors><title>Towards Detecting Compromised Accounts on Social Networks</title><categories>cs.CR cs.SI</categories><journal-ref>TDSC-2014-10-0271.R1</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compromising social network accounts has become a profitable course of action
for cybercriminals. By hijacking control of a popular media or business
account, attackers can distribute their malicious messages or disseminate fake
information to a large user base. The impacts of these incidents range from a
tarnished reputation to multi-billion dollar monetary losses on financial
markets. In our previous work, we demonstrated how we can detect large-scale
compromises (i.e., so-called campaigns) of regular online social network users.
In this work, we show how we can use similar techniques to identify compromises
of individual high-profile accounts. High-profile accounts frequently have one
characteristic that makes this detection reliable -- they show consistent
behavior over time. We show that our system, were it deployed, would have been
able to detect and prevent three real-world attacks against popular companies
and news agencies. Furthermore, our system, in contrast to popular media, would
not have fallen for a staged compromise instigated by a US restaurant chain for
publicity reasons.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03534</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03534</id><created>2015-09-11</created><authors><author><keyname>Gauthier</keyname><forenames>Thibault</forenames></author><author><keyname>Kaliszyk</keyname><forenames>Cezary</forenames></author></authors><title>Premise Selection and External Provers for HOL4</title><categories>cs.AI</categories><doi>10.1145/2676724.2693173</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning-assisted automated reasoning has recently gained popularity among
the users of Isabelle/HOL, HOL Light, and Mizar. In this paper, we present an
add-on to the HOL4 proof assistant and an adaptation of the HOLyHammer system
that provides machine learning-based premise selection and automated reasoning
also for HOL4. We efficiently record the HOL4 dependencies and extract features
from the theorem statements, which form a basis for premise selection.
HOLyHammer transforms the HOL4 statements in the various TPTP-ATP proof
formats, which are then processed by the ATPs. We discuss the different
evaluation settings: ATPs, accessible lemmas, and premise numbers. We measure
the performance of HOLyHammer on the HOL4 standard library. The results are
combined accordingly and compared with the HOL Light experiments, showing a
comparably high quality of predictions. The system directly benefits HOL4 users
by automatically finding proofs dependencies that can be reconstructed by
Metis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03542</identifier>
 <datestamp>2015-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03542</id><created>2015-09-11</created><updated>2015-11-25</updated><authors><author><keyname>Minaee</keyname><forenames>Shervin</forenames></author><author><keyname>Wang</keyname><forenames>Yao</forenames></author></authors><title>Fingerprint Recognition Using Translation Invariant Scattering Network</title><categories>cs.CV</categories><comments>IEEE Signal Processing in Medicine and Biology Symposium, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fingerprint recognition has drawn a lot of attention during last decades.
Different features and algorithms have been used for fingerprint recognition in
the past. In this paper, a powerful image representation called scattering
transform/network, is used for recognition. Scattering network is a
convolutional network where its architecture and filters are predefined wavelet
transforms. The first layer of scattering representation is similar to sift
descriptors and the higher layers capture higher frequency content of the
signal. After extraction of scattering features, their dimensionality is
reduced by applying principal component analysis (PCA). At the end, multi-class
SVM is used to perform template matching for the recognition task. The proposed
scheme is tested on a well-known fingerprint database and has shown promising
results with the best accuracy rate of 98\%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03543</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03543</id><created>2015-09-11</created><updated>2015-10-17</updated><authors><author><keyname>Goldberg</keyname><forenames>Leslie Ann</forenames></author><author><keyname>Jerrum</keyname><forenames>Mark</forenames></author></authors><title>The complexity of Boolean #MaximalCSP</title><categories>cs.CC</categories><comments>This version adds contextual material relating the results obtained
  here to earlier work in a different but related setting. The technical
  content is unchanged. 18 pages</comments><msc-class>68Q17</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the computational complexity of the problem of counting the
maximal satisfying assignments of a Constraint Satisfaction Problem (CSP) over
the Boolean domain {0,1}. A satisfying assignment is maximal if any new
assignment which is obtained from it by changing a 0 to a 1 is unsatisfying.
For each constraint language Gamma, #MaximalCSP(Gamma) denotes the problem of
counting the maximal satisfying assignments, given an input CSP with
constraints in Gamma. We give a complexity dichotomy for the problem of exactly
counting the maximal satisfying assignments and a complexity trichotomy for the
problem of approximately counting them. Relative to the problem #CSP(Gamma),
which is the problem of counting all satisfying assignments, the maximal
version can sometimes be easier but never harder. This finding contrasts with
the recent discovery that approximately counting maximal independent sets in a
bipartite graph is harder (under the usual complexity-theoretic assumptions)
than counting all independent sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03547</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03547</id><created>2015-09-11</created><updated>2015-12-30</updated><authors><author><keyname>Maity</keyname><forenames>Soumen</forenames></author><author><keyname>Akhtar</keyname><forenames>Yasmeen</forenames></author><author><keyname>Chandrasekharan</keyname><forenames>Reshma C</forenames></author><author><keyname>Colbourn</keyname><forenames>Charles J</forenames></author></authors><title>Improved Strength Four Covering Arrays with Three Symbols</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A covering array $t$-$CA(n,k,g)$, of size $n$, strength $t$, degree $k$, and
order $g$, is a $k\times n$ array on $g$ symbols such that every $t\times n$
sub-array contains every $t\times 1$ column on $g$ symbols at least once.
Covering arrays have been studied for their applications to software testing,
hardware testing, drug screening, and in areas where interactions of multiple
parameters are to be tested. In this paper, we present an algebraic
construction that improves many of the best known upper bounds on $n$ for
covering arrays 4-$CA(n,k,g)$ with $g=3$. The $coverage$ $measure$ $\mu_t(A)$
of a testing array $A$ is defined by the ratio between the number of distinct
$t$-tuples contained in the column vectors of $A$ and the total number of
$t$-tuples. A covering array is a testing array with full coverage. The
$covering$ $arrays$ $with$ $budget$ $constraints$ $problem$ is the problem of
constructing a testing array of size at most $n$ having largest possible
coverage measure, given values of $k,g$ and $n$. This paper presents several
strength four testing arrays with high coverage. The construction here is a
generalisation of the construction methods used by Chateauneuf, Colbourn and
Kreher, and Meagher and Stevens.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03548</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03548</id><created>2015-09-11</created><authors><author><keyname>M&#xf6;stl</keyname><forenames>Georg</forenames></author><author><keyname>Springer</keyname><forenames>Andreas</forenames></author></authors><title>MiXiM, PAWiS, and STEAM-Sim Integration - Combining Channel Models,
  Energy Awareness, and Real-life Application Code</title><categories>cs.NI cs.PF</categories><comments>Published in: A. F\&quot;orster, C. Minkenberg, G. R. Herrera, M. Kirsche
  (Eds.), Proc. of the 2nd OMNeT++ Community Summit, IBM Research - Zurich,
  Switzerland, September 3-4, 2015, arXiv:1509.03284, 2015</comments><report-no>OMNET/2015/01</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  After a decade of research in the field of wireless sensor networks (WSNs)
there are still open issues. WSNs impose several severe requirements regarding
energy consumption, processing capabilities, mobility, and robustness of
wireless transmissions. Simulation has shown to be the most cost-efficient
approach for evaluation of WSNs, thus a number of simulators are available.
Unfortunately, these simulation environments typically consider WSNs from a
special point of view. In this work we present the integration of three such
specialized frameworks, namely MiXiM, PAWiS, and STEAM-Sim. This integration
combines the strengths of the single frameworks such as realistic channel
models, mobility patterns, accurate energy models, and inclusion of real-life
application code. The result is a new simulation environment which enables a
more general consideration of WSNs. We implemented and verified our proposed
concept by means of static and mobile scenarios. As the presented results show,
the combined framework gives the same results regarding the functionality and
energy consumption as our &quot;golden model&quot;. Therefore the system integration was
successful and the framework is ready to be used by the community.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03550</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03550</id><created>2015-09-11</created><authors><author><keyname>Vesely</keyname><forenames>Vladimir</forenames></author><author><keyname>Marek</keyname><forenames>Marcel</forenames></author><author><keyname>Hykel</keyname><forenames>Tomas</forenames></author><author><keyname>Rysavy</keyname><forenames>Ondrej</forenames></author></authors><title>Skip This Paper - RINASim: Your Recursive InterNetwork Architecture
  Simulator</title><categories>cs.NI cs.PF</categories><comments>Published in: A. F\&quot;orster, C. Minkenberg, G. R. Herrera, M. Kirsche
  (Eds.), Proc. of the 2nd OMNeT++ Community Summit, IBM Research - Zurich,
  Switzerland, September 3-4, 2015, arXiv:1509.03284, 2015</comments><report-no>OMNET/2015/13</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recursive InterNetwork Architecture is a clean-slate approach to how to deal
with the current issues of the Internet based on the traditional TCP/IP
networking stack. Instead of using a fixed number of layers with dedicated
functionality, RINA proposes a single generic layer with programmable
functionality that may be recursively stacked. We introduce a brand new
framework for modeling and simulation of RINA that is intended for OMNeT++.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03553</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03553</id><created>2015-09-11</created><authors><author><keyname>Lebreton</keyname><forenames>Jean</forenames></author><author><keyname>Murad</keyname><forenames>Nour</forenames></author></authors><title>Implementation of a Wake-up Radio Cross-Layer Protocol in OMNeT++ /
  MiXiM</title><categories>cs.NI cs.PF</categories><comments>Published in: A. F\&quot;orster, C. Minkenberg, G. R. Herrera, M. Kirsche
  (Eds.), Proc. of the 2nd OMNeT++ Community Summit, IBM Research - Zurich,
  Switzerland, September 3-4, 2015, arXiv:1509.03284, 2015</comments><report-no>OMNET/2015/14</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the DoRa protocol, which is a new cross-layer protocol
for handling the double radio of nodes in wake-up radio scenario. The
implementation details in OMNET++/MiXiM are also given, with a focus on the
implemented MAC layers. The main goal of the DoRa protocol is to reduce energy
consumption in wireless sensor network, by taking full advantage of the passive
wake-up scheme. The performance of the DoRa protocol is then evaluated and
results are compared with B-MAC and IEEE 802.15.4 protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03556</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03556</id><created>2015-09-11</created><authors><author><keyname>Fangohr</keyname><forenames>Hans</forenames></author><author><keyname>O'Brien</keyname><forenames>Neil</forenames></author><author><keyname>Prabhakar</keyname><forenames>Anil</forenames></author><author><keyname>Kashyap</keyname><forenames>Arti</forenames></author></authors><title>Teaching Python programming with automatic assessment and feedback
  provision</title><categories>cs.CY</categories><comments>26 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a method of automatic feedback provision for students learning
programming and computational methods in Python. We have implemented, used and
refined this system since 2009 for growing student numbers, and summarise the
design and experience of using it. The core idea is to use a unit testing
framework: the teacher creates a set of unit tests, and the student code is
tested by running these tests. With our implementation, students typically
submit work for assessment, and receive feedback by email within a few minutes
after submission. The choice of tests and the reporting back to the student is
chosen to optimise the educational value for the students. The system very
significantly reduces the staff time required to establish whether a student's
solution is correct, and shifts the emphasis of computing laboratory student
contact time from assessing correctness to providing guidance. The self-paced
nature of the automatic feedback provision supports a student-centred learning
approach. Students can re-submit their work repeatedly and iteratively improve
their solution, and enjoy using the system. We include an evaluation of the
system and data from using it in a class of 425 students.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03557</identifier>
 <datestamp>2016-01-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03557</id><created>2015-07-17</created><updated>2016-01-05</updated><authors><author><keyname>Uecker</keyname><forenames>Martin</forenames></author><author><keyname>Lustig</keyname><forenames>Michael</forenames></author></authors><title>Estimating Absolute-Phase Maps Using ESPIRiT and Virtual Conjugate Coils</title><categories>cs.CV cs.CE physics.med-ph</categories><comments>15 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Purpose: To develop an ESPIRiT-based method to estimate coil sensitivities
with image phase as a building block for efficient and robust image
reconstruction with phase constraints. Theory and Methods: ESPIRiT is a new
framework for calibration of the coil sensitivities and reconstruction in
parallel Magnetic Resonance Imaging (MRI). Applying ESPIRiT to a combined set
of physical and virtual conjugate coils (VCC-ESPIRiT) implicitly exploits
conjugate symmetry in k-space similar to VCC-GRAPPA. Based on this method, a
new post-processing step is proposed for the explicit computation of coil
sensitivities that include the absolute phase of the image. The accuracy of the
computed maps is directly validated using a test based on projection onto fully
sampled coil images and also indirectly in phase-constrained parallel-imaging
reconstructions. Results: The proposed method can estimate accurate
sensitivities which include low-resolution image phase. In case of
high-frequency phase variations VCC-ESPIRiT yields an additional set of maps
that indicates the existence of a high-frequency phase component. Taking this
additional set of maps into account can improve the robustness of
phase-constrained parallel imaging. Conclusion: The extended VCC-ESPIRiT is a
useful tool for phase-constrained imaging.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03558</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03558</id><created>2015-09-11</created><authors><author><keyname>B&#xf6;hm</keyname><forenames>Sebastian</forenames></author><author><keyname>Kirsche</keyname><forenames>Michael</forenames></author></authors><title>Looking into Hardware-in-the-Loop Coupling of OMNeT++ and RoSeNet</title><categories>cs.NI cs.PF</categories><comments>Published in: A. F\&quot;orster, C. Minkenberg, G. R. Herrera, M. Kirsche
  (Eds.), Proc. of the 2nd OMNeT++ Community Summit, IBM Research - Zurich,
  Switzerland, September 3-4, 2015, arXiv:1509.03284, 2015</comments><report-no>OMNET/2015/15</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network emulation using real sensor node hardware is used to increase the
accuracy of pure network simulations. Coupling OMNeT++ with network emulation
platforms and tools introduces new application possibilities for both sides.
This work-in-progress report covers our experiences of using OMNeT++ as a test
driver for RoSeNet, a network emulation and test platform for low-power
wireless technologies like IEEE 802.15.4. OMNeT++ and RoSeNet were
interconnected to enable a co-simulation of real sensor networks with a MAC
layer simulation model. Experiences and insights on this Hardware-in-the-Loop
(HIL) simulation together with ideas to extend OMNeT++ and to provide a generic
interconnection API complete the report.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03559</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03559</id><created>2015-09-11</created><authors><author><keyname>Liu</keyname><forenames>Qian</forenames></author><author><keyname>Russell</keyname><forenames>Robert D.</forenames></author><author><keyname>Mizero</keyname><forenames>Fabrice</forenames></author><author><keyname>Veeraraghavan</keyname><forenames>Malathi</forenames></author><author><keyname>Dennis</keyname><forenames>John</forenames></author><author><keyname>Jamroz</keyname><forenames>Benjamin</forenames></author></authors><title>Implementation of PFC and RCM for RoCEv2 Simulation in OMNeT++</title><categories>cs.NI cs.PF</categories><comments>Published in: A. F\&quot;orster, C. Minkenberg, G. R. Herrera, M. Kirsche
  (Eds.), Proc. of the 2nd OMNeT++ Community Summit, IBM Research - Zurich,
  Switzerland, September 3-4, 2015, arXiv:1509.03284, 2015</comments><report-no>OMNET/2015/16</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As traffic patterns and network topologies become more and more complicated
in current enterprise data centers and TOP500 supercomputers, the probability
of network congestion increases. If no countermeasures are taken, network
congestion causes long communication delays and degrades network performance. A
congestion control mechanism is often provided to reduce the consequences of
congestion. However, it is usually difficult to configure and activate a
congestion control mechanism in production clusters and supercomputers due to
concerns that it may negatively impact jobs if the mechanism is not
appropriately configured. Therefore, simulations for these situations are
necessary to identify congestion points and sources, and more importantly, to
determine optimal settings that can be utilized to reduce congestion in those
complicated networks. In this paper, we use OMNeT++ to implement the IEEE
802.1Qbb Priority-based Flow Control (PFC) and RoCEv2 Congestion Management
(RCM) in order to simulate clusters with RoCEv2 interconnects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03561</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03561</id><created>2015-09-11</created><authors><author><keyname>Riebl</keyname><forenames>Raphael</forenames></author><author><keyname>Facchi</keyname><forenames>Christian</forenames></author></authors><title>Regain Control of Growing Dependencies in OMNeT++ Simulations</title><categories>cs.SE</categories><comments>Published in: A. F\&quot;orster, C. Minkenberg, G. R. Herrera, M. Kirsche
  (Eds.), Proc. of the 2nd OMNeT++ Community Summit, IBM Research - Zurich,
  Switzerland, September 3-4, 2015, arXiv:1509.03284, 2015</comments><report-no>OMNET/2015/17</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When designing simulation models, it is favourable to reuse existing models
as far as possible to reduce the effort from the first idea to simulation
results. Thanks to the OMNeT++ community, there are several toolboxes available
covering a wide range of network communication protocols. However, it can be
quite a daunting task to handle the build process when multiple existing
simulation models need to be combined with custom sources. Project references
provided by the OMNeT++ Integrated Development Environment (IDE) are just
partly up to the task because it can be barely automated. For this reason, a
new approach is presented to build complex simulation models with the help of
CMake, which is a wide-spread build tool for C and C++ projects. The resulting
toolchain allows to handle dependencies conveniently without need for any
changes in upstream projects and also takes special care of OMNeT++-specific
aspects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03562</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03562</id><created>2015-09-11</created><authors><author><keyname>Virdis</keyname><forenames>Antonio</forenames></author></authors><title>Optimization in the Loop: Implementing and Testing Scheduling Algorithms
  with SimuLTE</title><categories>cs.NI</categories><comments>Published in: A. F\&quot;orster, C. Minkenberg, G. R. Herrera, M. Kirsche
  (Eds.), Proc. of the 2nd OMNeT++ Community Summit, IBM Research - Zurich,
  Switzerland, September 3-4, 2015, arXiv:1509.03284, 2015</comments><report-no>OMNET/2015/18</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the main purposes of discrete event simulators such as OMNeT++ is to
test new algorithms or protocols in realistic environments. These often need to
be benchmarked against optimal/theoretical results obtained by running
commercial optimization solvers. The usual way to do this is to have the
simulator run in a standalone mode and generate (few) snapshots, which are then
fed to the optimization solvers. This allows one to compare the optimal and
suboptimal solutions in the snapshots, but does not allow to assess how the
system being studied would evolve over time if the optimal solution was
enforced every time. This requires optimization software to run directly in the
loop of the simulation, exchanging information with the latter. The goal of
this tutorial is to show how to integrate a commercial solver (CPLEX) into the
simulation loop of the OMNeT++ environment. For this purpose, we propose two
methods: a first one that uses a solver as an external program, and a second
one that exploits a C-written API for CPLEX known as Callable Library. We then
exemplify how to apply these two methods to SimuLTE, a simulation model for LTE
cellular networks, implementing and testing a simple solution to a well-known
resource-scheduling problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03564</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03564</id><created>2015-09-11</created><authors><author><keyname>Pfeffer</keyname><forenames>Avi</forenames></author><author><keyname>Ruttenberg</keyname><forenames>Brian</forenames></author><author><keyname>Sliva</keyname><forenames>Amy</forenames></author><author><keyname>Howard</keyname><forenames>Michael</forenames></author><author><keyname>Takata</keyname><forenames>Glenn</forenames></author></authors><title>Lazy Factored Inference for Functional Probabilistic Programming</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Probabilistic programming provides the means to represent and reason about
complex probabilistic models using programming language constructs. Even simple
probabilistic programs can produce models with infinitely many variables.
Factored inference algorithms are widely used for probabilistic graphical
models, but cannot be applied to these programs because all the variables and
factors have to be enumerated. In this paper, we present a new inference
framework, lazy factored inference (LFI), that enables factored algorithms to
be used for models with infinitely many variables. LFI expands the model to a
bounded depth and uses the structure of the program to precisely quantify the
effect of the unexpanded part of the model, producing lower and upper bounds to
the probability of the query.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03565</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03565</id><created>2015-09-11</created><authors><author><keyname>Zhao</keyname><forenames>Zhongliang</forenames></author><author><keyname>Rosario</keyname><forenames>Denis</forenames></author><author><keyname>Braun</keyname><forenames>Torsten</forenames></author><author><keyname>Cerqueira</keyname><forenames>Eduardo</forenames></author></authors><title>A Tutorial of the Mobile Multimedia Wireless Sensor Network OMNeT++
  Framework</title><categories>cs.NI cs.MM cs.PF</categories><comments>Published in: A. F\&quot;orster, C. Minkenberg, G. R. Herrera, M. Kirsche
  (Eds.), Proc. of the 2nd OMNeT++ Community Summit, IBM Research - Zurich,
  Switzerland, September 3-4, 2015, arXiv:1509.03284, 2015</comments><report-no>OMNET/2015/19</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we will give a detailed tutorial instruction about how to use
the Mobile Multi-Media Wireless Sensor Networks (M3WSN) simulation framework.
The M3WSN framework has been published as a scientific paper in the 6th
International Workshop on OMNeT++ (2013). M3WSN framework enables the
multimedia transmission of real video sequence. Therefore, a set of multimedia
algorithms, protocols, and services can be evaluated by using QoE metrics.
Moreover, key video-related information, such as frame types, GoP length and
intra-frame dependency can be used for creating new assessment and optimization
solutions. To support mobility, M3WSN utilizes different mobility traces to
enable the understanding of how the network behaves under mobile situations.
This tutorial will cover how to install and configure the M3WSN framework,
setting and running the experiments, creating mobility and video traces, and
how to evaluate the performance of different protocols. The tutorial will be
given in an environment of Ubuntu 12.04 LTS and OMNeT++ 4.2.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03570</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03570</id><created>2015-09-11</created><authors><author><keyname>Braun</keyname><forenames>Torsten</forenames></author><author><keyname>Hurni</keyname><forenames>Philipp</forenames></author><author><keyname>Bernardo</keyname><forenames>Vitor</forenames></author><author><keyname>Curado</keyname><forenames>Marilia</forenames></author></authors><title>Invited Abstract: Issues with State-based Energy Consumption Modelling</title><categories>cs.NI</categories><comments>Published in: A. F\&quot;orster, C. Minkenberg, G. R. Herrera, M. Kirsche
  (Eds.), Proc. of the 2nd OMNeT++ Community Summit, IBM Research - Zurich,
  Switzerland, September 3-4, 2015, arXiv:1509.03284, 2015</comments><report-no>OMNET/2015/21</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy consumption modelling by state based approaches often assume constant
energy consumption values in each state. However, it happens in certain
situations that during state transitions or even during a state the energy
consumption is not constant and does fluctuate. This paper discusses those
issues by presenting some examples from wireless sensor and wireless local area
networks for such cases and possible solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03573</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03573</id><created>2015-09-11</created><authors><author><keyname>Safavi</keyname><forenames>Mohammadhassan</forenames></author><author><keyname>Bastani</keyname><forenames>Saeed</forenames></author></authors><title>Invited Abstract: A Simulation Package for Energy Consumption of Content
  Delivery Networks (CDNs)</title><categories>cs.PF cs.NI</categories><comments>Published in: A. F\&quot;orster, C. Minkenberg, G. R. Herrera, M. Kirsche
  (Eds.), Proc. of the 2nd OMNeT++ Community Summit, IBM Research - Zurich,
  Switzerland, September 3-4, 2015, arXiv:1509.03284, 2015</comments><report-no>OMNET/2015/22</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Content Delivery Networks (CDNs) are becoming an integral part of the future
generation Internet. Traditionally, these networks have been designed with the
goals of traffic offload and the improvement of users' quality of experience
(QoE), but the energy consumption is also becoming an indispensable design
factor for CDNs to be a sustainable solution. To study and improve the CDN
architectures using this new design metric, we are planning to develop a
generic and flexible simulation package in OMNet++. This package is aimed to
render a holistic view about the CDN energy consumption behaviour by
incorporating the state-of-the-art energy consumption models proposed for the
individual elements of CDNs (e.g. servers, routers, wired and wireless links,
wireless devices, etc.) and for the various Internet contents (web pages,
files, streaming video, etc.).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03574</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03574</id><created>2015-09-11</created><updated>2015-09-18</updated><authors><author><keyname>Abdo</keyname><forenames>Hosam</forenames></author><author><keyname>Dimitrov</keyname><forenames>Darko</forenames></author><author><keyname>Gutman</keyname><forenames>Ivan</forenames></author></authors><title>On extremal trees with respect to the $F$-index</title><categories>cs.DM</categories><comments>13 pages, 2 Figures, 2 Tabels</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a study on the structure--dependency of the total $\pi$-electron energy
from 1972, Trinajsti\'c and one of the present authors have shown that it
depends on the sums $\sum_{v\in V}d(v)^2$ and $\sum_{v\in V}d(v)^3$, where
$d(v)$ is the degree of a vertex $v$ of the underling molecular graph $G$. The
first sum was later named {\it first Zagreb index} and over the years became
one of the most investigated graph--based molecular structure descriptors. On
the other hand, the second sum, except in very few works on the general first
Zagreb index and the zeroth--order general Randi\'c index, has been almost
completely neglected. Recently, this second sum was named {\em forgotten
index}, or shortly the $F$-{\em index}, and shown to have an exceptional
applicative potential. In this paper we examine the trees extremal with respect
to the $F$-index.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03575</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03575</id><created>2015-09-11</created><authors><author><keyname>Kiran</keyname><forenames>Ananda</forenames></author><author><keyname>Prashar</keyname><forenames>Navdeep</forenames></author></authors><title>FPGA Implementation of High Speed Baugh-Wooley Multiplier using
  Decomposition Logic</title><categories>cs.AR</categories><comments>6 pages, 3 figures, 2 tables, 3 equations</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The Baugh-Wooley algorithm is a well-known iterative algorithm for performing
multiplication in digital signal processing applications. Decomposition logic
is used with Baugh-Wooley algorithm to enhance the speed and to reduce the
critical path delay. In this paper a high speed multiplier is designed and
implemented using decomposition logic and Baugh-Wooley algorithm. The result is
compared with booth multiplier. FPGA based architecture is presented and design
has been implemented using Xilinx 12.3 device.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03585</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03585</id><created>2015-09-11</created><authors><author><keyname>Pu</keyname><forenames>Fuan</forenames></author><author><keyname>Luo</keyname><forenames>Jian</forenames></author><author><keyname>Luo</keyname><forenames>Guiming</forenames></author></authors><title>Some Supplementaries to The Counting Semantics for Abstract
  Argumentation</title><categories>cs.AI</categories><comments>8 pages, 3 figures, ICTAI 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dung's abstract argumentation framework consists of a set of interacting
arguments and a series of semantics for evaluating them. Those semantics
partition the powerset of the set of arguments into two classes: extensions and
non-extensions. In order to reason with a specific semantics, one needs to take
a credulous or skeptical approach, i.e. an argument is eventually accepted, if
it is accepted in one or all extensions, respectively. In our previous work
\cite{ref-pu2015counting}, we have proposed a novel semantics, called
\emph{counting semantics}, which allows for a more fine-grained assessment to
arguments by counting the number of their respective attackers and defenders
based on argument graph and argument game. In this paper, we continue our
previous work by presenting some supplementaries about how to choose the
damaging factor for the counting semantics, and what relationships with some
existing approaches, such as Dung's classical semantics, generic gradual
valuations. Lastly, an axiomatic perspective on the ranking semantics induced
by our counting semantics are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03590</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03590</id><created>2015-09-11</created><authors><author><keyname>Lera</keyname><forenames>Daniela</forenames></author><author><keyname>Sergeyev</keyname><forenames>Yaroslav D.</forenames></author></authors><title>Deterministic global optimization using space-filling curves and
  multiple estimates of Lipschitz and Holder constants</title><categories>math.OC cs.NA math.NA</categories><comments>26 pages, 10 figures, 4 tables</comments><msc-class>90C26, 90C56, 65K05, 28A80</msc-class><acm-class>G.1.6</acm-class><journal-ref>Communications in Nonlinear Science and Numerical Simulation,
  Volume 23, Issues 1-3, 2015, Pages 328-342</journal-ref><doi>10.1016/j.cnsns.2014.11.015</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the global optimization problem $\min_{y\in S} F(y)$ with $S$
being a hyperinterval in $\Re^N$ and $F(y)$ satisfying the Lipschitz condition
with an unknown Lipschitz constant is considered. It is supposed that the
function $F(y)$ can be multiextremal, non-differentiable, and given as a
`black-box'. To attack the problem, a new global optimization algorithm based
on the following two ideas is proposed and studied both theoretically and
numerically. First, the new algorithm uses numerical approximations to
space-filling curves to reduce the original Lipschitz multi-dimensional problem
to a univariate one satisfying the H\&quot;{o}lder condition. Second, the algorithm
at each iteration applies a new geometric technique working with a number of
possible H\&quot;{o}lder constants chosen from a set of values varying from zero to
infinity showing so that ideas introduced in a popular DIRECT method can be
used in the H\&quot;{o}lder global optimization. Convergence conditions of the
resulting deterministic global optimization method are established. Numerical
experiments carried out on several hundreds of test functions show quite a
promising performance of the new algorithm in comparison with its direct
competitors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03591</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03591</id><created>2015-09-11</created><authors><author><keyname>Dugan</keyname><forenames>Peter</forenames></author><author><keyname>Zollweg</keyname><forenames>John</forenames></author><author><keyname>Popescu</keyname><forenames>Marian</forenames></author><author><keyname>Risch</keyname><forenames>Denise</forenames></author><author><keyname>Glotin</keyname><forenames>Herve</forenames></author><author><keyname>LeCun</keyname><forenames>Yann</forenames></author><author><keyname>Clark</keyname><forenames>and Christopher</forenames></author></authors><title>High Performance Computer Acoustic Data Accelerator: A New System for
  Exploring Marine Mammal Acoustics for Big Data Applications</title><categories>cs.DC</categories><comments>Seven pages, submitted at International Conference on Machine
  Learning 2014, Workshop uLearnBio, unsupervised learning for bioacoustic
  applications</comments><msc-class>68-04</msc-class><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  This paper presents a new software model designed for distributed sonic
signal detection runtime using machine learning algorithms called DeLMA. A new
algorithm--Acoustic Data-mining Accelerator (ADA)--is also presented. ADA is a
robust yet scalable solution for efficiently processing big sound archives
using distributing computing technologies. Together, DeLMA and the ADA
algorithm provide a powerful tool currently being used by the Bioacoustics
Research Program (BRP) at the Cornell Lab of Ornithology, Cornell University.
This paper provides a high level technical overview of the system, and
discusses various aspects of the design. Basic runtime performance and project
summary are presented. The DeLMA-ADA baseline performance comparing desktop
serial configuration to a 64 core distributed HPC system shows as much as a 44
times faster increase in runtime execution. Performance tests using 48 cores on
the HPC shows a 9x to 12x efficiency over a 4 core desktop solution. Project
summary results for 19 east coast deployments show that the DeLMA-ADA solution
has processed over three million channel hours of sound to date.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03597</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03597</id><created>2015-09-11</created><authors><author><keyname>Abraham</keyname><forenames>Mathew P.</forenames></author><author><keyname>Kulkarni</keyname><forenames>Ankur A.</forenames></author></authors><title>New Results on the Existence of Open Loop Nash Equilibria in Discrete
  Time Dynamic Games</title><categories>math.OC cs.GT cs.SY</categories><comments>12 pages, under review with the IEEE Transactions on Automatic
  Control</comments><msc-class>91A50, 91A25, 90C30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the problem of finding conditions which guarantee the existence of
open-loop Nash equilibria in discrete time dynamic games (DTDGs). The classical
approach to DTDGs involves analyzing the problem using optimal control theory
which yields results mainly limited to linear-quadratic games. We show the
existence of equilibria for a class of DTDGs where the cost function of players
admits a quasi-potential function which leads to new results and, in some
cases, a generalization of similar results from linear-quadratic games. Our
results are obtained by introducing a new formulation for analysing DTDGs using
the concept of a conjectured state by the players. In this formulation, the
state of the game is modelled as dependent on players. Using this formulation
we show that there is an optimisation problem such that the solution of this
problem gives an equilibrium of the DTDG.
  To extend the result for more general games, we modify the DTDG with an
additional constraint of consistency of the conjectured state. Any equilibrium
of the original game is also an equilibrium of this modified game with
consistent conjectures.
  In the modified game, we show the existence of equilibria for DTDGs where the
cost function of players admits a potential function. We end with conditions
under which an equilibrium of the game with consistent conjectures is an
$\epsilon$-Nash equilibria of the original game.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03600</identifier>
 <datestamp>2015-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03600</id><created>2015-09-11</created><updated>2015-09-22</updated><authors><author><keyname>Kale</keyname><forenames>Satyen</forenames></author><author><keyname>Lee</keyname><forenames>Chansoo</forenames></author><author><keyname>P&#xe1;l</keyname><forenames>D&#xe1;vid</forenames></author></authors><title>Hardness of Online Sleeping Combinatorial Optimization Problems</title><categories>cs.LG cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that several online combinatorial optimization problems that admit
efficient no-regret algorithms become computationally hard in the sleeping
setting where a subset of actions becomes unavailable in each round.
Specifically, we show that the sleeping versions of these problems, using
per-action regret as the performance measure, are at least as hard as PAC
learning DNF expressions, a long standing open problem. We show hardness for
the sleeping versions of Online Shortest Paths, Online Minimum Spanning Tree,
Online $k$-Subsets, Online $k$-Truncated Permutations, Online Minimum Cut, and
Online Bipartite Matching. The hardness result for the sleeping version of the
Online Shortest Paths problem resolves an open problem presented at COLT 2015
(Koolen et al., 2015). We also give an efficient reduction of the task of
minimizing per-action regret to the task of minimizing ranking regret, a
different performance measure. Thus, existing efficient algorithms for
minimizing ranking regret under various restrictions of the adversary can be
used to efficiently minimize per-action regret as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03602</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03602</id><created>2015-09-11</created><authors><author><keyname>Basu</keyname><forenames>Saikat</forenames></author><author><keyname>Ganguly</keyname><forenames>Sangram</forenames></author><author><keyname>Mukhopadhyay</keyname><forenames>Supratik</forenames></author><author><keyname>DiBiano</keyname><forenames>Robert</forenames></author><author><keyname>Karki</keyname><forenames>Manohar</forenames></author><author><keyname>Nemani</keyname><forenames>Ramakrishna</forenames></author></authors><title>DeepSat - A Learning framework for Satellite Imagery</title><categories>cs.CV</categories><comments>Paper was accepted at ACM SIGSPATIAL 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Satellite image classification is a challenging problem that lies at the
crossroads of remote sensing, computer vision, and machine learning. Due to the
high variability inherent in satellite data, most of the current object
classification approaches are not suitable for handling satellite datasets. The
progress of satellite image analytics has also been inhibited by the lack of a
single labeled high-resolution dataset with multiple class labels. The
contributions of this paper are twofold - (1) first, we present two new
satellite datasets called SAT-4 and SAT-6, and (2) then, we propose a
classification framework that extracts features from an input image, normalizes
them and feeds the normalized feature vectors to a Deep Belief Network for
classification. On the SAT-4 dataset, our best network produces a
classification accuracy of 97.95% and outperforms three state-of-the-art object
recognition algorithms, namely - Deep Belief Networks, Convolutional Neural
Networks and Stacked Denoising Autoencoders by ~11%. On SAT-6, it produces a
classification accuracy of 93.9% and outperforms the other algorithms by ~15%.
Comparative studies with a Random Forest classifier show the advantage of an
unsupervised learning approach over traditional supervised learning techniques.
A statistical analysis based on Distribution Separability Criterion and
Intrinsic Dimensionality Estimation substantiates the effectiveness of our
approach in learning better representations for satellite imagery.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03603</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03603</id><created>2015-09-11</created><authors><author><keyname>Sun</keyname><forenames>Xiang</forenames></author><author><keyname>Ansari</keyname><forenames>Nirwan</forenames></author><author><keyname>Fan</keyname><forenames>Qiang</forenames></author></authors><title>Green Energy Aware Avatar Migration Strategy in Green Cloudlet Networks</title><categories>cs.DC cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a Green Cloudlet Network (\emph{GCN}) architecture to provide
seamless Mobile Cloud Computing (\emph{MCC}) services to User Equipments
(\emph{UE}s) with low latency in which each cloudlet is powered by both green
and brown energy. Fully utilizing green energy can significantly reduce the
operational cost of cloudlet providers. However, owing to the spatial dynamics
of energy demand and green energy generation, the energy gap among different
cloudlets in the network is unbalanced, i.e., some cloudlets' energy demands
can be fully provided by green energy but others need to utilize on-grid energy
(i.e., brown energy) to satisfy their energy demands. We propose a Green-energy
awarE Avatar migRation (\emph{GEAR}) strategy to minimize the on-grid energy
consumption in GCN by redistributing the energy demands via Avatar migration
among cloudlets according to cloudlets' green energy generation. Furthermore,
GEAR ensures the Service Level Agreement (\emph{SLA}) in terms of the maximum
Avatar propagation delay by avoiding Avatars hosted in the remote cloudlets. We
formulate the GEAR strategy as a mixed integer linear programming problem,
which is NP-hard, and thus apply the Branch and Bound search to find its
sub-optimal solution. Simulation results demonstrate that GEAR can save on-grid
energy consumption significantly as compared to the Follow me AvataR
(\emph{FAR}) migration strategy, which aims to minimize the propagation delay
between an UE and its Avatar.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03604</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03604</id><created>2015-09-11</created><authors><author><keyname>Huff</keyname><forenames>Kathryn D.</forenames></author><author><keyname>Gidden</keyname><forenames>Matthew J.</forenames></author><author><keyname>Carlsen</keyname><forenames>Robert W.</forenames></author><author><keyname>Flanagan</keyname><forenames>Robert R.</forenames></author><author><keyname>McGarry</keyname><forenames>Meghan B.</forenames></author><author><keyname>Opotowsky</keyname><forenames>Arrielle C.</forenames></author><author><keyname>Schneider</keyname><forenames>Erich A.</forenames></author><author><keyname>Scopatz</keyname><forenames>Anthony M.</forenames></author><author><keyname>Wilson</keyname><forenames>Paul P. H.</forenames></author></authors><title>Fundamental Concepts in the Cyclus Fuel Cycle Simulator Framework</title><categories>cs.SE cs.CE cs.MA cs.MS</categories><acm-class>D.2.13; I.6.7; I.6.8; D.2.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As nuclear power expands, technical, economic, political, and environmental
analyses of nuclear fuel cycles by simulators increase in importance. To date,
however, current tools are often fleet-based rather than discrete and privately
distributed rather than open source. Each of these choices presents a challenge
to modeling fidelity, generality, efficiency, robustness, and scientific
transparency. The Cyclus nuclear fuel cycle simulator framework and its
modeling ecosystem incorporate modern insights from simulation science and
software architecture to solve these problems so that challenges in nuclear
fuel cycle analysis can be better addressed. A summary of the Cyclus fuel cycle
simulator framework and its modeling ecosystem are presented. Additionally, the
implementation of each is discussed in the context of motivating challenges in
nuclear fuel cycle simulation. Finally, the current capabilities of Cyclus are
demonstrated for both open and closed fuel cycles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03611</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03611</id><created>2015-09-11</created><updated>2016-03-06</updated><authors><author><keyname>Rabinovich</keyname><forenames>Ella</forenames></author><author><keyname>Wintner</keyname><forenames>Shuly</forenames></author><author><keyname>Lewinsohn</keyname><forenames>Ofek Luis</forenames></author></authors><title>A Parallel Corpus of Translationese</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a set of bilingual English--French and English--German parallel
corpora in which the direction of translation is accurately and reliably
annotated. The corpora are diverse, consisting of parliamentary proceedings,
literary works, transcriptions of TED talks and political commentary. They will
be instrumental for research of translationese and its applications to (human
and machine) translation; specifically, they can be used for the task of
translationese identification, a research direction that enjoys a growing
interest in recent years. To validate the quality and reliability of the
corpora, we replicated previous results of supervised and unsupervised
identification of translationese, and further extended the experiments to
additional datasets and languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03614</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03614</id><created>2015-09-11</created><authors><author><keyname>Saur</keyname><forenames>Karla</forenames></author><author><keyname>Collard</keyname><forenames>Joseph</forenames></author><author><keyname>Foster</keyname><forenames>Nate</forenames></author><author><keyname>Guha</keyname><forenames>Arjun</forenames></author><author><keyname>Vanbever</keyname><forenames>Laurent</forenames></author><author><keyname>Hicks</keyname><forenames>Michael</forenames></author></authors><title>Morpheus: Safe and Flexible Dynamic Updates for SDNs</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  SDN controllers must be periodically modified to add features, improve
performance, and fix bugs, but current techniques for implementing dynamic
updates are inadequate. Simply halting old controllers and bringing up new ones
can cause state to be lost, which often leads to incorrect behavior-e.g., if
the state represents hosts blacklisted by a firewall, then traffic that should
be blocked may be allowed to pass through. Techniques based on record and
replay can reconstruct state automatically, but they are expensive to deploy
and can lead to incorrect behavior. Problematic scenarios are especially likely
to arise in distributed controllers and with semantics-altering updates.
  This paper presents a new approach to implementing dynamic controller updates
based on explicit state transfer. Instead of attempting to infer state changes
automatically-an approach that is expensive and fundamentally incomplete-our
framework gives programmers effective tools for implementing correct updates
that avoid major disruptions. We develop primitives that enable programmers to
directly (and easily, in most cases) initialize the new controller's state as a
function of old state and we design protocols that ensure consistent behavior
during the transition. We also present a prototype implementation called
Morpheus, and evaluate its effectiveness on representative case studies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03619</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03619</id><created>2015-09-11</created><authors><author><keyname>Goldfeld</keyname><forenames>Ziv</forenames></author><author><keyname>Cuff</keyname><forenames>Paul</forenames></author><author><keyname>Permuter</keyname><forenames>Haim H.</forenames></author></authors><title>Semantic-Security Capacity for Wiretap Channels of Type II</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The secrecy capacity of the type II wiretap channel (WTC II) with a noisy
main channel is currently an open problem. Herein its secrecy-capacity is
derived and shown to be equal to its semantic-security (SS) capacity. In this
setting, the legitimate users communicate via a discrete-memoryless (DM)
channel in the presence of an eavesdropper that has perfect access to a subset
of its choosing of the transmitted symbols, constrained to a fixed fraction of
the blocklength. The secrecy criterion is achieved simultaneously for all
possible eavesdropper subset choices. The SS criterion demands negligible
mutual information between the message and the eavesdropper's observations even
when maximized over all message distributions.
  A key tool for the achievability proof is a novel and stronger version of
Wyner's soft covering lemma. Specifically, a random codebook is shown to
achieve the soft-covering phenomenon with high probability. The probability of
failure is doubly-exponentially small in the blocklength. Since the combined
number of messages and subsets grows only exponentially with the blocklength,
SS for the WTC II is established by using the union bound and invoking the
stronger soft-covering lemma. The direct proof shows that rates up to the
weak-secrecy capacity of the classic WTC with a DM erasure channel (EC) to the
eavesdropper are achievable. The converse follows by establishing the capacity
of this DM wiretap EC as an upper bound for the WTC II. From a broader
perspective, the stronger soft-covering lemma constitutes a tool for showing
the existence of codebooks that satisfy exponentially many constraints, a
beneficial ability for many other applications in information theoretic
security.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03625</identifier>
 <datestamp>2015-09-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03625</id><created>2015-09-11</created><authors><author><keyname>Dorsch</keyname><forenames>Dominik</forenames></author><author><keyname>Rauhut</keyname><forenames>Holger</forenames></author></authors><title>Refined analysis of sparse MIMO radar</title><categories>cs.IT math.IT math.PR</categories><comments>31 pages</comments><msc-class>94A20, 94A12, 60B20, 90C25, 65F22</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze a multiple-input multiple-output (MIMO) radar model and provide
recovery results for a compressed sensing (CS) approach. In MIMO radar
different pulses are emitted by several transmitters and the echoes are
recorded at several receiver nodes. Under reasonable assumptions the
transformation from emitted pulses to the received echoes can approximately be
regarded as linear. For the considered model, and many radar tasks in general,
sparsity of targets within the considered angle-range-Doppler domain is a
natural assumption. Therefore, it is possible to apply methods from CS in order
to reconstruct the parameters of the targets. Assuming Gaussian random pulses
the resulting measurement matrix becomes a highly structured random matrix. Our
first main result provides an estimate for the well-known restricted isometry
property (RIP) ensuring stable and robust recovery. We require more
measurements than standard results from CS, like for example those for Gaussian
random measurements. Nevertheless, we show that due to the special structure of
the considered measurement matrix our RIP result is in fact optimal (up to
possibly logarithmic factors). Our further two main results on nonuniform
recovery (i.e., for a fixed sparse target scene) reveal how the fine structure
of the support set affects the (nonuniform) recovery performance. We show that
for certain &quot;balanced&quot; support sets reconstruction with essentially the optimal
number of measurements is possible. We prove recovery results for both perfect
recovery of the support set in case of exactly sparse vectors and an
$\ell_2$-norm approximation result for reconstruction under sparsity defect.Our
analysis complements earlier work by Strohmer &amp; Friedlander and deepens the
understanding of the considered MIMO radar model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03646</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03646</id><created>2015-09-11</created><authors><author><keyname>To</keyname><forenames>Quoc-Cuong</forenames></author><author><keyname>Nguyen</keyname><forenames>Benjamin</forenames></author><author><keyname>Pucheral</keyname><forenames>Philippe</forenames></author></authors><title>Key Exchange Protocol in the Trusted Data Servers Context</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this technical report is to complement the work in [To et al.
2014] by proposing a Group Key Exchange protocol so that the Querier and TDSs
(and TDSs themselves) can securely create and exchange the shared key. Then,
the security of this protocol is formally proved using the game-based model.
Finally, we perform the comparison between this protocol and other related
works.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03650</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03650</id><created>2015-09-11</created><authors><author><keyname>Wasly</keyname><forenames>Hussain</forenames></author><author><keyname>AlSoufi</keyname><forenames>Ali</forenames></author></authors><title>Impact of e-Government Services on Private Sector: An Empirical
  Assessment Model</title><categories>cs.CY</categories><comments>16 pages, 3 figures,6 tables, Journal paper, e-government, TAM,
  G2BeGEP Measurement framework, International Journal of Managing Information
  Technology (IJMIT), Vol. 7, No. 3, August 2015</comments><doi>10.5121/ijmit.2015.7302</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite the large investments in the field of e-Government (e-Gov) around the
world, little is known about the impact such investment. This is due to the
lack of guidance evaluation, absence of appropriate tools to measure the impact
of e-Gov on the private sector, as well as the lack of effective management to
resolve or eliminate the barriers to e-Gov services that led to the failure or
delay of many projects. This paper is primarily concerned in determining the
impact of e-Gov services on the private sector. A combination of Modified
Technology Acceptance Model (TAM), DeLone and McLean's of IS success will be
utilized as a research model and e-Gov Economics Project (eGEP) framework to
measure (Efficiency, Democracy &amp; Effectiveness impact) for G2B services. The
research result will help e-Gov decision makers to recognize the critical
factors that are responsible for G2B success, specifically factors they need to
pay attention to gain the highest return on their technology investment, hence
enabling them to measure the impact for e-Gov on the private sector. The paper
has also demonstrated the usefulness of Structural Equation Modeling (SEM) in
analysis of small data sets and in exploratory research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03660</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03660</id><created>2015-08-14</created><authors><author><keyname>Pont-Tuset</keyname><forenames>Jordi</forenames></author><author><keyname>Arbel&#xe1;ez</keyname><forenames>Pablo</forenames></author><author><keyname>Van Gool</keyname><forenames>Luc</forenames></author></authors><title>Oracle MCG: A first peek into COCO Detection Challenges</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recently presented COCO detection challenge will most probably be the
reference benchmark in object detection in the next years. COCO is two orders
of magnitude larger than Pascal and has four times the number of categories; so
in all likelihood researchers will be faced with a number of new challenges. At
this point, without any finished round of the competition, it is difficult for
researchers to put their techniques in context, or in other words, to know how
good their results are. In order to give a little context, this note evaluates
a hypothetical object detector consisting in an oracle picking the best object
proposal from a state-of-the-art technique. This oracle achieves a AP=0.292 in
segmented objects and AP=0.317 in bounding boxes, showing that indeed the
database is challenging, given that this value is the best one can expect if
working on object proposals without refinement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03677</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03677</id><created>2015-09-11</created><authors><author><keyname>Viswanathan</keyname><forenames>Sasi Prabhakaran</forenames></author><author><keyname>Sanyal</keyname><forenames>Amit Kumar</forenames></author><author><keyname>Izadi</keyname><forenames>Maziar</forenames></author></authors><title>Mechatronics Architecture of Smartphone-Based Spacecraft ADCS using
  VSCMG Actuators</title><categories>cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Hardware and software architecture of a novel spacecraft Attitude
Determination and Control System (ADCS) based on smartphones using Variable
Speed Control Moment Gyroscope (VSCMG) as actuator is proposed here. A
spacecraft ground simulator testbed for Hardware-in-the-loop (HIL) attitude
estimation and control with VSCMG is also described. The sensor breakouts with
independent micro-controller units are used in the conventional ADCS units,
which are replaced by a single integrated off-the-shelf smartphone. On-board
sensing, data acquisition, data uplink/downlink, state estimation and real-time
feedback control objectives can be performed using this novel spacecraft ADCS.
The attitude control and attitude determination (estimation) schemes have
appeared in prior publications, but are presented in brief here. Experimental
results from running the attitude estimation (filtering) scheme with the
&quot;onboard&quot; sensors of the smartphone in the HIL simulator are given. These
results, obtained in the Spacecraft Guidance, Navigation and Control Laboratory
at NMSU, demonstrate the excellent performance of this estimation scheme with
the noisy raw data from the smartphone sensors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03678</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03678</id><created>2015-09-11</created><updated>2016-02-15</updated><authors><author><keyname>Roberto</keyname><forenames>Elizabeth</forenames></author></authors><title>The Spatial Context of Residential Segregation</title><categories>physics.soc-ph cs.SI stat.ME</categories><comments>33 pages, 16 figures, LaTeX; revised introduction and background,
  edits throughout. arXiv admin note: text overlap with arXiv:1509.02574</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Residential segregation is defined in a variety of ways to address a common
concern: to what extent do social groups reside in separate or distinct places.
The spatial pattern of segregation varies widely across cities, and distinct
spatial patterns can be generated by different mechanisms and have different
consequences for residents and their communities. However, the methods commonly
employed to measure segregation ignore how a city is spatially organized. They
do not take into account the spatial context of segregation patterns, including
1) the spatial arrangement of residential locations and neighborhoods, and 2)
the physical connectivity or spatial boundaries between them. I have developed
a new method that overcomes the limitations of conventional approaches and
enables a closer examination of how individuals' residential contexts are
shaped by their spatial environment. I demonstrate the contribution of my
method using a series of stylized cities that represent patterns of racial
segregation observed in U.S. cities, such as Detroit. The results reveal
distinct spatial patterns of segregation for cities that conventional
approaches measure as having the same level of segregation. It offers the first
quantitative method for systematically studying how spatial boundaries
structure patterns of residential segregation within and across cities. Better
measurement is critical for advancing our understanding of segregation patterns
and processes, and explaining how and why segregation matters for individual
and community outcomes. My new approach bridges qualitative insight on the
spatial context of segregation with the quantitative measurement of segregation
for city populations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03688</identifier>
 <datestamp>2015-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03688</id><created>2015-09-11</created><updated>2015-09-17</updated><authors><author><keyname>Ravanbakhsh</keyname><forenames>Hadi</forenames></author><author><keyname>Sankaranarayanan</keyname><forenames>Sriram</forenames></author></authors><title>Counter-Example Guided Synthesis of Control Lyapunov Functions for
  Switched Systems</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the problem of synthesizing switching controllers for
stabilizing continuous-time plants. First, we introduce a class of control
Lyapunov functions (CLFs) for switched systems along with a switching strategy
that yields a closed loop system with a guaranteed minimum dwell time in each
switching mode. However, the challenge lies in automatically synthesizing
appropriate CLFs. Assuming a given fixed form for the CLF with unknown
coefficients, we derive quantified nonlinear constraints whose feasible
solutions (if any) correspond to CLFs for the original system. However, solving
quantified nonlinear constraints pose a challenge to most LMI/BMI-based
relaxations. Therefore, we investigate a general approach called
Counter-Example Guided Inductive Synthesis (CEGIS), that has been widely used
in the emerging area of automatic program synthesis. We show how a LMI-based
relaxation can be formulated within the CEGIS framework for synthesizing CLFs.
We also evaluate our approach on a number of interesting benchmarks, and
compare the performance of the new approach with our previous work that uses
off-the-shelf nonlinear constraint solvers instead of the LMI relaxation. The
results shows synthesizing CLFs by using LMI solvers inside a CEGIS framework
can be a computational feasible approach to synthesizing CLFs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03699</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03699</id><created>2015-09-11</created><authors><author><keyname>Wang</keyname><forenames>Huangxin</forenames></author><author><keyname>Zhang</keyname><forenames>Jean X.</forenames></author><author><keyname>Yang</keyname><forenames>Bo</forenames></author><author><keyname>Li</keyname><forenames>Fei</forenames></author></authors><title>Randomization Improving Online Time-Sensitive Revenue Maximization for
  Green Data Centers</title><categories>cs.DC</categories><comments>arXiv admin note: substantial text overlap with arXiv:1404.4865</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Green data centers have become more and more popular recently due to their
sustainability. The resource management module within a green data center,
which is in charge of dispatching jobs and scheduling energy, becomes
especially critical as it directly affects a center's profit and
sustainability. The thrust of managing a green data center's machine and energy
resources lies at the uncertainty of incoming job requests and future
showing-up green energy supplies. Thus, the decision of scheduling resources
has to be made in an online manner. Some heuristic deterministic online
algorithms have been proposed in recent literature. In this paper, we consider
online algorithms for green data centers and introduce a randomized solution
with the objective of maximizing net profit. Competitive analysis is employed
to measure online algorithms' theoretical performance. Our algorithm is
theoretical-sound and it outperforms the previously known deterministic
algorithms in many settings using real traces. To complement our study, optimal
offline algorithms are also designed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03700</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03700</id><created>2015-09-11</created><authors><author><keyname>Kovesi</keyname><forenames>Peter</forenames></author></authors><title>Good Colour Maps: How to Design Them</title><categories>cs.GR</categories><comments>42 pages, 25 figures</comments><acm-class>I.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many colour maps provided by vendors have highly uneven perceptual contrast
over their range. It is not uncommon for colour maps to have perceptual flat
spots that can hide a feature as large as one tenth of the total data range.
Colour maps may also have perceptual discontinuities that induce the appearance
of false features. Previous work in the design of perceptually uniform colour
maps has mostly failed to recognise that CIELAB space is only designed to be
perceptually uniform at very low spatial frequencies. The most important factor
in designing a colour map is to ensure that the magnitude of the incremental
change in perceptual lightness of the colours is uniform. The specific
requirements for linear, diverging, rainbow and cyclic colour maps are
developed in detail. To support this work two test images for evaluating colour
maps are presented. The use of colour maps in combination with relief shading
is considered and the conditions under which colour can enhance or disrupt
relief shading are identified. Finally, a set of new basis colours for the
construction of ternary images are presented. Unlike the RGB primaries these
basis colours produce images whereby the salience of structures are consistent
irrespective of the assignment of basis colours to data channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03705</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03705</id><created>2015-09-12</created><updated>2016-01-23</updated><authors><author><keyname>Wang</keyname><forenames>Yuting</forenames></author><author><keyname>Nadathur</keyname><forenames>Gopalan</forenames></author></authors><title>A Higher-Order Abstract Syntax Approach to Verified Transformations on
  Functional Programs</title><categories>cs.PL cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe an approach to the verified implementation of transformations on
functional programs that exploits the higher-order representation of syntax. In
this approach, transformations are specified using the logic of hereditary
Harrop formulas. On the one hand, these specifications serve directly as
implementations, being programs in the language Lambda Prolog. On the other
hand, they can be used as input to the Abella system which allows us to prove
properties about them and thereby about the implementations. We argue that this
approach is especially effective in realizing transformations that analyze
binding structure. We do this by describing concise encodings in Lambda Prolog
for transformations like typed closure conversion and code hoisting that are
sensitive to such structure and by showing how to prove their correctness using
Abella.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03706</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03706</id><created>2015-09-12</created><authors><author><keyname>Banerjee</keyname><forenames>Pradeep Kr.</forenames></author><author><keyname>Griffith</keyname><forenames>Virgil</forenames></author></authors><title>Synergy, Redundancy and Common Information</title><categories>cs.IT math.IT</categories><comments>16 pages, 3 figures</comments><msc-class>94A15, 94A17</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of decomposing the total mutual information conveyed
by a pair of predictor random variables about a target random variable into
redundant, unique and synergistic contributions. We focus on the relationship
between &quot;redundant information&quot; and the more familiar information-theoretic
notions of &quot;common information&quot;. Our main contribution is an impossibility
result. We show that for independent predictor random variables, any common
information based measure of redundancy cannot induce a nonnegative
decomposition of the total mutual information. Interestingly, this entails that
any reasonable measure of redundant information cannot be derived by
optimization over a single random variable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03712</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03712</id><created>2015-09-12</created><authors><author><keyname>K&#xfc;&#xe7;&#xfc;k</keyname><forenames>U&#x11f;ur</forenames></author><author><keyname>Say</keyname><forenames>A. C. Cem</forenames></author><author><keyname>Yakary&#x131;lmaz</keyname><forenames>Abuzer</forenames></author></authors><title>Inkdots as advice to small-space machines</title><categories>cs.FL cs.CC</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We examine inkdots placed on the input string as a way of providing advice to
finite automata, and establish the relations between this model and the
previously studied models of advised finite automata. The existence of an
infinite hierarchy of classes of languages that can be recognized with the help
of increasing numbers of inkdots as advice is shown. We study randomly placed
inkdots as advice to probabilistic finite automata, and demonstrate the
superiority of this model over its deterministic version. Even very slowly
growing amounts of space can become a resource of meaningful use if the
underlying advised model is extended with access to secondary memory, while it
is famously known that such small amounts of space are not useful for unadvised
one-way Turing machines. The effects of different forms of advice on the
succinctness of the advised machines are also examined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03721</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03721</id><created>2015-09-12</created><authors><author><keyname>Ghasempour</keyname><forenames>Mohsen</forenames></author><author><keyname>Garside</keyname><forenames>Jim</forenames></author><author><keyname>Jaleel</keyname><forenames>Aamer</forenames></author><author><keyname>Luj&#xe1;n</keyname><forenames>Mikel</forenames></author></authors><title>DReAM: Dynamic Re-arrangement of Address Mapping to Improve the
  Performance of DRAMs</title><categories>cs.AR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The initial location of data in DRAMs is determined and controlled by the
'address-mapping' and even modern memory controllers use a fixed and
run-time-agnostic address mapping. On the other hand, the memory access pattern
seen at the memory interface level will dynamically change at run-time. This
dynamic nature of memory access pattern and the fixed behavior of address
mapping process in DRAM controllers, implied by using a fixed address mapping
scheme, means that DRAM performance cannot be exploited efficiently. DReAM is a
novel hardware technique that can detect a workload-specific address mapping at
run-time based on the application access pattern which improves the performance
of DRAMs. The experimental results show that DReAM outperforms the best
evaluated address mapping on average by 9%, for mapping-sensitive workloads, by
2% for mapping-insensitive workloads, and up to 28% across all the workloads.
DReAM can be seen as an insurance policy capable of detecting which scenarios
are not well served by the predefined address mapping.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03722</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03722</id><created>2015-09-12</created><updated>2016-02-25</updated><authors><author><keyname>Qureshi</keyname><forenames>Muhammad Imran</forenames></author></authors><title>Computing isolated orbifolds in weighted flag varieties</title><categories>math.AG cs.SC</categories><comments>Minor Changes, few one line explainations added, To Appear in Journal
  of Symbolic Computation, 22 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a weighted flag variety $w\Sigma(\mu,u)$ corresponding to chosen fixed
parameters $\mu$ and $u$, we present an algorithm to compute lists of all
possible projectively Gorenstein $n$-folds, having canonical weight $k$ and
isolated orbifold points, appearing as weighted complete intersections in
$w\Sigma(\mu,u) $ or some projective cone(s) over $w\Sigma(\mu,u)$. We apply
our algorithm to compute lists of interesting classes of polarized 3-folds with
isolated orbifold points in the codimension 8 weighted $G_2$ variety. We also
show the existence of some families of log-terminal $\mathbb Q$-Fano 3-folds in
codimension 8 by explicitly constructing them as quasilinear sections of a
weighted $G_2$-variety.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03723</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03723</id><created>2015-09-12</created><authors><author><keyname>Liu</keyname><forenames>Xiao-Yang</forenames></author><author><keyname>Wang</keyname><forenames>Xiaodong</forenames></author><author><keyname>Kong</keyname><forenames>Linghe</forenames></author><author><keyname>Qiu</keyname><forenames>Meikang</forenames></author><author><keyname>Wu</keyname><forenames>Min-You</forenames></author></authors><title>An LS-Decomposition Approach for Robust Data Recovery in Wireless Sensor
  Networks</title><categories>cs.IT cs.NI math.IT</categories><comments>24 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless sensor networks are widely adopted in military, civilian and
commercial applications, which fuels an exponential explosion of sensory data.
However, a major challenge to deploy effective sensing systems is the presence
of {\em massive missing entries, measurement noise, and anomaly readings}.
Existing works assume that sensory data matrices have low-rank structures. This
does not hold in reality due to anomaly readings, causing serious performance
degradation. In this paper, we introduce an {\em LS-Decomposition} approach for
robust sensory data recovery, which decomposes a corrupted data matrix as the
superposition of a low-rank matrix and a sparse anomaly matrix. First, we prove
that LS-Decomposition solves a convex program with bounded approximation error.
Second, using data sets from the IntelLab, GreenOrbs, and NBDC-CTD projects, we
find that sensory data matrices contain anomaly readings. Third, we propose an
accelerated proximal gradient algorithm and prove that it approximates the
optimal solution with convergence rate $O(1/k^2)$ ($k$ is the number of
iterations). Evaluations on real data sets show that our scheme achieves
recovery error $\leq 5\%$ for sampling rate $\geq 50\%$ and almost exact
recovery for sampling rate $\geq 60\%$, while state-of-the-art methods have
error $10\% \sim 15\%$ at sampling rate $90\%$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03739</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03739</id><created>2015-09-12</created><authors><author><keyname>Roller</keyname><forenames>Roland</forenames></author><author><keyname>Agirre</keyname><forenames>Eneko</forenames></author><author><keyname>Soroa</keyname><forenames>Aitor</forenames></author><author><keyname>Stevenson</keyname><forenames>Mark</forenames></author></authors><title>Improving distant supervision using inference learning</title><categories>cs.CL</categories><comments>In Proceedings of the 53rd Annual Meeting of the Association for
  Computational Linguistics and the 7th International Joint Conference on
  Natural Language Processing (Volume 2: Short Papers)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distant supervision is a widely applied approach to automatic training of
relation extraction systems and has the advantage that it can generate large
amounts of labelled data with minimal effort. However, this data may contain
errors and consequently systems trained using distant supervision tend not to
perform as well as those based on manually labelled data. This work proposes a
novel method for detecting potential false negative training examples using a
knowledge inference method. Results show that our approach improves the
performance of relation extraction systems trained using distantly supervised
data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03740</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03740</id><created>2015-09-12</created><authors><author><keyname>Ghasempour</keyname><forenames>Mohsen</forenames></author><author><keyname>Jaleel</keyname><forenames>Aamer</forenames></author><author><keyname>Garside</keyname><forenames>Jim</forenames></author><author><keyname>Luj&#xe1;n</keyname><forenames>Mikel</forenames></author></authors><title>HAPPY: Hybrid Address-based Page Policy in DRAMs</title><categories>cs.AR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Memory controllers have used static page closure policies to decide whether a
row should be left open, open-page policy, or closed immediately, close-page
policy, after the row has been accessed. The appropriate choice for a
particular access can reduce the average memory latency. However, since
application access patterns change at run time, static page policies cannot
guarantee to deliver optimum execution time. Hybrid page policies have been
investigated as a means of covering these dynamic scenarios and are now
implemented in state-of-the-art processors. Hybrid page policies switch between
open-page and close-page policies while the application is running, by
monitoring the access pattern of row hits/conflicts and predicting future
behavior. Unfortunately, as the size of DRAM memory increases, fine-grain
tracking and analysis of memory access patterns does not remain practical. We
propose a compact memory address-based encoding technique which can improve or
maintain the performance of DRAMs page closure predictors while reducing the
hardware overhead in comparison with state-of-the-art techniques. As a case
study, we integrate our technique, HAPPY, with a state-of-the-art monitor, the
Intel-adaptive open-page policy predictor employed by the Intel Xeon X5650, and
a traditional Hybrid page policy. We evaluate them across 70 memory intensive
workload mixes consisting of single-thread and multi-thread applications. The
experimental results show that using the HAPPY encoding applied to the
Intel-adaptive page closure policy can reduce the hardware overhead by 5X for
the evaluated 64 GB memory (up to 40X for a 512 GB memory) while maintaining
the prediction accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03753</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03753</id><created>2015-09-12</created><updated>2015-09-15</updated><authors><author><keyname>Golovach</keyname><forenames>Petr A.</forenames></author><author><keyname>Heggernes</keyname><forenames>Pinar</forenames></author><author><keyname>Kant&#xe9;</keyname><forenames>Mamadou Moustapha</forenames></author><author><keyname>Kratsch</keyname><forenames>Dieter</forenames></author><author><keyname>S&#xe6;ther</keyname><forenames>Sigve H.</forenames></author><author><keyname>Villanger</keyname><forenames>Yngve</forenames></author></authors><title>Output-Polynomial Enumeration on Graphs of Bounded (Local) Linear
  MIM-Width</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The linear induced matching width (LMIM-width) of a graph is a width
parameter defined by using the notion of branch-decompositions of a set
function on ternary trees. In this paper we study output-polynomial enumeration
algorithms on graphs of bounded LMIM-width and graphs of bounded local
LMIM-width. In particular, we show that all 1-minimal and all 1-maximal
(\sigma,\rho)-dominating sets, and hence all minimal dominating sets, of graphs
of bounded LMIM-width can be enumerated with polynomial (linear) delay using
polynomial space. Furthermore, we show that all minimal dominating sets of a
unit square graph can be enumerated in incremental polynomial time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03755</identifier>
 <datestamp>2015-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03755</id><created>2015-09-12</created><updated>2015-09-16</updated><authors><author><keyname>Masramon</keyname><forenames>Gabriel Prat</forenames></author><author><keyname>Mu&#xf1;oz</keyname><forenames>Llu&#xed;s A. Belanche</forenames></author></authors><title>Toward better feature weighting algorithms: a focus on Relief</title><categories>cs.LG</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Feature weighting algorithms try to solve a problem of great importance
nowadays in machine learning: The search of a relevance measure for the
features of a given domain. This relevance is primarily used for feature
selection as feature weighting can be seen as a generalization of it, but it is
also useful to better understand a problem's domain or to guide an inductor in
its learning process. Relief family of algorithms are proven to be very
effective in this task. Some other feature weighting methods are reviewed in
order to give some context and then the different existing extensions to the
original algorithm are explained.
  One of Relief's known issues is the performance degradation of its estimates
when redundant features are present. A novel theoretical definition of
redundancy level is given in order to guide the work towards an extension of
the algorithm that is more robust against redundancy. A new extension is
presented that aims for improving the algorithms performance. Some experiments
were driven to test this new extension against the existing ones with a set of
artificial and real datasets and denoted that in certain cases it improves the
weight's estimation accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03775</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03775</id><created>2015-09-12</created><authors><author><keyname>Vale</keyname><forenames>Ronald D.</forenames></author></authors><title>Accelerating Scientific Publication in Biology</title><categories>q-bio.OT cs.DL</categories><comments>39 pages, 6 figures, 1 table, and a Q&amp;A related to pre-prints</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scientific publications enable results and ideas to be transmitted throughout
the scientific community. The number and type of journal publications also have
become the primary criteria used in evaluating career advancement. Our analysis
suggests that publication practices have changed considerably in the life
sciences over the past thirty years. More experimental data is now required for
publication, and the average time required for graduate students to publish
their first paper has increased and is approaching the desirable duration of
Ph.D. training. Since publication is generally a requirement for career
progression, schemes to reduce the time of graduate student and postdoctoral
training may be difficult to implement without also considering new mechanisms
for accelerating communication of their work. The increasing time to
publication also delays potential catalytic effects that ensue when many
scientists have access to new information. The time has come for life
scientists, funding agencies, and publishers to discuss how to communicate new
findings in a way that best serves the interests of the public and the
scientific community.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03778</identifier>
 <datestamp>2015-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03778</id><created>2015-09-12</created><updated>2015-11-05</updated><authors><author><keyname>Hammer</keyname><forenames>Julian</forenames></author><author><keyname>Hager</keyname><forenames>Georg</forenames></author><author><keyname>Eitzinger</keyname><forenames>Jan</forenames></author><author><keyname>Wellein</keyname><forenames>Gerhard</forenames></author></authors><title>Automatic Loop Kernel Analysis and Performance Modeling With Kerncraft</title><categories>cs.PF</categories><comments>11 pages, 4 figures, 8 listings</comments><doi>10.1145/2832087.2832092</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Analytic performance models are essential for understanding the performance
characteristics of loop kernels, which consume a major part of CPU cycles in
computational science. Starting from a validated performance model one can
infer the relevant hardware bottlenecks and promising optimization
opportunities. Unfortunately, analytic performance modeling is often tedious
even for experienced developers since it requires in-depth knowledge about the
hardware and how it interacts with the software. We present the &quot;Kerncraft&quot;
tool, which eases the construction of analytic performance models for streaming
kernels and stencil loop nests. Starting from the loop source code, the problem
size, and a description of the underlying hardware, Kerncraft can ideally
predict the single-core performance and scaling behavior of loops on multicore
processors using the Roofline or the Execution-Cache-Memory (ECM) model. We
describe the operating principles of Kerncraft with its capabilities and
limitations, and we show how it may be used to quickly gain insights by
accelerated analytic modeling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03781</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03781</id><created>2015-09-12</created><authors><author><keyname>Koczkodaj</keyname><forenames>W. W.</forenames></author><author><keyname>Szybowski</keyname><forenames>J.</forenames></author></authors><title>Axiomatization of Inconsistency Indicators for Pairwise Comparisons
  Matrices Revisited</title><categories>cs.OH</categories><comments>11 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This study proposes axioms for inconsistency indicators in pairwise
comparisons. The new observation (by Szybowski), that &quot;no $PC$ submatrix may
have a worse inconsistency indicator than the given $PC$ matrix&quot; is an
essential simplification of the axiomatization. The goal of formulating axioms
for all future definitions of new inconsistency indicators is difficult and as
illusive as the inconsistency concept itself. This study improves the
axiomatization proposed by Koczkodaj and Szwarc in 2014. As a side product, the
new axiom allows to prevent approximation error aberrations of an arbitrarily
large value in the input data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03784</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03784</id><created>2015-09-12</created><authors><author><keyname>Hurley</keyname><forenames>Ted</forenames></author></authors><title>Solving underdetermined systems with error-correcting codes</title><categories>cs.IT math.IT</categories><msc-class>94A99, 15B99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In an underdetermined system of equations $Ax=y$, where $A$ is an $m\times n$
matrix, only $u$ of the entries of $y$ with $u &lt; m$ are known. Thus $E_jw$,
called `measurements', are known for certain $j\in J \subset
\{0,1,\ldots,m-1\}$ where $\{E_i, i=0,1,\ldots, m-1\}$ are the rows of $A$ and
$|J|=u$. It is required, if possible, to solve the system uniquely when $x$ has
at most $t$ non-zero entries with $u\geq 2t$.
  Here such systems are considered from an error-correcting coding point of
view. The unknown $x$ can be shown to be the error vector of a code subject to
certain conditions on the rows of the matrix $A$. This reduces the problem to
finding a suitable decoding algorithm which then finds $x$.
  Decoding workable algorithms are shown to exist, from which the unknown $x$
may be determined, in cases where the known $2t$ values are evenly spaced (that
is, when the elements of $J$ are in arithmetic progression) for classes of
matrices satisfying certain row properties. These cases include Fourier
$n\times n $ matrices where the arithmetic difference $k$ satisfies
$\gcd(n,k)=1$, and classes of Vandermonde matrices $V(x_1,x_2,\ldots,x_n)$
(with $x_i\neq 0$) with arithmetic difference $k$ where the ratios $x_i/x_j$
for $i\neq j$ are not $k^{th}$ roots of unity. The decoding algorithm has
complexity $O(nt)$ and in some cases, including the Fourier matrix cases, the
complexity is $O(t^2)$.
  Matrices which have the property that the determinant of any square submatrix
is non-zero are of particular interest. Randomly choosing rows of such matrices
can then give $t$ error-correcting pairs to generate a `measuring' code
$C^\perp=\{E_j | j\in J\}$ with a decoding algorithm which finds $x$.
  This has applications to signal processing and compressed sensing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03789</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03789</id><created>2015-09-12</created><updated>2016-02-20</updated><authors><author><keyname>Yousefi</keyname><forenames>Bardia</forenames></author><author><keyname>Loo</keyname><forenames>Chu Kiong</forenames></author></authors><title>Bio-Inspired Human Action Recognition using Hybrid Max-Product
  Neuro-Fuzzy Classifier and Quantum-Behaved PSO</title><categories>cs.AI cs.CV</categories><comments>author's version, SWJ 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Studies on computational neuroscience through functional magnetic resonance
imaging (fMRI) and following biological inspired system stated that human
action recognition in the brain of mammalian leads two distinct pathways in the
model, which are specialized for analysis of motion (optic flow) and form
information. Principally, we have defined a novel and robust form features
applying active basis model as form extractor in form pathway in the biological
inspired model. An unbalanced synergetic neural net-work classifies shapes and
structures of human objects along with tuning its attention parameter by
quantum particle swarm optimization (QPSO) via initiation of Centroidal Voronoi
Tessellations. These tools utilized and justified as strong tools for following
biological system model in form pathway. But the final decision has done by
combination of ultimate outcomes of both pathways via fuzzy inference which
increases novality of proposed model. Combination of these two brain pathways
is done by considering each feature sets in Gaussian membership functions with
fuzzy product inference method. Two configurations have been proposed for form
pathway: applying multi-prototype human action templates using two time
synergetic neural network for obtaining uniform template regarding each
actions, and second scenario that it uses abstracting human action in four
key-frames. Experimental results showed promising accuracy performance on
different datasets (KTH and Weizmann).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03800</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03800</id><created>2015-09-12</created><authors><author><keyname>Zhu</keyname><forenames>Bing</forenames></author><author><keyname>Li</keyname><forenames>Hui</forenames></author><author><keyname>Shum</keyname><forenames>Kenneth W.</forenames></author><author><keyname>Li</keyname><forenames>Shuo-Yen Robert</forenames></author></authors><title>HFR Code: A Flexible Replication Scheme for Cloud Storage Systems</title><categories>cs.IT math.IT</categories><comments>Accepted for publication in IET Communications, Jul. 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fractional repetition (FR) codes are a family of repair-efficient storage
codes that provide exact and uncoded node repair at the minimum bandwidth
regenerating point. The advantageous repair properties are achieved by a
tailor-made two-layer encoding scheme which concatenates an outer
maximum-distance-separable (MDS) code and an inner repetition code. In this
paper, we generalize the application of FR codes and propose heterogeneous
fractional repetition (HFR) code, which is adaptable to the scenario where the
repetition degrees of coded packets are different. We provide explicit code
constructions by utilizing group divisible designs, which allow the design of
HFR codes over a large range of parameters. The constructed codes achieve the
system storage capacity under random access repair and have multiple repair
alternatives for node failures. Further, we take advantage of the systematic
feature of MDS codes and present a novel design framework of HFR codes, in
which storage nodes can be wisely partitioned into clusters such that data
reconstruction time can be reduced when contacting nodes in the same cluster.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03807</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03807</id><created>2015-09-13</created><authors><author><keyname>Quang</keyname><forenames>Le Xuan</forenames></author><author><keyname>Hoang</keyname><forenames>Le Huy</forenames></author><author><keyname>Chuan</keyname><forenames>Vu Dinh</forenames></author><author><keyname>Nam</keyname><forenames>Nguyen Hoai</forenames></author><author><keyname>Anh</keyname><forenames>Nguyen Thi Tu</forenames></author><author><keyname>Nhung</keyname><forenames>Vu Thi Hong</forenames></author></authors><title>Integrated Science, Technology, Engineering and Mathematics (STEM)
  Education through Active Experience of Designing Technical Toys in Vietnamese
  Schools</title><categories>cs.CY</categories><comments>12 pages, 7 figures, 2 tables, British Journal of Education, Society
  &amp; Behavioural Science, 2015</comments><doi>10.9734/BJESBS/2015/19429</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  STEM has attracted great consideration. The purpose of research is: (1) study
STEM education, (2) explore STEM education with the creative and experiential
activity, (3) suggest applying STEM education by designing technical toys for
the middle school student. This study used a qualitative approach to carry out
teaching integration for STEM education. The study applied to teaching the
technological field in Vietnamese middle schools. The design performed at the
Faculty of Technology Education, Hanoi National University of Education,
Vietnam in April 2015. This study used the integrated approach to design
subjects for STEM education. Two procedures for integration undertook with
analysis. A sample of producing technical toy was consistent with developing
students competencies. Integrated approach to STEM education through designing
technical toys is possible. Recently, there has been a booming interest in
Integrated Science, Technology, Engineering and Mathematics (STEM) education,
but the approaches to STEM still remains controversial in diverse educational
contexts. This study addressed this issue by exploring STEM education with the
use of creative and experiential activities in a Vietnamese educational
context. It also proposed a practical model for integrating STEM into teaching
technology in secondary schools by designing technical toys. The implementation
of the practical model suggests the possibility in using the integrated
approach to STEM education through designing technical toys for middle school
students in Vietnam. By applying the subject knowledge domains to solve real
world problems and settings, the students can experience the benefits of a
concrete and active learning in a meaningful and practical context. The
multidisciplinary and interdisciplinary integration approaches are consistent
with the development of the students competencies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03810</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03810</id><created>2015-09-13</created><authors><author><keyname>Bellili</keyname><forenames>Faouzi</forenames></author><author><keyname>Methenni</keyname><forenames>Achref</forenames></author><author><keyname>Amor</keyname><forenames>Souheib Ben</forenames></author><author><keyname>Affes</keyname><forenames>Sofi&#xe8;ne</forenames></author><author><keyname>St&#xe9;phenne</keyname><forenames>Alex</forenames></author></authors><title>Time Synchronization of Turbo-Coded Square-QAM-Modulated Transmissions:
  Code-Aided ML Estimator and Closed-Form Cram\'er-Rao Lower Bounds</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces a new maximum likelihood (ML) solution for the
code-aided (CA) timing recovery problem in square-QAM transmissions and
derives, for the very first time, its CA Cram\'er-Rao lower bounds (CRLBs) in
closed-form expressions. By exploiting the full symmetry of square-QAM
constellations and further scrutinizing the Gray-coding mechanism, we express
the likelihood function (LF) of the system explicitly in terms of the code
bits' \textit{a priori} log-likelihood ratios (LLRs). The timing recovery task
is then embedded in the turbo iteration loop wherein increasingly accurate
estimates for such LLRs are computed from the output of the soft-input
soft-output (SISO) decoders and exploited at a per-turbo-iteration basis in
order to refine the ML time delay estimate. The latter is then used to better
re-synchronize the system, through feedback to the matched filter (MF), so as
to obtain more reliable symbol-rate samples for the next turbo iteration. In
order to properly benchmark the new CA ML estimator, we also derive for the
very first time the closed-form expressions for the exact CRLBs of the
underlying turbo synchronization problem. Computer simulations will show that
the new closed-form CRLBs coincide exactly with their empirical counterparts
evaluated previously using exhaustive Monte-Carlo simulations. They will also
show unambiguously the potential performance gains in time synchronization that
can be achieved owing to the decoder assistance. Moreover, the new CA ML
estimator almost reaches the underlying CA CRLBs, even for small SNRs, thereby
confirming its statistical efficiency in practice. It also enjoys significant
improvements in computational complexity as compared to the most powerful
existing ML solution, namely the combined sum-product and
expectation-maximization (SP-EM) algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03815</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03815</id><created>2015-09-13</created><authors><author><keyname>Devismes</keyname><forenames>St&#xe9;phane</forenames></author><author><keyname>Johnen</keyname><forenames>Colette</forenames></author></authors><title>Silent Self-stabilizing BFS Tree Algorithms Revised</title><categories>cs.DC</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we revisit two fundamental results of the self-stabilizing
literature about silent BFS spanning tree constructions: the Dolev et al
algorithm and the Huang and Chen's algorithm. More precisely, we propose in the
composite atomicity model three straightforward adaptations inspired from those
algorithms. We then present a deep study of these three algorithms. Our results
are related to both correctness (convergence and closure, assuming a
distributed unfair daemon) and complexity (analysis of the stabilization time
in terms of rounds and steps).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03817</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03817</id><created>2015-09-13</created><authors><author><keyname>Parpalea</keyname><forenames>Mircea</forenames></author><author><keyname>Avesalon</keyname><forenames>Nicoleta</forenames></author><author><keyname>Ciurea</keyname><forenames>Eleonor</forenames></author></authors><title>Minimum parametric flow over time</title><categories>cs.DM cs.DS</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper presents a dynamic solution method for dynamic minimum parametric
networks flow. The solution method solves the problem for a special parametric
dynamic network with linear lower bound functions of a single parameter.
Instead directly work on the original network, the method implements a
labelling algorithm in the parametric dynamic residual network and uses
quickest paths from the source node to the sink node in the time-space network
along which repeatedly decreases the dynamic flow for a sequence of parameter
values, in their increasing order. In each iteration, the algorithm computes
both the minimum flow for a certain subinterval of the parameter values, and
the new breakpoint for the maximum parametric dynamic flow value function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03836</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03836</id><created>2015-09-13</created><authors><author><keyname>Srinivasarao</keyname><forenames>Batta Kota Naga</forenames></author><author><keyname>Chakrabarti</keyname><forenames>Indrajit</forenames></author></authors><title>Hardware Implementation of Compressed Sensing based Low Complex Video
  Encoder</title><categories>cs.MM</categories><comments>Submitted in IEEE transactions on VLSI</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a memory efficient VLSI architecture of low complex video
encoder using three dimensional (3-D) wavelet and Compressed Sensing (CS) is
proposed for space and low power video applications. Majority of the
conventional video coding schemes are based on hybrid model, which requires
complex operations like transform coding (DCT), motion estimation and
deblocking filter at the encoder. Complexity of the proposed encoder is reduced
by replacing those complex operations by 3-D DWT and CS at the encoder. The
proposed architecture uses 3-D DWT to enable the scalability with levels of
wavelet decomposition and also to exploit the spatial and the temporal
redundancies. CS provides the good error resilience and coding efficiency. At
the first stage of the proposed architecture for encoder, 3-D DWT has been
applied (Lifting based 2-D DWT in spatial domain and Haar wavelet in temporal
domain) on each frame of the group of frames (GOF), and in the second stage CS
module exploits the sparsity of the wavelet coefficients. Small set of linear
measurements are extracted by projecting the sparse 3-D wavelet coefficients
onto random Bernoulli matrix at the encoder. Compared with the best existing
3-D DWT architectures, the proposed architecture for 3-D DWT requires less
memory and provide high throughput. For an N?N image, the proposed 3-D DWT
architecture consumes a total of only 2?(3N +40P) words of on-chip memory for
the one level of decomposition. The proposed architecture for an encoder is
first of its kind and to the best of my knowledge, no architecture is noted for
comparison. The proposed VLSI architecture of the encoder has been synthesized
on 90-nm CMOS process technology and results show that it consumes 90.08 mW
power and occupies an area equivalent to 416.799 K equivalent gate at frequency
of 158 MHz.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03838</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03838</id><created>2015-09-13</created><authors><author><keyname>Anam</keyname><forenames>Mohammad Ashraful</forenames></author><author><keyname>Andreopoulos</keyname><forenames>Yiannis</forenames></author></authors><title>Failure Mitigation in Linear, Sesquilinear and Bijective Operations On
  Integer Data Streams Via Numerical Entanglement</title><categories>cs.DC</categories><comments>Proc. 21st IEEE International On-Line Testing Symposium (IOLTS 2015),
  July 2015, Halkidiki, Greece</comments><doi>10.1109/IOLTS.2015.7229844</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new roll-forward technique is proposed that recovers from any single
fail-stop failure in $M$ integer data streams ($M\geq3$) when undergoing
linear, sesquilinear or bijective (LSB) operations, such as: scaling,
additions/subtractions, inner or outer vector products and permutations. In the
proposed approach, the $M$ input integer data streams are linearly superimposed
to form $M$ numerically entangled integer data streams that are stored in-place
of the original inputs. A series of LSB operations can then be performed
directly using these entangled data streams. The output results can be
extracted from any $M-1$ entangled output streams by additions and arithmetic
shifts, thereby guaranteeing robustness to a fail-stop failure in any single
stream computation. Importantly, unlike other methods, the number of operations
required for the entanglement, extraction and recovery of the results is
linearly related to the number of the inputs and does not depend on the
complexity of the performed LSB operations. We have validated our proposal in
an Intel processor (Haswell architecture with AVX2 support) via convolution
operations. Our analysis and experiments reveal that the proposed approach
incurs only $1.8\%$ to $2.8\%$ reduction in processing throughput in comparison
to the failure-intolerant approach. This overhead is 9 to 14 times smaller than
that of the equivalent checksum-based method. Thus, our proposal can be used in
distributed systems and unreliable processor hardware, or safety-critical
applications, where robustness against fail-stop failures becomes a necessity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03840</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03840</id><created>2015-09-13</created><authors><author><keyname>Kim</keyname><forenames>Hongkeun</forenames></author><author><keyname>De Persis</keyname><forenames>Claudio</forenames></author></authors><title>Adaptation and Disturbance Rejection for Output Synchronization of
  Incrementally Output-feedback Passive Systems</title><categories>cs.SY</categories><comments>24 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This note addresses the output synchronization problem of incrementally
output-feedback passive nonlinear systems in the presence of exogenous
disturbances. Two kinds of distributed controllers are proposed; one placed at
the nodes and the other placed at the edges. Each of them is synthesized based
on the adaptive control method to cope with the shortage of passivity, and on
the internal model principle to deal with the disturbances. The proposed
controllers synchronize the outputs of the nonlinear systems when the solution
of the closed-loop system is bounded. Based on this, we present a class of
systems for which boundedness of the solutions is guaranteed. The analysis used
in this note is also applicable to a case where systems are coupled via links
modeled by dynamical systems. Simulation results of a network of Van der Pol
oscillators show the effectiveness of the proposed methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03842</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03842</id><created>2015-09-13</created><authors><author><keyname>Arslan</keyname><forenames>Omur</forenames></author><author><keyname>Koditschek</keyname><forenames>Daniel E.</forenames></author></authors><title>Voronoi-Based Coverage Control of Heterogeneous Disk-Shaped Robots</title><categories>cs.RO</categories><comments>9 pages, 4 figures, extended version of a paper in preparation for
  submission to a conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In distributed mobile sensing applications, networks of agents that are
heterogeneous respecting both actuation as well as body and sensory footprint
are often modelled by recourse to power diagrams --- generalized Voronoi
diagrams with additive weights. In this paper we adapt the body power diagram
to introduce its &quot;free subdiagram,&quot; generating a vector field planner that
solves the combined sensory coverage and collision avoidance problem via
continuous evaluation of an associated constrained optimization problem. We
propose practical extensions (a heuristic congestion manager that speeds
convergence and a lift of the point particle controller to the more practical
differential drive kinematics) that maintain the convergence and collision
guarantees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03843</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03843</id><created>2015-09-13</created><authors><author><keyname>Newton</keyname><forenames>Derrick</forenames></author></authors><title>Attack on a classical analogue of the Dunjko, Wallden, Kent and
  Andersson quantum digital signature protocol</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A quantum digital signature (QDS) protocol is investigated in respect of an
attacker who can impersonate other communicating principals in the style of
Lowe's attack on the Needham-Schroeder public-key authentication protocol. A
man-in-the-middle attack is identified in respect of a classical variant of the
protocol and it is suggested that a similar attack would be effective against
the QDS protocol. The attack has been confirmed through initial protocol
modelling using a automated theorem prover, ProVerif.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03844</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03844</id><created>2015-09-13</created><authors><author><keyname>Abbas</keyname><forenames>Alhabib</forenames></author><author><keyname>Deligiannis</keyname><forenames>Nikos</forenames></author><author><keyname>Andreopoulos</keyname><forenames>Yiannis</forenames></author></authors><title>Vectors of Locally Aggregated Centers for Compact Video Representation</title><categories>cs.MM cs.CV cs.IR</categories><comments>Proc. IEEE International Conference on Multimedia and Expo, ICME
  2015, Torino, Italy</comments><doi>10.1109/ICME.2015.7177501</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel vector aggregation technique for compact video
representation, with application in accurate similarity detection within large
video datasets. The current state-of-the-art in visual search is formed by the
vector of locally aggregated descriptors (VLAD) of Jegou et. al. VLAD generates
compact video representations based on scale-invariant feature transform (SIFT)
vectors (extracted per frame) and local feature centers computed over a
training set. With the aim to increase robustness to visual distortions, we
propose a new approach that operates at a coarser level in the feature
representation. We create vectors of locally aggregated centers (VLAC) by first
clustering SIFT features to obtain local feature centers (LFCs) and then
encoding the latter with respect to given centers of local feature centers
(CLFCs), extracted from a training set. The sum-of-differences between the LFCs
and the CLFCs are aggregated to generate an extremely-compact video description
used for accurate video segment similarity detection. Experimentation using a
video dataset, comprising more than 1000 minutes of content from the Open Video
Project, shows that VLAC obtains substantial gains in terms of mean Average
Precision (mAP) against VLAD and the hyper-pooling method of Douze et. al.,
under the same compaction factor and the same set of distortions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03846</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03846</id><created>2015-09-13</created><authors><author><keyname>Izumi</keyname><forenames>Taisuke</forenames></author></authors><title>Improving Lower Bound on Opaque Set for Equilateral Triangle</title><categories>cs.CG</categories><comments>11pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An opaque set (or a barrier) for $U \subseteq \mathbb{R}^2$ is a set $B$ of
finite-length curves such that any line intersecting $U$ also intersects $B$.
In this paper, we consider the lower bound for the shortest barrier when $U$ is
the unit equilateral triangle. The known best lower bound for triangles is the
classic one by Jones [Jones,1964], which exhibits that the length of the
shortest barrier for any convex polygon is at least the half of its perimeter.
That is, for the unit equilateral triangle, it must be at least $3/2$. Very
recently, this lower bounds are improved for convex $k$-gons for any $k\geq 4$
[Kawamura et al. 2014], but the case of triangles still lack the bound better
than Jones' one. The main result of this paper is to fill this missing piece:
We give the lower bound of $3/2 + 5 \cdot 10^{-13}$ for the unit-size
equilateral triangle. The proof is based on two new ideas, angle-restricted
barriers and a weighted sum of projection-cover conditions, which may be of
independently interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03853</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03853</id><created>2015-09-13</created><authors><author><keyname>Slavnov</keyname><forenames>Sergey</forenames></author></authors><title>On Banach spaces of sequences and free linear logic exponential modality</title><categories>cs.LO math.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a category of vector spaces modelling full propositional linear
logic, similar to probabilistic coherence spaces and to Koethe sequences
spaces. Its objects are {\it rigged sequences spaces}, Banach spaces of
sequences, with norms defined from pairing with finite sequences, and morphisms
are bounded linear maps, continuous in a suitable topology. The main interest
of the work is that our model gives a realization of the free linear logic
exponentials construction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03870</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03870</id><created>2015-09-13</created><authors><author><keyname>Ng</keyname><forenames>Raymond W. M.</forenames></author><author><keyname>Doulaty</keyname><forenames>Mortaza</forenames></author><author><keyname>Doddipatla</keyname><forenames>Rama</forenames></author><author><keyname>Aziz</keyname><forenames>Wilker</forenames></author><author><keyname>Shah</keyname><forenames>Kashif</forenames></author><author><keyname>Saz</keyname><forenames>Oscar</forenames></author><author><keyname>Hasan</keyname><forenames>Madina</forenames></author><author><keyname>AlHarbi</keyname><forenames>Ghada</forenames></author><author><keyname>Specia</keyname><forenames>Lucia</forenames></author><author><keyname>Hain</keyname><forenames>Thomas</forenames></author></authors><title>The USFD Spoken Language Translation System for IWSLT 2014</title><categories>cs.CL</categories><journal-ref>Proc. of 11th International Workshop on Spoken Language
  Translation (SLT 2014) 86-91, Lake Tahoe, USA, December 4th and 5th, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The University of Sheffield (USFD) participated in the International Workshop
for Spoken Language Translation (IWSLT) in 2014. In this paper, we will
introduce the USFD SLT system for IWSLT. Automatic speech recognition (ASR) is
achieved by two multi-pass deep neural network systems with adaptation and
rescoring techniques. Machine translation (MT) is achieved by a phrase-based
system. The USFD primary system incorporates state-of-the-art ASR and MT
techniques and gives a BLEU score of 23.45 and 14.75 on the English-to-French
and English-to-German speech-to-text translation task with the IWSLT 2014 data.
The USFD contrastive systems explore the integration of ASR and MT by using a
quality estimation system to rescore the ASR outputs, optimising towards better
translation. This gives a further 0.54 and 0.26 BLEU improvement respectively
on the IWSLT 2012 and 2014 evaluation data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03877</identifier>
 <datestamp>2016-02-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03877</id><created>2015-09-13</created><updated>2016-02-07</updated><authors><author><keyname>Zuo</keyname><forenames>Zhen</forenames></author><author><keyname>Shuai</keyname><forenames>Bing</forenames></author><author><keyname>Wang</keyname><forenames>Gang</forenames></author><author><keyname>Liu</keyname><forenames>Xiao</forenames></author><author><keyname>Wang</keyname><forenames>Xingxing</forenames></author><author><keyname>Wang</keyname><forenames>Bing</forenames></author></authors><title>Learning Contextual Dependencies with Convolutional Hierarchical
  Recurrent Neural Networks</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Existing deep convolutional neural networks (CNNs) have shown their great
success on image classification. CNNs mainly consist of convolutional and
pooling layers, both of which are performed on local image areas without
considering the dependencies among different image regions. However, such
dependencies are very important for generating explicit image representation.
In contrast, recurrent neural networks (RNNs) are well known for their ability
of encoding contextual information among sequential data, and they only require
a limited number of network parameters. General RNNs can hardly be directly
applied on non-sequential data. Thus, we proposed the hierarchical RNNs
(HRNNs). In HRNNs, each RNN layer focuses on modeling spatial dependencies
among image regions from the same scale but different locations. While the
cross RNN scale connections target on modeling scale dependencies among regions
from the same location but different scales. Specifically, we propose two
recurrent neural network models: 1) hierarchical simple recurrent network
(HSRN), which is fast and has low computational cost; and 2) hierarchical
long-short term memory recurrent network (HLSTM), which performs better than
HSRN with the price of more computational cost.
  In this manuscript, we integrate CNNs with HRNNs, and develop end-to-end
convolutional hierarchical recurrent neural networks (C-HRNNs). C-HRNNs not
only make use of the representation power of CNNs, but also efficiently encodes
spatial and scale dependencies among different image regions. On four of the
most challenging object/scene image classification benchmarks, our C-HRNNs
achieve state-of-the-art results on Places 205, SUN 397, MIT indoor, and
competitive results on ILSVRC 2012.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03891</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03891</id><created>2015-09-13</created><authors><author><keyname>Mehri</keyname><forenames>Soroush</forenames></author></authors><title>On Binary Classification with Single-Layer Convolutional Neural Networks</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Convolutional neural networks are becoming standard tools for solving object
recognition and visual tasks. However, most of the design and implementation of
these complex models are based on trail-and-error. In this report, the main
focus is to consider some of the important factors in designing convolutional
networks to perform better. Specifically, classification with wide single-layer
networks with large kernels as a general framework is considered. Particularly,
we will show that pre-training using unsupervised schemes is vital, reasonable
regularization is beneficial and applying of strong regularizers like dropout
could be devastating. Pool size is also could be as important as learning
procedure itself. In addition, it has been presented that using such a simple
and relatively fast model for classifying cats and dogs, performance is close
to state-of-the-art achievable by a combination of SVM models on color and
texture features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03907</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03907</id><created>2015-09-13</created><authors><author><keyname>Defant</keyname><forenames>Colin</forenames></author></authors><title>Binary Codes and Period-$2$ Orbits of Sequential Dynamical Systems</title><categories>math.CO cs.IT math.IT</categories><comments>15 pages, 2 figures</comments><msc-class>37E15, 05C69</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A sequential dynamical system (SDS) consists of an undirected simple graph
$Y$ with vertices $v_1,v_2,\ldots,v_n$, a collection of vertex functions
$\{f_{v_i}\}_{i=1}^n$, a permutation $\pi\in S_n$, and a collection of states
$A$. The vertices update their states in a sequential order specified by $\pi$
using the vertex functions $\{f_{v_i}\}_{i=1}^n$. Any sequential dynamical
system gives rise to an SDS-map $[Y,\{f_{v_i}\}_{i=1}^n,\pi]$, which maps each
initial configuration of vertex states to the new configuration of states
obtained after each vertex updates. When all vertex functions are equal to the
same function $f$, we write the SDS-map as $[Y,f,\pi]$.
  Let $\eta_n$ denote the maximum number of periodic orbits of period $2$ that
an SDS-map of the form $[K_n,f,\pi]$ can have. We show that $\eta_n$ is equal
to the maximum number of codewords in a binary code of length $n-1$ with
minimum distance at least $3$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03909</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03909</id><created>2015-09-13</created><authors><author><keyname>Zhuang</keyname><forenames>Yong</forenames></author><author><keyname>Ya&#x11f;an</keyname><forenames>Osman</forenames></author></authors><title>Information Propagation in Clustered Multilayer Networks</title><categories>physics.soc-ph cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In today's world, individuals interact with each other in more complicated
patterns than ever. Some individuals engage through online social networks
(e.g., Facebook, Twitter), while some communicate only through conventional
ways (e.g., face-to-face). Therefore, understanding the dynamics of information
propagation among humans calls for a multi-layer network model where an online
social network is conjoined with a physical network. In this work, we initiate
a study of information diffusion in a clustered multi-layer network model,
where all constituent layers are random networks with high clustering. We
assume that information propagates according to the SIR model and with
different information transmissibility across the networks. We give results for
the conditions, probability, and size of information epidemics, i.e., cases
where information starts from a single individual and reaches a positive
fraction of the population. We show that increasing the level of clustering in
either one of the layers increases the epidemic threshold and decreases the
final epidemic size in the whole system. An interesting finding is that
information with low transmissibility spreads more effectively with a small but
densely connected social network, whereas highly transmissible information
spreads better with the help of a large but loosely connected social network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03915</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03915</id><created>2015-09-13</created><updated>2016-03-05</updated><authors><author><keyname>Aziz</keyname><forenames>Haris</forenames></author></authors><title>Generalizing Top Trading Cycles for Housing Markets with Fractional
  Endowments</title><categories>cs.GT cs.DS</categories><msc-class>91A12, 68Q15</msc-class><acm-class>F.2; J.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The housing market setting constitutes a fundamental model of exchange
economies of goods. In most of the work concerning housing markets, it is
assumed that agents own and are allocated discrete houses. The drawback of this
assumption is that it does not cater for randomized assignments or allocation
of time-shares. Recently, house allocation with fractional endowment of houses
was considered by Athanassoglou and Sethuraman (2011) who posed the open
problem of generalizing Gale's Top Trading Cycles (TTC) algorithm to the case
of housing markets with fractional endowments. In this paper, we address the
problem and present a generalization of TTC called FTTC that is polynomial-time
as well as core stable and Pareto optimal with respect to stochastic dominance
even if there are indifferences in the preferences. We prove that if each agent
owns one discrete house, FTTC coincides with a state of the art strategyproof
mechanism for housing markets with discrete endowments and weak preferences.
  We show that FTTC satisfies a maximal set of desirable properties by proving
two impossibility theorems. Firstly, we prove that with respect to stochastic
dominance, core stability and no justified envy are incompatible. Secondly, we
prove that there exists no individual rational, Pareto optimal and weak
strategyproof mechanism, thereby answering another open problem posed by
Athanassoglou and Sethuraman (2011). The second impossibility implies a number
of results in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03917</identifier>
 <datestamp>2015-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03917</id><created>2015-09-13</created><updated>2015-11-15</updated><authors><author><keyname>Bhojanapalli</keyname><forenames>Srinadh</forenames></author><author><keyname>Kyrillidis</keyname><forenames>Anastasios</forenames></author><author><keyname>Sanghavi</keyname><forenames>Sujay</forenames></author></authors><title>Dropping Convexity for Faster Semi-definite Optimization</title><categories>stat.ML cs.DS cs.IT cs.LG cs.NA math.IT math.OC</categories><comments>42 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the minimization of a convex function $f(X)$ over the
space of $n\times n$ positive semidefinite matrices $X\succeq 0$, but when the
problem is recast as the non-convex problem $\min_U g(U)$ where $g(U) :=
f(UU^\top)$, with $U$ being an $n\times r$ matrix and $r\leq n$. We study the
performance of gradient descent on $g$ -- which we refer to as Factored
Gradient Descent (FGD) -- under standard assumptions on the original function
$f$.
  We provide a rule for selecting the step size, and with this choice show that
the local convergence rate of FGD mirrors that of standard gradient descent on
the original $f$ -- the error after $k$ steps is $O(1/k)$ for smooth $f$, and
exponentially small in $k$ when $f$ is (restricted) strongly convex. Note that
$g$ is not locally convex. In addition, we provide a procedure to initialize
FGD for (restricted) strongly convex objectives and when one only has access to
$f$ via a first-order oracle.
  FGD and similar procedures are widely used in practice for problems that can
be posed as matrix factorization; to the best of our knowledge, ours is the
first paper to provide precise convergence rate guarantees for general convex
functions under standard convex assumptions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03918</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03918</id><created>2015-09-13</created><authors><author><keyname>Barrio</keyname><forenames>Rafael A.</forenames></author><author><keyname>Govezensky</keyname><forenames>Tzipe</forenames></author><author><keyname>Dunbar</keyname><forenames>Robin</forenames></author><author><keyname>I&#xf1;iguez</keyname><forenames>Gerardo</forenames></author><author><keyname>Kaski</keyname><forenames>Kimmo</forenames></author></authors><title>Dynamics of deceptive interactions in social networks</title><categories>physics.soc-ph cs.CY cs.SI nlin.AO</categories><comments>17 pages, 8 figures; Supplementary Information (3 pages, 1 figure)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we examine the role of lies in human social relations by
implementing some salient characteristics of deceptive interactions into an
opinion formation model, so as to describe the dynamical behaviour of a social
network more realistically. In this model we take into account such basic
properties of social networks as the dynamics of the intensity of interactions,
the influence of public opinion, and the fact that in every human interaction
it might be convenient to deceive or withhold information depending on the
instantaneous situation of each individual in the network. We find that lies
shape the topology of social networks, especially the formation of tightly
linked, small communities with loose connections between them. We also find
that agents with a larger proportion of deceptive interactions are the ones
that connect communities of different opinion, and in this sense they have
substantial centrality in the network. We then discuss the consequences of
these results for the social behaviour of humans and predict the changes that
could arise due to a varying tolerance for lies in society.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03934</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03934</id><created>2015-09-13</created><updated>2015-09-15</updated><authors><author><keyname>Maloney</keyname><forenames>Sam</forenames></author></authors><title>Dpush: A scalable decentralized spam resistant unsolicited messaging
  protocol</title><categories>cs.CR cs.DC</categories><comments>Newest paper/design; signature was moved inside encrypted data for
  anonymity of sender to non recipients, grammar improvements</comments><acm-class>H.3.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Herein this paper is presented a novel invention - called Dpush - that
enables truly scalable spam resistant uncensorable automatically encrypted and
inherently authenticated messaging; thus restoring our ability to exert our
right to private communication, and thus a step forward in restoring an
uncorrupted democracy. Using a novel combination of a distributed hash table[1]
(DHT) and a proof of work[2] (POW), combined in a way that can only be called a
synergy, the emergent property of a scalable and spam resistant unsolicited
messaging protocol elegantly emerges. Notable is that the receiver does not
need to be online at the time the message is sent. This invention is already
implemented and operating within the package that is called MORPHiS - which is
a Sybil[3] resistant enhanced Kademlia[1] DHT implementation combined with an
already functioning implementation of Dpush, as well as a polished HTTP Dmail
interface to send and receive such messages today. MORPHiS is available for
free (GPLv2) at the https://morph.is website.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03936</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03936</id><created>2015-09-13</created><authors><author><keyname>Zhang</keyname><forenames>Zhanpeng</forenames></author><author><keyname>Luo</keyname><forenames>Ping</forenames></author><author><keyname>Loy</keyname><forenames>Chen Change</forenames></author><author><keyname>Tang</keyname><forenames>Xiaoou</forenames></author></authors><title>Learning Social Relation Traits from Face Images</title><categories>cs.CV cs.CY</categories><comments>To appear in International Conference on Computer Vision (ICCV) 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social relation defines the association, e.g, warm, friendliness, and
dominance, between two or more people. Motivated by psychological studies, we
investigate if such fine-grained and high-level relation traits can be
characterised and quantified from face images in the wild. To address this
challenging problem we propose a deep model that learns a rich face
representation to capture gender, expression, head pose, and age-related
attributes, and then performs pairwise-face reasoning for relation prediction.
To learn from heterogeneous attribute sources, we formulate a new network
architecture with a bridging layer to leverage the inherent correspondences
among these datasets. It can also cope with missing target attribute labels.
Extensive experiments show that our approach is effective for fine-grained
social relation learning in images and videos.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03937</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03937</id><created>2015-09-13</created><authors><author><keyname>Mehta</keyname><forenames>Ketan</forenames></author><author><keyname>Kliewer</keyname><forenames>J&#xf6;rg</forenames></author></authors><title>An Information Theoretic Approach Towards Assessing Perceptual Audio
  Quality using EEG</title><categories>cs.IT math.IT q-bio.NC</categories><comments>29 pages, 10 figures, IEEE Transactions on Molecular, Biological and
  Multi-scale Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a novel information theoretic model to interpret
the entire &quot;transmission chain&quot; comprising stimulus generation, brain
processing by the human subject, and the electroencephalograph (EEG) response
measurements as a nonlinear, time-varying communication channel with memory. We
use mutual information (MI) as a measure to assess audio quality perception by
directly measuring the brainwave responses of the human subjects using a high
resolution EEG. Our focus here is on audio where the quality is impaired by
time varying distortions. In particular, we conduct experiments where subjects
are presented with audio whose quality varies with time between different
possible quality levels. The recorded EEG measurements can be modeled as a
multidimensional Gaussian mixture model (GMM). In order to make the computation
of the MI feasible, we present a novel low-complexity approximation technique
for the differential entropy of the multidimensional GMM. We find the proposed
information theoretic approach to be successful in quantifying subjective audio
quality perception, with the results being consistent across different music
sequences and distortion types.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03942</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03942</id><created>2015-09-13</created><authors><author><keyname>Barbieri</keyname><forenames>Davide</forenames></author></authors><title>Geometry and dimensionality reduction of feature spaces in primary
  visual cortex</title><categories>q-bio.NC cs.CV math.GR</categories><doi>10.1117/12.2187026</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Some geometric properties of the wavelet analysis performed by visual neurons
are discussed and compared with experimental data. In particular, several
relationships between the cortical morphologies and the parametric dependencies
of extracted features are formalized and considered from a harmonic analysis
point of view.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03946</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03946</id><created>2015-09-14</created><authors><author><keyname>Kawahara</keyname><forenames>Yoshinobu</forenames></author><author><keyname>Yamaguchi</keyname><forenames>Yutaro</forenames></author></authors><title>Parametric Maxflows for Structured Sparse Learning with Convex
  Relaxations of Submodular Functions</title><categories>cs.LG cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The proximal problem for structured penalties obtained via convex relaxations
of submodular functions is known to be equivalent to minimizing separable
convex functions over the corresponding submodular polyhedra. In this paper, we
reveal a comprehensive class of structured penalties for which penalties this
problem can be solved via an efficiently solvable class of parametric maxflow
optimization. We then show that the parametric maxflow algorithm proposed by
Gallo et al. and its variants, which runs, in the worst-case, at the cost of
only a constant factor of a single computation of the corresponding maxflow
optimization, can be adapted to solve the proximal problems for those
penalties. Several existing structured penalties satisfy these conditions;
thus, regularized learning with these penalties is solvable quickly using the
parametric maxflow algorithm. We also investigate the empirical runtime
performance of the proposed framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03956</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03956</id><created>2015-09-14</created><authors><author><keyname>Solera</keyname><forenames>Francesco</forenames></author><author><keyname>Calderara</keyname><forenames>Simone</forenames></author><author><keyname>Cucchiara</keyname><forenames>Rita</forenames></author></authors><title>Learning to Divide and Conquer for Online Multi-Target Tracking</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online Multiple Target Tracking (MTT) is often addressed within the
tracking-by-detection paradigm. Detections are previously extracted
independently in each frame and then objects trajectories are built by
maximizing specifically designed coherence functions. Nevertheless, ambiguities
arise in presence of occlusions or detection errors. In this paper we claim
that the ambiguities in tracking could be solved by a selective use of the
features, by working with more reliable features if possible and exploiting a
deeper representation of the target only if necessary. To this end, we propose
an online divide and conquer tracker for static camera scenes, which partitions
the assignment problem in local subproblems and solves them by selectively
choosing and combining the best features. The complete framework is cast as a
structural learning task that unifies these phases and learns tracker
parameters from examples. Experiments on two different datasets highlights a
significant improvement of tracking performances (MOTA +10%) over the state of
the art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03966</identifier>
 <datestamp>2016-01-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03966</id><created>2015-09-14</created><updated>2016-01-24</updated><authors><author><keyname>Kumar</keyname><forenames>Animesh</forenames></author></authors><title>Bandlimited Spatial Field Sampling with Mobile Sensors in the Absence of
  Location Information</title><categories>cs.IT math.IT math.ST stat.TH</categories><comments>Submitted to IEEE Trans on Signal Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sampling of physical fields with mobile sensor is an emerging area. In this
context, this work introduces and proposes solutions to a fundamental question:
can a spatial field be estimated from samples taken at unknown sampling
locations?
  Unknown sampling location, sample quantization, unknown bandwidth of the
field, and presence of measurement-noise present difficulties in the process of
field estimation. In this work, except for quantization, the other three issues
will be tackled together in a mobile-sampling framework. Spatially bandlimited
fields are considered. It is assumed that measurement-noise affected field
samples are collected on spatial locations obtained from an unknown renewal
process. That is, the samples are obtained on locations obtained from a renewal
process, but the sampling locations and the renewal process distribution are
unknown. In this unknown sampling location setup, it is shown that the
mean-squared error in field estimation decreases as $O(1/n)$ where $n$ is the
average number of samples collected by the mobile sensor. The average number of
samples collected is determined by the inter-sample spacing distribution in the
renewal process. An algorithm to ascertain spatial field's bandwidth is
detailed, which works with high probability as the average number of samples
$n$ increases. This algorithm works in the same setup, i.e., in the presence of
measurement-noise and unknown sampling locations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03970</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03970</id><created>2015-09-14</created><authors><author><keyname>Gauvrit</keyname><forenames>Nicolas</forenames></author><author><keyname>Soler-Toscano</keyname><forenames>Fernando</forenames></author><author><keyname>Zenil</keyname><forenames>Hector</forenames></author></authors><title>Natural scene statistics mediate the perception of image complexity</title><categories>cs.AI cs.CV</categories><journal-ref>Visual Cognition 22 (8), 2014, pages 1084-1091</journal-ref><doi>10.1080/13506285.2014.950365</doi><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Humans are sensitive to complexity and regularity in patterns. The subjective
perception of pattern complexity is correlated to algorithmic
(Kolmogorov-Chaitin) complexity as defined in computer science, but also to the
frequency of naturally occurring patterns. However, the possible mediational
role of natural frequencies in the perception of algorithmic complexity remains
unclear. Here we reanalyze Hsu et al. (2010) through a mediational analysis,
and complement their results in a new experiment. We conclude that human
perception of complexity seems partly shaped by natural scenes statistics,
thereby establishing a link between the perception of complexity and the effect
of natural scene statistics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03976</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03976</id><created>2015-09-14</created><authors><author><keyname>Gast</keyname><forenames>Mikael</forenames></author><author><keyname>Hauptmann</keyname><forenames>Mathias</forenames></author><author><keyname>Karpinski</keyname><forenames>Marek</forenames></author></authors><title>Approximability of TSP on Power Law Graphs</title><categories>cs.DS cs.CC cs.DM math.CO math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the special case of Graphic TSP where the underlying
graph is a power law graph (PLG). We give a refined analysis of some of the
current best approximation algorithms and show that an improved approximation
ratio can be achieved for certain ranges of the power law exponent $\beta$. For
the value of power law exponent $\beta=1.5$ we obtain an approximation ratio of
$1.34$ for Graphic TSP. Moreover we study the $(1,2)$-TSP with the underlying
graph of $1$-edges being a PLG. We show improved approximation ratios in the
case of underlying deterministic PLGs for $\beta$ greater than $1.666$. For
underlying random PLGs we further improve the analysis and show even better
expected approximation ratio for the range of $\beta$ between $1$ and $3.5$. On
the other hand we prove the first explicit inapproximability bounds for
$(1,2)$-TSP for an underlying power law graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03977</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03977</id><created>2015-09-14</created><authors><author><keyname>Escandell-Montero</keyname><forenames>Pablo</forenames></author><author><keyname>Chermisi</keyname><forenames>Milena</forenames></author><author><keyname>Mart&#xed;nez-Mart&#xed;nez</keyname><forenames>Jos&#xe9; M.</forenames></author><author><keyname>G&#xf3;mez-Sanchis</keyname><forenames>Juan</forenames></author><author><keyname>Barbieri</keyname><forenames>Carlo</forenames></author><author><keyname>Soria-Olivas</keyname><forenames>Emilio</forenames></author><author><keyname>Mari</keyname><forenames>Flavio</forenames></author><author><keyname>Vila-Franc&#xe9;s</keyname><forenames>Joan</forenames></author><author><keyname>Stopper</keyname><forenames>Andrea</forenames></author><author><keyname>Gatti</keyname><forenames>Emanuele</forenames></author><author><keyname>Mart&#xed;n-Guerrero</keyname><forenames>Jos&#xe9; D.</forenames></author></authors><title>Optimization of anemia treatment in hemodialysis patients via
  reinforcement learning</title><categories>stat.ML cs.AI cs.LG</categories><comments>17 pages, 10 figures</comments><journal-ref>Artificial Intelligence in Medicine, Volume 62, Issue 1, September
  2014, Pages 47-60</journal-ref><doi>10.1016/j.artmed.2014.07.004</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Objective: Anemia is a frequent comorbidity in hemodialysis patients that can
be successfully treated by administering erythropoiesis-stimulating agents
(ESAs). ESAs dosing is currently based on clinical protocols that often do not
account for the high inter- and intra-individual variability in the patient's
response. As a result, the hemoglobin level of some patients oscillates around
the target range, which is associated with multiple risks and side-effects.
This work proposes a methodology based on reinforcement learning (RL) to
optimize ESA therapy.
  Methods: RL is a data-driven approach for solving sequential decision-making
problems that are formulated as Markov decision processes (MDPs). Computing
optimal drug administration strategies for chronic diseases is a sequential
decision-making problem in which the goal is to find the best sequence of drug
doses. MDPs are particularly suitable for modeling these problems due to their
ability to capture the uncertainty associated with the outcome of the treatment
and the stochastic nature of the underlying process. The RL algorithm employed
in the proposed methodology is fitted Q iteration, which stands out for its
ability to make an efficient use of data.
  Results: The experiments reported here are based on a computational model
that describes the effect of ESAs on the hemoglobin level. The performance of
the proposed method is evaluated and compared with the well-known Q-learning
algorithm and with a standard protocol. Simulation results show that the
performance of Q-learning is substantially lower than FQI and the protocol.
  Conclusion: Although prospective validation is required, promising results
demonstrate the potential of RL to become an alternative to current protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03979</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03979</id><created>2015-09-14</created><authors><author><keyname>Hsieh</keyname><forenames>Sung-Hsien</forenames></author><author><keyname>Lu</keyname><forenames>Chun-Shien</forenames></author><author><keyname>Pei</keyname><forenames>Soo-Chang</forenames></author></authors><title>Fast Greedy Approaches for Compressive Sensing of Large-Scale Signals</title><categories>cs.DS</categories><comments>10 pages, 3 figures, 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cost-efficient compressive sensing is challenging when facing large-scale
data, {\em i.e.}, data with large sizes. Conventional compressive sensing
methods for large-scale data will suffer from low computational efficiency and
massive memory storage. In this paper, we revisit well-known solvers called
greedy algorithms, including Orthogonal Matching Pursuit (OMP), Subspace
Pursuit (SP), Orthogonal Matching Pursuit with Replacement (OMPR). Generally,
these approaches are conducted by iteratively executing two main steps: 1)
support detection and 2) solving least square problem.
  To reduce the cost of Step 1, it is not hard to employ the sensing matrix
that can be implemented by operator-based strategy instead of matrix-based one
and can be speeded by fast Fourier Transform (FFT). Step 2, however, requires
maintaining and calculating a pseudo-inverse of a sub-matrix, which is random
and not structural, and, thus, operator-based matrix does not work. To overcome
this difficulty, instead of solving Step 2 by a closed-form solution, we
propose a fast and cost-effective least square solver, which combines a
Conjugate Gradient (CG) method with our proposed weighted least square problem
to iteratively approximate the ground truth yielded by a greedy algorithm.
Extensive simulations and theoretical analysis validate that the proposed
method is cost-efficient and is readily incorporated with the existing greedy
algorithms to remarkably improve the performance for large-scale problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03985</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03985</id><created>2015-09-14</created><updated>2016-02-15</updated><authors><author><keyname>Zhang</keyname><forenames>Rick</forenames></author><author><keyname>Rossi</keyname><forenames>Federico</forenames></author><author><keyname>Pavone</keyname><forenames>Marco</forenames></author></authors><title>Model Predictive Control of Autonomous Mobility-on-Demand Systems</title><categories>cs.SY</categories><comments>Extended version of ICRA16 paper, with full proofs of the theorems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a model predictive control (MPC) approach to
optimize vehicle scheduling and routing in an autonomous mobility-on-demand
(AMoD) system. In AMoD systems, robotic, self-driving vehicles transport
customers within an urban environment and are coordinated to optimize service
throughout the entire network. Specifically, we first propose a novel
discrete-time model of an AMoD system and we show that this formulation allows
the easy integration of a number of real-world constraints, e.g., electric
vehicle charging constraints. Second, leveraging our model, we design a model
predictive control algorithm for the optimal coordination of an AMoD system and
prove its stability in the sense of Lyapunov. At each optimization step, the
vehicle scheduling and routing problem is solved as a mixed integer linear
program (MILP) where the decision variables are binary variables representing
whether a vehicle will 1) wait at a station, 2) service a customer, or 3)
rebalance to another station. Finally, by using real-world data, we show that
the MPC algorithm can be run in real-time for moderately-sized systems and
outperforms previous control strategies for AMoD systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03988</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03988</id><created>2015-09-14</created><authors><author><keyname>Mekki</keyname><forenames>Kais</forenames><affiliation>CRAN</affiliation></author><author><keyname>Derigent</keyname><forenames>William</forenames><affiliation>CRAN</affiliation></author><author><keyname>Rondeau</keyname><forenames>Eric</forenames><affiliation>CRAN</affiliation></author><author><keyname>Zouinkhi</keyname><forenames>Ahmed</forenames></author><author><keyname>Abdelkrim</keyname><forenames>Mohamed Naceur</forenames></author></authors><title>An uniform data replication algorithm in wireless micro-sensor network
  for communicating materials application</title><categories>cs.NI</categories><comments>10th International Conference on Modeling, Optimization \&amp; Simulation
  (MOSIM'2014), Nov 2014, Nancy, France. 2014</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless sensor networks (WSN) have recently gained a great deal of attention
as a topic of research, with a wide range of applications being explored such
as communicating materials. Data dissemination and storage are very important
issues for sensor networks. The problem of designing data dissemination
protocols for communicating material needs different analyses related to
storage density and uniformity which has not been addressed sufficiently in the
literature. This paper details storage protocol on the material by systematic
dissemination through integrated wireless micro-sensors nodes. The performances
of our solutions are evaluated through simulation using Castalia/OMNeT++. The
results show that our algorithm provides uniform data storage in communicating
material for different density level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03990</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03990</id><created>2015-09-14</created><authors><author><keyname>Garg</keyname><forenames>Shivam</forenames></author><author><keyname>Philip</keyname><forenames>Geevarghese</forenames></author></authors><title>Raising The Bar For Vertex Cover: Fixed-parameter Tractability Above A
  Higher Guarantee</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the following above-guarantee parameterization of the
classical Vertex Cover problem: Given a graph $G$ and $k\in\mathbb{N}$ as
input, does $G$ have a vertex cover of size at most $(2LP-MM)+k$? Here $MM$ is
the size of a maximum matching of $G$, $LP$ is the value of an optimum solution
to the relaxed (standard) LP for Vertex Cover on $G$, and $k$ is the parameter.
Since $(2LP-MM)\geq{LP}\geq{MM}$, this is a stricter parameterization than
those---namely, above-$MM$, and above-$LP$---which have been studied so far.
  We prove that Vertex Cover is fixed-parameter tractable for this stricter
parameter $k$: We derive an algorithm which solves Vertex Cover in time
$O^{*}(3^{k})$, pushing the envelope further on the parameterized tractability
of Vertex Cover.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03992</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03992</id><created>2015-09-14</created><authors><author><keyname>Luo</keyname><forenames>Yuan</forenames></author><author><keyname>Gao</keyname><forenames>Lin</forenames></author><author><keyname>Huang</keyname><forenames>Jianwei</forenames></author></authors><title>MINE GOLD to Deliver Green Cognitive Communications</title><categories>cs.NI cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Geo-location database-assisted TV white space network reduces the need of
energy-intensive processes (such as spectrum sensing), hence can achieve green
cognitive communication effectively. The success of such a network relies on a
proper business model that provides incentives for all parties involved. In
this paper, we propose MINE GOLD (a Model of INformation markEt for
GeO-Location Database), which enables databases to sell the spectrum
information to unlicensed white space devices (WSDs) for profit. Specifically,
we focus on an oligopoly information market with multiple databases, and study
the interactions among databases and WSDs using a two-stage hierarchical model.
In Stage I, databases compete to sell information to WSDs by optimizing their
information prices. In Stage II, each WSD decides whether and from which
database to purchase the information, to maximize his benefit of using the TV
white space. We first characterize how the WSDs' purchasing behaviors
dynamically evolve, and what is the equilibrium point under fixed information
prices from the databases. We then analyze how the system parameters and the
databases' pricing decisions affect the market equilibrium, and what is the
equilibrium of the database price competition. Our numerical results show that,
perhaps counter-intuitively, the databases' aggregate revenue is not monotonic
with the number of databases. Moreover, numerical results show that a large
degree of positive network externality would improve the databases' revenues
and the system performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.03998</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.03998</id><created>2015-09-14</created><authors><author><keyname>Nguyen</keyname><forenames>Hieu Duy</forenames></author><author><keyname>Sun</keyname><forenames>Sumei</forenames></author></authors><title>Massive MIMO versus Small-Cell Systems: Spectral and Energy Efficiency
  Comparison</title><categories>cs.IT math.IT</categories><comments>11 pages, 9 figures. Submitted to IEEE Transactions on Wireless
  Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the downlink performance of two important 5G network
architectures, i.e. massive multiple-input multiple-output (M-MIMO) and
small-cell densification. We propose a comparative modeling for the two
systems, where the user and antenna/base station (BS) locations are distributed
according to Poisson point processes (PPPs). We then leverage both the
stochastic geometry results and large-system analytical tool to study the SIR
distribution and the average Shannon and outage rates of each network. By
comparing these results, we observe that for user-average spectral efficiency,
small-cell densification is favorable in crowded areas with moderate to high
user density and massive MIMO with low user density. However, small-cell
systems outperform M-MIMO in all cases when the performance metric is the
energy efficiency. The results of this paper are useful for the optimal design
of practical 5G networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04002</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04002</id><created>2015-09-14</created><authors><author><keyname>Nguyen</keyname><forenames>Hieu Duy</forenames></author><author><keyname>Sun</keyname><forenames>Sumei</forenames></author></authors><title>Stochastic Geometry-Based Performance Bounds for Non-Fading and Rayleigh
  Fading Ad Hoc Networks</title><categories>cs.IT math.IT</categories><comments>11 pages, 9 figures. Submitted to IEEE Transactions on Wireless
  Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the performance of non-fading and Rayleigh fading ad
hoc networks. We first characterize the distribution of the
signal-to-interference-plus-noise ratio (SINR) through the Laplace transform of
the inverted SINR for non-fading channels. Since most communication systems are
interference-limited, we also consider the case of negligible noise power, and
derive the upper and lower bounds for the signal-to-interference ratio (SIR)
distribution under both non-fading and fading cases. These bounds are of closed
forms and thus more convenient for theoretical analysis. Based on these
derivations, we obtain closed-form bounds for both the average Shannon and
outage rates. We also leverage the above results to study partial fading ad-hoc
systems. These results are useful for investigating and comparing
fifth-generation communication systems, for example massive multi-antenna and
small-cell networks as in our illustrative example.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04006</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04006</id><created>2015-09-14</created><authors><author><keyname>Endo</keyname><forenames>Hiroyuki</forenames></author><author><keyname>Han</keyname><forenames>Te Sun</forenames></author><author><keyname>Aoki</keyname><forenames>Takao</forenames></author><author><keyname>Sasaki</keyname><forenames>Masahide</forenames></author></authors><title>Numerical Study on Secrecy Capacity and Code Length Dependence of the
  Performances in Optical Wiretap Channels</title><categories>cs.IT math.IT quant-ph</categories><comments>12 pages, 17 figures</comments><journal-ref>IEEE Photonics Journal, vol.7, no.5, 7903418, October, 2015</journal-ref><doi>10.1109/JPHOT.2015.2472281</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Secrecy issues of free-space optical links realizing information
theoretically secure communications as well as high transmission rates are
discussed. We numerically study secrecy communication rates of optical wiretap
channel based on on-off keying modulation under typical conditions met in
satellite-ground links. It is shown that under reasonable degraded conditions
on a wiretapper, information theoretically secure communications should be
possible in a much wider distance range than a range limit of quantum key
distribution, enabling secure optical links between geostationary earth orbit
satellites and ground stations with currently available technologies. We also
provide the upper bounds on the decoding error probability and the leaked
information to estimate a necessary code length for given required levels of
performances. This result ensures that a reasonable length wiretap channel code
for our proposed scheme must exist.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04020</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04020</id><created>2015-09-14</created><updated>2015-09-15</updated><authors><author><keyname>Mustafa</keyname><forenames>Nabil H.</forenames></author></authors><title>A Note on the Size-Sensitive Packing Lemma</title><categories>cs.CG</categories><comments>Modified title of the paper. 2 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the size-sensitive packing lemma follows from a simple
modification of the standard proof, due to Haussler and simplified by Chazelle,
of the packing lemma.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04037</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04037</id><created>2015-09-14</created><authors><author><keyname>Aref</keyname><forenames>Samin</forenames></author><author><keyname>Wilson</keyname><forenames>Mark C.</forenames></author></authors><title>Measuring Partial Balance in Signed Networks</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Is the enemy of an enemy necessarily a friend, or a friend of a friend a
friend? If not, to what extent does this tend to hold? Such questions were
formulated in terms of signed (social) networks and necessary and sufficient
conditions for a network to be &quot;balanced&quot; were obtained around 1960. Since then
the idea that signed networks tend over time to become more balanced has been
widely used in several application areas, such as international relations.
However investigation of this hypothesis has been complicated by the lack of a
standard measure of partial balance, since complete balance is almost never
achieved in practice.
  We formalise the concept of a measure of partial balance, compare several
known measures on real-world and synthetic datasets, as well as investigating
their axiomatic properties. We use both well-known datasets from the sociology
literature, such as Read's New Guinean tribes, and much more recent ones
involving senate bill co-sponsorship. The synthetic data involves both
Erd\H{o}s-R\'enyi and Barab\'asi-Albert graphs.
  We find that under all our measures, real-world networks are more balanced
than random networks. We also show that some measures behave better than others
in terms of axioms, computational tractability and ability to differentiate
between graphs. We make some recommendations for measures to be used in future
work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04040</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04040</id><created>2015-09-14</created><authors><author><keyname>Steensgaard-Madsen</keyname><forenames>J&#xf8;rgen</forenames></author></authors><title>Programs as proofs</title><categories>cs.PL</categories><acm-class>D.3.1; D.3.3; F.3.1</acm-class><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  The Curry-Howard correspondence is about a relationship between types and
programs on the one hand and propositions and proofs on the other. The
implications for programming language design and program verification is an
active field of research.
  Transformer-like semantics of internal definitions that combine a defining
computation and an application will be presented. By specialisation for a given
defining computation one can derive inference rules for applications of defined
operations.
  With semantics of that kind for every operation, each application identifies
an axiom in a logic defined by the programming language, so a language can be
considered a theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04043</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04043</id><created>2015-09-14</created><authors><author><keyname>Filippou</keyname><forenames>Miltiades C.</forenames></author><author><keyname>Ropokis</keyname><forenames>George A.</forenames></author><author><keyname>Gesbert</keyname><forenames>David</forenames></author><author><keyname>Ratnarajah</keyname><forenames>Tharmalingam</forenames></author></authors><title>Joint Sensing and Reception Design of SIMO Hybrid Cognitive Radio
  Systems</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the problem of joint design of Spectrum Sensing (SS) and
receive beamforming (BF), with reference to a Cognitive Radio (CR) system, is
considered. The aim of the proposed design is the maximization of the
achievable average uplink rate of a Secondary User (SU), subject to an
outage-based Quality-of-Service (QoS) constraint for primary communication. A
hybrid CR system approach is studied, according to which, the system either
operates as an interweave (i.e., opportunistic) or as an underlay (i.e.,
spectrum sharing) CR system, based on SS results. A realistic Channel State
Information (CSI) framework is assumed, according to which, the direct channel
links are known by the multiple antenna receivers (RXs), while, merely
statistical (covariance) information is available for the interference links. A
new, closed form approximation is derived for the outage probability of primary
communication, and the problem of rate-optimal selection of SS parameters and
receive beamformers is addressed for hybrid, interweave and underlay CR
systems. It is proven that our proposed system design outperforms both underlay
and interweave CR systems for a range of system scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04048</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04048</id><created>2015-09-14</created><updated>2015-09-15</updated><authors><author><keyname>Kumar</keyname><forenames>Priyanka</forenames></author><author><keyname>Peri</keyname><forenames>Sathya</forenames></author></authors><title>Multiversion Conflict Notion for Transactional Memory Systems</title><categories>cs.DC</categories><comments>19 pages. arXiv admin note: substantial text overlap with
  arXiv:1307.8256</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, Software Transactional Memory systems (STMs) have garnered
significant interest as an elegant alternative for addressing concurrency
issues in memory. STM systems take optimistic approach. Multiple transactions
are allowed to execute concurrently. On completion, each transaction is
validated and if any inconsistency is observed it is aborted. Otherwise it is
allowed to commit.
  In databases a class of histories called as conflict-serializability (CSR)
based on the notion of conflicts have been identified, whose membership can be
efficiently verified. As a result, CSR is the commonly used correctness
criterion in databases. Similarly, using the notion of conflicts, a correctness
criterion, conflict-opacity (co-opacity) which is a sub-class of can be
designed whose membership can be verified in polynomial time. Using the
verification mechanism, an efficient STM implementation can be designed that is
permissive w.r.t co-opacity.
  By storing multiple versions for each transaction object, multi-version STMs
provide more concurrency than single-version STMs. But the main drawback of
co-opacity is that it does not admit histories that are uses multiple versions.
This has motivated us to develop a new conflict notions for multi-version STMs.
In this paper, we present a new conflict notion multi-version conflict. Using
this conflict notion, we identify a new subclass of opacity (a popular
correctness-criterion), mvc-opacity that admits multi-versioned histories and
whose membership can be verified in polynomial time. We show that co-opacity is
a proper subset of this class. The proposed conflict notion mv-conflict can be
applied on non-sequential histories as well unlike traditional conflicts.
Further, we believe that this conflict notion can be easily extended to other
correctness-criterion as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04064</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04064</id><created>2015-09-14</created><authors><author><keyname>Castronovo</keyname><forenames>Michael</forenames></author><author><keyname>Ernst</keyname><forenames>Damien</forenames></author><author><keyname>Couetoux</keyname><forenames>Adrien</forenames></author><author><keyname>Fonteneau</keyname><forenames>Raphael</forenames></author></authors><title>Benchmarking for Bayesian Reinforcement Learning</title><categories>cs.AI</categories><comments>37 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the Bayesian Reinforcement Learning (BRL) setting, agents try to maximise
the collected rewards while interacting with their environment while using some
prior knowledge that is accessed beforehand. Many BRL algorithms have already
been proposed, but even though a few toy examples exist in the literature,
there are still no extensive or rigorous benchmarks to compare them. The paper
addresses this problem, and provides a new BRL comparison methodology along
with the corresponding open source library. In this methodology, a comparison
criterion that measures the performance of algorithms on large sets of Markov
Decision Processes (MDPs) drawn from some probability distributions is defined.
In order to enable the comparison of non-anytime algorithms, our methodology
also includes a detailed analysis of the computation time requirement of each
algorithm. Our library is released with all source code and documentation: it
includes three test problems, each of which has two different prior
distributions, and seven state-of-the-art RL algorithms. Finally, our library
is illustrated by comparing all the available algorithms and the results are
discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04072</identifier>
 <datestamp>2015-11-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04072</id><created>2015-09-14</created><updated>2015-11-16</updated><authors><author><keyname>W&#xfc;thrich</keyname><forenames>Manuel</forenames></author><author><keyname>Cifuentes</keyname><forenames>Cristina Garcia</forenames></author><author><keyname>Trimpe</keyname><forenames>Sebastian</forenames></author><author><keyname>Meier</keyname><forenames>Franziska</forenames></author><author><keyname>Bohg</keyname><forenames>Jeannette</forenames></author><author><keyname>Issac</keyname><forenames>Jan</forenames></author><author><keyname>Schaal</keyname><forenames>Stefan</forenames></author></authors><title>Robust Gaussian Filtering</title><categories>stat.ML cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most widely-used state estimation algorithms, such as the Extended Kalman
Filter and the Unscented Kalman Filter, belong to the family of Gaussian
Filters (GF). Unfortunately, GFs fail if the measurement process is modelled by
a fat-tailed distribution. This is a severe limitation, because thin-tailed
measurement models, such as the analytically-convenient and therefore
widely-used Gaussian distribution, are sensitive to outliers. In this paper, we
show that mapping the measurements into a specific feature space enables any
existing GF algorithm to work with fat-tailed measurement models. We find a
feature function which is optimal under certain conditions. Simulation results
show that the proposed method allows for robust filtering in both linear and
nonlinear systems with measurements contaminated by fat-tailed noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04075</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04075</id><created>2015-09-14</created><authors><author><keyname>Ren</keyname><forenames>Zhijie</forenames></author><author><keyname>Goseling</keyname><forenames>Jasper</forenames></author><author><keyname>Weber</keyname><forenames>Jos H.</forenames></author><author><keyname>Gastpar</keyname><forenames>Michael</forenames></author></authors><title>Secure Transmission on the Two-hop Relay Channel with Scaled
  Compute-and-Forward</title><categories>cs.IT math.IT</categories><comments>This paper has been submitted to the IEEE transactions on information
  theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider communication on a two-hop channel, in which a
source wants to send information reliably and securely to the destination via a
relay. We consider both the untrusted relay case and the external eavesdropper
case. In the untrusted relay case, the relay behaves as an eavesdropper and
there is a cooperative node which sends a jamming signal to confuse the relay
when the it is receiving from the source. We propose two secure transmission
schemes using the scaled compute-and-forward technique. One of the schemes is
based on a random binning code and the other one is based on a lattice chain
code. It is proved that in either the high Signal-to-Noise-Ratio (SNR) scenario
and/or the restricted relay power scenario, if the destination is used as the
jammer, both schemes outperform all existing schemes and achieve the upper
bound. In particular, if the SNR is large and the source, the relay, and the
cooperative jammer have identical power and channels, both schemes achieve the
upper bound for secrecy rate, which is merely $1/2$ bit per channel use lower
than the channel capacity without secrecy constraints. We also prove that one
of our schemes achieves a positive secrecy rate in the external eavesdropper
case in which the relay is trusted and there exists an external eavesdropper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04076</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04076</id><created>2015-09-14</created><authors><author><keyname>B&#xf6;hmer</keyname><forenames>Kristof</forenames></author><author><keyname>Rinderle-Ma</keyname><forenames>Stefanie</forenames></author></authors><title>A systematic literature review on process model testing: Approaches,
  challenges, and research directions</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Testing is a key concern when developing process-oriented solutions as it
supports modeling experts who have to deal with increasingly complex models and
scenarios such as cross-organizational processes. However, the complexity of
the research landscape and the diverse set of approaches and goals impedes the
analysis and advancement of research and the identification of promising
research areas, challenges, and research directions. Hence, a systematic
literature review is conducted to identify interesting areas for future
research and to provide an overview of existing work. Over 6300 potentially
matching publications were determined during the search (literature databases,
selected conferences\journals, and snowballing). Finally, 153 publications from
2002 to 2013 were selected, analyzed, and classified. It was found that the
software engineering domain has influenced process model testing approaches
(e.g., regarding terminology and concepts), but recent publications are
presenting independent approaches. Additionally, historical data sources are
not exploited to their full potential and current testing related publications
frequently contain evaluations of relatively weak quality. Overall, the
publication landscape is unevenly distributed so that over 31 publications
concentrate on test-case generation but only 4 publications conduct performance
test. Hence, the full potential of such insufficiently covered testing areas is
not exploited. This systematic review provides a comprehensive overview of the
interdisciplinary topic of process model testing. Several open research
questions are identified, for example, how to apply testing to
cross-organizational or legacy processes and how to adequately include users
into the testing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04085</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04085</id><created>2015-09-14</created><authors><author><keyname>Kotselidis</keyname><forenames>Christos</forenames></author><author><keyname>Rodchenko</keyname><forenames>Andrey</forenames></author><author><keyname>Barrett</keyname><forenames>Colin</forenames></author><author><keyname>Nisbet</keyname><forenames>Andy</forenames></author><author><keyname>Mawer</keyname><forenames>John</forenames></author><author><keyname>Toms</keyname><forenames>Will</forenames></author><author><keyname>Clarkson</keyname><forenames>James</forenames></author><author><keyname>Gorgovan</keyname><forenames>Cosmin</forenames></author><author><keyname>d'Antras</keyname><forenames>Amanieu</forenames></author><author><keyname>Cakmakci</keyname><forenames>Yaman</forenames></author><author><keyname>Stratikopoulos</keyname><forenames>Thanos</forenames></author><author><keyname>Werner</keyname><forenames>Sebastian</forenames></author><author><keyname>Garside</keyname><forenames>Jim</forenames></author><author><keyname>Navaridas</keyname><forenames>Javier</forenames></author><author><keyname>Pop</keyname><forenames>Antoniu</forenames></author><author><keyname>Goodacre</keyname><forenames>John</forenames></author><author><keyname>Lujan</keyname><forenames>Mikel</forenames></author></authors><title>Project Beehive: A Hardware/Software Co-designed Stack for Runtime and
  Architectural Research</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The end of Dennard scaling combined with stagnation in architectural and
compiler optimizations makes it challenging to achieve significant performance
deltas. Solutions based solely in hardware or software are no longer sufficient
to maintain the pace of improvements seen during the past few decades. In
hardware, the end of single-core scaling resulted in the proliferation of
multi-core system architectures, however this has forced complex parallel
programming techniques into the mainstream. To further exploit physical
resources, systems are becoming increasingly heterogeneous with specialized
computing elements and accelerators. Programming across a range of disparate
architectures requires a new level of abstraction that programming languages
will have to adapt to. In software, emerging complex applications, from domains
such as Big Data and computer vision, run on multi-layered software stacks
targeting hardware with a variety of constraints and resources. Hence,
optimizing for the power-performance (and resiliency) space requires
experimentation platforms that offer quick and easy prototyping of
hardware/software co-designed techniques. To that end, we present Project
Beehive: A Hardware/Software co-designed stack for runtime and architectural
research. Project Beehive utilizes various state-of-the-art software and
hardware components along with novel and extensible co-design techniques. The
objective of Project Beehive is to provide a modern platform for
experimentation on emerging applications, programming languages, compilers,
runtimes, and low-power heterogeneous many-core architectures in a full-system
co-designed manner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04098</identifier>
 <datestamp>2015-11-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04098</id><created>2015-09-14</created><updated>2015-11-10</updated><authors><author><keyname>Cresci</keyname><forenames>Stefano</forenames></author><author><keyname>Di Pietro</keyname><forenames>Roberto</forenames></author><author><keyname>Petrocchi</keyname><forenames>Marinella</forenames></author><author><keyname>Spognardi</keyname><forenames>Angelo</forenames></author><author><keyname>Tesconi</keyname><forenames>Maurizio</forenames></author></authors><title>Fame for sale: efficient detection of fake Twitter followers</title><categories>cs.SI cs.CR cs.LG</categories><acm-class>H.2.8</acm-class><journal-ref>Decision Support Systems, 80, 56-71 (2015)</journal-ref><doi>10.1016/j.dss.2015.09.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  $\textit{Fake followers}$ are those Twitter accounts specifically created to
inflate the number of followers of a target account. Fake followers are
dangerous for the social platform and beyond, since they may alter concepts
like popularity and influence in the Twittersphere - hence impacting on
economy, politics, and society. In this paper, we contribute along different
dimensions. First, we review some of the most relevant existing features and
rules (proposed by Academia and Media) for anomalous Twitter accounts
detection. Second, we create a baseline dataset of verified human and fake
follower accounts. Such baseline dataset is publicly available to the
scientific community. Then, we exploit the baseline dataset to train a set of
machine-learning classifiers built over the reviewed rules and features. Our
results show that most of the rules proposed by Media provide unsatisfactory
performance in revealing fake followers, while features proposed in the past by
Academia for spam detection provide good results. Building on the most
promising features, we revise the classifiers both in terms of reduction of
overfitting and cost for gathering the data needed to compute the features. The
final result is a novel $\textit{Class A}$ classifier, general enough to thwart
overfitting, lightweight thanks to the usage of the less costly features, and
still able to correctly classify more than 95% of the accounts of the original
training set. We ultimately perform an information fusion-based sensitivity
analysis, to assess the global sensitivity of each of the features employed by
the classifier. The findings reported in this paper, other than being supported
by a thorough experimental methodology and interesting on their own, also pave
the way for further investigation on the novel issue of fake Twitter followers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04110</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04110</id><created>2015-09-14</created><authors><author><keyname>Amer</keyname><forenames>Ramy</forenames></author><author><keyname>El-Sherif</keyname><forenames>Amr A.</forenames></author><author><keyname>Ebrahim</keyname><forenames>Hanaa</forenames></author><author><keyname>Mokhtar</keyname><forenames>Amr</forenames></author></authors><title>Cooperative Cognitive Radio Network with Energy Harvesting: Stability
  Analysis</title><categories>cs.IT math.IT</categories><comments>7 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the maximum stable throughput of a cooperative
cognitive radio system with energy harvesting Primary User and Secondary User.
Each PU and SU has a data queue for data storage and a battery for energy
storage. These batteries harvest energy from the environment and store it for
data transmission in next time slots. The SU is allowed to access the PU
channel only when the PU is idle. The SU cooperates with the PU for its data
transmission, getting mutual benefits for both users, such that, the PU
exploits the SU power to relay a fraction of its undelivered packets, and the
SU gets more opportunities to access idle time slots.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04115</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04115</id><created>2015-09-14</created><authors><author><keyname>Je</keyname><forenames>Changsoo</forenames></author><author><keyname>Lee</keyname><forenames>Sang Wook</forenames></author><author><keyname>Park</keyname><forenames>Rae-Hong</forenames></author></authors><title>Color-Phase Analysis for Sinusoidal Structured Light in Rapid Range
  Imaging</title><categories>cs.CV cs.GR physics.optics</categories><comments>6 pages, 12 figures. 6th Asian Conference on Computer Vision (ACCV
  2004)</comments><acm-class>I.2.10; I.4.8</acm-class><journal-ref>Proc. 6th Asian Conference on Computer Vision (ACCV 2004), vol. 1,
  pp. 270-275, Jeju Island, Korea, January 27, 2004</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Active range sensing using structured-light is the most accurate and reliable
method for obtaining 3D information. However, most of the work has been limited
to range sensing of static objects, and range sensing of dynamic (moving or
deforming) objects has been investigated recently only by a few researchers.
Sinusoidal structured-light is one of the well-known optical methods for 3D
measurement. In this paper, we present a novel method for rapid high-resolution
range imaging using color sinusoidal pattern. We consider the real-world
problem of nonlinearity and color-band crosstalk in the color light projector
and color camera, and present methods for accurate recovery of color-phase. For
high-resolution ranging, we use high-frequency patterns and describe new
unwrapping algorithms for reliable range recovery. The experimental results
demonstrate the effectiveness of our methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04116</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04116</id><created>2015-09-14</created><authors><author><keyname>Forejt</keyname><forenames>Vojt&#x11b;ch</forenames></author><author><keyname>Kr&#x10d;&#xe1;l</keyname><forenames>Jan</forenames></author><author><keyname>K&#x159;et&#xed;nsk&#xfd;</keyname><forenames>Jan</forenames></author></authors><title>Controller synthesis for MDPs and Frequency LTL$\setminus$GU</title><categories>cs.LO</categories><comments>Extended version of a paper presented at LPAR 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantitative extensions of temporal logics have recently attracted
significant attention. In this work, we study frequency LTL (fLTL), an
extension of LTL which allows to speak about frequencies of events along an
execution. Such an extension is particularly useful for probabilistic systems
that often cannot fulfil strict qualitative guarantees on the behaviour. It has
been recently shown that controller synthesis for Markov decision processes and
fLTL is decidable when all the bounds on frequencies are 1. As a step towards a
complete quantitative solution, we show that the problem is decidable for the
fragment fLTL$\setminus$GU, where U does not occur in the scope of G (but still
F can). Our solution is based on a novel translation of such quantitative
formulae into equivalent deterministic automata.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04119</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04119</id><created>2015-09-14</created><authors><author><keyname>Maya</keyname><forenames>Juan Augusto</forenames></author><author><keyname>Galarza</keyname><forenames>Cecilia G.</forenames></author><author><keyname>Vega</keyname><forenames>Leonardo Rey</forenames></author></authors><title>Exploiting Spatial Correlation in Energy Constrained Distributed
  Detection</title><categories>cs.IT math.IT</categories><comments>This paper was submitted to IEEE Transactions on Signal Processing.
  Ancillary files are available for this paper to show some symbolic
  expressions using the MATLAB script &quot;SymExprForPfaPm.m&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the detection of a correlated random process immersed in noise in
a wireless sensor network. Each node has an individual energy constraint and
the communication with the processing central units are affected by the path
loss propagation effect. Guided by energy efficiency concerns, we consider the
partition of the whole network into clusters, each one with a coordination node
or \emph{cluster head}. Thus, the nodes transmit their measurements to the
corresponding cluster heads, which after some processing, communicate a summary
of the received information to the fusion center, which takes the final
decision about the state of the nature. As the network has a fixed size,
communication within smaller clusters will be less affected by the path loss
effect, reducing energy consumption in the information exchange process between
nodes and cluster heads. However, this limits the capability of the network of
beneficially exploiting the spatial correlation of the process, specially when
the spatial correlation coherence of the process is of the same scale as the
clusters size. Therefore, a trade-off is established between the energy
efficiency and the beneficial use of spatial correlation. The study of this
trade-off is the main goal of this paper. We derive tight approximations of the
false alarm and miss-detection error probabilities under the Neyman-Pearson
framework for the above scenario. We also consider the application of these
results to a particular network and correlation model obtaining closed form
expressions. Finally, we validate the results for more general network and
correlation models through numerical simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04125</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04125</id><created>2015-09-14</created><authors><author><keyname>Mattila</keyname><forenames>Robert</forenames></author><author><keyname>Mo</keyname><forenames>Yilin</forenames></author><author><keyname>Murray</keyname><forenames>Richard M.</forenames></author></authors><title>An Iterative Abstraction Algorithm for Reactive Correct-by-Construction
  Controller Synthesis</title><categories>cs.SY</categories><comments>A shorter version has been accepted for publication in the 54th IEEE
  Conference on Decision and Control (held Tuesday through Friday, December
  15-18, 2015 at the Osaka International Convention Center, Osaka, Japan)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the problem of synthesizing
correct-by-construction controllers for discrete-time dynamical systems. A
commonly adopted approach in the literature is to abstract the dynamical system
into a Finite Transition System (FTS) and thus convert the problem into a two
player game between the environment and the system on the FTS. The controller
design problem can then be solved using synthesis tools for general linear
temporal logic or generalized reactivity(1) specifications. In this article, we
propose a new abstraction algorithm. Instead of generating a single FTS to
represent the system, we generate two FTSs, which are under- and
over-approximations of the original dynamical system. We further develop an
iterative abstraction scheme by exploiting the concept of winning sets, i.e.,
the sets of states for which there exists a winning strategy for the system.
Finally, the efficiency of the new abstraction algorithm is illustrated by
numerical examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04153</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04153</id><created>2015-09-14</created><authors><author><keyname>van Delft</keyname><forenames>Bart</forenames></author><author><keyname>Bubel</keyname><forenames>Richard</forenames></author></authors><title>Dependency-Based Information Flow Analysis with Declassification in a
  Program Logic</title><categories>cs.LO cs.CR</categories><comments>Technical Report; 23 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a deductive approach for the analysis of secure information flows
with support for fine-grained policies that include declassifications in the
form of delimited information release. By explicitly tracking the dependencies
of program locations as a computation history, we maintain high precision,
while avoiding the need for comparing independent program runs. By considering
an explicit heap model, we argue that the proposed analysis can
straightforwardly be applied on object-oriented programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04172</identifier>
 <datestamp>2015-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04172</id><created>2015-09-14</created><updated>2015-10-01</updated><authors><author><keyname>Shokri-Ghadikolaei</keyname><forenames>Hossein</forenames></author><author><keyname>Fischione</keyname><forenames>Carlo</forenames></author></authors><title>Millimeter Wave Ad Hoc Networks: Noise-limited or Interference-limited?</title><categories>cs.IT cs.NI math.IT</categories><comments>accepted in IEEE GLOBECOM'15</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In millimeter wave (mmWave) communication systems, narrow beam operations
overcome severe channel attenuations, reduce multiuser interference, and thus
introduce the new concept of noise-limited mmWave wireless networks. The regime
of the network, whether noise-limited or interference-limited, heavily reflects
on the medium access control (MAC) layer throughput and on proper resource
allocation and interference management strategies. Yet, alternating presence of
these regimes and, more importantly, their dependence on the mmWave design
parameters are ignored in the current approaches to mmWave MAC layer design,
with the potential disastrous consequences on the throughput/delay performance.
In this paper, tractable closed-form expressions for collision probability and
MAC layer throughput of mmWave networks, operating under slotted ALOHA and
TDMA, are derived. The new analysis reveals that mmWave networks may exhibit a
non-negligible transitional behavior from a noise-limited regime to an
interference-limited regime, depending on the density of the transmitters,
density and size of obstacles, transmission probability, beamwidth, and
transmit power. It is concluded that a new framework of adaptive hybrid
resource allocation procedure, containing a proactive contention-based phase
followed by a reactive contention-free one with dynamic phase durations, is
necessary to cope with such transitional behavior.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04184</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04184</id><created>2015-09-14</created><authors><author><keyname>Gonz&#xe1;lez-Valiente</keyname><forenames>C. L.</forenames></author></authors><title>Emerging trends on the topic of Information Technology in the field of
  Educational Sciences: a bibliometric exploration</title><categories>cs.DL</categories><journal-ref>Education in the Knowledge Society (EKS); 2015, 16(3)</journal-ref><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The paper presents a bibliometric analysis on the topic of Information
Technology (IT) in the field of Educational Sciences, aimed at envisioning the
research emerging trends. The ERIC data base is used as a consultation source;
the results were subjected to productivity by authors, journals, and term
co-occurrence analysis indicators for the period 2009-2013. The productivity of
Computers &amp; Education, and Turkish Online Journal of Educational
Technology-TOJET, as well as the preceding authors from Canada, have been
emphasized. The more used terms are the following: Information technology,
foreign countries, educational technology, technology integration, and student
attitudes. Researches performed here seem to have a largely qualitative
character, highlighting computers and internet as the mostly explored
technological objects. The largest subject matter trend refers to the
integration of IT in the higher education learning context, and its incidence
over the teaching methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04186</identifier>
 <datestamp>2016-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04186</id><created>2015-09-14</created><updated>2016-02-25</updated><authors><author><keyname>Sharma</keyname><forenames>Gaurav</forenames></author><author><keyname>Jurie</keyname><forenames>Frederic</forenames></author><author><keyname>Schmid</keyname><forenames>Cordelia</forenames></author></authors><title>Expanded Parts Model for Semantic Description of Humans in Still Images</title><categories>cs.CV</categories><comments>Accepted for publication in IEEE Transactions on Pattern Analysis and
  Machine Intelligence (TPAMI)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce an Expanded Parts Model (EPM) for recognizing human attributes
(e.g. young, short hair, wearing suit) and actions (e.g. running, jumping) in
still images. An EPM is a collection of part templates which are learnt
discriminatively to explain specific scale-space regions in the images (in
human centric coordinates). This is in contrast to current models which consist
of a relatively few (i.e. a mixture of) 'average' templates. EPM uses only a
subset of the parts to score an image and scores the image sparsely in space,
i.e. it ignores redundant and random background in an image. To learn our
model, we propose an algorithm which automatically mines parts and learns
corresponding discriminative templates together with their respective locations
from a large number of candidate parts. We validate our method on three recent
challenging datasets of human attributes and actions. We obtain convincing
qualitative and state-of-the-art quantitative results on the three datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04199</identifier>
 <datestamp>2015-11-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04199</id><created>2015-09-14</created><updated>2015-11-09</updated><authors><author><keyname>Sopena</keyname><forenames>Eric</forenames><affiliation>LaBRI</affiliation></author></authors><title>i-MARK: A New Subtraction Division Game</title><categories>cs.DM</categories><comments>A few typos have been corrected, including the statement of Theorem 8</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given two finite sets of integers $S\subseteq\NNN\setminus\{0\}$ and
$D\subseteq\NNN\setminus\{0,1\}$,the impartial combinatorial game $\IMARK(S,D)$
is played on a heap of tokens. From a heap of $n$ tokens, each player can
moveeither to a heap of $n-s$ tokens for some $s\in S$, or to a heap of $n/d$
tokensfor some $d\in D$ if $d$ divides $n$.Such games can be considered as an
integral variant of \MARK-type games, introduced by Elwyn Berlekamp and Joe
Buhlerand studied by Aviezri Fraenkel and Alan Guo, for which it is allowed to
move from a heap of $n$ tokensto a heap of $\lfloor n/d\rfloor$ tokens for any
$d\in D$.Under normal convention, it is observed that the Sprague-Grundy
sequence of the game $\IMARK(S,D)$ is aperiodic for any sets $S$ and
$D$.However, we prove that, in many cases, this sequence is almost periodic and
that the set of winning positions is periodic.Moreover, in all these cases, the
Sprague-Grundy value of a heap of $n$ tokens can be computed in time $O(\log
n)$.We also prove that, under mis\`ere convention, the outcome sequence of
these games is purely periodic.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04200</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04200</id><created>2015-09-14</created><authors><author><keyname>Dabbene</keyname><forenames>Fabrizio</forenames><affiliation>LAAS-MAC</affiliation></author><author><keyname>Henrion</keyname><forenames>Didier</forenames><affiliation>LAAS-MAC</affiliation></author><author><keyname>Lagoa</keyname><forenames>Constantino</forenames></author></authors><title>Simple Approximations of Semialgebraic Sets and their Applications to
  Control</title><categories>math.OC cs.SY</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many uncertainty sets encountered in control systems analysis and design can
be expressed in terms of semialgebraic sets, that is as the intersection of
sets described by means of polynomial inequalities. Important examples are for
instance the solution set of linear matrix inequalities or the Schur/Hurwitz
stability domains. These sets often have very complicated shapes (non-convex,
and even non-connected), which renders very difficult their manipulation. It is
therefore of considerable importance to find simple-enough approximations of
these sets, able to capture their main characteristics while maintaining a low
level of complexity. For these reasons, in the past years several convex
approximations, based for instance on hyperrect-angles, polytopes, or
ellipsoids have been proposed. In this work, we move a step further, and
propose possibly non-convex approximations , based on a small volume polynomial
superlevel set of a single positive polynomial of given degree. We show how
these sets can be easily approximated by minimizing the L1 norm of the
polynomial over the semialgebraic set, subject to positivity constraints.
Intuitively, this corresponds to the trace minimization heuristic commonly
encounter in minimum volume ellipsoid problems. From a computational viewpoint,
we design a hierarchy of linear matrix inequality problems to generate these
approximations, and we provide theoretically rigorous convergence results, in
the sense that the hierarchy of outer approximations converges in volume (or,
equivalently, almost everywhere and almost uniformly) to the original set. Two
main applications of the proposed approach are considered. The first one aims
at reconstruction/approximation of sets from a finite number of samples. In the
second one, we show how the concept of polynomial superlevel set can be used to
generate samples uniformly distributed on a given semialgebraic set. The
efficiency of the proposed approach is demonstrated by different numerical
examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04203</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04203</id><created>2015-09-14</created><authors><author><keyname>Kandasamy</keyname><forenames>Saravanan</forenames></author><author><keyname>Morla</keyname><forenames>Ricardo</forenames></author><author><keyname>Ricardo</keyname><forenames>Manuel</forenames></author></authors><title>Power Interference Modeling for CSMA/CA based Networks using Directional
  Antenna</title><categories>cs.NI</categories><comments>Submitted to Elsevier's Journal of Computer Communications, 40 pages,
  17 figures and 25 references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In IEEE 802.11 based wireless networks adding more access points does not
always guarantee an increase of network capacity. In some cases, additional
access points may contribute to degrade the aggregated network throughput as
more interference is introduced.
  This paper characterizes the power interference in CSMA/CA based networks
consisting of nodes using directional antenna. The severity of the interference
is quantized via an improved form of the Attacking Case metric as the original
form of this metric was developed for nodes using omnidirectional antenna.
  The proposed metric is attractive because it considers nodes using
directional or omnidirectional antenna, and it enables the quantization of
interference in wireless networks using multiple transmission power schemes.
The improved Attacking Case metric is useful to study the aggregated throughput
of IEEE 802.11 based networks; reducing Attacking Case probably results in an
increase of aggregated throughput. This reduction can be implemented using
strategies such as directional antenna, transmit power control, or both.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04206</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04206</id><created>2015-09-14</created><authors><author><keyname>Koltzenburg</keyname><forenames>Claudia</forenames></author></authors><title>Nicht-propositionales Wissen aus Literaturlekt\&quot;ure und Bedingungen
  seiner Darstellbarkeit in Wikipedia-Eintr\&quot;agen zu literarischen Werken</title><categories>cs.CY</categories><comments>PhD Thesis, in German</comments><doi>10.6084/m9.figshare.1543379</doi><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  (English title) Wikipedia entries on fiction: Under what terms is it possible
to report about non-propositional knowledge gained from reading?
  Given that Wikipedia entries are likely to extert a strong influence on how
literary texts are perceived - due to their preferential ranking in Google -
there is some demand that research dealing with the transfer of knowledge on
literature to the public be more concerned with looking into both the content
that is available for free on the Web and any aspect that may come with writing
about literature for free. This contribution argues from within Wikipedia's
multidisciplinary consensus-driven space in which propositional knowledge is
given priority that it would be essential for entries on fiction to present
non-propositional knowledge as being one of its hallmarks. For this aim, a
special concept is developed that is designed to function as the study's formal
object: &quot;Erlesnis&quot; (which in German is a pun that combines &quot;Erlebnis&quot; -
adventure experience - and &quot;lesen&quot; - reading). It is defined as
non-propositional knowledge that has been acquired in an individual reading
process. Writing about one's own Erlesnis in new ways is being tried out in
four essays, and on de.wikipedia.org an experiment is conducted to find out
what community members think about the idea of including, in entries on
fiction, sections specifically designed to report about what people felt like
when reading a certain text. Finally, a draft typology of Erlesnis-writing is
suggested. This contribution is the first of its kind internationally to deal
with Wikipedia from the point of view of transfer of knowledge on literature to
the public. For the theory of this field of research some new aspects are
offered for debate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04207</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04207</id><created>2015-09-14</created><authors><author><keyname>Dias</keyname><forenames>Mart&#xed;n</forenames><affiliation>RMOD</affiliation></author><author><keyname>Polito</keyname><forenames>Guillermo</forenames><affiliation>RMOD</affiliation></author><author><keyname>Cassou</keyname><forenames>Damien</forenames><affiliation>RMOD</affiliation></author><author><keyname>Ducasse</keyname><forenames>St&#xe9;phane</forenames><affiliation>RMOD</affiliation></author></authors><title>DeltaImpactFinder: Assessing Semantic Merge Conflicts with Dependency
  Analysis</title><categories>cs.SE cs.PL</categories><comments>International Workshop on Smalltalk Technologies 2015, Jul 2015,
  Brescia, Italy</comments><proxy>ccsd</proxy><doi>10.1145/2811237.2811299</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In software development, version control systems (VCS) provide branching and
merging support tools. Such tools are popular among developers to concurrently
change a code-base in separate lines and reconcile their changes automatically
afterwards. However, two changes that are correct independently can introduce
bugs when merged together. We call semantic merge conflicts this kind of bugs.
Change impact analysis (CIA) aims at estimating the effects of a change in a
codebase. In this paper, we propose to detect semantic merge conflicts using
CIA. On a merge, DELTAIMPACTFINDER analyzes and compares the impact of a change
in its origin and destination branches. We call the difference between these
two impacts the delta-impact. If the delta-impact is empty, then there is no
indicator of a semantic merge conflict and the merge can continue
automatically. Otherwise, the delta-impact contains what are the sources of
possible conflicts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04210</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04210</id><created>2015-09-14</created><updated>2015-09-14</updated><authors><author><keyname>Gupta</keyname><forenames>Suyog</forenames></author><author><keyname>Zhang</keyname><forenames>Wei</forenames></author><author><keyname>Milthorpe</keyname><forenames>Josh</forenames></author></authors><title>Model Accuracy and Runtime Tradeoff in Distributed Deep Learning</title><categories>stat.ML cs.DC cs.LG cs.NE</categories><comments>15 pages, 9 figures, 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents Rudra, a parameter server based distributed computing
framework tuned for training large-scale deep neural networks. Using variants
of the asynchronous stochastic gradient descent algorithm we study the impact
of synchronization protocol, stale gradient updates, minibatch size, learning
rates, and number of learners on runtime performance and model accuracy. We
introduce a new learning rate modulation strategy to counter the effect of
stale gradients and propose a new synchronization protocol that can effectively
bound the staleness in gradients, improve runtime performance and achieve good
model accuracy. Our empirical investigation reveals a principled approach for
distributed training of neural networks: the mini-batch size per learner should
be reduced as more learners are added to the system to preserve the model
accuracy. We validate this approach using commonly-used image classification
benchmarks: CIFAR10 and ImageNet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04211</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04211</id><created>2015-09-14</created><authors><author><keyname>Ozmen</keyname><forenames>Mustafa</forenames></author><author><keyname>Gursoy</keyname><forenames>M. Cenk</forenames></author></authors><title>Wireless Throughput and Energy Efficiency with Random Arrivals and
  Statistical Queueing Constraints</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Throughput and energy efficiency in fading channels are studied in the
presence of randomly arriving data and statistical queueing constraints. In
particular, Markovian arrival models including discrete-time Markov, Markov
fluid, and Markov-modulated Poisson sources are considered. Employing the
effective bandwidth of time-varying sources and effective capacity of
time-varying wireless transmissions, maximum average arrival rates in the
presence of statistical queueing constraints are characterized. For the
two-state (ON/OFF) source models, throughput is determined in closed-form as a
function of the source statistics, channel characteristics, and quality of
service (QoS) constraints. Throughput is further studied in certain asymptotic
regimes. Furthermore, energy efficiency is analyzed by determining the minimum
energy per bit and wideband slope in the low signal-to-noise ratio (SNR)
regime. Overall, the impact of source characteristics, QoS requirements, and
channel fading correlations on the throughput and energy efficiency of wireless
systems is identified.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04218</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04218</id><created>2015-09-14</created><authors><author><keyname>Beraka</keyname><forenames>Mutaz</forenames></author><author><keyname>Al-Dhelaan</keyname><forenames>Abdullah</forenames></author><author><keyname>Al-Rodhaan</keyname><forenames>Mznah</forenames></author></authors><title>Collaborative Bibliographic System for Review/Survey Articles</title><categories>cs.DL</categories><comments>14 pages, International Journal of Computer Science and Information
  Technology (IJCSIT), Vol 7, No 4, August 2015</comments><doi>10.5121/ijcsit.2015.7402</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a Bibliographic system intends to exchange bibliographic
information of survey/review articles by relying on Web service technology. It
allows researchers and university students to interact with system via single
service using platform-independent standard named Web service to add, search
and retrieve bibliographic information of review articles in various science
and technology fields and build-up a dedicated database for these articles in
each science and technology field. Additionally, different implementation
scenarios of the proposed system are presented and described, and rich features
that offered by such system are studied and described. However, this paper
explains the proposed system using computing area due to the existence of
detailed taxonomy of this area, which allows defining the system, their
functionalities and features provided. However, the proposed system is not only
confined to computing area, it can support any other science and technology
area without any need to modify this system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04219</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04219</id><created>2015-09-14</created><authors><author><keyname>Baqapuri</keyname><forenames>Afroze Ibrahim</forenames></author></authors><title>Twitter Sentiment Analysis</title><categories>cs.CL cs.IR cs.SI</categories><comments>Bachelors Thesis Report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This project addresses the problem of sentiment analysis in twitter; that is
classifying tweets according to the sentiment expressed in them: positive,
negative or neutral. Twitter is an online micro-blogging and social-networking
platform which allows users to write short status updates of maximum length 140
characters. It is a rapidly expanding service with over 200 million registered
users - out of which 100 million are active users and half of them log on
twitter on a daily basis - generating nearly 250 million tweets per day. Due to
this large amount of usage we hope to achieve a reflection of public sentiment
by analysing the sentiments expressed in the tweets. Analysing the public
sentiment is important for many applications such as firms trying to find out
the response of their products in the market, predicting political elections
and predicting socioeconomic phenomena like stock exchange. The aim of this
project is to develop a functional classifier for accurate and automatic
sentiment classification of an unknown tweet stream.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04221</identifier>
 <datestamp>2015-09-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04221</id><created>2015-09-14</created><updated>2015-09-29</updated><authors><author><keyname>Kewat</keyname><forenames>Pramod Kumar</forenames></author><author><keyname>Kushwaha</keyname><forenames>Sarika</forenames></author></authors><title>Cyclic codes over the ring $\mathbb{F}_p[u,v,w]/\langle u^2, v^2, w^2,
  uv-vu, vw-wv, uw-wu \rangle$</title><categories>cs.IT math.IT</categories><comments>Rewriting of Section 4. arXiv admin note: substantial text overlap
  with arXiv:1508.07034, arXiv:1405.5981</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate cyclic codes over the ring $
\mathbb{F}_p[u,v,w]\langle u^2,$ $v^2, w^2$, $uv-vu, vw-wv, uw-wu \rangle$,
where $p$ is a prime number. Which is a part of family of Frobenius rings. We
find a unique set of generators for these codes and characterize the free
cyclic codes. We also study the rank and the Hamming distance of these codes.
We also constructs some good $p-ary$ codes as the Gray images of these cyclic
codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04225</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04225</id><created>2015-09-14</created><authors><author><keyname>Turgut</keyname><forenames>Esma</forenames></author><author><keyname>Gursoy</keyname><forenames>M. Cenk</forenames></author></authors><title>Average Error Probability Analysis in mmWave Cellular Networks</title><categories>cs.IT math.IT</categories><comments>Presented at IEEE VTC2015-Fall</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a mathematical framework for the analysis of average symbol
error probability (ASEP) in millimeter wave (mmWave) cellular networks with
Poisson Point Process (PPP) distributed base stations (BSs) is developed using
tools from stochastic geometry. The distinguishing features of mmWave
communications such as directional beamforming and having different path loss
laws for line-of-sight (LOS) and non-line-of-sight (NLOS) links are
incorporated in the average error probability analysis. First, average pairwise
error probability (APEP) expression is obtained by averaging pairwise error
probability (PEP) over fading and random shortest distance from mobile user
(MU) to its serving BS. Subsequently, average symbol error probability is
approximated from APEP using the nearest neighbor (NN) approximation. ASEP is
analyzed for different antenna gains and base station densities. Finally, the
effect of beamforming alignment errors on ASEP is investigated to get insight
on more realistic cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04227</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04227</id><created>2015-09-14</created><authors><author><keyname>Fani</keyname><forenames>Hossein</forenames></author><author><keyname>Zarrinkalam</keyname><forenames>Fattane</forenames></author><author><keyname>Zhao</keyname><forenames>Xin</forenames></author><author><keyname>Feng</keyname><forenames>Yue</forenames></author><author><keyname>Bagheri</keyname><forenames>Ebrahim</forenames></author><author><keyname>Du</keyname><forenames>Weichang</forenames></author></authors><title>Temporal Identification of Latent Communities on Twitter</title><categories>cs.SI</categories><comments>Submitted to WSDM 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  User communities in social networks are usually identified by considering
explicit structural social connections between users. While such communities
can reveal important information about their members such as family or
friendship ties and geographical proximity, they do not necessarily succeed at
pulling like-minded users that share the same interests together. In this
paper, we are interested in identifying communities of users that share similar
topical interests over time, regardless of whether they are explicitly
connected to each other on the social network. More specifically, we tackle the
problem of identifying temporal topic-based communities from Twitter, i.e.,
communities of users who have similar temporal inclination towards the current
emerging topics on Twitter. We model each topic as a collection of highly
correlated semantic concepts observed in tweets and identify them by clustering
the time-series based representation of each concept built based on each
concept's observation frequency over time. Based on the identified emerging
topics in a given time period, we utilize multivariate time series analysis to
model the contributions of each user towards the identified topics, which
allows us to detect latent user communities. Through our experiments on Twitter
data, we demonstrate i) the effectiveness of our topic detection method to
detect real world topics and ii) the effectiveness of our approach compared to
well-established approaches for community detection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04232</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04232</id><created>2015-09-14</created><authors><author><keyname>Ren</keyname><forenames>Carl Yuheng</forenames></author><author><keyname>Prisacariu</keyname><forenames>Victor Adrian</forenames></author><author><keyname>Reid</keyname><forenames>Ian D</forenames></author></authors><title>gSLICr: SLIC superpixels at over 250Hz</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a parallel GPU implementation of the Simple Linear Iterative
Clustering (SLIC) superpixel segmentation. Using a single graphic card, our
implementation achieves speedups of up to $83\times$ from the standard
sequential implementation. Our implementation is fully compatible with the
standard sequential implementation and the software is now available online and
is open source.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04237</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04237</id><created>2015-09-06</created><authors><author><keyname>Zhang</keyname><forenames>Jianping</forenames></author><author><keyname>Chen</keyname><forenames>Ke</forenames></author></authors><title>A Total Fractional-Order Variation Model for Image Restoration with
  Non-homogeneous Boundary Conditions and its Numerical Solution</title><categories>cs.CV math.NA</categories><comments>26 pages</comments><msc-class>62H35, 65N22, 65N55, 74G65, 74G75</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To overcome the weakness of a total variation based model for image
restoration, various high order (typically second order) regularization models
have been proposed and studied recently. In this paper we analyze and test a
fractional-order derivative based total $\alpha$-order variation model, which
can outperform the currently popular high order regularization models. There
exist several previous works using total $\alpha$-order variations for image
restoration; however first no analysis is done yet and second all tested
formulations, differing from each other, utilize the zero Dirichlet boundary
conditions which are not realistic (while non-zero boundary conditions violate
definitions of fractional-order derivatives). This paper first reviews some
results of fractional-order derivatives and then analyzes the theoretical
properties of the proposed total $\alpha$-order variational model rigorously.
It then develops four algorithms for solving the variational problem, one based
on the variational Split-Bregman idea and three based on direct solution of the
discretise-optimization problem. Numerical experiments show that, in terms of
restoration quality and solution efficiency, the proposed model can produce
highly competitive results, for smooth images, to two established high order
models: the mean curvature and the total generalized variation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04238</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04238</id><created>2015-09-14</created><authors><author><keyname>Barnes</keyname><forenames>Matt</forenames></author></authors><title>A Practioner's Guide to Evaluating Entity Resolution Results</title><categories>cs.DB stat.ML</categories><comments>Technical report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Entity resolution (ER) is the task of identifying records belonging to the
same entity (e.g. individual, group) across one or multiple databases.
Ironically, it has multiple names: deduplication and record linkage, among
others. In this paper we survey metrics used to evaluate ER results in order to
iteratively improve performance and guarantee sufficient quality prior to
deployment. Some of these metrics are borrowed from multi-class classification
and clustering domains, though some key differences exist differentiating
entity resolution from general clustering. Menestrina et al. empirically showed
rankings from these metrics often conflict with each other, thus our primary
motivation for studying them. This paper provides practitioners the basic
knowledge to begin evaluating their entity resolution results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04239</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04239</id><created>2015-09-14</created><authors><author><keyname>Lee</keyname><forenames>Wonjung</forenames></author><author><keyname>Lyons</keyname><forenames>Terry</forenames></author></authors><title>The adaptive patched cubature filter and its implementation</title><categories>cs.IT math.IT</categories><comments>to appear in Communications in Mathematical Sciences. arXiv admin
  note: substantial text overlap with arXiv:1311.6755</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are numerous contexts where one wishes to describe the state of a
randomly evolving system. Effective solutions combine models that quantify the
underlying uncertainty with available observational data to form scientifically
reasonable estimates for the uncertainty in the system state. Stochastic
differential equations are often used to mathematically model the underlying
system.
  The Kusuoka-Lyons-Victoir (KLV) approach is a higher order particle method
for approximating the weak solution of a stochastic differential equation that
uses a weighted set of scenarios to approximate the evolving probability
distribution to a high order of accuracy. The algorithm can be performed by
integrating along a number of carefully selected bounded variation paths. The
iterated application of the KLV method has a tendency for the number of
particles to increase. This can be addressed and, together with local dynamic
recombination, which simplifies the support of discrete measure without harming
the accuracy of the approximation, the KLV method becomes eligible to solve the
filtering problem in contexts where one desires to maintain an accurate
description of the ever-evolving conditioned measure.
  In addition to the alternate application of the KLV method and recombination,
we make use of the smooth nature of the likelihood function and high order
accuracy of the approximations to lead some of the particles immediately to the
next observation time and to build into the algorithm a form of automatic high
order adaptive importance sampling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04240</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04240</id><created>2015-09-11</created><authors><author><keyname>Misra</keyname><forenames>Neeraj Kumar</forenames></author><author><keyname>Kushwaha</keyname><forenames>Mukesh Kumar</forenames></author><author><keyname>Wairya</keyname><forenames>Subodh</forenames></author><author><keyname>Kumar</keyname><forenames>Amit</forenames></author></authors><title>Feasible methodology for optimization of a novel reversible binary
  compressor</title><categories>cs.AR cs.ET</categories><comments>13 pages, 9 figures</comments><journal-ref>International Journal of VLSI design &amp; Communication Systems
  (VLSICS) Vol.6, No.4, August 2015</journal-ref><doi>10.5121/vlsic.2015.6401</doi><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Now a day reversible logic is an attractive research area due to its low
power consumption in the area of VLSI circuit design. The reversible logic gate
is utilized to optimize power consumption by a feature of retrieving input
logic from an output logic because of bijective mapping between input and
output. In this manuscript, we design 4 2 and 5 2 reversible compressor
circuits using a new type of reversible gate. In addition, we propose new gate,
named as inventive0 gate for optimizing a compressor circuit. The utility of
the inventive0 gate is that it can be used as full adder and full subtraction
with low value of garbage outputs and quantum cost. An algorithm is shown for
designing a compressor structure. The comparative study shows that the proposed
compressor structure outperforms the existing ones in terms of garbage outputs,
number of gates and quantum cost. The compressor can reduce the effect of carry
(Produce from full adder) of the arithmetic frame design. In addition, we
implement a basic reversible gate of MOS transistor with less number of MOS
transistor count.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04250</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04250</id><created>2015-09-14</created><authors><author><keyname>Roko&#x161;</keyname><forenames>O.</forenames></author><author><keyname>M&#xe1;ca</keyname><forenames>J.</forenames></author></authors><title>The response of grandstands driven by filtered Gaussian white noise
  processes</title><categories>cs.CE</categories><comments>20 pages, 12 figures, 4 tables</comments><journal-ref>Advances in Engineering Software, Volume 72, June 2014, Pages
  85--94, Special Issue dedicated to Professor Zden\v{e}k Bittnar on the
  occasion of his Seventieth Birthday: Part 2</journal-ref><doi>10.1016/j.advengsoft.2013.05.008</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a semi-analytical estimate of the response of a
grandstand occupied by an active crowd and by a passive crowd. Filtered
Gaussian white noise processes are used to approximate the loading terms
representing an active crowd. Lumped biodynamic models with a single degree of
freedom are included to reflect passive spectators occupying the structure. The
response is described in terms of the first two moments, employing the It\^o
formula and the state augmentation method for the stationary time domain
solution. The quality of the approximation is compared on the basis of three
examples of varying complexity using Monte Carlo simulation based on a
synthetic generator available in the literature. For comparative purposes,
there is also a brief review of frequency domain estimates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04252</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04252</id><created>2015-09-14</created><authors><author><keyname>Kreienbuehl</keyname><forenames>Andreas</forenames></author><author><keyname>Naegel</keyname><forenames>Arne</forenames></author><author><keyname>Ruprecht</keyname><forenames>Daniel</forenames></author><author><keyname>Vogel</keyname><forenames>Andreas</forenames></author><author><keyname>Wittum</keyname><forenames>Gabriel</forenames></author><author><keyname>Krause</keyname><forenames>Rolf</forenames></author></authors><title>Parareal convergence for 2D unsteady flow around a cylinder</title><categories>cs.CE cs.DC cs.NA math.NA</categories><comments>16 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this technical report we study the convergence of Parareal for 2D
incompressible flow around a cylinder for different viscosities. Two methods
are used as fine integrator: backward Euler and a fractional step method. It is
found that Parareal converges better for the implicit Euler, likely because it
under-resolves the fine-scale dynamics as a result of numerical diffusion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04264</identifier>
 <datestamp>2015-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04264</id><created>2015-09-12</created><updated>2015-11-25</updated><authors><author><keyname>Jaffe</keyname><forenames>Klaus</forenames></author></authors><title>Agent based simulations visualize Adam Smith's invisible hand by solving
  Friedrich Hayek's Economic Calculus</title><categories>q-fin.EC cs.MA physics.soc-ph</categories><comments>Econophysics, Complexity, Synergy</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inspired by Adam Smith and Friedrich Hayek, many economists have postulated
the existence of invisible forces that drive economic markets. These market
forces interact in complex ways making it difficult to visualize or understand
the interactions in every detail. Here I show how these forces can transcend a
zero-sum game and become a win-win business interaction, thanks to emergent
social synergies triggered by division of labor. Computer simulations with the
model Sociodynamica show here the detailed dynamics underlying this phenomenon
in a simple virtual economy. In these simulations, independent agents act in an
economy exploiting and trading two different goods in a heterogeneous
environment. All and each of the various forces and individuals were tracked
continuously, allowing to unveil a synergistic effect on economic output
produced by the division of labor between agents. Running simulations in a
homogeneous environment, for example, eliminated all benefits of division of
labor. The simulations showed that the synergies unleashed by division of labor
arise if: Economies work in a heterogeneous environment; agents engage in
complementary activities whose optimization processes diverge; agents have
means to synchronize their activities. This insight, although trivial if viewed
a posteriori, improve our understanding of the source and nature of synergies
in real economic markets and might render economic and natural sciences more
consilient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04265</identifier>
 <datestamp>2015-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04265</id><created>2015-09-12</created><updated>2015-09-16</updated><authors><author><keyname>Masramon</keyname><forenames>Gabriel Prat</forenames></author><author><keyname>Mu&#xf1;oz</keyname><forenames>Llu&#xed;s A. Belanche</forenames></author></authors><title>Double Relief with progressive weighting function</title><categories>cs.LG cs.AI</categories><comments>arXiv admin note: substantial text overlap with arXiv:1509.03755</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Feature weighting algorithms try to solve a problem of great importance
nowadays in machine learning: The search of a relevance measure for the
features of a given domain. This relevance is primarily used for feature
selection as feature weighting can be seen as a generalization of it, but it is
also useful to better understand a problem's domain or to guide an inductor in
its learning process. Relief family of algorithms are proven to be very
effective in this task.
  On previous work, a new extension was proposed that aimed for improving the
algorithm's performance and it was shown that in certain cases it improved the
weights' estimation accuracy. However, it also seemed to be sensible to some
characteristics of the data. An improvement of that previously presented
extension is presented in this work that aims to make it more robust to problem
specific characteristics. An experimental design is proposed to test its
performance. Results of the tests prove that it indeed increase the robustness
of the previously proposed extension.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04268</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04268</id><created>2015-09-13</created><authors><author><keyname>Srinivasarao</keyname><forenames>Batta Kota Naga</forenames></author><author><keyname>Chakrabarti</keyname><forenames>Indrajit</forenames></author></authors><title>High Speed VLSI Architecture for 3-D Discrete Wavelet Transform</title><categories>cs.AR</categories><comments>Submitting to IET CDS. arXiv admin note: substantial text overlap
  with arXiv:1509.03836</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a memory efficient, high throughput parallel lifting
based running three dimensional discrete wavelet transform (3-D DWT)
architecture. 3-D DWT is constructed by combining the two spatial and four
temporal processors. Spatial processor (SP) apply the two dimensional DWT on a
frame, using lifting based 9/7 filter bank through the row rocessor (RP) in row
direction and then apply in the colum direction through column processor (CP).
To reduce the temporal memory and the latency, the temporal processor (TP) has
been designed with lifting based 1-D Haar wavelet filter. The proposed
architecture replaced the multiplications by pipeline shift-add operations to
reduce the CPD. Two spatial processors works simultaneously on two adjacent
frames and provide 2-D DWT coefficients as inputs to the temporal processors.
TPs apply the one dimensional DWT in temporal direction and provide eight 3-D
DWT coefficients per clock (throughput). Higher throughput reduces the
computing cycles per frame and enable the lower power consumption.
Implementation results shows that the proposed architecture has the advantage
in reduced memory, low power consumption, low latency, and high throughput over
the existing designs. The RTL of the proposed architecture is described using
verilog and synthesized using 90-nm technology CMOS standard cell library and
results show that it consumes 43.42 mW power and occupies an area equivalent to
231.45 K equivalent gate at frequency of 200 MHz. The proposed architecture has
also been synthesised for the Xilinx zynq 7020 series field programmable gate
array (FPGA).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04273</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04273</id><created>2015-09-14</created><authors><author><keyname>Brandst&#xe4;dt</keyname><forenames>Andreas</forenames></author><author><keyname>Dabrowski</keyname><forenames>Konrad K.</forenames></author><author><keyname>Huang</keyname><forenames>Shenwei</forenames></author><author><keyname>Paulusma</keyname><forenames>Dani&#xeb;l</forenames></author></authors><title>Bounding the Clique-Width of $H$-free Split Graphs</title><categories>cs.DM math.CO</categories><comments>17 pages, 5 figures. An extended abstract of this paper appeared in
  the proceedings of EuroComb 2015</comments><msc-class>05C75</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A graph is $H$-free if it has no induced subgraph isomorphic to $H$. We
continue a study into the boundedness of clique-width of subclasses of perfect
graphs. We identify five new classes of $H$-free split graphs whose
clique-width is bounded. Our main result, obtained by combining new and known
results, provides a classification of all but two stubborn cases, that is, with
two potential exceptions we determine all graphs $H$ for which the class of
$H$-free split graphs has bounded clique-width.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04274</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04274</id><created>2015-09-14</created><updated>2016-01-26</updated><authors><author><keyname>Saha</keyname><forenames>Swetank Kumar</forenames></author><author><keyname>Vira</keyname><forenames>Viral Vijay</forenames></author><author><keyname>Garg</keyname><forenames>Anuj</forenames></author><author><keyname>Koutsonikolas</keyname><forenames>Dimitrios</forenames></author></authors><title>60 GHz Multi-Gigabit Indoor WLANs: Dream or Reality?</title><categories>cs.NI</categories><acm-class>C.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The millimeter-wave (mmWave) technology, recently standardized by IEEE
802.11ad, is emerging as an attractive alternative to the traditional 2.4/5GHz
wireless systems, promising multi-Gigabit wireless throughput. However, the
high attenuation and vulnerability to blockage of 60 GHz links have limited its
applications (until recently) to short-range, line-of-sight, static scenarios.
On the other hand, the question of whether it is feasible to build
general-purpose WLANs out of mmWave radios in dynamic indoor environments with
non-line-of-sight links remains largely unanswered. In this paper, through
extensive measurements with COTS 802.11ad hardware in an indoor office
environment, we investigate the question of whether the mmWave technology, in
spite of its unique propagation characteristics, can serve as a viable choice
for providing multi-Gigabit ubiquitous wireless indoor connectivity. We study
the range of 60 GHz transmissions in indoor environments, the impact of antenna
height, location, orientation, and distance on 60 GHz performance, the
interaction among metrics from different layers of the network stack, the
increased opportunities for spatial reuse, and the impact of human blockage.
Our results reveal a number of challenges that we have to address for 60 GHz
multi-gigabit WLANs to become a reality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04303</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04303</id><created>2015-09-14</created><authors><author><keyname>Papazafeiropoulos</keyname><forenames>Anastasios K.</forenames></author></authors><title>Downlink Performance of Massive MIMO under General Channel Aging
  Conditions</title><categories>cs.IT math.IT</categories><comments>6 pages, 3 figures, accepted in IEEE Global Communications Conference
  (GLOBECOM 2015)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Massive multiple-input multiple-output (MIMO) is a promising technology
aiming at achieving high spectral efficiency by deploying a large number of
base station (BS) antennas using coherent combining. Channel aging due to user
mobility is a significant degrading factor of such systems. In addition, cost
efficiency of massive MIMO is a prerecuisite for their deployment, that leads
to low cost antenna elements inducing high phase noise. Since phase is
time-dependent, it contributes to channel aging. For this reason, we present a
novel joint channel-phase noise model, that enables us to study the downlink of
massive MIMO with maximum ratio transmission (MRT) precoder under these
conditions by means of the deterministic equivalent of the achievable sum-rate.
Among the noteworthy outcomes is that the degradation due to user mobility
dominates over the effect of phase noise. Nevertheless, we demonstrate that the
joint effects of phase noise and user mobility do not degrade the power scaling
law $1/\sqrt{M}$ ($M$ is the number of BS antennas), as has been established in
massive MIMO systems with imperfect channel state information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04309</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04309</id><created>2015-09-14</created><authors><author><keyname>Zhou</keyname><forenames>Xiaowei</forenames></author><author><keyname>Zhu</keyname><forenames>Menglong</forenames></author><author><keyname>Leonardos</keyname><forenames>Spyridon</forenames></author><author><keyname>Daniilidis</keyname><forenames>Kostas</forenames></author></authors><title>Sparse Representation for 3D Shape Estimation: A Convex Relaxation
  Approach</title><categories>cs.CV</categories><comments>Extended version of the paper: 3D Shape Estimation from 2D Landmarks:
  A Convex Relaxation Approach. X. Zhou et al., CVPR, 2015. arXiv admin note:
  substantial text overlap with arXiv:1411.2942</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the problem of estimating the 3D shape of an object defined by
a set of 3D landmarks, given their 2D correspondences in a single image. To
alleviate the reconstruction ambiguity, a widely used approach is to assume the
unknown shape as a linear combination of predefined basis shapes and the sparse
representation is usually adopted to capture complex shape variability. While
this approach has proven to be successful in many applications, a challenging
issue remains, i.e., the joint estimation of shape and viewpoint requires to
solve a nonconvex optimization problem. Previous methods often adopt an
alternating minimization scheme to alternately update the shape and viewpoint,
and the solution depends on initialization and might be stuck at local optimum.
In this paper, we propose a convex approach to addressing this issue and
develop an efficient algorithm to solve the proposed convex program. Moreover,
we propose a robust model to handle gross errors in the 2D correspondences. We
demonstrate the exact recovery property of the proposed model, its merits
compared to alternative methods and the applicability to recover 3D human poses
and car shapes from real images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04311</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04311</id><created>2015-08-22</created><authors><author><keyname>Ahmed</keyname><forenames>Irfan</forenames></author><author><keyname>Bhatti</keyname><forenames>Arif</forenames></author></authors><title>Design and Implementation of Performance Metrics for Evaluation of
  Assessments Data</title><categories>physics.ed-ph cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The objective of this paper is to design performance metrics and respective
formulas to quantitatively evaluate the achievement of set objectives and
expected outcomes both at the course and program levels. Evaluation is defined
as one or more processes for interpreting the data acquired through the
assessment processes in order to determine how well the set objectives and
outcomes are being attained. Even though assessment processes for accreditation
are well documented but existence of an evaluation process is assumed. This
paper focuses on evaluation process to provide insights and techniques for data
interpretation. It gives a complete evaluation process from the data collection
through various assessment methods, performance metrics, to the presentations
in the form of tables and graphs. Authors hope that the articulated description
of evaluation formulas will help convergence to high quality standard in
evaluation process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04315</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04315</id><created>2015-09-14</created><authors><author><keyname>Webb</keyname><forenames>Robert</forenames></author></authors><title>Implementing a teleo-reactive programming system</title><categories>cs.PL</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  This thesis explores the teleo-reactive programming paradigm for controlling
autonomous agents, such as robots. Teleo-reactive programming provides a
robust, opportunistic method for goal-directed programming that continuously
reacts to the sensed environment. In particular, the TR and TeleoR systems are
investigated. They influence the design of a teleo-reactive system programming
in Python, for controlling autonomous agents via the Pedro communications
architecture. To demonstrate the system, it is used as a controller in a simple
game.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04321</identifier>
 <datestamp>2015-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04321</id><created>2015-08-29</created><authors><author><keyname>Civelli</keyname><forenames>Stella</forenames></author><author><keyname>Barletti</keyname><forenames>Luigi</forenames></author><author><keyname>Secondini</keyname><forenames>Marco</forenames></author></authors><title>Numerical Methods for the Inverse Nonlinear Fourier Transform</title><categories>math.NA cs.IT cs.NA math.IT physics.optics</categories><comments>To be presented at the Tyrrhenian International Workshop on Digital
  Communications (TIWDC) 2015</comments><journal-ref>in Tyrrhenian International Workshop on Digital Communications
  (TIWDC) 2015 , pp.13-16, 22 Sept. 2015</journal-ref><doi>10.1109/TIWDC.2015.7323325</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new numerical method for the computation of the inverse
nonlinear Fourier transform and compare its computational complexity and
accuracy to those of other methods available in the literature. For a given
accuracy, the proposed method requires the lowest number of operations
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04328</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04328</id><created>2015-09-11</created><authors><author><keyname>Misra</keyname><forenames>Neeraj Kumar</forenames></author><author><keyname>Wairya</keyname><forenames>Subodh</forenames></author><author><keyname>Singh</keyname><forenames>Vinod Kumar</forenames></author></authors><title>Evolution of structure of some binary group based n bit comparator,
  n-to-2n decoder by reversible technique</title><categories>cs.ET</categories><comments>22 pages, 19 figure, journal</comments><journal-ref>International Journal of VLSI design &amp; Communication Systems
  (VLSICS) Vol.5, No.5, October 2014</journal-ref><doi>10.5121/vlsic.2014.5502</doi><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  Reversible logic has attracted substantial interest due to its low power
consumption which is the main concern of low power VLSI circuit design. In this
paper, a novel 4x4 reversible gate called inventive gate has been introduced
and using this gate 1-bit, 2-bit, 8-bit, 32-bit and n-bit group-based
reversible comparator have been constructed with low value of reversible
parameters. The MOS transistor realizations of 1-bit, 2- bit, and 8-bit of
reversible comparator are also presented and finding power, delay and power
delay product (PDP) with appropriate aspect ratio W/L. Novel inventive gate has
the ability to use as an n-to-2n decoder. Different proposed novel reversible
circuit design style is compared with the existing ones. The relative results
shows that the novel reversible gate wide utility, group-based reversible
comparator outperforms the present design style in terms of number of gates,
garbage outputs and constant input.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04332</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04332</id><created>2015-09-14</created><authors><author><keyname>Rahimian</keyname><forenames>Mohammad Amin</forenames></author><author><keyname>Shahrampour</keyname><forenames>Shahin</forenames></author><author><keyname>Jadbabaie</keyname><forenames>Ali</forenames></author></authors><title>Learning without Recall by Random Walks on Directed Graphs</title><categories>cs.SY math.OC stat.ML</categories><comments>6 pages, To Appear in Conference on Decision and Control 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a network of agents that aim to learn some unknown state of the
world using private observations and exchange of beliefs. At each time, agents
observe private signals generated based on the true unknown state. Each agent
might not be able to distinguish the true state based only on her private
observations. This occurs when some other states are observationally equivalent
to the true state from the agent's perspective. To overcome this shortcoming,
agents must communicate with each other to benefit from local observations. We
propose a model where each agent selects one of her neighbors randomly at each
time. Then, she refines her opinion using her private signal and the prior of
that particular neighbor. The proposed rule can be thought of as a Bayesian
agent who cannot recall the priors based on which other agents make inferences.
This learning without recall approach preserves some aspects of the Bayesian
inference while being computationally tractable. By establishing a
correspondence with a random walk on the network graph, we prove that under the
described protocol, agents learn the truth exponentially fast in the almost
sure sense. The asymptotic rate is expressed as the sum of the relative
entropies between the signal structures of every agent weighted by the
stationary distribution of the random walk.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04335</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04335</id><created>2015-09-14</created><authors><author><keyname>Kim</keyname><forenames>Hyeji</forenames></author><author><keyname>Gamal</keyname><forenames>Abbas El</forenames></author></authors><title>Capacity Theorems for Broadcast Channels with Two Channel State
  Components Known at the Receivers</title><categories>cs.IT math.IT</categories><comments>18 pages, 6 figures, submitted to IEEE Transactions on Information
  Theory. arXiv admin note: text overlap with arXiv:1401.6738</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We establish the capacity region of several classes of broadcast channels
with random state in which the channel to each user is selected from two
possible channel state components and the state is known only at the receivers.
When the channel components are deterministic, we show that the capacity region
is achieved via Marton coding. This channel model does not belong to any class
of broadcast channels for which the capacity region was previously known and is
useful in studying wireless communication channels when the fading state is
known only at the receivers. We then establish the capacity region when the
channel components are ordered, e.g., degraded. In particular we show that the
capacity region for the broadcast channel with degraded Gaussian vector channel
components is attained via Gaussian input distribution. Finally, we extend the
results on ordered channels to two broadcast channel examples with more than
two channel components, but show that these extensions do not hold in general.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04340</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04340</id><created>2015-09-14</created><authors><author><keyname>Cortes</keyname><forenames>Corinna</forenames></author><author><keyname>Goyal</keyname><forenames>Prasoon</forenames></author><author><keyname>Kuznetsov</keyname><forenames>Vitaly</forenames></author><author><keyname>Mohri</keyname><forenames>Mehryar</forenames></author></authors><title>Voted Kernel Regularization</title><categories>cs.LG</categories><comments>16 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an algorithm, Voted Kernel Regularization , that provides
the flexibility of using potentially very complex kernel functions such as
predictors based on much higher-degree polynomial kernels, while benefitting
from strong learning guarantees. The success of our algorithm arises from
derived bounds that suggest a new regularization penalty in terms of the
Rademacher complexities of the corresponding families of kernel maps. In a
series of experiments we demonstrate the improved performance of our algorithm
as compared to baselines. Furthermore, the algorithm enjoys several favorable
properties. The optimization problem is convex, it allows for learning with
non-PDS kernels, and the solutions are highly sparse, resulting in improved
classification speed and memory requirements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04343</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04343</id><created>2015-09-14</created><authors><author><keyname>Zhang</keyname><forenames>Chuang</forenames></author><author><keyname>Fan</keyname><forenames>Pingyi</forenames></author></authors><title>Power Allocation for Mixed Traffic Broadcast with Service Outage
  Constraint</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To transmit a mixture of real-time and non-real-time traffic in a broadcast
system, we impose a basic service rate $r_0$ for real-time traffic and use the
excess rate beyond $r_0$ to transmit non-real-time traffic. Considering the
time-varying nature of wireless channels, the basic service rate is guaranteed
with a service outage constraint, where service outage occurs when the channel
capacity is below the basic service rate. This approach is well suited for
providing growing services like video, real-time TV, etc., in group
transportation systems such as coach, high-speed train, and airplane. We show
that the optimal power allocation policy depends only on the statistics of the
minimum gain of all user channels, and it is a combination of water-filling and
channel inversion. We provide the optimal power allocation policy, which
guarantees that real-time traffic be delivered with quality of service (QoS)
for every user. Moreover, we show that the required minimum average power to
satisfy the service outage constraint increases linearly with the number of
users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04344</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04344</id><created>2015-09-14</created><authors><author><keyname>Gupta</keyname><forenames>Sushmita</forenames></author><author><keyname>Iwama</keyname><forenames>Kazuo</forenames></author><author><keyname>Miyazaki</keyname><forenames>Shuichi</forenames></author></authors><title>Stable Nash Equilibria in the Gale-Shapley Matching Game</title><categories>cs.DS cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article we study the stable marriage game induced by the
men-proposing Gale-Shapley algorithm. Our setting is standard: all the lists
are complete and the matching mechanism is the men-proposing Gale-Shapley
algorithm. It is well known that in this setting, men cannot cheat, but women
can. In fact, Teo, Sethuraman and Tan \cite{TST01}, show that there is a
polynomial time algorithm to obtain, for a given strategy (the set of all
lists) $Q$ and a woman $w$, the best partner attainable by changing her list.
However, what if the resulting matching is not stable with respect to $Q$?
Obviously, such a matching would be vulnerable to further manipulation, but is
not mentioned in \cite{TST01}. In this paper, we consider (safe) manipulation
that implies a stable matching in a most general setting. Specifically, our
goal is to decide for a given $Q$, if w can manipulate her list to obtain a
strictly better partner with respect to the true strategy $P$ (which may be
different from $Q$), and also the outcome is a stable matching for $P$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04349</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04349</id><created>2015-09-14</created><authors><author><keyname>Kamat</keyname><forenames>Niranjan</forenames></author><author><keyname>Nandi</keyname><forenames>Arnab</forenames></author></authors><title>A Closer Look at Variance Implementations in Modern Database Systems</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Variance is a popular and often necessary component of sampled aggregation
queries. It is typically used as a secondary measure to ascertain statistical
properties of the result such as its error. Yet, it is more expensive to
compute than simple, primary measures such as \texttt{SUM}, \texttt{MEAN}, and
\texttt{COUNT}.
  There exist numerous techniques to compute variance. While the definition of
variance is considered to require multiple passes on the data, other
mathematical representations can compute the value in a single pass. Some
single-pass representations, however, can suffer from severe precision loss,
especially for large number of data points.
  In this paper, we study variance implementations in various real-world
systems and find that major database systems such as PostgreSQL 9.4 and most
likely System X, a major commercially used closed-source database, use a
representation that is efficient, but suffers from floating point precision
loss resulting from catastrophic cancellation. We note deficiencies in another
popular representation, used by databases such as MySQL and Impala, that
suffers from not being distributive and therefore cannot take advantage of
modern parallel computational resources. We review literature over the past
five decades on variance calculation in both the statistics and database
communities, and summarize recommendations on implementing variance functions
in various settings, such as approximate query processing and large-scale
distributed aggregation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04355</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04355</id><created>2015-09-14</created><authors><author><keyname>Qian</keyname><forenames>Qi</forenames></author><author><keyname>Jin</keyname><forenames>Rong</forenames></author><author><keyname>Zhang</keyname><forenames>Lijun</forenames></author><author><keyname>Zhu</keyname><forenames>Shenghuo</forenames></author></authors><title>Towards Making High Dimensional Distance Metric Learning Practical</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we study distance metric learning (DML) for high dimensional
data. A typical approach for DML with high dimensional data is to perform the
dimensionality reduction first before learning the distance metric. The main
shortcoming of this approach is that it may result in a suboptimal solution due
to the subspace removed by the dimensionality reduction method. In this work,
we present a dual random projection frame for DML with high dimensional data
that explicitly addresses the limitation of dimensionality reduction for DML.
The key idea is to first project all the data points into a low dimensional
space by random projection, and compute the dual variables using the projected
vectors. It then reconstructs the distance metric in the original space using
the estimated dual variables. The proposed method, on one hand, enjoys the
light computation of random projection, and on the other hand, alleviates the
limitation of most dimensionality reduction methods. We verify both empirically
and theoretically the effectiveness of the proposed algorithm for high
dimensional DML.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04360</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04360</id><created>2015-09-14</created><authors><author><keyname>Williams</keyname><forenames>Joseph Jay</forenames></author><author><keyname>Heffernan</keyname><forenames>Neil</forenames></author></authors><title>A Methodology for Discovering how to Adaptively Personalize to Users
  using Experimental Comparisons</title><categories>cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explain and provide examples of a formalism that supports the methodology
of discovering how to adapt and personalize technology by combining randomized
experiments with variables associated with user models. We characterize a
formal relationship between the use of technology to conduct A/B experiments
and use of technology for adaptive personalization. The MOOClet Formalism [11]
captures the equivalence between experimentation and personalization in its
conceptualization of modular components of a technology. This motivates a
unified software design pattern that enables technology components that can be
compared in an experiment to also be adapted based on contextual data, or
personalized based on user characteristics. With the aid of a concrete use
case, we illustrate the potential of the MOOClet formalism for a methodology
that uses randomized experiments of alternative micro-designs to discover how
to adapt technology based on user characteristics, and then dynamically
implements these personalized improvements in real time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04366</identifier>
 <datestamp>2016-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04366</id><created>2015-09-14</created><updated>2016-01-15</updated><authors><author><keyname>Kindt</keyname><forenames>Philipp</forenames></author><author><keyname>Saur</keyname><forenames>Marco</forenames></author><author><keyname>Chakraborty</keyname><forenames>Samarjit</forenames></author></authors><title>Neighbor discovery latency in BLE-like duty-cycled protocols</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Protocols for minimizing the power consumption of wireless networks are
rapidly growing in importance. These protocols mostly rely on switching off a
device's CPU and wireless interface as long as possible. For neighbor discovery
and connection setup, purely interval-based protocols where one device
repeatedly sends out packets and the other device scans the channel for
multiple short amounts of time are widely used, e.g. in Bluetooth Low Energy
(BLE) or in ANT/ANT+. In this procedure, the neighbor-discovery latency (i.e.,
the time after which a packet is initially received) is determined by a
stochastic process. From discrete event simulations, it is known that the mean
discovery latency is subjected to strong variations for different
parametrizations. Hence, the parameter values have to be chosen carefully to
avoid long discovery procedures and thereby a high energy consumption. However,
except for special cases, no model for estimating this discovery latency is
known for this kind of protocols, yet. In this paper, we for the first time,
present a stochastic model that can compute the exact duration of the mean
discovery latency for every possible parametrization. Further, our proposed
model is capable of computing maximum worst-case discovery-latencies. It
reveals the interesting result that for almost all parametrizations, the
discovery latency is bounded.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04375</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04375</id><created>2015-09-14</created><authors><author><keyname>Babadi</keyname><forenames>Behtash</forenames></author><author><keyname>Kalouptsidis</keyname><forenames>Nicholas</forenames></author><author><keyname>Tarokh</keyname><forenames>Vahid</forenames></author></authors><title>Comment on &quot;Asymptotic Achievability of the Cram\'{e}r-Rao Bound for
  Noisy Compressive Sampling&quot;</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In [1], we proved the asymptotic achievability of the Cram\'{e}r-Rao bound in
the compressive sensing setting in the linear sparsity regime. In the proof, we
used an erroneous closed-form expression of $\alpha \sigma^2$ for the
genie-aided Cram\'{e}r-Rao bound $\sigma^2 \textrm{Tr}
(\mathbf{A}^*_\mathcal{I} \mathbf{A}_\mathcal{I})^{-1}$ from Lemma 3.5, which
appears in Eqs. (20) and (29). The proof, however, holds if one avoids
replacing $\sigma^2 \textrm{Tr} (\mathbf{A}^*_\mathcal{I}
\mathbf{A}_\mathcal{I})^{-1}$ by the expression of Lemma 3.5, and hence the
claim of the Main Theorem stands true.
  In Chapter 2 of the Ph. D. dissertation by Behtash Babadi [2], this error was
fixed and a more detailed proof in the non-asymptotic regime was presented. A
draft of Chapter 2 of [2] is included in this note, verbatim. We would like to
refer the interested reader to the full dissertation, which is electronically
archived in the ProQuest database [2], and a draft of which can be accessed
through the author's homepage under:
http://ece.umd.edu/~behtash/babadi_thesis_2011.pdf.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04376</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04376</id><created>2015-09-14</created><authors><author><keyname>Zhang</keyname><forenames>Bingwen</forenames></author><author><keyname>Xu</keyname><forenames>Weiyu</forenames></author><author><keyname>Cai</keyname><forenames>Jian-Feng</forenames></author><author><keyname>Lai</keyname><forenames>Lifeng</forenames></author></authors><title>Precise Phase Transition of Total Variation Minimization</title><categories>cs.IT cs.LG math.IT math.OC stat.ML</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Characterizing the phase transitions of convex optimizations in recovering
structured signals or data is of central importance in compressed sensing,
machine learning and statistics. The phase transitions of many convex
optimization signal recovery methods such as $\ell_1$ minimization and nuclear
norm minimization are well understood through recent years' research. However,
rigorously characterizing the phase transition of total variation (TV)
minimization in recovering sparse-gradient signal is still open. In this paper,
we fully characterize the phase transition curve of the TV minimization. Our
proof builds on Donoho, Johnstone and Montanari's conjectured phase transition
curve for the TV approximate message passing algorithm (AMP), together with the
linkage between the minmax Mean Square Error of a denoising problem and the
high-dimensional convex geometry for TV minimization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04385</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04385</id><created>2015-09-12</created><authors><author><keyname>Amarappa</keyname><forenames>S.</forenames></author><author><keyname>Sathyanarayana</keyname><forenames>S. V.</forenames></author></authors><title>Kannada named entity recognition and classification (nerc) based on
  multinomial na\&quot;ive bayes (mnb) classifier</title><categories>cs.CL</categories><comments>14 pages, 3 figures, International Journal on Natural Language
  Computing (IJNLC) Vol. 4, No.4, August 2015</comments><doi>10.5121/ijnlc.2015.4404</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Named Entity Recognition and Classification (NERC) is a process of
identification of proper nouns in the text and classification of those nouns
into certain predefined categories like person name, location, organization,
date, and time etc. NERC in Kannada is an essential and challenging task. The
aim of this work is to develop a novel model for NERC, based on Multinomial
Na\&quot;ive Bayes (MNB) Classifier. The Methodology adopted in this paper is based
on feature extraction of training corpus, by using term frequency, inverse
document frequency and fitting them to a tf-idf-vectorizer. The paper discusses
the various issues in developing the proposed model. The details of
implementation and performance evaluation are discussed. The experiments are
conducted on a training corpus of size 95,170 tokens and test corpus of 5,000
tokens. It is observed that the model works with Precision, Recall and
F1-measure of 83%, 79% and 81% respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04386</identifier>
 <datestamp>2015-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04386</id><created>2015-09-14</created><updated>2015-12-23</updated><authors><author><keyname>Kolchinsky</keyname><forenames>Artemy</forenames></author><author><keyname>Gates</keyname><forenames>Alexander J.</forenames></author><author><keyname>Rocha</keyname><forenames>Luis M.</forenames></author></authors><title>Modularity and the spread of perturbations in complex dynamical systems</title><categories>physics.soc-ph cs.SI nlin.AO physics.data-an</categories><journal-ref>Phys. Rev. E 92, 060801 (2015)</journal-ref><doi>10.1103/PhysRevE.92.060801</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a method to decompose dynamical systems based on the idea that
modules constrain the spread of perturbations. We find partitions of system
variables that maximize 'perturbation modularity', defined as the
autocovariance of coarse-grained perturbed trajectories. The measure
effectively separates the fast intramodular from the slow intermodular dynamics
of perturbation spreading (in this respect, it is a generalization of the
'Markov stability' method of network community detection). Our approach
captures variation of modular organization across different system states, time
scales, and in response to different kinds of perturbations: aspects of
modularity which are all relevant to real-world dynamical systems. It offers a
principled alternative to detecting communities in networks of statistical
dependencies between system variables (e.g., 'relevance networks' or
'functional networks'). Using coupled logistic maps, we demonstrate that the
method uncovers hierarchical modular organization planted in a system's
coupling matrix. Additionally, in homogeneously-coupled map lattices, it
identifies the presence of self-organized modularity that depends on the
initial state, dynamical parameters, and type of perturbations. Our approach
offers a powerful tool for exploring the modular organization of complex
dynamical systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04387</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04387</id><created>2015-09-14</created><authors><author><keyname>Chadha</keyname><forenames>Aman</forenames></author><author><keyname>Mallik</keyname><forenames>Sushmit</forenames></author><author><keyname>Chadha</keyname><forenames>Ankit</forenames></author><author><keyname>Johar</keyname><forenames>Ravdeep</forenames></author><author><keyname>Roja</keyname><forenames>M. Mani</forenames></author></authors><title>Dual-Layer Video Encryption using RSA Algorithm</title><categories>cs.CR cs.MM</categories><comments>arXiv admin note: text overlap with arXiv:1104.0800, arXiv:1112.0836
  by other authors</comments><journal-ref>International Journal of Computer Applications 116(1):33-40, April
  2015</journal-ref><doi>10.5120/20302-2341</doi><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  This paper proposes a video encryption algorithm using RSA and Pseudo Noise
(PN) sequence, aimed at applications requiring sensitive video information
transfers. The system is primarily designed to work with files encoded using
the Audio Video Interleaved (AVI) codec, although it can be easily ported for
use with Moving Picture Experts Group (MPEG) encoded files. The audio and video
components of the source separately undergo two layers of encryption to ensure
a reasonable level of security. Encryption of the video component involves
applying the RSA algorithm followed by the PN-based encryption. Similarly, the
audio component is first encrypted using PN and further subjected to encryption
using the Discrete Cosine Transform. Combining these techniques, an efficient
system, invulnerable to security breaches and attacks with favorable values of
parameters such as encryption/decryption speed, encryption/decryption ratio and
visual degradation; has been put forth. For applications requiring encryption
of sensitive data wherein stringent security requirements are of prime concern,
the system is found to yield negligible similarities in visual perception
between the original and the encrypted video sequence. For applications wherein
visual similarity is not of major concern, we limit the encryption task to a
single level of encryption which is accomplished by using RSA, thereby
quickening the encryption process. Although some similarity between the
original and encrypted video is observed in this case, it is not enough to
comprehend the happenings in the video.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04393</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04393</id><created>2015-09-15</created><authors><author><keyname>Liu</keyname><forenames>Haitao</forenames></author><author><keyname>Xu</keyname><forenames>Chunshan</forenames></author><author><keyname>Liang</keyname><forenames>Junying</forenames></author></authors><title>Dependency length minimization: Puzzles and Promises</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the recent issue of PNAS, Futrell et al. claims that their study of 37
languages gives the first large scale cross-language evidence for Dependency
Length Minimization, which is an overstatement that ignores similar previous
researches. In addition,this study seems to pay no attention to factors like
the uniformity of genres,which weakens the validity of the argument that DLM is
universal. Another problem is that this study sets the baseline random language
as projective, which fails to truly uncover the difference between natural
language and random language, since projectivity is an important feature of
many natural languages. Finally, the paper contends an &quot;apparent relationship
between head finality and dependency length&quot; despite the lack of an explicit
statistical comparison, which renders this conclusion rather hasty and
improper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04394</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04394</id><created>2015-09-15</created><authors><author><keyname>Adnan</keyname><forenames>Asif M</forenames></author><author><keyname>Radhakrishnan</keyname><forenames>Sridhar</forenames></author><author><keyname>Karabuk</keyname><forenames>Suleyman</forenames></author></authors><title>Efficient Kernel Fusion Techniques for Massive Video Data Analysis on
  GPGPUs</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Kernels are executable code segments and kernel fusion is a technique for
combing the segments in a coherent manner to improve execution time. For the
first time, we have developed a technique to fuse image processing kernels to
be executed on GPGPUs for improving execution time and total throughput (amount
of data processed in unit time). We have applied our techniques for feature
tracking on video images captured by a high speed digital video camera where
the number of frames captured varies between 600-1000 frames per second. Image
processing kernels are composed of multiple simple kernels, which executes on
the input image in a given sequence. A set of kernels that can be fused
together forms a partition (or fused kernel). Given a set of Kernels and the
data dependencies between them, it is difficult to determine the partitions of
kernels such that the total performance is maximized (execution time and
throughput). We have developed and implemented an optimization model to find
such a partition. We also developed an algorithm to fuse multiple kernels based
on their data dependencies. Additionally, to further improve performance on
GPGPU systems, we have provided methods to distribute data and threads to
processors. Our model was able to reduce data traffic, which resulted better
performance.The performance (both execution time and throughput) of the
proposed method for kernel fusing and its subsequent execution is shown to be 2
to 3 times higher than executing kernels in sequence. We have demonstrated our
technique for facial feature tracking with applications to Neuroscience.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04397</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04397</id><created>2015-09-15</created><authors><author><keyname>Gunasekar</keyname><forenames>Suriya</forenames></author><author><keyname>Ravikumar</keyname><forenames>Pradeep</forenames></author><author><keyname>Ghosh</keyname><forenames>Joydeep</forenames></author></authors><title>Exponential Family Matrix Completion under Structural Constraints</title><categories>stat.ML cs.LG</categories><comments>20 pages, 9 figures</comments><journal-ref>Gunasekar, Suriya, Pradeep Ravikumar, and Joydeep Ghosh.
  &quot;Exponential family matrix completion under structural constraints&quot;.
  Proceedings of The 31st International Conference on Machine Learning, pp.
  1917-1925, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the matrix completion problem of recovering a structured matrix
from noisy and partial measurements. Recent works have proposed tractable
estimators with strong statistical guarantees for the case where the underlying
matrix is low--rank, and the measurements consist of a subset, either of the
exact individual entries, or of the entries perturbed by additive Gaussian
noise, which is thus implicitly suited for thin--tailed continuous data.
Arguably, common applications of matrix completion require estimators for (a)
heterogeneous data--types, such as skewed--continuous, count, binary, etc., (b)
for heterogeneous noise models (beyond Gaussian), which capture varied
uncertainty in the measurements, and (c) heterogeneous structural constraints
beyond low--rank, such as block--sparsity, or a superposition structure of
low--rank plus elementwise sparseness, among others. In this paper, we provide
a vastly unified framework for generalized matrix completion by considering a
matrix completion setting wherein the matrix entries are sampled from any
member of the rich family of exponential family distributions; and impose
general structural constraints on the underlying matrix, as captured by a
general regularizer $\mathcal{R}(.)$. We propose a simple convex regularized
$M$--estimator for the generalized framework, and provide a unified and novel
statistical analysis for this general class of estimators. We finally
corroborate our theoretical results on simulated datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04399</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04399</id><created>2015-09-15</created><authors><author><keyname>Sarvadevabhatla</keyname><forenames>Ravi Kiran</forenames></author><author><keyname>R</keyname><forenames>Venkatesh Babu</forenames></author></authors><title>Analyzing structural characteristics of object category representations
  from their semantic-part distributions</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Studies from neuroscience show that part-mapping computations are employed by
human visual system in the process of object recognition. In this work, we
present an approach for analyzing semantic-part characteristics of object
category representations. For our experiments, we use category-epitome, a
recently proposed sketch-based spatial representation for objects. To enable
part-importance analysis, we first obtain semantic-part annotations of
hand-drawn sketches originally used to construct the corresponding epitomes. We
then examine the extent to which the semantic-parts are present in the epitomes
of a category and visualize the relative importance of parts as a word cloud.
Finally, we show how such word cloud visualizations provide an intuitive
understanding of category-level structural trends that exist in the
category-epitome object representations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04417</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04417</id><created>2015-09-15</created><authors><author><keyname>Haribabu</keyname><forenames>K.</forenames></author><author><keyname>Reddy</keyname><forenames>Dayakar</forenames></author><author><keyname>Hota</keyname><forenames>Chittaranjan</forenames></author><author><keyname>Yl&#xe4;-J&#xe4;&#xe4;ski</keyname><forenames>Antii</forenames></author><author><keyname>Tarkoma</keyname><forenames>Sasu</forenames></author></authors><title>Adaptive Lookup for Unstructured Peer-to-Peer Overlays</title><categories>cs.NI cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scalability and efficient global search in unstructured peer-to-peer overlays
have been extensively studied in the literature. The global search comes at the
expense of local interactions between peers. Most of the unstructured
peer-to-peer overlays do not provide any performance guarantee. In this work we
propose a novel Quality of Service enabled lookup for unstructured peer-to-peer
overlays that will allow the user's query to traverse only those overlay links
which satisfy the given constraints. Additionally, it also improves the
scalability by judiciously using the overlay resources. Our approach
selectively forwards the queries using QoS metrics like latency, bandwidth, and
overlay link status so as to ensure improved performance in a scenario where
the degree of peer joins and leaves are high. User is given only those results
which can be downloaded with the given constraints. Also, the protocol aims at
minimizing the message overhead over the overlay network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04420</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04420</id><created>2015-09-15</created><authors><author><keyname>Heras</keyname><forenames>J&#xf3;nathan</forenames></author><author><keyname>Mata</keyname><forenames>Gadea</forenames></author><author><keyname>Cuesto</keyname><forenames>Germ&#xe1;n</forenames></author><author><keyname>Rubio</keyname><forenames>Julio</forenames></author><author><keyname>Morales</keyname><forenames>Miguel</forenames></author></authors><title>Neuron detection in stack images: a persistent homology interpretation</title><categories>cs.CV q-bio.NC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automation and reliability are the two main requirements when computers are
applied in Life Sciences. In this paper we report on an application to neuron
recognition, an important step in our long-term project of providing software
systems to the study of neural morphology and functionality from biomedical
images. Our algorithms have been implemented in an ImageJ plugin called
NeuronPersistentJ, which has been validated experimentally. The soundness and
reliability of our approach are based on the interpretation of our processing
methods with respect to persistent homology, a well-known tool in computational
mathematics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04425</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04425</id><created>2015-09-15</created><authors><author><keyname>Lovelace</keyname><forenames>Robin</forenames></author><author><keyname>Goodman</keyname><forenames>Anna</forenames></author><author><keyname>Aldred</keyname><forenames>Rachel</forenames></author><author><keyname>Berkoff</keyname><forenames>Nikolai</forenames></author><author><keyname>Abbas</keyname><forenames>Ali</forenames></author><author><keyname>Woodcock</keyname><forenames>James</forenames></author></authors><title>The Propensity to Cycle Tool: An open source online system for
  sustainable transport planning</title><categories>cs.CY</categories><comments>Paper submitted for peer review. 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Encouraging cycling, as part of a wider sustainable mobility strategy, is an
increasingly common objective in transport planning institutions worldwide.
Emerging evidence shows that providing appropriate high-quality infrastructure
can boost local cycling rates. To maximize the benefits and cost-effectiveness
of new infrastructure, it is important to build in the right places. Cycle
paths, for example, will have the greatest impact if they are constructed along
'desire lines' of greatest latent demand. The Propensity to Cycle Tool (PCT)
seeks to inform such decisions by providing an evidence-based support tool that
models current and potential future distributions and volumes of cycling across
cities and regions. This paper describes this model and its application to case
study cities in England. Origin-destination (OD) data, combined with
quantitative information at the level of administrative zones, form the basis
of the model, which estimates cycling potential as a function of route
distance, hilliness and other factors at the OD and area level. Multiple
scenarios were generated and interactively displayed. These were: 'Government
Target', in which the rate of cycling doubles in England; 'Gender Equality', in
which women cycle as much as men; 'Go Dutch', in which English people cycle as
much as people in the Netherlands; and 'E-bikes', an exploratory analysis of
increasing the distance people are willing to cycle due to new technology. The
model is freely available online and can be accessed at
[geo8.webarch.net/master/](http://geo8.webarch.net/master/). This paper also
explains how the PCT's open source approach allows it to be deployed in new
cities and countries. We conclude that the method presented has potential to
assist with planning for cycling-dominated cities worldwide, which can in turn
assist with the global transition away from fossil fuels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04438</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04438</id><created>2015-09-15</created><updated>2016-02-22</updated><authors><author><keyname>Strau&#xdf;</keyname><forenames>Tobias</forenames></author><author><keyname>Leifert</keyname><forenames>Gundram</forenames></author><author><keyname>Gr&#xfc;ning</keyname><forenames>Tobias</forenames></author><author><keyname>Labahn</keyname><forenames>Roger</forenames></author></authors><title>Regular expressions for decoding of neural network outputs</title><categories>cs.NE</categories><comments>21 pages, 8 (+2) figures, 2 tables</comments><msc-class>49L20, 90C39, 82C32</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article proposes a convenient tool for decoding the output of neural
networks trained by Connectionist Temporal Classification (CTC) for handwritten
text recognition. We use regular expressions to describe the complex structures
expected in the writing. The corresponding finite automata are employed to
build a decoder. We analyze theoretically which calculations are relevant and
which can be avoided. A great speed-up results from an approximation. We
conclude that the approximation most likely fails if the regular expression
does not match the ground truth which is not harmful for many applications
since the low probability will be even underestimated. The proposed decoder is
very efficient compared to other decoding methods. The variety of applications
reaches from information retrieval to full text recognition. We refer to
applications where we integrated the proposed decoder successfully.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04442</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04442</id><created>2015-09-15</created><authors><author><keyname>Zhang</keyname><forenames>Meng</forenames></author><author><keyname>Liu</keyname><forenames>Yuan</forenames></author></authors><title>Energy Harvesting for Physical-Layer Security in OFDMA Networks</title><categories>cs.IT math.IT</categories><comments>to appear in IEEE Trans. Inf. Foren. Sec. arXiv admin note: text
  overlap with arXiv:1409.3635</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the simultaneous wireless information and power
transfer (SWIPT) in downlink multiuser orthogonal frequency-division multiple
access (OFDMA) systems, where each user applies power splitting scheme to
coordinate the energy harvesting and secrecy information decoding processes.
Assuming equal power allocation across subcarriers, we formulate the problem to
maximize the aggregate harvested power of all users while satisfying secrecy
rate requirement of individual user by joint subcarrier allocation and optimal
power splitting ratio selection. Due to the NP-hardness of the problem, we
propose two suboptimal algorithms to solve the problem in the dual domain. The
first one is an iterative algorithm that optimizes subcarrier allocation and
power splitting in an alternating way. The second algorithm is based on a
two-step approach that solves the subcarrier allocation and power splitting
sequentially. The numerical results show that the proposed methods outperform
conventional methods. It is also shown that the iterative algorithm performs
close to the upper bound and the step-wise algorithm provides good tradeoffs
between performance and complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04465</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04465</id><created>2015-09-15</created><authors><author><keyname>Chattopadhyay</keyname><forenames>Amit</forenames></author><author><keyname>Carr</keyname><forenames>Hamish</forenames></author><author><keyname>Duke</keyname><forenames>David</forenames></author><author><keyname>Geng</keyname><forenames>Zhao</forenames></author><author><keyname>Saeki</keyname><forenames>Osamu</forenames></author></authors><title>Multivariate Topology Simplification</title><categories>cs.CG</categories><comments>Under Review in Journal</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Topological simplification of scalar and vector fields is well-established as
an effective method for analysing and visualising complex data sets. For
multi-field data, topological analysis requires simultaneous advances both
mathematically and computationally. We propose a robust multivariate topology
simplification method based on ``lip''-pruning from the Reeb Space.
Mathematically, we show that the projection of the Jacobi Set of multivariate
data into the Reeb Space produces a Jacobi Structure that separates the Reeb
Space into simple components. We also show that the dual graph of these
components gives rise to a Reeb Skeleton that has properties similar to the
scalar contour tree and Reeb Graph, for topologically simple domains. We then
introduce a range measure to give a scaling-invariant total ordering of the
components or features that can be used for simplification. Computationally, we
show how to compute Jacobi Structure, Reeb Skeleton, Range and Geometric
Measures in the Joint Contour Net (an approximation of the Reeb Space) and that
these can be used for visualisation similar to the contour tree or Reeb Graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04473</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04473</id><created>2015-09-15</created><authors><author><keyname>Daiber</keyname><forenames>Joachim</forenames></author><author><keyname>Quiroz</keyname><forenames>Lautaro</forenames></author><author><keyname>Wechsler</keyname><forenames>Roger</forenames></author><author><keyname>Frank</keyname><forenames>Stella</forenames></author></authors><title>Splitting Compounds by Semantic Analogy</title><categories>cs.CL</categories><journal-ref>Proceedings of the 1st Deep Machine Translation Workshop. Prague,
  Czech Republic. 2015</journal-ref><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Compounding is a highly productive word-formation process in some languages
that is often problematic for natural language processing applications. In this
paper, we investigate whether distributional semantics in the form of word
embeddings can enable a deeper, i.e., more knowledge-rich, processing of
compounds than the standard string-based methods. We present an unsupervised
approach that exploits regularities in the semantic vector space (based on
analogies such as &quot;bookshop is to shop as bookshelf is to shelf&quot;) to produce
compound analyses of high quality. A subsequent compound splitting algorithm
based on these analyses is highly effective, particularly for ambiguous
compounds. German to English machine translation experiments show that this
semantic analogy-based compound splitter leads to better translations than a
commonly used frequency-based method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04491</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04491</id><created>2015-09-15</created><authors><author><keyname>Byrne</keyname><forenames>Evan</forenames></author><author><keyname>Schniter</keyname><forenames>Philip</forenames></author></authors><title>Sparse Multinomial Logistic Regression via Approximate Message Passing</title><categories>cs.IT math.IT stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For the problem of multi-class linear classification and feature selection,
we propose approximate message passing approaches to sparse multinomial
logistic regression. First, we propose two algorithms based on the Hybrid
Generalized Approximate Message Passing (HyGAMP) framework: one finds the
maximum a posteriori (MAP) linear classifier and the other finds an
approximation of the test-error-rate minimizing linear classifier. Then we
design computationally simplified variants of these two algorithms. Next, we
detail methods to tune the hyperparameters of their assumed statistical models
using Stein's unbiased risk estimate (SURE) and expectation-maximization (EM),
respectively. Finally, using both synthetic and real-world datasets, we
demonstrate improved error-rate and runtime performance relative to
state-of-the-art existing approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04492</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04492</id><created>2015-09-15</created><authors><author><keyname>Heide</keyname><forenames>Janus</forenames></author><author><keyname>Pedersen</keyname><forenames>Morten V.</forenames></author><author><keyname>Fitzek</keyname><forenames>Frank H. P.</forenames></author><author><keyname>edard</keyname><forenames>Muriel M</forenames></author></authors><title>Perpetual Codes for Network Coding</title><categories>cs.NI cs.IT math.IT</categories><comments>13 pages, 9 figures, original draft from 2012 included in this phd
  thesis:
  http://vbn.aau.dk/files/123241894/Thesis_Low_computational_complexity_network_coding_for_mobile_networks_.pdf</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Random Linear Network Coding (RLNC) provides a theoretically efficient method
for coding. Some of its practical drawbacks are the complexity of decoding and
the overhead due to the coding vectors. For computationally weak and
battery-driven platforms, these challenges are particular important. In this
work, we consider the coding variant Perpetual codes which are sparse,
non-uniform and the coding vectors have a compact representation. The sparsity
allows for fast encoding and decoding, and the non-uniform protection of
symbols enables recoding where the produced symbols are indistinguishable from
those encoded at the source. The presented results show that the approach can
provide a coding overhead arbitrarily close to that of RLNC, but at reduced
computational load. The achieved gain over RLNC grows with the generation size,
and both encoding and decoding throughput is approximately one order of
magnitude higher compared to RLNC at a generation size of 2048. Additionally,
the approach allows for easy adjustment between coding throughput and code
overhead, which makes it suitable for a broad range of platforms and
applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04498</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04498</id><created>2015-09-15</created><authors><author><keyname>Greifenberg</keyname><forenames>Timo</forenames></author><author><keyname>H&#xf6;lldobler</keyname><forenames>Katrin</forenames></author><author><keyname>Kolassa</keyname><forenames>Carsten</forenames></author><author><keyname>Look</keyname><forenames>Markus</forenames></author><author><keyname>Nazari</keyname><forenames>Pedram Mir Seyed</forenames></author><author><keyname>M&#xfc;ller</keyname><forenames>Klaus</forenames></author><author><keyname>Perez</keyname><forenames>Antonio Navarro</forenames></author><author><keyname>Plotnikov</keyname><forenames>Dimitri</forenames></author><author><keyname>Reiss</keyname><forenames>Dirk</forenames></author><author><keyname>Roth</keyname><forenames>Alexander</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author><author><keyname>Schindler</keyname><forenames>Martin</forenames></author><author><keyname>Wortmann</keyname><forenames>Andreas</forenames></author></authors><title>A Comparison of Mechanisms for Integrating Handwritten and Generated
  Code for Object-Oriented Programming Languages</title><categories>cs.SE</categories><comments>12 pages, 7 figures, 1 table, Proceedings of the 3rd International
  Conference on Model-Driven Engineering and Software Development. Angers,
  Loire Valley, France, pp. 74-85, 2015</comments><journal-ref>Proceedings of the 3rd International Conference on Model-Driven
  Engineering and Software Development. Angers, Loire Valley, France, pp.
  74-85, 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Code generation from models is a core activity in model-driven development
(MDD). For complex systems it is usually impossible to generate the entire
software system from models alone. Thus, MDD requires mechanisms for
integrating generated and handwritten code. Applying such mechanisms without
considering their effects can cause issues in projects with many model and code
artifacts, where a sound integration for generated and handwritten code is
necessary. We provide an overview of mechanisms for integrating generated and
handwritten code for object-oriented languages. In addition to that, we define
and apply criteria to compare these mechanisms. The results are intended to
help MDD tool developers in choosing an appropriate integration mechanism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04502</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04502</id><created>2015-09-15</created><authors><author><keyname>Haber</keyname><forenames>Arne</forenames></author><author><keyname>Look</keyname><forenames>Markus</forenames></author><author><keyname>Perez</keyname><forenames>Antonio Navarro</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author><author><keyname>V&#xf6;lkel</keyname><forenames>Steven</forenames></author><author><keyname>Wortmann</keyname><forenames>Andreas</forenames></author></authors><title>Integration of Heterogeneous Modeling Languages via Extensible and
  Composable Language Components</title><categories>cs.SE</categories><comments>12 pages, 11 figures. Proceedings of the 3rd International Conference
  on Model-Driven Engineering and Software Development. Angers, Loire Valley,
  France, pp. 19-31, 2015</comments><journal-ref>Proceedings of the 3rd International Conference on Model-Driven
  Engineering and Software Development. Angers, Loire Valley, France, pp.
  19-31, 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Effective model-driven engineering of complex systems requires to
appropriately describe different specific system aspects. To this end,
efficient integration of different heterogeneous modeling languages is
essential. Modeling language integaration is onerous and requires in-depth
conceptual and technical knowledge and ef- fort. Traditional modeling lanugage
integration approches require language engineers to compose monolithic language
aggregates for a specific task or project. Adapting these aggregates cannot be
to different contexts requires vast effort and makes these hardly reusable.
This contribution presents a method for the engineering of grammar-based
language components that can be independently developed, are syntactically
composable, and ultimately reusable. To this end, it introduces the concepts of
language aggregation, language embed- ding, and language inheritance, as well
as their realization in the language workbench MontiCore. The result is a
generalizable, systematic, and efficient syntax-oriented composition of
languages that allows the agile employment of modeling languages efficiently
tailored for individual software projects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04505</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04505</id><created>2015-09-15</created><authors><author><keyname>Ringert</keyname><forenames>Jan Oliver</forenames></author><author><keyname>Rumpe</keyname><forenames>Bernhard</forenames></author><author><keyname>Wortmann</keyname><forenames>Andreas</forenames></author></authors><title>Architecture and Behavior Modeling of Cyber-Physical Systems with
  MontiArcAutomaton</title><categories>cs.SE</categories><comments>89 pages, 10 figures, 36 listings. Shaker Verlag, ISBN
  978-3-8440-3120-1. Aachener Informatik-Berichte, Software Engineering, Band
  20. 2014</comments><journal-ref>Shaker Verlag, ISBN 978-3-8440-3120-1. Aachener
  Informatik-Berichte, Software Engineering, Band 20. 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This book presents MontiArcAutomaton, a modeling language for architecture
and be- havior modeling of Cyber-Physical Systems as interactive Component &amp;
Connector mod- els. MontiArcAutomaton extends the Architecture Description
Language MontiArc with automata to describe component behavior. The modeling
language MontiArcAutomaton provides syntactical elements for defin- ing
automata with states, variables, and transitions inside MontiArc components.
These syntactical elements and a basic set of well-formedness rules provide the
syntax for a fam- ily of modeling languages for state-based behavior modeling
in Component &amp; Connector architectures. We present two concrete language
profiles with additional well-formedness rules to model time-synchronous
component behavior and untimed, event-driven behav- ior of components. This
book gives an overview of the MontiArcAutomaton language including examples, a
language reference, and a context-free grammar for MontiArcAutomaton models. It
also provides syntax definition, well-formedness rules, and semantics for two
language profiles. We summarize projects and case studies applying
MontiArcAutomaton. MontiArcAutomaton is implemented using the DSL framework
MontiCore. Available tools include a textual editor with syntax highlighting
and code completion as well as a graphical editor and a powerful and extensible
code generation framework for target languages including EMF, Java, Mona, and
Python.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04513</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04513</id><created>2015-09-15</created><authors><author><keyname>Nguyen</keyname><forenames>Vinh</forenames></author><author><keyname>Bodenreider</keyname><forenames>Olivier</forenames></author><author><keyname>Thirunarayan</keyname><forenames>Krishnaprasad</forenames></author><author><keyname>Fu</keyname><forenames>Gang</forenames></author><author><keyname>Bolton</keyname><forenames>Evan</forenames></author><author><keyname>Rosinach</keyname><forenames>N&#xfa;ria Queralt</forenames></author><author><keyname>Furlong</keyname><forenames>Laura I.</forenames></author><author><keyname>Dumontier</keyname><forenames>Michel</forenames></author><author><keyname>Sheth</keyname><forenames>Amit</forenames></author></authors><title>On Reasoning with RDF Statements about Statements using Singleton
  Property Triples</title><categories>cs.AI cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Singleton Property (SP) approach has been proposed for representing and
querying metadata about RDF triples such as provenance, time, location, and
evidence. In this approach, one singleton property is created to uniquely
represent a relationship in a particular context, and in general, generates a
large property hierarchy in the schema. It has become the subject of important
questions from Semantic Web practitioners. Can an existing reasoner recognize
the singleton property triples? And how? If the singleton property triples
describe a data triple, then how can a reasoner infer this data triple from the
singleton property triples? Or would the large property hierarchy affect the
reasoners in some way? We address these questions in this paper and present our
study about the reasoning aspects of the singleton properties. We propose a
simple mechanism to enable existing reasoners to recognize the singleton
property triples, as well as to infer the data triples described by the
singleton property triples. We evaluate the effect of the singleton property
triples in the reasoning processes by comparing the performance on RDF datasets
with and without singleton properties. Our evaluation uses as benchmark the
LUBM datasets and the LUBM-SP datasets derived from LUBM with temporal
information added through singleton properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04515</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04515</id><created>2015-09-15</created><authors><author><keyname>Orduna-Malea</keyname><forenames>Enrique</forenames></author><author><keyname>Ayll&#xf3;n</keyname><forenames>Juan Manuel</forenames></author><author><keyname>Mart&#xed;n-Mart&#xed;n</keyname><forenames>Alberto</forenames></author><author><keyname>L&#xf3;pez-C&#xf3;zar</keyname><forenames>Emilio Delgado</forenames></author></authors><title>Improvements in Google Scholar Citations are for the summer: creating an
  institutional affiliation link feature</title><categories>cs.DL</categories><comments>20 pages, 21 figures</comments><report-no>14</report-no><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  This report describes the feature introduced by Google to provide
standardized access to institutional affiliations within Google Scholar
Citations. First, this new tool is described, pointing out its main
characteristics and functioning. Next, the coverage and precision of the tool
are evaluated. Two special cases (Google Inc. and Spanish Universities) are
briefly treated with the purpose of illustrating some aspects about the
accuracy of the tool for the task of gathering authors within their appropriate
institution. Finally, some inconsistencies, errors and malfunctioning are
identified, categorized and described. The report finishes by providing some
suggestions to improve the feature. The general conclusion is that the
standardized institutional affiliation link provided by Google Scholar
Citations, despite working pretty well for a large number of institutions
(especially Anglo-Saxon universities) still has a number of shortcomings and
pitfalls which need to be addressed in order to make this authority control
tool fully useful worldwide, both for searching purposes and for metric tasks
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04518</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04518</id><created>2015-09-15</created><authors><author><keyname>Sergeyev</keyname><forenames>Yaroslav D.</forenames></author><author><keyname>Kvasov</keyname><forenames>Dmitri E.</forenames></author></authors><title>A deterministic global optimization using smooth diagonal auxiliary
  functions</title><categories>math.OC cs.MS math.NA</categories><comments>25 pages, 7 figures, 3 tables</comments><msc-class>65K05, 90C26, 90C56</msc-class><acm-class>G.1.6; G.1.0</acm-class><journal-ref>Communications in Nonlinear Science and Numerical Simulation,
  2015, 21, 99-111</journal-ref><doi>10.1016/j.cnsns.2014.08.026</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many practical decision-making problems it happens that functions involved
in optimization process are black-box with unknown analytical representations
and hard to evaluate. In this paper, a global optimization problem is
considered where both the goal function~$f(x)$ and its gradient $f'(x)$ are
black-box functions. It is supposed that $f'(x)$ satisfies the Lipschitz
condition over the search hyperinterval with an unknown Lipschitz constant~$K$.
A new deterministic `Divide-the-Best' algorithm based on efficient diagonal
partitions and smooth auxiliary functions is proposed in its basic version, its
convergence conditions are studied and numerical experiments executed on eight
hundred test functions are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04521</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04521</id><created>2015-09-15</created><authors><author><keyname>Phogat</keyname><forenames>Karmvir Singh</forenames></author><author><keyname>Chatterjee</keyname><forenames>Debasish</forenames></author><author><keyname>Banavar</keyname><forenames>Ravi</forenames></author></authors><title>Multiple shooting technique for optimal attitude control of a spacecraft
  with momentum and control constraints</title><categories>cs.SY math.OC</categories><msc-class>49J21, 49N90</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article addresses an optimal control problem arising in attitude control
of a spacecraft under state and control constraints. The attitude dynamics
model is first derived in discrete time using discrete mechanics. Then the
energy optimal control problem is posed in discrete time and first order
necessary conditions are derived using variational analysis. Since both the
control and the states are subject to inequality constraints, the boundary
value problem obtained as first order necessary conditions is subject to
inequality constraints on the states and the Lagrange's multipliers
corresponding to the state inequality constraints. This boundary value problem
cannot be solved using classical multiple shooting techniques, so we propose a
modified multiple shooting algorithm that can handle the state inequality
constraints. We also demonstrate how the discrete time model derived using
discrete mechanics reduces the dimension of the problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04524</identifier>
 <datestamp>2016-03-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04524</id><created>2015-09-15</created><updated>2016-02-28</updated><authors><author><keyname>Renaville</keyname><forenames>Fran&#xe7;ois</forenames></author></authors><title>Open Access and Discovery Tools: How do Primo Libraries Manage Green
  Open Access Collections?</title><categories>cs.DL cs.IR</categories><comments>24 pages, 8 figures, 1 appendix</comments><journal-ref>Varnum, Ken (ed.). (2016). Exploring Discovery: The Front Door to
  Your Library's Licensed and Digitized Content&quot;. ALA Editions. 233-256 pp</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Scholarly Open Access repositories contain lots of treasures including rare
or otherwise unpublished materials and articles that scholars self-archive,
often as part of their institution's mandate. But it can be hard to discover
this material unless users know exactly where to look. Since the very
beginning, libraries have played a major role in supporting the OA movement.
Next to all services they can provide to support the deposit of research output
in the repositories, they can make Open Access materials widely discoverable by
their patrons through general search engines (Google, Bing...), specialized
search engines (like Google Scholar) and library discovery tools, thus
expanding their collection to include materials that they would not necessarily
pay for. In this paper, we intend to focus on two aspects regarding Open Access
and Primo discovery tool. In early 2013, Ex Libris Group started to add
institutional repositories to Primo Central Index (PCI), their mega-aggregation
of hundreds of millions of scholarly e-resources. After 2 years, it may be
interesting to take stock of the current situation of PCI regarding Open Access
institutional repositories. On basis of a survey to carry out among the Primo
community, the paper also shows how libraries using Primo discovery tool
integrate Green Open Access contents in their catalog. Two major ways are
possible for them: Firstly, they can directly harvest, index and manage any
repository in their Primo and display those free contents next to the more
traditional library collections; Secondly, if they are PCI subscribers, they
can quickly and easily activate any, if not all, of the Open Access
repositories contained PCI, making thus the contents of those directly
discoverable to their end users.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04525</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04525</id><created>2015-09-15</created><authors><author><keyname>Alsarem</keyname><forenames>Mazen</forenames><affiliation>LIRIS</affiliation></author><author><keyname>Portier</keyname><forenames>Pierre-Edouard</forenames><affiliation>LIRIS</affiliation></author><author><keyname>Calabretto</keyname><forenames>Sylvie</forenames><affiliation>LIRIS</affiliation></author><author><keyname>Kosch</keyname><forenames>Harald</forenames><affiliation>FMI</affiliation></author></authors><title>Ranking Entities in the Age of Two Webs, an Application to Semantic
  Snippets</title><categories>cs.IR</categories><proxy>ccsd</proxy><journal-ref>Extended Semantic Web Conference ESWC2015, May 2015, Portoroz,
  Slovenia. 9088, pp.541-555, 2015, The Semantic Web. Latest Advances and New
  Domains. \&amp;lt;10.1007/978-3-319-18818-8\_33\&amp;gt;</journal-ref><doi>10.1007/978-3-319-18818-8_33</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The advances of the Linked Open Data (LOD) initiative are giving rise to a
more structured Web of data. Indeed, a few datasets act as hubs (e.g., DBpedia)
connecting many other datasets. They also made possible new Web services for
entity detection inside plain text (e.g., DBpedia Spotlight), thus allowing for
new applications that can benefit from a combination of the Web of documents
and the Web of data. To ease the emergence of these new applications, we
propose a query-biased algorithm (LDRANK) for the ranking of web of data
resources with associated textual data. Our algorithm combines link analysis
with dimensionality reduction. We use crowdsourcing for building a publicly
available and reusable dataset for the evaluation of query-biased ranking of
Web of data resources detected in Web pages. We show that, on this dataset,
LDRANK outperforms the state of the art. Finally, we use this algorithm for the
construction of semantic snippets of which we evaluate the usefulness with a
crowdsourcing-based approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04538</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04538</id><created>2015-09-15</created><authors><author><keyname>Anderson</keyname><forenames>Brian D. O.</forenames></author><author><keyname>Mou</keyname><forenames>Shaoshuai</forenames></author><author><keyname>Morse</keyname><forenames>A. Stephen</forenames></author><author><keyname>Helmke</keyname><forenames>Uwe</forenames></author></authors><title>Decentralized gradient algorithm for solution of a linear equation</title><categories>cs.SY cs.DC</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper develops a technique for solving a linear equation $Ax=b$ with a
square and nonsingular matrix $A$, using a decentralized gradient algorithm. In
the language of control theory, there are $n$ agents, each storing at time $t$
an $n$-vector, call it $x_i(t)$, and a graphical structure associating with
each agent a vertex of a fixed, undirected and connected but otherwise
arbitrary graph $\mathcal G$ with vertex set and edge set $\mathcal V$ and
$\mathcal E$ respectively. We provide differential equation update laws for the
$x_i$ with the property that each $x_i$ converges to the solution of the linear
equation exponentially fast. The equation for $x_i$ includes additive terms
weighting those $x_j$ for which vertices in $\mathcal G$ corresponding to the
$i$-th and $j$-th agents are adjacent. The results are extended to the case
where $A$ is not square but has full row rank, and bounds are given on the
convergence rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04545</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04545</id><created>2015-09-15</created><authors><author><keyname>Kamal</keyname><forenames>Rossi</forenames></author><author><keyname>Hong</keyname><forenames>Choong Seon</forenames></author></authors><title>Resilient Big Data Monetization</title><categories>cs.NI</categories><comments>8 pages, NOMS 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Resilient Big Data monetization is devised as k-dominance and m-connectivity
problems, such that common-interests are connected by k-ways to measurement
tools, which are tied within each other in m-ways. Consequently, a greedy
approximation algorithm Plutus (i.e resembling Greek god of wealth) is
proposed, which isolates measurement tools to ac-quire domination over
common-interests, establishes synergy from common-interests to measurement
tools and then acquires divergence and sustains it within measurement tools
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04549</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04549</id><created>2015-09-15</created><authors><author><keyname>Thorup</keyname><forenames>Mikkel</forenames></author></authors><title>Linear Probing with 5-Independent Hashing</title><categories>cs.DS</categories><comments>arXiv admin note: text overlap with arXiv:1505.01523</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  These lecture notes show that linear probing takes expected constant time if
the hash function is 5-independent. This result was first proved by Pagh et al.
[STOC'07,SICOMP'09]. The simple proof here is essentially taken from [Patrascu
and Thorup ICALP'10]. The lecture is a nice illustration of the use of higher
moments in data structures, and could be used in a course on randomized
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04555</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04555</id><created>2015-09-15</created><authors><author><keyname>Rosas</keyname><forenames>Fernando</forenames></author><author><keyname>Ntranos</keyname><forenames>Vasilis</forenames></author><author><keyname>Ellison</keyname><forenames>Christopher J.</forenames></author><author><keyname>Pollin</keyname><forenames>Sofie</forenames></author><author><keyname>Verhelst</keyname><forenames>Marian</forenames></author></authors><title>Understanding interdependency through complex information sharing</title><categories>cs.IT math.IT</categories><comments>39 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The interactions between three or more random variables are often nontrivial,
poorly understood, and yet, are paramount for future advances in fields such as
network information theory, neuroscience, genetics and many others. In this
work, we propose to analyze these interactions as different modes of
information sharing. Towards this end, we introduce a novel axiomatic framework
for decomposing the joint entropy, which characterizes the various ways in
which random variables can share information. The key contribution of our
framework is to distinguish between interdependencies where the information is
shared redundantly, and synergistic interdependencies where the sharing
structure exists in the whole but not between the parts. We show that our
axioms determine unique formulas for all the terms of the proposed
decomposition for a number of cases of interest. Moreover, we show how these
results can be applied to several network information theory problems,
providing a more intuitive understanding of their fundamental limits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04556</identifier>
 <datestamp>2015-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04556</id><created>2015-09-10</created><updated>2015-09-15</updated><authors><author><keyname>Liu</keyname><forenames>Liang</forenames></author></authors><title>On the evolution of word usage of classical Chinese poetry</title><categories>physics.soc-ph cs.CL</categories><license>http://creativecommons.org/publicdomain/zero/1.0/</license><abstract>  The hierarchy of classical Chinese poetry has been broadly acknowledged by a
number of studies in Chinese literature. However, quantitative investigations
about the evolution of classical Chinese poetry are limited. The primary goal
of this study is to provide quantitative evidence of the evolutionary linkages,
with emphasis on word usage, among different period genres for classical
Chinese poetry. Specifically, various statistical analyses were performed to
find and compare the patterns of word usage in the poems of nine period genres,
including shi jing, chu ci, Han shi , Jin shi, Tang shi, Song shi, Yuan shi,
Ming shi, and Qing shi. The result of analysis indicates that each of nine
period genres has unique patterns of word usage, with some Chinese characters
being preferably used by the poems of a particular period genre. The analysis
on the general pattern of word preference implies a decreasing trend in the use
of ancient Chinese characters along the timeline of dynastic types of classical
Chinese poetry. The phylogenetic analysis based on the distance matrix suggests
that the evolution of different types of classical Chinese poetry is congruent
with their chronological order, suggesting that word frequencies contain useful
phylogenetic information and thus can be used to infer evolutionary linkages
among various types of classical Chinese poetry. The statistical analyses
conducted in this study can be applied to the data sets of general Chinese
literature. Such analyses can provide quantitative insights about the evolution
of general Chinese literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04565</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04565</id><created>2015-09-15</created><authors><author><keyname>Marc</keyname><forenames>Tilen</forenames></author></authors><title>Classification of vertex-transitive cubic partial cubes</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Partial cubes are graphs isometrically embeddable into hypercubes. In this
paper it is proved that every cubic, vertex-transitive partial cube is
isomorphic to one of the following graphs: $K_2\, \square \, C_{2n}$, for some
$n\geq 2$, generalized Petersen graph $G(10,3)$, permutahedron, truncated
cuboctahedron, or truncated icosidodecahedron. This complete classification of
cubic, vertex-transitive partial cubes, a family that includes all cubic,
distance-regular partial cubes (Weichsel, 1992), is a generalization of results
of Bre\v{s}ar et. al. from 2004 on cubic mirror graphs, and a contribution to
the classification of all cubic partial cubes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04575</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04575</id><created>2015-09-15</created><authors><author><keyname>Fabila-Monroy</keyname><forenames>Ruy</forenames></author><author><keyname>Huemer</keyname><forenames>Clemens</forenames></author></authors><title>Caratheodory's Theorem in Depth</title><categories>cs.CG math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $X$ be a finite set of points in $\mathbb{R}^d$. The Tukey depth of a
point $q$ with respect to $X$ is the minimum number $\tau_X(q)$ of points of
$X$ in a halfspace containing $q$. In this paper we prove a depth version of
Caratheodory's theorem. In particular, we prove that there exists a constant
$c$ (that depends only on $d$ and $\tau_X(q)$) and pairwise disjoint sets
$X_1,\dots, X_{d+1} \subset X$ such that the following holds. Each $X_i$ has at
least $c|X|$ points, and for every choice of points $x_i$ in $X_i$, $q$ is a
convex combination of $x_1,\dots, x_{d+1}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04580</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04580</id><created>2015-09-15</created><authors><author><keyname>Chen</keyname><forenames>Badong</forenames></author><author><keyname>Liu</keyname><forenames>Xi</forenames></author><author><keyname>Zhao</keyname><forenames>Haiquan</forenames></author><author><keyname>Pr&#xed;ncipe</keyname><forenames>Jos&#xe9; C.</forenames></author></authors><title>Maximum Correntropy Kalman Filter</title><categories>stat.ML cs.SY</categories><comments>11 pages, 11 figures, 7 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Traditional Kalman filter (KF) is derived under the well-known minimum mean
square error (MMSE) criterion, which is optimal under Gaussian assumption.
However, when the signals are non-Gaussian, especially when the system is
disturbed by some heavy-tailed impulsive noises, the performance of KF will
deteriorate seriously. To improve the robustness of KF against impulsive
noises, we propose in this work a new Kalman filter, called the maximum
correntropy Kalman filter (MCKF), which adopts the robust maximum correntropy
criterion (MCC) as the optimality criterion, instead of using the MMSE. Similar
to the traditional KF, the state mean and covariance matrix propagation
equations are used to give prior estimations of the state and covariance matrix
in MCKF. A novel fixed-point algorithm is then used to update the posterior
estimations. A sufficient condition that guarantees the convergence of the
fixed-point algorithm is given. Illustration examples are presented to
demonstrate the effectiveness and robustness of the new algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04581</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04581</id><created>2015-09-15</created><authors><author><keyname>Liu</keyname><forenames>Zhen</forenames></author></authors><title>Kernelized Deep Convolutional Neural Network for Describing Complex
  Images</title><categories>cs.CV cs.AI cs.IR cs.MM</categories><comments>9 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the impressive capability to capture visual content, deep convolutional
neural networks (CNN) have demon- strated promising performance in various
vision-based ap- plications, such as classification, recognition, and objec- t
detection. However, due to the intrinsic structure design of CNN, for images
with complex content, it achieves lim- ited capability on invariance to
translation, rotation, and re-sizing changes, which is strongly emphasized in
the s- cenario of content-based image retrieval. In this paper, to address this
problem, we proposed a new kernelized deep convolutional neural network. We
first discuss our motiva- tion by an experimental study to demonstrate the
sensitivi- ty of the global CNN feature to the basic geometric trans-
formations. Then, we propose to represent visual content with approximate
invariance to the above geometric trans- formations from a kernelized
perspective. We extract CNN features on the detected object-like patches and
aggregate these patch-level CNN features to form a vectorial repre- sentation
with the Fisher vector model. The effectiveness of our proposed algorithm is
demonstrated on image search application with three benchmark datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04587</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04587</id><created>2015-09-15</created><authors><author><keyname>Nellis</keyname><forenames>Adam</forenames></author><author><keyname>Kesseli</keyname><forenames>Pascal</forenames></author><author><keyname>Conmy</keyname><forenames>Philippa Ryan</forenames></author><author><keyname>Kroening</keyname><forenames>Daniel</forenames></author><author><keyname>Schrammel</keyname><forenames>Peter</forenames></author><author><keyname>Tautschnig</keyname><forenames>Michael</forenames></author></authors><title>Assisted Coverage Closure</title><categories>cs.SE</categories><comments>submitted paper</comments><acm-class>D.2.5</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The malfunction of safety-critical systems may cause damage to people and the
environment. Software within those systems is rigorously designed and verified
according to domain specific guidance, such as ISO26262 for automotive safety.
This paper describes academic and industrial co-operation in tool development
to support one of the most stringent of the requirements --- achieving full
code coverage in requirements-driven testing. We present a verification
workflow supported by a tool that integrates the coverage measurement tool
RapiCover with the test-vector generator FShell. The tool assists closing the
coverage gap by providing the engineer with test vectors that help in debugging
coverage-related code quality issues and creating new test cases, as well as
justifying the presence of unreachable parts of the code in order to finally
achieve full effective coverage according to the required criteria. To
illustrate the practical utility of the tool, we report about an application of
the tool to a case study from automotive industry.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04595</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04595</id><created>2015-09-15</created><authors><author><keyname>Bredereck</keyname><forenames>Robert</forenames></author><author><keyname>Chen</keyname><forenames>Jiehua</forenames></author><author><keyname>Woeginger</keyname><forenames>Gerhard J.</forenames></author></authors><title>Are there any nicely structured preference~profiles~nearby?</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the problem of deciding whether a given preference profile is
close to having a certain nice structure, as for instance single-peaked,
single-caved, single-crossing, value-restricted, best-restricted,
worst-restricted, medium-restricted, or group-separable profiles. We measure
this distance by the number of voters or alternatives that have to be deleted
to make the profile a nicely structured one. Our results classify the problem
variants with respect to their computational complexity, and draw a clear line
between computationally tractable (polynomial-time solvable) and
computationally intractable (NP-hard) questions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04612</identifier>
 <datestamp>2015-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04612</id><created>2015-09-15</created><updated>2015-09-16</updated><authors><author><keyname>Mosca</keyname><forenames>Alan</forenames></author><author><keyname>Magoulas</keyname><forenames>George D.</forenames></author></authors><title>Adapting Resilient Propagation for Deep Learning</title><categories>cs.NE cs.CV cs.LG stat.ML</categories><comments>Published in the proceedings of the UK workshop on Computational
  Intelligence 2015 (UKCI)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Resilient Propagation (Rprop) algorithm has been very popular for
backpropagation training of multilayer feed-forward neural networks in various
applications. The standard Rprop however encounters difficulties in the context
of deep neural networks as typically happens with gradient-based learning
algorithms. In this paper, we propose a modification of the Rprop that combines
standard Rprop steps with a special drop out technique. We apply the method for
training Deep Neural Networks as standalone components and in ensemble
formulations. Results on the MNIST dataset show that the proposed modification
alleviates standard Rprop's problems demonstrating improved learning speed and
accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04618</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04618</id><created>2015-09-11</created><authors><author><keyname>Misra</keyname><forenames>Neeraj Kumar</forenames></author><author><keyname>Kushwaha</keyname><forenames>Mukesh Kumar</forenames></author><author><keyname>Wairya</keyname><forenames>Subodh</forenames></author><author><keyname>Kumar</keyname><forenames>Amit</forenames></author></authors><title>Cost Efficient Design of Reversible Adder Circuits for Low Power
  Applications</title><categories>cs.AR</categories><comments>9 pages, 12 figures, journal</comments><license>http://creativecommons.org/licenses/by-sa/4.0/</license><abstract>  A large amount of research is currently going on in the field of reversible
logic, which have low heat dissipation, low power consumption, which is the
main factor to apply reversible in digital VLSI circuit design. This paper
introduces reversible gate named as Inventive0 gate. The novel gate is
synthesis the efficient adder modules with minimum garbage output and gate
count. The Inventive0 gate capable of implementing a 4-bit ripple carry adder
and carry skip adders.It is presented that Inventive0 gate is much more
efficient and optimized approach as compared to their existing design, in terms
of gate count, garbage outputs and constant inputs. In addition, some popular
available reversible gates are implemented in the MOS transistor design the
implementation kept in mind for minimum MOS transistor count and are completely
reversible in behavior more precise forward and backward computation. Lesser
architectural complexity show that the novel designs are compact, fast as well
as low power.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04619</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04619</id><created>2015-09-15</created><authors><author><keyname>Camlica</keyname><forenames>Zehra</forenames></author><author><keyname>Tizhoosh</keyname><forenames>H. R.</forenames></author><author><keyname>Khalvati</keyname><forenames>Farzad</forenames></author></authors><title>Medical Image Classification via SVM using LBP Features from
  Saliency-Based Folded Data</title><categories>cs.CV</categories><comments>To appear in proceedings of The 14th International Conference on
  Machine Learning and Applications (IEEE ICMLA 2015), Miami, Florida, USA,
  2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Good results on image classification and retrieval using support vector
machines (SVM) with local binary patterns (LBPs) as features have been
extensively reported in the literature where an entire image is retrieved or
classified. In contrast, in medical imaging, not all parts of the image may be
equally significant or relevant to the image retrieval application at hand. For
instance, in lung x-ray image, the lung region may contain a tumour, hence
being highly significant whereas the surrounding area does not contain
significant information from medical diagnosis perspective. In this paper, we
propose to detect salient regions of images during training and fold the data
to reduce the effect of irrelevant regions. As a result, smaller image areas
will be used for LBP features calculation and consequently classification by
SVM. We use IRMA 2009 dataset with 14,410 x-ray images to verify the
performance of the proposed approach. The results demonstrate the benefits of
saliency-based folding approach that delivers comparable classification
accuracies with state-of-the-art but exhibits lower computational cost and
storage requirements, factors highly important for big data analytics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04623</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04623</id><created>2015-09-15</created><authors><author><keyname>Huang</keyname><forenames>Zhenqi</forenames></author><author><keyname>Wang</keyname><forenames>Yu</forenames></author><author><keyname>Mitra</keyname><forenames>Sayan</forenames></author><author><keyname>Dullerud</keyname><forenames>Geir E.</forenames></author><author><keyname>Chaudhuri</keyname><forenames>Swarat</forenames></author></authors><title>Controller Synthesis with Inductive Proofs for Piecewise Linear Systems:
  an SMT-based Algorithm</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a controller synthesis algorithm for reach-avoid problems for
piecewise linear discrete-time systems. Our algorithm relies on SMT solvers and
in this paper we focus on piecewise constant control strategies. Our algorithm
generates feedback control laws together with inductive proofs of unbounded
time safety and progress properties with respect to the reach-avoid sets. Under
a reasonable robustness assump- tion, the algorithm is shown to be complete.
That is, it either generates a controller of the above type along with a proof
of correctness, or it establishes the impossibility of the existence of such
controllers. To achieve this, the algorithm iteratively attempts to solve a
weakened and strengthened versions of the SMT encoding of the reach-avoid
problem. We present preliminary experimental results on applying this algorithm
based on a prototype implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04624</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04624</id><created>2015-09-15</created><authors><author><keyname>Li</keyname><forenames>Lingxiang</forenames></author><author><keyname>Chen</keyname><forenames>Zhi</forenames></author><author><keyname>Fang</keyname><forenames>Jun</forenames></author><author><keyname>Petropulu</keyname><forenames>and Athina P.</forenames></author></authors><title>On the Secrecy Capacity of a MIMO Gaussian Wiretap Channel with a
  Cooperative Jammer</title><categories>cs.IT math.IT</categories><comments>13 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the secrecy capacity of a helper-assisted Gaussian wiretap channel
with a source, a legitimate receiver, an eavesdropper and an external helper,
where each terminal is equipped with multiple antennas. Determining the secrecy
capacity in this scenario generally requires solving a nonconvex secrecy rate
maximization (SRM) problem. To deal with this issue, we first reformulate the
original SRM problem into a sequence of convex subproblems. For the special
case of single-antenna legitimate receiver, we obtain the secrecy capacity via
a combination of convex optimization and one-dimensional search, while for the
general case of multi-antenna legitimate receiver, we propose an iterative
solution. To gain more insight into how the secrecy capacity of a
helper-assisted Gaussian wiretap channel behaves, we examine the achievable
secure degrees of freedom (s.d.o.f.) and obtain the maximal achievable s.d.o.f.
in closed-form. We also derive a closed-form solution to the original SRM
problem which achieves the maximal s.d.o.f.. Numerical results are presented to
illustrate the efficacy of the proposed schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04627</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04627</id><created>2015-09-15</created><authors><author><keyname>Burstedde</keyname><forenames>Carsten</forenames></author><author><keyname>Holke</keyname><forenames>Johannes</forenames></author></authors><title>A tetrahedral space-filling curve for non-conforming adaptive meshes</title><categories>cs.DC cs.MS</categories><comments>34 pages, 15 figures, 8 tables</comments><msc-class>65M50, 68W10, 65Y05, 65D18</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a space-filling curve for triangular and tetrahedral
red-refinement that can be computed using bitwise interleaving operations
similar to the well-known Z-order or Morton curve for cubical meshes. To store
the information necessary for random access, we suggest 10 bytes per triangle
and 14 bytes per tetrahedron. We present algorithms that compute the parent,
children, and face-neighbors of a mesh element in constant time, as well as the
next and previous element in the space-filling curve and whether a given
element is on the boundary of the root simplex or not. Furthermore, we prove
that the maximum number of face-connected components in any segment of this
curve is bounded by twice the refinement level plus one (minus one in 2d) and
that the number of corner-connected components is bounded by two. We conclude
with a scalability demonstration that creates and adapts selected meshes on a
large distributed-memory system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04634</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04634</id><created>2015-09-15</created><authors><author><keyname>Solin</keyname><forenames>Arno</forenames></author><author><keyname>Kok</keyname><forenames>Manon</forenames></author><author><keyname>Wahlstr&#xf6;m</keyname><forenames>Niklas</forenames></author><author><keyname>Sch&#xf6;n</keyname><forenames>Thomas B.</forenames></author><author><keyname>S&#xe4;rkk&#xe4;</keyname><forenames>Simo</forenames></author></authors><title>Modeling and interpolation of the ambient magnetic field by Gaussian
  processes</title><categories>cs.RO stat.ML</categories><comments>25 pages, 12 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Anomalies in the ambient magnetic field can be used as features in indoor
positioning and navigation. By using Maxwell's equations, we derive and present
a Bayesian non-parametric probabilistic modeling approach for interpolation and
extrapolation of the magnetic field. We model the magnetic field components
jointly by imposing a Gaussian process (GP) prior on the latent scalar
potential of the magnetic field. By rewriting the GP model in terms of a
Hilbert space representation, we circumvent the computational pitfalls
associated with GP modeling and provide a computationally efficient and
physically justified modeling tool for the ambient magnetic field. The model
allows for sequential updating of the estimate and time-dependent changes in
the magnetic field. The model is shown to work well in practice in different
applications: we demonstrate mapping of the magnetic field both with an
inexpensive Raspberry Pi powered robot and on foot using a standard smartphone.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04636</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04636</id><created>2015-09-15</created><authors><author><keyname>Gupta</keyname><forenames>Dhruv</forenames></author><author><keyname>Gohil</keyname><forenames>Gunvantsinh</forenames></author><author><keyname>Raval</keyname><forenames>Mehul S.</forenames></author></authors><title>Driver Friendly Headlight Controller for Driving in Developing Countries</title><categories>cs.SY</categories><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  In developing countries, night driving is extremely unsafe mainly due to; 1).
Poorly followed traffic rules and 2) bad road conditions. The number of
accidents is increasing at a frightening pace, necessitating the development of
a low cost automatic headlight control system to ensure safety. In most
accident cases, fatal collisions take place due to glare generated by excessive
headlight intensity (high beam) of the oncoming vehicle. In this paper, a user
friendly controller for headlight intensity control for driving on highway at
night has been proposed. Aim is to design simple and affordable system that can
alleviate effect of blind spot due to high glare on the vehicle windscreen.
Controller is based on Fuzzy inference system (FIS) and used incoming light
intensity as criteria. Also, relative distance and speed is derived using
incoming intensity of the oncoming vehicles. The system is designed considering
human tolerance levels of light intensity as the boundary values. The system
controls headlight beam voltage such that; a) intensity is maintained in human
visual comfort zone and b) blind spot is avoided. The super user feature
incorporated in the intensity controller offers personalized preferences to the
driver. Due to the use of the single parameter (intensity) for control, system
can be implemented using simplified hardware.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04639</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04639</id><created>2015-09-15</created><authors><author><keyname>Deka</keyname><forenames>Deepjyoti</forenames></author><author><keyname>Baldick</keyname><forenames>Ross</forenames></author><author><keyname>Vishwanath</keyname><forenames>Sriram</forenames></author></authors><title>Jamming aided Generalized Data Attacks: Exposing Vulnerabilities in
  Secure Estimation</title><categories>cs.CR math.OC</categories><comments>11 pages, 8 figures, A version of this will appear in HICSS 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Jamming refers to the deletion, corruption or damage of meter measurements
that prevents their further usage. This is distinct from adversarial data
injection that changes meter readings while preserving their utility in state
estimation. This paper presents a generalized attack regime that uses jamming
of secure and insecure measurements to greatly expand the scope of common
'hidden' and 'detectable' data injection attacks in literature. For 'hidden'
attacks, it is shown that with jamming, the optimal attack is given by the
minimum feasible cut in a specific weighted graph. More importantly, for
'detectable' data attacks, this paper shows that the entire range of relative
costs for adversarial jamming and data injection can be divided into three
separate regions, with distinct graph-cut based constructions for the optimal
attack. Approximate algorithms for attack design are developed and their
performances are demonstrated by simulations on IEEE test cases. Further, it is
proved that prevention of such attacks require security of all grid
measurements. This work comprehensively quantifies the dual adversarial
benefits of jamming: (a) reduced attack cost and (b) increased resilience to
secure measurements, that strengthen the potency of data attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04640</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04640</id><created>2015-09-15</created><authors><author><keyname>Charlin</keyname><forenames>Laurent</forenames></author><author><keyname>Ranganath</keyname><forenames>Rajesh</forenames></author><author><keyname>McInerney</keyname><forenames>James</forenames></author><author><keyname>Blei</keyname><forenames>David M.</forenames></author></authors><title>Dynamic Poisson Factorization</title><categories>cs.LG cs.IR stat.ML</categories><comments>RecSys 2015</comments><doi>10.1145/2792838.2800174</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Models for recommender systems use latent factors to explain the preferences
and behaviors of users with respect to a set of items (e.g., movies, books,
academic papers). Typically, the latent factors are assumed to be static and,
given these factors, the observed preferences and behaviors of users are
assumed to be generated without order. These assumptions limit the explorative
and predictive capabilities of such models, since users' interests and item
popularity may evolve over time. To address this, we propose dPF, a dynamic
matrix factorization model based on the recent Poisson factorization model for
recommendations. dPF models the time evolving latent factors with a Kalman
filter and the actions with Poisson distributions. We derive a scalable
variational inference algorithm to infer the latent factors. Finally, we
demonstrate dPF on 10 years of user click data from arXiv.org, one of the
largest repository of scientific papers and a formidable source of information
about the behavior of scientists. Empirically we show performance improvement
over both static and, more recently proposed, dynamic recommendation models. We
also provide a thorough exploration of the inferred posteriors over the latent
variables.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04648</identifier>
 <datestamp>2016-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04648</id><created>2015-09-15</created><updated>2016-03-03</updated><authors><author><keyname>Zia</keyname><forenames>M. Zeeshan</forenames></author><author><keyname>Nardi</keyname><forenames>Luigi</forenames></author><author><keyname>Jack</keyname><forenames>Andrew</forenames></author><author><keyname>Vespa</keyname><forenames>Emanuele</forenames></author><author><keyname>Bodin</keyname><forenames>Bruno</forenames></author><author><keyname>Kelly</keyname><forenames>Paul H. J.</forenames></author><author><keyname>Davison</keyname><forenames>Andrew J.</forenames></author></authors><title>Comparative Design Space Exploration of Dense and Semi-Dense SLAM</title><categories>cs.RO cs.CV</categories><comments>IEEE International Conference on Robotics and Automation 2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  SLAM has matured significantly over the past few years, and is beginning to
appear in serious commercial products. While new SLAM systems are being
proposed at every conference, evaluation is often restricted to qualitative
visualizations or accuracy estimation against a ground truth. This is due to
the lack of benchmarking methodologies which can holistically and
quantitatively evaluate these systems. Further investigation at the level of
individual kernels and parameter spaces of SLAM pipelines is non-existent,
which is absolutely essential for systems research and integration. We extend
the recently introduced SLAMBench framework to allow comparing two
state-of-the-art SLAM pipelines, namely KinectFusion and LSD-SLAM, along the
metrics of accuracy, energy consumption, and processing frame rate on two
different hardware platforms, namely a desktop and an embedded device. We also
analyze the pipelines at the level of individual kernels and explore their
algorithmic and hardware design spaces for the first time, yielding valuable
insights.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04664</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04664</id><created>2015-09-15</created><authors><author><keyname>Othman</keyname><forenames>A.</forenames></author><author><keyname>Tizhoosh</keyname><forenames>H. R.</forenames></author><author><keyname>Khalvati</keyname><forenames>F.</forenames></author></authors><title>Self-Configuring and Evolving Fuzzy Image Thresholding</title><categories>cs.CV</categories><comments>To appear in proceedings of The 14th International Conference on
  Machine Learning and Applications (IEEE ICMLA 2015), Miami, Florida, USA,
  2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Every segmentation algorithm has parameters that need to be adjusted in order
to achieve good results. Evolving fuzzy systems for adjustment of segmentation
parameters have been proposed recently (Evolving fuzzy image segmentation --
EFIS [1]. However, similar to any other algorithm, EFIS too suffers from a few
limitations when used in practice. As a major drawback, EFIS depends on
detection of the object of interest for feature calculation, a task that is
highly application-dependent. In this paper, a new version of EFIS is proposed
to overcome these limitations. The new EFIS, called self-configuring EFIS
(SC-EFIS), uses available training data to auto-configure the parameters that
are fixed in EFIS. As well, the proposed SC-EFIS relies on a feature selection
process that does not require the detection of a region of interest (ROI).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04674</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04674</id><created>2015-09-15</created><authors><author><keyname>Papazafeiropoulos</keyname><forenames>Anastasios K.</forenames></author><author><keyname>Sharma</keyname><forenames>Shree Krishna</forenames></author><author><keyname>Chatzinotas</keyname><forenames>Symeon</forenames></author></authors><title>Impact of Transceiver Impairments on the Capacity of Dual-Hop Relay
  Massive MIMO Systems</title><categories>cs.IT math.IT</categories><comments>6 pages, 4 figures, accepted in IEEE Global Communications Conference
  (GLOBECOM 2015) - Workshop on Massive MIMO: From theory to practice, 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite the deleterious effect of hardware impairments on communication
systems, most prior works have not investigated their impact on widely used
relay systems. Most importantly, the application of inexpensive transceivers,
being prone to hardware impairments, is the most cost-efficient way for the
implementation of massive multiple-input multiple-output (MIMO) systems.
Consequently, the direction of this paper is towards the investigation of the
impact of hardware impairments on MIMO relay networks with large number of
antennas. Specifically, we obtain the general expression for the ergodic
capacity of dual-hop (DH) amplify-and-forward (AF) relay systems. Next, given
the advantages of the free probability (FP) theory with comparison to other
known techniques in the area of large random matrix theory, we pursue a large
limit analysis in terms of number of antennas and users by shedding light to
the behavior of relay systems inflicted by hardware impairments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04692</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04692</id><created>2015-09-15</created><authors><author><keyname>Anderson</keyname><forenames>Joshua A.</forenames></author><author><keyname>Irrgang</keyname><forenames>M. Eric</forenames></author><author><keyname>Glotzer</keyname><forenames>Sharon C.</forenames></author></authors><title>Scalable Metropolis Monte Carlo for simulation of hard shapes</title><categories>cond-mat.soft cs.MS physics.comp-ph</categories><comments>5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We design and implement HPMC, a scalable hard particle Monte Carlo simulation
toolkit, and release it open source as part of HOOMD-blue. HPMC runs in
parallel on many CPUs and many GPUs using domain decomposition. We employ BVH
trees instead of cell lists on the CPU for fast performance, especially with
large particle size disparity, and optimize inner loops with SIMD vector
intrinsics on the CPU. Our GPU kernel proposes many trial moves in parallel on
a checkerboard and uses a block-level queue to redistribute work among threads
and avoid divergence. HPMC supports a wide variety of shape classes, including
spheres / disks, unions of spheres, convex polygons, convex spheropolygons,
concave polygons, ellipsoids / ellipses, convex polyhedra, convex
spheropolyhedra, spheres cut by planes, and concave polyhedra. NVT and NPT
ensembles can be run in 2D or 3D triclinic boxes. Additional integration
schemes permit Frenkel-Ladd free energy computations and implicit depletant
simulations. In a benchmark system of a fluid of 4096 pentagons, HPMC performs
10 million sweeps in 10 minutes on 96 CPU cores on XSEDE Comet. The same
simulation would take 7.6 hours in serial. HPMC also scales to large system
sizes, and the same benchmark with 16.8 million particles runs in 1.4 hours on
2048 GPUs on OLCF Titan.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04693</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04693</id><created>2015-09-15</created><authors><author><keyname>Wang</keyname><forenames>Xiang</forenames></author><author><keyname>Haynes</keyname><forenames>Ronald D.</forenames></author><author><keyname>Feng</keyname><forenames>Qihong</forenames></author></authors><title>Well Control Optimization using Derivative-Free Algorithms and a
  Multiscale Approach</title><categories>math.OC cs.CE physics.flu-dyn</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we use numerical optimization algorithms and a multiscale
approach in order to find an optimal well management strategy over the life of
the reservoir. The large number of well rates for each control step make the
optimization problem more difficult and at a high risk of achieving a
suboptimal solution. Moreover, the optimal number of adjustments is not known a
priori. Adjusting well controls too frequently will increase unnecessary well
management and operation cost, and an excessively low number of control
adjustments may not be enough to obtain a good yield. We investigate three
derivative-free optimization algorithms, chosen for their robust and parallel
nature, to determine optimal well control strategies. The algorithms chosen
include generalized pattern search (GPS), particle swarm optimization (PSO) and
covariance matrix adaptation evolution strategy (CMA-ES). These three
algorithms encompass the breadth of available black-box optimization
strategies: deterministic local search, stochastic global search and stochastic
local search. In addition, we hybridize the three derivative-free algorithms
with a multiscale regularization approach. Starting with a reasonably small
number of control steps, the control intervals are subsequently refined during
the optimization. Results for experiments studied indicate that CMA-ES performs
best among the three algorithms in solving both small and large scale problems.
When hybridized with a multiscale regularization approach, the ability to find
the optimal solution is further enhanced, with the performance of GPS improving
the most. Topics affecting the performance of the multiscale approach are
discussed in this paper, including the effect of control frequency on the well
control problem. The parameter settings for GPS, PSO, and CMA-ES, within the
multiscale approach are considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04696</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04696</id><created>2015-09-15</created><authors><author><keyname>Ball</keyname><forenames>Taylor</forenames></author><author><keyname>Bell</keyname><forenames>Robert W.</forenames></author><author><keyname>Guzman</keyname><forenames>Jonathan</forenames></author><author><keyname>Hanson-Colvin</keyname><forenames>Madeleine</forenames></author><author><keyname>Schonscheck</keyname><forenames>Nikolas</forenames></author></authors><title>On the cop number of generalized Petersen graphs</title><categories>math.CO cs.DM</categories><comments>12 pages, 5 figures</comments><msc-class>05C57</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the cop number of every generalized Petersen graph is at most 4.
The strategy is to play a modified game of cops and robbers on an infinite
cyclic covering space where the objective is to capture the robber or force the
robber towards an end of the infinite graph. We prove that finite isometric
subtrees are 1-guardable and apply this to determine the exact cop number of
some families of generalized Petersen graphs. We also extend these ideas to
prove that the cop number of any connected I-graph is at most 5.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04698</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04698</id><created>2015-09-15</created><authors><author><keyname>Arafa</keyname><forenames>Ahmed</forenames></author><author><keyname>Ulukus</keyname><forenames>Sennur</forenames></author></authors><title>Optimal Policies for Wireless Networks with Energy Harvesting
  Transmitters and Receivers: Effects of Decoding Costs</title><categories>cs.IT cs.NI math.IT</categories><comments>To appear in IEEE Journal on Selected Areas - Series on Green
  Communications and Networking</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the effects of decoding costs in energy harvesting communication
systems. In our setting, receivers, in addition to transmitters, rely solely on
energy harvested from nature, and need to spend some energy in order to decode
their intended packets. We model the decoding energy as an increasing convex
function of the rate of the incoming data. In this setting, in addition to the
traditional energy causality constraints at the transmitters, we have the
decoding causality constraints at the receivers, where energy spent by the
receiver for decoding cannot exceed its harvested energy. We first consider the
point-to-point single-user problem where the goal is to maximize the total
throughput by a given deadline subject to both energy and decoding causality
constraints. We show that decoding costs at the receiver can be represented as
generalized data arrivals at the transmitter, and thereby moving all system
constraints to the transmitter side. Then, we consider several multi-user
settings. We start with a two-hop network where the relay and the destination
have decoding costs, and show that separable policies, where the transmitter's
throughput is maximized irrespective of the relay's transmission energy
profile, are optimal. Next, we consider the multiple access channel (MAC) and
the broadcast channel (BC) where the transmitters and the receivers harvest
energy from nature, and characterize the maximum departure region. In all
multi-user settings considered, we decompose our problems into inner and outer
problems. We solve the inner problems by exploiting the structure of the
particular model, and solve the outer problems by water-filling algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04699</identifier>
 <datestamp>2015-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04699</id><created>2015-09-15</created><authors><author><keyname>Jouannaud</keyname><forenames>Jean-Pierre</forenames><affiliation>UP11, LIX</affiliation></author><author><keyname>Liu</keyname><forenames>Jiaxiang</forenames><affiliation>LIX</affiliation></author><author><keyname>Ogawa</keyname><forenames>Mizuhito</forenames><affiliation>JAIST</affiliation></author></authors><title>Confluence of Layered Rewrite Systems</title><categories>cs.LO</categories><proxy>ccsd</proxy><journal-ref>Stephan Kreutzer. Proceedings, 24th annual EATCS Computer Science
  Logic, Sep 2015, Berlin, Germany. LIPICS, vol 41, 2015, Proceedings, 24th
  annual EATCS Computer Science Logic.
  \&amp;lt;http://drops.dagstuhl.de/portals/extern/index.php?semnr=15014\&amp;gt;</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the new, Turing-complete class of layered systems, whose
lefthand sides of rules can only be overlapped at a multiset of disjoint or
equal positions. Layered systems define a natural notion of rank for terms: the
maximal number of non-overlapping redexes along a path from the root to a leaf.
Overlappings are allowed in finite or infinite trees. Rules may be
non-terminating, non-left-linear, or non-right-linear. Using a novel
unification technique, cyclic unification, and the so-alled subrewriting
relation, we show that rank non-increasing layered systems are confluent
provided their cyclic critical pairs have cyclic-joinable decreasing diagrams.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04705</identifier>
 <datestamp>2015-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04705</id><created>2015-09-15</created><authors><author><keyname>Astakhova</keyname><forenames>N. N.</forenames></author><author><keyname>Demidova</keyname><forenames>L. A.</forenames></author><author><keyname>Nikulchev</keyname><forenames>E. V.</forenames></author></authors><title>Forecasting Method for Grouped Time Series with the Use of k-Means
  Algorithm</title><categories>cs.LG</categories><comments>18 pages</comments><journal-ref>Applied Mathematical Sciences, 2015, 9(97):4813-4830</journal-ref><doi>10.12988/ams.2015.55391</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The paper is focused on the forecasting method for time series groups with
the use of algorithms for cluster analysis. $K$-means algorithm is suggested to
be a basic one for clustering. The coordinates of the centers of clusters have
been put in correspondence with summarizing time series data the centroids of
the clusters. A description of time series, the centroids of the clusters, is
implemented with the use of forecasting models. They are based on strict binary
trees and a modified clonal selection algorithm. With the help of such
forecasting models, the possibility of forming analytic dependences is shown.
It is suggested to use a common forecasting model, which is constructed for
time series the centroid of the cluster, in forecasting the private
(individual) time series in the cluster. The promising application of the
suggested method for grouped time series forecasting is demonstrated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04706</identifier>
 <datestamp>2015-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04706</id><created>2015-09-15</created><authors><author><keyname>Kazantsev</keyname><forenames>Daniil</forenames></author><author><keyname>Ovtchinnikov</keyname><forenames>Evgueni</forenames></author><author><keyname>Lionheart</keyname><forenames>William R. B.</forenames></author><author><keyname>Withers</keyname><forenames>Philip J.</forenames></author><author><keyname>Lee</keyname><forenames>Peter D.</forenames></author></authors><title>Direct high-order edge-preserving regularization for tomographic image
  reconstruction</title><categories>cs.CV cs.MS cs.NA math.NA</categories><comments>16 pages, 11 figures</comments><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  In this paper we present a new two-level iterative algorithm for tomographic
image reconstruction. The algorithm uses a regularization technique, which we
call edge-preserving Laplacian, that preserves sharp edges between objects
while damping spurious oscillations in the areas where the reconstructed image
is smooth. Our numerical simulations demonstrate that the proposed method
outperforms total variation (TV) regularization and it is competitive with the
combined TV-L2 penalty. Obtained reconstructed images show increased
signal-to-noise ratio and visually appealing structural features. Computer
implementation and parameter control of the proposed technique is
straightforward, which increases the feasibility of it across many tomographic
applications. In this paper, we applied our method to the under-sampled
computed tomography (CT) projection data and also considered a case of
reconstruction in emission tomography The MATLAB code is provided to support
obtained results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04729</identifier>
 <datestamp>2015-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04729</id><created>2015-09-15</created><authors><author><keyname>Jacobs</keyname><forenames>Christian T.</forenames></author><author><keyname>Avdis</keyname><forenames>Alexandros</forenames></author><author><keyname>Mouradian</keyname><forenames>Simon L.</forenames></author><author><keyname>Piggott</keyname><forenames>Matthew D.</forenames></author></authors><title>Integrating Research Data Management into Geographical Information
  Systems</title><categories>cs.DL cs.CE</categories><comments>Accepted, camera-ready version. To appear in the Proceedings of the
  5th International Workshop on Semantic Digital Archives
  (http://sda2015.dke-research.de/), held in Pozna\'n, Poland on 18 September
  2015 as part of the 19th International Conference on Theory and Practice of
  Digital Libraries (http://tpdl2015.info/)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ocean modelling requires the production of high-fidelity computational meshes
upon which to solve the equations of motion. The production of such meshes by
hand is often infeasible, considering the complexity of the bathymetry and
coastlines. The use of Geographical Information Systems (GIS) is therefore a
key component to discretising the region of interest and producing a mesh
appropriate to resolve the dynamics. However, all data associated with the
production of a mesh must be provided in order to contribute to the overall
recomputability of the subsequent simulation. This work presents the
integration of research data management in QMesh, a tool for generating meshes
using GIS. The tool uses the PyRDM library to provide a quick and easy way for
scientists to publish meshes, and all data required to regenerate them, to
persistent online repositories. These repositories are assigned unique
identifiers to enable proper citation of the meshes in journal articles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04733</identifier>
 <datestamp>2015-10-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04733</id><created>2015-09-15</created><updated>2015-10-25</updated><authors><author><keyname>Artikov</keyname><forenames>Akmal</forenames></author><author><keyname>Dorodnykh</keyname><forenames>Aleksandr</forenames></author><author><keyname>Kashinskaya</keyname><forenames>Yana</forenames></author><author><keyname>Samosvat</keyname><forenames>Egor</forenames></author></authors><title>Factorization threshold models for scale-free networks generation</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many real networks such as the World Wide Web, financial, biological,
citation and social networks have a power-law degree distribution. Networks
with this feature are also called scale-free. Several models for producing
scale-free networks have been obtained by now and most of them are based on the
preferential attachment approach. We will offer the model with another
scale-free property explanation. The main idea is to approximate the network's
adjacency matrix by multiplication of the matrices $V$ and $V^T$, where $V$ is
the matrix of vertices' latent features. This approach is called matrix
factorization and is successfully used in the link prediction problem. To
create a generative model of scale-free networks we will sample latent features
$V$ from some probabilistic distribution and try to generate a network's
adjacency matrix. Entries in the generated matrix are dot products of latent
features which are real numbers. In order to create an adjacency matrix, we
approximate entries with the Boolean domain $\{0, 1\}$. We have incorporated
the threshold parameter $\theta$ into the model for discretization of a dot
product. Actually, we have been influenced by the geographical threshold models
which were recently proven to have good results in a scale-free networks
generation. The overview of our results is the following. First, we will
describe our model formally. Second, we will tune the threshold $\theta$ in
order to generate sparse growing networks. Finally, we will show that our model
produces scale-free networks with the fixed power-law exponent which equals
two. In order to generate oriented networks with tunable power-law exponents
and to obtain other model properties, we will offer different modifications of
our model. Some of our results will be demonstrated using computer simulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04734</identifier>
 <datestamp>2015-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04734</id><created>2015-09-15</created><authors><author><keyname>Sol&#xe9;-Ribalta</keyname><forenames>Albert</forenames></author><author><keyname>Granell</keyname><forenames>Clara</forenames></author><author><keyname>G&#xf3;mez</keyname><forenames>Sergio</forenames></author><author><keyname>Arenas</keyname><forenames>Alex</forenames></author></authors><title>Information transfer in community structured multiplex networks</title><categories>physics.soc-ph cs.SI</categories><comments>13 pages, 6 figures</comments><journal-ref>Frontiers in Physics 3 (2015) 61</journal-ref><doi>10.3389/fphy.2015.00061</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The study of complex networks that account for different types of
interactions has become a subject of interest in the last few years, specially
because its representational power in the description of users interactions in
diverse online social platforms (Facebook, Twitter, Instagram, etc.). The
mathematical description of these interacting networks has been coined under
the name of multilayer networks, where each layer accounts for a type of
interaction. It has been shown that diffusive processes on top of these
networks present a phenomenology that cannot be explained by the naive
superposition of single layer diffusive phenomena but require the whole
structure of interconnected layers. Nevertheless, the description of diffusive
phenomena on multilayer networks has obviated the fact that social networks
have strong mesoscopic structure represented by different communities of
individuals driven by common interests, or any other social aspect. In this
work, we study the transfer of information in multilayer networks with
community structure. The final goal is to understand and quantify, if the
existence of well-defined community structure at the level of individual
layers, together with the multilayer structure of the whole network, enhances
or deteriorates the diffusion of packets of information.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04738</identifier>
 <datestamp>2015-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04738</id><created>2015-09-14</created><authors><author><keyname>Boyer</keyname><forenames>H.</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Guichard</keyname><forenames>S.</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Jean</keyname><forenames>A.</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Libelle</keyname><forenames>T.</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Bigot</keyname><forenames>Dimitri</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Miranville</keyname><forenames>F.</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Boji&#x107;</keyname><forenames>M.</forenames></author></authors><title>Validation of daylighting model in CODYRUN building simulation code</title><categories>cs.OH</categories><comments>ICRET 2014 : 2014 International Conference on Renewable Energy
  Technologies, Nov 2014, Hong Kong, China. 2014</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  CODYRUN is a multi-zone software integrating thermal building simulation,
airflow, and pollutant transfer. A first question thus arose as to the
integration of indoor lighting conditions into the simulation, leading to a new
model calculating natural and artificial lighting. The results of this new
daylighting module were then compared with results of other simulation codes
and experimental cases both in artificial and natural environments. Excellent
agreements were obtained, such as the values for luminous efficiencies in a
tropical and humid climate. In this paper, a comparison of the model output
with detailed measures is presented using a dedicated test cell in Reunion
Island (French overseas territory in the Indian Ocean), thus confirming the
interest for thermal and daylighting designs in low-energy buildings.
Introduction Several software packages are available for thermal and airflow
simulation in buildings. The most frequently used are ENERGY+ [1], ESP-r [2],
and TRNSYS [3]. These applications allow an increasing number of models to be
integrated, such as airflow, pollutant transport, and daylighting. In the
latter category, we may note ENERGY+, ESP-r and ECOTECT [4] software. After
more than 20 years of developing a specific code named CODYRUN, we decided to
add a lighting module to our software. This paper therefore provides some
details on this evolution and elements of validation. The CODYRUN initial
software and its validation Developed by the Physics and Mathematical
Engineering Laboratory for Energy and Environment at the University of Reunion
Island, CODYRUN [5-14] is a multi-zone software program integrating ventilation
and moisture transport transfer in buildings. The software employs a zone
approach based on nodal analysis and resolves a coupled system describing
thermal and airflow phenomena. Numerous validation tests of the CODYRUN code
were successfully applied to the software. Apart from the daylighting model,
the majority applied the BESTEST procedure [15]. The International Energy
Agency (IEA) sponsors a number of programs to improve the use and associated
technologies of energy. The National Renewable Energy Laboratory (NREL)
developed BESTEST, which is a method based on comparative testing of building
simulation programs, on the IEA's behalf. The procedure consists of a series of
test cases buildings that are designed to isolate individual aspects of
building energy and test the extremes of a program. As the modelling approach
is very different between codes, the test cases are specified so that input
equivalency can be defined thus allowing the different cases to be modelled by
most of codes. The basis for comparison is a range of results from a number of
programs considered to be a state-of-art in United States and Europe.
Associated with other specific comparisons, a very confident level of
validation was obtained for the CODYRUN initial software [8].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04740</identifier>
 <datestamp>2015-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04740</id><created>2015-09-15</created><authors><author><keyname>Peixoto</keyname><forenames>Tiago P.</forenames></author><author><keyname>Rosvall</keyname><forenames>Martin</forenames></author></authors><title>Modeling sequences and temporal networks with dynamic community
  structures</title><categories>cs.SI cond-mat.stat-mech physics.soc-ph stat.ML</categories><comments>5 Pages, 2 figures, 2 tables + Supplemental material</comments><license>http://creativecommons.org/licenses/by-nc-sa/4.0/</license><abstract>  Community-detection methods that describe large-scale patterns in the
dynamics on and of networks suffer from effects of limited memory and arbitrary
time binning. We develop a variable-order Markov chain model that generalizes
the stochastic block model for discrete time-series as well as temporal
networks. The temporal model does not use time binning but takes full advantage
of the time-ordering of the tokens or edges. When the edge ordering is random,
we recover the traditional static block model as a special case. Based on
statistical evidence and without overfitting, we show how a Bayesian
formulation of the model allows us to select the most appropriate Markov order
and number of communities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04745</identifier>
 <datestamp>2015-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04745</id><created>2015-09-15</created><authors><author><keyname>Yan</keyname><forenames>Muxi</forenames></author><author><keyname>Casey</keyname><forenames>Jasson</forenames></author><author><keyname>Shome</keyname><forenames>Prithviraj</forenames></author><author><keyname>Sprintson</keyname><forenames>Alex</forenames></author><author><keyname>Sutton</keyname><forenames>Andrew</forenames></author></authors><title>{\AE}therFlow: Principled Wireless Support in SDN</title><categories>cs.NI</categories><comments>This paper is to appear in the CoolSDN Workshop of ICNP 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software Defined Networking (SDN) drastically changes the meaning and process
of designing, building, testing, and operating networks. The current support
for wireless net- working in SDN technologies has lagged behind its development
and deployment for wired networks. The purpose of this work is to bring
principled support for wireless access networks so that they can receive the
same level of programmability as wireline interfaces. Specifically we aim to
integrate wireless protocols into the general SDN framework by proposing a new
set of abstractions in wireless devices and the interfaces to manipulate them.
We validate our approach by implementing our design as an extension of an
existing OpenFlow data plane and deploying it in an IEEE 802.11 access point.
We demonstrate the viability of software-defined wireless access networks by
developing and testing a wireless handoff application. The results of the
exper- iment show that our framework is capable of providing new capabilities
in an efficient manner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04747</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04747</id><created>2015-09-15</created><updated>2016-02-09</updated><authors><author><keyname>Afshang</keyname><forenames>Mehrnaz</forenames></author><author><keyname>Dhillon</keyname><forenames>Harpreet S.</forenames></author><author><keyname>Chong</keyname><forenames>Peter Han Joo</forenames></author></authors><title>Fundamentals of Cluster-Centric Content Placement in Cache-Enabled
  Device-to-Device Networks</title><categories>cs.IT math.IT</categories><comments>16 pages, 10 figures. Submitted to IEEE Transactions on
  Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper develops a comprehensive analytical framework with foundations in
stochastic geometry to characterize the performance of cluster-centric content
placement in a cache-enabled device-to-device (D2D) network. Different from
device-centric content placement, cluster-centric placement focuses on placing
content in each cluster such that the collective performance of all the devices
in each cluster is optimized. Modeling the locations of the devices by a
Poisson cluster process, we define and analyze the performance for three
general cases: (i)$k$-Tx case: receiver of interest is chosen uniformly at
random in a cluster and its content of interest is available at the $k^{th}$
closest device to the cluster center, (ii) $\ell$-Rx case: receiver of interest
is the $\ell^{th}$ closest device to the cluster center and its content of
interest is available at a device chosen uniformly at random from the same
cluster, and (iii) baseline case: the receiver of interest is chosen uniformly
at random in a cluster and its content of interest is available at a device
chosen independently and uniformly at random from the same cluster. Easy-to-use
expressions for the key performance metrics, such as coverage probability and
area spectral efficiency (ASE) of the whole network, are derived for all three
cases. Our analysis concretely demonstrates significant improvement in the
network performance when the device on which content is cached or device
requesting content from cache is biased to lie closer to the cluster center
compared to baseline case. Based on this insight, we develop and analyze a new
generative model for cluster-centric D2D networks that allows to study the
effect of intra-cluster interfering devices that are more likely to lie closer
to the cluster center.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04751</identifier>
 <datestamp>2015-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04751</id><created>2015-09-15</created><authors><author><keyname>Dubnov</keyname><forenames>Tammuz</forenames></author><author><keyname>Wang</keyname><forenames>Cheng-i</forenames></author></authors><title>Free-body Gesture Tracking and Augmented Reality Improvisation for Floor
  and Aerial Dance</title><categories>cs.MM cs.CV cs.HC</categories><comments>8 pages. Technical paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes an updated interactive performance system for floor and
Aerial Dance that controls visual and sonic aspects of the presentation via a
depth sensing camera (MS Kinect). In order to detect, measure and track free
movement in space, 3 degree of freedom (3-DOF) tracking in space (on the ground
and in the air) is performed using IR markers with a method for multi target
tracking capabilities added and described in detail. An improved gesture
tracking and recognition system, called Action Graph (AG), is described in the
paper. Action Graph uses an efficient incremental construction from a single
long sequence of movement features and automatically captures repeated
sub-segments in the movement from start to finish with no manual interaction
needed with other advanced capabilities discussed as well. By using the new
model for the gesture we can unify an entire choreography piece by dynamically
tracking and recognizing gestures and sub-portions of the piece. This gives the
performer the freedom to improvise based on a set of recorded gestures/portions
of the choreography and have the system dynamically respond in relation to the
performer within a set of related rehearsed actions, an ability that has not
been seen in any other system to date.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04764</identifier>
 <datestamp>2015-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04764</id><created>2015-09-15</created><authors><author><keyname>Guruswami</keyname><forenames>Venkatesan</forenames></author><author><keyname>Wootters</keyname><forenames>Mary</forenames></author></authors><title>Repairing Reed-Solomon Codes</title><categories>cs.IT cs.CC math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A fundamental fact about polynomial interpolation is that $k$ evaluations of
a degree-$(k-1)$ polynomial $f$ are sufficient to determine $f$. This is also
necessary in a strong sense: given $k-1$ evaluations, we learn nothing about
the value of $f$ on any $k$'th point. In this paper, we study a variant of the
polynomial interpolation problem. Instead of querying entire evaluations of
$f$, we may query &quot;partial&quot; evaluations; that is, we may ask for a few bits
instead of an entire field element. We show that in this model, one can do
significantly better than in the traditional setting, in terms of the amount of
information required to determine the missing evaluation. More precisely, we
show that only $O(k)$ bits are necessary to recover a missing evaluation. In
contrast, the traditional method of looking at $k$ evaluations requires
$\Omega(k\log(k))$ bits. We also show that our result is optimal for linear
methods, even up to the leading constants.
  Our motivation comes from the use of Reed-Solomon (RS) codes for distributed
storage systems, in particular for the exact repair problem. Codes which solve
this problem well are known, but they are not RS codes, and RS codes are still
often used in practice; it was a question of (Dimakis et al., 2011) how well RS
codes could perform in this setting. Our results imply non-trivial exact repair
schemes for RS codes. In some parameter regimes---those with small levels of
sub-packetization---our scheme for RS codes outperforms all known regenerating
codes. Even with a high degree of sub-packetization, our methods give
non-trivial schemes, and we give an improved repair scheme for a specific
(14,10)-RS code used in the Facebook Hadoop Analytics cluster.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04767</identifier>
 <datestamp>2015-09-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04767</id><created>2015-09-15</created><updated>2015-09-25</updated><authors><author><keyname>Zhang</keyname><forenames>Ziming</forenames></author><author><keyname>Saligrama</keyname><forenames>Venkatesh</forenames></author></authors><title>Zero-Shot Learning via Semantic Similarity Embedding</title><categories>cs.CV stat.ML</categories><comments>accepted for ICCV 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider a version of the zero-shot learning problem where
seen class source and target domain data are provided. The goal during
test-time is to accurately predict the class label of an unseen target domain
instance based on revealed source domain side information (\eg attributes) for
unseen classes. Our method is based on viewing each source or target data as a
mixture of seen class proportions and we postulate that the mixture patterns
have to be similar if the two instances belong to the same unseen class. This
perspective leads us to learning source/target embedding functions that map an
arbitrary source/target domain data into a same semantic space where similarity
can be readily measured. We develop a max-margin framework to learn these
similarity functions and jointly optimize parameters by means of cross
validation. Our test results are compelling, leading to significant improvement
in terms of accuracy on most benchmark datasets for zero-shot recognition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04771</identifier>
 <datestamp>2015-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04771</id><created>2015-09-15</created><authors><author><keyname>Chung</keyname><forenames>Moo K.</forenames></author><author><keyname>Vilalta</keyname><forenames>Victoria G.</forenames></author><author><keyname>Rathouz</keyname><forenames>Paul J.</forenames></author><author><keyname>Lahey</keyname><forenames>Benjamin B.</forenames></author><author><keyname>Zald</keyname><forenames>David H.</forenames></author></authors><title>Linear Embedding of Large-Scale Brain Networks for Twin fMRI</title><categories>cs.AI q-bio.NC stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many human brain network studies, we do not have sufficient number (n) of
images relative to the number (p) of voxels due to the prohibitively expensive
cost of scanning enough subjects. Thus, brain network models usually suffer the
small-n large-p problem. Such a problem is often remedied by sparse network
models, which are usually solved numerically by optimizing L1-penalties.
Unfortunately, due to the computational bottleneck associated with optimizing
L1-penalties, it is not practical to apply such methods to learn large-scale
brain networks. In this paper, we introduce a new sparse network model based on
cross-correlations that bypass the computational bottleneck. Our model can
build the sparse brain networks at voxel level with p &gt; 25000. Instead of using
a single sparse parameter that may not be optimal in other studies and
datasets, we propose to analyze the collection of networks at every possible
sparse parameter in a coherent mathematical framework using graph filtrations.
The method is subsequently applied in determining the extend of genetic effects
on functional brain networks at voxel-level for the first time using twin fMRI.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04783</identifier>
 <datestamp>2015-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04783</id><created>2015-09-15</created><authors><author><keyname>Zhang</keyname><forenames>Ziming</forenames></author><author><keyname>Chen</keyname><forenames>Yuting</forenames></author><author><keyname>Saligrama</keyname><forenames>Venkatesh</forenames></author></authors><title>Group Membership Prediction</title><categories>cs.CV stat.ML</categories><comments>accepted for ICCV 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The group membership prediction (GMP) problem involves predicting whether or
not a collection of instances share a certain semantic property. For instance,
in kinship verification given a collection of images, the goal is to predict
whether or not they share a {\it familial} relationship. In this context we
propose a novel probability model and introduce latent {\em view-specific} and
{\em view-shared} random variables to jointly account for the view-specific
appearance and cross-view similarities among data instances. Our model posits
that data from each view is independent conditioned on the shared variables.
This postulate leads to a parametric probability model that decomposes group
membership likelihood into a tensor product of data-independent parameters and
data-dependent factors. We propose learning the data-independent parameters in
a discriminative way with bilinear classifiers, and test our prediction
algorithm on challenging visual recognition tasks such as multi-camera person
re-identification and kinship verification. On most benchmark datasets, our
method can significantly outperform the current state-of-the-art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04784</identifier>
 <datestamp>2015-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04784</id><created>2015-09-15</created><authors><author><keyname>Xu</keyname><forenames>Liang</forenames></author><author><keyname>Xie</keyname><forenames>Lihua</forenames></author><author><keyname>Xiao</keyname><forenames>Nan</forenames></author></authors><title>Mean Square Capacity of Power Constrained Fading Channels with Causal
  Encoders and Decoders</title><categories>math.OC cs.SY</categories><comments>Accepted by the 54th IEEE Conference on Decision and Control</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is concerned with the mean square stabilization problem of
discrete-time LTI systems over a power constrained fading channel. Different
from existing research works, the channel considered in this paper suffers from
both fading and additive noises. We allow any form of causal channel
encoders/decoders, unlike linear encoders/decoders commonly studied in the
literature. Sufficient conditions and necessary conditions for the mean square
stabilizability are given in terms of channel parameters such as transmission
power and fading and additive noise statistics in relation to the unstable
eigenvalues of the open-loop system matrix. The corresponding mean square
capacity of the power constrained fading channel under causal encoders/decoders
is given. It is proved that this mean square capacity is smaller than the
corresponding Shannon channel capacity. In the end, numerical examples are
presented, which demonstrate that the causal encoders/decoders render less
restrictive stabilizability conditions than those under linear
encoders/decoders studied in the existing works.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04788</identifier>
 <datestamp>2015-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04788</id><created>2015-09-15</created><authors><author><keyname>Yao</keyname><forenames>Bing</forenames></author><author><keyname>Wang</keyname><forenames>Xiaomin</forenames></author><author><keyname>Liu</keyname><forenames>Xia</forenames></author><author><keyname>Xu</keyname><forenames>Jin</forenames></author></authors><title>Growing Network Models Having Part Edges Removed/added Randomly</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Since network motifs are an important property of networks and some networks
have the behaviors of rewiring or reducing or adding edges between old vertices
before new vertices entering the networks, we construct our non-randomized
model N(t) and randomized model N'(t) that have the predicated ?xed subgraphs
like motifs and satisfy both properties of growth and preferential attachment
by means of the recursive algorithm from the lower levels of the so-called
bound growing network models. To show the scale-free property of the randomized
model N'(t), we design a new method, called edge-cumulative distribution, and
democrat two edge-cumulative distributions of N(t) and N'(t) are equivalent to
each other.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04805</identifier>
 <datestamp>2016-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04805</id><created>2015-09-15</created><updated>2016-01-12</updated><authors><author><keyname>Zhuang</keyname><forenames>Binnan</forenames></author><author><keyname>Guo</keyname><forenames>Dongning</forenames></author><author><keyname>Honig</keyname><forenames>Michael L.</forenames></author></authors><title>Energy-Efficient Cell Activation, User Association, and Spectrum
  Allocation in Heterogeneous Networks</title><categories>cs.IT math.IT</categories><comments>The paper will appear in JSAC Special Issue on Energy-Efficient
  Techniques for 5G Wireless Communication Systems</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Next generation (5G) cellular networks are expected to be supported by an
extensive infrastructure with many-fold increase in the number of cells per
unit area compared to today. The total energy consumption of base transceiver
stations (BTSs) is an important issue for both economic and environmental
reasons. In this paper, an optimization-based framework is proposed for
energy-efficient global radio resource management in heterogeneous wireless
networks. Specifically, with stochastic arrivals of known rates intended for
users, the smallest set of BTSs is activated with jointly optimized user
association and spectrum allocation to stabilize the network first and then
minimize the delay. The scheme can be carried out periodically on a relatively
slow timescale to adapt to aggregate traffic variations and average channel
conditions. Numerical results show that the proposed scheme significantly
reduces the energy consumption and increases the quality of service compared to
existing schemes in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04806</identifier>
 <datestamp>2015-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04806</id><created>2015-09-15</created><authors><author><keyname>Su&#xe1;rez-Ruiz</keyname><forenames>Francisco</forenames></author><author><keyname>Pham</keyname><forenames>Quang-Cuong</forenames></author></authors><title>A Framework for Fine Robotic Assembly</title><categories>cs.RO</categories><comments>8 pages, 7 figures, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Fine robotic assembly, in which the parts to be assembled are small and
fragile and lie in an unstructured environment, is still out of reach of
today's industrial robots. The main difficulties arise in the precise
localization of the parts in an unstructured environment and the control of
contact interactions. Our contribution in this paper is twofold. First, we
propose a taxonomy of the manipulation primitives that are specifically
involved in fine assembly. Such a taxonomy is crucial for designing a scalable
robotic system (both hardware and software) given the complexity of real-world
assembly tasks. Second, we present a hardware and software architecture where
we have addressed, in an integrated way, a number of issues arising in fine
assembly, such as workspace optimization, external wrench compensation,
position-based force control, etc. Finally, we show the above taxonomy and
architecture in action on a highly dexterous task -- bimanual pin insertion --
which is one of the key steps in our long term project, the autonomous assembly
of an IKEA chair.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04811</identifier>
 <datestamp>2015-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04811</id><created>2015-09-16</created><authors><author><keyname>Tedla</keyname><forenames>Tadele</forenames></author></authors><title>amLite: Amharic Transliteration Using Key Map Dictionary</title><categories>cs.CL cs.IR</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  amLite is a framework developed to map ASCII transliterated Amharic texts
back to the original Amharic letter texts. The aim of such a framework is to
make existing Amharic linguistic data consistent and interoperable among
researchers. For achieving the objective, a key map dictionary is constructed
using the possible ASCII combinations actively in use for transliterating
Amharic letters; and a mapping of the combinations to the corresponding Amharic
letters is done. The mapping is then used to replace the Amharic linguistic
text back to form the original Amharic letters text. The framework indicated
97.7, 99.7 and 98.4 percentage accuracy on converting the three sample random
test data. It is; however, possible to improve the accuracy of the framework by
adding an exception to the implementation of the algorithm, or by preprocessing
the input text prior to conversion. This paper outlined the rationales behind
the need for developing the framework and the processes undertaken in the
development.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04823</identifier>
 <datestamp>2015-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04823</id><created>2015-09-16</created><authors><author><keyname>Xu</keyname><forenames>Shasha</forenames></author><author><keyname>Lyu</keyname><forenames>Weijie</forenames></author><author><keyname>Li</keyname><forenames>Huilin</forenames></author></authors><title>Optimizing coverage of 3D Wireless Multimedia Sensor Networks by means
  of deploying redundant sensors</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Coverage is one of the fundamental issues in wireless multimedia sensor
networks (WMSNs). It reflects the ability of WMSNs to detect the fields.
Motivated by the existing-enhancing algorithm of traditional 2D WMSNs, a new 3D
WMSNs sensing model is established and a new coverage-enhancing algorithm based
on this model is proposed. This algorithm defines the sensing model as
trapezoidal pyramid, calculates the key parameters (tilt angle) then improves
coverage ratio by optimizing it. However, there still exists redundant sensors
in this optimized networks. Aiming at efficiently utilizing these redundant
sensors and enhancing coverage ratio, the authors selects the redundant sensors
by introducing the set cover model algorithm, further deploys them to the
uncovered area following the greedy policy, so that the whole path coverage
performance of WMSNs is enhanced.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04826</identifier>
 <datestamp>2015-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04826</id><created>2015-09-16</created><authors><author><keyname>Diaz-Mercado</keyname><forenames>Yancy</forenames></author><author><keyname>Egerstedt</keyname><forenames>Magnus</forenames></author></authors><title>Inter-Robot Interactions in Multi-Robot Systems Using Braids</title><categories>cs.MA cs.RO cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a framework for multi-robot coordination and motion
planning with emphasis on inter-agent interactions. We focus on the
characterization of inter-agent interactions with sufficient level of
abstraction so as to allow for the enforcement of desired interaction patterns
in a provably safe (i.e., collision-free) manner, e.g., for achieving rich
movement patterns in a shared space, or to exchange sensor information. We
propose to specify interaction patterns through elements of the so-called braid
group. This allows us to not focus on a particular pattern per se, but rather
on the problem of being able to execute a whole class of patterns. The result
from such a construction is a hybrid system driven by symbolic inputs that must
be mapped onto actual paths that both realize the desired interaction levels
and remain safe in the sense that collisions are avoided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04845</identifier>
 <datestamp>2015-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04845</id><created>2015-09-16</created><authors><author><keyname>Modenini</keyname><forenames>Andrea</forenames></author><author><keyname>Ugolini</keyname><forenames>Alessandro</forenames></author><author><keyname>Piemontese</keyname><forenames>Amina</forenames></author><author><keyname>Colavolpe</keyname><forenames>Giulio</forenames></author></authors><title>On the Use of Multiple Satellites to Improve the Spectral Efficiency of
  Broadcast Transmissions</title><categories>cs.IT math.IT</categories><comments>13 pages, 20 figures, to appear on IEEE Transactions on Broadcasting</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the use of multiple co-located satellites to improve the spectral
efficiency of broadcast transmissions. In particular, we assume that two
satellites transmit on overlapping geographical coverage areas, with
overlapping frequencies. We first describe the theoretical framework based on
network information theory and, in particular, on the theory for multiple
access channels. The application to different scenarios will be then
considered, including the bandlimited additive white Gaussian noise channel
with average power constraint and different models for the nonlinear satellite
channel. The comparison with the adoption of frequency division multiplexing
and with the Alamouti space-time block coding is also provided. The main
conclusion is that a strategy based on overlapped signals is the most
convenient in the case of no power unbalance, although it requires the adoption
of a multiuser detection strategy at the receiver.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04846</identifier>
 <datestamp>2015-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04846</id><created>2015-09-16</created><authors><author><keyname>Grassl</keyname><forenames>Markus</forenames></author><author><keyname>Harada</keyname><forenames>Masaaki</forenames></author></authors><title>New self-dual additive $\mathbb{F}_4$-codes constructed from circulant
  graphs</title><categories>math.CO cs.IT math.IT quant-ph</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to construct quantum $[[n,0,d]]$ codes for $(n,d)=(56,15)$,
$(57,15)$, $(58,16)$, $(63,16)$, $(67,17)$, $(70,18)$, $(71,18)$, $(79,19)$,
$(83,20)$, $(87,20)$, $(89,21)$, $(95,20)$, we construct self-dual additive
$\mathbb{F}_4$-codes of length $n$ and minimum weight $d$ from circulant
graphs. The quantum codes with these parameters are constructed for the first
time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04852</identifier>
 <datestamp>2015-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04852</id><created>2015-09-16</created><authors><author><keyname>Araiza-Illan</keyname><forenames>Dejanira</forenames></author><author><keyname>Western</keyname><forenames>David</forenames></author><author><keyname>Pipe</keyname><forenames>Anthony</forenames></author><author><keyname>Eder</keyname><forenames>Kerstin</forenames></author></authors><title>Coverage-Driven Verification - An approach to verify code for robots
  that directly interact with humans</title><categories>cs.RO</categories><comments>Accepted for publication, Haifa Verification Conference (HVC) 2015</comments><doi>10.1007/978-3-319-26287-1_5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Collaborative robots could transform several industries, such as
manufacturing and healthcare, but they present a significant challenge to
verification. The complex nature of their working environment necessitates
testing in realistic detail under a broad range of circumstances. We propose
the use of Coverage-Driven Verification (CDV) to meet this challenge. By
automating the simulation-based testing process as far as possible, CDV
provides an efficient route to coverage closure. We discuss the need, practical
considerations, and potential benefits of transferring this approach from
microelectronic design verification to the field of human-robot interaction. We
demonstrate the validity and feasibility of the proposed approach by
constructing a custom CDV testbench and applying it to the verification of an
object handover task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04853</identifier>
 <datestamp>2015-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04853</id><created>2015-09-16</created><authors><author><keyname>Dasgupta</keyname><forenames>Anirban</forenames></author><author><keyname>George</keyname><forenames>Anjith</forenames></author><author><keyname>Happy</keyname><forenames>SL</forenames></author><author><keyname>Routray</keyname><forenames>Aurobinda</forenames></author></authors><title>An On-board Video Database of Human Drivers</title><categories>cs.CV</categories><comments>4 pages, 5 figures. arXiv admin note: text overlap with
  arXiv:1505.04055</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Detection of fatigue due to drowsiness or loss of attention in human drivers
is an evolving area of research. Several algorithms have been implemented to
detect the level of fatigue in human drivers by capturing videos of facial
image sequences and extracting facial features such as eye closure rates, eye
gaze, head nodding, blink frequency etc. However, availability of standard
video database to validate such algorithms is insufficient. This paper
discusses the creation of such a database created under on-board conditions
during the day as well as night. Passive Near Infra-red (NIR) illumination has
been used for illuminating the face during night driving since prolonged
exposure to active Infra-Red lighting may lead to many health issues. The
database contains videos of 30 subjects under actual driving conditions.
Variation is ensured as the database contains different head orientations and
with different facial expressions, facial occlusions and illumination
variation. This new database can be a very valuable resource for development
and evaluation of algorithms for the video based detection of driver fatigue.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04857</identifier>
 <datestamp>2015-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04857</id><created>2015-09-16</created><authors><author><keyname>Gerencs&#xe9;r</keyname><forenames>Bal&#xe1;zs</forenames></author><author><keyname>Cloquet</keyname><forenames>Christophe</forenames></author><author><keyname>Blondel</keyname><forenames>Vincent</forenames></author></authors><title>Markov modeling of Twitter tweet inter-arrival times</title><categories>stat.AP cs.SI physics.soc-ph</categories><msc-class>62P25 (Primary), 91C20(Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce an improved model for human communication patterns, in
particular for arrival times of Twitter tweets. We introduce a concept that
allows to capture the dependence of subsequent waiting times between such
events. The presence of such dependence not only matches intuition, but the
data shows a significantly better fit for the new model, thus confirming the
proposition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04863</identifier>
 <datestamp>2015-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04863</id><created>2015-09-16</created><authors><author><keyname>Hsieh</keyname><forenames>Sung-Hsien</forenames></author><author><keyname>Lu</keyname><forenames>Chun-Shien</forenames></author><author><keyname>Pei</keyname><forenames>and Soo-Chang</forenames></author></authors><title>Fast Template Matching by Subsampled Circulant Matrix</title><categories>cs.DS cs.CV</categories><comments>7 pages, 1 figure, 2 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Template matching is widely used for many applications in image and signal
processing and usually is time-critical. Traditional methods usually focus on
how to reduce the search locations by coarse-to-fine strategy or full search
combined with pruning strategy. However, the computation cost of those methods
is easily dominated by the size of signal N instead of that of template K. This
paper proposes a probabilistic and fast matching scheme, which computation
costs requires O(N) additions and O(K \log K) multiplications, based on
cross-correlation. The nuclear idea is to first downsample signal, which size
becomes O(K), and then subsequent operations only involves downsampled signals.
The probability of successful match depends on cross-correlation between signal
and the template. We show the sufficient condition for successful match and
prove that the probability is high for binary signals with K^2/log K &gt;= O(N).
The experiments shows this proposed scheme is fast and efficient and supports
the theoretical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04874</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04874</id><created>2015-09-16</created><updated>2015-09-18</updated><authors><author><keyname>Huang</keyname><forenames>Lichao</forenames></author><author><keyname>Yang</keyname><forenames>Yi</forenames></author><author><keyname>Deng</keyname><forenames>Yafeng</forenames></author><author><keyname>Yu</keyname><forenames>Yinan</forenames></author></authors><title>DenseBox: Unifying Landmark Localization with End to End Object
  Detection</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How can a single fully convolutional neural network (FCN) perform on object
detection? We introduce DenseBox, a unified end-to-end FCN framework that
directly predicts bounding boxes and object class confidences through all
locations and scales of an image. Our contribution is two-fold. First, we show
that a single FCN, if designed and optimized carefully, can detect multiple
different objects extremely accurately and efficiently. Second, we show that
when incorporating with landmark localization during multi-task learning,
DenseBox further improves object detection accuray. We present experimental
results on public benchmark datasets including MALF face detection and KITTI
car detection, that indicate our DenseBox is the state-of-the-art system for
detecting challenging objects such as faces and cars.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04880</identifier>
 <datestamp>2015-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04880</id><created>2015-09-16</created><authors><author><keyname>Kim</keyname><forenames>Eunjung</forenames></author><author><keyname>Oum</keyname><forenames>Sang-il</forenames></author><author><keyname>Paul</keyname><forenames>Christophe</forenames></author><author><keyname>Sau</keyname><forenames>Ignasi</forenames></author><author><keyname>Thilikos</keyname><forenames>Dimitrios M.</forenames></author></authors><title>An FPT 2-Approximation for Tree-Cut Decomposition</title><categories>cs.DS cs.DM</categories><comments>17 pages, 3 figures</comments><msc-class>68R10, 05C85</msc-class><acm-class>G.2.2; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The tree-cut width of a graph is a graph parameter defined by Wollan [J.
Comb. Theory, Ser. B, 110:47-66, 2015] with the help of tree-cut
decompositions. In certain cases, tree-cut width appears to be more adequate
than treewidth as an invariant that, when bounded, can accelerate the
resolution of intractable problems. While designing algorithms for problems
with bounded tree-cut width, it is important to have a parametrically tractable
way to compute the exact value of this parameter or, at least, some constant
approximation of it. In this paper we give a parameterized 2-approximation
algorithm for the computation of tree-cut width; for an input $n$-vertex graph
$G$ and an integer $w$, our algorithm either confirms that the tree-cut width
of $G$ is more than $w$ or returns a tree-cut decomposition of $G$ certifying
that its tree-cut width is at most $2w$, in time $2^{O(w^2\log w)} \cdot n^2$.
Prior to this work, no constructive parameterized algorithms, even approximated
ones, existed for computing the tree-cut width of a graph. As a consequence of
the Graph Minors series by Robertson and Seymour, only the existence of a
decision algorithm was known.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04887</identifier>
 <datestamp>2015-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04887</id><created>2015-09-16</created><authors><author><keyname>Dasgupta</keyname><forenames>Anirban</forenames></author><author><keyname>Kabi</keyname><forenames>Bibek</forenames></author><author><keyname>George</keyname><forenames>Anjith</forenames></author><author><keyname>Happy</keyname><forenames>SL</forenames></author><author><keyname>Routray</keyname><forenames>Aurobinda</forenames></author></authors><title>A Drowsiness Detection Scheme Based on Fusion of Voice and Vision Cues</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Drowsiness level detection of an individual is very important in many safety
critical applications such as driving. There are several invasive and contact
based methods such as use of blood biochemical, brain signals etc. which can
estimate the level of drowsiness very accurately. However, these methods are
very difficult to implement in practical scenarios, as they cause discomfort to
the user. This paper presents a combined voice and vision based drowsiness
detection system well suited to detect the drowsiness level of an automotive
driver. The vision and voice based detection, being non-contact methods, has
the advantage of their feasibility of implementation. The authenticity of these
methods have been cross-validated using brain signals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04904</identifier>
 <datestamp>2015-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04904</id><created>2015-09-16</created><authors><author><keyname>Parida</keyname><forenames>Pramod Kumar</forenames></author><author><keyname>Marwala</keyname><forenames>Tshilidzi</forenames></author><author><keyname>Chakraverty</keyname><forenames>Snehashish</forenames></author></authors><title>Causal Model Analysis using Collider v-structure with Negative
  Percentage Mapping</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A major problem of causal inference is the arrangement of dependent nodes in
a directed acyclic graph (DAG) with path coefficients and observed confounders.
Path coefficients do not provide the units to measure the strength of
information flowing from one node to the other. Here we proposed the method of
causal structure learning using collider v-structures (CVS) with Negative
Percentage Mapping (NPM) to get selective thresholds of information strength,
to direct the edges and subjective confounders in a DAG. The NPM is used to
scale the strength of information passed through nodes in units of percentage
from interval from 0 to 1. The causal structures are constructed by bottom up
approach using path coefficients, causal directions and confounders, derived
implementing collider v-structure and NPM. The method is self-sufficient to
observe all the latent confounders present in the causal model and capable of
detecting every responsible causal direction. The results are tested for
simulated datasets of non-Gaussian distributions and compared with DirectLiNGAM
and ICA-LiNGAM to check efficiency of the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04905</identifier>
 <datestamp>2015-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04905</id><created>2015-09-16</created><authors><author><keyname>Doran</keyname><forenames>Derek</forenames></author></authors><title>On the discovery of social roles in large scale social systems</title><categories>cs.SI physics.soc-ph</categories><journal-ref>Social Network Analysis And Mining, Vol. 5, No. 49, December 2015</journal-ref><doi>10.1007/s13278-015-0290-0</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The social role of a participant in a social system is a label
conceptualizing the circumstances under which she interacts within it. They may
be used as a theoretical tool that explains why and how users participate in an
online social system. Social role analysis also serves practical purposes, such
as reducing the structure of complex systems to rela- tionships among roles
rather than alters, and enabling a comparison of social systems that emerge in
similar contexts. This article presents a data-driven approach for the
discovery of social roles in large scale social systems. Motivated by an
analysis of the present art, the method discovers roles by the conditional
triad censuses of user ego-networks, which is a promising tool because they
capture the degree to which basic social forces push upon a user to interact
with others. Clusters of censuses, inferred from samples of large scale network
carefully chosen to preserve local structural prop- erties, define the social
roles. The promise of the method is demonstrated by discussing and discovering
the roles that emerge in both Facebook and Wikipedia. The article con- cludes
with a discussion of the challenges and future opportunities in the discovery
of social roles in large social systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04916</identifier>
 <datestamp>2015-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04916</id><created>2015-09-16</created><authors><author><keyname>Liu</keyname><forenames>Li</forenames></author><author><keyname>Yu</keyname><forenames>Mengyang</forenames></author><author><keyname>Shao</keyname><forenames>Ling</forenames></author></authors><title>Projection Bank: From High-dimensional Data to Medium-length Binary
  Codes</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, very high-dimensional feature representations, e.g., Fisher Vector,
have achieved excellent performance for visual recognition and retrieval.
However, these lengthy representations always cause extremely heavy
computational and storage costs and even become unfeasible in some large-scale
applications. A few existing techniques can transfer very high-dimensional data
into binary codes, but they still require the reduced code length to be
relatively long to maintain acceptable accuracies. To target a better balance
between computational efficiency and accuracies, in this paper, we propose a
novel embedding method called Binary Projection Bank (BPB), which can
effectively reduce the very high-dimensional representations to
medium-dimensional binary codes without sacrificing accuracies. Instead of
using conventional single linear or bilinear projections, the proposed method
learns a bank of small projections via the max-margin constraint to optimally
preserve the intrinsic data similarity. We have systematically evaluated the
proposed method on three datasets: Flickr 1M, ILSVR2010 and UCF101, showing
competitive retrieval and recognition accuracies compared with state-of-the-art
approaches, but with a significantly smaller memory footprint and lower coding
complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04927</identifier>
 <datestamp>2015-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04927</id><created>2015-09-16</created><authors><author><keyname>Blum</keyname><forenames>Norbert</forenames></author></authors><title>Maximum Matching in General Graphs Without Explicit Consideration of
  Blossoms Revisited</title><categories>cs.DS cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We reduce the problem of finding an augmenting path in a general graph to a
reachability problem in a directed bipartite graph. A slight modification of
depth-first search leads to an algorithm for finding such paths. Although this
setting is equivalent to the traditional terminology of blossoms due to
Edmonds, there are some advantages. Mainly, this point of view enables the
description of algorithms for the solution of matching problems without
explicit analysis of blossoms, nested blossoms, and so on. Exemplary, we
describe an efficient realization of the Hopcroft-Karp approach for the
computation of a maximum cardinality matching in general graphs and a variant
of Edmonds' primal-dual algorithm for the maximum weighted matching problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04932</identifier>
 <datestamp>2015-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04932</id><created>2015-09-16</created><authors><author><keyname>Ma</keyname><forenames>Meijie</forenames></author></authors><title>Cycles in enhanced hypercubes</title><categories>cs.DM math.CO</categories><comments>9 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The enhanced hypercube $Q_{n,k}$ is a variant of the hypercube $Q_n$. We
investigate all the lengths of cycles that an edge of the enhanced hypercube
lies on. It is proved that every edge of $Q_{n,k}$ lies on a cycle of every
even length from $4$ to $2^n$; if $k$ is even, every edge of $Q_{n,k}$ also
lies on a cycle of every odd length from $k+3$ to $2^n-1$, and some special
edges lie on a shortest odd cycle of length $k+1$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04934</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04934</id><created>2015-09-16</created><authors><author><keyname>Saz</keyname><forenames>Oscar</forenames></author><author><keyname>Doulaty</keyname><forenames>Mortaza</forenames></author><author><keyname>Hain</keyname><forenames>Thomas</forenames></author></authors><title>Background-tracking Acoustic Features for Genre Identification of
  Broadcast Shows</title><categories>cs.SD</categories><journal-ref>IEEE Spoken Language Technology Workshop (SLT 2014), pp118-123,
  7-10 Dec 2014, Lake Tahoe, NV, USA</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel method for extracting acoustic features that
characterise the background environment in audio recordings. These features are
based on the output of an alignment that fits multiple parallel
background--based Constrained Maximum Likelihood Linear Regression
transformations asynchronously to the input audio signal. With this setup, the
resulting features can track changes in the audio background like appearance
and disappearance of music, applause or laughter, independently of the speakers
in the foreground of the audio. The ability to provide this type of acoustic
description in audiovisual data has many potential applications, including
automatic classification of broadcast archives or improving automatic
transcription and subtitling. In this paper, the performance of these features
in a genre identification task in a set of 332 BBC shows is explored. The
proposed background--tracking features outperform short--term Perceptual Linear
Prediction features in this task using Gaussian Mixture Model classifiers (62%
vs 72% accuracy). The use of more complex classifiers, Hidden Markov Models and
Support Vector Machines, increases the performance of the system with the novel
background--tracking features to 79% and 81% in accuracy respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04942</identifier>
 <datestamp>2015-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04942</id><created>2015-09-16</created><authors><author><keyname>Jia</keyname><forenames>Xu</forenames></author><author><keyname>Gavves</keyname><forenames>Efstratios</forenames></author><author><keyname>Fernando</keyname><forenames>Basura</forenames></author><author><keyname>Tuytelaars</keyname><forenames>Tinne</forenames></author></authors><title>Guiding Long-Short Term Memory for Image Caption Generation</title><categories>cs.CV</categories><comments>accepted by ICCV 2015</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we focus on the problem of image caption generation. We propose
an extension of the long short term memory (LSTM) model, which we coin gLSTM
for short. In particular, we add semantic information extracted from the image
as extra input to each unit of the LSTM block, with the aim of guiding the
model towards solutions that are more tightly coupled to the image content.
Additionally, we explore different length normalization strategies for beam
search in order to prevent from favoring short sentences. On various benchmark
datasets such as Flickr8K, Flickr30K and MS COCO, we obtain results that are on
par with or even outperform the current state-of-the-art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04944</identifier>
 <datestamp>2015-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04944</id><created>2015-09-16</created><authors><author><keyname>Hon</keyname><forenames>Wing-Kai</forenames></author><author><keyname>Kloks</keyname><forenames>Ton</forenames></author><author><keyname>Liu</keyname><forenames>Hsiang-Hsuan</forenames></author><author><keyname>Wang</keyname><forenames>Hung-Lung</forenames></author><author><keyname>Wang</keyname><forenames>Yue-Li</forenames></author></authors><title>Convexities in Some Special Graph Classes ---New Results in AT-free
  Graphs and Beyond</title><categories>cs.DM</categories><comments>20 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study convexity properties of graphs. In this paper we present a
linear-time algorithm for the geodetic number in tree-cographs. Settling a
10-year-old conjecture, we prove that the Steiner number is at least the
geodetic number in AT-free graphs. Computing a maximal and proper monophonic
set in $\AT$-free graphs is NP-complete. We present polynomial algorithms for
the monophonic number in permutation graphs and the geodetic number in $P_4$-
sparse graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04954</identifier>
 <datestamp>2015-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04954</id><created>2015-09-16</created><authors><author><keyname>Yang</keyname><forenames>Heng</forenames></author><author><keyname>Zhang</keyname><forenames>Renqiao</forenames></author><author><keyname>Robinson</keyname><forenames>Peter</forenames></author></authors><title>Human and Sheep Facial Landmarks Localisation by Triplet Interpolated
  Features</title><categories>cs.CV</categories><comments>submitted to WACV2016</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a method for localisation of facial landmarks on
human and sheep. We introduce a new feature extraction scheme called
triplet-interpolated feature used at each iteration of the cascaded shape
regression framework. It is able to extract features from similar semantic
location given an estimated shape, even when head pose variations are large and
the facial landmarks are very sparsely distributed. Furthermore, we study the
impact of training data imbalance on model performance and propose a training
sample augmentation scheme that produces more initialisations for training
samples from the minority. More specifically, the augmentation number for a
training sample is made to be negatively correlated to the value of the fitted
probability density function at the sample's position. We evaluate the proposed
scheme on both human and sheep facial landmarks localisation. On the benchmark
300w human face dataset, we demonstrate the benefits of our proposed methods
and show very competitive performance when comparing to other methods. On a
newly created sheep face dataset, we get very good performance despite the fact
that we only have a limited number of training samples and a set of sparse
landmarks are annotated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04956</identifier>
 <datestamp>2015-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04956</id><created>2015-09-16</created><authors><author><keyname>G&#xf3;mez</keyname><forenames>Francisco</forenames></author><author><keyname>Mora</keyname><forenames>Joaqu&#xed;n</forenames></author><author><keyname>G&#xf3;mez</keyname><forenames>Emilia</forenames></author><author><keyname>D&#xed;az-B&#xe1;&#xf1;ez</keyname><forenames>Jos&#xe9; Miguel</forenames></author></authors><title>Melodic Contour and Mid-Level Global Features Applied to the Analysis of
  Flamenco Cantes</title><categories>cs.SD cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work focuses on the topic of melodic characterization and similarity in
a specific musical repertoire: a cappella flamenco singing, more specifically
in debla and martinete styles. We propose the combination of manual and
automatic description. First, we use a state-of-the-art automatic transcription
method to account for general melodic similarity from music recordings. Second,
we define a specific set of representative mid-level melodic features, which
are manually labeled by flamenco experts. Both approaches are then contrasted
and combined into a global similarity measure. This similarity measure is
assessed by inspecting the clusters obtained through phylogenetic algorithms
algorithms and by relating similarity to categorization in terms of style.
Finally, we discuss the advantage of combining automatic and expert annotations
as well as the need to include repertoire-specific descriptions for meaningful
melodic characterization in traditional music collections.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.04995</identifier>
 <datestamp>2015-10-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.04995</id><created>2015-09-16</created><updated>2015-10-16</updated><authors><author><keyname>Wright</keyname><forenames>Matthew</forenames></author><author><keyname>Gomes</keyname><forenames>Gabriel</forenames></author><author><keyname>Horowitz</keyname><forenames>Roberto</forenames></author><author><keyname>Kurzhanskiy</keyname><forenames>Alex A.</forenames></author></authors><title>A new model for multi-commodity macroscopic modeling of complex traffic
  networks</title><categories>cs.SY</categories><comments>34 pages with appendix and examples. Figures in black and white. v3:
  Typos corrected</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a macroscopic modeling framework for a network of roads and
multi-commodity traffic. The proposed framework is based on the
Lighthill-Whitham-Richards kinematic wave theory; more precisely, on its
discretization, the Cell Transmission Model (CTM), adapted for networks and
multi-commodity traffic. The resulting model is called the Link-Node CTM
(LNCTM).
  In the LNCTM, we use the fundamental diagram of an &quot;inverse lambda&quot; shape
that allows modeling of the capacity drop and the hysteresis behavior of the
traffic state in a link that goes from free flow to congestion and back.
  A model of the node with multiple input and multiple output links accepting
multi-commodity traffic is a cornerstone of the LNCTM. We present the
multi-input-multi-output (MIMO) node model for multi-commodity traffic that
supersedes previously developed node models. The analysis and comparison with
previous node models are provided.
  Sometimes, certain traffic commodities may choose between multiple output
links in a node based on the current traffic state of the node's input and
output links. For such situations, we propose a local traffic assignment
algorithm that computes how incoming traffic of a certain commodity should be
distributed between output links, if this information is not known a priori.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05001</identifier>
 <datestamp>2015-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05001</id><created>2015-09-16</created><authors><author><keyname>Ronagh</keyname><forenames>Pooya</forenames></author><author><keyname>Woods</keyname><forenames>Brad</forenames></author><author><keyname>Iranmanesh</keyname><forenames>Ehsan</forenames></author></authors><title>Solving constrained quadratic binary problems via quantum adiabatic
  evolution</title><categories>math.OC cs.ET quant-ph</categories><comments>20 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantum adiabatic evolution is perceived as useful for binary quadratic
programming problems that are a priori unconstrained. For constrained problems,
it is a common practice to relax linear equality constraints as penalty terms
in the objective function. However, there has not yet been proposed a method
for efficiently dealing with inequality constraints using the quantum adiabatic
approach. In this paper, we give a method for solving the Lagrangian dual of a
binary quadratic programming (BQP) problem in the presence of inequality
constraints and employ this procedure within a branch-and-bound framework for
constrained BQP (CBQP) problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05008</identifier>
 <datestamp>2015-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05008</id><created>2015-09-02</created><updated>2015-12-15</updated><authors><author><keyname>Escobedo</keyname><forenames>Ram&#xf3;n</forenames></author><author><keyname>Iba&#xf1;ez</keyname><forenames>Aitziber</forenames></author><author><keyname>Zuazua</keyname><forenames>Enrique</forenames></author></authors><title>Optimal strategies for driving a mobile agent in a guidance by repulsion
  model</title><categories>math.OC cs.SY math.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a guidance by repulsion model based on a driver-evader interaction
where the driver, assumed to be faster than the evader, follows the evader but
cannot be arbitrarily close to it, and the evader tries to move away from the
driver beyond a short distance. The key ingredient allowing the driver to guide
the evader is that the driver is able to display a circumvention maneuver
around the evader, in such a way that the trajectory of the evader is modified
in the direction of the repulsion that the driver exerts on the evader. The
evader can thus be driven towards any given target or along a sufficiently
smooth path by controlling a single discrete parameter acting on driver's
behavior. The control parameter serves both to activate/deactivate the
circumvention mode and to select the clockwise/counterclockwise direction of
the circumvention maneuver. Assuming that the circumvention mode is more
expensive than the pursuit mode, and that the activation of the circumvention
mode has a high cost, we formulate an optimal control problem for the optimal
strategy to drive the evader to a given target. By means of numerical shooting
methods, we find the optimal open-loop control which reduces the number of
activations of the circumvention mode to one and which minimizes the time spent
in the active~mode. Our numerical simulations show that the system is highly
sensitive to small variations of the control function, and that the cost
function has a nonlinear regime which contributes to the complexity of the
behavior of the system, so that a general open-loop control would not be of
practical interest. We then propose a feedback control law that corrects from
deviations while preventing from an excesive use of the circumvention mode,
finding numerically that the feedback law significantly reduces the cost
obtained with the open-loop control.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05009</identifier>
 <datestamp>2016-02-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05009</id><created>2015-09-16</created><updated>2016-02-14</updated><authors><author><keyname>Cohen</keyname><forenames>Nadav</forenames></author><author><keyname>Sharir</keyname><forenames>Or</forenames></author><author><keyname>Shashua</keyname><forenames>Amnon</forenames></author></authors><title>On the Expressive Power of Deep Learning: A Tensor Analysis</title><categories>cs.NE cs.LG cs.NA stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It has long been conjectured that hypotheses spaces suitable for data that is
compositional in nature, such as text or images, may be more efficiently
represented with deep hierarchical networks than with shallow ones. Despite the
vast empirical evidence supporting this belief, theoretical justifications to
date are limited. In particular, they do not account for the locality, sharing
and pooling constructs of convolutional networks, the most successful deep
learning architecture to date. In this work we derive a deep network
architecture based on arithmetic circuits that inherently employs locality,
sharing and pooling. An equivalence between the networks and hierarchical
tensor factorizations is established. We show that a shallow network
corresponds to CP (rank-1) decomposition, whereas a deep network corresponds to
Hierarchical Tucker decomposition.
  Using tools from measure theory and matrix algebra, we prove that besides a
negligible set, all functions that can be implemented by a deep network of
polynomial size, require exponential size in order to be realized (or even
approximated) by a shallow network. Since log-space computation transforms our
networks into SimNets, the result applies directly to a deep learning
architecture demonstrating promising empirical performance. The construction
and theory developed in this paper shed new light on various practices and
ideas employed by the deep learning community.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05016</identifier>
 <datestamp>2015-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05016</id><created>2015-09-16</created><authors><author><keyname>Jain</keyname><forenames>Ashesh</forenames></author><author><keyname>Singh</keyname><forenames>Avi</forenames></author><author><keyname>Koppula</keyname><forenames>Hema S</forenames></author><author><keyname>Soh</keyname><forenames>Shane</forenames></author><author><keyname>Saxena</keyname><forenames>Ashutosh</forenames></author></authors><title>Recurrent Neural Networks for Driver Activity Anticipation via
  Sensory-Fusion Architecture</title><categories>cs.CV cs.AI cs.RO</categories><comments>Follow-up of ICCV 2015 Brain4Cars http://www.brain4cars.com</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Anticipating the future actions of a human is a widely studied problem in
robotics that requires spatio-temporal reasoning. In this work we propose a
deep learning approach for anticipation in sensory-rich robotics applications.
We introduce a sensory-fusion architecture which jointly learns to anticipate
and fuse information from multiple sensory streams. Our architecture consists
of Recurrent Neural Networks (RNNs) that use Long Short-Term Memory (LSTM)
units to capture long temporal dependencies. We train our architecture in a
sequence-to-sequence prediction manner, and it explicitly learns to predict the
future given only a partial temporal context. We further introduce a novel loss
layer for anticipation which prevents over-fitting and encourages early
anticipation. We use our architecture to anticipate driving maneuvers several
seconds before they happen on a natural driving data set of 1180 miles. The
context for maneuver anticipation comes from multiple sensors installed on the
vehicle. Our approach shows significant improvement over the state-of-the-art
in maneuver anticipation by increasing the precision from 77.4% to 90.5% and
recall from 71.2% to 87.4%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05022</identifier>
 <datestamp>2015-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05022</id><created>2015-09-14</created><authors><author><keyname>Kondrashova</keyname><forenames>Elizaveta V.</forenames></author></authors><title>Problem of optimization of a transport traffic at preliminary
  registration of queires with use of CBSMAP-model</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of optimization of a transport traffic at preliminary
registration of demands with use of the CBSMAP model is investigated. For the
solution of an objective application of the queueing theory and the theory of
controlled processes is supposed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05024</identifier>
 <datestamp>2015-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05024</id><created>2015-09-13</created><authors><author><keyname>Vilisov</keyname><forenames>Valery</forenames></author></authors><title>Modeling Concordances of Company's Investment Directions With Its Market
  Attraction</title><categories>q-fin.PM cs.CY stat.AP</categories><doi>10.13140/RG.2.1.1707.8248</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work models the interconnection of company's investment managers'
representations and the market attraction of its shares. The models that
reflect the connection of the company's market effectiveness indices and
parameters of its economic activity are created on the basis of the
Mean-Variance Analysis and Regression Analysis. On another side, expert
evaluation methods also clarified the same influence parameters, but it was
made according to the opinion of company managers. These two evaluation rows
are used when making managerial decisions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05025</identifier>
 <datestamp>2015-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05025</id><created>2015-09-15</created><authors><author><keyname>Osipov</keyname><forenames>I. V.</forenames></author><author><keyname>Volinsky</keyname><forenames>A. A.</forenames></author><author><keyname>Nikulchev</keyname><forenames>E.</forenames></author><author><keyname>Plokhov</keyname><forenames>D.</forenames></author></authors><title>Study of Monetization as a Way of Motivating Freemium Service Users</title><categories>cs.CY</categories><comments>8 pages. arXiv admin note: substantial text overlap with
  arXiv:1501.04157</comments><journal-ref>Contemporary Engineering Sciences, 2015, 8(20):911-918</journal-ref><doi>10.12988/ces2015.57212</doi><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  The paper describes user behavior as a result of introducing monetization in
the freemium educational online platform. Monetization resulted in alternative
system growth mechanisms causing viral increase in the number of users. System
metrics in terms of the $K$-factor was utilized as an indicator of the system
user base growth. The weekly $K$-factor doubled as a result of monetization
introduction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05053</identifier>
 <datestamp>2015-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05053</id><created>2015-09-16</created><authors><author><keyname>Khuong</keyname><forenames>Paul-Virak</forenames></author><author><keyname>Morin</keyname><forenames>Pat</forenames></author></authors><title>Array Layouts for Comparison-Based Searching</title><categories>cs.DS</categories><comments>43 pages; 23 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We attempt to determine the best order and search algorithm to store $n$
comparable data items in an array, $A$, of length $n$ so that we can, for any
query value, $x$, quickly find the smallest value in $A$ that is greater than
or equal to $x$. In particular, we consider the important case where there are
many such queries to the same array, $A$, which resides entirely in RAM. In
addition to the obvious sorted order/binary search combination we consider the
Eytzinger (BFS) layout normally used for heaps, an implicit B-tree layout that
generalizes the Eytzinger layout, and the van Emde Boas layout commonly used in
the cache-oblivious algorithms literature.
  After extensive testing and tuning on a wide variety of modern hardware, we
arrive at the conclusion that, for small values of $n$, sorted order, combined
with a good implementation of binary search is best. For larger values of $n$,
we arrive at the surprising conclusion that the Eytzinger layout is usually the
fastest. The latter conclusion is unexpected and goes counter to earlier
experimental work by Brodal, Fagerberg, and Jacob (SODA~2003), who concluded
that both the B-tree and van Emde Boas layouts were faster than the Eytzinger
layout for large values of $n$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05054</identifier>
 <datestamp>2015-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05054</id><created>2015-09-16</created><authors><author><keyname>Irofti</keyname><forenames>Paul</forenames></author><author><keyname>Dumitrescu</keyname><forenames>Bogdan</forenames></author></authors><title>Overcomplete Dictionary Learning with Jacobi Atom Updates</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dictionary learning for sparse representations is traditionally approached
with sequential atom updates, in which an optimized atom is used immediately
for the optimization of the next atoms. We propose instead a Jacobi version, in
which groups of atoms are updated independently, in parallel. Extensive
numerical evidence for sparse image representation shows that the parallel
algorithms, especially when all atoms are updated simultaneously, give better
dictionaries than their sequential counterparts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05057</identifier>
 <datestamp>2015-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05057</id><created>2015-09-16</created><authors><author><keyname>Short</keyname><forenames>Taylor</forenames></author></authors><title>On some conjectures concerning critical independent sets of a graph</title><categories>math.CO cs.DM</categories><comments>10 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $G$ be a simple graph with vertex set $V(G)$. A set $S\subseteq V(G)$ is
independent if no two vertices from $S$ are adjacent. For $X\subseteq V(G)$,
the difference of $X$ is $d(X) = |X|-|N(X)|$ and an independent set $A$ is
critical if $d(A) = \max \{d(X): X\subseteq V(G) \text{ is an independent
set}\}$ (possibly $A=\emptyset$). Let $\text{nucleus}(G)$ and
$\text{diadem}(G)$ be the intersection and union, respectively, of all maximum
size critical independent sets in $G$. In this paper, we will give two new
characterizations of K\&quot;{o}nig-Egerv\'{a}ry graphs involving
$\text{nucleus}(G)$ and $\text{diadem}(G)$. We also prove a related lower bound
for the independence number of a graph. This work answers several conjectures
posed by Jarden, Levit, and Mandrescu.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05064</identifier>
 <datestamp>2015-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05064</id><created>2015-09-16</created><authors><author><keyname>Hand</keyname><forenames>Paul</forenames></author><author><keyname>Lee</keyname><forenames>Choongbum</forenames></author><author><keyname>Voroninski</keyname><forenames>Vladislav</forenames></author></authors><title>Exact simultaneous recovery of locations and structure from known
  orientations and corrupted point correspondences</title><categories>cs.CV cs.IT math.CO math.IT math.OC</categories><comments>arXiv admin note: text overlap with arXiv:1506.01437</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $t_1,\ldots,t_{n_l} \in \mathbb{R}^d$ and $p_1,\ldots,p_{n_s} \in
\mathbb{R}^d$ and consider the bipartite location recovery problem: given a
subset of pairwise direction observations $\{(t_i - p_j) / \|t_i -
p_j\|_2\}_{i,j \in [n_l] \times [n_s]}$, where a constant fraction of these
observations are arbitrarily corrupted, find $\{t_i\}_{i \in [n_ll]}$ and
$\{p_j\}_{j \in [n_s]}$ up to a global translation and scale. We study the
recently introduced ShapeFit algorithm as a method for solving this bipartite
location recovery problem. In this case, ShapeFit consists of a simple convex
program over $d(n_l + n_s)$ real variables. We prove that this program recovers
a set of $n_l+n_s$ i.i.d. Gaussian locations exactly and with high probability
if the observations are given by a bipartite Erd\H{o}s-R\'{e}nyi graph, $d$ is
large enough, and provided that at most a constant fraction of observations
involving any particular location are adversarially corrupted. This recovery
theorem is based on a set of deterministic conditions that we prove are
sufficient for exact recovery. Finally, we propose a modified pipeline for the
Structure for Motion problem, based on this bipartite location recovery
problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05065</identifier>
 <datestamp>2015-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05065</id><created>2015-09-16</created><authors><author><keyname>Brandao</keyname><forenames>Fernando G. S. L.</forenames></author><author><keyname>Harrow</keyname><forenames>Aram W.</forenames></author></authors><title>Estimating operator norms using covering nets</title><categories>quant-ph cs.CC cs.DS math.OC</categories><comments>24 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present several polynomial- and quasipolynomial-time approximation schemes
for a large class of generalized operator norms. Special cases include the
$2\rightarrow q$ norm of matrices for $q&gt;2$, the support function of the set of
separable quantum states, finding the least noisy output of
entanglement-breaking quantum channels, and approximating the injective tensor
norm for a map between two Banach spaces whose factorization norm through
$\ell_1^n$ is bounded.
  These reproduce and in some cases improve upon the performance of previous
algorithms by Brand\~ao-Christandl-Yard and followup work, which were based on
the Sum-of-Squares hierarchy and whose analysis used techniques from quantum
information such as the monogamy principle of entanglement. Our algorithms, by
contrast, are based on brute force enumeration over carefully chosen covering
nets. These have the advantage of using less memory, having much simpler proofs
and giving new geometric insights into the problem. Net-based algorithms for
similar problems were also presented by Shi-Wu and Barak-Kelner-Steurer, but in
each case with a run-time that is exponential in the rank of some matrix. We
achieve polynomial or quasipolynomial runtimes by using the much smaller nets
that exist in $\ell_1$ spaces. This principle has been used in learning theory,
where it is known as Maurey's empirical method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05066</identifier>
 <datestamp>2015-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05066</id><created>2015-09-16</created><authors><author><keyname>Gupta</keyname><forenames>Priyank</forenames></author><author><keyname>Koudas</keyname><forenames>Nick</forenames></author><author><keyname>Shang</keyname><forenames>Europa</forenames></author><author><keyname>Johnson</keyname><forenames>Ryan</forenames></author><author><keyname>Zuzarte</keyname><forenames>Calisto</forenames></author></authors><title>Processing Analytical Workloads Incrementally</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Analysis of large data collections using popular machine learning and
statistical algorithms has been a topic of increasing research interest. A
typical analysis workload consists of applying an algorithm to build a model on
a data collection and subsequently refining it based on the results.
  In this paper we introduce model materialization and incremental model reuse
as first class citizens in the execution of analysis workloads. We materialize
built models instead of discarding them in a way that can be reused in
subsequent computations. At the same time we consider manipulating an existing
model (adding or deleting data from it) in order to build a new one. We discuss
our approach in the context of popular machine learning models. We specify the
details of how to incrementally maintain models as well as outline the suitable
optimizations required to optimally use models and their incremental
adjustments to build new ones. We detail our techniques for linear regression,
naive bayes and logistic regression and present the suitable algorithms and
optimizations to handle these models in our framework.
  We present the results of a detailed performance evaluation, using real and
synthetic data sets. Our experiments analyze the various trade offs inherent in
our approach and demonstrate vast performance benefits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05069</identifier>
 <datestamp>2015-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05069</id><created>2015-09-16</created><authors><author><keyname>Hellmuth</keyname><forenames>Marc</forenames></author><author><keyname>Wieseke</keyname><forenames>Nicolas</forenames></author></authors><title>On Tree Representations of Relations and Graphs: Symbolic Ultrametrics
  and Cograph Edge Decompositions</title><categories>cs.DM</categories><comments>arXiv admin note: substantial text overlap with arXiv:1501.03931</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Tree representations of (sets of) symmetric binary relations, or equivalently
edge-colored undirected graphs, are of central interest, e.g.\ in
phylogenomics. In this context symbolic ultrametrics play a crucial role.
Symbolic ultrametrics define an edge-colored complete graph that allows to
represent the topology of this graph as a vertex-colored tree. Here, we are
interested in the structure and the complexity of certain combinatorial
problems resulting from considerations based on symbolic ultrametrics, and on
algorithms to solve them.
  This includes, the characterization of symbolic ultrametrics that
additionally distinguishes between edges and non-edges of \emph{arbitrary}
edge-colored graphs $G$ and thus, yielding a tree representation of $G$, by
means of so-called cographs. Moreover, we address the problem of finding
&quot;closest&quot; symbolic ultrametrics and show the NP-completeness of the three
problems: symbolic ultrametric editing, completion and deletion. Finally, as
not all graphs are cographs, and hence, don't have a tree representation, we
ask, furthermore, what is the minimum number of cotrees needed to represent the
topology of an arbitrary non-cograph $G$. This is equivalent to find an optimal
cograph edge $k$-decomposition $\{E_1,\dots,E_k\}$ of $E$ so that each subgraph
$(V,E_i)$ of $G$ is a cograph. We investigate this problem in full detail,
resulting in several new open problems, and NP-hardness results.
  For all optimization problems proven to be NP-hard we will provide integer
linear program (ILP) formulations to efficiently solve them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05083</identifier>
 <datestamp>2015-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05083</id><created>2015-09-16</created><updated>2015-11-24</updated><authors><author><keyname>Wu</keyname><forenames>Lingfei</forenames></author><author><keyname>Baggio</keyname><forenames>Jacopo A.</forenames></author><author><keyname>Janssen</keyname><forenames>Marco A.</forenames></author></authors><title>The Role of Diverse Strategies in Sustainable Knowledge Production</title><categories>physics.soc-ph cs.SI</categories><comments>10 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online communities are becoming increasingly important as platforms for
large-scale human cooperation. These communities allow users seeking and
sharing professional skills to solve problems collaboratively. To investigate
how users cooperate to complete a large number of knowledge-producing tasks, we
analyze StackExchange, one of the largest question and answer systems in the
world. We construct attention networks to model the growth of 110 communities
in the StackExchange system and quantify individual answering strategies using
the linking dynamics of attention networks. We identify two types of users
taking different strategies. One strategy (type A) aims at performing
maintenance by doing simple tasks, while the other strategy (type B) aims
investing time in doing challenging tasks. We find that the number of type A
needs to be twice as big as type B users for a sustainable growth of
communities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05086</identifier>
 <datestamp>2015-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05086</id><created>2015-09-16</created><authors><author><keyname>Landford</keyname><forenames>Jordan</forenames></author><author><keyname>Meier</keyname><forenames>Rich</forenames></author><author><keyname>Barella</keyname><forenames>Richard</forenames></author><author><keyname>Zhao</keyname><forenames>Xinghui</forenames></author><author><keyname>Cotilla-Sanchez</keyname><forenames>Eduardo</forenames></author><author><keyname>Bass</keyname><forenames>Robert B.</forenames></author><author><keyname>Wallace</keyname><forenames>Scott</forenames></author></authors><title>Fast Sequence Component Analysis for Attack Detection in Synchrophasor
  Networks</title><categories>cs.LG cs.CR</categories><comments>8 pages, 4 figures, submitted to IEEE Transactions</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modern power systems have begun integrating synchrophasor technologies into
part of daily operations. Given the amount of solutions offered and the
maturity rate of application development it is not a matter of &quot;if&quot; but a
matter of &quot;when&quot; in regards to these technologies becoming ubiquitous in
control centers around the world. While the benefits are numerous, the
functionality of operator-level applications can easily be nullified by
injection of deceptive data signals disguised as genuine measurements. Such
deceptive action is a common precursor to nefarious, often malicious activity.
A correlation coefficient characterization and machine learning methodology are
proposed to detect and identify injection of spoofed data signals. The proposed
method utilizes statistical relationships intrinsic to power system parameters,
which are quantified and presented. Several spoofing schemes have been
developed to qualitatively and quantitatively demonstrate detection
capabilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05087</identifier>
 <datestamp>2015-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05087</id><created>2015-09-16</created><authors><author><keyname>Thill</keyname><forenames>Matthew</forenames></author><author><keyname>Hassibi</keyname><forenames>Babak</forenames></author></authors><title>Group Frames with Few Distinct Inner Products and Low Coherence</title><categories>cs.IT math.IT</categories><journal-ref>IEEE Transactions on Signal Processing, 63 (19). pp. 5222-5237
  (2015)</journal-ref><doi>10.1109/TSP.2015.2450195</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Frame theory has been a popular subject in the design of structured signals
and codes in recent years, with applications ranging from the design of
measurement matrices in compressive sensing, to spherical codes for data
compression and data transmission, to spacetime codes for MIMO communications,
and to measurement operators in quantum sensing. High-performance codes usually
arise from designing frames whose elements have mutually low coherence.
Building off the original &quot;group frame&quot; design of Slepian which has since been
elaborated in the works of Vale and Waldron, we present several new frame
constructions based on cyclic and generalized dihedral groups. Slepian's
original construction was based on the premise that group structure allows one
to reduce the number of distinct inner pairwise inner products in a frame with
$n$ elements from $\frac{n(n-1)}{2}$ to $n-1$. All of our constructions further
utilize the group structure to produce tight frames with even fewer distinct
inner product values between the frame elements. When $n$ is prime, for
example, we use cyclic groups to construct $m$-dimensional frame vectors with
at most $\frac{n-1}{m}$ distinct inner products. We use this behavior to bound
the coherence of our frames via arguments based on the frame potential, and
derive even tighter bounds from combinatorial and algebraic arguments using the
group structure alone. In certain cases, we recover well-known Welch bound
achieving frames. In cases where the Welch bound has not been achieved, and is
not known to be achievable, we obtain frames with close to Welch bound
performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05091</identifier>
 <datestamp>2015-10-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05091</id><created>2015-09-16</created><updated>2015-10-09</updated><authors><author><keyname>Bogale</keyname><forenames>Tadilo Endeshaw</forenames></author><author><keyname>Bao</keyname><forenames>Long</forenames></author><author><keyname>Wang</keyname><forenames>Xianbin</forenames></author></authors><title>Hybrid Analog-Digital Channel Estimation and Beamforming:
  Training-Throughput Tradeoff</title><categories>cs.IT math.IT</categories><comments>IEEE Transactions on Communication (To appear)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper designs hybrid analog-digital channel estimation and beamforming
techniques for multiuser massive multiple input multiple output (MIMO) systems
with limited number of radio frequency (RF) chains. For these systems, first we
design novel minimum mean square error (MMSE) hybrid analog-digital channel
estimator by considering both perfect and imperfect channel covariance matrix
knowledge cases. Then, we utilize the estimated channels to enable beamforming
for data transmission. When the channel covariance matrices of all user
equipments (UEs) are known perfectly, we show that there is a tradeoff between
the training duration and throughput. Specifically, we exploit that the optimal
training duration that maximizes the throughput depends on the covariance
matrices of all UEs, number of RF chains and channel coherence time ($T_c$). We
also show that the training time optimization problem can be formulated as a
concave maximization problem {for some system parameter settings} where its
global optimal solution is obtained efficiently using existing tools. In
particular, when the base station equipped with $64$ antennas and $1$ RF chain
is serving one single antenna UE, $T_c=128$ symbol periods ($T_s$) and signal
to noise ratio of $10$dB, we have found that the optimal training durations are
$4T_s$ and $20T_s$ for highly correlated and uncorrelated Rayleigh fading
channel coefficients, respectively. The analytical expressions are validated by
performing numerical and extensive Monte Carlo simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05093</identifier>
 <datestamp>2015-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05093</id><created>2015-09-16</created><authors><author><keyname>Bogale</keyname><forenames>Tadilo Endeshaw</forenames></author><author><keyname>Le</keyname><forenames>Long Bao</forenames></author><author><keyname>Wang</keyname><forenames>Xianbin</forenames></author><author><keyname>Vandendorpe</keyname><forenames>Luc</forenames></author></authors><title>Pilot Contamination Mitigation for Wideband Massive MMO: Number of Cells
  Vs Multipath</title><categories>cs.IT math.IT</categories><comments>IEEE Globecom 2015 (Accepted)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes novel joint channel estimation and beamforming approach
for multicell wideband massive multiple input multiple output (MIMO) systems.
Using our channel estimation and beamforming approach, we determine the number
of cells $N_c$ that can utilize the same time and frequency resource while
mitigating the effect of pilot contamination. The proposed approach exploits
the multipath characteristics of wideband channels. Specifically, when the
channel has $L$ multipath taps, it is shown that $N_c\leq L$ cells can reliably
estimate the channels of their user equipments (UEs) and perform beamforming
while mitigating the effect of pilot contamination. For example, in a long term
evolution (LTE) channel environment having delay spread $T_d=4.69\mu$ second
and channel bandwidth $B=2.5$MHz, we have found that $L=18$ cells can use this
band. In practice, $T_d$ is constant for a particular environment and carrier
frequency, and hence $L$ increases as the bandwidth increases. The proposed
channel estimation and beamforming design is linear, simple to implement and
significantly outperforms the existing designs, and is validated by extensive
simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05096</identifier>
 <datestamp>2015-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05096</id><created>2015-09-16</created><authors><author><keyname>Maity</keyname><forenames>Suman Kalyan</forenames></author><author><keyname>Ghuku</keyname><forenames>Bhadreswar</forenames></author><author><keyname>Upmanyu</keyname><forenames>Abhishek</forenames></author><author><keyname>Mukherjee</keyname><forenames>Animesh</forenames></author></authors><title>Out of vocabulary words decrease, running texts prevail and hashtags
  coalesce: Twitter as an evolving sociolinguistic system</title><categories>cs.SI</categories><comments>10 pages, 13 figures, HICSS-49</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Twitter is one of the most popular social media. Due to the ease of
availability of data, Twitter is used significantly for research purposes.
Twitter is known to evolve in many aspects from what it was at its birth;
nevertheless, how it evolved its own linguistic style is still relatively
unknown. In this paper, we study the evolution of various sociolinguistic
aspects of Twitter over large time scales. To the best of our knowledge, this
is the first comprehensive study on the evolution of such aspects of this OSN.
We performed quantitative analysis both on the word level as well as on the
hashtags since it is perhaps one of the most important linguistic units of this
social media. We studied the (in)formality aspects of the linguistic styles in
Twitter and find that it is neither fully formal nor completely informal; while
on one hand, we observe that Out-Of-Vocabulary words are decreasing over time
(pointing to a formal style), on the other hand it is quite evident that
whitespace usage is getting reduced with a huge prevalence of running texts
(pointing to an informal style). We also analyze and propose quantitative
reasons for repetition and coalescing of hashtags in Twitter. We believe that
such phenomena may be strongly tied to different evolutionary aspects of human
languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05100</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05100</id><created>2015-09-16</created><updated>2015-09-24</updated><authors><author><keyname>Collard</keyname><forenames>Joseph</forenames></author><author><keyname>Gupta</keyname><forenames>Nimish</forenames></author><author><keyname>Shambaugh</keyname><forenames>Rian</forenames></author><author><keyname>Weiss</keyname><forenames>Aaron</forenames></author><author><keyname>Guha</keyname><forenames>Arjun</forenames></author></authors><title>On Static Verification of Puppet System Configurations</title><categories>cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Puppet is a configuration management system used by hundreds of organizations
to manage thousands of machines. It is designed to automate tasks such as
application configuration, service orchestration, VM provisioning, and more.
The heart of Puppet is a declarative domain-specific language that specifies a
collection of resources (packages, user accounts, files, etc.), their desired
state (e.g., installed or not installed), and the dependencies between them.
Although Puppet performs some static checking, there are many opportunities for
errors to occur. Puppet configurations are often underconstrained (to allow
them to be composed with each other) and underspecified (to allow them to be
applied to a variety of machine states), which makes errors difficult to detect
and debug. Even if a configuration is bug-free, when a machine is updated to a
new configuration, it is easy for the machine state and its configuration to
&quot;drift&quot; apart.
  In this paper, we identify determinism as the essential property that allows
us to reason about configuration correctness and configuration updates. First,
we present a sound, complete, and scalable algorithm to verify that
configurations are deterministic. Our approach is to encode configurations as
logical formulas in an SMT solver and apply partial order reduction and program
slicing to achieve scalability. We apply our tool to real-world Puppet
configurations gleaned from open-source projects that suffered determinacy
bugs. Second, we consider the configuration update problem: a live update from
version 1 to version 2 does not have the same effect as applying version 2 to a
new machine. By treating configurations as functions---which we can do after we
verify that they are deterministic---we build a simple program synthesis tool
to calculate an update to version 1 that has the same effect that version 2
would have had on the original machine.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05108</identifier>
 <datestamp>2015-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05108</id><created>2015-09-16</created><authors><author><keyname>Rossi</keyname><forenames>Paulo V.</forenames></author><author><keyname>Kabashima</keyname><forenames>Yoshiyuki</forenames></author><author><keyname>Inoue</keyname><forenames>Jun-ichi</forenames></author></authors><title>Online compressed sensing</title><categories>cs.IT cond-mat.dis-nn math.IT</categories><comments>5 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we explore the possibilities and limitations of recovering
sparse signals in an online fashion. Employing a mean field approximation to
the Bayes recursion formula yields an online signal recovery algorithm that can
be performed with a computational cost that is linearly proportional to the
signal length per update. Analysis of the resulting algorithm indicates that
the online algorithm asymptotically saturates the optimal performance limit
achieved by the offline method in the presence of Gaussian measurement noise,
while differences in the allowable computational costs may result in
fundamental gaps of the achievable performance in the absence of noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05112</identifier>
 <datestamp>2015-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05112</id><created>2015-09-16</created><authors><author><keyname>Pajaziti</keyname><forenames>Arianit</forenames></author></authors><title>Models and Representations for Fractal Social Organizations</title><categories>cs.MA</categories><comments>Work conducted as part of the Master thesis at
  https://www.uantwerpen.be/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our subject is oriented towards investigation of potential ways of societal
organization, that allow for collective intelligent organization and management
of resources. The main objective of such organizations is the exploration of
the social energy from the existing societies. We conjecture that an
organizational model that fulfills the mentioned requirements is the Fractal
Social Organization (FSO). Our goal is to prove and verify the effectiveness of
this model by performing various simulations using the NetLogo environment, a
tool that allows agent-based rapid prototyping. We begin by simulating trivial
real life activities that demonstrate the main properties of the core unit of
the FSO, namely the SoC. Further, more complex scenarios involving various
nested SoCs are simulated. Two main simulation models are presented, allowing
us to obtain preliminary results using the FSO concepts as a potential
solution. In the first simulation model we demonstrate that by the use of FSO
properties the individuals may benefit by receiving more qualitative healthcare
services, while in the second simulation model we show how it might be possible
to improve the fall detection systems by the use of FSO mechanism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05113</identifier>
 <datestamp>2015-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05113</id><created>2015-09-16</created><authors><author><keyname>Kallus</keyname><forenames>Nathan</forenames></author><author><keyname>Udell</keyname><forenames>Madeleine</forenames></author></authors><title>Learning Preferences from Assortment Choices in a Heterogeneous
  Population</title><categories>stat.ML cs.LG math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of learning the preferences of a heterogeneous
customer population by observing their choices from an assortment of products,
ads, or other offerings. Our observation model takes a form common in
assortment planning: each arriving customer chooses from an assortment of
offerings consisting of a subset of all possibilities. One-size-fits-all choice
modeling can fit heterogeneous populations quite poorly, and misses the
opportunity for assortment customization in online retail. On the other hand,
time, revenue, and inventory targets rule out exploring the preferences of
every customer or segment. In this paper we propose a mixture choice model with
a natural underlying low-dimensional structure, and show how to estimate its
parameters. In our model, the preferences of each customer or segment follow a
separate parametric choice model, but the underlying structure of these
parameters over all the models has low dimension. We show that a nuclear-norm
regularized maximum likelihood estimator can learn the preferences of all
customers using a number of observations much smaller than the number of
item-customer combinations. This result shows the potential for structural
assumptions to speed up learning and improve revenues in assortment planning
and customization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05117</identifier>
 <datestamp>2016-02-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05117</id><created>2015-09-16</created><authors><author><keyname>Yuan</keyname><forenames>Jing</forenames></author><author><keyname>Li</keyname><forenames>Lixiang</forenames></author><author><keyname>Peng</keyname><forenames>Haipeng</forenames></author><author><keyname>Kurths</keyname><forenames>J&#xfc;rgen</forenames></author><author><keyname>Hua</keyname><forenames>Xiaojing</forenames></author><author><keyname>Yang</keyname><forenames>Yixian</forenames></author></authors><title>The effect of randomness for dependency map on the robustness of
  interdependent lattices</title><categories>cs.SI physics.soc-ph</categories><doi>10.1063/1.4939984</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For interdependent networks with identity dependency map, percolation is
exactly the same with that on a single network and follows a second-order phase
transition, while for random dependency, percolation follows a first-order
phase transition. In real networks, the dependency relations between networks
are neither identical nor completely random. Thus in this paper, we study the
influence of randomness for dependency maps on the robustness of interdependent
lattice networks. We introduce approximate entropy($ApEn$) as the measure of
randomness of the dependency maps. We find that there is critical $ApEn_c$
below which the percolation is continuous, but for larger $ApEn$, it is a
first-order transition. With the increment of $ApEn$, the $p_c$ increases until
$ApEn$ reaching ${ApEn}_c'$ and then remains almost constant. The time scale of
the system shows rich properties as $ApEn$ increases. Our results uncover that
randomness is one of the important factors that lead to cascading failures of
spatially interdependent networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05121</identifier>
 <datestamp>2015-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05121</id><created>2015-09-17</created><authors><author><keyname>Cabreros</keyname><forenames>Irineo</forenames></author><author><keyname>Abbe</keyname><forenames>Emmanuel</forenames></author><author><keyname>Tsirigos</keyname><forenames>Aristotelis</forenames></author></authors><title>Detecting Community Structures in Hi-C Genomic Data</title><categories>q-bio.GN cs.SI stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Community detection (CD) algorithms are applied to Hi-C data to discover new
communities of loci in the 3D conformation of human and mouse DNA. We find that
CD has some distinct advantages over pre-existing methods: (1) it is capable of
finding a variable number of communities, (2) it can detect communities of DNA
loci either adjacent or distant in the 1D sequence, and (3) it allows us to
obtain a principled value of k, the number of communities present. Forcing k =
2, our method recovers earlier findings of Lieberman-Aiden, et al. (2009), but
letting k be a parameter, our method obtains as optimal value k = 6,
discovering new candidate communities. In addition to discovering large
communities that partition entire chromosomes, we also show that CD can detect
small-scale topologically associating domains (TADs) such as those found in
Dixon, et al. (2012). CD thus provides a natural and flexible statistical
framework for understanding the folding structure of DNA at multiple scales in
Hi-C data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05137</identifier>
 <datestamp>2015-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05137</id><created>2015-09-17</created><authors><author><keyname>Liu</keyname><forenames>Juan</forenames></author><author><keyname>Chen</keyname><forenames>Wei</forenames></author><author><keyname>Letaief</keyname><forenames>Khaled B.</forenames></author></authors><title>Joint Channel and Queue Aware Scheduling for Wireless Links with
  Multiple Fading States</title><categories>cs.IT cs.PF math.IT</categories><comments>conference version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we address the delay optimal scheduling problem for wireless
transmission with fixed modulation over multi-state fading channels. We propose
a stochastic scheduling policy which schedules the source to transmit with
probability jointly based on the buffer and channel states, with an average
power constraint at the transmitter. Our objective is to minimize the average
queueing delay by choosing the optimal transmission probabilities. Using Markov
chain modeling, we formulate a power-constrained delay minimization problem,
and then transform it into a Linear Programming (LP) one. By analyzing its
property, we can derive the optimal threshold-based scheduling policy together
with the corresponding transmission probabilities. Our theoretical analysis is
corroborated by simulation results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05142</identifier>
 <datestamp>2015-09-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05142</id><created>2015-09-17</created><updated>2015-09-21</updated><authors><author><keyname>Das</keyname><forenames>Sourish</forenames></author><author><keyname>Roy</keyname><forenames>Sasanka</forenames></author><author><keyname>Sambasivan</keyname><forenames>Rajiv</forenames></author></authors><title>Fast Gaussian Process Regression for Big Data</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gaussian Processes are widely used for regression tasks. A known limitation
in the application of Gaussian Processes to regression tasks is that the
computation of the solution requires performing a matrix inversion. The
solution also requires the storage of a large matrix in memory. These factors
restrict the application of Gaussian Process regression to small and moderate
size data sets. We present an algorithm based on empirically determined subset
selection that works well on both real world and synthetic datasets. On the
synthetic and real world datasets used in this study, the algorithm
demonstrated sub-linear time and space complexity. The correctness proof for
the algorithm is also presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05144</identifier>
 <datestamp>2015-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05144</id><created>2015-09-17</created><authors><author><keyname>Jacobs</keyname><forenames>Swen</forenames></author><author><keyname>Tentrup</keyname><forenames>Leander</forenames></author><author><keyname>Zimmermann</keyname><forenames>Martin</forenames></author></authors><title>Distributed and Parametric Synthesis</title><categories>cs.LO cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the synthesis of distributed implementations for specifications
in Parametric Linear Temporal Logic (PLTL). PLTL extends LTL by temporal
operators equipped with parameters that bound their scope. For single process
synthesis it is well-established that such parametric extensions do not
increase worst-case complexities. For synchronous systems, we show that,
despite being more powerful, the distributed realizability problem for PLTL is
not harder than its LTL counterpart. The case of asynchronous systems requires
assumptions on the scheduler beyond fairness to ensure that bounds can be met
at all, i.e., even fair schedulers can delay processes arbitrary long and
thereby prevent the system from satisfying its PLTL specification. Thus, we
employ the concept of bounded fair scheduling, where every process is
guaranteed to be scheduled in bounded intervals and give a semi-decision
procedure for the resulting distributed assume-guarantee realizability problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05153</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05153</id><created>2015-09-17</created><updated>2015-09-17</updated><authors><author><keyname>Pan</keyname><forenames>Wei</forenames></author><author><keyname>Yuan</keyname><forenames>Ye</forenames></author><author><keyname>Ljung</keyname><forenames>Lennart</forenames></author><author><keyname>Goncalves</keyname><forenames>Jorge</forenames></author><author><keyname>Stan</keyname><forenames>Guy-Bart</forenames></author></authors><title>Identifying Biochemical Reaction Networks From Heterogeneous Datasets</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a new method to identify biochemical reaction
networks (i.e. both reactions and kinetic parameters) from heterogeneous
datasets. Such datasets can contain (a) data from several replicates of an
experiment performed on a biological system; (b) data measured from a
biochemical network subjected to different experimental conditions, for
example, changes/perturbations in biological inductions, temperature, gene
knock-out, gene over-expression, etc. Simultaneous integration of various
datasets to perform system identification has the potential to avoid
non-identifiability issues typically arising when only single datasets are
used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05160</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05160</id><created>2015-09-17</created><updated>2015-09-24</updated><authors><author><keyname>Verma</keyname><forenames>Amit Kumar</forenames></author><author><keyname>Pal</keyname><forenames>Manjish</forenames></author></authors><title>Evolving Social Networks via Friend Recommendations</title><categories>cs.SI</categories><comments>5 pages, 8 figures, 2 algorithms</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A social network grows over a period of time with the formation of new
connections and relations. In recent years we have witnessed a massive growth
of online social networks like Facebook, Twitter etc. So it has become a
problem of extreme importance to know the destiny of these networks. Thus
predicting the evolution of a social network is a question of extreme
importance. A good model for evolution of a social network can help in
understanding the properties responsible for the changes occurring in a network
structure. In this paper we propose such a model for evolution of social
networks. We model the social network as an undirected graph where nodes
represent people and edges represent the friendship between them. We define the
evolution process as a set of rules which resembles very closely to how a
social network grows in real life. We simulate the evolution process and show,
how starting from an initial network, a network evolves using this model. We
also discuss how our model can be used to model various complex social networks
other than online social networks like political networks, various
organizations etc..
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05172</identifier>
 <datestamp>2015-11-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05172</id><created>2015-09-17</created><updated>2015-11-27</updated><authors><author><keyname>Hallak</keyname><forenames>Assaf</forenames></author><author><keyname>Tamar</keyname><forenames>Aviv</forenames></author><author><keyname>Munos</keyname><forenames>Remi</forenames></author><author><keyname>Mannor</keyname><forenames>Shie</forenames></author></authors><title>Generalized Emphatic Temporal Difference Learning: Bias-Variance
  Analysis</title><categories>stat.ML cs.LG</categories><comments>arXiv admin note: text overlap with arXiv:1508.03411</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the off-policy evaluation problem in Markov decision processes
with function approximation. We propose a generalization of the recently
introduced \emph{emphatic temporal differences} (ETD) algorithm
\citep{SuttonMW15}, which encompasses the original ETD($\lambda$), as well as
several other off-policy evaluation algorithms as special cases. We call this
framework \ETD, where our introduced parameter $\beta$ controls the decay rate
of an importance-sampling term. We study conditions under which the projected
fixed-point equation underlying \ETD\ involves a contraction operator, allowing
us to present the first asymptotic error bounds (bias) for \ETD. Our results
show that the original ETD algorithm always involves a contraction operator,
and its bias is bounded. Moreover, by controlling $\beta$, our proposed
generalization allows trading-off bias for variance reduction, thereby
achieving a lower total error.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05173</identifier>
 <datestamp>2015-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05173</id><created>2015-09-17</created><authors><author><keyname>Simpson</keyname><forenames>Andrew J. R.</forenames></author></authors><title>Taming the ReLU with Parallel Dither in a Deep Neural Network</title><categories>cs.LG</categories><msc-class>68Txx</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rectified Linear Units (ReLU) seem to have displaced traditional 'smooth'
nonlinearities as activation-function-du-jour in many - but not all - deep
neural network (DNN) applications. However, nobody seems to know why. In this
article, we argue that ReLU are useful because they are ideal demodulators -
this helps them perform fast abstract learning. However, this fast learning
comes at the expense of serious nonlinear distortion products - decoy features.
We show that Parallel Dither acts to suppress the decoy features, preventing
overfitting and leaving the true features cleanly demodulated for rapid,
reliable learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05176</identifier>
 <datestamp>2015-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05176</id><created>2015-09-17</created><authors><author><keyname>Thakar</keyname><forenames>Pooja</forenames></author></authors><title>Performance Analysis and Prediction in Educational Data Mining: A
  Research Travelogue</title><categories>cs.CY</categories><comments>9 pages</comments><journal-ref>International Journal of Computer Applications 110(15):60-68,
  January 2015</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this era of computerization, education has also revamped itself and is not
limited to old lecture method. The regular quest is on to find out new ways to
make it more effective and efficient for students. Nowadays, lots of data is
collected in educational databases, but it remains unutilized. In order to get
required benefits from such a big data, powerful tools are required. Data
mining is an emerging powerful tool for analysis and prediction. It is
successfully applied in the area of fraud detection, advertising, marketing,
loan assessment and prediction. But, it is in nascent stage in the field of
education. Considerable amount of work is done in this direction, but still
there are many untouched areas. Moreover, there is no unified approach among
these researches. This paper presents a comprehensive survey, a travelogue
(2002-2014) towards educational data mining and its scope in future.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05177</identifier>
 <datestamp>2016-01-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05177</id><created>2015-09-17</created><updated>2015-10-15</updated><authors><author><keyname>Eswaran</keyname><forenames>K.</forenames></author><author><keyname>Singh</keyname><forenames>Vishwajeet</forenames></author></authors><title>Some Theorems for Feed Forward Neural Networks</title><categories>cs.NE</categories><comments>15 pages 13 figures</comments><msc-class>62M45</msc-class><doi>10.5120/ijca2015907021</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce a new method which employs the concept of
&quot;Orientation Vectors&quot; to train a feed forward neural network and suitable for
problems where large dimensions are involved and the clusters are
characteristically sparse. The new method is not NP hard as the problem size
increases. We `derive' the method by starting from Kolmogrov's method and then
relax some of the stringent conditions. We show for most classification
problems three layers are sufficient and the network size depends on the number
of clusters. We prove as the number of clusters increase from N to N+dN the
number of processing elements in the first layer only increases by d(logN), and
are proportional to the number of classes, and the method is not NP hard.
  Many examples are solved to demonstrate that the method of Orientation
Vectors requires much less computational effort than Radial Basis Function
methods and other techniques wherein distance computations are required, in
fact the present method increases logarithmically with problem size compared to
the Radial Basis Function method and the other methods which depend on distance
computations e.g statistical methods where probabilistic distances are
calculated. A practical method of applying the concept of Occum's razor to
choose between two architectures which solve the same classification problem
has been described. The ramifications of the above findings on the field of
Deep Learning have also been briefly investigated and we have found that it
directly leads to the existence of certain types of NN architectures which can
be used as a &quot;mapping engine&quot;, which has the property of &quot;invertibility&quot;, thus
improving the prospect of their deployment for solving problems involving Deep
Learning and hierarchical classification. The latter possibility has a lot of
future scope in the areas of machine learning and cloud computing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05181</identifier>
 <datestamp>2015-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05181</id><created>2015-09-17</created><authors><author><keyname>Zhao</keyname><forenames>Dengji</forenames></author><author><keyname>Ramchurn</keyname><forenames>Sarvapali D.</forenames></author><author><keyname>Jennings</keyname><forenames>Nicholas R.</forenames></author></authors><title>Efficient Task Collaboration with Execution Uncertainty</title><categories>cs.AI cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a general task allocation problem, involving multiple agents that
collaboratively accomplish tasks and where agents may fail to successfully
complete the tasks assigned to them (known as execution uncertainty). The goal
is to choose an allocation that maximises social welfare while taking their
execution uncertainty into account. We show that this can be achieved by using
the post-execution verification (PEV)-based mechanism if and only if agents'
valuations satisfy a multilinearity condition. We then consider a more complex
setting where an agent's execution uncertainty is not completely predictable by
the agent alone but aggregated from all agents' private opinions (known as
trust). We show that PEV-based mechanism with trust is still truthfully
implementable if and only if the trust aggregation is multilinear.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05186</identifier>
 <datestamp>2015-09-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05186</id><created>2015-09-17</created><updated>2015-09-18</updated><authors><author><keyname>Liu</keyname><forenames>Shicong</forenames></author><author><keyname>Shao</keyname><forenames>Junru</forenames></author><author><keyname>Lu</keyname><forenames>Hongtao</forenames></author></authors><title>Accelerated Distance Computation with Encoding Tree for High Dimensional
  Data</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel distance to calculate distance between high dimensional
vector pairs, utilizing vector quantization generated encodings. Vector
quantization based methods are successful in handling large scale high
dimensional data. These methods compress vectors into short encodings, and
allow efficient distance computation between an uncompressed vector and
compressed dataset without decompressing explicitly. However for large
datasets, these distance computing methods perform excessive computations. We
avoid excessive computations by storing the encodings on an Encoding
Tree(E-Tree), interestingly the memory consumption is also lowered. We also
propose Encoding Forest(E-Forest) to further lower the computation cost. E-Tree
and E-Forest is compatible with various existing quantization-based methods. We
show by experiments our methods speed-up distance computing for high
dimensional data drastically, and various existing algorithms can benefit from
our methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05192</identifier>
 <datestamp>2015-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05192</id><created>2015-09-17</created><authors><author><keyname>Wang</keyname><forenames>Bo</forenames></author><author><keyname>Li</keyname><forenames>Kezhi</forenames></author><author><keyname>Chen</keyname><forenames>Zhongjian</forenames></author><author><keyname>Wang</keyname><forenames>Xinan</forenames></author></authors><title>A balanced rail-to-rail all digital comparator using only standard cells</title><categories>cs.OH</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An all-digital comparator with full input range is presented. It outperforms
the nowaday all-digital comparators with its large rail-to-rail input range.
This is achieved by the proposed Yin-yang balance mechanism between the two
logic gates: NAND3 and OAI (Or-And-Invert). The important design considerations
to achieve this balance are presented, such as the driving strength
manipulation and the use of pre-distortion technique. Constructed only by
commercially available digital standard cells, the layout of the proposed
comparator is generated automatically by standard digital Place &amp; Route routine
within several minutes. The Verilog code for the proposed circuit is given, and
the circuit is successfully implemented in 130nm CMOS technology with the power
consumption of 0.176mW at the clock of 330MHz.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05194</identifier>
 <datestamp>2015-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05194</id><created>2015-09-17</created><authors><author><keyname>Liu</keyname><forenames>Shicong</forenames></author><author><keyname>Shao</keyname><forenames>Junru</forenames></author><author><keyname>Lu</keyname><forenames>Hongtao</forenames></author></authors><title>HCLAE: High Capacity Locally Aggregating Encodings for Approximate
  Nearest Neighbor Search</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vector quantization-based approaches are successful to solve Approximate
Nearest Neighbor (ANN) problems which are critical to many applications. The
idea is to generate effective encodings to allow fast distance approximation.
We propose quantization-based methods should partition the data space finely
and exhibit locality of the dataset to allow efficient non-exhaustive search.
In this paper, we introduce the concept of High Capacity Locality Aggregating
Encodings (HCLAE) to this end, and propose Dictionary Annealing (DA) to learn
HCLAE by a simulated annealing procedure. The quantization error is lower than
other state-of-the-art. The algorithms of DA can be easily extended to an
online learning scheme, allowing effective handle of large scale data. Further,
we propose Aggregating-Tree (A-Tree), a non-exhaustive search method using
HCLAE to perform efficient ANN-Search. A-Tree achieves magnitudes of speed-up
on ANN-Search tasks, compared to the state-of-the-art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05195</identifier>
 <datestamp>2015-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05195</id><created>2015-09-17</created><authors><author><keyname>Liu</keyname><forenames>Shicong</forenames></author><author><keyname>Lu</keyname><forenames>Hongtao</forenames></author><author><keyname>Shao</keyname><forenames>Junru</forenames></author></authors><title>Improved Residual Vector Quantization for High-dimensional Approximate
  Nearest Neighbor Search</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantization methods have been introduced to perform large scale approximate
nearest search tasks. Residual Vector Quantization (RVQ) is one of the
effective quantization methods. RVQ uses a multi-stage codebook learning scheme
to lower the quantization error stage by stage. However, there are two major
limitations for RVQ when applied to on high-dimensional approximate nearest
neighbor search: 1. The performance gain diminishes quickly with added stages.
2. Encoding a vector with RVQ is actually NP-hard. In this paper, we propose an
improved residual vector quantization (IRVQ) method, our IRVQ learns codebook
with a hybrid method of subspace clustering and warm-started k-means on each
stage to prevent performance gain from dropping, and uses a multi-path encoding
scheme to encode a vector with lower distortion. Experimental results on the
benchmark datasets show that our method gives substantially improves RVQ and
delivers better performance compared to the state-of-the-art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05196</identifier>
 <datestamp>2015-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05196</id><created>2015-09-17</created><authors><author><keyname>Simonetto</keyname><forenames>Andrea</forenames></author><author><keyname>Mokhtari</keyname><forenames>Aryan</forenames></author><author><keyname>Koppel</keyname><forenames>Alec</forenames></author><author><keyname>Leus</keyname><forenames>Geert</forenames></author><author><keyname>Ribeiro</keyname><forenames>Alejandro</forenames></author></authors><title>A Class of Prediction-Correction Methods for Time-Varying Convex
  Optimization</title><categories>cs.IT math.IT math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers unconstrained convex optimization problems with
time-varying objective functions. We propose algorithms with a discrete
time-sampling scheme to find and track the solution trajectory based on
prediction and correction steps, while sampling the problem data at a constant
rate of $1/h$, where $h$ is the length of the sampling interval. The prediction
step is derived by analyzing the iso-residual dynamics of the optimality
conditions. The correction step adjusts for the distance between the current
prediction and the optimizer at each time step, and consists either of one or
multiple gradient steps or Newton steps, which respectively correspond to the
gradient trajectory tracking (GTT) or Newton trajectory tracking (NTT)
algorithms. Under suitable conditions, we establish that the asymptotic error
incurred by both proposed methods behaves as $O(h^2)$, and in some cases as
$O(h^4)$, which outperforms the state-of-the-art error bound of $O(h)$ for
correction-only methods in the gradient-correction step. Moreover, when the
characteristics of the objective function variation are not available, we
propose approximate gradient and Newton tracking algorithms (AGT and ANT,
respectively) that still attain these asymptotical error bounds. Numerical
simulations demonstrate the practical utility of the proposed methods and that
they improve upon existing techniques by several orders of magnitude.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05197</identifier>
 <datestamp>2016-03-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05197</id><created>2015-09-17</created><updated>2016-03-06</updated><authors><author><keyname>Qu</keyname><forenames>Chenhao</forenames></author><author><keyname>Calheiros</keyname><forenames>Rodrigo N.</forenames></author><author><keyname>Buyya</keyname><forenames>Rajkumar</forenames></author></authors><title>A Reliable and Cost-Efficient Auto-Scaling System for Web Applications
  Using Heterogeneous Spot Instances</title><categories>cs.DC</categories><doi>10.1016/j.jnca.2016.03.001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud providers sell their idle capacity on markets through an auction-like
mechanism to increase their return on investment. The instances sold in this
way are called spot instances. In spite that spot instances are usually 90%
cheaper than on-demand instances, they can be terminated by provider when their
bidding prices are lower than market prices. Thus, they are largely used to
provision fault-tolerant applications only. In this paper, we explore how to
utilize spot instances to provision web applications, which are usually
considered availability-critical. The idea is to take advantage of differences
in price among various types of spot instances to reach both high availability
and significant cost saving. We first propose a fault-tolerant model for web
applications provisioned by spot instances. Based on that, we devise novel
auto-scaling polices for hourly billed cloud markets. We implemented the
proposed model and policies both on a simulation testbed for repeatable
validation and Amazon EC2. The experiments on the simulation testbed and the
real platform against the benchmarks show that the proposed approach can
greatly reduce resource cost and still achieve satisfactory Quality of Service
(QoS) in terms of response time and availability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05208</identifier>
 <datestamp>2015-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05208</id><created>2015-09-17</created><authors><author><keyname>Lemeshevsky</keyname><forenames>Sergey V.</forenames></author><author><keyname>Naumovich</keyname><forenames>Semion A.</forenames></author><author><keyname>Naumovich</keyname><forenames>Sergey S.</forenames></author><author><keyname>Vabishchevich</keyname><forenames>Petr N.</forenames></author><author><keyname>Zakharov</keyname><forenames>Petr E.</forenames></author></authors><title>Numerical simulation of the stress-strain state of the dental system</title><categories>cs.CE</categories><comments>19 pages, 9 figures</comments><msc-class>65N30, 65D18, 74S05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present mathematical models, computational algorithms and software, which
can be used for prediction of results of prosthetic treatment. More interest
issue is biomechanics of the periodontal complex because any prosthesis is
accompanied by a risk of overloading the supporting elements. Such risk can be
avoided by the proper load distribution and prediction of stresses that occur
during the use of dentures. We developed the mathematical model of the
periodontal complex and its software implementation. This model is based on
linear elasticity theory and allows to calculate the stress and strain fields
in periodontal ligament and jawbone. The input parameters for the developed
model can be divided into two groups. The first group of parameters describes
the mechanical properties of periodontal ligament, teeth and jawbone (for
example, elasticity of periodontal ligament etc.). The second group
characterized the geometric properties of objects: the size of the teeth, their
spatial coordinates, the size of periodontal ligament etc. The mechanical
properties are the same for almost all, but the input of geometrical data is
complicated because of their individual characteristics. In this connection, we
develop algorithms and software for processing of images obtained by computed
tomography (CT) scanner and for constructing individual digital model of the
tooth-periodontal ligament-jawbone system of the patient. Integration of models
and algorithms described allows to carry out biomechanical analysis on
three-dimensional digital model and to select prosthesis design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05209</identifier>
 <datestamp>2015-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05209</id><created>2015-09-17</created><authors><author><keyname>Trenta</keyname><forenames>Antonio</forenames></author><author><keyname>Hunter</keyname><forenames>Anthony</forenames></author><author><keyname>Riedel</keyname><forenames>Sebastian</forenames></author></authors><title>Extraction of evidence tables from abstracts of randomized clinical
  trials using a maximum entropy classifier and global constraints</title><categories>cs.CL cs.AI</categories><comments>27 pages, 10 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Systematic use of the published results of randomized clinical trials is
increasingly important in evidence-based medicine. In order to collate and
analyze the results from potentially numerous trials, evidence tables are used
to represent trials concerning a set of interventions of interest. An evidence
table has columns for the patient group, for each of the interventions being
compared, for the criterion for the comparison (e.g. proportion who survived
after 5 years from treatment), and for each of the results. Currently, it is a
labour-intensive activity to read each published paper and extract the
information for each field in an evidence table. There have been some NLP
studies investigating how some of the features from papers can be extracted, or
at least the relevant sentences identified. However, there is a lack of an NLP
system for the systematic extraction of each item of information required for
an evidence table. We address this need by a combination of a maximum entropy
classifier, and integer linear programming. We use the later to handle
constraints on what is an acceptable classification of the features to be
extracted. With experimental results, we demonstrate substantial advantages in
using global constraints (such as the features describing the patient group,
and the interventions, must occur before the features describing the results of
the comparison).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05238</identifier>
 <datestamp>2016-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05238</id><created>2015-09-17</created><updated>2016-01-29</updated><authors><author><keyname>H&#xe4;nsel</keyname><forenames>Katrin</forenames></author><author><keyname>Wilde</keyname><forenames>Natalie</forenames></author><author><keyname>Haddadi</keyname><forenames>Hamed</forenames></author><author><keyname>Alomainy</keyname><forenames>Akram</forenames></author></authors><title>Wearable Computing for Health and Fitness: Exploring the Relationship
  between Data and Human Behaviour</title><categories>cs.CY cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Health and fitness wearable technology has recently advanced, making it
easier for an individual to monitor their behaviours. Previously self generated
data interacts with the user to motivate positive behaviour change, but issues
arise when relating this to long term mention of wearable devices. Previous
studies within this area are discussed. We also consider a new approach where
data is used to support instead of motivate, through monitoring and logging to
encourage reflection. Based on issues highlighted, we then make recommendations
on the direction in which future work could be most beneficial.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05240</identifier>
 <datestamp>2015-09-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05240</id><created>2015-09-17</created><authors><author><keyname>Holub</keyname><forenames>&#x160;t&#x11b;p&#xe1;n</forenames></author><author><keyname>Shallit</keyname><forenames>Jeffrey</forenames></author></authors><title>Periods and borders of random words</title><categories>cs.FL math.CO</categories><msc-class>68R15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the behavior of the periods and border lengths of random words
over a fixed alphabet. We show that the asymptotic probability that a random
word has a given maximal border length $k$ is a constant, depending only on $k$
and the alphabet size $\ell$. We give a recurrence that allows us to determine
these constants with any required precision. This also allows us to evaluate
the expected period of a random word. For the binary case, the expected period
is asymptotically about $n-1.641$. We also give explicit formulas for the
probability that a random word is unbordered or has maximum border length one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05243</identifier>
 <datestamp>2016-02-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05243</id><created>2015-09-17</created><updated>2016-02-19</updated><authors><author><keyname>Nicolaides</keyname><forenames>Christos</forenames></author><author><keyname>Juanes</keyname><forenames>Ruben</forenames></author><author><keyname>Cueto-Felgueroso</keyname><forenames>Luis</forenames></author></authors><title>Self-organization of network dynamics into local quantized states</title><categories>physics.soc-ph cs.SI nlin.AO nlin.PS</categories><comments>11 pages, 4 figures</comments><journal-ref>Scientific Reports, 6, 21360 (2016)</journal-ref><doi>10.1038/srep21360</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Self-organization and pattern formation in network-organized systems emerges
from the collective activation and interaction of many interconnected units. A
striking feature of these non-equilibrium structures is that they are often
localized and robust: only a small subset of the nodes, or cell assembly, is
activated. Understanding the role of cell assemblies as basic functional units
in neural networks and socio-technical systems emerges as a fundamental
challenge in network theory. A key open question is how these elementary
building blocks emerge, and how they operate, linking structure and function in
complex networks. Here we show that a network analogue of the Swift-Hohenberg
continuum model---a minimal-ingredients model of nodal activation and
interaction within a complex network---is able to produce a complex suite of
localized patterns. Hence, the spontaneous formation of robust operational cell
assemblies in complex networks can be explained as the result of
self-organization, even in the absence of synaptic reinforcements. Our results
show that these self-organized, local structures can provide robust functional
units to understand natural and socio-technical network-organized processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05251</identifier>
 <datestamp>2015-12-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05251</id><created>2015-09-17</created><updated>2015-12-04</updated><authors><author><keyname>Delbracio</keyname><forenames>Mauricio</forenames></author><author><keyname>Sapiro</keyname><forenames>Guillermo</forenames></author></authors><title>Hand-held Video Deblurring via Efficient Fourier Aggregation</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Videos captured with hand-held cameras often suffer from a significant amount
of blur, mainly caused by the inevitable natural tremor of the photographer's
hand. In this work, we present an algorithm that removes blur due to camera
shake by combining information in the Fourier domain from nearby frames in a
video. The dynamic nature of typical videos with the presence of multiple
moving objects and occlusions makes this problem of camera shake removal
extremely challenging, in particular when low complexity is needed. Given an
input video frame, we first create a consistent registered version of
temporally adjacent frames. Then, the set of consistently registered frames is
block-wise fused in the Fourier domain with weights depending on the Fourier
spectrum magnitude. The method is motivated from the physiological fact that
camera shake blur has a random nature and therefore, nearby video frames are
generally blurred differently. Experiments with numerous videos recorded in the
wild, along with extensive comparisons, show that the proposed algorithm
achieves state-of-the-art results while at the same time being much faster than
its competitors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05254</identifier>
 <datestamp>2016-02-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05254</id><created>2015-09-17</created><updated>2016-02-02</updated><authors><author><keyname>Kuortti</keyname><forenames>Juha</forenames></author><author><keyname>Malinen</keyname><forenames>Jarmo</forenames></author></authors><title>Post-processing speech recordings during MRI</title><categories>cs.SD physics.bio-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss post-processing of speech that has been recorded during Magnetic
Resonance Imaging (MRI) of the vocal tract. Such speech recordings are
contaminated by high levels of acoustic noise from the MRI scanner. Also, the
frequency response of the sound signal path is not flat as a result of severe
restrictions on recording instrumentation due to MRI technology.
  The post-processing algorithm for noise reduction is based on adaptive
spectral filtering. The speech material consists of samples of prolonged vowel
productions that are used for validation of the post-processing algorithm. The
comparison data is recorded in anechoic chamber from the same test subject.
Formant analysis is carried out for the post-processed speech and the
comparison data. Artificially noise-contaminated vowel samples are used for
validation experiments to determine performance of the algorithm where using
true data would be difficult.
  The properties of recording instrumentation or the post-processing algorithm
do not explain the consistent frequency dependent discrepancy between formant
data from experiments during MRI and in anechoic chamber. It is shown that the
discrepancy is statistically significant, in particular, where it is largest at
1 kHz and 2 kHz. The reflecting surfaces of the MRI head and neck coil are
suspected to change the speech acoustics which results in &quot;external formants&quot;
at these frequencies. However, the role of test subject adaptation to noise and
constrained space acoustics during an MRI examination cannot be ruled out.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1509.05255</identifier>
 <datestamp>2015-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1509.05255</id><created>2015-09-17</created><updated>2015-10-22</updated><authors><author><keyname>Ng</keyname><forenames>S. -L.</forenames></author><author><keyname>Paterson</keyname><forenames>M. B.</forenames></author></authors><title>Disjoint difference families and their applications</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Difference sets and their generalisations to difference families arise from
the study of designs and many other applications. Here we give a brief survey
of some of these applications, noting in particular the diverse definitions of
difference families and the variations in priorities in constructions. We
propose a definition of disjoint difference families that encompasses these
variations and allows a comparison of the similarities and disparities. We then
focus on two constructions of disjoint difference families arising from
frequency hopping sequences and showed that they are in fact the same. We
conclude with a discussion of the notion of equivalence for frequency hopping
sequences and for disjoint difference families.
</abstract></arXiv>
</metadata>
</record>
<resumptionToken cursor="83000" completeListSize="102538">1122234|84001</resumptionToken>
</ListRecords>
</OAI-PMH>
