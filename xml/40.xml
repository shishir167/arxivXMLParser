<?xml version="1.0" encoding="UTF-8"?>
<OAI-PMH xmlns="http://www.openarchives.org/OAI/2.0/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.openarchives.org/OAI/2.0/ http://www.openarchives.org/OAI/2.0/OAI-PMH.xsd">
<responseDate>2016-03-09T01:07:16Z</responseDate>
<request verb="ListRecords" resumptionToken="1122234|39001">http://export.arxiv.org/oai2</request>
<ListRecords>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2439</identifier>
 <datestamp>2013-06-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2439</id><created>2012-12-11</created><authors><author><keyname>Winiarczyk</keyname><forenames>Ryszard</forenames></author><author><keyname>Gawron</keyname><forenames>Piotr</forenames></author><author><keyname>Miszczak</keyname><forenames>Jaros&#x142;aw Adam</forenames></author><author><keyname>Pawela</keyname><forenames>&#x141;ukasz</forenames></author><author><keyname>Pucha&#x142;a</keyname><forenames>Zbigniew</forenames></author></authors><title>Analysis of patent activity in the field of quantum information
  processing</title><categories>physics.soc-ph cs.ET quant-ph</categories><comments>15 pages, 4 figures, 6 tables</comments><journal-ref>Int. J. Quantum Inform. 11, 1350007 (2013)</journal-ref><doi>10.1142/S021974991350007X</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper provides an analysis of patent activity in the field of quantum
information processing. Data from the PatentScope database from the years
1993-2011 was used. In order to predict the future trends in the number of
filed patents time series models were used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2442</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2442</id><created>2012-10-19</created><authors><author><keyname>Boutilier</keyname><forenames>Craig</forenames></author><author><keyname>Zemel</keyname><forenames>Richard S.</forenames></author><author><keyname>Marlin</keyname><forenames>Benjamin</forenames></author></authors><title>Active Collaborative Filtering</title><categories>cs.IR cs.LG stat.ML</categories><comments>Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)</comments><proxy>auai</proxy><report-no>UAI-P-2003-PG-98-106</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Collaborative filtering (CF) allows the preferences of multiple users to be
pooled to make recommendations regarding unseen products. We consider in this
paper the problem of online and interactive CF: given the current ratings
associated with a user, what queries (new ratings) would most improve the
quality of the recommendations made? We cast this terms of expected value of
information (EVOI); but the online computational cost of computing optimal
queries is prohibitive. We show how offline prototyping and computation of
bounds on EVOI can be used to dramatically reduce the required online
computation. The framework we develop is general, but we focus on derivations
and empirical study in the specific case of the multiple-cause vector
quantization model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2443</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2443</id><created>2012-10-19</created><authors><author><keyname>Boutilier</keyname><forenames>Craig</forenames></author><author><keyname>Das</keyname><forenames>Rajarshi</forenames></author><author><keyname>Kephart</keyname><forenames>Jeffrey O.</forenames></author><author><keyname>Tesauro</keyname><forenames>Gerald</forenames></author><author><keyname>Walsh</keyname><forenames>William E.</forenames></author></authors><title>Cooperative Negotiation in Autonomic Systems using Incremental Utility
  Elicitation</title><categories>cs.GT</categories><comments>Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)</comments><proxy>auai</proxy><report-no>UAI-P-2003-PG-89-97</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Decentralized resource allocation is a key problem for large-scale autonomic
(or self-managing) computing systems. Motivated by a data center scenario, we
explore efficient techniques for resolving resource conflicts via cooperative
negotiation. Rather than computing in advance the functional dependence of each
element's utility upon the amount of resource it receives, which could be
prohibitively expensive, each element's utility is elicited incrementally. Such
incremental utility elicitation strategies require the evaluation of only a
small set of sampled utility function points, yet they find near-optimal
allocations with respect to a minimax regret criterion. We describe preliminary
computational experiments that illustrate the benefit of our approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2444</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2444</id><created>2012-10-19</created><authors><author><keyname>Booth</keyname><forenames>Richard</forenames></author><author><keyname>Richter</keyname><forenames>Eva</forenames></author></authors><title>On revising fuzzy belief bases</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)</comments><proxy>auai</proxy><report-no>UAI-P-2003-PG-81-88</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We look at the problem of revising fuzzy belief bases, i.e., belief base
revision in which both formulas in the base as well as revision-input formulas
can come attached with varying truth-degrees. Working within a very general
framework for fuzzy logic which is able to capture a variety of types of
inference under uncertainty, such as truth-functional fuzzy logics and certain
types of probabilistic inference, we show how the idea of rational change from
'crisp' base revision, as embodied by the idea of partial meet revision, can be
faithfully extended to revising fuzzy belief bases. We present and axiomatise
an operation of partial meet fuzzy revision and illustrate how the operation
works in several important special instances of the framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2445</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2445</id><created>2012-10-19</created><authors><author><keyname>Bolt</keyname><forenames>Janneke H.</forenames></author><author><keyname>Renooij</keyname><forenames>Silja</forenames></author><author><keyname>van der Gaag</keyname><forenames>Linda C.</forenames></author></authors><title>Upgrading Ambiguous Signs in QPNs</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)</comments><proxy>auai</proxy><report-no>UAI-P-2003-PG-73-80</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  WA qualitative probabilistic network models the probabilistic relationships
between its variables by means of signs. Non-monotonic influences have
associated an ambiguous sign. These ambiguous signs typically lead to
uninformative results upon inference. A non-monotonic influence can, however,
be associated with a, more informative, sign that indicates its effect in the
current state of the network. To capture this effect, we introduce the concept
of situational sign. Furthermore, if the network converts to a state in which
all variables that provoke the non-monotonicity have been observed, a
non-monotonic influence reduces to a monotonic influence. We study the
persistence and propagation of situational signs upon inference and give a
method to establish the sign of a reduced influence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2446</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2446</id><created>2012-10-19</created><authors><author><keyname>Bobbio</keyname><forenames>Andrea</forenames></author><author><keyname>Montani</keyname><forenames>Stefania</forenames></author><author><keyname>Portinale</keyname><forenames>Luigi</forenames></author></authors><title>Parametric Dependability Analysis through Probabilistic Horn Abduction</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)</comments><proxy>auai</proxy><report-no>UAI-P-2003-PG-65-72</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dependability modeling and evaluation is aimed at investigating that a system
performs its function correctly in time. A usual way to achieve a high
reliability, is to design redundant systems that contain several replicas of
the same subsystem or component. State space methods for dependability analysis
may suffer of the state space explosion problem in such a kind of situation.
Combinatorial models, on the other hand, require the simplified assumption of
statistical independence; however, in case of redundant systems, this does not
guarantee a reduced number of modeled elements. In order to provide a more
compact system representation, parametric system modeling has been investigated
in the literature, in such a way that a set of replicas of a given subsystem is
parameterized so that only one representative instance is explicitly included.
While modeling aspects can be suitably addressed by these approaches,
analytical tools working on parametric characterizations are often more
difficult to be defined and the standard approach is to 'unfold' the parametric
model, in order to exploit standard analysis algorithms working at the unfolded
'ground' level. Moreover, parameterized combinatorial methods still require the
statistical independence assumption. In the present paper we consider the
formalism of Parametric Fault Tree (PFT) and we show how it can be related to
Probabilistic Horn Abduction (PHA). Since PHA is a framework where both
modeling and analysis can be performed in a restricted first-order language, we
aim at showing that converting a PFT into a PHA knowledge base will allow an
approach to dependability analysis directly exploiting parametric
representation. We will show that classical qualitative and quantitative
dependability measures can be characterized within PHA. Furthermore, additional
modeling aspects (such as noisy gates and local dependencies) as well as
additional reliability measures (such as posterior probability analysis) can be
naturally addressed by this conversion. A simple example of a multi-processor
system with several replicated units is used to illustrate the approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2447</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2447</id><created>2012-10-19</created><authors><author><keyname>Bishop</keyname><forenames>Christopher M.</forenames></author><author><keyname>Svensen</keyname><forenames>Markus</forenames></author></authors><title>Bayesian Hierarchical Mixtures of Experts</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)</comments><proxy>auai</proxy><report-no>UAI-P-2003-PG-57-64</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Hierarchical Mixture of Experts (HME) is a well-known tree-based model
for regression and classification, based on soft probabilistic splits. In its
original formulation it was trained by maximum likelihood, and is therefore
prone to over-fitting. Furthermore the maximum likelihood framework offers no
natural metric for optimizing the complexity and structure of the tree.
Previous attempts to provide a Bayesian treatment of the HME model have relied
either on ad-hoc local Gaussian approximations or have dealt with related
models representing the joint distribution of both input and output variables.
In this paper we describe a fully Bayesian treatment of the HME model based on
variational inference. By combining local and global variational methods we
obtain a rigourous lower bound on the marginal probability of the data under
the model. This bound is optimized during the training phase, and its resulting
value can be used for model order selection. We present results using this
approach for a data set describing robot arm kinematics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2448</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2448</id><created>2012-10-19</created><authors><author><keyname>Bilmes</keyname><forenames>Jeff A.</forenames></author><author><keyname>Bartels</keyname><forenames>Chris</forenames></author></authors><title>On Triangulating Dynamic Graphical Models</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)</comments><proxy>auai</proxy><report-no>UAI-P-2003-PG-47-56</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces new methodology to triangulate dynamic Bayesian
networks (DBNs) and dynamic graphical models (DGMs). While most methods to
triangulate such networks use some form of constrained elimination scheme based
on properties of the underlying directed graph, we find it useful to view
triangulation and elimination using properties only of the resulting undirected
graph, obtained after the moralization step. We first briefly introduce the
Graphical model toolkit (GMTK) and its notion of dynamic graphical models, one
that slightly extends the standard notion of a DBN. We next introduce the
'boundary algorithm', a method to find the best boundary between partitions in
a dynamic model. We find that using this algorithm, the notions of forward- and
backward-interface become moot - namely, the size and fill-in of the best
forward- and backward- interface are identical. Moreover, we observe that
finding a good partition boundary allows for constrained elimination orders
(and therefore graph triangulations) that are not possible using standard
slice-by-slice constrained eliminations. More interestingly, with certain
boundaries it is possible to obtain constrained elimination schemes that lie
outside the space of possible triangulations using only unconstrained
elimination. Lastly, we report triangulation results on invented graphs,
standard DBNs from the literature, novel DBNs used in speech recognition
research systems, and also random graphs. Using a number of different
triangulation quality measures (max clique size, state-space, etc.), we find
that with our boundary algorithm the triangulation quality can dramatically
improve.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2449</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2449</id><created>2012-10-19</created><authors><author><keyname>Bidyuk</keyname><forenames>Bozhena</forenames></author><author><keyname>Dechter</keyname><forenames>Rina</forenames></author></authors><title>An Empirical Study of w-Cutset Sampling for Bayesian Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)</comments><proxy>auai</proxy><report-no>UAI-P-2003-PG-37-46</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper studies empirically the time-space trade-off between sampling and
inference in a sl cutset sampling algorithm. The algorithm samples over a
subset of nodes in a Bayesian network and applies exact inference over the
rest. Consequently, while the size of the sampling space decreases, requiring
less samples for convergence, the time for generating each single sample
increases. The w-cutset sampling selects a sampling set such that the
induced-width of the network when the sampling set is observed is bounded by w,
thus requiring inference whose complexity is exponential in w. In this paper,
we investigate performance of w-cutset sampling over a range of w values and
measure the accuracy of w-cutset sampling as a function of w. Our experiments
demonstrate that the cutset sampling idea is quite powerful showing that an
optimal balance between inference and sampling benefits substantially from
restricting the cutset size, even at the cost of more complex inference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2450</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2450</id><created>2012-10-19</created><authors><author><keyname>Benferhat</keyname><forenames>Salem</forenames></author><author><keyname>Lagrue</keyname><forenames>Sylvain</forenames></author><author><keyname>Papini</keyname><forenames>Odile</forenames></author></authors><title>A possibilistic handling of partially ordered information</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)</comments><proxy>auai</proxy><report-no>UAI-P-2003-PG-29-36</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a standard possibilistic logic, prioritized information are encoded by
means of weighted knowledge base. This paper proposes an extension of
possibilistic logic for dealing with partially ordered information. We Show
that all basic notions of standard possibilitic logic (sumbsumption, syntactic
and semantic inference, etc.) have natural couterparts when dealing with
partially ordered information. We also propose an algorithm which computes
possibilistic conclusions of a partial knowledge base of a partially ordered
knowlege base.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2451</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2451</id><created>2012-12-11</created><authors><author><keyname>Boudia</keyname><forenames>Omar Rafik Merad</forenames></author><author><keyname>Feham</keyname><forenames>Mohamed</forenames></author></authors><title>RSAED: Robust and Secure Aggregation of Encrypted Data in Wireless
  Sensor Networks</title><categories>cs.CR</categories><comments>15 pages</comments><doi>10.5121/ijnsa.2012.4601</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, secure in-network aggregation in wireless sensor networks becomes a
challenge issue, there is an extensive research on this area due to the large
number of applications where the sensors are deployed and the security needs.
In the last few years, aggregation of encrypted data has been proposed in order
to maintain secrecy between the sensors and the sink, so the end-to-end data
confidentiality is provided. However, the data integrity was not addressed. In
this paper, we propose RSAED that allows integrity verification at intermediate
nodes, ensures the base station to receive ciphertexts which come only from
legitimate nodes and also improves the efficiency. Through implementation
results, we evaluate our scheme using computation and communication overhead.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2452</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2452</id><created>2012-10-19</created><authors><author><keyname>Bacchus</keyname><forenames>Fahiem</forenames></author><author><keyname>Dalmao</keyname><forenames>Shannon</forenames></author><author><keyname>Pitassi</keyname><forenames>Toniann</forenames></author></authors><title>Value Elimination: Bayesian Inference via Backtracking Search</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)</comments><proxy>auai</proxy><report-no>UAI-P-2003-PG-20-28</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Backtracking search is a powerful algorithmic paradigm that can be used to
solve many problems. It is in a certain sense the dual of variable elimination;
but on many problems, e.g., SAT, it is vastly superior to variable elimination
in practice. Motivated by this we investigate the application of backtracking
search to the problem of Bayesian inference (Bayes). We show that natural
generalizations of known techniques allow backtracking search to achieve
performance guarantees similar to standard algorithms for Bayes, and that there
exist problems on which backtracking can in fact do much better. We also
demonstrate that these ideas can be applied to implement a Bayesian inference
engine whose performance is competitive with standard algorithms. Since
backtracking search can very naturally take advantage of context specific
structure, the potential exists for performance superior to standard algorithms
on many problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2453</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2453</id><created>2012-10-19</created><authors><author><keyname>Azari</keyname><forenames>David</forenames></author><author><keyname>Horvitz</keyname><forenames>Eric J.</forenames></author><author><keyname>Dumais</keyname><forenames>Susan</forenames></author><author><keyname>Brill</keyname><forenames>Eric</forenames></author></authors><title>Web-Based Question Answering: A Decision-Making Perspective</title><categories>cs.IR cs.CL</categories><comments>Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)</comments><proxy>auai</proxy><report-no>UAI-P-2003-PG-11-19</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe an investigation of the use of probabilistic models and
cost-benefit analyses to guide resource-intensive procedures used by a
Web-based question answering system. We first provide an overview of research
on question-answering systems. Then, we present details on AskMSR, a prototype
web-based question answering system. We discuss Bayesian analyses of the
quality of answers generated by the system and show how we can endow the system
with the ability to make decisions about the number of queries issued to a
search engine, given the cost of queries and the expected value of query
results in refining an ultimate answer. Finally, we review the results of a set
of experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2455</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2455</id><created>2012-10-19</created><authors><author><keyname>Allen</keyname><forenames>David</forenames></author><author><keyname>Darwiche</keyname><forenames>Adnan</forenames></author></authors><title>New Advances in Inference by Recursive Conditioning</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)</comments><proxy>auai</proxy><report-no>UAI-P-2003-PG-2-10</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recursive Conditioning (RC) was introduced recently as the first any-space
algorithm for inference in Bayesian networks which can trade time for space by
varying the size of its cache at the increment needed to store a floating point
number. Under full caching, RC has an asymptotic time and space complexity
which is comparable to mainstream algorithms based on variable elimination and
clustering (exponential in the network treewidth and linear in its size). We
show two main results about RC in this paper. First, we show that its actual
space requirements under full caching are much more modest than those needed by
mainstream methods and study the implications of this finding. Second, we show
that RC can effectively deal with determinism in Bayesian networks by employing
standard logical techniques, such as unit resolution, allowing a significant
reduction in its time requirements in certain cases. We illustrate our results
using a number of benchmark networks, including the very challenging ones that
arise in genetic linkage analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2456</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2456</id><created>2012-10-19</created><authors><author><keyname>Flores</keyname><forenames>Julia M.</forenames></author><author><keyname>Gamez</keyname><forenames>Jose A.</forenames></author><author><keyname>Olesen</keyname><forenames>Kristian G.</forenames></author></authors><title>Incremental Compilation of Bayesian networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)</comments><proxy>auai</proxy><report-no>UAI-P-2003-PG-233-240</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most methods of exact probability propagation in Bayesian networks do not
carry out the inference directly over the network, but over a secondary
structure known as a junction tree or a join tree (JT). The process of
obtaining a JT is usually termed {sl compilation}. As compilation is usually
viewed as a whole process; each time the network is modified, a new compilation
process has to be carried out. The possibility of reusing an already existing
JT, in order to obtain the new one regarding only the modifications in the
network has received only little attention in the literature. In this paper we
present a method for incremental compilation of a Bayesian network, following
the classical scheme in which triangulation plays the key role. In order to
perform incremental compilation we propose to recompile only those parts of the
JT which can have been affected by the networks modifications. To do so, we
exploit the technique OF maximal prime subgraph decomposition in determining
the minimal subgraph(s) that have to be recompiled, and thereby the minimal
subtree(s) of the JT that should be replaced by new subtree(s).We focus on
structural modifications : addition and deletion of links and variables.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2457</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2457</id><created>2012-10-19</created><authors><author><keyname>Finzi</keyname><forenames>Alberto</forenames></author><author><keyname>Lukasiewicz</keyname><forenames>Thomas</forenames></author></authors><title>Structure-Based Causes and Explanations in the Independent Choice Logic</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)</comments><proxy>auai</proxy><report-no>UAI-P-2003-PG-225-232</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is directed towards combining Pearl's structural-model approach to
causal reasoning with high-level formalisms for reasoning about actions. More
precisely, we present a combination of Pearl's structural-model approach with
Poole's independent choice logic. We show how probabilistic theories in the
independent choice logic can be mapped to probabilistic causal models. This
mapping provides the independent choice logic with appealing concepts of
causality and explanation from the structural-model approach. We illustrate
this along Halpern and Pearl's sophisticated notions of actual cause,
explanation, and partial explanation. This mapping also adds first-order
modeling capabilities and explicit actions to the structural-model approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2458</identifier>
 <datestamp>2012-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2458</id><created>2012-10-19</created><authors><author><keyname>da Rocha</keyname><forenames>Jose Carlos Ferreira</forenames></author><author><keyname>Cozman</keyname><forenames>Fabio Gagliardi</forenames></author><author><keyname>de Campos</keyname><forenames>Cassio Polpo</forenames></author></authors><title>Inference in Polytrees with Sets of Probabilities</title><categories>cs.AI stat.CO</categories><comments>Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)</comments><proxy>auai</proxy><report-no>UAI-P-2003-PG-217-224</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inferences in directed acyclic graphs associated with probability sets and
probability intervals are NP-hard, even for polytrees. In this paper we focus
on such inferences, and propose: 1) a substantial improvement on Tessems A / R
algorithm FOR polytrees WITH probability intervals; 2) a new algorithm FOR
direction - based local search(IN sets OF probability) that improves ON
existing methods; 3) a collection OF branch - AND - bound algorithms that
combine the previous techniques.The first two techniques lead TO approximate
solutions, WHILE branch - AND - bound procedures can produce either exact OR
approximate solutions.We report ON dramatic improvements ON existing techniques
FOR inference WITH probability sets AND intervals, IN SOME cases reducing the
computational effort BY many orders OF magnitude.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2459</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2459</id><created>2012-10-19</created><authors><author><keyname>Feng</keyname><forenames>Zhengzhu</forenames></author><author><keyname>Hansen</keyname><forenames>Eric A.</forenames></author><author><keyname>Zilberstein</keyname><forenames>Shlomo</forenames></author></authors><title>Symbolic Generalization for On-line Planning</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)</comments><proxy>auai</proxy><report-no>UAI-P-2003-PG-209-216</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Symbolic representations have been used successfully in off-line planning
algorithms for Markov decision processes. We show that they can also improve
the performance of on-line planners. In addition to reducing computation time,
symbolic generalization can reduce the amount of costly real-world interactions
required for convergence. We introduce Symbolic Real-Time Dynamic Programming
(or sRTDP), an extension of RTDP. After each step of on-line interaction with
an environment, sRTDP uses symbolic model-checking techniques to generalizes
its experience by updating a group of states rather than a single state. We
examine two heuristic approaches to dynamic grouping of states and show that
they accelerate the planning process significantly in terms of both CPU time
and the number of steps of interaction with the environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2460</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2460</id><created>2012-10-19</created><authors><author><keyname>Elidan</keyname><forenames>Gal</forenames></author><author><keyname>Friedman</keyname><forenames>Nir</forenames></author></authors><title>The Information Bottleneck EM Algorithm</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)</comments><proxy>auai</proxy><report-no>UAI-P-2003-PG-200-208</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning with hidden variables is a central challenge in probabilistic
graphical models that has important implications for many real-life problems.
The classical approach is using the Expectation Maximization (EM) algorithm.
This algorithm, however, can get trapped in local maxima. In this paper we
explore a new approach that is based on the Information Bottleneck principle.
In this approach, we view the learning problem as a tradeoff between two
information theoretic objectives. The first is to make the hidden variables
uninformative about the identity of specific instances. The second is to make
the hidden variables informative about the observed attributes. By exploring
different tradeoffs between these two objectives, we can gradually converge on
a high-scoring solution. As we show, the resulting, Information Bottleneck
Expectation Maximization (IB-EM) algorithm, manages to find solutions that are
superior to standard EM methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2461</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2461</id><created>2012-10-19</created><authors><author><keyname>Eiter</keyname><forenames>Thomas</forenames></author><author><keyname>Lukasiewicz</keyname><forenames>Thomas</forenames></author></authors><title>Probabilistic Reasoning about Actions in Nonmonotonic Causal Theories</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)</comments><proxy>auai</proxy><report-no>UAI-P-2003-PG-192-199</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the language {m P}{cal C}+ for probabilistic reasoning about
actions, which is a generalization of the action language {cal C}+ that allows
to deal with probabilistic as well as nondeterministic effects of actions. We
define a formal semantics of {m P}{cal C}+ in terms of probabilistic
transitions between sets of states. Using a concept of a history and its belief
state, we then show how several important problems in reasoning about actions
can be concisely formulated in our formalism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2462</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2462</id><created>2012-10-19</created><authors><author><keyname>Drton</keyname><forenames>Mathias</forenames></author><author><keyname>Richardson</keyname><forenames>Thomas S.</forenames></author></authors><title>A New Algorithm for Maximum Likelihood Estimation in Gaussian Graphical
  Models for Marginal Independence</title><categories>stat.ME cs.LG stat.ML</categories><comments>Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)</comments><proxy>auai</proxy><report-no>UAI-P-2003-PG-184-191</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graphical models with bi-directed edges (&lt;-&gt;) represent marginal
independence: the absence of an edge between two vertices indicates that the
corresponding variables are marginally independent. In this paper, we consider
maximum likelihood estimation in the case of continuous variables with a
Gaussian joint distribution, sometimes termed a covariance graph model. We
present a new fitting algorithm which exploits standard regression techniques
and establish its convergence properties. Moreover, we contrast our procedure
to existing estimation methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2463</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2463</id><created>2012-10-19</created><authors><author><keyname>Dechter</keyname><forenames>Rina</forenames></author><author><keyname>Mateescu</keyname><forenames>Robert</forenames></author></authors><title>A Simple Insight into Iterative Belief Propagation's Success</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)</comments><proxy>auai</proxy><report-no>UAI-P-2003-PG-175-183</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Non - ergodic belief networks the posterior belief OF many queries given
evidence may become zero.The paper shows that WHEN belief propagation IS
applied iteratively OVER arbitrary networks(the so called, iterative OR loopy
belief propagation(IBP)) it IS identical TO an arc - consistency algorithm
relative TO zero - belief queries(namely assessing zero posterior
probabilities). This implies that zero - belief conclusions derived BY belief
propagation converge AND are sound.More importantly it suggests that the
inference power OF IBP IS AS strong AND AS weak, AS that OF arc -
consistency.This allows the synthesis OF belief networks FOR which belief
propagation IS useless ON one hand, AND focuses the investigation OF classes OF
belief network FOR which belief propagation may be zero - complete.Finally, ALL
the above conclusions apply also TO Generalized belief propagation algorithms
that extend loopy belief propagation AND allow a crisper understanding OF their
power.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2464</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2464</id><created>2012-10-19</created><authors><author><keyname>Dash</keyname><forenames>Denver</forenames></author><author><keyname>Druzdzel</keyname><forenames>Marek J.</forenames></author></authors><title>A Robust Independence Test for Constraint-Based Learning of Causal
  Structure</title><categories>cs.AI cs.LG stat.ML</categories><comments>Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)</comments><proxy>auai</proxy><report-no>UAI-P-2003-PG-167-174</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Constraint-based (CB) learning is a formalism for learning a causal network
with a database D by performing a series of conditional-independence tests to
infer structural information. This paper considers a new test of independence
that combines ideas from Bayesian learning, Bayesian network inference, and
classical hypothesis testing to produce a more reliable and robust test. The
new test can be calculated in the same asymptotic time and space required for
the standard tests such as the chi-squared test, but it allows the
specification of a prior distribution over parameters and can be used when the
database is incomplete. We prove that the test is correct, and we demonstrate
empirically that, when used with a CB causal discovery algorithm with
noninformative priors, it recovers structural features more reliably and it
produces networks with smaller KL-Divergence, especially as the number of nodes
increases or the number of records decreases. Another benefit is the dramatic
reduction in the probability that a CB algorithm will stall during the search,
providing a remedy for an annoying problem plaguing CB learning when the
database is small.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2465</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2465</id><created>2012-10-19</created><authors><author><keyname>Crick</keyname><forenames>Christopher</forenames></author><author><keyname>Pfeffer</keyname><forenames>Avi</forenames></author></authors><title>Loopy Belief Propagation as a Basis for Communication in Sensor Networks</title><categories>cs.AI cs.NI</categories><comments>Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)</comments><proxy>auai</proxy><report-no>UAI-P-2003-PG-159-166</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sensor networks are an exciting new kind of computer system. Consisting of a
large number of tiny, cheap computational devices physically distributed in an
environment, they gather and process data about the environment in real time.
One of the central questions in sensor networks is what to do with the data,
i.e., how to reason with it and how to communicate it. This paper argues that
the lessons of the UAI community, in particular that one should produce and
communicate beliefs rather than raw sensor values, are highly relevant to
sensor networks. We contend that loopy belief propagation is particularly well
suited to communicating beliefs in sensor networks, due to its compact
implementation and distributed nature. We investigate the ability of loopy
belief propagation to function under the stressful conditions likely to prevail
in sensor networks. Our experiments show that it performs well and degrades
gracefully. It converges to appropriate beliefs even in highly asynchronous
settings where some nodes communicate far less frequently than others; it
continues to function if some nodes fail to participate in the propagation
process; and it can track changes in the environment that occur while beliefs
are propagating. As a result, we believe that sensor networks present an
important application opportunity for UAI.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2466</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2466</id><created>2012-10-19</created><authors><author><keyname>Corduneanu</keyname><forenames>Adrian</forenames></author><author><keyname>Jaakkola</keyname><forenames>Tommi S.</forenames></author></authors><title>On Information Regularization</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)</comments><proxy>auai</proxy><report-no>UAI-P-2003-PG-151-158</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We formulate a principle for classification with the knowledge of the
marginal distribution over the data points (unlabeled data). The principle is
cast in terms of Tikhonov style regularization where the regularization penalty
articulates the way in which the marginal density should constrain otherwise
unrestricted conditional distributions. Specifically, the regularization
penalty penalizes any information introduced between the examples and labels
beyond what is provided by the available labeled examples. The work extends
Szummer and Jaakkola's information regularization (NIPS 2002) to multiple
dimensions, providing a regularizer independent of the covering of the space
used in the derivation. We show in addition how the information regularizer can
be used as a measure of complexity of the classification task with unlabeled
data and prove a relevant sample-complexity bound. We illustrate the
regularization principle in practice by restricting the class of conditional
distributions to be logistic regression models and constructing the
regularization penalty from a finite set of unlabeled examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2468</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2468</id><created>2012-10-19</created><authors><author><keyname>Chickering</keyname><forenames>David Maxwell</forenames></author><author><keyname>Meek</keyname><forenames>Christopher</forenames></author><author><keyname>Heckerman</keyname><forenames>David</forenames></author></authors><title>Large-Sample Learning of Bayesian Networks is NP-Hard</title><categories>cs.LG cs.AI stat.ML</categories><comments>Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)</comments><proxy>auai</proxy><report-no>UAI-P-2003-PG-124-133</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we provide new complexity results for algorithms that learn
discrete-variable Bayesian networks from data. Our results apply whenever the
learning algorithm uses a scoring criterion that favors the simplest model able
to represent the generative distribution exactly. Our results therefore hold
whenever the learning algorithm uses a consistent scoring criterion and is
applied to a sufficiently large dataset. We show that identifying high-scoring
structures is hard, even when we are given an independence oracle, an inference
oracle, and/or an information oracle. Our negative results also apply to the
learning of discrete-variable Bayesian networks in which each node has at most
k parents, for all k &gt; 3.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2469</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2469</id><created>2012-10-19</created><authors><author><keyname>Chaudhari</keyname><forenames>Sanjay</forenames></author><author><keyname>Richardson</keyname><forenames>Thomas S.</forenames></author></authors><title>Using the structure of d-connecting paths as a qualitative measure of
  the strength of dependence</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)</comments><proxy>auai</proxy><report-no>UAI-P-2003-PG-116-123</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Pearls concept OF a d - connecting path IS one OF the foundations OF the
modern theory OF graphical models : the absence OF a d - connecting path IN a
DAG indicates that conditional independence will hold IN ANY distribution
factorising according TO that graph. IN this paper we show that IN singly -
connected Gaussian DAGs it IS possible TO USE the form OF a d - connection TO
obtain qualitative information about the strength OF conditional
dependence.More precisely, the squared partial correlations BETWEEN two given
variables, conditioned ON different subsets may be partially ordered BY
examining the relationship BETWEEN the d - connecting path AND the SET OF
variables conditioned upon.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2470</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2470</id><created>2012-10-19</created><authors><author><keyname>Chan</keyname><forenames>Hei</forenames></author><author><keyname>Darwiche</keyname><forenames>Adnan</forenames></author></authors><title>Reasoning about Bayesian Network Classifiers</title><categories>cs.LG cs.AI stat.ML</categories><comments>Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)</comments><proxy>auai</proxy><report-no>UAI-P-2003-PG-107-115</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bayesian network classifiers are used in many fields, and one common class of
classifiers are naive Bayes classifiers. In this paper, we introduce an
approach for reasoning about Bayesian network classifiers in which we
explicitly convert them into Ordered Decision Diagrams (ODDs), which are then
used to reason about the properties of these classifiers. Specifically, we
present an algorithm for converting any naive Bayes classifier into an ODD, and
we show theoretically and experimentally that this algorithm can give us an ODD
that is tractable in size even given an intractable number of instances. Since
ODDs are tractable representations of classifiers, our algorithm allows us to
efficiently test the equivalence of two naive Bayes classifiers and
characterize discrepancies between them. We also show a number of additional
results including a count of distinct classifiers that can be induced by
changing some CPT in a naive Bayes classifier, and the range of allowable
changes to a CPT which keeps the current classifier unchanged.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2471</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2471</id><created>2012-10-19</created><authors><author><keyname>Lu</keyname><forenames>Fletcher</forenames></author><author><keyname>Schuurmans</keyname><forenames>Dale</forenames></author></authors><title>Monte Carlo Matrix Inversion Policy Evaluation</title><categories>cs.LG cs.AI cs.NA</categories><comments>Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)</comments><proxy>auai</proxy><report-no>UAI-P-2003-PG-386-393</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In 1950, Forsythe and Leibler (1950) introduced a statistical technique for
finding the inverse of a matrix by characterizing the elements of the matrix
inverse as expected values of a sequence of random walks. Barto and Duff (1994)
subsequently showed relations between this technique and standard dynamic
programming and temporal differencing methods. The advantage of the Monte Carlo
matrix inversion (MCMI) approach is that it scales better with respect to
state-space size than alternative techniques. In this paper, we introduce an
algorithm for performing reinforcement learning policy evaluation using MCMI.
We demonstrate that MCMI improves on runtime over a maximum likelihood
model-based policy evaluation approach and on both runtime and accuracy over
the temporal differencing (TD) policy evaluation approach. We further improve
on MCMI policy evaluation by adding an importance sampling technique to our
algorithm to reduce the variance of our estimator. Lastly, we illustrate
techniques for scaling up MCMI to large state spaces in order to perform policy
improvement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2472</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2472</id><created>2012-10-19</created><authors><author><keyname>Lizotte</keyname><forenames>Daniel J.</forenames></author><author><keyname>Madani</keyname><forenames>Omid</forenames></author><author><keyname>Greiner</keyname><forenames>Russell</forenames></author></authors><title>Budgeted Learning of Naive-Bayes Classifiers</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)</comments><proxy>auai</proxy><report-no>UAI-P-2003-PG-378-385</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Frequently, acquiring training data has an associated cost. We consider the
situation where the learner may purchase data during training, subject TO a
budget. IN particular, we examine the CASE WHERE each feature label has an
associated cost, AND the total cost OF ALL feature labels acquired during
training must NOT exceed the budget.This paper compares methods FOR choosing
which feature label TO purchase next, given the budget AND the CURRENT belief
state OF naive Bayes model parameters.Whereas active learning has traditionally
focused ON myopic(greedy) strategies FOR query selection, this paper presents a
tractable method FOR incorporating knowledge OF the budget INTO the decision
making process, which improves performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2473</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2473</id><created>2012-10-19</created><authors><author><keyname>Liu</keyname><forenames>Liping</forenames></author><author><keyname>Shenoy</keyname><forenames>Catherine</forenames></author><author><keyname>Shenoy</keyname><forenames>Prakash P.</forenames></author></authors><title>A Linear Belief Function Approach to Portfolio Evaluation</title><categories>cs.AI q-fin.ST</categories><comments>Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)</comments><proxy>auai</proxy><report-no>UAI-P-2003-PG-370-377</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  By elaborating on the notion of linear belief functions (Dempster 1990; Liu
1996), we propose an elementary approach to knowledge representation for expert
systems using linear belief functions. We show how to use basic matrices to
represent market information and financial knowledge, including complete
ignorance, statistical observations, subjective speculations, distributional
assumptions, linear relations, and empirical asset pricing models. We then
appeal to Dempster's rule of combination to integrate the knowledge for
assessing an overall belief of portfolio performance, and updating the belief
by incorporating additional information. We use an example of three gold stocks
to illustrate the approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2474</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2474</id><created>2012-10-19</created><authors><author><keyname>Lebanon</keyname><forenames>Guy</forenames></author></authors><title>Learning Riemannian Metrics</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)</comments><proxy>auai</proxy><report-no>UAI-P-2003-PG-362-369</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a solution to the problem of estimating a Riemannian metric
associated with a given differentiable manifold. The metric learning problem is
based on minimizing the relative volume of a given set of points. We derive the
details for a family of metrics on the multinomial simplex. The resulting
metric has applications in text classification and bears some similarity to
TFIDF representation of text documents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2475</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2475</id><created>2012-10-19</created><authors><author><keyname>Lawrence</keyname><forenames>Gregory</forenames></author><author><keyname>Cowan</keyname><forenames>Noah</forenames></author><author><keyname>Russell</keyname><forenames>Stuart</forenames></author></authors><title>Efficient Gradient Estimation for Motor Control Learning</title><categories>cs.LG cs.SY</categories><comments>Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)</comments><proxy>auai</proxy><report-no>UAI-P-2003-PG-354-361</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The task of estimating the gradient of a function in the presence of noise is
central to several forms of reinforcement learning, including policy search
methods. We present two techniques for reducing gradient estimation errors in
the presence of observable input noise applied to the control signal. The first
method extends the idea of a reinforcement baseline by fitting a local linear
model to the function whose gradient is being estimated; we show how to find
the linear model that minimizes the variance of the gradient estimate, and how
to estimate the model from data. The second method improves this further by
discounting components of the gradient vector that have high variance. These
methods are applied to the problem of motor control learning, where actuator
noise has a significant influence on behavior. In particular, we apply the
techniques to learn locally optimal controllers for a dart-throwing task using
a simulated three-link arm; we demonstrate that proposed methods significantly
improve the reward function gradient estimate and, consequently, the learning
curve, over existing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2476</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2476</id><created>2012-10-19</created><authors><author><keyname>Larkin</keyname><forenames>David Ephraim</forenames></author></authors><title>Approximate Decomposition: A Method for Bounding and Estimating
  Probabilistic and Deterministic Queries</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)</comments><proxy>auai</proxy><report-no>UAI-P-2003-PG-346-353</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce a method for approximating the solution to
inference and optimization tasks in uncertain and deterministic reasoning. Such
tasks are in general intractable for exact algorithms because of the large
number of dependency relationships in their structure. Our method effectively
maps such a dense problem to a sparser one which is in some sense &quot;closest&quot;.
Exact methods can be run on the sparser problem to derive bounds on the
original answer, which can be quite sharp. We present empirical results
demonstrating that our method works well on the tasks of belief inference and
finding the probability of the most probable explanation in belief networks,
and finding the cost of the solution that violates the smallest number of
constraints in constraint satisfaction problems. On one large CPCS network, for
example, we were able to calculate upper and lower bounds on the conditional
probability of a variable, given evidence, that were almost identical in the
average case.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2477</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2477</id><created>2012-10-19</created><authors><author><keyname>Shyong</keyname><affiliation>Tony</affiliation></author><author><keyname>Lam</keyname><forenames>K.</forenames></author><author><keyname>Pennock</keyname><forenames>David M</forenames></author><author><keyname>Cosley</keyname><forenames>Dan</forenames></author><author><keyname>Lawrence</keyname><forenames>Steve</forenames></author></authors><title>1 Billion Pages = 1 Million Dollars? Mining the Web to Play &quot;Who Wants
  to be a Millionaire?&quot;</title><categories>cs.IR cs.CL</categories><comments>Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)</comments><proxy>auai</proxy><report-no>UAI-P-2003-PG-337-345</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We exploit the redundancy and volume of information on the web to build a
computerized player for the ABC TV game show 'Who Wants To Be A Millionaire?'
The player consists of a question-answering module and a decision-making
module. The question-answering module utilizes question transformation
techniques, natural language parsing, multiple information retrieval
algorithms, and multiple search engines; results are combined in the spirit of
ensemble learning using an adaptive weighting scheme. Empirically, the system
correctly answers about 75% of questions from the Millionaire CD-ROM, 3rd
edition - general-interest trivia questions often about popular culture and
common knowledge. The decision-making module chooses from allowable actions in
the game in order to maximize expected risk-adjusted winnings, where the
estimated probability of answering correctly is a function of past performance
and confidence in in correctly answering the current question. When given a six
question head start (i.e., when starting from the $2,000 level), we find that
the system performs about as well on average as humans starting at the
beginning. Our system demonstrates the potential of simple but well-chosen
techniques for mining answers from unstructured information such as the web.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2478</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2478</id><created>2012-10-19</created><authors><author><keyname>Jin</keyname><forenames>Rong</forenames></author><author><keyname>Si</keyname><forenames>Luo</forenames></author><author><keyname>Zhai</keyname><forenames>ChengXiang</forenames></author></authors><title>Preference-based Graphic Models for Collaborative Filtering</title><categories>cs.IR</categories><comments>Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)</comments><proxy>auai</proxy><report-no>UAI-P-2003-PG-329-336</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Collaborative filtering is a very useful general technique for exploiting the
preference patterns of a group of users to predict the utility of items to a
particular user. Previous research has studied several probabilistic graphic
models for collaborative filtering with promising results. However, while these
models have succeeded in capturing the similarity among users and items in one
way or the other, none of them has considered the fact that users with similar
interests in items can have very different rating patterns; some users tend to
assign a higher rating to all items than other users. In this paper, we propose
and study of two new graphic models that address the distinction between user
preferences and ratings. In one model, called the decoupled model, we introduce
two different variables to decouple a users preferences FROM his ratings. IN
the other, called the preference model, we model the orderings OF items
preferred BY a USER, rather than the USERs numerical ratings of items.
Empirical study over two datasets of movie ratings shows that appropriate
modeling of the distinction between user preferences and ratings improves the
performance substantially and consistently. Specifically, the proposed
decoupled model outperforms all five existing approaches that we compare with
significantly, but the preference model is not very successful. These results
suggest that explicit modeling of the underlying user preferences is very
important for collaborative filtering, but we can not afford ignoring the
rating information completely.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2479</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2479</id><created>2012-10-19</created><authors><author><keyname>Hopkins</keyname><forenames>Mark</forenames></author></authors><title>LAYERWIDTH: Analysis of a New Metric for Directed Acyclic Graphs</title><categories>cs.DS cs.AI cs.DM</categories><comments>Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)</comments><proxy>auai</proxy><report-no>UAI-P-2003-PG-321-328</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze a new property of directed acyclic graphs (DAGs), called
layerwidth, arising from a class of DAGs proposed by Eiter and Lukasiewicz.
This class of DAGs permits certain problems of structural model-based causality
and explanation to be tractably solved. In this paper, we first address an open
question raised by Eiter and Lukasiewicz - the computational complexity of
deciding whether a given graph has a bounded layerwidth. After proving that
this problem is NP-complete, we proceed by proving numerous important
properties of layerwidth that are helpful in efficiently computing the optimal
layerwidth. Finally, we compare this new DAG property to two other important
DAG properties: treewidth and bandwidth.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2480</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2480</id><created>2012-10-19</created><authors><author><keyname>Heskes</keyname><forenames>Tom</forenames></author><author><keyname>Albers</keyname><forenames>Kees</forenames></author><author><keyname>Kappen</keyname><forenames>Hilbert</forenames></author></authors><title>Approximate Inference and Constrained Optimization</title><categories>cs.LG cs.AI stat.ML</categories><comments>Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)</comments><proxy>auai</proxy><report-no>UAI-P-2003-PG-313-320</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Loopy and generalized belief propagation are popular algorithms for
approximate inference in Markov random fields and Bayesian networks. Fixed
points of these algorithms correspond to extrema of the Bethe and Kikuchi free
energy. However, belief propagation does not always converge, which explains
the need for approaches that explicitly minimize the Kikuchi/Bethe free energy,
such as CCCP and UPS. Here we describe a class of algorithms that solves this
typically nonconvex constrained minimization of the Kikuchi free energy through
a sequence of convex constrained minimizations of upper bounds on the Kikuchi
free energy. Intuitively one would expect tighter bounds to lead to faster
algorithms, which is indeed convincingly demonstrated in our simulations.
Several ideas are applied to obtain tight convex bounds that yield dramatic
speed-ups over CCCP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2481</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2481</id><created>2012-10-19</created><authors><author><keyname>Hauskrecht</keyname><forenames>Milos</forenames></author><author><keyname>Singliar</keyname><forenames>Tomas</forenames></author></authors><title>Monte-Carlo optimizations for resource allocation problems in stochastic
  network systems</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)</comments><proxy>auai</proxy><report-no>UAI-P-2003-PG-305-312</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Real-world distributed systems and networks are often unreliable and subject
to random failures of its components. Such a stochastic behavior affects
adversely the complexity of optimization tasks performed routinely upon such
systems, in particular, various resource allocation tasks. In this work we
investigate and develop Monte Carlo solutions for a class of two-stage
optimization problems in stochastic networks in which the expected value of
resource allocations before and after stochastic failures needs to be
optimized. The limitation of these problems is that their exact solutions are
exponential in the number of unreliable network components: thus, exact methods
do not scale-up well to large networks often seen in practice. We first prove
that Monte Carlo optimization methods can overcome the exponential bottleneck
of exact methods. Next we support our theoretical findings on resource
allocation experiments and show a very good scale-up potential of the new
methods to large stochastic networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2482</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2482</id><created>2012-10-19</created><authors><author><keyname>Gretton</keyname><forenames>Charles</forenames></author><author><keyname>Price</keyname><forenames>David</forenames></author><author><keyname>Thiebaux</keyname><forenames>Sylvie</forenames></author></authors><title>Implementation and Comparison of Solution Methods for Decision Processes
  with Non-Markovian Rewards</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)</comments><proxy>auai</proxy><report-no>UAI-P-2003-PG-289-296</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper examines a number of solution methods for decision processes with
non-Markovian rewards (NMRDPs). They all exploit a temporal logic specification
of the reward function to automatically translate the NMRDP into an equivalent
Markov decision process (MDP) amenable to well-known MDP solution methods. They
differ however in the representation of the target MDP and the class of MDP
solution methods to which they are suited. As a result, they adopt different
temporal logics and different translations. Unfortunately, no implementation of
these methods nor experimental let alone comparative results have ever been
reported. This paper is the first step towards filling this gap. We describe an
integrated system for solving NMRDPs which implements these methods and several
variants under a common interface; we use it to compare the various approaches
and identify the problem features favoring one over the other.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2483</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2483</id><created>2012-10-19</created><authors><author><keyname>Globerson</keyname><forenames>Amir</forenames></author><author><keyname>Chechik</keyname><forenames>Gal</forenames></author><author><keyname>Tishby</keyname><forenames>Naftali</forenames></author></authors><title>Sufficient Dimensionality Reduction with Irrelevant Statistics</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)</comments><proxy>auai</proxy><report-no>UAI-P-2003-PG-281-288</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of finding a reduced dimensionality representation of categorical
variables while preserving their most relevant characteristics is fundamental
for the analysis of complex data. Specifically, given a co-occurrence matrix of
two variables, one often seeks a compact representation of one variable which
preserves information about the other variable. We have recently introduced
``Sufficient Dimensionality Reduction' [GT-2003], a method that extracts
continuous reduced dimensional features whose measurements (i.e., expectation
values) capture maximal mutual information among the variables. However, such
measurements often capture information that is irrelevant for a given task.
Widely known examples are illumination conditions, which are irrelevant as
features for face recognition, writing style which is irrelevant as a feature
for content classification, and intonation which is irrelevant as a feature for
speech recognition. Such irrelevance cannot be deduced apriori, since it
depends on the details of the task, and is thus inherently ill defined in the
purely unsupervised case. Separating relevant from irrelevant features can be
achieved using additional side data that contains such irrelevant structures.
This approach was taken in [CT-2002], extending the information bottleneck
method, which uses clustering to compress the data. Here we use this
side-information framework to identify features whose measurements are
maximally informative for the original data set, but carry as little
information as possible on a side data set. In statistical terms this can be
understood as extracting statistics which are maximally sufficient for the
original dataset, while simultaneously maximally ancillary for the side
dataset. We formulate this tradeoff as a constrained optimization problem and
characterize its solutions. We then derive a gradient descent algorithm for
this problem, which is based on the Generalized Iterative Scaling method for
finding maximum entropy distributions. The method is demonstrated on synthetic
data, as well as on real face recognition datasets, and is shown to outperform
standard methods such as oriented PCA.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2484</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2484</id><created>2012-10-19</created><authors><author><keyname>Giang</keyname><forenames>Phan H.</forenames></author><author><keyname>Shenoy</keyname><forenames>Prakash P.</forenames></author></authors><title>Decision Making with Partially Consonant Belief Functions</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)</comments><proxy>auai</proxy><report-no>UAI-P-2003-PG-272-280</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies decision making for Walley's partially consonant belief
functions (pcb). In a pcb, the set of foci are partitioned. Within each
partition, the foci are nested. The pcb class includes probability functions
and possibility functions as extreme cases. Unlike earlier proposals for a
decision theory with belief functions, we employ an axiomatic approach. We
adopt an axiom system similar in spirit to von Neumann - Morgenstern's linear
utility theory for a preference relation on pcb lotteries. We prove a
representation theorem for this relation. Utility for a pcb lottery is a
combination of linear utility for probabilistic lottery and binary utility for
possibilistic lottery.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2485</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2485</id><created>2012-10-19</created><authors><author><keyname>Gao</keyname><forenames>Yong</forenames></author></authors><title>Phase Transition of Tractability in Constraint Satisfaction and Bayesian
  Network Inference</title><categories>cs.AI cs.DS</categories><comments>Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)</comments><proxy>auai</proxy><report-no>UAI-P-2003-PG-265-271</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There has been great interest in identifying tractable subclasses of NP
complete problems and designing efficient algorithms for these tractable
classes. Constraint satisfaction and Bayesian network inference are two
examples of such problems that are of great importance in AI and algorithms. In
this paper we study, under the frameworks of random constraint satisfaction
problems and random Bayesian networks, a typical tractable subclass
characterized by the treewidth of the problems. We show that the property of
having a bounded treewidth for CSPs and Bayesian network inference problem has
a phase transition that occurs while the underlying structures of problems are
still sparse. This implies that algorithms making use of treewidth based
structural knowledge only work efficiently in a limited range of random
instance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2486</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2486</id><created>2012-10-19</created><authors><author><keyname>Frey</keyname><forenames>Brendan J.</forenames></author></authors><title>Extending Factor Graphs so as to Unify Directed and Undirected Graphical
  Models</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)</comments><proxy>auai</proxy><report-no>UAI-P-2003-PG-257-264</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The two most popular types of graphical model are directed models (Bayesian
networks) and undirected models (Markov random fields, or MRFs). Directed and
undirected models offer complementary properties in model construction,
expressing conditional independencies, expressing arbitrary factorizations of
joint distributions, and formulating message-passing inference algorithms. We
show that the strengths of these two representations can be combined in a
single type of graphical model called a 'factor graph'. Every Bayesian network
or MRF can be easily converted to a factor graph that expresses the same
conditional independencies, expresses the same factorization of the joint
distribution, and can be used for probabilistic inference through application
of a single, simple message-passing algorithm. In contrast to chain graphs,
where message-passing is implemented on a hypergraph, message-passing can be
directly implemented on the factor graph. We describe a modified 'Bayes-ball'
algorithm for establishing conditional independence in factor graphs, and we
show that factor graphs form a strict superset of Bayesian networks and MRFs.
In particular, we give an example of a commonly-used 'mixture of experts' model
fragment, whose independencies cannot be represented in a Bayesian network or
an MRF, but can be represented in a factor graph. We finish by giving examples
of real-world problems that are not well suited to representation in Bayesian
networks and MRFs, but are well-suited to representation in factor graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2487</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2487</id><created>2012-10-19</created><authors><author><keyname>Frank</keyname><forenames>Eibe</forenames></author><author><keyname>Hall</keyname><forenames>Mark</forenames></author><author><keyname>Pfahringer</keyname><forenames>Bernhard</forenames></author></authors><title>Locally Weighted Naive Bayes</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)</comments><proxy>auai</proxy><report-no>UAI-P-2003-PG-249-256</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite its simplicity, the naive Bayes classifier has surprised machine
learning researchers by exhibiting good performance on a variety of learning
problems. Encouraged by these results, researchers have looked to overcome
naive Bayes primary weakness - attribute independence - and improve the
performance of the algorithm. This paper presents a locally weighted version of
naive Bayes that relaxes the independence assumption by learning local models
at prediction time. Experimental results show that locally weighted naive Bayes
rarely degrades accuracy compared to standard naive Bayes and, in many cases,
improves accuracy dramatically. The main advantage of this method compared to
other techniques for enhancing naive Bayes is its conceptual and computational
simplicity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2488</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2488</id><created>2012-10-19</created><authors><author><keyname>Frank</keyname><forenames>Ari</forenames></author><author><keyname>Geiger</keyname><forenames>Dan</forenames></author><author><keyname>Yakhini</keyname><forenames>Zohar</forenames></author></authors><title>A Distance-Based Branch and Bound Feature Selection Algorithm</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)</comments><proxy>auai</proxy><report-no>UAI-P-2003-PG-241-248</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is no known efficient method for selecting k Gaussian features from n
which achieve the lowest Bayesian classification error. We show an example of
how greedy algorithms faced with this task are led to give results that are not
optimal. This motivates us to propose a more robust approach. We present a
Branch and Bound algorithm for finding a subset of k independent Gaussian
features which minimizes the naive Bayesian classification error. Our algorithm
uses additive monotonic distance measures to produce bounds for the Bayesian
classification error in order to exclude many feature subsets from evaluation,
while still returning an optimal solution. We test our method on synthetic data
as well as data obtained from gene expression profiling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2490</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2490</id><created>2012-10-19</created><authors><author><keyname>Salakhutdinov</keyname><forenames>Ruslan R</forenames></author><author><keyname>Roweis</keyname><forenames>Sam T</forenames></author><author><keyname>Ghahramani</keyname><forenames>Zoubin</forenames></author></authors><title>On the Convergence of Bound Optimization Algorithms</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)</comments><proxy>auai</proxy><report-no>UAI-P-2003-PG-509-516</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many practitioners who use the EM algorithm complain that it is sometimes
slow. When does this happen, and what can be done about it? In this paper, we
study the general class of bound optimization algorithms - including
Expectation-Maximization, Iterative Scaling and CCCP - and their relationship
to direct optimization algorithms such as gradient-based methods for parameter
learning. We derive a general relationship between the updates performed by
bound optimization methods and those of gradient and second-order methods and
identify analytic conditions under which bound optimization algorithms exhibit
quasi-Newton behavior, and conditions under which they possess poor,
first-order convergence. Based on this analysis, we consider several specific
algorithms, interpret and analyze their convergence properties and provide some
recipes for preprocessing input to these algorithms to yield faster convergence
behavior. We report empirical results supporting our analysis and showing that
simple data preprocessing can result in dramatically improved performance of
bound optimizers in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2491</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2491</id><created>2012-10-19</created><authors><author><keyname>Rusakov</keyname><forenames>Dmitry</forenames></author><author><keyname>Geiger</keyname><forenames>Dan</forenames></author></authors><title>Automated Analytic Asymptotic Evaluation of the Marginal Likelihood for
  Latent Models</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)</comments><proxy>auai</proxy><report-no>UAI-P-2003-PG-501-508</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present and implement two algorithms for analytic asymptotic evaluation of
the marginal likelihood of data given a Bayesian network with hidden nodes. As
shown by previous work, this evaluation is particularly hard for latent
Bayesian network models, namely networks that include hidden variables, where
asymptotic approximation deviates from the standard BIC score. Our algorithms
solve two central difficulties in asymptotic evaluation of marginal likelihood
integrals, namely, evaluation of regular dimensionality drop for latent
Bayesian network models and computation of non-standard approximation formulas
for singular statistics for these models. The presented algorithms are
implemented in Matlab and Maple and their usage is demonstrated for marginal
likelihood approximations for Bayesian networks with hidden variables.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2493</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2493</id><created>2012-10-19</created><authors><author><keyname>Rosencrantz</keyname><forenames>Matthew</forenames></author><author><keyname>Gordon</keyname><forenames>Geoffrey</forenames></author><author><keyname>Thrun</keyname><forenames>Sebastian</forenames></author></authors><title>Decentralized Sensor Fusion With Distributed Particle Filters</title><categories>cs.AI cs.RO</categories><comments>Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)</comments><proxy>auai</proxy><report-no>UAI-P-2003-PG-493-500</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a scalable Bayesian technique for decentralized state
estimation from multiple platforms in dynamic environments. As has long been
recognized, centralized architectures impose severe scaling limitations for
distributed systems due to the enormous communication overheads. We propose a
strictly decentralized approach in which only nearby platforms exchange
information. They do so through an interactive communication protocol aimed at
maximizing information flow. Our approach is evaluated in the context of a
distributed surveillance scenario that arises in a robotic system for playing
the game of laser tag. Our results, both from simulation and using physical
robots, illustrate an unprecedented scaling capability to large teams of
vehicles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2494</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2494</id><created>2012-10-19</created><authors><author><keyname>Rosales</keyname><forenames>Romer</forenames></author><author><keyname>Frey</keyname><forenames>Brendan J.</forenames></author></authors><title>Learning Generative Models of Similarity Matrices</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)</comments><proxy>auai</proxy><report-no>UAI-P-2003-PG-485-492</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a probabilistic (generative) view of affinity matrices along with
inference algorithms for a subclass of problems associated with data
clustering. This probabilistic view is helpful in understanding different
models and algorithms that are based on affinity functions OF the data. IN
particular, we show how(greedy) inference FOR a specific probabilistic model IS
equivalent TO the spectral clustering algorithm.It also provides a framework
FOR developing new algorithms AND extended models. AS one CASE, we present new
generative data clustering models that allow us TO infer the underlying
distance measure suitable for the clustering problem at hand. These models seem
to perform well in a larger class of problems for which other clustering
algorithms (including spectral clustering) usually fail. Experimental
evaluation was performed in a variety point data sets, showing excellent
performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2495</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2495</id><created>2012-10-19</created><authors><author><keyname>Pineau</keyname><forenames>Joelle</forenames></author><author><keyname>Gordon</keyname><forenames>Geoffrey</forenames></author><author><keyname>Thrun</keyname><forenames>Sebastian</forenames></author></authors><title>Policy-contingent abstraction for robust robot control</title><categories>cs.RO cs.AI cs.SY</categories><comments>Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)</comments><proxy>auai</proxy><report-no>UAI-P-2003-PG-477-484</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a scalable control algorithm that enables a deployed
mobile robot system to make high-level decisions under full consideration of
its probabilistic belief. Our approach is based on insights from the rich
literature of hierarchical controllers and hierarchical MDPs. The resulting
controller has been successfully deployed in a nursing facility near
Pittsburgh, PA. To the best of our knowledge, this work is a unique instance of
applying POMDPs to high-level robotic control problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2496</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2496</id><created>2012-10-19</created><authors><author><keyname>Perny</keyname><forenames>Patrice</forenames></author><author><keyname>Spanjaard</keyname><forenames>Olivier</forenames></author></authors><title>An Axiomatic Approach to Robustness in Search Problems with Multiple
  Scenarios</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)</comments><proxy>auai</proxy><report-no>UAI-P-2003-PG-469-476</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is devoted to the search of robust solutions in state space graphs
when costs depend on scenarios. We first present axiomatic requirements for
preference compatibility with the intuitive idea of robustness.This leads us to
propose the Lorenz dominance rule as a basis for robustness analysis. Then,
after presenting complexity results about the determination of robust
solutions, we propose a new sophistication of A* specially designed to
determine the set of robust paths in a state space graph. The behavior of the
algorithm is illustrated on a small example. Finally, an axiomatic
justification of the refinement of robustness by an OWA criterion is provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2497</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2497</id><created>2012-10-19</created><authors><author><keyname>Park</keyname><forenames>James D.</forenames></author><author><keyname>Darwiche</keyname><forenames>Adnan</forenames></author></authors><title>Solving MAP Exactly using Systematic Search</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)</comments><proxy>auai</proxy><report-no>UAI-P-2003-PG-459-468</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  MAP is the problem of finding a most probable instantiation of a set of
variables in a Bayesian network given some evidence. Unlike computing posterior
probabilities, or MPE (a special case of MAP), the time and space complexity of
structural solutions for MAP are not only exponential in the network treewidth,
but in a larger parameter known as the &quot;constrained&quot; treewidth. In practice,
this means that computing MAP can be orders of magnitude more expensive than
computing posterior probabilities or MPE. This paper introduces a new, simple
upper bound on the probability of a MAP solution, which admits a tradeoff
between the bound quality and the time needed to compute it. The bound is shown
to be generally much tighter than those of other methods of comparable
complexity. We use this proposed upper bound to develop a branch-and-bound
search algorithm for solving MAP exactly. Experimental results demonstrate that
the search algorithm is able to solve many problems that are far beyond the
reach of any structure-based method for MAP. For example, we show that the
proposed algorithm can compute MAP exactly and efficiently for some networks
whose constrained treewidth is more than 40.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2498</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2498</id><created>2012-10-19</created><authors><author><keyname>Nodelman</keyname><forenames>Uri</forenames></author><author><keyname>Shelton</keyname><forenames>Christian R.</forenames></author><author><keyname>Koller</keyname><forenames>Daphne</forenames></author></authors><title>Learning Continuous Time Bayesian Networks</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)</comments><proxy>auai</proxy><report-no>UAI-P-2003-PG-451-458</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Continuous time Bayesian networks (CTBNs) describe structured stochastic
processes with finitely many states that evolve over continuous time. A CTBN is
a directed (possibly cyclic) dependency graph over a set of variables, each of
which represents a finite state continuous time Markov process whose transition
model is a function of its parents. We address the problem of learning
parameters and structure of a CTBN from fully observed data. We define a
conjugate prior for CTBNs, and show how it can be used both for Bayesian
parameter estimation and as the basis of a Bayesian score for structure
learning. Because acyclicity is not a constraint in CTBNs, we can show that the
structure learning problem is significantly easier, both in theory and in
practice, than structure learning for dynamic Bayesian networks (DBNs).
Furthermore, as CTBNs can tailor the parameters and dependency structure to the
different time granularities of the evolution of different variables, they can
provide a better fit to continuous-time processes than DBNs with a fixed time
granularity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2499</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2499</id><created>2012-10-19</created><authors><author><keyname>Nikovski</keyname><forenames>Daniel N.</forenames></author><author><keyname>Brand</keyname><forenames>Matthew</forenames></author></authors><title>Marginalizing Out Future Passengers in Group Elevator Control</title><categories>cs.AI cs.SY</categories><comments>Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)</comments><proxy>auai</proxy><report-no>UAI-P-2003-PG-443-450</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Group elevator scheduling is an NP-hard sequential decision-making problem
with unbounded state spaces and substantial uncertainty. Decision-theoretic
reasoning plays a surprisingly limited role in fielded systems. A new
opportunity for probabilistic methods has opened with the recent discovery of a
tractable solution for the expected waiting times of all passengers in the
building, marginalized over all possible passenger itineraries. Though
commercially competitive, this solution does not contemplate future passengers.
Yet in up-peak traffic, the effects of future passengers arriving at the lobby
and entering elevator cars can dominate all waiting times. We develop a
probabilistic model of how these arrivals affect the behavior of elevator cars
at the lobby, and demonstrate how this model can be used to very significantly
reduce the average waiting time of all passengers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2500</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2500</id><created>2012-10-19</created><authors><author><keyname>Nielsen</keyname><forenames>Jens D.</forenames></author><author><keyname>Kocka</keyname><forenames>Tomas</forenames></author><author><keyname>Pena</keyname><forenames>Jose M.</forenames></author></authors><title>On Local Optima in Learning Bayesian Networks</title><categories>cs.LG cs.AI stat.ML</categories><comments>Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)</comments><proxy>auai</proxy><report-no>UAI-P-2003-PG-435-442</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes and evaluates the k-greedy equivalence search algorithm
(KES) for learning Bayesian networks (BNs) from complete data. The main
characteristic of KES is that it allows a trade-off between greediness and
randomness, thus exploring different good local optima. When greediness is set
at maximum, KES corresponds to the greedy equivalence search algorithm (GES).
When greediness is kept at minimum, we prove that under mild assumptions KES
asymptotically returns any inclusion optimal BN with nonzero probability.
Experimental results for both synthetic and real data are reported showing that
KES often finds a better local optima than GES. Moreover, we use KES to
experimentally confirm that the number of different local optima is often huge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2501</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2501</id><created>2012-10-19</created><authors><author><keyname>Mugica</keyname><forenames>Francisco</forenames></author><author><keyname>Nebot</keyname><forenames>Angela</forenames></author><author><keyname>Gomez</keyname><forenames>Pilar</forenames></author></authors><title>Dealing with uncertainty in fuzzy inductive reasoning methodology</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)</comments><proxy>auai</proxy><report-no>UAI-P-2003-PG-427-434</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this research is to develop a reasoning under uncertainty strategy
in the context of the Fuzzy Inductive Reasoning (FIR) methodology. FIR emerged
from the General Systems Problem Solving developed by G. Klir. It is a data
driven methodology based on systems behavior rather than on structural
knowledge. It is a very useful tool for both the modeling and the prediction of
those systems for which no previous structural knowledge is available. FIR
reasoning is based on pattern rules synthesized from the available data. The
size of the pattern rule base can be very large making the prediction process
quite difficult. In order to reduce the size of the pattern rule base, it is
possible to automatically extract classical Sugeno fuzzy rules starting from
the set of pattern rules. The Sugeno rule base preserves pattern rules
knowledge as much as possible. In this process some information is lost but
robustness is considerably increased. In the forecasting process either the
pattern rule base or the Sugeno fuzzy rule base can be used. The first option
is desirable when the computational resources make it possible to deal with the
overall pattern rule base or when the extracted fuzzy rules are not accurate
enough due to uncertainty associated to the original data. In the second
option, the prediction process is done by means of the classical Sugeno
inference system. If the amount of uncertainty associated to the data is small,
the predictions obtained using the Sugeno fuzzy rule base will be very
accurate. In this paper a mixed pattern/fuzzy rules strategy is proposed to
deal with uncertainty in such a way that the best of both perspectives is used.
Areas in the data space with a higher level of uncertainty are identified by
means of the so-called error models. The prediction process in these areas
makes use of a mixed pattern/fuzzy rules scheme, whereas areas identified with
a lower level of uncertainty only use the Sugeno fuzzy rule base. The proposed
strategy is applied to a real biomedical system, i.e., the central nervous
system control of the cardiovascular system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2502</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2502</id><created>2012-10-19</created><authors><author><keyname>Meuleau</keyname><forenames>Nicolas</forenames></author><author><keyname>Smith</keyname><forenames>David</forenames></author></authors><title>Optimal Limited Contingency Planning</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)</comments><proxy>auai</proxy><report-no>UAI-P-2003-PG-417-426</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a given problem, the optimal Markov policy can be considerred as a
conditional or contingent plan containing a (potentially large) number of
branches. Unfortunately, there are applications where it is desirable to
strictly limit the number of decision points and branches in a plan. For
example, it may be that plans must later undergo more detailed simulation to
verify correctness and safety, or that they must be simple enough to be
understood and analyzed by humans. As a result, it may be necessary to limit
consideration to plans with only a small number of branches. This raises the
question of how one goes about finding optimal plans containing only a limited
number of branches. In this paper, we present an any-time algorithm for optimal
k-contingency planning (OKP). It is the first optimal algorithm for limited
contingency planning that is not an explicit enumeration of possible contingent
plans. By modelling the problem as a Partially Observable Markov Decision
Process, it implements the Bellman optimality principle and prunes the solution
space. We present experimental results of applying this algorithm to some
simple test cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2503</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2503</id><created>2012-10-19</created><authors><author><keyname>Meek</keyname><forenames>Christopher</forenames></author><author><keyname>Chickering</keyname><forenames>David Maxwell</forenames></author></authors><title>Practically Perfect</title><categories>cs.AI stat.ML</categories><comments>Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)</comments><proxy>auai</proxy><report-no>UAI-P-2003-PG-411-416</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The property of perfectness plays an important role in the theory of Bayesian
networks. First, the existence of perfect distributions for arbitrary sets of
variables and directed acyclic graphs implies that various methods for reading
independence from the structure of the graph (e.g., Pearl, 1988; Lauritzen,
Dawid, Larsen &amp; Leimer, 1990) are complete. Second, the asymptotic reliability
of various search methods is guaranteed under the assumption that the
generating distribution is perfect (e.g., Spirtes, Glymour &amp; Scheines, 2000;
Chickering &amp; Meek, 2002). We provide a lower-bound on the probability of
sampling a non-perfect distribution when using a fixed number of bits to
represent the parameters of the Bayesian network. This bound approaches zero
exponentially fast as one increases the number of bits used to represent the
parameters. This result implies that perfect distributions with fixed-length
representations exist. We also provide a lower-bound on the number of bits
needed to guarantee that a distribution sampled from a uniform Dirichlet
distribution is perfect with probability greater than 1/2. This result is
useful for constructing randomized reductions for hardness proofs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2504</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2504</id><created>2012-10-19</created><authors><author><keyname>McCallum</keyname><forenames>Andrew</forenames></author></authors><title>Efficiently Inducing Features of Conditional Random Fields</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)</comments><proxy>auai</proxy><report-no>UAI-P-2003-PG-403-410</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conditional Random Fields (CRFs) are undirected graphical models, a special
case of which correspond to conditionally-trained finite state machines. A key
advantage of these models is their great flexibility to include a wide array of
overlapping, multi-granularity, non-independent features of the input. In face
of this freedom, an important question that remains is, what features should be
used? This paper presents a feature induction method for CRFs. Founded on the
principle of constructing only those feature conjunctions that significantly
increase log-likelihood, the approach is based on that of Della Pietra et al
[1997], but altered to work with conditional rather than joint probabilities,
and with additional modifications for providing tractability specifically for a
sequence model. In comparison with traditional approaches, automated feature
induction offers both improved accuracy and more than an order of magnitude
reduction in feature count; it enables the use of richer, higher-order Markov
models, and offers more freedom to liberally guess about which atomic features
may be relevant to a task. The induction method applies to linear-chain CRFs,
as well as to more arbitrary CRF structures, also known as Relational Markov
Networks [Taskar &amp; Koller, 2002]. We present experimental results on a named
entity extraction task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2505</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2505</id><created>2012-10-19</created><authors><author><keyname>Marinescu</keyname><forenames>Radu</forenames></author><author><keyname>Kask</keyname><forenames>Kalev</forenames></author><author><keyname>Dechter</keyname><forenames>Rina</forenames></author></authors><title>Systematic vs. Non-systematic Algorithms for Solving the MPE Task</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)</comments><proxy>auai</proxy><report-no>UAI-P-2003-PG-394-402</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper continues the study of partitioning based inference of heuristics
for search in the context of solving the Most Probable Explanation task in
Bayesian Networks. We compare two systematic Branch and Bound search
algorithms, BBBT (for which the heuristic information is constructed during
search and allows dynamic variable/value ordering) and its predecessor BBMB
(for which the heuristic information is pre-compiled), against a number of
popular local search algorithms for the MPE problem. We show empirically that,
when viewed as approximation schemes, BBBT/BBMB are superior to all of these
best known SLS algorithms, especially when the domain sizes increase beyond 2.
This is in contrast with the performance of SLS vs. systematic search on
CSP/SAT problems, where SLS often significantly outperforms systematic
algorithms. As far as we know, BBBT/BBMB are currently the best performing
algorithms for solving the MPE task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2506</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2506</id><created>2012-10-19</created><authors><author><keyname>Zhang</keyname><forenames>Jiji</forenames></author><author><keyname>Spirtes</keyname><forenames>Peter L.</forenames></author></authors><title>Strong Faithfulness and Uniform Consistency in Causal Inference</title><categories>cs.AI stat.ME</categories><comments>Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)</comments><proxy>auai</proxy><report-no>UAI-P-2003-PG-632-639</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A fundamental question in causal inference is whether it is possible to
reliably infer manipulation effects from observational data. There are a
variety of senses of asymptotic reliability in the statistical literature,
among which the most commonly discussed frequentist notions are pointwise
consistency and uniform consistency. Uniform consistency is in general
preferred to pointwise consistency because the former allows us to control the
worst case error bounds with a finite sample size. In the sense of pointwise
consistency, several reliable causal inference algorithms have been established
under the Markov and Faithfulness assumptions [Pearl 2000, Spirtes et al.
2001]. In the sense of uniform consistency, however, reliable causal inference
is impossible under the two assumptions when time order is unknown and/or
latent confounders are present [Robins et al. 2000]. In this paper we present
two natural generalizations of the Faithfulness assumption in the context of
structural equation models, under which we show that the typical algorithms in
the literature (in some cases with modifications) are uniformly consistent even
when the time order is unknown. We also discuss the situation where latent
confounders may be present and the sense in which the Faithfulness assumption
is a limiting case of the stronger assumptions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2507</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2507</id><created>2012-10-19</created><authors><author><keyname>Yuan</keyname><forenames>Changhe</forenames></author><author><keyname>Druzdzel</keyname><forenames>Marek J.</forenames></author></authors><title>An Importance Sampling Algorithm Based on Evidence Pre-propagation</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)</comments><proxy>auai</proxy><report-no>UAI-P-2003-PG-624-631</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Precision achieved by stochastic sampling algorithms for Bayesian networks
typically deteriorates in face of extremely unlikely evidence. To address this
problem, we propose the Evidence Pre-propagation Importance Sampling algorithm
(EPIS-BN), an importance sampling algorithm that computes an approximate
importance function by the heuristic methods: loopy belief Propagation and
e-cutoff. We tested the performance of e-cutoff on three large real Bayesian
networks: ANDES, CPCS, and PATHFINDER. We observed that on each of these
networks the EPIS-BN algorithm gives us a considerable improvement over the
current state of the art algorithm, the AIS-BN algorithm. In addition, it
avoids the costly learning stage of the AIS-BN algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2508</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2508</id><created>2012-10-19</created><authors><author><keyname>Yu</keyname><forenames>Kai</forenames></author><author><keyname>Schwaighofer</keyname><forenames>Anton</forenames></author><author><keyname>Tresp</keyname><forenames>Volker</forenames></author><author><keyname>Ma</keyname><forenames>Wei-Ying</forenames></author><author><keyname>Zhang</keyname><forenames>HongJiang</forenames></author></authors><title>Collaborative Ensemble Learning: Combining Collaborative and
  Content-Based Information Filtering via Hierarchical Bayes</title><categories>cs.LG cs.IR stat.ML</categories><comments>Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)</comments><proxy>auai</proxy><report-no>UAI-P-2003-PG-616-623</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Collaborative filtering (CF) and content-based filtering (CBF) have widely
been used in information filtering applications. Both approaches have their
strengths and weaknesses which is why researchers have developed hybrid
systems. This paper proposes a novel approach to unify CF and CBF in a
probabilistic framework, named collaborative ensemble learning. It uses
probabilistic SVMs to model each user's profile (as CBF does).At the prediction
phase, it combines a society OF users profiles, represented by their respective
SVM models, to predict an active users preferences(the CF idea).The combination
scheme is embedded in a probabilistic framework and retains an intuitive
explanation.Moreover, collaborative ensemble learning does not require a global
training stage and thus can incrementally incorporate new data.We report
results based on two data sets. For the Reuters-21578 text data set, we
simulate user ratings under the assumption that each user is interested in only
one category. In the second experiment, we use users' opinions on a set of 642
art images that were collected through a web-based survey. For both data sets,
collaborative ensemble achieved excellent performance in terms of
recommendation accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2509</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2509</id><created>2012-10-19</created><authors><author><keyname>Young</keyname><forenames>Joel</forenames></author><author><keyname>Dean</keyname><forenames>Thomas L.</forenames></author></authors><title>Exploiting Locality in Searching the Web</title><categories>cs.IR cs.AI</categories><comments>Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)</comments><proxy>auai</proxy><report-no>UAI-P-2003-PG-608-615</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Published experiments on spidering the Web suggest that, given training data
in the form of a (relatively small) subgraph of the Web containing a subset of
a selected class of target pages, it is possible to conduct a directed search
and find additional target pages significantly faster (with fewer page
retrievals) than by performing a blind or uninformed random or systematic
search, e.g., breadth-first search. If true, this claim motivates a number of
practical applications. Unfortunately, these experiments were carried out in
specialized domains or under conditions that are difficult to replicate. We
present and apply an experimental framework designed to reexamine and resolve
the basic claims of the earlier work, so that the supporting experiments can be
replicated and built upon. We provide high-performance tools for building
experimental spiders, make use of the ground truth and static nature of the
WT10g TREC Web corpus, and rely on simple well understand machine learning
techniques to conduct our experiments. In this paper, we describe the basic
framework, motivate the experimental design, and report on our findings
supporting and qualifying the conclusions of the earlier research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2510</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2510</id><created>2012-10-19</created><authors><author><keyname>Yeang</keyname><forenames>Chen-Hsiang</forenames></author><author><keyname>Szummer</keyname><forenames>Martin</forenames></author></authors><title>Markov Random Walk Representations with Continuous Distributions</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)</comments><proxy>auai</proxy><report-no>UAI-P-2003-PG-600-607</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Representations based on random walks can exploit discrete data distributions
for clustering and classification. We extend such representations from discrete
to continuous distributions. Transition probabilities are now calculated using
a diffusion equation with a diffusion coefficient that inversely depends on the
data density. We relate this diffusion equation to a path integral and derive
the corresponding path probability measure. The framework is useful for
incorporating continuous data densities and prior knowledge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2511</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2511</id><created>2012-10-19</created><authors><author><keyname>Yamazaki</keyname><forenames>Keisuke</forenames></author><author><keyname>Watanbe</keyname><forenames>Sumio</forenames></author></authors><title>Stochastic complexity of Bayesian networks</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)</comments><proxy>auai</proxy><report-no>UAI-P-2003-PG-592-599</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bayesian networks are now being used in enormous fields, for example,
diagnosis of a system, data mining, clustering and so on. In spite of their
wide range of applications, the statistical properties have not yet been
clarified, because the models are nonidentifiable and non-regular. In a
Bayesian network, the set of its parameter for a smaller model is an analytic
set with singularities in the space of large ones. Because of these
singularities, the Fisher information matrices are not positive definite. In
other words, the mathematical foundation for learning was not constructed. In
recent years, however, we have developed a method to analyze non-regular models
using algebraic geometry. This method revealed the relation between the models
singularities and its statistical properties. In this paper, applying this
method to Bayesian networks with latent variables, we clarify the order of the
stochastic complexities.Our result claims that the upper bound of those is
smaller than the dimension of the parameter space. This means that the Bayesian
generalization error is also far smaller than that of regular model, and that
Schwarzs model selection criterion BIC needs to be improved for Bayesian
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2512</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2512</id><created>2012-10-19</created><authors><author><keyname>Xing</keyname><forenames>Eric P.</forenames></author><author><keyname>Jordan</keyname><forenames>Michael I.</forenames></author><author><keyname>Russell</keyname><forenames>Stuart</forenames></author></authors><title>A Generalized Mean Field Algorithm for Variational Inference in
  Exponential Families</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)</comments><proxy>auai</proxy><report-no>UAI-P-2003-PG-583-591</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The mean field methods, which entail approximating intractable probability
distributions variationally with distributions from a tractable family, enjoy
high efficiency, guaranteed convergence, and provide lower bounds on the true
likelihood. But due to requirement for model-specific derivation of the
optimization equations and unclear inference quality in various models, it is
not widely used as a generic approximate inference algorithm. In this paper, we
discuss a generalized mean field theory on variational approximation to a broad
class of intractable distributions using a rich set of tractable distributions
via constrained optimization over distribution spaces. We present a class of
generalized mean field (GMF) algorithms for approximate inference in complex
exponential family models, which entails limiting the optimization over the
class of cluster-factorizable distributions. GMF is a generic method requiring
no model-specific derivations. It factors a complex model into a set of
disjoint variable clusters, and uses a set of canonical fix-point equations to
iteratively update the cluster distributions, and converge to locally optimal
cluster marginals that preserve the original dependency structure within each
cluster, hence, fully decomposed the overall inference problem. We empirically
analyzed the effect of different tractable family (clusters of different
granularity) on inference quality, and compared GMF with BP on several
canonical models. Possible extension to higher-order MF approximation is also
discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2513</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2513</id><created>2012-10-19</created><authors><author><keyname>Welling</keyname><forenames>Max</forenames></author><author><keyname>Zemel</keyname><forenames>Richard S.</forenames></author><author><keyname>Hinton</keyname><forenames>Geoffrey E.</forenames></author></authors><title>Efficient Parametric Projection Pursuit Density Estimation</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)</comments><proxy>auai</proxy><report-no>UAI-P-2003-PG-575-582</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Product models of low dimensional experts are a powerful way to avoid the
curse of dimensionality. We present the ``under-complete product of experts'
(UPoE), where each expert models a one dimensional projection of the data. The
UPoE is fully tractable and may be interpreted as a parametric probabilistic
model for projection pursuit. Its ML learning rules are identical to the
approximate learning rules proposed before for under-complete ICA. We also
derive an efficient sequential learning algorithm and discuss its relationship
to projection pursuit density estimation and feature induction algorithms for
additive random field models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2514</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2514</id><created>2012-10-19</created><authors><author><keyname>Wang</keyname><forenames>Shaojun</forenames></author><author><keyname>Schuurmans</keyname><forenames>Dale</forenames></author><author><keyname>Peng</keyname><forenames>Fuchun</forenames></author><author><keyname>Zhao</keyname><forenames>Yunxin</forenames></author></authors><title>Boltzmann Machine Learning with the Latent Maximum Entropy Principle</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)</comments><proxy>auai</proxy><report-no>UAI-P-2003-PG-567-574</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new statistical learning paradigm for Boltzmann machines based
on a new inference principle we have proposed: the latent maximum entropy
principle (LME). LME is different both from Jaynes maximum entropy principle
and from standard maximum likelihood estimation.We demonstrate the LME
principle BY deriving new algorithms for Boltzmann machine parameter
estimation, and show how robust and fast new variant of the EM algorithm can be
developed.Our experiments show that estimation based on LME generally yields
better results than maximum likelihood estimation, particularly when inferring
hidden units from small amounts of data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2515</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2515</id><created>2012-10-19</created><authors><author><keyname>Stewart</keyname><forenames>Benjamin</forenames></author><author><keyname>Ko</keyname><forenames>Jonathan</forenames></author><author><keyname>Fox</keyname><forenames>Dieter</forenames></author><author><keyname>Konolige</keyname><forenames>Kurt</forenames></author></authors><title>The Revisiting Problem in Mobile Robot Map Building: A Hierarchical
  Bayesian Approach</title><categories>cs.AI cs.RO</categories><comments>Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)</comments><proxy>auai</proxy><report-no>UAI-P-2003-PG-551-558</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an application of hierarchical Bayesian estimation to robot map
building. The revisiting problem occurs when a robot has to decide whether it
is seeing a previously-built portion of a map, or is exploring new territory.
This is a difficult decision problem, requiring the probability of being
outside of the current known map. To estimate this probability, we model the
structure of a &quot;typical&quot; environment as a hidden Markov model that generates
sequences of views observed by a robot navigating through the environment. A
Dirichlet prior over structural models is learned from previously explored
environments. Whenever a robot explores a new environment, the posterior over
the model is estimated by Dirichlet hyperparameters. Our approach is
implemented and tested in the context of multi-robot map merging, a
particularly difficult instance of the revisiting problem. Experiments with
robot data show that the technique yields strong improvements over alternative
methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2516</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2516</id><created>2012-10-19</created><authors><author><keyname>Silva</keyname><forenames>Ricardo</forenames></author><author><keyname>Scheines</keyname><forenames>Richard</forenames></author><author><keyname>Glymour</keyname><forenames>Clark</forenames></author><author><keyname>Spirtes</keyname><forenames>Peter L.</forenames></author></authors><title>Learning Measurement Models for Unobserved Variables</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)</comments><proxy>auai</proxy><report-no>UAI-P-2003-PG-543-550</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Observed associations in a database may be due in whole or part to variations
in unrecorded (latent) variables. Identifying such variables and their causal
relationships with one another is a principal goal in many scientific and
practical domains. Previous work shows that, given a partition of observed
variables such that members of a class share only a single latent common cause,
standard search algorithms for causal Bayes nets can infer structural relations
between latent variables. We introduce an algorithm for discovering such
partitions when they exist. Uniquely among available procedures, the algorithm
is (asymptotically) correct under standard assumptions in causal Bayes net
search algorithms, requires no prior knowledge of the number of latent
variables, and does not depend on the mathematical form of the relationships
among the latent variables. We evaluate the algorithm on a variety of simulated
data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2517</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2517</id><created>2012-10-19</created><authors><author><keyname>Segal</keyname><forenames>Eran</forenames></author><author><keyname>Pe'er</keyname><forenames>Dana</forenames></author><author><keyname>Regev</keyname><forenames>Aviv</forenames></author><author><keyname>Koller</keyname><forenames>Daphne</forenames></author><author><keyname>Friedman</keyname><forenames>Nir</forenames></author></authors><title>Learning Module Networks</title><categories>cs.LG cs.CE stat.ML</categories><comments>Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)</comments><proxy>auai</proxy><report-no>UAI-P-2003-PG-525-534</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Methods for learning Bayesian network structure can discover dependency
structure between observed variables, and have been shown to be useful in many
applications. However, in domains that involve a large number of variables, the
space of possible network structures is enormous, making it difficult, for both
computational and statistical reasons, to identify a good model. In this paper,
we consider a solution to this problem, suitable for domains where many
variables have similar behavior. Our method is based on a new class of models,
which we call module networks. A module network explicitly represents the
notion of a module - a set of variables that have the same parents in the
network and share the same conditional probability distribution. We define the
semantics of module networks, and describe an algorithm that learns a module
network from data. The algorithm learns both the partitioning of the variables
into modules and the dependency structure between the variables. We evaluate
our algorithm on synthetic data, and on real data in the domains of gene
expression and the stock market. Our results show that module networks
generalize better than Bayesian networks, and that the learned module network
structure reveals regularities that are obscured in learned Bayesian networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2518</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2518</id><created>2012-10-19</created><authors><author><keyname>Sharma</keyname><forenames>Rita</forenames></author><author><keyname>Poole</keyname><forenames>David L</forenames></author></authors><title>Efficient Inference in Large Discrete Domains</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)</comments><proxy>auai</proxy><report-no>UAI-P-2003-PG-535-542</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we examine the problem of inference in Bayesian Networks with
discrete random variables that have very large or even unbounded domains. For
example, in a domain where we are trying to identify a person, we may have
variables that have as domains, the set of all names, the set of all postal
codes, or the set of all credit card numbers. We cannot just have big tables of
the conditional probabilities, but need compact representations. We provide an
inference algorithm, based on variable elimination, for belief networks
containing both large domain and normal discrete random variables. We use
intensional (i.e., in terms of procedures) and extensional (in terms of listing
the elements) representations of conditional probabilities and of the
intermediate factors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2519</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2519</id><created>2012-10-19</created><authors><author><keyname>Costa</keyname><forenames>Vitor Santos</forenames></author><author><keyname>Page</keyname><forenames>David</forenames></author><author><keyname>Qazi</keyname><forenames>Maleeha</forenames></author><author><keyname>Cussens</keyname><forenames>James</forenames></author></authors><title>CLP(BN): Constraint Logic Programming for Probabilistic Knowledge</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Nineteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2003)</comments><proxy>auai</proxy><report-no>UAI-P-2003-PG-517-524</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present CLP(BN), a novel approach that aims at expressing Bayesian
networks through the constraint logic programming framework. Arguably, an
important limitation of traditional Bayesian networks is that they are
propositional, and thus cannot represent relations between multiple similar
objects in multiple contexts. Several researchers have thus proposed
first-order languages to describe such networks. Namely, one very successful
example of this approach are the Probabilistic Relational Models (PRMs), that
combine Bayesian networks with relational database technology. The key
difficulty that we had to address when designing CLP(cal{BN}) is that logic
based representations use ground terms to denote objects. With probabilitic
data, we need to be able to uniquely represent an object whose value we are not
sure about. We use {sl Skolem functions} as unique new symbols that uniquely
represent objects with unknown value. The semantics of CLP(cal{BN}) programs
then naturally follow from the general framework of constraint logic
programming, as applied to a specific domain where we have probabilistic data.
This paper introduces and defines CLP(cal{BN}), and it describes an
implementation and initial experiments. The paper also shows how CLP(cal{BN})
relates to Probabilistic Relational Models (PRMs), Ngo and Haddawys
Probabilistic Logic Programs, AND Kersting AND De Raedts Bayesian Logic
Programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2529</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2529</id><created>2012-12-11</created><authors><author><keyname>Cabarle</keyname><forenames>Francis George C.</forenames></author><author><keyname>Bu&#xf1;o</keyname><forenames>Kelvin C.</forenames></author><author><keyname>Adorna</keyname><forenames>Henry N.</forenames></author></authors><title>On The Delays In Spiking Neural P Systems</title><categories>cs.NE cs.DC cs.ET</categories><comments>Presented at the 6th Symposium on the Mathematical Aspects of
  Computer Science (SMACS2012), Boracay, Philippines. 6 figures, 6 pages, 2
  columns</comments><msc-class>97P20</msc-class><acm-class>F.4.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we extend and improve the results done in a previous work on
simulating Spiking Neural P systems (SNP systems in short) with delays using
SNP systems without delays. We simulate the former with the latter over
sequential, iteration, join, and split routing. Our results provide
constructions so that both systems halt at exactly the same time, start with
only one spike, and produce the same number of spikes to the environment after
halting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2531</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2531</id><created>2012-12-11</created><authors><author><keyname>Mohammed</keyname><forenames>Kabeer</forenames></author><author><keyname>Reddy</keyname><forenames>Dr. Bhaskara</forenames></author></authors><title>Enhanced Image Analysis Using Cached Mobile Robots</title><categories>cs.OH</categories><comments>6 Pages one flowchart</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the field of Artificial intelligence Image processing plays a vital role
in Decision making. Nowadays Mobile robots work as a Network sharing
Centralized Database. All Image inputs are compared against this database and
decision is made. In some cases the Centralized database is in other side of
the globe and Mobile robots compare Input image through satellite link this
sometime results in delays in decision making which may result in castrophe.
This Research paper is about how to make image processing in mobile robots less
time consuming and fast decision making. This research paper compares search
techniques employed currently and optimum search method which we are going to
state. Nowadays Mobile robots are extensively used in environments which are
dangerous to human beings. In this dangerous situations quick Decision making
makes the difference between Hit and Miss this can also results in Day to day
tasks performed by Mobile robots Successful or Failure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2537</identifier>
 <datestamp>2014-07-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2537</id><created>2012-12-11</created><authors><author><keyname>Renes</keyname><forenames>Joseph M.</forenames></author><author><keyname>Wilde</keyname><forenames>Mark M.</forenames></author></authors><title>Polar codes for private and quantum communication over arbitrary
  channels</title><categories>quant-ph cs.IT math.IT</categories><comments>14 pages; subsumes and extends the results of arXiv:1201.2906 and
  arXiv:1203.5794</comments><journal-ref>IEEE Transactions on Information Theory, vol. 60, no. 6, pages
  3090-3103, June 2014</journal-ref><doi>10.1109/TIT.2014.2314463</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We construct new polar coding schemes for the transmission of quantum or
private classical information over arbitrary quantum channels. In the former
case, our coding scheme achieves the symmetric coherent information and in the
latter the symmetric private information. Both schemes are built from a polar
coding construction capable of transmitting classical information over a
quantum channel [Wilde and Guha, IEEE Transactions on Information Theory, in
press]. Appropriately merging two such classical-quantum schemes, one for
transmitting &quot;amplitude&quot; information and the other for transmitting &quot;phase,&quot;
leads to the new private and quantum coding schemes, similar to the
construction for Pauli and erasure channels in [Renes, Dupuis, and Renner,
Physical Review Letters 109, 050504 (2012)]. The encoding is entirely similar
to the classical case, and thus efficient. The decoding can also be performed
by successive cancellation, as in the classical case, but no efficient
successive cancellation scheme is yet known for arbitrary quantum channels. An
efficient code construction is unfortunately still unknown. Generally, our two
coding schemes require entanglement or secret-key assistance, respectively, but
we extend two known conditions under which the needed assistance rate vanishes.
Finally, although our results are formulated for qubit channels, we show how
the scheme can be extended to multiple qubits. This then demonstrates a
near-explicit coding method for realizing one of the most striking phenomena in
quantum information theory: the superactivation effect, whereby two quantum
channels which individually have zero quantum capacity can have a non-zero
quantum capacity when used together.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2546</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2546</id><created>2012-12-11</created><authors><author><keyname>Masci</keyname><forenames>Jonathan</forenames></author><author><keyname>Angulo</keyname><forenames>Jes&#xfa;s</forenames></author><author><keyname>Schmidhuber</keyname><forenames>J&#xfc;rgen</forenames></author></authors><title>A Learning Framework for Morphological Operators using Counter-Harmonic
  Mean</title><categories>cs.CV</categories><comments>Submitted to ISMM'13</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel framework for learning morphological operators using
counter-harmonic mean. It combines concepts from morphology and convolutional
neural networks. A thorough experimental validation analyzes basic
morphological operators dilation and erosion, opening and closing, as well as
the much more complex top-hat transform, for which we report a real-world
application from the steel industry. Using online learning and stochastic
gradient descent, our system learns both the structuring element and the
composition of operators. It scales well to large datasets and online settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2547</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2547</id><created>2012-12-11</created><authors><author><keyname>Ramos</keyname><forenames>Marlon</forenames></author><author><keyname>Crokidakis</keyname><forenames>Nuno</forenames></author><author><keyname>Anteneodo</keyname><forenames>Celia</forenames></author></authors><title>Information spreading with aging in heterogeneous populations</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>12 pages, 4 figures, submitted for publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the critical properties of a model of information spreading based on
the SIS epidemic model. Spreading rates decay with time, as ruled by two
parameters, $\epsilon$ and $l$, that can be either constant or randomly
distributed in the population. The spreading dynamics is developed on top of
Erd\&quot;os-Renyi networks. We present the mean-field analytical solution of the
model in its simplest formulation, and Monte Carlo simulations are performed
for the more heterogeneous cases. The outcomes show that the system undergoes a
nonequilibrium phase transition whose critical point depends on the parameters
$\epsilon$ and $l$. In addition, we conclude that the more heterogeneous the
population, the more favored the information spreading over the network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2549</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2549</id><created>2012-12-11</created><authors><author><keyname>Saranurak</keyname><forenames>Thatchaphol</forenames></author><author><keyname>Jindal</keyname><forenames>Gorav</forenames></author></authors><title>Subtraction makes computing integers faster</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show some facts regarding the question whether, for any number $n$, the
length of the shortest Addition Multiplications Chain (AMC) computing $n$ is
polynomial in the length of the shortest division-free Straight Line Program
(SLP) that computes $n$.
  If the answer to this question is &quot;yes&quot;, then we can show a stronger upper
bound for $\mathrm{PosSLP}$, the important problem which essentially captures
the notion of efficient computation over the reals. If the answer is &quot;no&quot;, then
this would demonstrate how subtraction helps generating integers
super-polynomially faster, given that addition and multiplication can be done
in unit time.
  In this paper, we show that, for almost all numbers, AMCs and SLPs need same
asymptotic length for computation. However, for one specific form of numbers,
SLPs are strictly more powerful than AMCs by at least one step of computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2563</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2563</id><created>2012-12-11</created><authors><author><keyname>Muniyal</keyname><forenames>Balachandra</forenames></author><author><keyname>Prakash</keyname><forenames>Krishna</forenames></author><author><keyname>Sharma</keyname><forenames>Shashank</forenames></author></authors><title>Wireless Public key Infrastructure for Mobile Phones</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile devices play an important role in the wireless network environment for
providing different services over internet. The business transactions over
wireless electronic devices are not secure and hence the messages are prone to
be intercepted and modified by an intruder. So, devices supporting wireless
internet must be guaranteed at the same level of security as the wired network.
PKI (Public Key Infrastructure) used in the wired environment is not suitable
for wireless environment because of the less powerful processor and small
memory. This arises a need for the development of a Wireless Public Key
Infrastructure (WPKI) that provides the similar security level as the wired PKI
suitable for mobile phone. In this paper, a discussion of public key
infrastructure and an experimental set up for Wireless Public key
Infrastructure for mobile phones are made.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2567</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2567</id><created>2012-12-11</created><authors><author><keyname>Tengviel</keyname><forenames>John</forenames></author><author><keyname>Diawuo</keyname><forenames>K.</forenames></author><author><keyname>Dotche</keyname><forenames>K. A.</forenames></author></authors><title>The effect of the number of mobile nodes on varying speeds of manets</title><categories>cs.NI</categories><comments>10 pages, 4 figures</comments><journal-ref>International Journal of Network Security &amp; Its Applications
  (IJNSA), Vol.4, No.6, November 2012</journal-ref><doi>10.5121/ijnsa.2012.4607</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile Ad hoc Networks are dynamic networks populated by mobile devices, or
mobile nodes.The Mobile Nodes are free to move anywhere and at any time. The
population of the nodes may have some influence on the mobility rate of the
mobile nodes. This paper presents simulation results using Matrix Laboratory
software. The study investigates the influence of mobile nodes parameters such
as number of nodes on the nodes speeds and nodes distribution in a given area.
The results have indicated that the number of mobile nodes have impact on the
speeds of the nodes in a location.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2573</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2573</id><created>2012-12-11</created><authors><author><keyname>Kumar</keyname><forenames>K. S. Sesh</forenames><affiliation>LIENS, INRIA Paris - Rocquencourt</affiliation></author><author><keyname>Bach</keyname><forenames>Francis</forenames><affiliation>LIENS, INRIA Paris - Rocquencourt</affiliation></author></authors><title>Convex Relaxations for Learning Bounded Treewidth Decomposable Graphs</title><categories>cs.LG cs.DS stat.ML</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of learning the structure of undirected graphical
models with bounded treewidth, within the maximum likelihood framework. This is
an NP-hard problem and most approaches consider local search techniques. In
this paper, we pose it as a combinatorial optimization problem, which is then
relaxed to a convex optimization problem that involves searching over the
forest and hyperforest polytopes with special structures, independently. A
supergradient method is used to solve the dual problem, with a run-time
complexity of $O(k^3 n^{k+2} \log n)$ for each iteration, where $n$ is the
number of variables and $k$ is a bound on the treewidth. We compare our
approach to state-of-the-art methods on synthetic datasets and classical
benchmarks, showing the gains of the novel convex approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2582</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2582</id><created>2012-12-11</created><authors><author><keyname>Sengupta</keyname><forenames>Madhumita</forenames></author><author><keyname>Mandal</keyname><forenames>J. K.</forenames></author></authors><title>Self Authentication of color image through Wavelet Transformation
  Technique (SAWT)</title><categories>cs.CR cs.MM</categories><comments>5Pages, Published in proceeding of International Conference on
  Computing and Systems, ISBN 93-80813-01-5, pp 151-155, University of Burdwan,
  19th 20th November, 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper a self organized legal document/content authentication,
copyright protection in composite domain has been proposed without using any
external information. Average values of transformed red and green components in
frequency domain generated through wavelet transform are embedded into the blue
component of the color image matrix in spatial domain. A reverse transformation
is made in RG matrix to obtain embedded image in association with blue
component in spatial domain. Reverse procedure is done during decoding where
transformed average values are obtained from red and green components and
compared with the same from blue component for authentication. Results are
compared with existing technique which shows better performance interns of
PSNR, MSE &amp; IF.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2587</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2587</id><created>2012-12-11</created><authors><author><keyname>Bouramoul</keyname><forenames>Abdelkrim</forenames></author><author><keyname>Kholladi</keyname><forenames>Mohamed-Khireddine</forenames></author><author><keyname>Doan</keyname><forenames>Bich-Li&#xea;n</forenames></author></authors><title>An ontology-based approach for semantics ranking of the web search
  engines results</title><categories>cs.IR</categories><comments>6 pages, 5 figures, appears in: (ICMCS), 2012 International
  Conference on Multimedia Computing and Systems, Print ISBN: 978-1-4673-1518-0</comments><journal-ref>In The 3rd International Conference on Multimedia Computing and
  Systems, IEEE. 797-802, Tanger, Morocco, 2012</journal-ref><doi>10.1109/ICMCS.2012.6320318</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work falls in the areas of information retrieval and semantic web, and
aims to improve the evaluation of web search tools. Indeed, the huge number of
information on the web as well as the growth of new inexperienced users creates
new challenges for information retrieval; certainly the current search engines
(such as Google, Bing and Yahoo) offer an efficient way to browse the web
content. However, this type of tool does not take into account the semantic
driven by the query terms and document words. This paper proposes a new
semantic based approach for the evaluation of information retrieval systems;
the goal is to increase the selectivity of search tools and to improve how
these tools are evaluated. The test of the proposed approach for the evaluation
of search engines has proved its applicability to real search tools. The
results showed that semantic evaluation is a promising way to improve the
performance and behavior of search engines as well as the relevance of the
results that they return.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2591</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2591</id><created>2012-12-11</created><authors><author><keyname>Muharar</keyname><forenames>Rusdha</forenames></author><author><keyname>Zakhour</keyname><forenames>Randa</forenames></author><author><keyname>Evans</keyname><forenames>Jamie</forenames></author></authors><title>Base Station Cooperation with Feedback Optimization: A Large System
  Analysis</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study feedback optimization problems that maximize the
users' signal to interference plus noise ratio (SINR) in a two-cell MIMO
broadcast channel. Assuming the users learn their direct and interfering
channels perfectly, they can feed back this information to the base stations
(BSs) over the uplink channels. The BSs then use the channel information to
design their transmission scheme. Two types of feedback are considered: analog
and digital. In the analog feedback case, the users send their unquantized and
uncoded CSI over the uplink channels. In this context, given a user's fixed
transmit power, we investigate how he/she should optimally allocate it to feed
back the direct and interfering (or cross) CSI for two types of base station
cooperation schemes, namely, Multi-Cell Processing (MCP) and Coordinated
Beamforming (CBf). In the digital feedback case, the direct and cross link
channel vectors of each user are quantized separately, each using RVQ, with
different size codebooks. The users then send the index of the quantization
vector in the corresponding codebook to the BSs. Similar to the feedback
optimization problem in the analog feedback, we investigate the optimal bit
partitioning for the direct and interfering link for both types of cooperation.
We focus on regularized channel inversion precoding structures and perform our
analysis in the large system limit in which the number of users per cell ($K$)
and the number of antennas per BS ($N$) tend to infinity with their ratio
$\beta=\frac{K}{N}$ held fixed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2607</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2607</id><created>2012-12-11</created><authors><author><keyname>Touri</keyname><forenames>Behrouz</forenames><affiliation>Hassanzadeh</affiliation></author><author><keyname>Farnoud</keyname><forenames>Farzad</forenames><affiliation>Hassanzadeh</affiliation></author><author><keyname>Neidic</keyname><forenames>Angelia</forenames></author><author><keyname>Milenkovic</keyname><forenames>Olgica</forenames></author></authors><title>A General Framework for Distributed Vote Aggregation</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a general model for opinion dynamics in a social network together
with several possibilities for object selections at times when the agents are
communicating. We study the limiting behavior of such a dynamics and show that
this dynamics almost surely converges. We consider some special implications of
the convergence result for gossip and top-$k$ selective gossip models. In
particular, we provide an answer to the open problem of the convergence
property of the top-$k$ selective gossip model, and show that the convergence
holds in a much more general setting. Moreover, we propose an extension of the
gossip and top-$k$ selective gossip models and provide some results for their
limiting behavior.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2614</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2614</id><created>2012-12-11</created><authors><author><keyname>Voskoglou</keyname><forenames>Michael Gr.</forenames></author></authors><title>A Study on Fuzzy Systems</title><categories>cs.AI</categories><comments>9 pages, 3 figures, 1 table</comments><msc-class>03E72</msc-class><journal-ref>American Journal of Computational and Applied Mathematics, 2(5),
  232-240, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We use princiles of fuzzy logic to develop a general model representing
several processes in a system's operation characterized by a degree of
vagueness and/or uncertainy. Further, we introduce three altenative measures of
a fuzzy system's effectiveness connected to the above model. An applcation is
also developed for the Mathematical Modelling process illustrating our results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2616</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2616</id><created>2012-12-11</created><authors><author><keyname>Petersen</keyname><forenames>Alexander M.</forenames></author><author><keyname>Tenenbaum</keyname><forenames>Joel N.</forenames></author><author><keyname>Havlin</keyname><forenames>Shlomo</forenames></author><author><keyname>Stanley</keyname><forenames>H. Eugene</forenames></author><author><keyname>Perc</keyname><forenames>Matjaz</forenames></author></authors><title>Languages cool as they expand: Allometric scaling and the decreasing
  need for new words</title><categories>physics.soc-ph cond-mat.stat-mech cs.CL stat.AP</categories><comments>9 two-column pages, 7 figures; accepted for publication in Scientific
  Reports</comments><journal-ref>Sci. Rep. 2 (2012) 943</journal-ref><doi>10.1038/srep00943</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We analyze the occurrence frequencies of over 15 million words recorded in
millions of books published during the past two centuries in seven different
languages. For all languages and chronological subsets of the data we confirm
that two scaling regimes characterize the word frequency distributions, with
only the more common words obeying the classic Zipf law. Using corpora of
unprecedented size, we test the allometric scaling relation between the corpus
size and the vocabulary size of growing languages to demonstrate a decreasing
marginal need for new words, a feature that is likely related to the underlying
correlations between words. We calculate the annual growth fluctuations of word
use which has a decreasing trend as the corpus size increases, indicating a
slowdown in linguistic evolution following language expansion. This &quot;cooling
pattern&quot; forms the basis of a third statistical regularity, which unlike the
Zipf and the Heaps law, is dynamical in nature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2617</identifier>
 <datestamp>2012-12-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2617</id><created>2012-12-11</created><authors><author><keyname>Hulme</keyname><forenames>William</forenames></author><author><keyname>Richt&#xe1;rik</keyname><forenames>Peter</forenames></author><author><keyname>McGuire</keyname><forenames>Lynne</forenames></author><author><keyname>Green</keyname><forenames>Alison</forenames></author></authors><title>Optimal diagnostic tests for sporadic Creutzfeldt-Jakob disease based on
  support vector machine classification of RT-QuIC data</title><categories>q-bio.QM cs.LG stat.AP</categories><comments>32 pages, 12 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we study numerical construction of optimal clinical diagnostic
tests for detecting sporadic Creutzfeldt-Jakob disease (sCJD). A cerebrospinal
fluid sample (CSF) from a suspected sCJD patient is subjected to a process
which initiates the aggregation of a protein present only in cases of sCJD.
This aggregation is indirectly observed in real-time at regular intervals, so
that a longitudinal set of data is constructed that is then analysed for
evidence of this aggregation. The best existing test is based solely on the
final value of this set of data, which is compared against a threshold to
conclude whether or not aggregation, and thus sCJD, is present. This test
criterion was decided upon by analysing data from a total of 108 sCJD and
non-sCJD samples, but this was done subjectively and there is no supporting
mathematical analysis declaring this criterion to be exploiting the available
data optimally. This paper addresses this deficiency, seeking to validate or
improve the test primarily via support vector machine (SVM) classification.
Besides this, we address a number of additional issues such as i) early
stopping of the measurement process, ii) the possibility of detecting the
particular type of sCJD and iii) the incorporation of additional patient data
such as age, sex, disease duration and timing of CSF sampling into the
construction of the test.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2654</identifier>
 <datestamp>2012-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2654</id><created>2012-12-11</created><authors><author><keyname>Kas</keyname><forenames>M.</forenames></author><author><keyname>Appala</keyname><forenames>S.</forenames></author><author><keyname>Wang</keyname><forenames>C.</forenames></author><author><keyname>Carley</keyname><forenames>C.</forenames></author><author><keyname>Carley</keyname><forenames>L. R.</forenames></author><author><keyname>Tonguz</keyname><forenames>O. K.</forenames></author></authors><title>What if Routers Were Social? Analyzing Wireless Mesh Networks from a
  Social Networks Perspective</title><categories>cs.NI</categories><comments>IEEE Wireless Communications, vol. 19, no. 6, December 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless Mesh Networks (WMNs) consist of radio nodes organized in a mesh
topology for serving wireless mesh clients to communicate with one another or
to connect to the Internet. Nodes in a mesh network can communicate with each
other either directly or through one or more intermediate nodes, similar to
social networks. WMNs share many common properties with social networks. We
first identify the differences and similarities between social networks and
WMNs and then use metrics that are typically used for social network analysis
(SNA) to assess real WMNs. Analyzing real WMN data collected from the UCSB
MeshNet and MIT Roofnet testbeds reveals that using SNA metrics are helpful in
designing WMNs with better performance. We demonstrate the validity of our
conclusions and this new approach by focusing on two sample applications of
social networks: network reliability assessment and channel access scheduling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2657</identifier>
 <datestamp>2012-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2657</id><created>2012-12-11</created><authors><author><keyname>Ryabokon</keyname><forenames>Anna</forenames></author></authors><title>Study: Symmetry breaking for ASP</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In their nature configuration problems are combinatorial (optimization)
problems. In order to find a configuration a solver has to instantiate a number
of components of a some type and each of these components can be used in a
relation defined for a type. Therefore, many solutions of a configuration
problem have symmetric ones which can be obtained by replacing some component
of a solution by another one of the same type. These symmetric solutions
decrease performance of optimization algorithms because of two reasons: a) they
satisfy all requirements and cannot be pruned out from the search space; and b)
existence of symmetric optimal solutions does not allow to prove the optimum in
a feasible time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2668</identifier>
 <datestamp>2012-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2668</id><created>2012-12-11</created><authors><author><keyname>Kontoyiannis</keyname><forenames>Ioannis</forenames></author><author><keyname>Verdu</keyname><forenames>Sergio</forenames></author></authors><title>Lossless Data Compression at Finite Blocklengths</title><categories>cs.IT math.IT math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper provides an extensive study of the behavior of the best achievable
rate (and other related fundamental limits) in variable-length lossless
compression. In the non-asymptotic regime, the fundamental limits of
fixed-to-variable lossless compression with and without prefix constraints are
shown to be tightly coupled. Several precise, quantitative bounds are derived,
connecting the distribution of the optimal codelengths to the source
information spectrum, and an exact analysis of the best achievable rate for
arbitrary sources is given.
  Fine asymptotic results are proved for arbitrary (not necessarily prefix)
compressors on general mixing sources. Non-asymptotic, explicit Gaussian
approximation bounds are established for the best achievable rate on Markov
sources. The source dispersion and the source varentropy rate are defined and
characterized. Together with the entropy rate, the varentropy rate serves to
tightly approximate the fundamental non-asymptotic limits of fixed-to-variable
compression for all but very small blocklengths.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2669</identifier>
 <datestamp>2012-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2669</id><created>2012-12-11</created><authors><author><keyname>Soria</keyname><forenames>Francisco Rub&#xe9;n Castillo</forenames></author><author><keyname>Torres</keyname><forenames>Gustavo Fern&#xe1;ndez</forenames></author><author><keyname>Algredo-Badillo</keyname><forenames>Ignacio</forenames></author></authors><title>A Lossless Data Hiding Technique based on AES-DWT</title><categories>cs.CR</categories><comments>9 pages, 15 figures, 2 tables; IJCSI International Journal of
  Computer Science Issues, Vol. 9, Issue 5, No 3, September 2012. ISSN
  (Online): 1694-0814. www.IJCSI.org</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a new data hiding technique. The new technique uses
steganography and cryptography on images with a size of 256x256 pixels and an
8-bit grayscale format. There are design restrictions such as a fixed-size
cover image, and reconstruction without error of the hidden image. The
steganography technique uses a Haar-DWT (Discrete Wavelet Transform) with hard
thresholding and LSB (Less Significant Bit) technique on the cover image. The
algorithms used for compressing and ciphering the secret image are lossless JPG
and AES, respectively. The proposed technique is used to generate a stego image
which provides a double type of security that is robust against attacks.
Results are reported for different thresholds levels in terms of PSNR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2671</identifier>
 <datestamp>2012-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2671</id><created>2012-12-11</created><authors><author><keyname>P&#xe9;rez</keyname><forenames>Ernesto Cort&#xe9;s</forenames></author><author><keyname>Algredo-Badillo</keyname><forenames>Ignacio</forenames></author><author><keyname>Rodr&#xed;guez</keyname><forenames>V&#xed;ctor Hugo Garc&#xed;a</forenames></author></authors><title>Performance Analysis of ANFIS in short term Wind Speed Prediction</title><categories>cs.AI</categories><comments>9 pages, 11 figures, 1 table; IJCSI International Journal of Computer
  Science Issues, Vol. 9, Issue 5, No 3, September 2012. ISSN (Online):
  1694-0814. www.IJCSI.org</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Results are presented on the performance of Adaptive Neuro-Fuzzy Inference
system (ANFIS) for wind velocity forecasts in the Isthmus of Tehuantepec region
in the state of Oaxaca, Mexico. The data bank was provided by the
meteorological station located at the University of Isthmus, Tehuantepec
campus, and this data bank covers the period from 2008 to 2011. Three data
models were constructed to carry out 16, 24 and 48 hours forecasts using the
following variables: wind velocity, temperature, barometric pressure, and date.
The performance measure for the three models is the mean standard error (MSE).
In this work, performance analysis in short-term prediction is presented,
because it is essential in order to define an adequate wind speed model for
eolian parks, where a right planning provide economic benefits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2676</identifier>
 <datestamp>2012-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2676</id><created>2012-12-11</created><authors><author><keyname>Gerow</keyname><forenames>Aaron</forenames></author><author><keyname>Keane</keyname><forenames>Mark</forenames></author></authors><title>Mining the Web for the Voice of the Herd to Track Stock Market Bubbles</title><categories>cs.CL cs.IR physics.soc-ph q-fin.GN</categories><comments>Proceedings of the 22nd International Joint Conference on Artificial
  Intelligence (IJCAI '11), Barcelona, Spain, 16-22 July, 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that power-law analyses of financial commentaries from newspaper
web-sites can be used to identify stock market bubbles, supplementing
traditional volatility analyses. Using a four-year corpus of 17,713 online,
finance-related articles (10M+ words) from the Financial Times, the New York
Times, and the BBC, we show that week-to-week changes in power-law
distributions reflect market movements of the Dow Jones Industrial Average
(DJI), the FTSE-100, and the NIKKEI-225. Notably, the statistical regularities
in language track the 2007 stock market bubble, showing emerging structure in
the language of commentators, as progressively greater agreement arose in their
positive perceptions of the market. Furthermore, during the bubble period, a
marked divergence in positive language occurs as revealed by a Kullback-Leibler
analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2686</identifier>
 <datestamp>2012-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2686</id><created>2012-12-11</created><authors><author><keyname>Goodfellow</keyname><forenames>Ian</forenames></author><author><keyname>Courville</keyname><forenames>Aaron</forenames></author><author><keyname>Bengio</keyname><forenames>Yoshua</forenames></author></authors><title>Joint Training of Deep Boltzmann Machines</title><categories>stat.ML cs.LG</categories><comments>4 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new method for training deep Boltzmann machines jointly. Prior
methods require an initial learning pass that trains the deep Boltzmann machine
greedily, one layer at a time, or do not perform well on classifi- cation
tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2692</identifier>
 <datestamp>2012-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2692</id><created>2012-12-11</created><authors><author><keyname>Osman</keyname><forenames>Ghazali</forenames></author><author><keyname>Hitam</keyname><forenames>Muhammad Suzuri</forenames></author><author><keyname>Ismail</keyname><forenames>Mohd Nasir</forenames></author></authors><title>Enhanced skin colour classifier using RGB Ratio model</title><categories>cs.CV</categories><comments>14 pages; International Journal on Soft Computing (IJSC) Vol.3, No.4,
  November 2012</comments><msc-class>68T10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Skin colour detection is frequently been used for searching people, face
detection, pornographic filtering and hand tracking. The presence of skin or
non-skin in digital image can be determined by manipulating pixels colour or
pixels texture. The main problem in skin colour detection is to represent the
skin colour distribution model that is invariant or least sensitive to changes
in illumination condition. Another problem comes from the fact that many
objects in the real world may possess almost similar skin-tone colour such as
wood, leather, skin-coloured clothing, hair and sand. Moreover, skin colour is
different between races and can be different from a person to another, even
with people of the same ethnicity. Finally, skin colour will appear a little
different when different types of camera are used to capture the object or
scene. The objective in this study is to develop a skin colour classifier based
on pixel-based using RGB ratio model. The RGB ratio model is a newly proposed
method that belongs under the category of an explicitly defined skin region
model. This skin classifier was tested with SIdb dataset and two benchmark
datasets; UChile and TDSD datasets to measure classifier performance. The
performance of skin classifier was measured based on true positive (TF) and
false positive (FP) indicator. This newly proposed model was compared with
Kovac, Saleh and Swift models. The experimental results showed that the RGB
ratio model outperformed all the other models in term of detection rate. The
RGB ratio model is able to reduce FP detection that caused by reddish objects
colour as well as be able to detect darkened skin and skin covered by shadow.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2693</identifier>
 <datestamp>2013-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2693</id><created>2012-12-11</created><updated>2013-02-14</updated><authors><author><keyname>Perevalov</keyname><forenames>Eugene</forenames></author><author><keyname>Grace</keyname><forenames>David</forenames></author></authors><title>Towards the full information chain theory: question difficulty</title><categories>physics.data-an cs.IT math.IT</categories><comments>39 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A general problem of optimal information acquisition for its use in decision
making problems is considered. This motivates the need for developing
quantitative measures of information sources' capabilities for supplying
accurate information depending on the particular content of the latter. In this
article, the notion of a real valued difficulty functional for questions
identified with partitions of problem parameter space is introduced and the
overall form of this functional is derived that satisfies a particular system
of reasonable postulates. It is found that, in an isotropic case, the resulting
difficulty functional depends on a single scalar function on the parameter
space that can be interpreted -- using parallels with classical thermodynamics
-- as a temperature-like quantity, with the question difficulty itself playing
the role of thermal energy. Quantitative relationships between difficulty
functionals of different questions are also explored.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2696</identifier>
 <datestamp>2013-02-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2696</id><created>2012-12-11</created><updated>2013-02-14</updated><authors><author><keyname>Perevalov</keyname><forenames>Eugene</forenames></author><author><keyname>Grace</keyname><forenames>David</forenames></author></authors><title>Towards the full information chain theory: answer depth and source
  models</title><categories>physics.data-an cs.IT math.IT</categories><comments>45 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A problem of optimal information acquisition for its use in general decision
making problems is considered. This motivates the need for developing
quantitative measures of information sources' capabilities for supplying
accurate information depending on the particular content of the latter. A
companion article developed the notion of a question difficulty functional for
questions concerning input data for a decision making problem. Here, answers
which an information source may provide in response to such questions are
considered. In particular, a real valued answer depth functional measuring the
degree of accuracy of such answers is introduced and its overall form is
derived under the assumption of isotropic knowledge structure of the
information source. Additionally, information source models that relate answer
depth to question difficulty are discussed. It turns out to be possible to
introduce a notion of an information source capacity as the highest value of
the answer depth the source is capable of providing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2701</identifier>
 <datestamp>2012-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2701</id><created>2012-12-11</created><authors><author><keyname>Gharan</keyname><forenames>Shayan Oveis</forenames></author><author><keyname>Trevisan</keyname><forenames>Luca</forenames></author></authors><title>A Universal upper bound on Graph Diameter based on Laplacian Eigenvalues</title><categories>cs.DM math.CO math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove that the diameter of any unweighted connected graph G is O(k log
n/lambda_k), for any k&gt;= 2. Here, lambda_k is the k smallest eigenvalue of the
normalized laplacian of G. This solves a problem posed by Gil Kalai.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2720</identifier>
 <datestamp>2012-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2720</id><created>2012-12-12</created><authors><author><keyname>Rao</keyname><forenames>R. Velumadhava</forenames></author><author><keyname>Selvamani</keyname><forenames>K.</forenames></author><author><keyname>Elakkiya</keyname><forenames>R.</forenames></author></authors><title>A secure key transfer protocol for group communication</title><categories>cs.CR cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Providing security for messages in group communication is more essential and
critical nowadays. In group oriented applications such as Video conferencing
and entertainment applications, it is necessary to secure the confidential data
in such a way that intruders are not able to modify or transmit the data. Key
transfer protocols fully rely on trusted Key Generation Center (KGC) to compute
group key and to transport the group keys to all communication parties in a
secured and secret manner. In this paper, an efficient key generation and key
transfer protocol has been proposed where KGC can broadcast group key
information to all group members in a secure way. Hence, only authorized group
members will be able to retrieve the secret key and unauthorized members cannot
retrieve the secret key. Hence, inorder to maintain the forward and backward
secrecy, the group keys are updated whenever a new member joins or leaves the
communication group. The proposed algorithm is more efficient and relies on NP
class. In addition, the keys are distributed to the group users in a safe and
secure way. Moreover, the key generated is also very strong since it uses
cryptographic techniques which provide efficient computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2725</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2725</id><created>2012-12-12</created><updated>2013-10-21</updated><authors><author><keyname>Xi</keyname><forenames>Feng</forenames></author><author><keyname>Chen</keyname><forenames>Sheng Yao</forenames></author><author><keyname>Liu</keyname><forenames>Zhong</forenames></author></authors><title>Chaotic Analog-to-Information Conversion: Principle and
  Reconstructability with Parameter Identifiability</title><categories>cs.IT math.IT nlin.CD</categories><comments>36 pages,13 figures</comments><doi>10.1142/S0218127413501988</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a chaos-based analog-to-information conversion system for
the acquisition and reconstruction of sparse analog signals. The sparse signal
acts as an excitation term of a continuous-time chaotic system and the
compressive measurements are performed by sampling chaotic system outputs. The
reconstruction is realized through the estimation of the sparse coefficients
with principle of chaotic parameter estimation. With the deterministic
formulation, the analysis on the reconstructability is conducted via the
sensitivity matrix from the parameter identifiability of chaotic systems. For
the sparsity-regularized nonlinear least squares estimation, it is shown that
the sparse signal is locally reconstructable if the columns of the
sparsity-regularized sensitivity matrix are linearly independent. A Lorenz
system excited by the sparse multitone signal is taken as an example to
illustrate the principle and the performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2744</identifier>
 <datestamp>2012-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2744</id><created>2012-12-12</created><authors><author><keyname>Agosta</keyname><forenames>John Mark</forenames></author><author><keyname>Chandrashekar</keyname><forenames>Jaideep</forenames></author><author><keyname>Crovella</keyname><forenames>Mark</forenames></author><author><keyname>Taft</keyname><forenames>Nina</forenames></author><author><keyname>Ting</keyname><forenames>Daniel</forenames></author></authors><title>Mixture Models of Endhost Network Traffic</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we focus on modeling a little studied type of traffic, namely
the network traffic generated from endhosts. We introduce a parsimonious
parametric model of the marginal distribution for connection arrivals. We
employ mixture models based on a convex combination of component distributions
with both heavy and light-tails. These models can be fitted with high accuracy
using maximum likelihood techniques. Our methodology assumes that the
underlying user data can be fitted to one of many modeling options, and we
apply Bayesian model selection criteria as a rigorous way to choose the
preferred combination of components. Our experiments show that a simple
Pareto-exponential mixture model is preferred for a wide range of users, over
both simpler and more complex alternatives. This model has the desirable
property of modeling the entire distribution, effectively segmenting the
traffic into the heavy-tailed as well as the non-heavy-tailed components. We
illustrate that this technique has the flexibility to capture the wide
diversity of user behaviors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2747</identifier>
 <datestamp>2013-08-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2747</id><created>2012-12-12</created><updated>2013-08-08</updated><authors><author><keyname>Berkholz</keyname><forenames>Christoph</forenames></author><author><keyname>Krebs</keyname><forenames>Andreas</forenames></author><author><keyname>Verbitsky</keyname><forenames>Oleg</forenames></author></authors><title>Bounds for the quantifier depth in finite-variable logics: Alternation
  hierarchy</title><categories>cs.LO</categories><comments>28 pages, 7 figures. Section 7 is expanded</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given two structures $G$ and $H$ distinguishable in $\fo k$ (first-order
logic with $k$ variables), let $A^k(G,H)$ denote the minimum alternation depth
of a $\fo k$ formula distinguishing $G$ from $H$. Let $A^k(n)$ be the maximum
value of $A^k(G,H)$ over $n$-element structures. We prove the strictness of the
quantifier alternation hierarchy of $\fo 2$ in a strong quantitative form,
namely $A^2(n)\ge n/8-2$, which is tight up to a constant factor. For each
$k\ge2$, it holds that $A^k(n)&gt;\log_{k+1}n-2$ even over colored trees, which is
also tight up to a constant factor if $k\ge3$. For $k\ge 3$ the last lower
bound holds also over uncolored trees, while the alternation hierarchy of $\fo
2$ collapses even over all uncolored graphs.
  We also show examples of colored graphs $G$ and $H$ on $n$ vertices that can
be distinguished in $\fo 2$ much more succinctly if the alternation number is
increased just by one: while in $\Sigma_{i}$ it is possible to distinguish $G$
from $H$ with bounded quantifier depth, in $\Pi_{i}$ this requires quantifier
depth $\Omega(n^2)$. The quadratic lower bound is best possible here because,
if $G$ and $H$ can be distinguished in $\fo k$ with $i$ quantifier
alternations, this can be done with quantifier depth $n^{2k-2}$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2752</identifier>
 <datestamp>2013-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2752</id><created>2012-12-12</created><updated>2013-06-24</updated><authors><author><keyname>Tajan</keyname><forenames>Romain</forenames></author><author><keyname>Poulliat</keyname><forenames>Charly</forenames></author><author><keyname>Fijalkow</keyname><forenames>Inbar</forenames></author></authors><title>Secondary Resource Allocation for Opportunistic Spectrum Sharing with
  IR-HARQ based Primary Users</title><categories>cs.IT math.IT</categories><comments>This paper has been withdrawn by the author</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose to address the problem of a secondary resource allocation when a
primary Incremental Redundancy Hybrid Automatic Repeat reQuest (IR-HARQ)
protocol. The Secondary Users (SUs) intend to use their knowledge of the
IR-HARQ protocol to maximize their long-term throughput under a constraint of
minimal Primary Users (PUs) throughput. The ACcumulated Mutual Information
(ACMI), required to model the primary IR-HARQ protocol, is used to define a
Constrained Markov Decision Process (CMDP). The SUs resource allocation is then
shown to be a solution of this CMDP. The allocation problem is then considered
as an infinite dimensional space linear programming. Solving the dual of this
linear programming is similar to solving an unconstrained MDP. A solution is
finally given using the Relative Value Iteration (RVI) algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2762</identifier>
 <datestamp>2012-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2762</id><created>2012-12-12</created><authors><author><keyname>Stone</keyname><forenames>Christopher</forenames></author><author><keyname>Toth</keyname><forenames>Rita</forenames></author><author><keyname>Costello</keyname><forenames>Ben de Lacy</forenames></author><author><keyname>Bull</keyname><forenames>Larry</forenames></author><author><keyname>Adamatzky</keyname><forenames>Andrew</forenames></author></authors><title>Coevolving Cellular Automata with Memory for Chemical Computing: Boolean
  Logic Gates in the B-Z Reaction</title><categories>cs.ET nlin.CG</categories><journal-ref>Proceedings of the 10th international conference on Parallel
  Problem Solving from Nature: PPSN X. Pages 579-588 (2008)</journal-ref><doi>10.1007/978-3-540-87700-4_58</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose that the behaviour of non-linear media can be controlled
automatically through coevolutionary systems. By extension, forms of
unconventional computing, i.e., massively parallel non-linear computers, can be
realised by such an approach. In this study a light-sensitive sub-excitable
Belousov-Zhabotinsky reaction is controlled using various heterogeneous
cellular automata. A checkerboard image comprising of varying light intensity
cells is projected onto the surface of a catalyst-loaded gel resulting in rich
spatio-temporal chemical wave behaviour. The coevolved cellular automata are
shown to be able to control chemical activity through dynamic control of the
light intensity. The approach is demonstrated through the creation of a number
of simple Boolean logic gates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2767</identifier>
 <datestamp>2012-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2767</id><created>2012-12-12</created><authors><author><keyname>Psorakis</keyname><forenames>Ioannis</forenames></author><author><keyname>Rezek</keyname><forenames>Iead</forenames></author><author><keyname>Frankel</keyname><forenames>Zach</forenames></author><author><keyname>Roberts</keyname><forenames>Stephen J.</forenames></author></authors><title>Bayesian one-mode projection for dynamic bipartite graphs</title><categories>stat.ML cond-mat.stat-mech cs.LG</categories><comments>11 pages, 5 figures</comments><report-no>PARG 12-12(1)</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a Bayesian methodology for one-mode projecting a bipartite network
that is being observed across a series of discrete time steps. The resulting
one mode network captures the uncertainty over the presence/absence of each
link and provides a probability distribution over its possible weight values.
Additionally, the incorporation of prior knowledge over previous states makes
the resulting network less sensitive to noise and missing observations that
usually take place during the data collection process. The methodology consists
of computationally inexpensive update rules and is scalable to large problems,
via an appropriate distributed implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2778</identifier>
 <datestamp>2012-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2778</id><created>2012-12-12</created><authors><author><keyname>Bonifaci</keyname><forenames>Vincenzo</forenames></author><author><keyname>Marchetti-Spaccamela</keyname><forenames>Alberto</forenames></author><author><keyname>Stiller</keyname><forenames>Sebastian</forenames></author><author><keyname>Wiese</keyname><forenames>Andreas</forenames></author></authors><title>Feasibility Tests for Recurrent Real-Time Tasks in the Sporadic DAG
  Model</title><categories>cs.DS cs.OS</categories><acm-class>D.4.1; F.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A model has been proposed in [Baruah et al., in Proceedings of the IEEE
Real-Time Systems Symposium 2012] for representing recurrent
precedence-constrained tasks to be executed on multiprocessor platforms, where
each recurrent task is modeled by a directed acyclic graph (DAG), a period, and
a relative deadline. Each vertex of the DAG represents a sequential job, while
the edges of the DAG represent precedence constraints between these jobs. All
the jobs of the DAG are released simultaneously and have to be completed within
some specified relative deadline. The task may release jobs in this manner an
unbounded number of times, with successive releases occurring at least the
specified period apart. The feasibility problem is to determine whether such a
recurrent task can be scheduled to always meet all deadlines on a specified
number of dedicated processors.
  The case of a single task has been considered in [Baruah et al., 2012]. The
main contribution of this paper is to consider the case of multiple tasks. We
show that EDF has a speedup bound of 2-1/m, where m is the number of
processors. Moreover, we present polynomial and pseudopolynomial schedulability
tests, of differing effectiveness, for determining whether a set of sporadic
DAG tasks can be scheduled by EDF to meet all deadlines on a specified number
of processors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2788</identifier>
 <datestamp>2013-07-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2788</id><created>2012-12-12</created><updated>2013-07-02</updated><authors><author><keyname>Buesser</keyname><forenames>Pierre</forenames></author><author><keyname>Tomassini</keyname><forenames>Marco</forenames></author></authors><title>Evolution of Cooperation on Spatially Embedded Networks</title><categories>physics.soc-ph cs.SI</categories><comments>arXiv admin note: text overlap with arXiv:1201.6257</comments><journal-ref>Physical Review E, 86, 066107, 2012</journal-ref><doi>10.1103/PhysRevE.86.066107</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work we study the behavior of classical two-person, two-strategies
evolutionary games on networks embedded in a Euclidean two-dimensional space
with different kinds of degree distributions and topologies going from regular
to random, and to scale-free ones. Using several imitative microscopic
dynamics, we study the evolution of global cooperation on the above network
classes and find that specific topologies having a hierarchical structure and
an inhomogeneous degree distribution, such as Apollonian and grid-based
networks, are very conducive to cooperation. Spatial scale-free networks are
still good for cooperation but to a lesser degree. Both classes of networks
enhance average cooperation in all games with respect to standard random
geometric graphs and regular grids by shifting the boundaries between
cooperative and defective regions. These findings might be useful in the design
of interaction structures that maintain cooperation when the agents are
constrained to live in physical two-dimensional space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2789</identifier>
 <datestamp>2013-03-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2789</id><created>2012-12-12</created><authors><author><keyname>Choe</keyname><forenames>Changil</forenames></author><author><keyname>Hong</keyname><forenames>Hyejong</forenames></author><author><keyname>Kim</keyname><forenames>Kukhwan</forenames></author></authors><title>Formal Design and Verification of N-M Switching Control System</title><categories>cs.LO</categories><report-no>KISU-MATH-2012-E-R-008</report-no><journal-ref>Romanian Journal of Mathematics and Computer Science, Vol. 2, No.
  2, 2012, 36-43</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Production factories in which stable voltage is critical, e.g.,
electro-plating factory, require constantly stable voltage to minimize loss by
adjusting incoming voltage in real time, even if low-quality electricity is
supplied from outside. To solve such problem often being raised from the
factories located in the area with unstable electricity supply, we designed N-M
switching control system and verified its correctness using LTL model checking
technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2791</identifier>
 <datestamp>2012-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2791</id><created>2012-12-12</created><authors><author><keyname>Belanche</keyname><forenames>Llu&#xed;s A.</forenames></author></authors><title>Understanding (dis)similarity measures</title><categories>cs.AI cs.IR</categories><comments>10 pages, 2 figures</comments><report-no>LSI-12-16-R</report-no><acm-class>I.5.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Intuitively, the concept of similarity is the notion to measure an inexact
matching between two entities of the same reference set. The notions of
similarity and its close relative dissimilarity are widely used in many fields
of Artificial Intelligence. Yet they have many different and often partial
definitions or properties, usually restricted to one field of application and
thus incompatible with other uses. This paper contributes to the design and
understanding of similarity and dissimilarity measures for Artificial
Intelligence. A formal dual definition for each concept is proposed, joined
with a set of fundamental properties. The behavior of the properties under
several transformations is studied and revealed as an important matter to bear
in mind. We also develop several practical examples that work out the proposed
approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2823</identifier>
 <datestamp>2012-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2823</id><created>2012-12-12</created><authors><author><keyname>Song</keyname><forenames>Shuran</forenames></author><author><keyname>Xiao</keyname><forenames>Jianxiong</forenames></author></authors><title>Tracking Revisited using RGBD Camera: Baseline and Benchmark</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although there has been significant progress in the past decade,tracking is
still a very challenging computer vision task, due to problems such as
occlusion and model drift.Recently, the increased popularity of depth sensors
e.g. Microsoft Kinect has made it easy to obtain depth data at low cost.This
may be a game changer for tracking, since depth information can be used to
prevent model drift and handle occlusion.In this paper, we construct a
benchmark dataset of 100 RGBD videos with high diversity, including deformable
objects, various occlusion conditions and moving cameras. We propose a very
simple but strong baseline model for RGBD tracking, and present a quantitative
comparison of several state-of-the-art tracking algorithms.Experimental results
show that including depth information and reasoning about occlusion
significantly improves tracking performance. The datasets, evaluation details,
source code for the baseline algorithm, and instructions for submitting new
models will be made available online after acceptance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2825</identifier>
 <datestamp>2012-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2825</id><created>2012-12-12</created><authors><author><keyname>Koutsoupias</keyname><forenames>Elias</forenames></author><author><keyname>Leonardi</keyname><forenames>Stefano</forenames></author><author><keyname>Roughgarden</keyname><forenames>Tim</forenames></author></authors><title>Near-Optimal Multi-Unit Auctions with Ordered Bidders</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We construct prior-free auctions with constant-factor approximation
guarantees with ordered bidders, in both unlimited and limited supply settings.
We compare the expected revenue of our auctions on a bid vector to the monotone
price benchmark, the maximum revenue that can be obtained from a bid vector
using supply-respecting prices that are nonincreasing in the bidder ordering
and bounded above by the second-highest bid. As a consequence, our auctions are
simultaneously near-optimal in a wide range of Bayesian multi-unit
environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2831</identifier>
 <datestamp>2013-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2831</id><created>2012-12-12</created><updated>2013-05-14</updated><authors><author><keyname>Kafsi</keyname><forenames>Mohamed</forenames></author><author><keyname>Grossglauser</keyname><forenames>Matthias</forenames></author><author><keyname>Thiran</keyname><forenames>Patrick</forenames></author></authors><title>The Entropy of Conditional Markov Trajectories</title><categories>cs.IT math.IT stat.AP</categories><comments>Accepted for publication in IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To quantify the randomness of Markov trajectories with fixed initial and
final states, Ekroot and Cover proposed a closed-form expression for the
entropy of trajectories of an irreducible finite state Markov chain. Numerous
applications, including the study of random walks on graphs, require the
computation of the entropy of Markov trajectories conditioned on a set of
intermediate states. However, the expression of Ekroot and Cover does not allow
for computing this quantity. In this paper, we propose a method to compute the
entropy of conditional Markov trajectories through a transformation of the
original Markov chain into a Markov chain that exhibits the desired conditional
distribution of trajectories. Moreover, we express the entropy of Markov
trajectories - a global quantity - as a linear combination of local entropies
associated with the Markov chain states.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2834</identifier>
 <datestamp>2013-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2834</id><created>2012-12-12</created><updated>2013-06-10</updated><authors><author><keyname>Yaghoobi</keyname><forenames>Mehrdad</forenames></author><author><keyname>Daudet</keyname><forenames>Laurent</forenames></author><author><keyname>Davies</keyname><forenames>Michael E.</forenames></author></authors><title>Dictionary Subselection Using an Overcomplete Joint Sparsity Model</title><categories>cs.LG math.OC stat.ML</categories><comments>the title previously was &quot;Optimal Dictionary Selection Using an
  Overcomplete Joint Sparsity Model&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many natural signals exhibit a sparse representation, whenever a suitable
describing model is given. Here, a linear generative model is considered, where
many sparsity-based signal processing techniques rely on such a simplified
model. As this model is often unknown for many classes of the signals, we need
to select such a model based on the domain knowledge or using some exemplar
signals. This paper presents a new exemplar based approach for the linear model
(called the dictionary) selection, for such sparse inverse problems. The
problem of dictionary selection, which has also been called the dictionary
learning in this setting, is first reformulated as a joint sparsity model. The
joint sparsity model here differs from the standard joint sparsity model as it
considers an overcompleteness in the representation of each signal, within the
range of selected subspaces. The new dictionary selection paradigm is examined
with some synthetic and realistic simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2845</identifier>
 <datestamp>2012-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2845</id><created>2012-12-12</created><authors><author><keyname>Hiller</keyname><forenames>Jonathan</forenames></author><author><keyname>Lipson</keyname><forenames>Hod</forenames></author></authors><title>Dynamic Simulation of Soft Heterogeneous Objects</title><categories>cs.GR cs.RO physics.comp-ph</categories><comments>Source code download at: http://www.voxcad.com or
  http://sourceforge.net/projects/voxcad/</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a 2D and 3D simulation engine that quantitatively models
the statics, dynamics, and non-linear deformation of heterogeneous soft bodies
in a computationally efficient manner. There is a large body of work simulating
compliant mechanisms. These normally assume small deformations with homogeneous
material properties actuated with external forces. There is also a large body
of research on physically-based deformable objects for applications in computer
graphics with the purpose of generating realistic appearances at the expense of
accuracy. Here we present a simulation framework in which an object may be
composed of any number of interspersed materials with varying properties
(stiffness, density, etc.) to enable true heterogeneous multi-material
simulation. Collisions are handled to prevent self-penetration due to large
deformation, which also allows multiple bodies to interact. A volumetric
actuation method is implemented to impart motion to the structures which opens
the door to the design of novel structures and mechanisms. The simulator was
implemented efficiently such that objects with thousands of degrees of freedom
can be simulated at suitable framerates for user interaction using a single
thread of a typical desktop computer. The code is written in platform agnostic
C++ and is fully open source. This research opens the door to the dynamic
simulation of freeform 3D multi-material mechanisms and objects in a manner
suitable for design automation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2857</identifier>
 <datestamp>2013-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2857</id><created>2012-12-12</created><updated>2013-01-16</updated><authors><author><keyname>Bistarelli</keyname><forenames>Stefano</forenames></author><author><keyname>Santini</keyname><forenames>Francesco</forenames></author></authors><title>ConArg: a Tool to Solve (Weighted) Abstract Argumentation Frameworks
  with (Soft) Constraints</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  ConArg is a Constraint Programming-based tool that can be used to model and
solve different problems related to Abstract Argumentation Frameworks (AFs). To
implement this tool we have used JaCoP, a Java library that provides the user
with a Finite Domain Constraint Programming paradigm. ConArg is able to
randomly generate networks with small-world properties in order to find
conflict-free, admissible, complete, stable grounded, preferred, semi-stable,
stage and ideal extensions on such interaction graphs. We present the main
features of ConArg and we report the performance in time, showing also a
comparison with ASPARTIX [1], a similar tool using Answer Set Programming. The
use of techniques for constraint solving can tackle the complexity of the
problems presented in [2]. Moreover we suggest semiring-based soft constraints
as a mean to parametrically represent and solve Weighted Argumentation
Frameworks: different kinds of preference levels related to attacks, e.g., a
score representing a &quot;fuzziness&quot;, a &quot;cost&quot; or a probability, can be represented
by choosing different instantiation of the semiring algebraic structure. The
basic idea is to provide a common computational and quantitative framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2860</identifier>
 <datestamp>2012-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2860</id><created>2012-12-12</created><authors><author><keyname>Egger</keyname><forenames>Jan</forenames></author><author><keyname>Kapur</keyname><forenames>Tina</forenames></author><author><keyname>Nimsky</keyname><forenames>Christopher</forenames></author><author><keyname>Kikinis</keyname><forenames>Ron</forenames></author></authors><title>Pituitary Adenoma Volumetry with 3D Slicer</title><categories>cs.CV</categories><comments>7 pages, 5 figures, 2 tables, 30 references</comments><journal-ref>(2012) PLoS ONE 7(12): e51788</journal-ref><doi>10.1371/journal.pone.0051788</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this study, we present pituitary adenoma volumetry using the free and open
source medical image computing platform for biomedical research: (3D) Slicer.
Volumetric changes in cerebral pathologies like pituitary adenomas are a
critical factor in treatment decisions by physicians and in general the volume
is acquired manually. Therefore, manual slice-by-slice segmentations in
magnetic resonance imaging (MRI) data, which have been obtained at regular
intervals, are performed. In contrast to this manual time consuming
slice-by-slice segmentation process Slicer is an alternative which can be
significantly faster and less user intensive. In this contribution, we compare
pure manual segmentations of ten pituitary adenomas with semi-automatic
segmentations under Slicer. Thus, physicians drew the boundaries completely
manually on a slice-by-slice basis and performed a Slicer-enhanced segmentation
using the competitive region-growing based module of Slicer named GrowCut.
Results showed that the time and user effort required for GrowCut-based
segmentations were on average about thirty percent less than the pure manual
segmentations. Furthermore, we calculated the Dice Similarity Coefficient (DSC)
between the manual and the Slicer-based segmentations to proof that the two are
comparable yielding an average DSC of 81.97\pm3.39%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2864</identifier>
 <datestamp>2012-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2864</id><created>2012-12-12</created><authors><author><keyname>Nikolic</keyname><forenames>Jelena</forenames></author><author><keyname>Peric</keyname><forenames>Zoran</forenames></author><author><keyname>Velimirovic</keyname><forenames>Lazar</forenames></author></authors><title>Simple Solution for Designing the Piecewise Linear Scalar Companding
  Quantizer for Gaussian Source</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To overcome the difficulties in determining an inverse compressor function
for a Gaussian source, which appear in designing the nonlinear optimal
companding quantizers and also in the nonlinear optimal companding quantization
procedure, in this paper a piecewise linear compressor function based on the
first derivate approximation of the optimal compressor function is proposed. We
show that the approximations used in determining the piecewise linear
compressor function contribute to the simple solution for designing the novel
piecewise linear scalar companding quantizer (PLSCQ) for a Gaussian source of
unit variance. For the given number of segments, we perform optimization
procedure in order to obtain optimal value of the support region threshold
which maximizes the signal to quantization noise ratio (SQNR) of the proposed
PLSCQ. We study how the SQNR of the considered PLSCQ depends on the number of
segments and we show that for the given number of quantization levels, SQNR of
the PLSCQ approaches the one of the nonlinear optimal companding quantizer with
the increase of the number of segments. The presented features of the proposed
PLSCQ indicate that the obtained model should be of high practical significance
for quantization of signals having Gaussian probability density function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2865</identifier>
 <datestamp>2012-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2865</id><created>2012-12-12</created><updated>2012-12-18</updated><authors><author><keyname>Wunder</keyname><forenames>Gerhard</forenames></author><author><keyname>Fischer</keyname><forenames>Robert F. H.</forenames></author><author><keyname>Boche</keyname><forenames>Holger</forenames></author><author><keyname>Litsyn</keyname><forenames>Simon</forenames></author><author><keyname>No</keyname><forenames>Jong-Seon</forenames></author></authors><title>The PAPR Problem in OFDM Transmission: New Directions for a Long-Lasting
  Problem</title><categories>cs.IT math.IT math.MG math.PR</categories><comments>Accepted for publication in IEEE Signal Processing Magazine</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Peak power control for multicarrier communications has been a long-lasting
problem in signal processing and communications. However, industry and academia
are confronted with new challenges regarding energy efficient system design.
Particularly, the envisioned boost in network energy efficiency (e.g. at least
by a factor of 1000 in the Green Touch consortium) will tighten the
requirements on component level so that the efficiency gap with respect to
single-carrier transmission must considerably diminish. This paper reflects
these challenges together with a unified framework and new directions in this
field. The combination of large deviation theory, de-randomization and selected
elements of Banach space geometry will offer a novel approach and will provide
ideas and concepts for researchers with a background in industry as well as
those from academia.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2866</identifier>
 <datestamp>2012-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2866</id><created>2012-12-12</created><authors><author><keyname>Ghosal</keyname><forenames>Prasun</forenames></author><author><keyname>Chakraborty</keyname><forenames>Arijit</forenames></author><author><keyname>Banerjee</keyname><forenames>Sabyasachee</forenames></author><author><keyname>Barman</keyname><forenames>Satabdi</forenames></author></authors><title>Speed Optimization In Unplanned Traffic Using Bio-Inspired Computing And
  Population Knowledge Base</title><categories>cs.CY cs.AI cs.ET</categories><comments>19 pages</comments><msc-class>68T05, 68T35</msc-class><acm-class>I.2; I.2.6</acm-class><journal-ref>Computer Science &amp; Engineering: An International Journal (CSEIJ),
  Vol.2, No.3, June 2012, pp. 79 - 97</journal-ref><doi>10.5121/cseij.2012.2307</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bio-Inspired Algorithms on Road Traffic Congestion and safety is a very
promising research problem. Searching for an efficient optimization method to
increase the degree of speed optimization and thereby increasing the traffic
Flow in an unplanned zone is a widely concerning issue. However, there has been
a limited research effort on the optimization of the lane usage with speed
optimization. The main objective of this article is to find avenues or
techniques in a novel way to solve the problem optimally using the knowledge
from analysis of speeds of vehicles, which, in turn will act as a guide for
design of lanes optimally to provide better optimized traffic. The accident
factors adjust the base model estimates for individual geometric design element
dimensions and for traffic control features. The application of these
algorithms in partially modified form in accordance of this novel Speed
Optimization Technique in an Unplanned Traffic analysis technique is applied to
the proposed design and speed optimization plan. The experimental results based
on real life data are quite encouraging.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2874</identifier>
 <datestamp>2012-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2874</id><created>2012-12-12</created><authors><author><keyname>Ghosal</keyname><forenames>Prasun</forenames></author><author><keyname>Karmakar</keyname><forenames>Sankar</forenames></author></authors><title>Diametrical Mesh Of Tree (D2D-MoT) Architecture: A Novel Routing
  Solution For NoC</title><categories>cs.ET cs.AR</categories><comments>5 pages</comments><msc-class>68-06</msc-class><acm-class>C.1.2; C.1.3; C.2.2</acm-class><journal-ref>International Journal of Advanced Engineering Technology (IJAET),
  Vol.III, Issue I, 2012, pp. 243-247</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network-on-chip (NoC) is a new aspect for designing of future System-On-Chips
(SoC) where a vast number of IP cores are connected through interconnection
network. The communication between the nodes occurred by routing packets rather
than wires. It supports high degree of scalability, reusability and parallelism
in communication. In this paper, we present a Mesh routing architecture, which
is called Diametrical 2D Mesh of Tree, based on Mesh-of-Tree (MoT) routing and
Diametrical 2D Mesh. It has the advantage of having small diameter as well as
large bisection width and small node degree clubbed with being the fastest
network in terms of speed. The routing algorithm ensures that the packets will
always reach from source to sink through shortest path and is deadlock free.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2893</identifier>
 <datestamp>2012-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2893</id><created>2012-12-12</created><authors><author><keyname>Fan</keyname><forenames>Jianqing</forenames></author><author><keyname>Tong</keyname><forenames>Xin</forenames></author><author><keyname>Zeng</keyname><forenames>Yao</forenames></author></authors><title>Communication Learning in Social Networks: Finite Population and the
  Rates</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Following the Bayesian communication learning paradigm, we propose a finite
population learning concept to capture the level of information aggregation in
any given network, where agents are allowed to communicate with neighbors
repeatedly before making a single decision. This concept helps determine the
occurrence of effective information aggregation in a finite network and reveals
explicit interplays among parameters. It also enables meaningful comparative
statics regarding the effectiveness of information aggregation in networks.
Moreover, it offers a solid foundation to address, with a new perfect learning
concept, long run dynamics of learning behavior and the associated learning
rates as population diverges. Our conditions for the occurrence of finite
population learning and perfect learning in communication networks are very
tractable and transparent.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2894</identifier>
 <datestamp>2012-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2894</id><created>2012-12-04</created><authors><author><keyname>Kung</keyname><forenames>H. T.</forenames></author><author><keyname>Yu</keyname><forenames>Chia-Mu</forenames></author></authors><title>Reducing Reconciliation Communication Cost with Compressed Sensing</title><categories>cs.IT cs.DC math.IT</categories><comments>4 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a reconciliation problem, where two hosts wish to synchronize
their respective sets. Efficient solutions for minimizing the communication
cost between the two hosts have been previously proposed in the literature.
However, they rely on prior knowledge about the size of the set differences
between the two sets to be reconciled. In this paper, we propose a method which
can achieve comparable efficiency without assuming this prior knowledge. Our
method uses compressive sensing techniques which can leverage the expected
sparsity in set differences. We study the performance of the method via
theoretical analysis and numerical simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2902</identifier>
 <datestamp>2013-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2902</id><created>2012-12-12</created><updated>2013-04-28</updated><authors><author><keyname>Schneider</keyname><forenames>Michael</forenames></author><author><keyname>Rudolph</keyname><forenames>Sebastian</forenames></author><author><keyname>Sutcliffe</keyname><forenames>Geoff</forenames></author></authors><title>Modeling in OWL 2 without Restrictions</title><categories>cs.AI</categories><comments>Technical Report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Semantic Web ontology language OWL 2 DL comes with a variety of language
features that enable sophisticated and practically useful modeling. However,
the use of these features has been severely restricted in order to retain
decidability of the language. For example, OWL 2 DL does not allow a property
to be both transitive and asymmetric, which would be desirable, e.g., for
representing an ancestor relation. In this paper, we argue that the so-called
global restrictions of OWL 2 DL preclude many useful forms of modeling, by
providing a catalog of basic modeling patterns that would be available in OWL 2
DL if the global restrictions were discarded. We then report on the results of
evaluating several state-of-the-art OWL 2 DL reasoners on problems that use
combinations of features in a way that the global restrictions are violated.
The systems turn out to rely heavily on the global restrictions and are thus
largely incapable of coping with the modeling patterns. Next we show how
off-the-shelf first-order logic theorem proving technology can be used to
perform reasoning in the OWL 2 direct semantics, the semantics that underlies
OWL 2 DL, but without requiring the global restrictions. Applying a naive
proof-of-concept implementation of this approach to the test problems was
successful in all cases. Based on our observations, we make suggestions for
future lines of research on expressive description logic-style OWL reasoning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2908</identifier>
 <datestamp>2012-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2908</id><created>2012-12-12</created><authors><author><keyname>Bassil</keyname><forenames>Youssef</forenames></author></authors><title>A Text Steganography Method Using Pangram and Image Mediums</title><categories>cs.CR</categories><comments>LACSC - Lebanese Association for Computational Sciences -
  http://www.lacsc.org; International Journal of Scientific &amp; Engineering
  Research (IJSER), vol. 3, no. 12, December 2012. arXiv admin note: text
  overlap with arXiv:1212.2067</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Steganography is the art and science of writing hidden messages in such a way
that no one apart from the sender and the receiver would realize that a secret
communicating is taking place. Unlike cryptography which only scrambles secret
data keeping them overt, steganography covers secret data into medium files
such as image files and transmits them in total secrecy avoiding drawing
eavesdroppers suspicions. However, considering that the public channel is
monitored by eavesdroppers, it is vulnerable to stego-attacks which refer to
randomly trying to break the medium file and recover the secret data out of it.
That is often true because steganalysts assume that the secret data are encoded
into a single medium file and not into multiple ones that complement each
other. This paper proposes a text steganography method for hiding secret
textual data using two mediums; a Pangram sentence containing all the
characters of the alphabet, and an uncompressed image file. The algorithm tries
to search for every character of the secret message into the Pangram text. The
search starts from a random index called seed and ends up on the index of the
first occurrence of the character being searched for. As a result, two indexes
are obtained, the seed and the offset indexes. Together they are embedded into
the three LSBs of the color channels of the image medium. Ultimately, both
mediums mainly the Pangram and the image are sent to the receiver. The
advantage of the proposed method is that it makes the covert data hard to be
recovered by unauthorized parties as it uses two mediums, instead of one, to
deliver the secret data. Experiments conducted, illustrated an example that
explained how to encode and decode a secret text message using the Pangram and
the image mediums.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2917</identifier>
 <datestamp>2012-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2917</id><created>2012-12-12</created><authors><author><keyname>Pfaltz</keyname><forenames>John L.</forenames></author></authors><title>Entropy in Social Networks</title><categories>math.CO cs.SI</categories><comments>13 pages, 7 figures. This paper was presented at SocInfo 2012,
  Lausanne Switzerland, Dec. 2012, but was inadvertantly omitted from the
  proceedings</comments><msc-class>05C82, 05C62</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the concepts of closed sets and closure operators as
mathematical tools for the study of social networks. Dynamic networks are
represented by transformations. It is shown that under continuous
change/transformation, all networks tend to &quot;break down&quot; and become less
complex. It is a kind of entropy. The product of this theoretical decomposition
is an abundance of triadically closed clusters which sociologists have observed
in practice. This gives credence to the relevance of this kind of mathematical
analysis in the sociological context.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2950</identifier>
 <datestamp>2014-03-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2950</id><created>2012-12-12</created><updated>2014-03-12</updated><authors><author><keyname>Kyn&#x10d;l</keyname><forenames>Jan</forenames></author></authors><title>Improved enumeration of simple topological graphs</title><categories>math.CO cs.DM</categories><comments>41 pages, 19 figures; fixed Lemma 13, caption to Figure 7 and
  Subsection 3.6, other minor corrections</comments><msc-class>05C10</msc-class><journal-ref>Discrete and Computational Geometry 50 (2013), Issue 3, 727-770</journal-ref><doi>10.1007/s00454-013-9535-8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A simple topological graph T = (V(T), E(T)) is a drawing of a graph in the
plane where every two edges have at most one common point (an endpoint or a
crossing) and no three edges pass through a single crossing. Topological graphs
G and H are isomorphic if H can be obtained from G by a homeomorphism of the
sphere, and weakly isomorphic if G and H have the same set of pairs of crossing
edges. We generalize results of Pach and Toth and the author's previous results
on counting different drawings of a graph under both notions of isomorphism. We
prove that for every graph G with n vertices, m edges and no isolated vertices
the number of weak isomorphism classes of simple topological graphs that
realize G is at most 2^O(n^2 log(m/n)), and at most 2^O(mn^{1/2} log n) if m &lt;
n^{3/2}. As a consequence we obtain a new upper bound 2^O(n^{3/2} log n) on the
number of intersection graphs of n pseudosegments. We improve the upper bound
on the number of weak isomorphism classes of simple complete topological graphs
with n vertices to 2^{n^2 alpha(n)^O(1)}, using an upper bound on the size of a
set of permutations with bounded VC-dimension recently proved by Cibulka and
the author. We show that the number of isomorphism classes of simple
topological graphs that realize G is at most 2^{m^2+O(mn)} and at least
2^Omega(m^2) for graphs with m &gt; (6+epsilon)n.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2952</identifier>
 <datestamp>2012-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2952</id><created>2012-12-12</created><authors><author><keyname>K&#xe4;rkk&#xe4;inen</keyname><forenames>Juha</forenames></author><author><keyname>Kempa</keyname><forenames>Dominik</forenames></author><author><keyname>Puglisi</keyname><forenames>Simon J.</forenames></author></authors><title>Linear Time Lempel-Ziv Factorization: Simple, Fast, Small</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computing the LZ factorization (or LZ77 parsing) of a string is a
computational bottleneck in many diverse applications, including data
compression, text indexing, and pattern discovery. We describe new linear time
LZ factorization algorithms, some of which require only 2n log n + O(log n)
bits of working space to factorize a string of length n. These are the most
space efficient linear time algorithms to date, using n log n bits less space
than any previous linear time algorithm. The algorithms are also practical,
simple to implement, and very fast in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2953</identifier>
 <datestamp>2012-12-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2953</id><created>2012-12-12</created><authors><author><keyname>Axvig</keyname><forenames>Nathan</forenames></author></authors><title>LP Pseudocodewords of Cycle Codes are Half-Integral</title><categories>math.CO cs.IT math.IT</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In his Ph.D. disseration, Feldman and his collaborators define the linear
programming decoder for binary linear codes, which is a linear programming
relaxation of the maximum-likelihood decoding problem. This decoder does not,
in general, attain maximum-likelihood performance; however, the source of this
discrepancy is known to be the presence of non-integral extreme points
(vertices) within the fundamental polytope, vectors which are also called
nontrivial linear programming pseudocodewords. Restricting to the class of
cycle codes, we provide necessary conditions for a vector to be a linear
programming pseudocodeword. In particular, the components of any such
pseudocodeword can only assume values of zero, one-half, or one.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2958</identifier>
 <datestamp>2013-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2958</id><created>2012-12-07</created><updated>2013-03-04</updated><authors><author><keyname>El-Dosuky</keyname><forenames>M. A.</forenames></author><author><keyname>Rashad</keyname><forenames>M. Z.</forenames></author><author><keyname>Hamza</keyname><forenames>T. T.</forenames></author><author><keyname>EL-Bassiouny</keyname><forenames>A. H.</forenames></author></authors><title>Spike and Tyke, the Quantized Neuron Model</title><categories>cs.NE cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modeling spike firing assumes that spiking statistics are Poisson, but real
data violates this assumption. To capture non-Poissonian features, in order to
fix the inevitable inherent irregularity, researchers rescale the time axis
with tedious computational overhead instead of searching for another
distribution. Spikes or action potentials are precisely-timed changes in the
ionic transport through synapses adjusting the synaptic weight, successfully
modeled and developed as a memristor. Memristance value is multiples of initial
resistance. This reminds us with the foundations of quantum mechanics. We try
to quantize potential and resistance, as done with energy. After reviewing
Planck curve for blackbody radiation, we propose the quantization equations. We
introduce and prove a theorem that quantizes the resistance. Then we define the
tyke showing its basic characteristics. Finally we give the basic
transformations to model spiking and link an energy quantum to a tyke.
Investigation shows how this perfectly models the neuron spiking, with over 97%
match.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2963</identifier>
 <datestamp>2012-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2963</id><created>2012-12-12</created><authors><author><keyname>Alonso-Sanz</keyname><forenames>Ramon</forenames></author><author><keyname>Adamatzky</keyname><forenames>Andrew</forenames></author></authors><title>On beta-skeleton automata with memory</title><categories>cs.ET cs.FL nlin.CG</categories><journal-ref>Journal of Computational Science 2 (2011) 1, 57--66</journal-ref><doi>10.1016/j.jocs.2010.12.001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A \beta-skeleton is a proximity undirected graph whose connectivity is
determined by the parameter \beta. We study \beta-skeleton automata where every
node is a finite state machine taking two states, and updating its states
depending on the states of adjacent automata-nodes. We allow automata-nodes to
remember their previous states. In computational experiments we study how
memory affects the global space-time dynamics on \beta-skeleton automata.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.2991</identifier>
 <datestamp>2012-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.2991</id><created>2012-12-12</created><authors><author><keyname>Hershey</keyname><forenames>Shawn</forenames></author><author><keyname>Bernstein</keyname><forenames>Jeff</forenames></author><author><keyname>Bradley</keyname><forenames>Bill</forenames></author><author><keyname>Schweitzer</keyname><forenames>Andrew</forenames></author><author><keyname>Stein</keyname><forenames>Noah</forenames></author><author><keyname>Weber</keyname><forenames>Theo</forenames></author><author><keyname>Vigoda</keyname><forenames>Ben</forenames></author></authors><title>Accelerating Inference: towards a full Language, Compiler and Hardware
  stack</title><categories>cs.SE cs.AI stat.ML</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We introduce Dimple, a fully open-source API for probabilistic modeling.
Dimple allows the user to specify probabilistic models in the form of graphical
models, Bayesian networks, or factor graphs, and performs inference (by
automatically deriving an inference engine from a variety of algorithms) on the
model. Dimple also serves as a compiler for GP5, a hardware accelerator for
inference.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3013</identifier>
 <datestamp>2012-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3013</id><created>2012-12-12</created><authors><author><keyname>Massoudi</keyname><forenames>K.</forenames></author><author><keyname>Modena</keyname><forenames>G.</forenames></author></authors><title>Product/Brand extraction from WikiPedia</title><categories>cs.IR cs.AI</categories><comments>17 pages. Manuscript first creation date: November 27, 2009. At the
  time of first creation both authors were affiliated with the University of
  Amsterdam (The Netherlands)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we describe the task of extracting product and brand pages from
wikipedia. We present an experimental environment and setup built on top of a
dataset of wikipedia pages we collected. We introduce a method for recognition
of product pages modelled as a boolean probabilistic classification task. We
show that this approach can lead to promising results and we discuss
alternative approaches we considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3023</identifier>
 <datestamp>2012-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3023</id><created>2012-12-12</created><authors><author><keyname>Nasution</keyname><forenames>Mahyuddin K. M.</forenames></author><author><keyname>Noah</keyname><forenames>Shahrul Azman Mohd</forenames></author></authors><title>Keyword Extraction for Identifying Social Actors</title><categories>cs.IR cs.CL</categories><comments>7 pages, nothing, draft to ICOCSIM 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Identifying the social actor has become one of tasks in Artificial
Intelligence, whereby extracting keyword from Web snippets depend on the use of
web is steadily gaining ground in this research. We develop therefore an
approach based on overlap principle for utilizing a collection of features in
web snippets, where use of keyword will eliminate the un-relevant web pages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3032</identifier>
 <datestamp>2013-03-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3032</id><created>2012-12-12</created><updated>2013-03-21</updated><authors><author><keyname>Xiao</keyname><forenames>Jinyou</forenames></author><author><keyname>Ye</keyname><forenames>Wenjing</forenames></author><author><keyname>Wen</keyname><forenames>Lihua</forenames></author></authors><title>Efficiency improvement of the frequency-domain BEM for rapid transient
  elastodynamic analysis</title><categories>cs.CE physics.comp-ph</categories><journal-ref>Computational Mechanics, 2013</journal-ref><doi>10.1007/s00466-013-0852-9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The frequency-domain fast boundary element method (BEM) combined with the
exponential window technique leads to an efficient yet simple method for
elastodynamic analysis. In this paper, the efficiency of this method is further
enhanced by three strategies. Firstly, we propose to use exponential window
with large damping parameter to improve the conditioning of the BEM matrices.
Secondly, the frequency domain windowing technique is introduced to alleviate
the severe Gibbs oscillations in time-domain responses caused by large damping
parameters. Thirdly, a solution extrapolation scheme is applied to obtain
better initial guesses for solving the sequential linear systems in the
frequency domain. Numerical results of three typical examples with the problem
size up to 0.7 million unknowns clearly show that the first and third
strategies can significantly reduce the computational time. The second strategy
can effectively eliminate the Gibbs oscillations and result in accurate
time-domain responses.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3034</identifier>
 <datestamp>2012-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3034</id><created>2012-12-12</created><authors><author><keyname>Telgarsky</keyname><forenames>Rastislav</forenames></author></authors><title>Multi-target tracking algorithms in 3D</title><categories>cs.CV cs.DM</categories><comments>7 pages, 2 figures, conference proceedings</comments><msc-class>65D18, 68W05</msc-class><acm-class>G.4; I.2.10; I.4.7; I.4.8; I.4.9; I.5.3</acm-class><journal-ref>Scientific Issues, MATHEMATICA IV, Ruzomberok 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ladars provide a unique capability for identification of objects and motions
in scenes with fixed 3D field of view (FOV). This paper describes algorithms
for multi-target tracking in 3D scenes including the preprocessing
(mathematical morphology and Parzen windows), labeling of connected components,
sorting of targets by selectable attributes (size, length of track, velocity),
and handling of target states (acquired, coasting, re-acquired and tracked) in
order to assemble the target trajectories. This paper is derived from working
algorithms coded in Matlab, which were tested and reviewed by others, and does
not speculate about usage of general formulas or frameworks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3036</identifier>
 <datestamp>2012-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3036</id><created>2012-12-12</created><authors><author><keyname>King</keyname><forenames>Andrew D.</forenames></author><author><keyname>Reed</keyname><forenames>Bruce A.</forenames></author></authors><title>Claw-free graphs, skeletal graphs, and a stronger conjecture on
  $\omega$, $\Delta$, and $\chi$</title><categories>cs.DM math.CO</categories><comments>32 pages, 6 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The second author's $\omega$, $\Delta$, $\chi$ conjecture proposes that every
graph satisties $\chi \leq \lceil \frac 12 (\Delta+1+\omega)\rceil$. In this
paper we prove that the conjecture holds for all claw-free graphs. Our approach
uses the structure theorem of Chudnovsky and Seymour.
  Along the way we discuss a stronger local conjecture, and prove that it holds
for claw-free graphs with a three-colourable complement. To prove our results
we introduce a very useful $\chi$-preserving reduction on homogeneous pairs of
cliques, and thus restrict our view to so-called &quot;skeletal&quot; graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3041</identifier>
 <datestamp>2012-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3041</id><created>2012-12-12</created><authors><author><keyname>Gard-Murray</keyname><forenames>Alexander S.</forenames></author><author><keyname>Bar-Yam</keyname><forenames>Yaneer</forenames></author></authors><title>Complexity and the Limits of Revolution: What Will Happen to the Arab
  Spring?</title><categories>physics.soc-ph cs.SI nlin.AO</categories><comments>15 pages, 1 figure</comments><report-no>New England Complex Systems Institute Report 2012-12-01</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recent social unrest across the Middle East and North Africa has deposed
dictators who had ruled for decades. While the events have been hailed as an
&quot;Arab Spring&quot; by those who hope that repressive autocracies will be replaced by
democracies, what sort of regimes will eventually emerge from the crisis
remains far from certain. Here we provide a complex systems framework,
validated by historical precedent, to help answer this question. We describe
the dynamics of governmental change as an evolutionary process similar to
biological evolution, in which complex organizations gradually arise by
replication, variation and competitive selection. Different kinds of
governments, however, have differing levels of complexity. Democracies must be
more systemically complex than autocracies because of their need to incorporate
large numbers of people in decision-making. This difference has important
implications for the relative robustness of democratic and autocratic
governments after revolutions. Revolutions may disrupt existing evolved
complexity, limiting the potential for building more complex structures
quickly. Insofar as systemic complexity is reduced by revolution, democracy is
harder to create in the wake of unrest than autocracy. Applying this analysis
to the Middle East and North Africa, we infer that in the absence of stable
institutions or external assistance, new governments are in danger of facing
increasingly insurmountable challenges and reverting to autocracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3055</identifier>
 <datestamp>2015-08-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3055</id><created>2012-12-13</created><updated>2015-08-21</updated><authors><author><keyname>Gamkrelidze</keyname><forenames>Alexander</forenames></author><author><keyname>Hotz</keyname><forenames>Gunter</forenames></author><author><keyname>Varamashvili</keyname><forenames>Levan</forenames></author></authors><title>New Invariants for the Graph Isomorphism Problem</title><categories>cs.DS cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we introduce a novel polynomial-time algorithm to compute graph
invariants based on the modified random walk idea on graphs. However not proved
to be a full graph invariant by now, our method gives the right answer for the
graph instances other well-known methods could not compute (such as special
Furer Gadgets and point-line incidence graphs of finite projective planes of
higher degrees
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3060</identifier>
 <datestamp>2012-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3060</id><created>2012-12-13</created><authors><author><keyname>Touseef</keyname><forenames>Muhammad</forenames><affiliation>UIIT, PMAS, Arid Agriculture University, Rawalpindi, Pakistan</affiliation></author><author><keyname>Qaisar</keyname><forenames>Zahid Hussain</forenames><affiliation>Institute of Engineering and Technology, National Fertilizers Corporation</affiliation></author></authors><title>A use case driven approach for system level testing</title><categories>cs.SE</categories><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 9,
  Issue 5, No 1, September 2012 ISSN (Online): 1694-0814 www.IJCSI.org</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Use case scenarios are created during the analysis phase to specify software
system requirements and can also be used for creating system level test cases.
Using use cases to get system tests has several benefits including test design
at early stages of software development life cycle that reduces over all
development cost of the system. Current approaches for system testing using use
cases involve functional details and does not include guards as passing
criteria i.e. use of class diagram that seem to be difficult at very initial
level which lead the need of specification based testing without involving
functional details. In this paper, we proposed a technique for system testing
directly derived from the specification without involving functional details.
We utilize initial and post conditions applied as guards at each level of the
use cases that enables us generation of formalized test cases and makes it
possible to generate test cases for each flow of the system. We used use case
scenarios to generate system level test cases, whereas system sequence diagram
is being used to bridge the gap between the test objective and test cases,
derived from the specification of the system. Since, a state chart derived from
the combination of sequence diagrams can model the entire behavior of the
system.Generated test cases can be employed and executed to state chart in
order to capture behavior of the system with the state change.All these steps
enable us to systematically refine the specification to achieve the goals of
system testing at early development stages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3067</identifier>
 <datestamp>2012-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3067</id><created>2012-12-13</created><authors><author><keyname>Jagli</keyname><forenames>Dhanamma</forenames></author><author><keyname>T</keyname><forenames>Mamatha</forenames></author><author><keyname>Mahalingam</keyname><forenames>Swetha</forenames></author><author><keyname>Ojha</keyname><forenames>Namrata</forenames></author></authors><title>The application of cause effect graph for the college placement process</title><categories>cs.SE</categories><comments>9 pages,5 figures,IJSEA</comments><journal-ref>International Journal of Software Engineering &amp; Applications
  (IJSEA), Vol.3, No.6, November 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a case study on the application of cause effect graph for
representing the college placement process. This paper begins with giving a
brief overview of the college placement process which will serve as the basis
for developing the cause effect graph and the decision table for the same in a
systematic manner. Finally, it concludes with the design of test cases thus
giving a complete and clear representation about the application of
cause-effect graph in the software testing domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3074</identifier>
 <datestamp>2012-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3074</id><created>2012-12-13</created><authors><author><keyname>Dubey</keyname><forenames>Jigyasu</forenames></author><author><keyname>Tokekar</keyname><forenames>Vrinda</forenames></author></authors><title>Identification of efficient peers in P2P computing system for real time
  applications</title><categories>cs.DC cs.NI</categories><comments>12 Pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Currently the Peer to Peer computing paradigm rises as an economic solution
for the large scale computation problems. However due to the dynamic nature of
peers it is very difficult to use this type of systems for the computations of
real time applications. Strict deadline of scientific and real time
applications require predictable performance in such applications. We propose
an algorithm to identify the group of reliable peers, from the available peers
on the Internet, for the processing of real time applications tasks. The
algorithm is based on joint evaluation of peer properties like peer
availability, credibility, computation time and the turnaround time of the peer
with respect to the task distributor peer. Here we also define a method to
calculate turnaround time on task distributor peers at application level.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3079</identifier>
 <datestamp>2013-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3079</id><created>2012-12-13</created><updated>2013-01-10</updated><authors><author><keyname>Bhattacharya</keyname><forenames>Sayan</forenames></author><author><keyname>Kulkarni</keyname><forenames>Janardhan</forenames></author><author><keyname>Xu</keyname><forenames>Xiaoming</forenames></author></authors><title>Constant-Competitive Prior-Free Auction with Ordered Bidders</title><categories>cs.GT cs.DS</categories><comments>The same result has been obtained independently by E. Koutsoupias, S.
  Leonardi and T. Roughgarden</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A central problem in Microeconomics is to design auctions with good revenue
properties. In this setting, the bidders' valuations for the items are private
knowledge, but they are drawn from publicly known prior distributions. The goal
is to find a truthful auction (no bidder can gain in utility by misreporting
her valuation) that maximizes the expected revenue.
  Naturally, the optimal-auction is sensitive to the prior distributions. An
intriguing question is to design a truthful auction that is oblivious to these
priors, and yet manages to get a constant factor of the optimal revenue. Such
auctions are called prior-free.
  Goldberg et al. presented a constant-approximate prior-free auction when
there are identical copies of an item available in unlimited supply, bidders
are unit-demand, and their valuations are drawn from i.i.d. distributions. The
recent work of Leonardi et al. [STOC 2012] generalized this problem to non
i.i.d. bidders, assuming that the auctioneer knows the ordering of their
reserve prices. Leonardi et al. proposed a prior-free auction that achieves a
$O(\log^* n)$ approximation. We improve upon this result, by giving the first
prior-free auction with constant approximation guarantee.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3090</identifier>
 <datestamp>2013-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3090</id><created>2012-12-13</created><updated>2013-09-24</updated><authors><author><keyname>Li</keyname><forenames>Wei</forenames></author><author><keyname>Yuan</keyname><forenames>Chun-Ming</forenames></author><author><keyname>Gao</keyname><forenames>Xiao-Shan</forenames></author></authors><title>Sparse Difference Resultant</title><categories>cs.SC math.AG</categories><comments>43 pages. arXiv admin note: text overlap with arXiv:1111.1084</comments><msc-class>12H10, 68W30</msc-class><acm-class>I.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, the concept of sparse difference resultant for a Laurent
transformally essential system of difference polynomials is introduced and a
simple criterion for the existence of sparse difference resultant is given. The
concept of transformally homogenous polynomial is introduced and the sparse
difference resultant is shown to be transformally homogenous. It is shown that
the vanishing of the sparse difference resultant gives a necessary condition
for the corresponding difference polynomial system to have non-zero solutions.
The order and degree bounds for sparse difference resultant are given. Based on
these bounds, an algorithm to compute the sparse difference resultant is
proposed, which is single exponential in terms of the number of variables, the
Jacobi number, and the size of the Laurent transformally essential system.
Furthermore, the precise order and degree, a determinant representation, and a
Poisson-type product formula for the difference resultant are given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3093</identifier>
 <datestamp>2012-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3093</id><created>2012-12-13</created><authors><author><keyname>van der Zypen</keyname><forenames>Dominic</forenames></author></authors><title>Hadwiger's conjecture for graphs with infinite chromatic number</title><categories>math.CO cs.DM math.LO</categories><comments>2 pages</comments><msc-class>05C15, 05C83</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We construct a connected graph H such that (1) \chi(H) = \omega; (2)
K_\omega, the complete graph on \omega points, is not a minor of H. Therefore
Hadwiger's conjecture does not hold for graphs with infinite coloring number.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3119</identifier>
 <datestamp>2012-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3119</id><created>2012-12-13</created><authors><author><keyname>Lef&#xe8;vre</keyname><forenames>Augustin</forenames></author><author><keyname>Glineur</keyname><forenames>Fran&#xe7;ois</forenames></author><author><keyname>Absil</keyname><forenames>P. -A.</forenames></author></authors><title>A nuclear-norm based convex formulation for informed source separation</title><categories>cs.SD</categories><comments>Submitted to ESANN 2013 conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of separating audio sources from a single linear
mixture. The goal is to find a decomposition of the single channel spectrogram
into a sum of individual contributions associated to a certain number of
sources. In this paper, we consider an informed source separation problem in
which the input spectrogram is partly annotated. We propose a convex
formulation that relies on a nuclear norm penalty to induce low rank for the
contributions. We show experimentally that solving this model with a simple
subgradient method outperforms a previously introduced nonnegative matrix
factorization (NMF) technique, both in terms of source separation quality and
computation time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3133</identifier>
 <datestamp>2013-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3133</id><created>2012-12-13</created><authors><author><keyname>Mei</keyname><forenames>Gang</forenames></author><author><keyname>Tipper</keyname><forenames>John C.</forenames></author><author><keyname>Xu</keyname><forenames>Nengxiong</forenames></author></authors><title>The Modified Direct Method: an Approach for Smoothing Planar and Surface
  Meshes</title><categories>cs.CG cs.NA math.NA</categories><comments>18 pages</comments><journal-ref>Procedia Computer Science 18 (2013) 2436-2439</journal-ref><doi>10.1016/j.procs.2013.05.418</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Modi?ed Direct Method (MDM) is an iterative mesh smoothing method for
smoothing planar and surface meshes, which is developed from the non-iterative
smoothing method originated by Balendran [1]. When smooth planar meshes, the
performance of the MDM is e?ectively identical to that of Laplacian smoothing,
for triangular and quadrilateral meshes; however, the MDM outperforms Laplacian
smoothing for tri-quad meshes. When smooth surface meshes, for trian-gular,
quadrilateral and quad-dominant mixed meshes, the mean quality(MQ) of all mesh
elements always increases and the mean square error (MSE) decreases during
smoothing; For tri-dominant mixed mesh, the quality of triangles always
descends while that of quads ascends. Test examples show that the MDM is
convergent for both planar and surface triangular, quadrilateral and tri-quad
meshes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3138</identifier>
 <datestamp>2012-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3138</id><created>2012-12-13</created><authors><author><keyname>Georw</keyname><forenames>Aaron</forenames></author><author><keyname>Keane</keyname><forenames>Mark</forenames></author></authors><title>Identifying Metaphor Hierarchies in a Corpus Analysis of Finance
  Articles</title><categories>cs.CL</categories><journal-ref>Proceedings of the 33rd Annual Meeting of the Cognitive Science
  Society (CogSci '11), Boston, MA, USA, 20-23 July, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using a corpus of over 17,000 financial news reports (involving over 10M
words), we perform an analysis of the argument-distributions of the UP- and
DOWN-verbs used to describe movements of indices, stocks, and shares. Using
measures of the overlap in the argument distributions of these verbs and
k-means clustering of their distributions, we advance evidence for the proposal
that the metaphors referred to by these verbs are organised into hierarchical
structures of superordinate and subordinate groups.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3139</identifier>
 <datestamp>2013-02-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3139</id><created>2012-12-13</created><updated>2013-02-04</updated><authors><author><keyname>Gerow</keyname><forenames>Aaron</forenames></author><author><keyname>Keane</keyname><forenames>Mark</forenames></author></authors><title>Identifying Metaphoric Antonyms in a Corpus Analysis of Finance Articles</title><categories>cs.CL</categories><comments>arXiv admin note: text overlap with arXiv:1212.3138</comments><journal-ref>Proceedings of the 33rd Annual Meeting of the Cognitive Science
  Society (CogSci '11), Boston, MA, USA, 20-23 July, 2011</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using a corpus of 17,000+ financial news reports (involving over 10M words),
we perform an analysis of the argument-distributions of the UP and DOWN verbs
used to describe movements of indices, stocks and shares. In Study 1
participants identified antonyms of these verbs in a free-response task and a
matching task from which the most commonly identified antonyms were compiled.
In Study 2, we determined whether the argument-distributions for the verbs in
these antonym-pairs were sufficiently similar to predict the most
frequently-identified antonym. Cosine similarity correlates moderately with the
proportions of antonym-pairs identified by people (r = 0.31). More
impressively, 87% of the time the most frequently-identified antonym is either
the first- or second-most similar pair in the set of alternatives. The
implications of these results for distributional approaches to determining
metaphoric knowledge are discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3153</identifier>
 <datestamp>2012-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3153</id><created>2012-12-13</created><updated>2012-12-20</updated><authors><author><keyname>Peric</keyname><forenames>Zoran</forenames></author><author><keyname>Nikolic</keyname><forenames>Jelena</forenames></author><author><keyname>Velimirovic</keyname><forenames>Lazar</forenames></author><author><keyname>Stankovic</keyname><forenames>Miomir</forenames></author><author><keyname>Aleksic</keyname><forenames>Danijela</forenames></author></authors><title>Asymmetrical two-level scalar quantizer with extended Huffman coding for
  compression of Laplacian source</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a novel model of the two-level scalar quantizer with
extended Huffman coding. It is designed for the average bit rate to approach
the source entropy as close as possible provided that the signal to
quantization noise ratio (SQNR) value does not decrease more than 1 dB from the
optimal SQNR value. Assuming the asymmetry of representation levels for the
symmetric Laplacian probability density function, the unequal probabilities of
representation levels are obtained, i.e. the proper basis for further
implementation of lossless compression techniques is provided. In this paper,
we are concerned with extended Huffman coding technique that provides the
shortest length of codewords for blocks of two or more symbols. For the
proposed quantizer with extended Huffman coding the convergence of the average
bit rate to the source entropy is examined in the case of two to five symbol
blocks. It is shown that the higher SQNR is achieved by the proposed
asymmetrical quantizer with extended Huffman coding when compared with the
symmetrical quantizers with extended Huffman coding having equal average bit
rates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3162</identifier>
 <datestamp>2012-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3162</id><created>2012-12-13</created><authors><author><keyname>Gerow</keyname><forenames>Aaron</forenames></author><author><keyname>Ahmad</keyname><forenames>Khurshid</forenames></author></authors><title>Diachronic Variation in Grammatical Relations</title><categories>cs.CL</categories><journal-ref>Proceedings of the 24th International Conference on Computational
  Linguistics (COLING 2012), Mumbai, India</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a method of finding and analyzing shifts in grammatical relations
found in diachronic corpora. Inspired by the econometric technique of measuring
return and volatility instead of relative frequencies, we propose them as a way
to better characterize changes in grammatical patterns like nominalization,
modification and comparison. To exemplify the use of these techniques, we
examine a corpus of NIPS papers and report trends which manifest at the token,
part-of-speech and grammatical levels. Building up from frequency observations
to a second-order analysis, we show that shifts in frequencies overlook deeper
trends in language, even when part-of-speech information is included. Examining
token, POS and grammatical levels of variation enables a summary view of
diachronic text as a whole. We conclude with a discussion about how these
methods can inform intuitions about specialist domains as well as changes in
language use as a whole.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3170</identifier>
 <datestamp>2012-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3170</id><created>2012-12-13</created><authors><author><keyname>Pantisano</keyname><forenames>Francesco</forenames></author><author><keyname>Bennis</keyname><forenames>Mehdi</forenames></author><author><keyname>Saad</keyname><forenames>Walid</forenames></author><author><keyname>Debbah</keyname><forenames>Merouane</forenames></author><author><keyname>Latva-aho</keyname><forenames>Matti</forenames></author></authors><title>Improving Macrocell - Small Cell Coexistence through Adaptive
  Interference Draining</title><categories>cs.GT cs.IT cs.SY math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The deployment of underlay small base stations (SBSs) is expected to
significantly boost the spectrum efficiency and the coverage of next-generation
cellular networks. However, the coexistence of SBSs underlaid to an existing
macro-cellular network faces important challenges, notably in terms of spectrum
sharing and interference management. In this paper, we propose a novel
game-theoretic model that enables the SBSs to optimize their transmission rates
by making decisions on the resource occupation jointly in the frequency and
spatial domains. This procedure, known as interference draining, is performed
among cooperative SBSs and allows to drastically reduce the interference
experienced by both macro- and small cell users. At the macrocell side, we
consider a modified water-filling policy for the power allocation that allows
each macrocell user (MUE) to focus the transmissions on the degrees of freedom
over which the MUE experiences the best channel and interference conditions.
This approach not only represents an effective way to decrease the received
interference at the MUEs but also grants the SBSs tier additional transmission
opportunities and allows for a more agile interference management. Simulation
results show that the proposed approach yields significant gains at both
macrocell and small cell tiers, in terms of average achievable rate per user,
reaching up to 37%, relative to the non-cooperative case, for a network with
150 MUEs and 200 SBSs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3171</identifier>
 <datestamp>2015-03-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3171</id><created>2012-12-13</created><authors><author><keyname>Grabska-Gradzi&#x144;ska</keyname><forenames>Iwona</forenames></author><author><keyname>Kulig</keyname><forenames>Andrzej</forenames></author><author><keyname>Kwapie&#x144;</keyname><forenames>Jaros&#x142;aw</forenames></author><author><keyname>O&#x15b;wi&#x119;cimka</keyname><forenames>Pawe&#x142;</forenames></author><author><keyname>Dro&#x17c;d&#x17c;</keyname><forenames>Stanis&#x142;aw</forenames></author></authors><title>Multifractal analysis of sentence lengths in English literary texts</title><categories>physics.data-an cs.CL physics.soc-ph</categories><comments>5 pages, 5 figures, WCIT 2012 conference</comments><journal-ref>AWERProcedia Information Technology &amp; Computer Science 03, 1700,
  2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents analysis of 30 literary texts written in English by
different authors. For each text, there were created time series representing
length of sentences in words and analyzed its fractal properties using two
methods of multifractal analysis: MFDFA and WTMM. Both methods showed that
there are texts which can be considered multifractal in this representation but
a majority of texts are not multifractal or even not fractal at all. Out of 30
books, only a few have so-correlated lengths of consecutive sentences that the
analyzed signals can be interpreted as real multifractals. An interesting
direction for future investigations would be identifying what are the specific
features which cause certain texts to be multifractal and other to be
monofractal or even not fractal at all.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3177</identifier>
 <datestamp>2012-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3177</id><created>2012-12-13</created><authors><author><keyname>Ramachandran</keyname><forenames>Rajesh</forenames></author><author><keyname>Sharma</keyname><forenames>Vinod</forenames></author><author><keyname>Viswanath</keyname><forenames>Pramod</forenames></author></authors><title>Information Capacity of an Energy Harvesting Sensor Node</title><categories>cs.IT math.IT</categories><comments>30 Pages, Submitted to IEEE Trans. Inform. Theory. arXiv admin note:
  substantial text overlap with arXiv:1009.5158, arXiv:1010.5416</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy harvesting sensor nodes are gaining popularity due to their ability to
improve the network life time and are becoming a preferred choice supporting
'green communication'. In this paper we focus on communicating reliably over an
AWGN channel using such an energy harvesting sensor node. An important part of
this work involves appropriate modeling of the energy harvesting, as done via
various practical architectures. Our main result is the characterization of the
Shannon capacity of the communication system. The key technical challenge
involves dealing with the dynamic (and stochastic) nature of the (quadratic)
cost of the input to the channel. As a corollary, we find close connections
between the capacity achieving energy management policies and the queueing
theoretic throughput optimal policies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3185</identifier>
 <datestamp>2013-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3185</id><created>2012-12-13</created><updated>2013-06-02</updated><authors><author><keyname>Zhao</keyname><forenames>Hong</forenames></author><author><keyname>Min</keyname><forenames>Fan</forenames></author><author><keyname>Zhu</keyname><forenames>William</forenames></author></authors><title>Cost-Sensitive Feature Selection of Data with Errors</title><categories>cs.LG</categories><comments>This paper has been withdrawn by the author due to an error in Figure
  4</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In data mining applications, feature selection is an essential process since
it reduces a model's complexity. The cost of obtaining the feature values must
be taken into consideration in many domains. In this paper, we study the
cost-sensitive feature selection problem on numerical data with measurement
errors, test costs and misclassification costs. The major contributions of this
paper are four-fold. First, a new data model is built to address test costs and
misclassification costs as well as error boundaries. Second, a covering-based
rough set with measurement errors is constructed. Given a confidence interval,
the neighborhood is an ellipse in a two-dimension space, or an ellipsoidal in a
three-dimension space, etc. Third, a new cost-sensitive feature selection
problem is defined on this covering-based rough set. Fourth, both backtracking
and heuristic algorithms are proposed to deal with this new problem. The
algorithms are tested on six UCI (University of California - Irvine) data sets.
Experimental results show that (1) the pruning techniques of the backtracking
algorithm help reducing the number of operations significantly, and (2) the
heuristic algorithm usually obtains optimal results. This study is a step
toward realistic applications of cost-sensitive learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3186</identifier>
 <datestamp>2013-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3186</id><created>2012-12-13</created><updated>2013-04-05</updated><authors><author><keyname>Barato</keyname><forenames>A. C.</forenames></author><author><keyname>Hartich</keyname><forenames>D</forenames></author><author><keyname>Seifert</keyname><forenames>U.</forenames></author></authors><title>Information-theoretic vs. thermodynamic entropy production in autonomous
  sensory networks</title><categories>cond-mat.stat-mech cs.IT math.IT physics.data-an</categories><comments>6 pages, 4 figures</comments><journal-ref>Phys. Rev. E 87, 042104 (2013)</journal-ref><doi>10.1103/PhysRevE.87.042104</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For sensory networks, we determine the rate with which they acquire
information about the changing external conditions. Comparing this rate with
the thermodynamic entropy production that quantifies the cost of maintaining
the network, we find that there is no universal bound restricting the rate of
obtaining information to be less than this thermodynamic cost. These results
are obtained within a general bipartite model consisting of a stochastically
changing environment that affects the instantaneous transition rates within the
system. Moreover, they are illustrated with a simple four-states model
motivated by cellular sensing. On the technical level, we obtain an upper bound
on the rate of mutual information analytically and calculate this rate with a
numerical method that estimates the entropy of a time-series generated with a
simulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3193</identifier>
 <datestamp>2012-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3193</id><created>2012-12-13</created><authors><author><keyname>Martinez</keyname><forenames>Oscar</forenames></author></authors><title>An Efficient Algorithm to Calculate the Center of the Biggest Inscribed
  Circle in an Irregular Polygon</title><categories>math.MG cs.DS</categories><comments>10 pages, 11 figures</comments><msc-class>51K99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, an efficient algorithm to find the center of the biggest
circle inscribed in a given polygon is described. This work was inspired by the
publication of Daniel Garcia-Castellanos &amp; Umberto Lombardo and their algorithm
used to find a landmass' poles of inaccessibility. Two more efficient
algorithms were found, one of them only applicable when the problem can be
described as a linear problem, like in the case of a convex polygon.
  Keywords: distance geometry, euclidean distance, inscribed circle, irregular
polygon, algorithm, mathematical optimization, Monte Carlo, linear programming,
maximin
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3216</identifier>
 <datestamp>2012-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3216</id><created>2012-12-13</created><authors><author><keyname>Raw</keyname><forenames>Ram Shringar</forenames></author><author><keyname>Das</keyname><forenames>Sanjoy</forenames></author><author><keyname>Singh</keyname><forenames>Nanhay</forenames></author><author><keyname>Kumar</keyname><forenames>Sanjeet</forenames></author><author><keyname>Kumar</keyname><forenames>Shailender</forenames></author></authors><title>Feasibility Evaluation of VANET using Directional-Location Aided Routing
  (D-LAR) Protocol</title><categories>cs.NI</categories><journal-ref>IJCSI International Journal of Computer Science Issues, Vol. 9,
  Issue 5, No 3, September 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vehicular Ad hoc Networks (VANETs) allow vehicles to form a self-organized
network without any fixed infrastructure. VANETs have received wide attention
and numerous research issues have been identified in the recent time. The
design and implementation of efficient and scalable routing protocols for
VANETs is a challenging task due to high dynamics and mobility constraints. In
this paper, we have proposed D-LAR (Directional-Location Aided Routing), is an
extension of Location Aided Routing (LAR) with Directional Routing (DIR)
capability. D-LAR is a greedy approach based-position based routing protocol to
forward packet to the node present in request zone within the transmission
range of the source node as most suitable next-hop node. We have justified the
feasibility of our proposed protocol for VANET.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3217</identifier>
 <datestamp>2012-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3217</id><created>2012-12-11</created><authors><author><keyname>Joosten</keyname><forenames>J. J.</forenames></author></authors><title>Complexity fits the fittest</title><categories>cs.LO cs.CC nlin.AO</categories><comments>arXiv admin note: substantial text overlap with arXiv:1211.1878</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we shall relate computational complexity to the principle of
natural selection. We shall do this by giving a philosophical account of
complexity versus universality. It seems sustainable to equate universal
systems to complex systems or at least to potentially complex systems. Post's
problem on the existence of (natural) intermediate degrees (between decidable
and universal RE) then finds its analog in the Principle of Computional
Equivalence (PCE). In this paper we address possible driving forces --if any--
behind PCE. Both the natural aspects as well as the cognitive ones are
investigated. We postulate a principle GNS that we call the Generalized Natural
Selection principle that together with the Church-Turing thesis is seen to be
in close correspondence to a weak version of PCE. Next, we view our cognitive
toolkit in an evolutionary light and postulate a principle in analogy with
Fodor's language principle. In the final part of the paper we reflect on ways
to provide circumstantial evidence for GNS by means of theorems, experiments
or, simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3220</identifier>
 <datestamp>2012-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3220</id><created>2012-12-07</created><authors><author><keyname>El-Dosuky</keyname><forenames>M. A.</forenames></author><author><keyname>Rashad</keyname><forenames>M. Z.</forenames></author><author><keyname>Hamza</keyname><forenames>T. T.</forenames></author><author><keyname>EL-Bassiouny</keyname><forenames>A. H.</forenames></author></authors><title>New SpiroPlanck Heuristics for High Energy Physics Networking and Future
  Internet Testbeds</title><categories>cs.NI cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The need for data intensive Grids, and advanced networks with high
performance that support our science has made the High Energy Physics community
a leading and a key co-developer of leading edge wide area networks. This paper
gives an overview of the status for the world's research networks and major
international links used by the high energy physics and other scientific
communities, showing some Future Internet testbed architectures, scalability,
geographic scope, and extension between networks. The resemblance between
wireless sensor network and future internet network, especially in scale
consideration as density and network coverage, inspires us to adopt the models
of the former to the later. Then we test this assumption to see that this
provides a concise working model. This paper collects some heuristics that we
call them SpiroPlanck and employs them to model the coverage of dense networks.
In this paper, we propose a framework for the operation of FI testbeds
containing a test scenario, new representation and visualization techniques,
and possible performance measures. Investigations show that it is very
promising and could be seen as a good optimization
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3225</identifier>
 <datestamp>2012-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3225</id><created>2012-12-13</created><updated>2012-12-18</updated><authors><author><keyname>Saha</keyname><forenames>Sayan</forenames></author><author><keyname>Das</keyname><forenames>Saptarshi</forenames></author><author><keyname>Acharya</keyname><forenames>Anish</forenames></author><author><keyname>Kumar</keyname><forenames>Abhishek</forenames></author><author><keyname>Mukherjee</keyname><forenames>Sumit</forenames></author><author><keyname>Pan</keyname><forenames>Indranil</forenames></author><author><keyname>Gupta</keyname><forenames>Amitava</forenames></author></authors><title>Identification of Nonlinear Systems From the Knowledge Around Different
  Operating Conditions: A Feed-Forward Multi-Layer ANN Based Approach</title><categories>cs.SY cs.NE</categories><comments>&quot;6 pages, 9 figures; The Second IEEE International Conference on
  Parallel, Distributed and Grid Computing (PDGC-2012), December 2012, Solan&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper investigates nonlinear system identification using system output
data at various linearized operating points. A feed-forward multi-layer
Artificial Neural Network (ANN) based approach is used for this purpose and
tested for two target applications i.e. nuclear reactor power level monitoring
and an AC servo position control system. Various configurations of ANN using
different activation functions, number of hidden layers and neurons in each
layer are trained and tested to find out the best configuration. The training
is carried out multiple times to check for consistency and the mean and
standard deviation of the root mean square errors (RMSE) are reported for each
configuration.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3228</identifier>
 <datestamp>2012-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3228</id><created>2012-12-11</created><authors><author><keyname>Song</keyname><forenames>Peiyou</forenames></author><author><keyname>Shu</keyname><forenames>Anhei</forenames></author><author><keyname>Phipps</keyname><forenames>David</forenames></author><author><keyname>Wallach</keyname><forenames>Dan</forenames></author><author><keyname>Tiwari</keyname><forenames>Mohit</forenames></author><author><keyname>Crandall</keyname><forenames>Jedidiah</forenames></author><author><keyname>Luger</keyname><forenames>George</forenames></author></authors><title>Language Without Words: A Pointillist Model for Natural Language
  Processing</title><categories>cs.CL cs.IR cs.SI</categories><comments>5 pages, 2 figures</comments><acm-class>I.2.7; H.2.8; H.3.1</acm-class><journal-ref>The 6th International Conference on Soft Computing and Intelligent
  Systems (SCIS-ISIS 2012) Kobe, Japan</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper explores two separate questions: Can we perform natural language
processing tasks without a lexicon?; and, Should we? Existing natural language
processing techniques are either based on words as units or use units such as
grams only for basic classification tasks. How close can a machine come to
reasoning about the meanings of words and phrases in a corpus without using any
lexicon, based only on grams?
  Our own motivation for posing this question is based on our efforts to find
popular trends in words and phrases from online Chinese social media. This form
of written Chinese uses so many neologisms, creative character placements, and
combinations of writing systems that it has been dubbed the &quot;Martian Language.&quot;
Readers must often use visual queues, audible queues from reading out loud, and
their knowledge and understanding of current events to understand a post. For
analysis of popular trends, the specific problem is that it is difficult to
build a lexicon when the invention of new ways to refer to a word or concept is
easy and common. For natural language processing in general, we argue in this
paper that new uses of language in social media will challenge machines'
abilities to operate with words as the basic unit of understanding, not only in
Chinese but potentially in other languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3229</identifier>
 <datestamp>2012-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3229</id><created>2012-12-11</created><authors><author><keyname>Tunc</keyname><forenames>Ilker</forenames></author><author><keyname>Shaw</keyname><forenames>Leah B.</forenames></author></authors><title>Effects of community structure on epidemic spread in an adaptive network</title><categories>q-bio.PE cs.SI nlin.AO physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When an epidemic spreads in a population, individuals may adaptively change
the structure of their social contact network to reduce risk of infection. Here
we study the spread of an epidemic on an adaptive network with community
structure. We model the effect of two communities with different average
degrees. The disease model is susceptible-infected-susceptible (SIS), and
adaptation is rewiring of links between susceptibles and infectives. The
bifurcation structure is obtained, and a mean field model is developed that
accurately predicts the steady state behavior of the system. We show that an
epidemic can alter the community structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3233</identifier>
 <datestamp>2015-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3233</id><created>2012-12-13</created><updated>2015-12-02</updated><authors><author><keyname>Bienkowski</keyname><forenames>Marcin</forenames></author><author><keyname>Byrka</keyname><forenames>Jaroslaw</forenames></author><author><keyname>Chrobak</keyname><forenames>Marek</forenames></author><author><keyname>Dobbs</keyname><forenames>Neil</forenames></author><author><keyname>Nowicki</keyname><forenames>Tomasz</forenames></author><author><keyname>Sviridenko</keyname><forenames>Maxim</forenames></author><author><keyname>Swirszcz</keyname><forenames>Grzegorz</forenames></author><author><keyname>Young</keyname><forenames>Neal E.</forenames></author></authors><title>Approximation Algorithms for the Joint Replenishment Problem with
  Deadlines</title><categories>cs.DS</categories><msc-class>68W25, 90C05</msc-class><acm-class>G.1.6</acm-class><journal-ref>J. Scheduling 18(6): 545-560 (2015)</journal-ref><doi>10.1007/s10951-014-0392-y</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Joint Replenishment Problem (JRP) is a fundamental optimization problem
in supply-chain management, concerned with optimizing the flow of goods from a
supplier to retailers. Over time, in response to demands at the retailers, the
supplier ships orders, via a warehouse, to the retailers. The objective is to
schedule these orders to minimize the sum of ordering costs and retailers'
waiting costs.
  We study the approximability of JRP-D, the version of JRP with deadlines,
where instead of waiting costs the retailers impose strict deadlines. We study
the integrality gap of the standard linear-program (LP) relaxation, giving a
lower bound of 1.207, a stronger, computer-assisted lower bound of 1.245, as
well as an upper bound and approximation ratio of 1.574. The best previous
upper bound and approximation ratio was 1.667; no lower bound was previously
published. For the special case when all demand periods are of equal length we
give an upper bound of 1.5, a lower bound of 1.2, and show APX-hardness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3248</identifier>
 <datestamp>2012-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3248</id><created>2012-12-13</created><authors><author><keyname>Shamsoddin-Motlagh</keyname><forenames>Ebrahim</forenames></author></authors><title>A survey of service oriented architecture systems testing</title><categories>cs.SE</categories><journal-ref>International Journal of Software Engineering &amp; Applications
  (IJSEA), Vol.3, No.6, November 2012</journal-ref><doi>10.5121/ijsea.2012.3602</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Service oriented architecture (SOA) is one of the latest software
architectures. This architecture is created in direction of the business
requirements and removed the gap between softwares and businesses. The software
testing is the rising cost of activities in development software. SOA has
different specifications and features proportion of the other software
architectures. First this paper reviews SOA testing challenges and existing
solution(s) for those challenges. Then that reports a survey of recent research
to SOA systems testing, that covers both functional and non-functional testing.
Those are presented for different levels of functional testing, including unit,
integration, and regression testing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3251</identifier>
 <datestamp>2013-10-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3251</id><created>2012-12-13</created><updated>2013-10-04</updated><authors><author><keyname>Tan</keyname><forenames>Tony</forenames></author></authors><title>Automata for two-variable logic over trees with ordered data values</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Data trees are trees in which each node, besides carrying a label from a
finite alphabet, also carries a data value from an infinite domain. They have
been used as an abstraction model for reasoning tasks on {XML} and
verification. However, most existing approaches consider the case where only
equality test can be performed on the data values.
  In this paper we study data trees in which the data values come from a
linearly ordered domain, and in addition to equality test, we can test whether
the data value in a node is greater than the one in another node. We introduce
an automata model for them which we call ordered-data tree automata (ODTA),
provide its logical characterisation, and prove that its non-emptiness problem
is decidable in 3-NEXPTIME. We also show that the two-variable logic on
unranked trees, studied by Bojanczyk, Muscholl, Schwentick and Segoufin in
2009, corresponds precisely to a special subclass of this automata model.
  Then we define a slightly weaker version of ODTA, which we call weak ODTA,
and provide its logical characterisation. The complexity of the non-emptiness
problem drops to NP. However, a number of existing formalisms and models
studied in the literature can be captured already by weak ODTA. We also show
that the definition of ODTA can be easily modified, to the case where the data
values come from a tree-like partially ordered domain, such as strings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3257</identifier>
 <datestamp>2012-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3257</id><created>2012-12-13</created><authors><author><keyname>Gerhardt</keyname><forenames>Ilja</forenames></author><author><keyname>Hanke</keyname><forenames>Timo</forenames></author></authors><title>Homomorphic Payment Addresses and the Pay-to-Contract Protocol</title><categories>cs.CR</categories><comments>11 pages, 5 figures</comments><acm-class>E.3; K.4.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an electronic payment protocol for typical customer-merchant
relations which does not require a trusted (signed) payment descriptor to be
sent from the merchant to the customer. Instead, the destination &quot;account&quot;
number for the payment is solely created on the customer side. This eliminates
the need for any encrypted or authenticated communication in the protocol and
is secure even if the merchant's online infrastructure is compromised.
Moreover, the payment transaction itself serves as a timestamped receipt for
the customer. It proves what has been paid for and who received the funds,
again without relying on any merchant signatures. In particular, funds and
receipt are exchanged in a single atomic action. The asymmetric nature of the
customer-merchant relation is crucial.
  The protocol is specifically designed with bitcoin in mind as the underlying
payment system. Thereby, it has the useful benefit that all transactions are
public. However, the only essential requirement on the payment system is that
&quot;accounts&quot; are arbitrary user-created keypairs of a cryptosystem whose keypairs
enjoy a homomorphic property. All ElGamal-type cryptosystems have this feature.
For use with bitcoin we propose the design of a deterministic bitcoin wallet
whose addresses can be indexed by clear text strings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3268</identifier>
 <datestamp>2013-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3268</id><created>2012-12-13</created><updated>2013-09-17</updated><authors><author><keyname>Puy</keyname><forenames>Gilles</forenames></author><author><keyname>Vandergheynst</keyname><forenames>Pierre</forenames></author></authors><title>Robust image reconstruction from multi-view measurements</title><categories>cs.CV</categories><comments>Accepted in SIAM Journal on Imaging Sciences</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel method to accurately reconstruct a set of images
representing a single scene from few linear multi-view measurements. Each
observed image is modeled as the sum of a background image and a foreground
one. The background image is common to all observed images but undergoes
geometric transformations, as the scene is observed from different viewpoints.
In this paper, we assume that these geometric transformations are represented
by a few parameters, e.g., translations, rotations, affine transformations,
etc.. The foreground images differ from one observed image to another, and are
used to model possible occlusions of the scene. The proposed reconstruction
algorithm estimates jointly the images and the transformation parameters from
the available multi-view measurements. The ideal solution of this multi-view
imaging problem minimizes a non-convex functional, and the reconstruction
technique is an alternating descent method built to minimize this functional.
The convergence of the proposed algorithm is studied, and conditions under
which the sequence of estimated images and parameters converges to a critical
point of the non-convex functional are provided. Finally, the efficiency of the
algorithm is demonstrated using numerical simulations for applications such as
compressed sensing or super-resolution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3276</identifier>
 <datestamp>2014-07-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3276</id><created>2012-12-13</created><updated>2014-07-05</updated><authors><author><keyname>Sabato</keyname><forenames>Sivan</forenames></author><author><keyname>Shalev-Shwartz</keyname><forenames>Shai</forenames></author><author><keyname>Srebro</keyname><forenames>Nathan</forenames></author><author><keyname>Hsu</keyname><forenames>Daniel</forenames></author><author><keyname>Zhang</keyname><forenames>Tong</forenames></author></authors><title>Learning Sparse Low-Threshold Linear Classifiers</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of learning a non-negative linear classifier with a
$1$-norm of at most $k$, and a fixed threshold, under the hinge-loss. This
problem generalizes the problem of learning a $k$-monotone disjunction. We
prove that we can learn efficiently in this setting, at a rate which is linear
in both $k$ and the size of the threshold, and that this is the best possible
rate. We provide an efficient online learning algorithm that achieves the
optimal rate, and show that in the batch case, empirical risk minimization
achieves this rate as well. The rates we show are tighter than the uniform
convergence rate, which grows with $k^2$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3282</identifier>
 <datestamp>2013-09-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3282</id><created>2012-12-13</created><updated>2013-09-05</updated><authors><author><keyname>Murphy</keyname><forenames>Niall</forenames><affiliation>Universidad Polit&#xe9;cnica de Madrid</affiliation></author><author><keyname>Woods</keyname><forenames>Damien</forenames><affiliation>California Institute of Technology</affiliation></author></authors><title>AND and/or OR: Uniform Polynomial-Size Circuits</title><categories>cs.CC</categories><comments>In Proceedings MCU 2013, arXiv:1309.1043</comments><proxy>EPTCS</proxy><acm-class>F.1.1; F.1.3</acm-class><journal-ref>EPTCS 128, 2013, pp. 150-166</journal-ref><doi>10.4204/EPTCS.128.20</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the complexity of uniform OR circuits and AND circuits of
polynomial-size and depth. As their name suggests, OR circuits have OR gates as
their computation gates, as well as the usual input, output and constant (0/1)
gates. As is the norm for Boolean circuits, our circuits have multiple sink
gates, which implies that an OR circuit computes an OR function on some subset
of its input variables. Determining that subset amounts to solving a number of
reachability questions on a polynomial-size directed graph (which input gates
are connected to the output gate?), taken from a very sparse set of graphs.
However, it is not obvious whether or not this (restricted) reachability
problem can be solved, by say, uniform AC^0 circuits (constant depth,
polynomial-size, AND, OR, NOT gates). This is one reason why characterizing the
power of these simple-looking circuits in terms of uniform classes turns out to
be intriguing. Another is that the model itself seems particularly natural and
worthy of study.
  Our goal is the systematic characterization of uniform polynomial-size OR
circuits, and AND circuits, in terms of known uniform machine-based complexity
classes. In particular, we consider the languages reducible to such uniform
families of OR circuits, and AND circuits, under a variety of reduction types.
We give upper and lower bounds on the computational power of these language
classes. We find that these complexity classes are closely related to tallyNL,
the set of unary languages within NL, and to sets reducible to tallyNL.
Specifically, for a variety of types of reductions (many-one, conjunctive truth
table, disjunctive truth table, truth table, Turing) we give characterizations
of languages reducible to OR circuit classes in terms of languages reducible to
tallyNL classes. Then, some of these OR classes are shown to coincide, and some
are proven to be distinct. We give analogous results for AND circuits. Finally,
for many of our OR circuit classes, and analogous AND circuit classes, we prove
whether or not the two classes coincide, although we leave one such inclusion
open.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3289</identifier>
 <datestamp>2012-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3289</id><created>2012-12-13</created><authors><author><keyname>Gupta</keyname><forenames>Smrati</forenames></author><author><keyname>V&#xe1;zquez-Castro</keyname><forenames>M. A.</forenames></author></authors><title>Compute and Forward: End to End Performance over Residue Class Signal
  Constellation</title><categories>cs.IT math.IT</categories><comments>4 pages, submitted to IEEE Communication Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this letter, the problem of implementing compute and forward (CF) is
addressed. We present a practical signal model to implement CF which is built
on the basis of Gaussian integer lattice partitions. We provide practical
decoding functions at both relay and destination nodes thereby providing a
framework for complete analysis of CF. Our main result is the analytical
derivation and simulations based validation of union bound of probability of
error for end to end performance of CF. We show that the performance is not
limited by the linear combination decoding at the relay but by the full rank
requirement of the coefficient matrix at the destination.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3293</identifier>
 <datestamp>2014-06-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3293</id><created>2012-12-13</created><updated>2014-06-07</updated><authors><author><keyname>Marichal</keyname><forenames>Jean-Luc</forenames></author><author><keyname>Teheux</keyname><forenames>Bruno</forenames></author></authors><title>Pivotal decompositions of functions</title><categories>math.RA cs.DM</categories><msc-class>94C10</msc-class><journal-ref>Discrete Applied Mathematics 174 (2014) 102-112</journal-ref><doi>10.1016/j.dam.2014.04.013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We extend the well-known Shannon decomposition of Boolean functions to more
general classes of functions. Such decompositions, which we call pivotal
decompositions, express the fact that every unary section of a function only
depends upon its values at two given elements. Pivotal decompositions appear to
hold for various function classes, such as the class of lattice polynomial
functions or the class of multilinear polynomial functions. We also define
function classes characterized by pivotal decompositions and function classes
characterized by their unary members and investigate links between these two
concepts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3295</identifier>
 <datestamp>2013-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3295</id><created>2012-12-13</created><updated>2012-12-30</updated><authors><author><keyname>Prakash</keyname><forenames>Aaditya</forenames></author></authors><title>Measures of Fault Tolerance in Distributed Simulated Annealing</title><categories>cs.DC</categories><comments>4 pages, 2 figures</comments><acm-class>C.1.4; B.8.1; G.1.6</acm-class><journal-ref>International Conference on Perspective of Computer Confluence
  with Sciences, pp 111-114, 2012, Excel Publisher</journal-ref><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  In this paper, we examine the different measures of Fault Tolerance in a
Distributed Simulated Annealing process. Optimization by Simulated Annealing on
a distributed system is prone to various sources of failure. We analyse
simulated annealing algorithm, its architecture in distributed platform and
potential sources of failures. We examine the behaviour of tolerant distributed
system for optimization task. We present possible methods to overcome the
failures and achieve fault tolerance for the distributed simulated annealing
process. We also examine the implementation of Simulated Annealing in MapReduce
system and possible ways to prevent failures in reaching the global optima.
This paper will be beneficial to those who are interested in implementing a
large scale distributed simulated annealing optimization problem of industrial
or academic interest. We recommend hybrid tolerance technique to optimize the
trade-off between efficiency and availability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3308</identifier>
 <datestamp>2012-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3308</id><created>2012-12-10</created><authors><author><keyname>Schultz</keyname><forenames>Ulrik Pagh</forenames></author><author><keyname>Stinckwich</keyname><forenames>Serge</forenames></author></authors><title>Proceedings of the Second International Workshop on Domain-Specific
  Languages and Models for Robotic Systems (DSLRob 2011)</title><categories>cs.RO</categories><comments>Index submission</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Proceedings of the Second International Workshop on Domain-Specific Languages
and Models for Robotic Systems (DSLRob'11), held in conjunction with the 2011
IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS
2011), September 2011 in San Francisco, USA.
  The main topics of the workshop were Domain-Specific Languages (DSLs) and
Model-driven Software Development (MDSD) for robotics. A domain-specific
language (DSL) is a programming language dedicated to a particular problem
domain that offers specific notations and abstractions that increase programmer
productivity within that domain. Models offer a high-level way for domain users
to specify the functionality of their system at the right level of abstraction.
DSLs and models have historically been used for programming complex systems.
However recently they have garnered interest as a separate field of study.
Robotic systems blend hardware and software in a holistic way that
intrinsically raises many crosscutting concerns (concurrency, uncertainty, time
constraints, ...), for which reason, traditional general-purpose languages
often lead to a poor fit between the language features and the implementation
requirements. DSLs and models offer a powerful, systematic way to overcome this
problem, enabling the programmer to quickly and precisely implement novel
software solutions to complex problems
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3320</identifier>
 <datestamp>2012-12-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3320</id><created>2012-12-13</created><authors><author><keyname>Duch</keyname><forenames>Jordi</forenames></author><author><keyname>Zeng</keyname><forenames>Xiao Han T.</forenames></author><author><keyname>Sales-Pardo</keyname><forenames>Marta</forenames></author><author><keyname>Radicchi</keyname><forenames>Filippo</forenames></author><author><keyname>Otis</keyname><forenames>Shayna</forenames></author><author><keyname>Woodruff</keyname><forenames>Teresa K.</forenames></author><author><keyname>Amaral</keyname><forenames>Luis A. Nunes</forenames></author></authors><title>The Possible Role of Resource Requirements and Academic Career-Choice
  Risk on Gender Differences in Publication Rate and Impact</title><categories>physics.soc-ph cs.DL physics.data-an</categories><comments>9 figures and 3 tables</comments><journal-ref>PLoS ONE 7(12): e51332</journal-ref><doi>10.1371/journal.pone.0051332</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many studies demonstrate that there is still a significant gender bias,
especially at higher career levels, in many areas including science,
technology, engineering, and mathematics (STEM). We investigated
field-dependent, gender-specific effects of the selective pressures individuals
experience as they pursue a career in academia within seven STEM disciplines.
We built a unique database that comprises 437,787 publications authored by
4,292 faculty members at top United States research universities. Our analyses
reveal that gender differences in publication rate and impact are
discipline-specific. Our results also support two hypotheses. First, the
widely-reported lower publication rates of female faculty are correlated with
the amount of research resources typically needed in the discipline considered,
and thus may be explained by the lower level of institutional support
historically received by females. Second, in disciplines where pursuing an
academic position incurs greater career risk, female faculty tend to have a
greater fraction of higher impact publications than males. Our findings have
significant, field-specific, policy implications for achieving diversity at the
faculty level within the STEM disciplines.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3333</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3333</id><created>2012-12-13</created><authors><author><keyname>Kaehler</keyname><forenames>Ralf</forenames></author><author><keyname>Abel</keyname><forenames>Tom</forenames></author></authors><title>Single-Pass GPU-Raycasting for Structured Adaptive Mesh Refinement Data</title><categories>astro-ph.IM cs.GR</categories><comments>12 pages, 7 figures. submitted to Visualization and Data Analysis
  2013</comments><doi>10.1117/12.2008552</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Structured Adaptive Mesh Refinement (SAMR) is a popular numerical technique
to study processes with high spatial and temporal dynamic range. It reduces
computational requirements by adapting the lattice on which the underlying
differential equations are solved to most efficiently represent the solution.
Particularly in astrophysics and cosmology such simulations now can capture
spatial scales ten orders of magnitude apart and more. The irregular locations
and extensions of the refined regions in the SAMR scheme and the fact that
different resolution levels partially overlap, poses a challenge for GPU-based
direct volume rendering methods. kD-trees have proven to be advantageous to
subdivide the data domain into non-overlapping blocks of equally sized cells,
optimal for the texture units of current graphics hardware, but previous
GPU-supported raycasting approaches for SAMR data using this data structure
required a separate rendering pass for each node, preventing the application of
many advanced lighting schemes that require simultaneous access to more than
one block of cells. In this paper we present a single-pass GPU-raycasting
algorithm for SAMR data that is based on a kD-tree. The tree is efficiently
encoded by a set of 3D-textures, which allows to adaptively sample complete
rays entirely on the GPU without any CPU interaction. We discuss two different
data storage strategies to access the grid data on the GPU and apply them to
several datasets to prove the benefits of the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3341</identifier>
 <datestamp>2012-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3341</id><created>2012-12-13</created><authors><author><keyname>Chanda</keyname><forenames>Abhishek</forenames></author><author><keyname>Westphal</keyname><forenames>Cedric</forenames></author></authors><title>Content as a Network Primitive</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The current functionality supported by OpenFlowbased software defined
networking (SDN) includes switching, routing, tunneling, and some basic fire
walling while operating on traffic flows. However, the semantics of SDN do not
allow for other operations on the traffic, nor does it allow operations at a
higher granularity. In this work, we describe a method to expand the SDN
framework to add other network primitives. In particular, we present a method
to integrate different network elements (like cache, proxy etc). Here, we focus
on storage and caching, but our method could be expanded to other functionality
seamlessly. We also present a method to identify content so as to perform
per-content policy, as opposed to per flow policy. We have implemented the
proposed mechanisms to demonstrate its feasibility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3357</identifier>
 <datestamp>2013-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3357</id><created>2012-12-13</created><updated>2013-11-17</updated><authors><author><keyname>Cali</keyname><forenames>Andrea</forenames></author><author><keyname>Gottlob</keyname><forenames>Georg</forenames></author><author><keyname>Kifer</keyname><forenames>Michael</forenames></author></authors><title>Taming the Infinite Chase: Query Answering under Expressive Integrity
  Constraints</title><categories>cs.LO cs.DB</categories><comments>Pre-print</comments><journal-ref>Journal of Artificial Intelligence Research, vol. 48, pp. 115-174,
  2013</journal-ref><doi>10.1613/jair.3873</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The chase algorithm is a fundamental tool for query evaluation and query
containment under constraints, where the constraints are (sub-classes of)
tuple-generating dependencies (TGDs) and equality generating depencies (EGDs).
So far, most of the research on this topic has focused on cases where the chase
procedure terminates, with some notable exceptions. In this paper we take a
general approach, and we propose large classes of TGDs under which the chase
does not always terminate. Our languages, in particular, are inspired by
guarded logic: we show that by enforcing syntactic properties on the form of
the TGDs, we are able to ensure decidability of the problem of answering
conjunctive queries despite the non-terminating chase. We provide tight
complexity bounds for the problem of conjunctive query evaluation for several
classes of TGDs. We then introduce EGDs, and provide a condition under which
EGDs do not interact with TGDs, and therefore do not take part in query
answering. We show applications of our classes of constraints to the problem of
answering conjunctive queries under F-Logic Lite, a recently introduced
ontology language, and under prominent tractable Description Logics languages.
All the results in this paper immediately extend to the problem of conjunctive
query containment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3359</identifier>
 <datestamp>2012-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3359</id><created>2012-12-13</created><authors><author><keyname>Achanta</keyname><forenames>Hema Kumari</forenames></author><author><keyname>Xu</keyname><forenames>Weiyu</forenames></author><author><keyname>Dasgupta</keyname><forenames>Soura</forenames></author></authors><title>Matrix Design for Optimal Sensing</title><categories>cs.IT math.IT</categories><comments>conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We design optimal $2 \times N$ ($2 &lt;N$) matrices, with unit columns, so that
the maximum condition number of all the submatrices comprising 3 columns is
minimized. The problem has two applications. When estimating a 2-dimensional
signal by using only three of $N$ observations at a given time, this minimizes
the worst-case achievable estimation error. It also captures the problem of
optimum sensor placement for monitoring a source located in a plane, when only
a minimum number of required sensors are active at any given time. For
arbitrary $N\geq3$, we derive the optimal matrices which minimize the maximum
condition number of all the submatrices of three columns. Surprisingly, a
uniform distribution of the columns is \emph{not} the optimal design for odd
$N\geq 7$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3362</identifier>
 <datestamp>2016-01-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3362</id><created>2012-12-13</created><authors><author><keyname>Manipatruni</keyname><forenames>Sasikanth</forenames></author><author><keyname>Nikonov</keyname><forenames>Dmitri E.</forenames></author><author><keyname>Young</keyname><forenames>Ian A.</forenames></author></authors><title>Material Targets for Scaling All Spin Logic</title><categories>cond-mat.mes-hall cond-mat.mtrl-sci cs.ET</categories><comments>21 pages, 8 figures</comments><acm-class>B.3; B.6.1; B.7.0; B.7.1; B.7.2; J.2; G.4; I.6.1</acm-class><journal-ref>Phys. Rev. Applied 5, 014002 (2016)</journal-ref><doi>10.1103/PhysRevApplied.5.014002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  All-spin logic devices are promising candidates to augment and complement
beyond-CMOS integrated circuit computing due to non-volatility, ultra-low
operating voltages, higher logical efficiency, and high density integration.
However, the path to reach lower energy-delay product performance compared to
CMOS transistors currently is not clear. We show that scaling and engineering
the nanoscale magnetic materials and interfaces is the key to realizing spin
logic devices that can surpass energy-delay performance of CMOS transistors.
With validated stochastic nano-magnetic and vector spin transport numerical
models, we derive the target material and interface properties for the
nanomagnets and channels. We identified promising new directions for material
engineering/discovery focusing on systematic scaling of magnetic anisotropy
(Hk) with saturation magnetization (Ms), use of perpendicular magnetic
anisotropy, and interface spin mixing conductance of ferromagnet/spin channel
interface (Gmix). We provide systematic targets for scaling spin logic
energy-delay product toward a 2 aJ.ns energy-delay product, comprehending the
stochastic noise for nanomagnets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3368</identifier>
 <datestamp>2012-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3368</id><created>2012-12-13</created><authors><author><keyname>Jha</keyname><forenames>P. K.</forenames></author><author><keyname>Mandal</keyname><forenames>J. K.</forenames></author></authors><title>Space Efficient Cryptographic Protocol Using Recursive Bitwise &amp; Pairs
  Of Bits Of Operation (RBPBO)</title><categories>cs.CR</categories><comments>5 page paper in Proceeding of International Conference on Computing
  and Systems ICCS 2010, ISBN 93-80813-01-5, pp 151-155, University of Burdwan,
  19th, 20th November, 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The technique considers a message as binary string on which a Efficient
Cryptographic Protocol using Recursive Bitwise amd pairs of Bits of operation
(RBPBO) is performed. A block of n bits is taken as an input stream, where n
varies from 4 to 256, from a continuous stream of bits and the technique
operates on it to generate the intermediate encrypted stream. This technique
directly involves all the bits of blocks in a boolean operation and a session
key. The same operation is performed repeatedly for different block sizes as
per the specification of a session key to generate the final encrypted stream.
It is a kind of block cipher and symmetric in nature hence, decoding is done
following the same procedure. A comparison of the proposed technique with
existing and industrially accepted RSA and TDES has also been done in terms of
frequency distribution and non homogeneity of source and encrypted files. Key
words: Space Efficient Cryptographic Protocol using Recursive Bitwise &amp; pairs
of Bits of operation (RBPBO),Cipher text, Block cipher, Session Key.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3370</identifier>
 <datestamp>2012-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3370</id><created>2012-12-13</created><authors><author><keyname>Mandal</keyname><forenames>J. K.</forenames></author><author><keyname>Ghatak</keyname><forenames>S.</forenames></author></authors><title>A Novel Technique for Secret Message / Image Transmission through (2,
  2)Visual Cryptographic Protocol (SMITVCP)</title><categories>cs.CR</categories><comments>7 page paper in Proceeding of International Conference on Computing
  and Systems ICCS 2010, ISBN 93-80813-01-5, pp 151-155, University of Burdwan,
  19th, 20th November, 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper a secret message/image transmission technique has been proposed
through (2, 2) visual cryptographic share which is non-interpretable in
general. A binary image is taken as cover image and authenticating
message/image has been fabricated into it through a hash function where two
bits in each pixel within four bits from LSB of the pixel is embedded and as a
result it converts the binary image to gray scale one. (2,2) visual
cryptographic shares are generated from this converted gray scale image. During
decoding shares are combined to regenerate the authenticated image from where
the secret message/image is obtained through the same hash function along with
reduction of noise. Noise reduction is also done on regenerated authenticated
image to regenerate original cover image at destination.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3371</identifier>
 <datestamp>2012-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3371</id><created>2012-12-13</created><authors><author><keyname>Ghoshal</keyname><forenames>Nabin</forenames></author><author><keyname>Mandal</keyname><forenames>J. K.</forenames></author></authors><title>Image Authentication Technique in Frequency Domain based on Discrete
  Fourier Transformation (IATFDDFT)</title><categories>cs.CR</categories><comments>7 page paper in Proceeding of International Conference on Computing
  and Systems ICCS 2010, ISBN 93-80813-01-5, pp 151-155, University of Burdwan,
  19th, 20th November, 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper a novel data embedding technique in frequency domain has been
proposed using Discrete Fourier Transform (DFT) for image authentication and
secured message transmission based on hiding a large volume of data into gray
images. Image authentication is done by embedding message or image in frequency
domain by choosing image blocks of size 2 x 2, called mask, from the source
image in row major order and transform it into the frequency domain using DFT.
Three bits of authenticating message/image/message-digest are fabricated within
the real parts of each source image byte except first frequency component of
each mask. The dimension of authenticating image followed by message digest
(MD) and the content of authenticating message/image are also embedded. Inverse
DFT (IDFT) is performed on embedded data to transform embedded frequency
component to spatial component. In order to keep the quantum value positive and
non negative in spatial domain a strong and robust technique is incorporated
mainly on the first frequency component and sometimes on other component
depends upon situations. The decoding is done by applying the reverse
algorithm. Experimental results conform that the proposed algorithm performs
better than DCT, QFT and SCDFT schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3373</identifier>
 <datestamp>2012-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3373</id><created>2012-12-13</created><authors><author><keyname>Mandal</keyname><forenames>J. K.</forenames></author><author><keyname>Mukhopadhyay</keyname><forenames>Somnath</forenames></author></authors><title>A Novel Directional Weighted Minimum Deviation (DWMD) Based Filter for
  Removal of Random Valued Impulse Noise</title><categories>cs.CV</categories><comments>7 page paper in Proceeding of International Conference on Computing
  and Systems ICCS 2010, ISBN 93-80813-01-5, pp 151-155, University of Burdwan,
  19th, 20th November, 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The most median-based de noising methods works fine for restoring the images
corrupted by Randomn Valued Impulse Noise with low noise level but very poor
with highly corrupted images. In this paper a directional weighted minimum
deviation (DWMD) based filter has been proposed for removal of high random
valued impulse noise (RVIN). The proposed approach based on Standard Deviation
(SD) works in two phases. The first phase detects the contaminated pixels by
differencing between the test pixel and its neighbor pixels aligned with four
main directions. The second phase filters only those pixels keeping others
intact. The filtering scheme is based on minimum standard deviation of the four
directional pixels. Extensive simulations show that the proposed filter not
only provide better performance of de noising RVIN but can preserve more
details features even thin lines or dots. This technique shows better
performance in terms of PSNR, Image Fidelity and Computational Cost compared to
the existing filters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3374</identifier>
 <datestamp>2013-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3374</id><created>2012-12-13</created><updated>2013-07-12</updated><authors><author><keyname>&#x15e;ahin</keyname><forenames>Alphan</forenames></author><author><keyname>G&#xfc;ven&#xe7;</keyname><forenames>Ismail</forenames></author><author><keyname>Arslan</keyname><forenames>H&#xfc;seyin</forenames></author></authors><title>A Survey on Multicarrier Communications: Prototype Filters, Lattice
  Structures, and Implementation Aspects</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Communications Surveys &amp; Tutorials, 27 pages, 14
  figures (main), (First Revision)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Due to their numerous advantages, communications over multicarrier schemes
constitute an appealing approach for broadband wireless systems. Especially,
the strong penetration of orthogonal frequency division multiplexing (OFDM)
into the communications standards has triggered heavy investigation on
multicarrier systems, leading to re-consideration of different approaches as an
alternative to OFDM. The goal of the present survey is not only to provide a
unified review of waveform design options for multicarrier schemes, but also to
pave the way for the evolution of the multicarrier schemes from the current
state of the art to future technologies. In particular, a generalized framework
on multicarrier schemes is presented, based on what to transmit, i.e., symbols,
how to transmit, i.e., filters, and where/when to transmit, i.e., lattice.
Capitalizing on this framework, different variations of orthogonal,
bi-orthogonal, and nonorthogonal multicarrier schemes are discussed. In
addition, filter design for various multicarrier systems is reviewed
considering four different design perspectives: energy concentration, rapid
decay, spectrum nulling, and channel/hardware characteristics. Subsequently,
evaluation tools which may be used to compare different filters in multicarrier
schemes are studied. Finally, multicarrier schemes are evaluated from the view
of the practical implementation issues, such as lattice adaptation,
equalization, synchronization, multiple antennas, and hardware impairments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3376</identifier>
 <datestamp>2013-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3376</id><created>2012-12-13</created><updated>2013-03-29</updated><authors><author><keyname>Jiang</keyname><forenames>Feng</forenames></author><author><keyname>Chen</keyname><forenames>Jie</forenames></author><author><keyname>Swindlehurst</keyname><forenames>A. Lee</forenames></author></authors><title>Linearly Reconfigurable Kalman Filtering for a Vector Process</title><categories>cs.IT math.IT</categories><comments>5 pages, 2 figures, accepted by IEEE ICASSP 2013, Feb. 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider a dynamic linear system in state-space form where
the observation equation depends linearly on a set of parameters. We address
the problem of how to dynamically calculate these parameters in order to
minimize the mean-squared error (MSE) of the state estimate achieved by a
Kalman filter. We formulate and solve two kinds of problems under a quadratic
constraint on the observation parameters: minimizing the sum MSE (Min-Sum-MSE)
or minimizing the maximum MSE (Min-Max-MSE). In each case, the optimization
problem is divided into two sub-problems for which optimal solutions can be
found: a semidefinite programming (SDP) problem followed by a constrained
least-squares minimization. A more direct solution is shown to exist for the
special case of a scalar observation; in particular, the Min-Sum-MSE solution
can be found directly using a generalized eigendecomposition, and is optimally
solved utilizing Rayleigh quotient, and the Min-Max-MSE problem reduces to an
SDP feasibility test that can be solved via the bisection method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3380</identifier>
 <datestamp>2012-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3380</id><created>2012-12-13</created><authors><author><keyname>Yang</keyname><forenames>Jed</forenames></author></authors><title>Rectangular tileability and complementary tileability are undecidable</title><categories>math.CO cs.CC cs.CG</categories><comments>16 pages, 8 figures</comments><msc-class>52C20 (Primary) 05B45, 68Q17 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Does a given a set of polyominoes tile some rectangle? We show that this
problem is undecidable. In a different direction, we also consider tiling a
cofinite subset of the plane. The tileability is undecidable for many variants
of this problem. However, we present an algorithm for testing whether the
complement of a finite region is tileable by a set of rectangles.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3381</identifier>
 <datestamp>2012-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3381</id><created>2012-12-13</created><authors><author><keyname>Tran</keyname><forenames>Nguyen H.</forenames></author><author><keyname>Hong</keyname><forenames>Choong Seon</forenames></author><author><keyname>Lee</keyname><forenames>Sungwon</forenames></author><author><keyname>Han</keyname><forenames>Zhu</forenames></author></authors><title>Optimal Pricing Effect on Equilibrium Behaviors of Delay-Sensitive Users
  in Cognitive Radio Networks</title><categories>cs.NI</categories><comments>30 pages, one column, double space</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper studies price-based spectrum access control in cognitive radio
networks, which characterizes network operators' service provisions to
delay-sensitive secondary users (SUs) via pricing strategies. Based on the two
paradigms of shared-use and exclusive-use dynamic spectrum access (DSA), we
examine three network scenarios corresponding to three types of secondary
markets. In the first monopoly market with one operator using opportunistic
shared-use DSA, we study the operator's pricing effect on the equilibrium
behaviors of self-optimizing SUs in a queueing system. %This queue represents
the congestion of the multiple SUs sharing the operator's single \ON-\OFF
channel that models the primary users (PUs) traffic. We provide a queueing
delay analysis with the general distributions of the SU service time and PU
traffic using the renewal theory. In terms of SUs, we show that there exists a
unique Nash equilibrium in a non-cooperative game where SUs are players
employing individual optimal strategies. We also provide a sufficient condition
and iterative algorithms for equilibrium convergence. In terms of operators,
two pricing mechanisms are proposed with different goals: revenue maximization
and social welfare maximization. In the second monopoly market, an operator
exploiting exclusive-use DSA has many channels that will be allocated
separately to each entering SU. We also analyze the pricing effect on the
equilibrium behaviors of the SUs and the revenue-optimal and socially-optimal
pricing strategies of the operator in this market. In the third duopoly market,
we study a price competition between two operators employing shared-use and
exclusive-use DSA, respectively, as a two-stage Stackelberg game. Using a
backward induction method, we show that there exists a unique equilibrium for
this game and investigate the equilibrium convergence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3385</identifier>
 <datestamp>2014-09-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3385</id><created>2012-12-13</created><updated>2014-09-13</updated><authors><author><keyname>Shi</keyname><forenames>Mao</forenames></author><author><keyname>Deng</keyname><forenames>Jiansong</forenames></author></authors><title>Approximating rational Bezier curves by constrained Bezier curves of
  arbitrary degree</title><categories>math.NA cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a method to obtain a constrained approximation of a
rational B\'{e}zier curve by a polynomial B\'{e}zier curve. This problem is
reformulated as an approximation problem between two polynomial B\'{e}zier
curves based on weighted least-squares method, where weight functions
$\rho(t)=\omega(t)$ and $\rho(t)=\omega(t)^{2}$ are studied respectively. The
efficiency of the proposed method is tested using some examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3390</identifier>
 <datestamp>2012-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3390</id><created>2012-12-13</created><authors><author><keyname>Majumder</keyname><forenames>Anirban</forenames></author><author><keyname>Shrivastava</keyname><forenames>Nisheeth</forenames></author></authors><title>Know Your Personalization: Learning Topic level Personalization in
  Online Services</title><categories>cs.LG cs.IR</categories><comments>privacy, personalization</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online service platforms (OSPs), such as search engines, news-websites,
ad-providers, etc., serve highly pe rsonalized content to the user, based on
the profile extracted from his history with the OSP. Although personalization
(generally) leads to a better user experience, it also raises privacy concerns
for the user---he does not know what is present in his profile and more
importantly, what is being used to per sonalize content for him. In this paper,
we capture OSP's personalization for an user in a new data structure called the
person alization vector ($\eta$), which is a weighted vector over a set of
topics, and present techniques to compute it for users of an OSP. Our approach
treats OSPs as black-boxes, and extracts $\eta$ by mining only their output,
specifical ly, the personalized (for an user) and vanilla (without any user
information) contents served, and the differences in these content. We
formulate a new model called Latent Topic Personalization (LTP) that captures
the personalization vector into a learning framework and present efficient
inference algorithms for it. We do extensive experiments for search result
personalization using both data from real Google users and synthetic datasets.
Our results show high accuracy (R-pre = 84%) of LTP in finding personalized
topics. For Google data, our qualitative results show how LTP can also
identifies evidences---queries for results on a topic with high $\eta$ value
were re-ranked. Finally, we show how our approach can be used to build a new
Privacy evaluation framework focused at end-user privacy on commercial OSPs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3393</identifier>
 <datestamp>2012-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3393</id><created>2012-12-14</created><authors><author><keyname>Hunter</keyname><forenames>Timothy</forenames></author><author><keyname>Das</keyname><forenames>Tathagata</forenames></author><author><keyname>Zaharia</keyname><forenames>Matei</forenames></author><author><keyname>Abbeel</keyname><forenames>Pieter</forenames></author><author><keyname>Bayen</keyname><forenames>Alexandre M.</forenames></author></authors><title>Large Scale Estimation in Cyberphysical Systems using Streaming Data: a
  Case Study with Smartphone Traces</title><categories>cs.RO cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Controlling and analyzing cyberphysical and robotics systems is increasingly
becoming a Big Data challenge. Pushing this data to, and processing in the
cloud is more efficient than on-board processing. However, current cloud-based
solutions are not suitable for the latency requirements of these applications.
We present a new concept, Discretized Streams or D-Streams, that enables
massively scalable computations on streaming data with latencies as short as a
second.
  We experiment with an implementation of D-Streams on top of the Spark
computing framework. We demonstrate the usefulness of this concept with a novel
algorithm to estimate vehicular traffic in urban networks. Our online EM
algorithm can estimate traffic on a very large city network (the San Francisco
Bay Area) by processing tens of thousands of observations per second, with a
latency of a few seconds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3403</identifier>
 <datestamp>2013-09-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3403</id><created>2012-12-14</created><updated>2013-09-03</updated><authors><author><keyname>Guo</keyname><forenames>Longkun</forenames></author><author><keyname>Liao</keyname><forenames>Kewen</forenames></author></authors><title>A Parameterized Approximation Algorithm for The Shallow-Light Steiner
  Tree Problem</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For a given graph $G=(V,\, E)$ with a terminal set $S$ and a selected root
$r\in S$, a positive integer cost and a delay on every edge and a delay
constraint $D\in Z^{+}$, the shallow-light Steiner tree (\emph{SLST}) problem
is to compute a minimum cost tree spanning the terminals of $S$, in which the
delay between root and every vertex is restrained by $D$. This problem is
NP-hard and very hard to approximate. According to known inapproximability
results, this problem admits no approximation with ratio better than factor
$(1,\, O(\log^{2}n))$ unless $NP\subseteq DTIME(n^{\log\log n})$
\cite{khandekar2013some}, while it admits no approximation ratio better than
$(1,\, O(\log|V|))$ for D=4 unless $NP\subseteq DTIME(n^{\log\log n})$
\cite{bar2001generalized}. Hence, the paper focus on parameterized algorithm
for \emph{SLST}. We firstly present an exact algorithm for \emph{SLST} with
time complexity $O(3^{|S|}|V|D+2^{|S|}|V|^{2}D^{2}+|V|^{3}D^{3})$, where $|S|$
and $|V|$ are the number of terminals and vertices respectively. This is a
pseudo polynomial time parameterized algorithm with respect to the
parameterization: &quot;number of terminals&quot;. Later, we improve this algorithm such
that it runs in polynomial time
$O(\frac{|V|^{2}}{\epsilon}3^{|S|}+\frac{|V|^{4}}{\epsilon}2^{|S|}+\frac{|V|^{6}}{\epsilon})$,
and computes a Steiner tree with delay bounded by $(1+\epsilon)D$ and cost
bounded by the cost of an optimum solution, where $\epsilon&gt;0$ is any small
real number. To the best of our knowledge, this is the first parameterized
approximation algorithm for the \emph{SLST} problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3404</identifier>
 <datestamp>2012-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3404</id><created>2012-12-14</created><authors><author><keyname>Barik</keyname><forenames>Nikhilesh</forenames></author><author><keyname>Karforma</keyname><forenames>Sunil</forenames></author><author><keyname>Mondal</keyname><forenames>J. K.</forenames></author><author><keyname>Ghosh</keyname><forenames>Arpita</forenames></author></authors><title>Towards Design and Implementation of Space Efficient and Secured
  Transmission scheme on EGovernance data</title><categories>cs.CR</categories><comments>6 Page paper in Proceeding of International Conference on Computing
  and Systems ICCS 2010, ISBN 93-80813-01-5, pp 151-155, University of Burdwan,
  19th, 20th November, 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We know that large amount of data and information should be transmitted
through internet during transactions in E-Governance. Smart E-Governance system
should deliver speedy, space efficient, cost effective and secure services
among other governments and its citizens utilizing benefits of Information and
Communication Technologies (ICT). This paper proposes to develop a space
efficient and secured data transmission scheme using Modified Huffman algorithm
for compression, which will also yield better bandwidth utilization and inner
encryption technique with one way hash function SHA (Secured Hash Algorithm) to
ensure Message integrity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3416</identifier>
 <datestamp>2014-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3416</id><created>2012-12-14</created><authors><author><keyname>Cong</keyname><forenames>Shuang</forenames></author><author><keyname>Meng</keyname><forenames>Fangfang</forenames></author><author><keyname>Liu</keyname><forenames>Jianxiu</forenames></author></authors><title>Implicit Lyapunov Control for the Quantum Liouville Equation</title><categories>cs.SY math-ph math.MP quant-ph</categories><comments>8 pages, 2 figures</comments><msc-class>65M12</msc-class><journal-ref>Control Theory and Informatics,Vol. 4, No. 6, pp. 21- 32, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A quantum system whose internal Hamiltonian is not strongly regular or/and
control Hamiltonians are not full connected, are thought to be in the
degenerate cases. In this paper, convergence problems of the multi-control
Hamiltonians closed quantum systems in the degenerate cases are solved by
introducing implicit function perturbations and choosing an implicit Lyapunov
function based on the average value of an imaginary mechanical quantity. For
the diagonal and non-diagonal tar-get states, respectively, control laws are
designed. The convergence of the control system is proved, and an explicit
design principle of the imaginary mechanical quantity is proposed. By using the
proposed method, the multi-control Hamiltonians closed quantum systems in the
degenerate cases can converge from any initial state to an arbitrary target
state unitarily equivalent to the initial state. Finally, numerical simulations
are studied to verify the effectiveness of the proposed control method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3418</identifier>
 <datestamp>2012-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3418</id><created>2012-12-14</created><authors><author><keyname>Bampas</keyname><forenames>Evangelos</forenames><affiliation>MIS</affiliation></author><author><keyname>Lamani</keyname><forenames>Anissa</forenames><affiliation>MIS</affiliation></author><author><keyname>Petit</keyname><forenames>Franck</forenames><affiliation>LIP6</affiliation></author><author><keyname>Valero</keyname><forenames>Mathieu</forenames><affiliation>LIP6, INRIA Rocquencourt</affiliation></author></authors><title>Self-Stabilizing Balancing Algorithm for Containment-Based Trees</title><categories>cs.DC cs.CC</categories><comments>(2012)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Containment-based trees encompass various handy structures such as B+-trees,
R-trees and M-trees. They are widely used to build data indexes,
range-queryable overlays, publish/subscribe systems both in centralized and
distributed contexts. In addition to their versatility, their balanced shape
ensures an overall satisfactory performance. Re- cently, it has been shown that
their distributed implementations can be fault-resilient. However, this
robustness is achieved at the cost of un-balancing the structure. While the
structure remains correct in terms of searchability, its performance can be
significantly decreased. In this paper, we propose a distributed
self-stabilizing algorithm to balance containment-based trees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3425</identifier>
 <datestamp>2012-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3425</id><created>2012-12-14</created><authors><author><keyname>Erokhin</keyname><forenames>Victor</forenames></author><author><keyname>Howard</keyname><forenames>Gerard David</forenames></author><author><keyname>Adamatzky</keyname><forenames>Andrew</forenames></author></authors><title>Organic Memristor Devices for Logic Elements with Memory</title><categories>cs.ET physics.chem-ph</categories><journal-ref>Int J Bifurcation Chaos 22 (2012) 1250283</journal-ref><doi>10.1142/S0218127412502835</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Memristors are promising next-generation memory candidates that are
nonvolatile, possess low power requirements and are capable of nanoscale
fabrication. In this article we physically realise and describe the use of
organic memristors in designing statefull boolean logic gates for the AND OR
and NOT operations. The output of these gates is analog and dependent on the
length of time that suitable charge is applied to the inputs, displaying a
learning property. Results may be also interpreted in a traditional binary
manner through use of a suitable thresholding function at the output. The
memristive property of the gate allows the for the production of analog outputs
that vary based on the charge-dependent nonvolatile state of the memristor. We
provide experimental results of physical fabrication of three types of logic
gate. A simulation of a one-bit full adder comprised of memristive logic gates
is also included, displaying varying response to two distinct input patterns.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3441</identifier>
 <datestamp>2012-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3441</id><created>2012-12-14</created><authors><author><keyname>Howard</keyname><forenames>Gerard</forenames></author><author><keyname>Gale</keyname><forenames>Ella</forenames></author><author><keyname>Bull</keyname><forenames>Larry</forenames></author><author><keyname>Costello</keyname><forenames>Ben de Lacy</forenames></author><author><keyname>Adamatzky</keyname><forenames>Andy</forenames></author></authors><title>Evolution of Plastic Learning in Spiking Networks via Memristive
  Connections</title><categories>cs.ET cs.NE</categories><journal-ref>IEEE Trans Evolutionary Computation 16 (2012) 711--729</journal-ref><doi>10.1109/TEVC.2011.2170199</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article presents a spiking neuroevolutionary system which implements
memristors as plastic connections, i.e. whose weights can vary during a trial.
The evolutionary design process exploits parameter self-adaptation and variable
topologies, allowing the number of neurons, connection weights, and
inter-neural connectivity pattern to emerge. By comparing two phenomenological
real-world memristor implementations with networks comprised of (i) linear
resistors (ii) constant-valued connections, we demonstrate that this approach
allows the evolution of networks of appropriate complexity to emerge whilst
exploiting the memristive properties of the connections to reduce learning
time. We extend this approach to allow for heterogeneous mixtures of memristors
within the networks; our approach provides an in-depth analysis of network
structure. Our networks are evaluated on simulated robotic navigation tasks;
results demonstrate that memristive plasticity enables higher performance than
constant-weighted connections in both static and dynamic reward scenarios, and
that mixtures of memristive elements provide performance advantages when
compared to homogeneous memristive networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3454</identifier>
 <datestamp>2012-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3454</id><created>2012-12-14</created><authors><author><keyname>Fahrenberg</keyname><forenames>Uli</forenames><affiliation>Irisa / INRIA Rennes, France</affiliation></author><author><keyname>Legay</keyname><forenames>Axel</forenames><affiliation>Irisa / INRIA Rennes, France</affiliation></author><author><keyname>Thrane</keyname><forenames>Claus</forenames><affiliation>Aalborg University, Denmark</affiliation></author></authors><title>Proceedings Quantities in Formal Methods</title><categories>cs.LO cs.FL cs.LG cs.SE</categories><proxy>EPTCS</proxy><journal-ref>EPTCS 103, 2012</journal-ref><doi>10.4204/EPTCS.103</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains the proceedings of the Workshop on Quantities in Formal
Methods, QFM 2012, held in Paris, France on 28 August 2012. The workshop was
affiliated with the 18th Symposium on Formal Methods, FM 2012. The focus of the
workshop was on quantities in modeling, verification, and synthesis. Modern
applications of formal methods require to reason formally on quantities such as
time, resources, or probabilities. Standard formal methods and tools have
gotten very good at modeling (and verifying) qualitative properties: whether or
not certain events will occur. During the last years, these methods and tools
have been extended to also cover quantitative aspects, notably leading to tools
like e.g. UPPAAL (for real-time systems), PRISM (for probabilistic systems),
and PHAVer (for hybrid systems). A lot of work remains to be done however
before these tools can be used in the industrial applications at which they are
aiming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3458</identifier>
 <datestamp>2012-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3458</id><created>2012-12-14</created><authors><author><keyname>Carbone</keyname><forenames>Marco</forenames></author><author><keyname>Lanese</keyname><forenames>Ivan</forenames></author><author><keyname>Silva</keyname><forenames>Alexandra</forenames></author><author><keyname>Sokolova</keyname><forenames>Ana</forenames></author></authors><title>Proceedings Fifth Interaction and Concurrency Experience</title><categories>cs.PL cs.LO cs.SE</categories><comments>EPTCS 104, 2012</comments><proxy>EPTCS</proxy><doi>10.4204/EPTCS.104</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains the proceedings of ICE'12, the 5th Interaction and
Concurrency Experience workshop, which was held in Stockholm, Sweden on the
16th of June 2012 as a satellite event of DisCoTec'12. The topic of ICE'12 was
Distributed Coordination, Execution Models, and Resilient Interaction. The ICE
procedure for paper selection allows for PC members to interact, anonymously,
with authors. During the review phase, each submitted paper is published on a
Wiki and associated with a discussion forum whose access is restricted to the
authors and to all the PC members not declaring a conflict of interests. The PC
members post comments and questions that the authors reply to. Each paper was
reviewed by four PC members, and altogether 8 papers were accepted for
publication. We were proud to host two invited talks, Marcello Bonsangue and
Ichiro Hasuo, whose abstracts are included in this volume together with the
regular papers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3467</identifier>
 <datestamp>2012-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3467</id><created>2012-12-14</created><authors><author><keyname>Kim</keyname><forenames>Hyun Kwang</forenames></author><author><keyname>Toan</keyname><forenames>Phan Thanh</forenames></author></authors><title>Improved Semidefinite Programming Bound on Sizes of Codes</title><categories>cs.IT math.CO math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $A(n,d)$ (respectively $A(n,d,w)$) be the maximum possible number of
codewords in a binary code (respectively binary constant-weight $w$ code) of
length $n$ and minimum Hamming distance at least $d$. By adding new linear
constraints to Schrijver's semidefinite programming bound, which is obtained
from block-diagonalising the Terwilliger algebra of the Hamming cube, we obtain
two new upper bounds on $A(n,d)$, namely $A(18,8) \leq 71$ and $A(19,8) \leq
131$. Twenty three new upper bounds on $A(n,d,w)$ for $n \leq 28$ are also
obtained by a similar way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3471</identifier>
 <datestamp>2012-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3471</id><created>2012-12-14</created><authors><author><keyname>Karpinski</keyname><forenames>Marek</forenames></author><author><keyname>Lingas</keyname><forenames>Andrzej</forenames></author><author><keyname>Sledneu</keyname><forenames>Dzmitry</forenames></author></authors><title>Optimal Cuts and Partitions in Tree Metrics in Polynomial Time</title><categories>cs.DS cs.CC cs.DM math.CO math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a polynomial time dynamic programming algorithm for optimal
partitions in the shortest path metric induced by a tree. This resolves, among
other things, the exact complexity status of the optimal partition problems in
one dimensional geometric metric settings. Our method of solution could be also
of independent interest in other applications. We discuss also an extension of
our method to the class of metrics induced by the bounded treewidth graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3480</identifier>
 <datestamp>2012-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3480</id><created>2012-12-14</created><authors><author><keyname>Richter</keyname><forenames>Stefan</forenames></author><author><keyname>Quian&#xe9;-Ruiz</keyname><forenames>Jorge-Arnulfo</forenames></author><author><keyname>Schuh</keyname><forenames>Stefan</forenames></author><author><keyname>Dittrich</keyname><forenames>Jens</forenames></author></authors><title>Towards Zero-Overhead Adaptive Indexing in Hadoop</title><categories>cs.DB cs.DC</categories><comments>Tech Report, Saarland University</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several research works have focused on supporting index access in MapReduce
systems. These works have allowed users to significantly speed up selective
MapReduce jobs by orders of magnitude. However, all these proposals require
users to create indexes upfront, which might be a difficult task in certain
applications (such as in scientific and social applications) where workloads
are evolving or hard to predict. To overcome this problem, we propose LIAH
(Lazy Indexing and Adaptivity in Hadoop), a parallel, adaptive approach for
indexing at minimal costs for MapReduce systems. The main idea of LIAH is to
automatically and incrementally adapt to users' workloads by creating clustered
indexes on HDFS data blocks as a byproduct of executing MapReduce jobs. Besides
distributing indexing efforts over multiple computing nodes, LIAH also
parallelises indexing with both map tasks computation and disk I/O. All this
without any additional data copy in main memory and with minimal
synchronisation. The beauty of LIAH is that it piggybacks index creation on map
tasks, which read relevant data from disk to main memory anyways. Hence, LIAH
does not introduce any additional read I/O-costs and exploit free CPU cycles.
As a result and in contrast to existing adaptive indexing works, LIAH has a
very low (or invisible) indexing overhead, usually for the very first job.
Still, LIAH can quickly converge to a complete index, i.e. all HDFS data blocks
are indexed. Especially, LIAH can trade early job runtime improvements with
fast complete index convergence. We compare LIAH with HAIL, a state-of-the-art
indexing technique, as well as with standard Hadoop with respect to indexing
overhead and workload performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3493</identifier>
 <datestamp>2012-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3493</id><created>2012-12-14</created><updated>2012-12-17</updated><authors><author><keyname>Molina</keyname><forenames>Alejandro</forenames></author><author><keyname>Torres-Moreno</keyname><forenames>Juan-Manuel</forenames></author><author><keyname>da Cunha</keyname><forenames>Iria</forenames></author><author><keyname>SanJuan</keyname><forenames>Eric</forenames></author><author><keyname>Sierra</keyname><forenames>Gerardo</forenames></author></authors><title>Sentence Compression in Spanish driven by Discourse Segmentation and
  Language Models</title><categories>cs.CL cs.IR</categories><comments>7 pages, 3 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Previous works demonstrated that Automatic Text Summarization (ATS) by
sentences extraction may be improved using sentence compression. In this work
we present a sentence compressions approach guided by level-sentence discourse
segmentation and probabilistic language models (LM). The results presented here
show that the proposed solution is able to generate coherent summaries with
grammatical compressed sentences. The approach is simple enough to be
transposed into other languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3496</identifier>
 <datestamp>2013-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3496</id><created>2012-12-14</created><authors><author><keyname>Honkonen</keyname><forenames>I.</forenames><affiliation>Finnish Meteorological Institute, Helsinki, Finland</affiliation><affiliation>Department of Physics, University of Helsinki, Helsinki, Finland</affiliation></author><author><keyname>von Alfthan</keyname><forenames>S.</forenames><affiliation>Finnish Meteorological Institute, Helsinki, Finland</affiliation></author><author><keyname>Sandroos</keyname><forenames>A.</forenames><affiliation>Finnish Meteorological Institute, Helsinki, Finland</affiliation></author><author><keyname>Janhunen</keyname><forenames>P.</forenames><affiliation>Finnish Meteorological Institute, Helsinki, Finland</affiliation></author><author><keyname>Palmroth</keyname><forenames>M.</forenames><affiliation>Finnish Meteorological Institute, Helsinki, Finland</affiliation></author></authors><title>Parallel grid library for rapid and flexible simulation development</title><categories>cs.DC physics.comp-ph physics.space-ph</categories><comments>Accepted to Computer Physics Communications, 36 pages, 13 figures</comments><journal-ref>Computer Physics Communications, Volume 184, Issue 4, April 2013,
  Pages 1297-1309</journal-ref><doi>10.1016/j.cpc.2012.12.017</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an easy to use and flexible grid library for developing highly
scalable parallel simulations. The distributed cartesian cell-refinable grid
(dccrg) supports adaptive mesh refinement and allows an arbitrary C++ class to
be used as cell data. The amount of data in grid cells can vary both in space
and time allowing dccrg to be used in very different types of simulations, for
example in fluid and particle codes. Dccrg transfers the data between
neighboring cells on different processes transparently and asynchronously
allowing one to overlap computation and communication. This enables excellent
scalability at least up to 32 k cores in magnetohydrodynamic tests depending on
the problem and hardware. In the version of dccrg presented here part of the
mesh metadata is replicated between MPI processes reducing the scalability of
adaptive mesh refinement (AMR) to between 200 and 600 processes. Dccrg is free
software that anyone can use, study and modify and is available at
[https://gitorious.org/dccrg]. Users are also kindly requested to cite this
work when publishing results obtained with dccrg.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3501</identifier>
 <datestamp>2012-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3501</id><created>2012-12-14</created><authors><author><keyname>Bj&#xf6;rklund</keyname><forenames>Henrik</forenames></author><author><keyname>Schuster</keyname><forenames>Martin</forenames></author><author><keyname>Schwentick</keyname><forenames>Thomas</forenames></author><author><keyname>Kulbatzki</keyname><forenames>Joscha</forenames></author></authors><title>On optimum left-to-right strategies for active context-free games</title><categories>cs.DB</categories><comments>To appear in ICDT 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Active context-free games are two-player games on strings over finite
alphabets with one player trying to rewrite the input string to match a target
specification. These games have been investigated in the context of exchanging
Active XML (AXML) data. While it was known that the rewriting problem is
undecidable in general, it is shown here that it is EXPSPACE-complete to decide
for a given context-free game, whether all safely rewritable strings can be
safely rewritten in a left-to-right manner, a problem that was previously
considered by Abiteboul et al. Furthermore, it is shown that the corresponding
problem for games with finite replacement languages is EXPTIME-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3502</identifier>
 <datestamp>2012-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3502</id><created>2012-12-14</created><authors><author><keyname>Dwivedi</keyname><forenames>Shri Prakash</forenames></author></authors><title>Adaptive Scheduling in Real-Time Systems Through Period Adjustment</title><categories>cs.OS</categories><comments>8 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Real time system technology traditionally developed for safety critical
systems, has now been extended to support multimedia systems and virtual
reality. A large number of real-time application, related to multimedia and
adaptive control system, require more flexibility than classical real-time
theory usually permits. This paper proposes an efficient adaptive scheduling
framework in real-time systems based on period adjustment. Under this model
periodic task can change their execution rates based on their importance value
to keep the system underloaded. We propose Period_Adjust algorithm, which
consider the tasks whose periods are bounded as well as the tasks whose periods
are not bounded.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3517</identifier>
 <datestamp>2012-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3517</id><created>2012-12-14</created><authors><author><keyname>Gast</keyname><forenames>Mikael</forenames></author><author><keyname>Hauptmann</keyname><forenames>Mathias</forenames></author><author><keyname>Karpinski</keyname><forenames>Marek</forenames></author></authors><title>Inapproximability of Dominating Set in Power Law Graphs</title><categories>cs.CC cs.DM cs.DS math.CO math.OC</categories><comments>23 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give logarithmic lower bounds for the approximability of the Minimum
Dominating Set problem in connected (alpha,beta)-Power Law Graphs. We give also
a best up to now upper approximation bound on the problem for the case of the
parameters beta&gt;2. We develop also a new functional method for proving lower
approximation bounds and display a sharp phase transition between
approximability and inapproximability of the underlying problem. This method
could also be of independent interest.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3524</identifier>
 <datestamp>2013-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3524</id><created>2012-12-14</created><updated>2013-11-08</updated><authors><author><keyname>Tremblay</keyname><forenames>Nicolas</forenames></author><author><keyname>Barrat</keyname><forenames>Alain</forenames></author><author><keyname>Forest</keyname><forenames>Cary</forenames></author><author><keyname>Nornberg</keyname><forenames>Mark</forenames></author><author><keyname>Pinton</keyname><forenames>Jean-Fran&#xe7;ois</forenames></author><author><keyname>Borgnat</keyname><forenames>Pierre</forenames></author></authors><title>Bootstrapping under constraint for the assessment of group behavior in
  human contact networks</title><categories>physics.soc-ph cs.SI math.ST stat.TH</categories><journal-ref>Phys. Rev. E 88, 052812 (2013)</journal-ref><doi>10.1103/PhysRevE.88.052812</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The increasing availability of time --and space-- resolved data describing
human activities and interactions gives insights into both static and dynamic
properties of human behavior. In practice, nevertheless, real-world datasets
can often be considered as only one realisation of a particular event. This
highlights a key issue in social network analysis: the statistical significance
of estimated properties. In this context, we focus here on the assessment of
quantitative features of specific subset of nodes in empirical networks. We
present a method of statistical resampling based on bootstrapping groups of
nodes under constraints within the empirical network. The method enables us to
define acceptance intervals for various Null Hypotheses concerning relevant
properties of the subset of nodes under consideration, in order to characterize
by a statistical test its behavior as ``normal'' or not. We apply this method
to a high resolution dataset describing the face-to-face proximity of
individuals during two co-located scientific conferences. As a case study, we
show how to probe whether co-locating the two conferences succeeded in bringing
together the two corresponding groups of scientists.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3530</identifier>
 <datestamp>2015-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3530</id><created>2012-12-14</created><updated>2013-12-30</updated><authors><author><keyname>Bekkers</keyname><forenames>Erik</forenames></author><author><keyname>Duits</keyname><forenames>Remco</forenames></author><author><keyname>Berendschot</keyname><forenames>Tos</forenames></author><author><keyname>Romeny</keyname><forenames>Bart ter Haar</forenames></author></authors><title>A Multi-Orientation Analysis Approach to Retinal Vessel Tracking</title><categories>cs.CV</categories><comments>Accepted at JMIV. The final publication will become available at
  springerlink.com</comments><journal-ref>Journal of Mathematical Imaging and Vision 49(3) (2014) 583-610</journal-ref><doi>10.1007/s10851-013-0488-6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a method for retinal vasculature extraction based on
biologically inspired multi-orientation analysis. We apply multi-orientation
analysis via so-called invertible orientation scores, modeling the cortical
columns in the visual system of higher mammals. This allows us to generically
deal with many hitherto complex problems inherent to vessel tracking, such as
crossings, bifurcations, parallel vessels, vessels of varying widths and
vessels with high curvature. Our approach applies tracking in invertible
orientation scores via a novel geometrical principle for curve optimization in
the Euclidean motion group SE(2). The method runs fully automatically and
provides a detailed model of the retinal vasculature, which is crucial as a
sound basis for further quantitative analysis of the retina, especially in
screening applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3534</identifier>
 <datestamp>2012-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3534</id><created>2012-12-14</created><authors><author><keyname>Cate</keyname><forenames>Balder ten</forenames></author><author><keyname>Dalmau</keyname><forenames>V&#xed;ctor</forenames></author></authors><title>A note on the product homomorphism problem and CQ-definability</title><categories>cs.DM cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The product homomorphism problem (PHP) takes as input a finite collection of
relational structures A1, ..., An and another relational structure B, all over
the same schema, and asks whether there is a homomorphism from the direct
product A1 x ... x An to B. This problem is clearly solvable in
non-deterministic exponential time. It follows from results in [1] that the
problem is NExpTime-complete. The proof, based on a reduction from an
exponential tiling problem, uses structures of bounded domain size but with
relations of unbounded arity. In this note, we provide a self-contained proof
of NExpTime-hardness of PHP, and we show that it holds already for directed
graphs, as well as for structures of bounded arity with a bounded domain size
(but without a bound on the number of relations). We also present an
application to the CQ-definability problem (also known as the PP-definability
problem).
  [1] Ross Willard. Testing expressibility is hard. In David Cohen, editor, CP,
volume 6308 of Lecture Notes in Computer Science, pages 9-23. Springer, 2010.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3536</identifier>
 <datestamp>2014-12-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3536</id><created>2012-12-14</created><authors><author><keyname>Gonzaga</keyname><forenames>Flavio B.</forenames></author><author><keyname>Barbosa</keyname><forenames>Valmir C.</forenames></author><author><keyname>Xex&#xe9;o</keyname><forenames>Geraldo B.</forenames></author></authors><title>The network structure of mathematical knowledge according to the
  Wikipedia, MathWorld, and DLMF online libraries</title><categories>cs.SI cs.IR math.HO physics.soc-ph</categories><journal-ref>Network Science 2 (2014), 367-386</journal-ref><doi>10.1017/nws.2014.20</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the network structure of Wikipedia (restricted to its mathematical
portion), MathWorld, and DLMF. We approach these three online mathematical
libraries from the perspective of several global and local network-theoretic
features, providing for each one the appropriate value or distribution, along
with comparisons that, if possible, also include the whole of the Wikipedia or
the Web. We identify some distinguishing characteristics of all three
libraries, most of them supposedly traceable to the libraries' shared nature of
relating to a very specialized domain. Among these characteristics are the
presence of a very large strongly connected component in each of the
corresponding directed graphs, the complete absence of any clear power laws
describing the distribution of local features, and the rise to prominence of
some local features (e.g., stress centrality) that can be used to effectively
search for keywords in the libraries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3540</identifier>
 <datestamp>2012-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3540</id><created>2012-12-14</created><authors><author><keyname>Bitton</keyname><forenames>Yehonatan</forenames></author><author><keyname>Fire</keyname><forenames>Michael</forenames></author><author><keyname>Kagan</keyname><forenames>Dima</forenames></author><author><keyname>Shapira</keyname><forenames>Bracha</forenames></author><author><keyname>Rokach</keyname><forenames>Lior</forenames></author><author><keyname>Bar-Ilan</keyname><forenames>Judit</forenames></author></authors><title>Social Network Based Search for Experts</title><categories>cs.SI cs.HC cs.IR physics.soc-ph</categories><comments>Participated in HCIR 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our system illustrates how information retrieved from social networks can be
used for suggesting experts for specific tasks. The system is designed to
facilitate the task of finding the appropriate person(s) for a job, as a
conference committee member, an advisor, etc. This short description will
demonstrate how the system works in the context of the HCIR2012 published
tasks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3541</identifier>
 <datestamp>2012-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3541</id><created>2012-12-14</created><authors><author><keyname>Tai</keyname><forenames>Allen H.</forenames></author></authors><title>An inventory model for group-buying auction</title><categories>math.OC cs.GT math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Group-buying auction has become a popular marketing strategy in the last
decade. In this paper, a stochastic model is developed for an inventory system
subjects to demands from group-buying auctions. The model discussed here takes
into the account of the costs of inventory, transportation, dispatching and
re-order as well as the penalty cost of non-successful auctions. Since a new
cycle begins whenever there is a replenishment of products, the long-run
average costs of the model can be obtained by using the renewal theory. A
closed form solution of the optimal replenishment quantity is also derived.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3544</identifier>
 <datestamp>2012-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3544</id><created>2012-12-14</created><authors><author><keyname>Ammari</keyname><forenames>Habib</forenames></author><author><keyname>Boulier</keyname><forenames>Thomas</forenames></author><author><keyname>Garnier</keyname><forenames>Josselin</forenames></author><author><keyname>Kang</keyname><forenames>Hyeonbae</forenames></author><author><keyname>Wang</keyname><forenames>Han</forenames></author></authors><title>Tracking of a Mobile Target Using Generalized Polarization Tensors</title><categories>cs.NA cs.CE math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we apply an extended Kalman filter to track both the location
and the orientation of a mobile target from multistatic response measurements.
We also analyze the effect of the limited-view aspect on the stability and the
efficiency of our tracking approach. Our algorithm is based on the use of the
generalized polarization tensors, which can be reconstructed from the
multistatic response measurements by solving a linear system. The system has
the remarkable property that low order generalized polarization tensors are not
affected by the error caused by the instability of higher orders in the
presence of measurement noise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3550</identifier>
 <datestamp>2012-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3550</id><created>2012-12-14</created><authors><author><keyname>Hajizadeh</keyname><forenames>Saeed</forenames></author><author><keyname>Hodtani</keyname><forenames>Ghosheh Abed</forenames></author></authors><title>State-Dependent Multiple Access Channels with Feedback</title><categories>cs.IT math.IT</categories><comments>10 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we examine discrete memoryless Multiple Access Channels (MACs)
with two-sided feedback in the presence of two correlated channel states that
are correlated in the sense of Slepian-Wolf (SW). We find achievable rate
region for this channel when the states are provided non-causally to the
transmitters and show that our achievable rate region subsumes Cover-Leung
achievable rate for the discrete memoryless MAC with two-sided feedback as its
special case. We also find the capacity region of discrete memoryless MAC with
two-sided feedback and with SW-type correlated states available causally or
strictly causally to the transmitters. We also study discrete memoryless MAC
with partial feedback in the presence of two SW-type correlated channel states
that are provided non-causally, causally, or strictly causally to the
transmitters. An achievable rate region is found when channel states are
non-causally provided to the transmitters whereas capacity regions are
characterized when channel states are causally, or strictly causally available
at the transmitters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3555</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3555</id><created>2012-12-14</created><updated>2012-12-24</updated><authors><author><keyname>Dobre</keyname><forenames>Dan</forenames></author><author><keyname>Karame</keyname><forenames>Ghassan</forenames></author><author><keyname>Li</keyname><forenames>Wenting</forenames></author><author><keyname>Majuntke</keyname><forenames>Matthias</forenames></author><author><keyname>Suri</keyname><forenames>Neeraj</forenames></author><author><keyname>Vukolic</keyname><forenames>Marko</forenames></author></authors><title>Proofs of Writing for Efficient and Robust Storage</title><categories>cs.DC cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present PoWerStore, the first efficient robust storage protocol that
achieves optimal latency without using digital signatures. PoWerStore's
robustness comprises tolerating asynchrony, maximum number of Byzantine storage
servers, any number of Byzantine readers and crash-faulty writers, and
guaranteeing wait-freedom and linearizability of read/write operations.
PoWerStore's efficiency stems from combining lightweight authentication,
erasure coding and metadata write-backs where readers write-back only metadata
to achieve linearizability. At the heart of PoWerStore are Proofs of Writing
(PoW): a novel storage technique based on lightweight cryptography. PoW enable
reads and writes in the single-writer variant of PoWerStore to have latency of
2 rounds of communication between a client and storage servers in the
worst-case (which we show optimal). We further present and implement a
multi-writer PoWerStore variant featuring 3-round writes/reads where the third
read round is invoked only under active attacks, and show that it outperforms
existing robust storage protocols, including crash-tolerant ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3557</identifier>
 <datestamp>2012-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3557</id><created>2012-12-14</created><updated>2012-12-18</updated><authors><author><keyname>Monemizadeh</keyname><forenames>Mostafa</forenames></author><author><keyname>Hajizadeh</keyname><forenames>Saeed</forenames></author><author><keyname>Seyedin</keyname><forenames>Seyed Alireza</forenames></author><author><keyname>Hodtani</keyname><forenames>Ghosheh Abed</forenames></author></authors><title>Compound Multiple Access Channel with Common Message and Intersymbol
  Interference</title><categories>cs.IT math.IT</categories><comments>5 pages, 2 figures, This paper is presented at the International
  Symposium on Telecommunications (IST)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we characterize the capacity region for the two-user linear
Gaussian compound Multiple Access Channel with common message (MACC) and with
intersymbol interference (ISI) under an input power constraint. The region is
obtained by converting the channel to its equivalent memoryless one by defining
an n-block memoryless circular Gaussian compound MACC model and applying the
discrete Fourier transform (DFT) to decompose the n-block channel into a set of
independent parallel channels whose capacities can be found easily. Indeed, the
capacity region of the original Gaussian compound MACC equals that of the
n-block circular Gaussian compound MACC in the limit of infinite block length.
Then by using the obtained capacity region, we derive the capacity region of
the strong interference channel with common message and ISI.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3559</identifier>
 <datestamp>2012-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3559</id><created>2012-12-14</created><authors><author><keyname>Funk</keyname><forenames>Russell J.</forenames></author><author><keyname>Owen-Smith</keyname><forenames>Jason</forenames></author></authors><title>A Dynamic Network Approach to Breakthrough Innovation</title><categories>cs.SI cs.DL physics.soc-ph</categories><comments>37 pages, 3 figures, 5 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper outlines a framework for the study of innovation that treats
discoveries as additions to evolving networks. As inventions enter they expand
or limit the reach of the ideas they build on by influencing how successive
discoveries use those ideas. The approach is grounded in novel measures of the
extent to which an innovation amplifies or disrupts the status quo. Those
measures index the effects inventions have on subsequent uses of prior
discoveries. In so doing, they characterize a theoretically important but
elusive feature of innovation. We validate our approach by showing it: (1)
discriminates among innovations of similar impact in analyses of U.S. patents;
(2) identifies discoveries that amplify and disrupt technology streams in
select case studies; (3) implies disruptive patents decrease the use of their
predecessors by 60% in difference-in-differences estimation; and, (4) yields
novel findings in analyses of patenting at 110 U.S. universities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3585</identifier>
 <datestamp>2015-03-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3585</id><created>2012-12-14</created><updated>2015-03-10</updated><authors><author><keyname>Dumbser</keyname><forenames>Michael</forenames></author><author><keyname>Zanotti</keyname><forenames>Olindo</forenames></author><author><keyname>Hidalgo</keyname><forenames>Arturo</forenames></author><author><keyname>Balsara</keyname><forenames>Dinshaw S.</forenames></author></authors><title>ADER-WENO Finite Volume Schemes with Space-Time Adaptive Mesh Refinement</title><categories>math.NA astro-ph.IM cs.NA physics.comp-ph</categories><comments>With updated bibliography information</comments><journal-ref>Journal of Computational Physics, Volume 248, p. 257-286 (2013)</journal-ref><doi>10.1016/j.jcp.2013.04.017</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the first high order one-step ADER-WENO finite volume scheme with
Adaptive Mesh Refinement (AMR) in multiple space dimensions. High order spatial
accuracy is obtained through a WENO reconstruction, while a high order one-step
time discretization is achieved using a local space-time discontinuous Galerkin
predictor method. Due to the one-step nature of the underlying scheme, the
resulting algorithm is particularly well suited for an AMR strategy on
space-time adaptive meshes, i.e.with time-accurate local time stepping. The AMR
property has been implemented 'cell-by-cell', with a standard tree-type
algorithm, while the scheme has been parallelized via the Message Passing
Interface (MPI) paradigm. The new scheme has been tested over a wide range of
examples for nonlinear systems of hyperbolic conservation laws, including the
classical Euler equations of compressible gas dynamics and the equations of
magnetohydrodynamics (MHD). High order in space and time have been confirmed
via a numerical convergence study and a detailed analysis of the computational
speed-up with respect to highly refined uniform meshes is also presented. We
also show test problems where the presented high order AMR scheme behaves
clearly better than traditional second order AMR methods. The proposed scheme
that combines for the first time high order ADER methods with space--time
adaptive grids in two and three space dimensions is likely to become a useful
tool in several fields of computational physics, applied mathematics and
mechanics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3618</identifier>
 <datestamp>2013-07-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3618</id><created>2012-12-14</created><updated>2013-07-08</updated><authors><author><keyname>Komendantskaya</keyname><forenames>Ekaterina</forenames><affiliation>School of Computing, University of Dundee</affiliation></author><author><keyname>Heras</keyname><forenames>J&#xf3;nathan</forenames><affiliation>School of Computing, University of Dundee</affiliation></author><author><keyname>Grov</keyname><forenames>Gudmund</forenames><affiliation>School of Mathematical and Computer Sciences, Heriot-Watt University</affiliation></author></authors><title>Machine Learning in Proof General: Interfacing Interfaces</title><categories>cs.AI cs.LG cs.LO</categories><comments>In Proceedings UITP 2012, arXiv:1307.1528</comments><proxy>EPTCS</proxy><acm-class>I.2.6; F.4.1</acm-class><journal-ref>EPTCS 118, 2013, pp. 15-41</journal-ref><doi>10.4204/EPTCS.118.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present ML4PG - a machine learning extension for Proof General. It allows
users to gather proof statistics related to shapes of goals, sequences of
applied tactics, and proof tree structures from the libraries of interactive
higher-order proofs written in Coq and SSReflect. The gathered data is
clustered using the state-of-the-art machine learning algorithms available in
MATLAB and Weka. ML4PG provides automated interfacing between Proof General and
MATLAB/Weka. The results of clustering are used by ML4PG to provide proof hints
in the process of interactive proof development.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3621</identifier>
 <datestamp>2012-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3621</id><created>2012-12-14</created><authors><author><keyname>Gluesing-Luerssen</keyname><forenames>Heide</forenames></author><author><keyname>Forney</keyname><forenames>G. David</forenames><suffix>Jr</suffix></author></authors><title>Local Irreducibility of Tail-Biting Trellises</title><categories>cs.IT math.IT math.OC</categories><msc-class>94B05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates tail-biting trellis realizations for linear block
codes. Intrinsic trellis properties are used to characterize irreducibility on
given intervals of the time axis. It proves beneficial to always consider the
trellis and its dual simultaneously. A major role is played by trellis
properties that amount to observability and controllability for fragments of
the trellis of various lengths. For fragments of length less than the minimum
span length of the code it is shown that fragment observability and fragment
controllability are equivalent to irreducibility. For reducible trellises, a
constructive reduction procedure is presented. The considerations also lead to
a characterization for when the dual of a trellis allows a product
factorization into elementary (&quot;atomic&quot;) trellises.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3624</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3624</id><created>2012-12-14</created><authors><author><keyname>Khabbazibasmenj</keyname><forenames>Arash</forenames></author><author><keyname>Vorobyov</keyname><forenames>Sergiy A.</forenames></author></authors><title>Robust Adaptive Beamforming for General-Rank Signal Model with Positive
  Semi-Definite Constraint via POTDC</title><categories>cs.IT math.IT math.OC</categories><comments>29 pages, 7 figures, 2 tables, Submitted to IEEE Trans. Signal
  Processing on August 2012</comments><journal-ref>A. Khabbazibasmenj and S.A. Vorobyov, &quot;Robust adaptive beamforming
  for general-rank signal model with positive semi-definite constraint via
  POTDC,&quot; IEEE Trans. Signal Processing, vol. 61, no. 23, pp. 6103-6117, Dec.
  2013</journal-ref><doi>10.1109/TSP.2013.2281301</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The robust adaptive beamforming (RAB) problem for general-rank signal model
with an additional positive semi-definite constraint is considered. Using the
principle of the worst-case performance optimization, such RAB problem leads to
a difference-of-convex functions (DC) optimization problem. The existing
approaches for solving the resulted non-convex DC problem are based on
approximations and find only suboptimal solutions. Here we solve the non-convex
DC problem rigorously and give arguments suggesting that the solution is
globally optimal. Particularly, we rewrite the problem as the minimization of a
one-dimensional optimal value function whose corresponding optimization problem
is non-convex. Then, the optimal value function is replaced with another
equivalent one, for which the corresponding optimization problem is convex. The
new one-dimensional optimal value function is minimized iteratively via
polynomial time DC (POTDC) algorithm.We show that our solution satisfies the
Karush-Kuhn-Tucker (KKT) optimality conditions and there is a strong evidence
that such solution is also globally optimal. Towards this conclusion, we
conjecture that the new optimal value function is a convex function. The new
RAB method shows superior performance compared to the other state-of-the-art
general-rank RAB methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3631</identifier>
 <datestamp>2012-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3631</id><created>2012-12-14</created><authors><author><keyname>Sprechmann</keyname><forenames>Pablo</forenames></author><author><keyname>Bronstein</keyname><forenames>Alex M.</forenames></author><author><keyname>Sapiro</keyname><forenames>Guillermo</forenames></author></authors><title>Learning efficient sparse and low rank models</title><categories>cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Parsimony, including sparsity and low rank, has been shown to successfully
model data in numerous machine learning and signal processing tasks.
Traditionally, such modeling approaches rely on an iterative algorithm that
minimizes an objective function with parsimony-promoting terms. The inherently
sequential structure and data-dependent complexity and latency of iterative
optimization constitute a major limitation in many applications requiring
real-time performance or involving large-scale data. Another limitation
encountered by these modeling techniques is the difficulty of their inclusion
in discriminative learning scenarios. In this work, we propose to move the
emphasis from the model to the pursuit algorithm, and develop a process-centric
view of parsimonious modeling, in which a learned deterministic
fixed-complexity pursuit process is used in lieu of iterative optimization. We
show a principled way to construct learnable pursuit process architectures for
structured sparse and robust low rank models, derived from the iteration of
proximal descent algorithms. These architectures learn to approximate the exact
parsimonious representation at a fraction of the complexity of the standard
optimization methods. We also show that appropriate training regimes allow to
naturally extend parsimonious models to discriminative settings.
State-of-the-art results are demonstrated on several challenging problems in
image and audio processing with several orders of magnitude speedup compared to
the exact optimization algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3633</identifier>
 <datestamp>2014-04-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3633</id><created>2012-12-14</created><updated>2014-04-13</updated><authors><author><keyname>Chaouche</keyname><forenames>Fatima Affif</forenames></author><author><keyname>Rutherford</keyname><forenames>Carrie</forenames></author><author><keyname>Whitty</keyname><forenames>Robin</forenames></author></authors><title>Pancyclicity when each cycle must pass exactly $k$ Hamilton cycle chords</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is known that $\Theta(\log n)$ chords must be added to an $n$-cycle to
produce a pancyclic graph; for vertex pancyclicity, where every vertex belongs
to a cycle of every length, $\Theta(n)$ chords are required. A possibly
`intermediate' variation is the following: given $k$, $1\leq k\leq n$, how many
chords must be added to ensure that there exist cycles of every length each of
which passes exactly $k$ chords? For fixed $k$, we establish a lower bound of
$\Omega\big(n^{1/k}\big)$ on the growth rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3634</identifier>
 <datestamp>2012-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3634</id><created>2012-12-14</created><authors><author><keyname>Froud</keyname><forenames>Hanane</forenames></author><author><keyname>Lachkar</keyname><forenames>Abdelmonaim</forenames></author><author><keyname>Ouatik</keyname><forenames>Said Alaoui</forenames></author></authors><title>A comparative study of root-based and stem-based approaches for
  measuring the similarity between arabic words for arabic text mining
  applications</title><categories>cs.CL cs.IR</categories><journal-ref>Advanced Computing An International Journal (ACIJ), November 2012,
  Volume 3, Number 6</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Representation of semantic information contained in the words is needed for
any Arabic Text Mining applications. More precisely, the purpose is to better
take into account the semantic dependencies between words expressed by the
co-occurrence frequencies of these words. There have been many proposals to
compute similarities between words based on their distributions in contexts. In
this paper, we compare and contrast the effect of two preprocessing techniques
applied to Arabic corpus: Rootbased (Stemming), and Stem-based (Light Stemming)
approaches for measuring the similarity between Arabic words with the well
known abstractive model -Latent Semantic Analysis (LSA)- with a wide variety of
distance functions and similarity measures, such as the Euclidean Distance,
Cosine Similarity, Jaccard Coefficient, and the Pearson Correlation
Coefficient. The obtained results show that, on the one hand, the variety of
the corpus produces more accurate results; on the other hand, the Stem-based
approach outperformed the Root-based one because this latter affects the words
meanings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3638</identifier>
 <datestamp>2013-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3638</id><created>2012-12-14</created><updated>2012-12-31</updated><authors><author><keyname>Ng</keyname><forenames>Derrick Wing Kwan</forenames></author><author><keyname>Lo</keyname><forenames>Ernest S.</forenames></author><author><keyname>Schober</keyname><forenames>Robert</forenames></author></authors><title>Energy-Efficient Resource Allocation in Multiuser OFDM Systems with
  Wireless Information and Power Transfer</title><categories>cs.IT math.IT</categories><comments>6 pages. The paper has been accepted for publication at the IEEE
  Wireless Communications and Networking Conference (WCNC) 2013, Shanghai,
  China, Apr. 2013</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper, we study the resource allocation algorithm design for
multiuser orthogonal frequency division multiplexing (OFDM) downlink systems
with simultaneous wireless information and power transfer. The algorithm design
is formulated as a non-convex optimization problem for maximizing the energy
efficiency of data transmission (bit/Joule delivered to the users). In
particular, the problem formulation takes into account the minimum required
system data rate, heterogeneous minimum required power transfers to the users,
and the circuit power consumption. Subsequently, by exploiting the method of
time-sharing and the properties of nonlinear fractional programming, the
considered non-convex optimization problem is solved using an efficient
iterative resource allocation algorithm. For each iteration, the optimal power
allocation and user selection solution are derived based on Lagrange dual
decomposition. Simulation results illustrate that the proposed iterative
resource allocation algorithm achieves the maximum energy efficiency of the
system and reveal how energy efficiency, system capacity, and wireless power
transfer benefit from the presence of multiple users in the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3640</identifier>
 <datestamp>2013-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3640</id><created>2012-12-14</created><updated>2013-01-20</updated><authors><author><keyname>Zhang</keyname><forenames>Xi</forenames></author><author><keyname>Zhou</keyname><forenames>Xiangyun</forenames></author><author><keyname>McKay</keyname><forenames>Matthew R.</forenames></author></authors><title>On the Design of Artificial-Noise-Aided Secure Multi-Antenna
  Transmission in Slow Fading Channels</title><categories>cs.IT math.IT</categories><comments>to appear in IEEE Transactions on Vehicular Technology</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the design of artificial-noise-aided secure
multi-antenna transmission in slow fading channels. The primary design concerns
include the transmit power allocation and the rate parameters of the wiretap
code. We consider two scenarios with different complexity levels: i) the design
parameters are chosen to be fixed for all transmissions, ii) they are
adaptively adjusted based on the instantaneous channel feedback from the
intended receiver. In both scenarios, we provide explicit design solutions for
achieving the maximal throughput subject to a secrecy constraint, given by a
maximum allowable secrecy outage probability. We then derive accurate
approximations for the maximal throughput in both scenarios in the high
signal-to-noise ratio region, and give new insights into the additional power
cost for achieving a higher security level, whilst maintaining a specified
target throughput. In the end, the throughput gain of adaptive transmission
over non-adaptive transmission is also quantified and analyzed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3641</identifier>
 <datestamp>2012-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3641</id><created>2012-12-14</created><authors><author><keyname>Lukotka</keyname><forenames>Robert</forenames></author><author><keyname>Macajova</keyname><forenames>Edita</forenames></author><author><keyname>Mazak</keyname><forenames>Jan</forenames></author><author><keyname>Skoviera</keyname><forenames>Martin</forenames></author></authors><title>Small snarks with large oddness</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We estimate the minimum number of vertices of a cubic graph with given
oddness and cyclic connectivity. We prove that a bridgeless cubic graph $G$
with oddness $\omega(G)$ other than the Petersen graph has at least
$5.41\cdot\omega(G)$ vertices, and for each integer $k$ with $2\le k\le 6$ we
construct an infinite family of cubic graphs with cyclic connectivity $k$ and
small oddness ratio $|V(G)|/\omega(G)$. In particular, for cyclic connectivity
2, 4, 5, and 6 we improve the upper bounds on the oddness ratio of snarks to
7.5, 13, 25, and 99 from the known values 9, 15, 76, and 118, respectively. In
addition, we construct a cyclically 4-connected snark of girth 5 with oddness 4
on 44 vertices, improving the best previous value of 46.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3648</identifier>
 <datestamp>2013-05-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3648</id><created>2012-12-14</created><updated>2013-05-26</updated><authors><author><keyname>Voisin</keyname><forenames>Julien</forenames></author><author><keyname>Guyeux</keyname><forenames>Christophe</forenames></author><author><keyname>Bahi</keyname><forenames>Jacques M.</forenames></author></authors><title>The Metadata Anonymization Toolkit</title><categories>cs.CR</categories><comments>7 pages, one figure</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This document summarizes the experience of Julien Voisin during the 2011
edition of the well-known \emph{Google Summer of Code}. This project is a first
step in the domain of metadata anonymization in Free Software. This article is
articulated in three parts. First, a state of the art and a categorization of
usual metadata, then the privacy policy is exposed/discussed in order to find
the right balance between information lost and privacy enhancement. Finally,
the specification of the Metadata Anonymization Toolkit (MAT) is presented, and
future possible works are sketched.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3654</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3654</id><created>2012-12-14</created><authors><author><keyname>Gao</keyname><forenames>Jie</forenames></author><author><keyname>Vorobyov</keyname><forenames>Sergiy A.</forenames></author><author><keyname>Jiang</keyname><forenames>Hai</forenames></author><author><keyname>Zhang</keyname><forenames>Jianshu</forenames></author><author><keyname>Haardt</keyname><forenames>Martin</forenames></author></authors><title>Sum-Rate Maximization with Minimum Power Consumption for MIMO DF Two-Way
  Relaying: Part II - Network Optimization</title><categories>cs.IT math.IT</categories><comments>24 pages, 5 figures, submitted to the IEEE Trans. Signal Processing
  in Aug. 2012</comments><journal-ref>J. Gao, S.A. Vorobyov, H. Jiang, J. Zhang, M. Haardt, &quot;Sum rate
  maximization with minimum power consumption for MIMO DF TWR. Part II Network
  optimization,&quot; IEEE Trans. Signal Processing, vol. 61, no. 14, pp. 3578-3591,
  July 2013</journal-ref><doi>10.1109/TSP.2013.2263501</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In Part II of this two-part paper, a sum-rate-maximizing power allocation
with minimum power consumption is found for multiple-input multiple-output
(MIMO) decode-and-forward (DF) two-way relaying (TWR) in a network optimization
scenario. In this scenario, the relay and the source nodes jointly optimize
their power allocation strategies to achieve network optimality. Unlike the
relay optimization scenario considered in part I which features low complexity
but does not achieve network optimality, the network-level optimal power
allocation can be achieved in the network optimization scenario at the cost of
higher complexity. The network optimization problem is considered in two cases
each with several subcases. It is shown that the considered problem, which is
originally nonconvex, can be transferred into different convex problems for all
but two subcases. For the remaining two subcases, one for each case, it is
proved that the optimal strategies for the source nodes and the relay must
satisfy certain properties. Based on these properties, an algorithm is proposed
for finding the optimal solution. The effect of asymmetry in the number of
antennas, power limits, and channel statistics is also considered. Such
asymmetry is shown to have a negative effect on both the achievable sum-rate
and the power allocation efficiency in MIMO DF TWR. Simulation results
demonstrate the performance of the proposed algorithm and the effect of
asymmetry in the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3669</identifier>
 <datestamp>2014-07-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3669</id><created>2012-12-15</created><updated>2014-07-21</updated><authors><author><keyname>Modena</keyname><forenames>Gabriele</forenames></author></authors><title>A metric for software vulnerabilities classification</title><categories>cs.SE cs.LG</categories><comments>The original version of this paper was written in Feb 2009 to report
  results of a Machine Learning research project at the University of
  Amsterdam. At the time this research has been carried out the author was
  affiliated (Graduate Student) with the University of Amsterdam, The
  Netherlands</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Vulnerability discovery and exploits detection are two wide areas of study in
software engineering. This preliminary work tries to combine existing methods
with machine learning techniques to define a metric classification of
vulnerable computer programs. First a feature set has been defined and later
two models have been tested against real world vulnerabilities. A relation
between the classifier choice and the features has also been outlined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3677</identifier>
 <datestamp>2012-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3677</id><created>2012-12-15</created><authors><author><keyname>Schaible</keyname><forenames>Johann</forenames></author><author><keyname>Mayr</keyname><forenames>Philipp</forenames></author></authors><title>Discovering Links for Metadata Enrichment on Computer Science Papers</title><categories>cs.DL</categories><comments>22 pages, 4 figures, 7 listings, presented at SWIB12</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  At the very beginning of compiling a bibliography, usually only basic
information, such as title, authors and publication date of an item are known.
In order to gather additional information about a specific item, one typically
has to search the library catalog or use a web search engine. This look-up
procedure implies a manual effort for every single item of a bibliography. In
this technical report we present a proof of concept which utilizes Linked Data
technology for the simple enrichment of sparse metadata sets. This is done by
discovering owl:sameAs links be- tween an initial set of computer science
papers and resources from external data sources like DBLP, ACM and the Semantic
Web Conference Corpus. In this report, we demonstrate how the link discovery
tool Silk is used to detect additional information and to enrich an initial set
of records in the computer science domain. The pros and cons of silk as link
discovery tool are summarized in the end.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3682</identifier>
 <datestamp>2012-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3682</id><created>2012-12-15</created><authors><author><keyname>Khoshkhah</keyname><forenames>Kaveh</forenames></author><author><keyname>Soltani</keyname><forenames>Hossein</forenames></author><author><keyname>Zaker</keyname><forenames>Manouchehr</forenames></author></authors><title>Dynamic monopolies in directed graphs: the spread of unilateral
  influence in social networks</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $G$ be a directed graph such that the in-degree of any vertex $G$ is at
least one. Let also ${\mathcal{\tau}}: V(G)\rightarrow \Bbb{N}$ be an
assignment of thresholds to the vertices of $G$. A subset $M$ of vertices of
$G$ is called a dynamic monopoly for $(G,\tau)$ if the vertex set of $G$ can be
partitioned into $D_0\cup... \cup D_t$ such that $D_0=M$ and for any $i\geq 1$
and any $v\in D_i$, the number of edges from $D_0\cup... \cup D_{i-1}$ to $v$
is at least $\tau(v)$. One of the most applicable and widely studied threshold
assignments in directed graphs is strict majority threshold assignment in which
for any vertex $v$, $\tau(v)=\lceil (deg^{in}(v)+1)/2 \rceil$, where
$deg^{in}(v)$ stands for the in-degree of $v$. By a strict majority dynamic
monopoly of a graph $G$ we mean any dynamic monopoly of $G$ with strict
majority threshold assignment for the vertices of $G$. In this paper we first
discuss some basic upper and lower bounds for the size of dynamic monopolies
with general threshold assignments and then obtain some hardness complexity
results concerning the smallest size of dynamic monopolies in directed graphs.
Next we show that any directed graph on $n$ vertices and with positive minimum
in-degree admits a strict majority dynamic monopoly with $n/2$ vertices. We
show that this bound is achieved by a polynomial time algorithm. This upper
bound improves greatly the best known result. The final note of the paper deals
with the possibility of the improvement of the latter $n/2$ bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3689</identifier>
 <datestamp>2013-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3689</id><created>2012-12-15</created><updated>2013-10-24</updated><authors><author><keyname>Tomamichel</keyname><forenames>Marco</forenames></author><author><keyname>Tan</keyname><forenames>Vincent Y. F.</forenames></author></authors><title>A Tight Upper Bound for the Third-Order Asymptotics for Most Discrete
  Memoryless Channels</title><categories>cs.IT math.IT</categories><comments>published version</comments><journal-ref>IEEE Transactions on Information Theory 59 (11), p. 7041-7051
  (2013)</journal-ref><doi>10.1109/TIT.2013.2276077</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper shows that the logarithm of the epsilon-error capacity (average
error probability) for n uses of a discrete memoryless channel is upper bounded
by the normal approximation plus a third-order term that does not exceed 1/2
log n + O(1) if the epsilon-dispersion of the channel is positive. This matches
a lower bound by Y. Polyanskiy (2010) for discrete memoryless channels with
positive reverse dispersion. If the epsilon-dispersion vanishes, the logarithm
of the epsilon-error capacity is upper bounded by the n times the capacity plus
a constant term except for a small class of DMCs and epsilon &gt;= 1/2.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3690</identifier>
 <datestamp>2013-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3690</id><created>2012-12-15</created><updated>2012-12-29</updated><authors><author><keyname>Monemizadeh</keyname><forenames>Mostafa</forenames></author><author><keyname>Hajizadeh</keyname><forenames>Saeed</forenames></author><author><keyname>Hodtani</keyname><forenames>Ghosheh Abed</forenames></author><author><keyname>Seyedin</keyname><forenames>Seyed Alireza</forenames></author></authors><title>Capacity Bounds for Dirty Paper with Exponential Dirt</title><categories>cs.IT math.IT</categories><comments>7 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The additive exponential noise channel with additive exponential interference
(AENC-AEI) known non-causally at the transmitter is studied. This channel can
be considered as an exponential version of the discrete memoryless channel with
state known non-causally at the encoder considered by Gelfand and Pinsker. We
make use of Gelfand-Pinsker classic capacity Theorem to derive inner and outer
bounds on the capacity of this channel under a non-negative input constraint as
well as a constraint on the mean value of the input. First we obtain an outer
bound for AENC-AEI. Then by using the input distribution achieving the outer
bound, we derive an inner bound which this inner bound coincides with the
obtained outer bound at high signal to noise ratios (SNRs) and therefore, gives
the capacity of the AENC-AEI at high SNRs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3704</identifier>
 <datestamp>2012-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3704</id><created>2012-12-15</created><authors><author><keyname>Batoul</keyname><forenames>Aicha</forenames></author><author><keyname>Guenda</keyname><forenames>Kenza</forenames></author><author><keyname>Gulliver</keyname><forenames>T. Aaron</forenames></author></authors><title>Some Constacyclic Codes over Finite Chain Rings</title><categories>cs.IT math.IT</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  For $\lambda$ an $n$-th power of a unit in a finite chain ring we prove that
$\lambda$-constacyclic repeated-root codes over some finite chain rings are
equivalent to cyclic codes. This allows us to simplify the structure of some
constacylic codes. We also study the $\alpha +p \beta$-constacyclic codes of
length $p^s$ over the Galois ring $GR(p^e,r)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3719</identifier>
 <datestamp>2012-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3719</id><created>2012-12-15</created><authors><author><keyname>Sengupta</keyname><forenames>Madhumita</forenames></author><author><keyname>Mandal</keyname><forenames>J. K.</forenames></author><author><keyname>Ghoshal</keyname><forenames>N.</forenames></author></authors><title>An Authentication Technique in Frequency Domain through Wavelet
  Transform (ATFDWT)</title><categories>cs.CR</categories><comments>18 Page Journal in 'Advances in Modelling Signal Processing and
  Pattern Recognition' (AMSE), vol-54, Issue 2, 20011</comments><journal-ref>Advances in Modelling Signal Processing and Pattern Recognition
  (AMSE), vol-54, Issue 2, 20011, Published Journal:2011-Vol.54No1-2</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper a DWT based steganography in frequency domain, termed as ATFDWT
has been proposed. Here, the cover image is transformed into the time domain
signal through DWT, resulting four sub-image components as 'Low resolution',
'Horizontal orientation', 'Vertical orientation' and 'Diagonal orientation'.
The secret message/image bits stream in varying positions are embedded in
'Vertical orientation sub-image' followed by reverse transformation to generate
embedded/encrypted image. The decoding is done through the reverse procedure.
The experimental results against statistical and visual attack has been
computed and compared with the existing technique like IAFDDFTT[1], in terms of
Mean Square Error (MSE), Peak Signal to Noise Ratio (PSNR), Standard
Deviation(SD) and Image Fidelity(IF) analysis, which shows better performances
in ATFDWT.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3736</identifier>
 <datestamp>2014-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3736</id><created>2012-12-15</created><updated>2014-04-28</updated><authors><author><keyname>Punnen</keyname><forenames>Abraham P.</forenames></author><author><keyname>Sripratak</keyname><forenames>Piyashat</forenames></author><author><keyname>Karapetyan</keyname><forenames>Daniel</forenames></author></authors><title>The bipartite unconstrained 0-1 quadratic programming problem:
  polynomially solvable cases</title><categories>math.OC cs.DM math.CO</categories><comments>20 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the bipartite unconstrained 0-1 quadratic programming problem
(BQP01) which is a generalization of the well studied unconstrained 0-1
quadratic programming problem (QP01). BQP01 has numerous applications and the
problem is known to be MAX SNP hard. We show that if the rank of an associated
$m\times n$ cost matrix $Q=(q_{ij})$ is fixed, then BQP01 can be solved in
polynomial time. When $Q$ is of rank one, we provide an $O(n\log n)$ algorithm
and this complexity reduces to $O(n)$ with additional assumptions. Further, if
$q_{ij}=a_i+b_j$ for some $a_i$ and $b_j$, then BQP01 is shown to be solvable
in $O(mn\log n)$ time. By restricting $m=O(\log n),$ we obtain yet another
polynomially solvable case of BQP01 but the problem remains MAX SNP hard if
$m=O(\sqrt[k]{n})$ for a fixed $k$. Finally, if the minimum number of rows and
columns to be deleted from $Q$ to make the remaining matrix non-negative is
$O(\log n)$ then we show that BQP01 polynomially solvable but it is NP-hard if
this number is $O(\sqrt[k]{n})$ for any fixed $k$.
  Keywords: quadratic programming, 0-1 variables, polynomial algorithms,
complexity, pseudo-Boolean programming.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3741</identifier>
 <datestamp>2012-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3741</id><created>2012-12-15</created><authors><author><keyname>Devanur</keyname><forenames>Nikhil R.</forenames></author><author><keyname>Hartline</keyname><forenames>Jason D.</forenames></author><author><keyname>Yan</keyname><forenames>Qiqi</forenames></author></authors><title>Envy Freedom and Prior-free Mechanism Design</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the provision of an abstract service to single-dimensional
agents. Our model includes position auctions, single-minded combinatorial
auctions, and constrained matching markets. When the agents' values are drawn
from a distribution, the Bayesian optimal mechanism is given by Myerson (1981)
as a virtual-surplus optimizer. We develop a framework for prior-free mechanism
design and analysis. A good mechanism in our framework approximates the optimal
mechanism for the distribution if there is a distribution; moreover, when there
is no distribution this mechanism still performs well.
  We define and characterize optimal envy-free outcomes in symmetric
single-dimensional environments. Our characterization mirrors Myerson's theory.
Furthermore, unlike in mechanism design where there is no point-wise optimal
mechanism, there is always a point-wise optimal envy-free outcome.
  Envy-free outcomes and incentive-compatible mechanisms are similar in
structure and performance. We therefore use the optimal envy-free revenue as a
benchmark for measuring the performance of a prior-free mechanism. A good
mechanism is one that approximates the envy free benchmark on any profile of
agent values. We show that good mechanisms exist, and in particular, a natural
generalization of the random sampling auction of Goldberg et al. (2001) is a
constant approximation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3746</identifier>
 <datestamp>2013-06-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3746</id><created>2012-12-15</created><authors><author><keyname>Steinbrecher</keyname><forenames>Greg</forenames></author></authors><title>Cross-Layer Design to Maintain Earthquake Sensor Network Connectivity
  After Loss of Infrastructure</title><categories>cs.NI</categories><comments>To be published in MILCOM 2012 - Track 2: Networking Protocols and
  Performance</comments><doi>10.1109/MILCOM.2012.6415858</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present the design of a cross-layer protocol to maintain connectivity in
an earthquake monitoring and early warning sensor network in the absence of
communications infrastructure. Such systems, by design, warn of events that
severely damage or destroy communications infrastructure. However, the data
they provide is of critical importance to emergency and rescue decision making
in the immediate aftermath of such events, as is continued early warning of
aftershocks, tsunamis, or other subsequent dangers. Utilizing a beyond
line-of-sight (BLOS) HF physical layer, we propose an adaptable cross-layer
network design that meets these specialized requirements. We are able to
provide ultra high connectivity (UHC) early warning on strict time deadlines
under worst-case channel conditions along with providing sufficient capacity
for continued seismic data collection from a 1000 sensor network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3747</identifier>
 <datestamp>2012-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3747</id><created>2012-12-15</created><authors><author><keyname>Hu</keyname><forenames>Su</forenames></author><author><keyname>Guan</keyname><forenames>Yong Liang</forenames></author><author><keyname>Bi</keyname><forenames>Guoan</forenames></author><author><keyname>Li</keyname><forenames>Shaoqian</forenames></author></authors><title>Cluster-based Transform Domain Communication Systems for High Spectrum
  Efficiency</title><categories>cs.NI cs.IT math.IT</categories><comments>15 pages, 9 figures, Accepted for publication in IET Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a cluster-based transform domain communication system
(TDCS) to improve spectrum efficiency. Unlike the utilities of clusters in
orthogonal frequency division multiplex (OFDM) systems, the cluster-based TDCS
framework divides entire unoccupied spectrum bins into $L$ clusters, where each
one represents a data steam independently, to achieve $L$ times of spectrum
efficiency compared to that of the traditional one. Among various schemes of
spectrum bin spacing and allocation, the TDCS with random allocation scheme
appears to be an ideal candidate to significantly improve spectrum efficiency
without seriously degrading power efficiency. In multipath fading channel, the
coded TDCS with random allocation scheme achieves robust BER performance due to
a large degree of frequency diversity. Furthermore, our study shows that the
smaller spectrum bin spacing should be configured for the cluster-based TDCS to
achieve higher spectrum efficiency and more robust BER performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3753</identifier>
 <datestamp>2014-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3753</id><created>2012-12-16</created><updated>2014-07-25</updated><authors><author><keyname>Oymak</keyname><forenames>Samet</forenames></author><author><keyname>Jalali</keyname><forenames>Amin</forenames></author><author><keyname>Fazel</keyname><forenames>Maryam</forenames></author><author><keyname>Eldar</keyname><forenames>Yonina C.</forenames></author><author><keyname>Hassibi</keyname><forenames>Babak</forenames></author></authors><title>Simultaneously Structured Models with Application to Sparse and Low-rank
  Matrices</title><categories>cs.IT math.IT math.OC</categories><comments>38 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The topic of recovery of a structured model given a small number of linear
observations has been well-studied in recent years. Examples include recovering
sparse or group-sparse vectors, low-rank matrices, and the sum of sparse and
low-rank matrices, among others. In various applications in signal processing
and machine learning, the model of interest is known to be structured in
several ways at the same time, for example, a matrix that is simultaneously
sparse and low-rank.
  Often norms that promote each individual structure are known, and allow for
recovery using an order-wise optimal number of measurements (e.g., $\ell_1$
norm for sparsity, nuclear norm for matrix rank). Hence, it is reasonable to
minimize a combination of such norms. We show that, surprisingly, if we use
multi-objective optimization with these norms, then we can do no better,
order-wise, than an algorithm that exploits only one of the present structures.
This result suggests that to fully exploit the multiple structures, we need an
entirely new convex relaxation, i.e. not one that is a function of the convex
relaxations used for each structure. We then specialize our results to the case
of sparse and low-rank matrices. We show that a nonconvex formulation of the
problem can recover the model from very few measurements, which is on the order
of the degrees of freedom of the matrix, whereas the convex problem obtained
from a combination of the $\ell_1$ and nuclear norms requires many more
measurements. This proves an order-wise gap between the performance of the
convex and nonconvex recovery problems in this case. Our framework applies to
arbitrary structure-inducing norms as well as to a wide range of measurement
ensembles. This allows us to give performance bounds for problems such as
sparse phase retrieval and low-rank tensor completion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3765</identifier>
 <datestamp>2012-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3765</id><created>2012-12-16</created><authors><author><keyname>Soleimani</keyname><forenames>Hamid</forenames></author><author><keyname>Ahmadi</keyname><forenames>Arash</forenames></author><author><keyname>Bavandpour</keyname><forenames>Mohammad</forenames></author></authors><title>Biologically Inspired Spiking Neurons : Piecewise Linear Models and
  Digital Implementation</title><categories>cs.LG cs.NE q-bio.NC</categories><comments>14 pages, 16 figures</comments><journal-ref>IEEE Transactions On Circuits And Systems I: Regular Papers, Vol.
  59, NO. 12, December 2012</journal-ref><doi>10.1109/TCSI.2012.2206463</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  There has been a strong push recently to examine biological scale simulations
of neuromorphic algorithms to achieve stronger inference capabilities. This
paper presents a set of piecewise linear spiking neuron models, which can
reproduce different behaviors, similar to the biological neuron, both for a
single neuron as well as a network of neurons. The proposed models are
investigated, in terms of digital implementation feasibility and costs,
targeting large scale hardware implementation. Hardware synthesis and physical
implementations on FPGA show that the proposed models can produce precise
neural behaviors with higher performance and considerably lower implementation
costs compared with the original model. Accordingly, a compact structure of the
models which can be trained with supervised and unsupervised learning
algorithms has been developed. Using this structure and based on a spike rate
coding, a character recognition case study has been implemented and tested.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3767</identifier>
 <datestamp>2012-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3767</id><created>2012-12-16</created><updated>2012-12-18</updated><authors><author><keyname>Lim</keyname><forenames>Hao Wooi</forenames></author><author><keyname>Tay</keyname><forenames>Yong Haur</forenames></author></authors><title>Visual Objects Classification with Sliding Spatial Pyramid Matching</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a method for visual object classification using only a single
feature, transformed color SIFT with a variant of Spatial Pyramid Matching
(SPM) that we called Sliding Spatial Pyramid Matching (SSPM), trained with an
ensemble of linear regression (provided by LINEAR) to obtained state of the art
result on Caltech-101 of 83.46%. SSPM is a special version of SPM where instead
of dividing an image into K number of regions, a subwindow of fixed size is
slide around the image with a fixed step size. For each subwindow, a histogram
of visual words is generated. To obtained the visual vocabulary, instead of
performing K-means clustering, we randomly pick N exemplars from the training
set and encode them with a soft non-linear mapping method. We then trained 15
models, each with a different visual word size with linear regression. All 15
models are then averaged together to form a single strong model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3777</identifier>
 <datestamp>2012-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3777</id><created>2012-12-16</created><authors><author><keyname>Kristinsson</keyname><forenames>Benedikt</forenames></author></authors><title>The Arduino as a Hardware Random-Number Generator</title><categories>cs.CR cs.IT math.IT</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Cheap micro-controllers, such as the Arduino or other controllers based on
the Atmel AVR CPUs are being deployed in a wide variety of projects, ranging
from sensors networks to robotic submarines. In this paper, we investigate the
feasibility of using the Arduino as a true random number generator (TRNG). The
Arduino Reference Manual recommends using it to seed a pseudo random number
generator (PRNG) due to its ability to read random atmospheric noise from its
analog pins. This is an enticing application since true bits of entropy are
hard to come by. Unfortunately, we show by statistical methods that the
atmospheric noise of an Arduino is largely predictable in a variety of
settings, and is thus a weak source of entropy. We explore various methods to
extract true randomness from the micro-controller and conclude that it should
not be used to produce randomness from its analog pins.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3782</identifier>
 <datestamp>2014-02-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3782</id><created>2012-12-16</created><updated>2014-02-12</updated><authors><author><keyname>Ducoffe</keyname><forenames>Guillaume</forenames></author><author><keyname>Mazauric</keyname><forenames>Dorian</forenames></author><author><keyname>Chaintreau</keyname><forenames>Augustin</forenames></author></authors><title>Can Selfish Groups be Self-Enforcing?</title><categories>cs.DM cs.GT cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Algorithmic graph theory has thoroughly analyzed how, given a network
describing constraints between various nodes, groups can be formed among these
so that the resulting configuration optimizes a \emph{global} metric. In
contrast, for various social and economic networks, groups are formed \emph{de
facto} by the choices of selfish players. A fundamental problem in this setting
is the existence and convergence to a \emph{self-enforcing} configuration:
assignment of players into groups such that no player has an incentive to move
into another group than hers. Motivated by information sharing on social
networks -- and the difficult tradeoff between its benefits and the associated
privacy risk -- we study the possible emergence of such stable configurations
in a general selfish group formation game.
  Our paper considers this general game for the first time, and it completes
its analysis. We show that convergence critically depends on the level of
\emph{collusions} among the players -- which allow multiple players to move
simultaneously as long as \emph{all of them} benefit. Solving a previously open
problem we exactly show when, depending on collusions, convergence occurs
within polynomial time, non-polynomial time, and when it never occurs. We also
prove that previously known bounds on convergence time are all loose: by a
novel combinatorial analysis of the evolution of this game we are able to
provide the first \emph{asymptotically exact} formula on its convergence.
Moreover, we extend these results by providing a complete analysis when groups
may \emph{overlap}, and for general utility functions representing
\emph{multi-modal} interactions. Finally, we prove that collusions have a
significant and \emph{positive} effect on the \emph{efficiency} of the
equilibrium that is attained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3786</identifier>
 <datestamp>2012-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3786</id><created>2012-12-16</created><authors><author><keyname>Shriram</keyname><forenames>Revati</forenames></author><author><keyname>Sundhararajan</keyname><forenames>Dr. M.</forenames></author><author><keyname>Daimiwal</keyname><forenames>Nivedita</forenames></author></authors><title>Brain Connectivity Analysis Methods for Better Understanding of Coupling</title><categories>cs.OH</categories><comments>7 Pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Action, cognition, emotion and perception can be mapped in the brain by using
set of techniques. Translating unimodal concepts from one modality to another
is an important step towards understanding the neural mechanisms. This paper
provides a comprehensive survey of multimodal analysis of brain signals such as
fMRI, EEG, MEG, NIRS and motivations, assumptions and pitfalls associated with
it. All these non-invasive brain modalities complement and restrain each other
and hence improve our understating of functional and neuronal organization. By
combining the various modalities together, we can exploit the strengths and
flaws of individual brain imaging methods. Integrated anatomical analysis and
functional measurements of human brain offer a powerful paradigm for the brain
mapping. Here we provide the brief review on non invasive brain modalities,
describe the future of co-analysis of these brain signals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3789</identifier>
 <datestamp>2013-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3789</id><created>2012-12-16</created><authors><author><keyname>Marburger</keyname><forenames>Jan</forenames></author></authors><title>Adjoint-Based Optimal Control of Time-Dependent Free Boundary Problems</title><categories>math.OC cs.CE cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we show a simplified optimisation approach for free boundary
problems in arbitrary space dimensions. This approach is mainly based on an
extended operator splitting which allows a decoupling of the domain deformation
and solving the remaining partial differential equation. First we give a short
introduction to free boundary problems and the problems occurring in
optimisation. Then we introduce the extended operator splitting and apply it to
a general minimisation subject to a time-dependent scalar-valued partial
differential equation. This yields a time-discretised optimisation problem
which allows us a quite simple application of adjoint-based optimisation
methods. Finally, we verify this approach numerically by the optimisation of a
flow problem (Navier-Stokes equation) and the final shape of a Stefan-type
problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3799</identifier>
 <datestamp>2012-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3799</id><created>2012-12-16</created><authors><author><keyname>Fan</keyname><forenames>Yi-Zheng</forenames></author><author><keyname>Huang</keyname><forenames>Tao</forenames></author><author><keyname>Zhu</keyname><forenames>Ming</forenames></author></authors><title>Compressed Sensing Based on Random Symmetric Bernoulli Matrix</title><categories>cs.IT math.IT</categories><comments>arXiv admin note: text overlap with arXiv:0902.4394 by other authors</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The task of compressed sensing is to recover a sparse vector from a small
number of linear and non-adaptive measurements, and the problem of finding a
suitable measurement matrix is very important in this field. While most recent
works focused on random matrices with entries drawn independently from certain
probability distributions, in this paper we show that a partial random
symmetric Bernoulli matrix whose entries are not independent, can be used to
recover signal from observations successfully with high probability. The
experimental results also show that the proposed matrix is a suitable
measurement matrix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3806</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3806</id><created>2012-12-16</created><updated>2013-04-21</updated><authors><author><keyname>Dagand</keyname><forenames>Pierre-Evariste</forenames></author><author><keyname>McBride</keyname><forenames>Conor</forenames></author></authors><title>A Categorical Treatment of Ornaments</title><categories>cs.PL math.CT</categories><comments>32 pages, technical report, extends paper to appear in LICS'13</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ornaments aim at taming the multiplication of special-purpose datatype in
dependently-typed theory. In its original form, the definition of ornaments is
tied to a particular universe of datatypes. Being a type theoretic object,
constructions on ornaments are typically explained through an operational
narrative. This overbearing concreteness calls for an abstract model of
ornaments.
  In this paper, we give a categorical model of ornaments. As a necessary first
step, we abstract the universe of datatypes using the theory of polynomial
functors. We are then able to characterize ornaments as cartesian morphisms
between polynomial functors. We thus gain access to powerful mathematical tools
that shall help us understand and develop ornaments.
  We shall also illustrate the adequacy of our model. Firstly, we rephrase the
standard ornamental constructions into our framework. Thanks to its
conciseness, this process gives us a deeper understanding of the structures at
play. Secondly, we develop new ornamental constructions, by translating
categorical structures into type theoretic artifacts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3817</identifier>
 <datestamp>2012-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3817</id><created>2012-12-16</created><authors><author><keyname>Wang</keyname><forenames>Xing M.</forenames></author></authors><title>Probability Bracket Notation: Markov State Chain Projector, Hidden
  Markov Models and Dynamic Bayesian Networks</title><categories>cs.AI math.PR</categories><msc-class>62F15</msc-class><acm-class>G.3; I.2.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  After a brief discussion of Markov Evolution Formula (MEF) expressed in
Probability Bracket Notation (PBN), its close relation with the joint
probability distribution (JPD) of Visible Markov Models (VMM) is demonstrated
by introducing Markov State Chain Projector (MSCP). The state basis and the
observed basis are defined in the Sequential Event Space (SES) of Hidden Markov
Models (HMM). The JPD of HMM is derived by using basis transformation in SES.
The Viterbi algorithm is revisited and applied to the famous Weather HMM
example, whose node graph and inference results are displayed by using software
package Elvira. In the end, the formulas of VMM, HMM and some factorial HMM
(FHMM) are expressed in PBN as instances of dynamic Bayesian Networks (DBN).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3838</identifier>
 <datestamp>2012-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3838</id><created>2012-12-16</created><authors><author><keyname>Choe</keyname><forenames>Changil</forenames></author><author><keyname>Van Hung</keyname><forenames>Dang</forenames></author><author><keyname>Han</keyname><forenames>Song</forenames></author></authors><title>Towards Approximate Model Checking DC and PDC Specifications</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  DC has proved to be a promising tool for the specification and verification
of functional requirements on the design of hard real-time systems. Many works
were devoted to develop effective techniques for checking the models of hard
real-time systems against DC specifications. DC model checking theory is still
evolving and yet there is no available tools supporting practical verifications
due to the high undecidability of calculus and the great complexity of model
checking. Present situation of PDC model checking is much worse than the one of
DC model checking. In view of the results so far achieved, it is desirable to
develop approximate model checking techniques for DC and PDC specifications.
This work was motivated to develop approximate techniques checking automata
models of hard real-time systems for DC and PDC specifications. Unlike previous
works which only deal with decidable formulas, we want to develop approximate
techniques covering whole DC and PDC formulas. The first results of our work,
namely, approximate techniques checking real-time automata models of systems
for LDI and PLDI specifications, are described in this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3844</identifier>
 <datestamp>2012-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3844</id><created>2012-12-16</created><authors><author><keyname>Hajizadeh</keyname><forenames>Saeed</forenames></author><author><keyname>Hodtani</keyname><forenames>Ghosheh Abed</forenames></author></authors><title>Three-Receiver Broadcast Channel with Side Information</title><categories>cs.IT math.IT</categories><comments>13 pages, 4 figures, This paper is partially presented at 2012 IEEE
  International Symposium on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Three-Receiver broadcast channels (BC) are of interest due to their
information-theoretic differences with two-receiver one. In this paper, we
derive achievable rate regions for two classes of 3-receiver BC with side
information (SI), i.e. Multilevel BC (MBC) and 3-receiver less noisy BC, using
a combination of superposition coding, Gelfand-Pinsker binning scheme and
Nair-El Gamal indirect decoding. Our rate region for MBC subsumes Steinberg
rate region for 2-receiver degraded BC with SI as its special case. We will
also show that the obtained achievable rate regions in the first two cases are
tight for several classes of non-deterministic, semi-deterministic, and
deterministic 3-receiver BC when SI is available both at the transmitter and at
the receivers. We also prove that as far as a receiver is deterministic in the
three-receiver less noisy BC, the presence of side information at that receiver
does not affect the capacity region. We have also provided the writing on dirty
paper (WDP) property for 3-receiver BC is provided as an example. In the last
section, we provide simple bounds on the capacity region of the Additive
Exponential noise three-receiver broadcast channels with Additive Exponential
interference (AEN-3BC-EI).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3849</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3849</id><created>2012-12-16</created><updated>2013-01-17</updated><authors><author><keyname>Bhattacharyya</keyname><forenames>Arnab</forenames></author><author><keyname>Fischer</keyname><forenames>Eldar</forenames></author><author><keyname>Hatami</keyname><forenames>Hamed</forenames></author><author><keyname>Hatami</keyname><forenames>Pooya</forenames></author><author><keyname>Lovett</keyname><forenames>Shachar</forenames></author></authors><title>Every locally characterized affine-invariant property is testable</title><categories>cs.CC cs.DS math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let F = F_p for any fixed prime p &gt;= 2. An affine-invariant property is a
property of functions on F^n that is closed under taking affine transformations
of the domain. We prove that all affine-invariant property having local
characterizations are testable. In fact, we show a proximity-oblivious test for
any such property P, meaning that there is a test that, given an input function
f, makes a constant number of queries to f, always accepts if f satisfies P,
and rejects with positive probability if the distance between f and P is
nonzero. More generally, we show that any affine-invariant property that is
closed under taking restrictions to subspaces and has bounded complexity is
testable.
  We also prove that any property that can be described as the property of
decomposing into a known structure of low-degree polynomials is locally
characterized and is, hence, testable. For example, whether a function is a
product of two degree-d polynomials, whether a function splits into a product
of d linear polynomials, and whether a function has low rank are all examples
of degree-structural properties and are therefore locally characterized.
  Our results depend on a new Gowers inverse theorem by Tao and Ziegler for low
characteristic fields that decomposes any polynomial with large Gowers norm
into a function of low-degree non-classical polynomials. We establish a new
equidistribution result for high rank non-classical polynomials that drives the
proofs of both the testability results and the local characterization of
degree-structural properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3850</identifier>
 <datestamp>2012-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3850</id><created>2012-12-16</created><authors><author><keyname>Noorshams</keyname><forenames>Nima</forenames></author><author><keyname>Wainwright</keyname><forenames>Martin J.</forenames></author></authors><title>Belief Propagation for Continuous State Spaces: Stochastic
  Message-Passing with Quantitative Guarantees</title><categories>cs.IT cs.LG math.IT stat.ML</categories><comments>Portions of the results were presented at the International Symposium
  on Information Theory 2012. The results were also submitted to the Journal of
  Machine Learning Research on December 16th 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The sum-product or belief propagation (BP) algorithm is a widely used
message-passing technique for computing approximate marginals in graphical
models. We introduce a new technique, called stochastic orthogonal series
message-passing (SOSMP), for computing the BP fixed point in models with
continuous random variables. It is based on a deterministic approximation of
the messages via orthogonal series expansion, and a stochastic approximation
via Monte Carlo estimates of the integral updates of the basis coefficients. We
prove that the SOSMP iterates converge to a \delta-neighborhood of the unique
BP fixed point for any tree-structured graph, and for any graphs with cycles in
which the BP updates satisfy a contractivity condition. In addition, we
demonstrate how to choose the number of basis coefficients as a function of the
desired approximation accuracy \delta and smoothness of the compatibility
functions. We illustrate our theory with both simulated examples and in
application to optical flow estimation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3852</identifier>
 <datestamp>2012-12-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3852</id><created>2012-12-16</created><updated>2012-12-18</updated><authors><author><keyname>Fagiolo</keyname><forenames>Giorgio</forenames></author><author><keyname>Mastrorillo</keyname><forenames>Marina</forenames></author></authors><title>The International-Migration Network</title><categories>physics.soc-ph cs.SI</categories><comments>34 pages, 8 tables, 28 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies international migration from a complex-network
perspective. We define the international-migration network (IMN) as the
weighted-directed graph where nodes are world countries and links account for
the stock of migrants originated in a given country and living in another
country at a given point in time. We characterize the binary and weighted
architecture of the network and its evolution over time in the period
1960-2000. We find that the IMN is organized around a modular structure
characterized by a small-world pattern displaying disassortativity and high
clustering, with power-law distributed weighted-network statistics. We also
show that a parsimonious gravity model of migration can account for most of
observed IMN topological structure. Overall, our results suggest that
socio-economic, geographical and political factors are more important than
local-network properties in shaping the structure of the IMN.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3853</identifier>
 <datestamp>2012-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3853</id><created>2012-12-16</created><authors><author><keyname>Ramaswamy</keyname><forenames>Vinod</forenames></author><author><keyname>Adlakha</keyname><forenames>Sachin</forenames></author><author><keyname>Shakkottai</keyname><forenames>Srinivas</forenames></author><author><keyname>Wierman</keyname><forenames>Adam</forenames></author></authors><title>Incentives for P2P-Assisted Content Distribution: If You Can't Beat 'Em,
  Join 'Em</title><categories>cs.SI</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The rapid growth of content distribution on the Internet has brought with it
proportional increases in the costs of distributing content. Adding to
distribution costs is the fact that digital content is easily duplicable, and
hence can be shared in an illicit peer-to-peer (P2P) manner that generates no
revenue for the content provider. In this paper, we study whether the content
provider can recover lost revenue through a more innovative approach to
distribution. In particular, we evaluate the benefits of a hybrid
revenue-sharing system that combines a legitimate P2P swarm and a centralized
client-server approach. We show how the revenue recovered by the content
provider using a server-supported legitimate P2P swarm can exceed that of the
monopolistic scheme by an order of magnitude. Our analytical results are
obtained in a fluid model, and supported by stochastic simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3859</identifier>
 <datestamp>2013-11-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3859</id><created>2012-12-16</created><updated>2013-11-24</updated><authors><author><keyname>Jalali</keyname><forenames>Shirin</forenames></author><author><keyname>Ho</keyname><forenames>Tracey</forenames></author></authors><title>On Capacity Region of Wiretap Networks</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we consider the problem of secure network coding where an
adversary has access to an unknown subset of links chosen from a known
collection of links subsets. We study the capacity region of such networks,
commonly called &quot;wiretap networks&quot;, subject to weak and strong secrecy
constraints, and consider both zero-error and asymptotically zero-error
communication. We prove that in general discrete memoryless networks modeled by
discrete memoryless channels, the capacity region subject to strong secrecy
requirement and the capacity region subject to weak secrecy requirement are
equal. In particular, this result shows that requiring strong secrecy in a
wiretap network with asymptotically zero probability of error does not shrink
the capacity region compared to the case of weak secrecy requirement. We also
derive inner and outer bounds on the network coding capacity region of wiretap
networks subject to weak secrecy constraint, for both zero probability of error
and asymptotically zero probability of error, in terms of the entropic region.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3866</identifier>
 <datestamp>2013-05-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3866</id><created>2012-12-16</created><updated>2013-04-30</updated><authors><author><keyname>Santhanam</keyname><forenames>Narayana</forenames></author><author><keyname>Anantharam</keyname><forenames>Venkat</forenames></author></authors><title>Agnostic insurability of model classes</title><categories>math.ST cs.IT math.IT stat.TH</categories><acm-class>G.3; D.4.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivated by problems in insurance, our task is to predict finite upper
bounds on a future draw from an unknown distribution $p$ over the set of
natural numbers. We can only use past observations generated independently and
identically distributed according to $p$. While $p$ is unknown, it is known to
belong to a given collection ${\cal P}$ of probability distributions on the
natural numbers.
  The support of the distributions $p \in {\cal P}$ may be unbounded, and the
prediction game goes on for \emph{infinitely} many draws. We are allowed to
make observations without predicting upper bounds for some time. But we must,
with probability 1, start and then continue to predict upper bounds after a
finite time irrespective of which $p \in {\cal P}$ governs the data.
  If it is possible, without knowledge of $p$ and for any prescribed confidence
however close to 1, to come up with a sequence of upper bounds that is never
violated over an infinite time window with confidence at least as big as
prescribed, we say the model class ${\cal P}$ is \emph{insurable}.
  We completely characterize the insurability of any class ${\cal P}$ of
distributions over natural numbers by means of a condition on how the
neighborhoods of distributions in ${\cal P}$ should be, one that is both
necessary and sufficient.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3870</identifier>
 <datestamp>2012-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3870</id><created>2012-12-16</created><authors><author><keyname>H&#xf6;lzl</keyname><forenames>Johannes</forenames><affiliation>Technische Universit&#xe4;t M&#xfc;nchen</affiliation></author><author><keyname>Nipkow</keyname><forenames>Tobias</forenames><affiliation>Technische Universit&#xe4;t M&#xfc;nchen</affiliation></author></authors><title>Interactive verification of Markov chains: Two distributed protocol case
  studies</title><categories>cs.LO</categories><comments>In Proceedings QFM 2012, arXiv:1212.3454</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 103, 2012, pp. 17-31</journal-ref><doi>10.4204/EPTCS.103.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Probabilistic model checkers like PRISM only check probabilistic systems of a
fixed size. To guarantee the desired properties for an arbitrary size,
mathematical analysis is necessary. We show for two case studies how this can
be done in the interactive proof assistant Isabelle/HOL. The first case study
is a detailed description of how we verified properties of the ZeroConf
protocol, a decentral address allocation protocol. The second case study shows
the more involved verification of anonymity properties of the Crowds protocol,
an anonymizing protocol.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3871</identifier>
 <datestamp>2012-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3871</id><created>2012-12-16</created><authors><author><keyname>Abdulla</keyname><forenames>Parosh Aziz</forenames><affiliation>Uppsala University, Sweden</affiliation></author><author><keyname>Atig</keyname><forenames>Mohamed Faouzi</forenames><affiliation>Uppsala University, Sweden</affiliation></author><author><keyname>Stenman</keyname><forenames>Jari</forenames><affiliation>Uppsala University, Sweden</affiliation></author></authors><title>Adding Time to Pushdown Automata</title><categories>cs.LO cs.FL</categories><comments>In Proceedings QFM 2012, arXiv:1212.3454</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 103, 2012, pp. 1-16</journal-ref><doi>10.4204/EPTCS.103.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this tutorial, we illustrate through examples how we can combine two
classical models, namely those of pushdown automata (PDA) and timed automata,
in order to obtain timed pushdown automata (TPDA). Furthermore, we describe how
the reachability problem for TPDAs can be reduced to the reachability problem
for PDAs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3872</identifier>
 <datestamp>2012-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3872</id><created>2012-12-16</created><authors><author><keyname>Larsen</keyname><forenames>Kim G.</forenames><affiliation>Department of Computer Science, Aalborg University</affiliation></author><author><keyname>Mardare</keyname><forenames>Radu</forenames><affiliation>Department of Computer Science, Aalborg University</affiliation></author><author><keyname>Thrane</keyname><forenames>Claus</forenames><affiliation>Department of Computer Science, Aalborg University</affiliation></author></authors><title>Parameterized Metatheory for Continuous Markovian Logic</title><categories>cs.LO</categories><comments>In Proceedings QFM 2012, arXiv:1212.3454</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 103, 2012, pp. 33-47</journal-ref><doi>10.4204/EPTCS.103.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper shows that a classic metalogical framework, including all Boolean
operators, can be used to support the development of a metric behavioural
theory for Markov processes. Previously, only intuitionistic frameworks or
frameworks without negation and logical implication have been developed to
fulfill this task. The focus of this paper is on continuous Markovian logic
(CML), a logic that characterizes stochastic bisimulation of Markov processes
with an arbitrary measurable state space and continuous-time transitions. For a
parameter epsilon&gt;0 interpreted as observational error, we introduce an
epsilon-parameterized metatheory for CML: we define the concepts of
epsilon-satisfiability and epsilon-provability related by a sound and complete
axiomatization and prove a series of &quot;parameterized&quot; metatheorems including
decidability, weak completeness and finite model property. We also prove
results regarding the relations between metalogical concepts defined for
different parameters. Using this framework, we can characterize both the
stochastic bisimulation relation and various observational preorders based on
behavioural pseudometrics. The main contribution of this paper is proving that
all these analyses can actually be done using a unified complete Boolean
framework. This extends the state of the art in this field, since the related
works only propose intuitionistic contexts that limit, for instance, the use of
the Boolean logical implication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3873</identifier>
 <datestamp>2012-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3873</id><created>2012-12-16</created><authors><author><keyname>Mao</keyname><forenames>Hua</forenames><affiliation>AAU</affiliation></author><author><keyname>Chen</keyname><forenames>Yingke</forenames><affiliation>AAU</affiliation></author><author><keyname>Jaeger</keyname><forenames>Manfred</forenames><affiliation>AAU</affiliation></author><author><keyname>Nielsen</keyname><forenames>Thomas D.</forenames><affiliation>AAU</affiliation></author><author><keyname>Larsen</keyname><forenames>Kim G.</forenames><affiliation>AAU</affiliation></author><author><keyname>Nielsen</keyname><forenames>Brian</forenames><affiliation>AAU</affiliation></author></authors><title>Learning Markov Decision Processes for Model Checking</title><categories>cs.LG cs.LO cs.SE</categories><comments>In Proceedings QFM 2012, arXiv:1212.3454</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 103, 2012, pp. 49-63</journal-ref><doi>10.4204/EPTCS.103.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Constructing an accurate system model for formal model verification can be
both resource demanding and time-consuming. To alleviate this shortcoming,
algorithms have been proposed for automatically learning system models based on
observed system behaviors. In this paper we extend the algorithm on learning
probabilistic automata to reactive systems, where the observed system behavior
is in the form of alternating sequences of inputs and outputs. We propose an
algorithm for automatically learning a deterministic labeled Markov decision
process model from the observed behavior of a reactive system. The proposed
learning algorithm is adapted from algorithms for learning deterministic
probabilistic finite automata, and extended to include both probabilistic and
nondeterministic transitions. The algorithm is empirically analyzed and
evaluated by learning system models of slot machines. The evaluation is
performed by analyzing the probabilistic linear temporal logic properties of
the system as well as by analyzing the schedulers, in particular the optimal
schedulers, induced by the learned models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3874</identifier>
 <datestamp>2012-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3874</id><created>2012-12-16</created><authors><author><keyname>Aristiz&#xe1;bal</keyname><forenames>Andr&#xe9;s</forenames><affiliation>CNRS/DGA and LIX &#xc9;cole Polytechnique de Paris</affiliation></author><author><keyname>Bonchi</keyname><forenames>Filippo</forenames><affiliation>ENS Lyon, Universit&#xe9; de Lyon, LIP</affiliation></author><author><keyname>Pino</keyname><forenames>Luis</forenames><affiliation>INRIA/DGA and LIX &#xc9;cole Polytechnique de Paris</affiliation></author><author><keyname>Valencia</keyname><forenames>Frank</forenames><affiliation>CNRS and LIX &#xc9;cole Polytechnique de Paris</affiliation></author></authors><title>Reducing Weak to Strong Bisimilarity in CCP</title><categories>cs.PL cs.LO</categories><comments>In Proceedings ICE 2012, arXiv:1212.3458. arXiv admin note: text
  overlap with arXiv:1212.1548</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 104, 2012, pp. 2-16</journal-ref><doi>10.4204/EPTCS.104.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Concurrent constraint programming (ccp) is a well-established model for
concurrency that singles out the fundamental aspects of asynchronous systems
whose agents (or processes) evolve by posting and querying (partial)
information in a global medium. Bisimilarity is a standard behavioural
equivalence in concurrency theory. However, only recently a well-behaved notion
of bisimilarity for ccp, and a ccp partition refinement algorithm for deciding
the strong version of this equivalence have been proposed. Weak bisimiliarity
is a central behavioural equivalence in process calculi and it is obtained from
the strong case by taking into account only the actions that are observable in
the system. Typically, the standard partition refinement can also be used for
deciding weak bisimilarity simply by using Milner's reduction from weak to
strong bisimilarity; a technique referred to as saturation. In this paper we
demonstrate that, because of its involved labeled transitions, the
above-mentioned saturation technique does not work for ccp. We give an
alternative reduction from weak ccp bisimilarity to the strong one that allows
us to use the ccp partition refinement algorithm for deciding this equivalence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3875</identifier>
 <datestamp>2012-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3875</id><created>2012-12-16</created><authors><author><keyname>Lozes</keyname><forenames>&#xc9;tienne</forenames><affiliation>Universit&#xe4;t Kassel, Germany</affiliation></author><author><keyname>Villard</keyname><forenames>Jules</forenames><affiliation>University College London, UK</affiliation></author></authors><title>Shared Contract-Obedient Endpoints</title><categories>cs.PL cs.LO</categories><comments>In Proceedings ICE 2012, arXiv:1212.3458</comments><proxy>EPTCS</proxy><acm-class>F.3.1</acm-class><journal-ref>EPTCS 104, 2012, pp. 17-31</journal-ref><doi>10.4204/EPTCS.104.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most of the existing verification techniques for message-passing programs
suppose either that channel endpoints are used in a linear fashion, where at
most one thread may send or receive from an endpoint at any given time, or that
endpoints may be used arbitrarily by any number of threads. The former approach
usually forbids the sharing of channels while the latter limits what is
provable about programs. In this paper we propose a midpoint between these
techniques by extending a proof system based on separation logic to allow
sharing of endpoints. We identify two independent mechanisms for supporting
sharing: an extension of fractional shares to endpoints, and a new technique
based on what we call reflexive ownership transfer. We demonstrate on a number
of examples that a linear treatment of sharing is possible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3876</identifier>
 <datestamp>2012-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3876</id><created>2012-12-16</created><authors><author><keyname>Costa</keyname><forenames>Gabriele</forenames><affiliation>Dipartimento di Informatica, Sistemistica e Telematica Universita di Genova</affiliation></author><author><keyname>Martinelli</keyname><forenames>Fabio</forenames><affiliation>Istituto di Informatica e Telematica, Consiglio Nazionale delle Ricerche</affiliation></author><author><keyname>Yautsiukhin</keyname><forenames>Artsiom</forenames><affiliation>Istituto di Informatica e Telematica, Consiglio Nazionale delle Ricerche</affiliation></author></authors><title>Metric-Aware Secure Service Orchestration</title><categories>cs.CR cs.DC cs.FL</categories><comments>In Proceedings ICE 2012, arXiv:1212.3458</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 104, 2012, pp. 32-46</journal-ref><doi>10.4204/EPTCS.104.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Secure orchestration is an important concern in the internet of service. Next
to providing the required functionality the composite services must also
provide a reasonable level of security in order to protect sensitive data.
Thus, the orchestrator has a need to check whether the complex service is able
to satisfy certain properties. Some properties are expressed with metrics for
precise definition of requirements. Thus, the problem is to analyse the values
of metrics for a complex business process.
  In this paper we extend our previous work on analysis of secure orchestration
with quantifiable properties. We show how to define, verify and enforce
quantitative security requirements in one framework with other security
properties. The proposed approach should help to select the most suitable
service architecture and guarantee fulfilment of the declared security
requirements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3877</identifier>
 <datestamp>2012-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3877</id><created>2012-12-16</created><authors><author><keyname>Bliudze</keyname><forenames>Simon</forenames><affiliation>Ecole Polytechnique F&#xe9;d&#xe9;rale de Lausanne</affiliation></author></authors><title>Towards a Theory of Glue</title><categories>cs.LO cs.PL</categories><comments>In Proceedings ICE 2012, arXiv:1212.3458</comments><proxy>EPTCS</proxy><acm-class>F.3.2</acm-class><journal-ref>EPTCS 104, 2012, pp. 48-66</journal-ref><doi>10.4204/EPTCS.104.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose and study the notions of behaviour type and composition operator
making a first step towards the definition of a formal framework for studying
behaviour composition in a setting sufficiently general to provide insight into
how the component-based systems should be modelled and compared. We illustrate
the proposed notions on classical examples (Traces, Labelled Transition Systems
and Coalgebras). Finally, the definition of memoryless glue operators, takes us
one step closer to a formal understanding of the separation of concerns
principle stipulating that computational aspects of a system should be
localised within its atomic components, whereas coordination layer responsible
for managing concurrency should be realised by memoryless glue operators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3878</identifier>
 <datestamp>2012-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3878</id><created>2012-12-16</created><authors><author><keyname>Ghica</keyname><forenames>Dan R.</forenames><affiliation>University of Birmingham</affiliation></author><author><keyname>Al-Zobaidi</keyname><forenames>Zaid</forenames><affiliation>University of Birmingham</affiliation></author></authors><title>Coherent Minimisation: Towards efficient tamper-proof compilation</title><categories>cs.PL cs.FL cs.LO</categories><comments>In Proceedings ICE 2012, arXiv:1212.3458</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 104, 2012, pp. 83-98</journal-ref><doi>10.4204/EPTCS.104.8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automata representing game-semantic models of programs are meant to operate
in environments whose input-output behaviour is constrained by the rules of a
game. This can lead to a notion of equivalence between states which is weaker
than the conventional notion of bisimulation, since not all actions are
available to the environment. An environment which attempts to break the rules
of the game is, effectively, mounting a low-level attack against a system. In
this paper we show how (and why) to enforce game rules in games-based hardware
synthesis and how to use this weaker notion of equivalence, called coherent
equivalence, to aggressively minimise automata.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3879</identifier>
 <datestamp>2012-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3879</id><created>2012-12-16</created><authors><author><keyname>Rot</keyname><forenames>Jurriaan</forenames><affiliation>LIACS - Leiden University, The Netherlands</affiliation></author><author><keyname>As&#x103;voae</keyname><forenames>Irina M&#x103;riuca</forenames><affiliation>Alexandru Ioan Cuza University, Romania</affiliation></author><author><keyname>de Boer</keyname><forenames>Frank</forenames><affiliation>Centrum Wiskunde en Informatica</affiliation></author><author><keyname>Bonsangue</keyname><forenames>Marcello M.</forenames><affiliation>LIACS - Leiden University, The Netherlands</affiliation></author><author><keyname>Lucanu</keyname><forenames>Dorel</forenames><affiliation>Alexandru Ioan Cuza University, Romania</affiliation></author></authors><title>Interacting via the Heap in the Presence of Recursion</title><categories>cs.PL cs.LO</categories><comments>In Proceedings ICE 2012, arXiv:1212.3458</comments><proxy>EPTCS</proxy><acm-class>F.1.1, F.1.2, F.3.1, F.3.3</acm-class><journal-ref>EPTCS 104, 2012, pp. 99-113</journal-ref><doi>10.4204/EPTCS.104.9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Almost all modern imperative programming languages include operations for
dynamically manipulating the heap, for example by allocating and deallocating
objects, and by updating reference fields. In the presence of recursive
procedures and local variables the interactions of a program with the heap can
become rather complex, as an unbounded number of objects can be allocated
either on the call stack using local variables, or, anonymously, on the heap
using reference fields. As such a static analysis is, in general, undecidable.
  In this paper we study the verification of recursive programs with unbounded
allocation of objects, in a simple imperative language for heap manipulation.
We present an improved semantics for this language, using an abstraction that
is precise. For any program with a bounded visible heap, meaning that the
number of objects reachable from variables at any point of execution is
bounded, this abstraction is a finitary representation of its behaviour, even
though an unbounded number of objects can appear in the state. As a
consequence, for such programs model checking is decidable.
  Finally we introduce a specification language for temporal properties of the
heap, and discuss model checking these properties against heap-manipulating
programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3881</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3881</id><created>2012-12-16</created><authors><author><keyname>Gan</keyname><forenames>Yu</forenames></author><author><keyname>Tang</keyname><forenames>Ming</forenames></author><author><keyname>Yang</keyname><forenames>Han-Xin</forenames></author></authors><title>Optimal forwarding ratio on dynamical networks with heterogeneous
  mobility</title><categories>physics.soc-ph cs.SI</categories><comments>6 pages, 5 figures</comments><msc-class>82-XX, 82C70</msc-class><doi>10.1140/epjb/e2013-40036-1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As the discovery of non-Poissonian statistics of human mobility trajectories,
more attention has been paid to understanding the role of these patterns in
different dynamics. In this study, we first introduce the heterogeneous
mobility of mobile agents into dynamical networks, and then investigate the
forwarding strategy on the heterogeneous dynamical networks. We find that the
faster speed and the higher proportion of high-speed agents can enhance the
network throughput and reduce the mean traveling time in the case of random
forwarding. A hierarchical structure in the dependence of high-speed is
observed: the network throughput remains unchanged in small and large
high-speed value. It is interesting to find that the slightly preferential
forwarding to high-speed agents can maximize the network capacity. Through
theoretical analysis and numerical simulations, we show that the optimal
forwarding ratio stems from local structural heterogeneity of low-speed agents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3883</identifier>
 <datestamp>2014-02-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3883</id><created>2012-12-16</created><updated>2014-02-21</updated><authors><author><keyname>Mahmood</keyname><forenames>Mir H.</forenames></author><author><keyname>Bell</keyname><forenames>Mark R.</forenames></author></authors><title>Bayes Information-theoretic Radar Waveform Design and Delay-Doppler
  Resolution for Extended Targets</title><categories>cs.IT math.IT</categories><comments>The paper has been withdrawn because a more efficient algorithm for
  waveform design has been proposed the authors. The journal version of the
  paper will be uploaded shortly</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider the problem of information-theoretic waveform
design for active sensing systems such as radar for extended targets. Contrary
to the popular formulation of the problem in the estimation-theoretic context,
we are rather interested in a Bayes decision theoretic approach where a target
present in the environment belongs to two or more classes whose priors are
known. Optimal information theory based transmit waveforms are designed by
maximizing mutual information (MI) between the received signal and the target
impulse response, resulting in a novel iterative design equation. We also
derive signal to noise ratio (SNR) maximization based waveforms. In an effort
to quantize the benefits of such a design approach, the delay-Doppler ambiguity
function of information-theoretic waveforms are presented and is compared with
Barker codes of similar time-bandwidth product. It is found that the ambiguity
function of information-theoretic waveforms has very sharp main lobe in general
and excellent time autocorrelation properties in particular.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3884</identifier>
 <datestamp>2015-06-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3884</id><created>2012-12-16</created><updated>2015-06-28</updated><authors><author><keyname>Schuppan</keyname><forenames>Viktor</forenames></author></authors><title>Extracting Unsatisfiable Cores for LTL via Temporal Resolution</title><categories>cs.LO</categories><comments>Full version of an Acta Informatica paper</comments><doi>10.1007/s00236-015-0242-1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Unsatisfiable cores (UCs) are a well established means for debugging in a
declarative setting. Still, there are few tools that perform automated
extraction of UCs for LTL. Existing tools compute a UC as an unsatisfiable
subset of the set of top-level conjuncts of an LTL formula. Using resolution
graphs to extract UCs is common in other domains such as SAT. In this article
we construct and optimize resolution graphs for temporal resolution as
implemented in the temporal resolution-based solver TRP++, and we use them to
extract UCs for propositional LTL. The resulting UCs are more fine-grained than
the UCs obtained from existing tools because UC extraction also simplifies
top-level conjuncts instead of treating them as atomic entities. For example,
given an unsatisfiable LTL formula of the form $\phi \equiv ({\bf G} \psi)
\wedge {\bf F} \psi'$ existing tools return $\phi$ as a UC irrespective of the
complexity of $\psi$ and $\psi'$, whereas the approach presented in this
article continues to remove parts not required for unsatisfiability inside
$\psi$ and $\psi'$. Our approach also identifies groups of occurrences of a
proposition that do not interact in a proof of unsatisfiability. We implement
our approach in TRP++. Our experimental evaluation demonstrates that our
approach (i) extracts UCs that are often significantly smaller than the input
formula with an acceptable overhead and (ii) produces more fine-grained UCs
than competing tools while remaining at least competitive in terms of run time
and memory usage. The source code of our tool is publicly available.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3886</identifier>
 <datestamp>2012-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3886</id><created>2012-12-17</created><authors><author><keyname>Chen</keyname><forenames>Qiuhui</forenames></author><author><keyname>Li</keyname><forenames>Luoqing</forenames></author><author><keyname>Wang</keyname><forenames>Yi</forenames></author></authors><title>Amplitudes of mono-components and representation by generalized sampling
  functions</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A mono-component is a real-valued signal of finite energy that has
non-negative instantaneous frequencies, which may be defined as the derivative
of the phase function of the given real-valued signal through the approach of
canonical amplitude-phase modulation. We study in this article how the
amplitude is determined by its phase in a canonical amplitude-phase modulation.
Our finding is that such an amplitude can be perfectly reconstructed by a
sampling formula using the so-called generalized sampling functions and their
Hilbert transforms. The regularity of such an amplitude is identified to be at
least continuous. Meanwhile, we also make a very interesting and new
characterization of the band-limited functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3889</identifier>
 <datestamp>2012-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3889</id><created>2012-12-17</created><authors><author><keyname>Aurora</keyname><forenames>Pawan</forenames></author><author><keyname>Singh</keyname><forenames>Sumit</forenames></author><author><keyname>Mehta</keyname><forenames>Shashank K.</forenames></author></authors><title>Partial Degree Bounded Edge Packing Problem with Arbitrary Bounds</title><categories>cs.DS</categories><comments>12 pages</comments><msc-class>68W25</msc-class><acm-class>G.1.6; G.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the Partial Degree Bounded Edge Packing (PDBEP) problem introduced
in [5] by Zhang. They have shown that this problem is NP-Hard even for uniform
degree constraint. They also presented approximation algorithms for the case
when all the vertices have degree constraint of 1 and 2 with approximation
ratio of 2 and 32=11 respectively. In this work we study general degree
constraint case (arbitrary degree constraint for each vertex) and present two
combinatorial approximation algorithms with approximation factors 4 and 2. We
also study integer program based solution and present an iterative rounding
algorithm with approximation factor 3/(1 - \epsilon)^2 for any positive
\epsilon. Next we study the same problem with weighted edges. In this case we
present an O(log n) approximation algorithm. Zhang has given an exact O(n^2)
complexity algorithm for trees in case of uniform degree constraint. We improve
their result by giving O(nlog n) complexity exact algorithm for trees with
general degree constraint.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3900</identifier>
 <datestamp>2012-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3900</id><created>2012-12-17</created><updated>2012-12-21</updated><authors><author><keyname>Hong</keyname><forenames>Liangjie</forenames></author></authors><title>A Tutorial on Probabilistic Latent Semantic Analysis</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this tutorial, I will discuss the details about how Probabilistic Latent
Semantic Analysis (PLSA) is formalized and how different learning algorithms
are proposed to learn the model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3903</identifier>
 <datestamp>2012-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3903</id><created>2012-12-17</created><authors><author><keyname>Natarajan</keyname><forenames>Lakshmi Prasad</forenames></author><author><keyname>Rajan</keyname><forenames>B. Sundar</forenames></author></authors><title>Full-Rate, Full-Diversity, Finite Feedback Space-Time Schemes with
  Minimum Feedback and Transmission Duration</title><categories>cs.IT math.IT</categories><comments>12 pages, 5 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper a MIMO quasi static block fading channel with finite N-ary
delay-free, noise-free feedback is considered. The transmitter uses a set of N
Space-Time Block Codes (STBCs), one corresponding to each of the N possible
feedback values, to encode and transmit information. The feedback function used
at the receiver and the N component STBCs used at the transmitter together
constitute a Finite Feedback Scheme (FFS). Although a number of FFSs are
available in the literature that provably achieve full-diversity, there is no
known universal criterion to determine whether a given arbitrary FFS achieves
full-diversity or not. Further, all known full-diversity FFSs for T&lt;N_t where
N_t is the number of transmit antennas, have rate at the most 1. In this paper
a universal necessary condition for any FFS to achieve full-diversity is given,
using which the notion of Feedback-Transmission duration optimal (FT-Optimal)
FFSs - schemes that use minimum amount of feedback N given the transmission
duration T, and minimum transmission duration given the amount of feedback to
achieve full-diversity - is introduced. When there is no feedback (N=1) an
FT-optimal scheme consists of a single STBC with T=N_t, and the universal
necessary condition reduces to the well known necessary and sufficient
condition for an STBC to achieve full-diversity: every non-zero codeword
difference matrix of the STBC must be of rank N_t. Also, a sufficient condition
for full-diversity is given for the FFSs in which the component STBC with the
largest minimum Euclidean distance is chosen. Using this sufficient condition
full-rate (rate N_t) full-diversity FT-Optimal schemes are constructed for all
(N_t,T,N) with NT=N_t. These are the first full-rate full-diversity FFSs
reported in the literature for T&lt;N_t. Simulation results show that the new
schemes have the best error performance among all known FFSs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3906</identifier>
 <datestamp>2012-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3906</id><created>2012-12-17</created><authors><author><keyname>Nasution</keyname><forenames>Mahyuddin K. M.</forenames></author></authors><title>Simple Search Engine Model: Adaptive Properties</title><categories>cs.IR</categories><comments>6 pages, noting, draf</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the relationship between query and search engine by
exploring the adaptive properties based on a simple search engine. We used set
theory and utilized the words and terms for defining singleton space of event
in a search engine model, and then provided the inclusion between one singleton
to another.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3913</identifier>
 <datestamp>2015-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3913</id><created>2012-12-17</created><updated>2015-08-31</updated><authors><author><keyname>Zhou</keyname><forenames>Guoxu</forenames></author><author><keyname>Cichocki</keyname><forenames>Andrzej</forenames></author><author><keyname>Zhang</keyname><forenames>Yu</forenames></author><author><keyname>Mandic</keyname><forenames>Danilo</forenames></author></authors><title>Group Component Analysis for Multi-block Data: Common and Individual
  Feature Extraction</title><categories>cs.CV cs.LG</categories><comments>13 pages,11 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Very often data we encounter in practice is a collection of matrices rather
than a single matrix. These multi-block data are naturally linked and hence
often share some common features and at the same time they have their own
individual features, due to the background in which they are measured and
collected. In this study we proposed a new scheme of common and individual
feature analysis (CIFA) that processes multi-block data in a linked way aiming
at discovering and separating their common and individual features. According
to whether the number of common features is given or not, two efficient
algorithms were proposed to extract the common basis which is shared by all
data. Then feature extraction is performed on the common and the individual
spaces separately by incorporating the techniques such as dimensionality
reduction and blind source separation. We also discussed how the proposed CIFA
can significantly improve the performance of classification and clustering
tasks by exploiting common and individual features of samples respectively. Our
experimental results show some encouraging features of the proposed methods in
comparison to the state-of-the-art methods on synthetic and real data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3922</identifier>
 <datestamp>2012-12-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3922</id><created>2012-12-17</created><authors><author><keyname>Boyer</keyname><forenames>H.</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Bojic</keyname><forenames>M.</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Ennamiri</keyname><forenames>H.</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Calogine</keyname><forenames>D.</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Guichard</keyname><forenames>S.</forenames><affiliation>PIMENT</affiliation></author></authors><title>Interroom radiative couplings through windows and large openings in
  buildings: Proposal of a simplified model</title><categories>cs.CE</categories><proxy>ccsd</proxy><journal-ref>JP Journal of Heat and Mass Transfer 6, 2 (2012) 191-211</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A simplified model of indoor short wave radiation couplings adapted to
multi-zone simulations is proposed, thanks to a simplifying hypothesis and to
the introduction of an indoor short wave exchange matrix. The specific
properties of this matrix appear useful to quantify the thermal radiation
exchanges between the zones separated by windows or large openings. Integrated
in CODYRUN software, this module is detailed and compared to experimental
measurements carried out on a real scale tropical building.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3924</identifier>
 <datestamp>2012-12-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3924</id><created>2012-12-17</created><authors><author><keyname>Boyer</keyname><forenames>H.</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Lauret</keyname><forenames>A. P.</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Adelard</keyname><forenames>L.</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Mara</keyname><forenames>T. A.</forenames><affiliation>PIMENT</affiliation></author></authors><title>Building ventilation: A pressure airflow model computer generation and
  elements of validation</title><categories>cs.CE</categories><proxy>ccsd</proxy><journal-ref>Energy and Buildings 29, 3 (1999) 283-292</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The calculation of airflows is of great importance for detailed building
thermal simulation computer codes, these airflows most frequently constituting
an important thermal coupling between the building and the outside on one hand,
and the different thermal zones on the other. The driving effects of air
movement, which are the wind and the thermal buoyancy, are briefly outlined and
we look closely at their coupling in the case of buildings, by exploring the
difficulties associated with large openings. Some numerical problems tied to
the resolving of the non-linear system established are also covered. Part of a
detailled simulation software (CODYRUN), the numerical implementation of this
airflow model is explained, insisting on data organization and processing
allowing the calculation of the airflows. Comparisons are then made between the
model results and in one hand analytical expressions and in another and
experimental measurements in case of a collective dwelling.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3925</identifier>
 <datestamp>2012-12-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3925</id><created>2012-12-17</created><authors><author><keyname>Garde</keyname><forenames>F.</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Boyer</keyname><forenames>H.</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Gatina</keyname><forenames>J. C.</forenames><affiliation>PIMENT</affiliation></author></authors><title>Elaboration of global quality standards for natural and low energy
  cooling in French tropical island buildings</title><categories>cs.CE</categories><comments>cited By (since 1996) 13</comments><proxy>ccsd</proxy><journal-ref>Building and Environment 34, 1 (1999) 71-83</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Electric load profiles of tropical islands in developed countries are
characterised by morning, midday and evening peaks arising from all year round
high power demand in the commercial and residential sectors, due mostly to air
conditioning appliances and bad thermal conception of the building. The work
presented in this paper has led to the conception of a global quality standards
obtained through optimized bioclimatic urban planning and architectural design,
the use of passive cooling architectural components, natural ventilation and
energy efficient systems such as solar water heaters. We evaluated, with the
aid of an airflow and thermal building simulation software (CODYRUN), the
impact of each technical solution on thermal comfort within the building. These
technical solutions have been implemented in 280 new pilot dwelling projects
through the year 1996.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3928</identifier>
 <datestamp>2012-12-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3928</id><created>2012-12-17</created><authors><author><keyname>Lauret</keyname><forenames>A. J. P.</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Mara</keyname><forenames>T. A.</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Boyer</keyname><forenames>H.</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Adelard</keyname><forenames>L.</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Garde</keyname><forenames>F.</forenames><affiliation>PIMENT</affiliation></author></authors><title>A validation methodology aid for improving a thermal building model:
  Case of diffuse radiation accounting in a tropical climate</title><categories>cs.CE</categories><proxy>ccsd</proxy><journal-ref>Energy and Buildings 33, 7 (2001) 711-718</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  As part of our efforts to complete the software CODYRUN validation, we chose
as test building a block of flats constructed in Reunion Island, which has a
humid tropical climate. The sensitivity analysis allowed us to study the
effects of both diffuse and direct solar radiation on our model of this
building. With regard to the choice and location of sensors, this stage of the
study also led us to measure the solar radiation falling on the windows. The
comparison of measured and predicted radiation clearly showed that our
predictions over-estimated the incoming solar radiation, and we were able to
trace the problem to the algorithm which calculates diffuse solar radiation. By
calculating view factors between the windows and the associated shading
devices, changes to the original program allowed us to improve the predictions,
and so this article shows the importance of sensitivity analysis in this area
of research.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3930</identifier>
 <datestamp>2012-12-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3930</id><created>2012-12-17</created><authors><author><keyname>Adelard</keyname><forenames>L.</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Boyer</keyname><forenames>H.</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Garde</keyname><forenames>F.</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Gatina</keyname><forenames>J. -C.</forenames><affiliation>PIMENT</affiliation></author></authors><title>Detailed weather data generator for building simulations</title><categories>cs.CE</categories><proxy>ccsd</proxy><journal-ref>Energy and Buildings 31, 1 (2000) 75-88</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Thermal buildings simulation softwares need meteorological files in thermal
comfort, energetic evaluation studies. Few tools can make significant
meteorological data available such as generated typical year, representative
days, or artificial meteorological database. This paper deals about the
presentation of a new software, RUNEOLE, used to provide weather data in
buildings applications with a method adapted to all kind of climates. RUNEOLE
associates three modules of description, modelling and generation of weather
data. The statistical description of an existing meteorological database makes
typical representative days available and leads to the creation of model
libraries. The generation module leads to the generation of non existing
sequences. This software tends to be usable for the searchers and designers, by
means of interactivity, facilitated use and easy communication. The conceptual
basis of this tool will be exposed and we'll propose two examples of
applications in building physics for tropical humid climates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3950</identifier>
 <datestamp>2013-06-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3950</id><created>2012-12-17</created><updated>2013-06-10</updated><authors><author><keyname>Sanabria-Russo</keyname><forenames>Luis</forenames></author><author><keyname>Cano</keyname><forenames>Cristina</forenames></author><author><keyname>Bellalta</keyname><forenames>Boris</forenames></author></authors><title>Localization Procedure for Randomly Deployed Wireless Sensor Networks</title><categories>cs.NI</categories><comments>To be presented at the 9th International Wireless Communications &amp;
  Mobile Computing Conference (IWCMC 2013), July 1-5, Cagliari - Italy. This
  work has been partially supported by the Spanish Government, through the
  project CISNETS (Plan Nacional I+D+i, TEC2012-32354)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless Sensor Networks (WSNs) are composed of nodes that gather metrics
like temperature, pollution or pressure from events generated by external
entities. Localization in WSNs is paramount, given that the collected metrics
must be related to the place of occurrence. This document presents an
alternative way towards localization in randomly deployed WSNs based on the
composability of localization protocols. Results show a totally distributed
localization procedure that achieves a higher number of located nodes than the
conventional, individual execution of localization protocols while maintaining
the same low levels of battery consumption.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3964</identifier>
 <datestamp>2012-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3964</id><created>2012-12-17</created><authors><author><keyname>Bera</keyname><forenames>Suman K.</forenames></author><author><keyname>Dutta</keyname><forenames>Sourav</forenames></author><author><keyname>Narang</keyname><forenames>Ankur</forenames></author><author><keyname>Bhattacherjee</keyname><forenames>Souvik</forenames></author></authors><title>Advanced Bloom Filter Based Algorithms for Efficient Approximate Data
  De-Duplication in Streams</title><categories>cs.IR</categories><comments>41 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Applications involving telecommunication call data records, web pages, online
transactions, medical records, stock markets, climate warning systems, etc.,
necessitate efficient management and processing of such massively exponential
amount of data from diverse sources. De-duplication or Intelligent Compression
in streaming scenarios for approximate identification and elimination of
duplicates from such unbounded data stream is a greater challenge given the
real-time nature of data arrival. Stable Bloom Filters (SBF) addresses this
problem to a certain extent. .
  In this work, we present several novel algorithms for the problem of
approximate detection of duplicates in data streams. We propose the Reservoir
Sampling based Bloom Filter (RSBF) combining the working principle of reservoir
sampling and Bloom Filters. We also present variants of the novel Biased
Sampling based Bloom Filter (BSBF) based on biased sampling concepts. We also
propose a randomized load balanced variant of the sampling Bloom Filter
approach to efficiently tackle the duplicate detection. In this work, we thus
provide a generic framework for de-duplication using Bloom Filters. Using
detailed theoretical analysis we prove analytical bounds on the false positive
rate, false negative rate and convergence rate of the proposed structures. We
exhibit that our models clearly outperform the existing methods. We also
demonstrate empirical analysis of the structures using real-world datasets (3
million records) and also with synthetic datasets (1 billion records) capturing
various input distributions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3979</identifier>
 <datestamp>2012-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3979</id><created>2012-12-17</created><authors><author><keyname>Li</keyname><forenames>Shuqin</forenames></author><author><keyname>Huang</keyname><forenames>Jianwei</forenames></author><author><keyname>Li</keyname><forenames>Shuo-Yen Robert</forenames></author></authors><title>Dynamic Profit Maximization of Cognitive Mobile Virtual Network Operator</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the profit maximization problem of a cognitive virtual network
operator in a dynamic network environment. We consider a downlink OFDM
communication system with various network dynamics, including dynamic user
demands, uncertain sensing spectrum resources, dynamic spectrum prices, and
time-varying channel conditions. In addition, heterogenous users and imperfect
sensing technology are incorporated to make the network model more realistic.
By exploring the special structural of the problem, we develop a low-complexity
on-line control policies that determine pricing and resource scheduling without
knowing the statistics of dynamic network parameters. We show that the proposed
algorithms can achieve arbitrarily close to the optimal profit with a proper
trade-off with the queuing delay.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3981</identifier>
 <datestamp>2012-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3981</id><created>2012-12-17</created><authors><author><keyname>Cheriyan</keyname><forenames>Joseph</forenames></author><author><keyname>Vegh</keyname><forenames>Laszlo A.</forenames></author></authors><title>Approximating Minimum-Cost k-Node Connected Subgraphs via
  Independence-Free Graphs</title><categories>cs.DM cs.DS math.CO</categories><comments>20 pages, 1 figure, 28 references</comments><acm-class>G.2.2; F.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a 6-approximation algorithm for the minimum-cost $k$-node
connected spanning subgraph problem, assuming that the number of nodes is at
least $k^3(k-1)+k$. We apply a combinatorial preprocessing, based on the
Frank-Tardos algorithm for $k$-outconnectivity, to transform any input into an
instance such that the iterative rounding method gives a 2-approximation
guarantee. This is the first constant-factor approximation algorithm even in
the asymptotic setting of the problem, that is, the restriction to instances
where the number of nodes is lower bounded by a function of $k$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3996</identifier>
 <datestamp>2012-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3996</id><created>2012-12-17</created><authors><author><keyname>Hadjaz</keyname><forenames>Areski</forenames><affiliation>INRIA Saclay - Ile de France</affiliation></author><author><keyname>Marceau</keyname><forenames>Ga&#xe9;tan</forenames><affiliation>INRIA Saclay - Ile de France</affiliation></author><author><keyname>Sav&#xe9;ant</keyname><forenames>Pierre</forenames><affiliation>TRT</affiliation></author><author><keyname>Schoenauer</keyname><forenames>Marc</forenames><affiliation>INRIA Saclay - Ile de France, MSR - INRIA</affiliation></author></authors><title>Increasing Air Traffic: What is the Problem?</title><categories>cs.AI cs.SY</categories><comments>SESAR 2nd Innovation Days (2012)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, huge efforts are made to modernize the air traffic management
systems to cope with uncertainty, complexity and sub-optimality. An answer is
to enhance the information sharing between the stakeholders. This paper
introduces a framework that bridges the gap between air traffic management and
air traffic control on the one hand, and bridges the gap between the ground,
the approach and the en-route centers on the other hand. An original system is
presented, that has three essential components: the trajectory models, the
optimization process, and the monitoring process. The uncertainty of the
trajectory is modeled with a Bayesian Network, where the nodes are associated
to two types of random variables: the time of overflight on metering points of
the airspace, and the traveling time of the routes linking these points. The
resulting Bayesian Network covers the complete airspace, and Monte- Carlo
simulations are done to estimate the probabilities of sector congestion and
delays. On top of this trajectory model, an optimization process minimizes
these probabilities by tuning the parameters of the Bayesian trajectory model
related to overflight times on metering points. The last component is the
monitoring process, that continuously updates the situation of the airspace,
modifying the trajectories uncertainties according to actual positions of
aircraft. After each update, a new optimal set of overflight times is computed,
and can be communicated to the controllers as clearances for the aircraft
pilots. The paper presents a formal specification of this global optimization
problem, whose underlying rationale was derived with the help of air traffic
controllers at Thales Air Systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.3998</identifier>
 <datestamp>2012-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.3998</id><created>2012-12-17</created><authors><author><keyname>Hadjaz</keyname><forenames>Areski</forenames><affiliation>INRIA Saclay - Ile de France</affiliation></author><author><keyname>Marceau</keyname><forenames>Ga&#xe9;tan</forenames><affiliation>INRIA Saclay - Ile de France</affiliation></author><author><keyname>Sav&#xe9;ant</keyname><forenames>Pierre</forenames><affiliation>TRT</affiliation></author><author><keyname>Schoenauer</keyname><forenames>Marc</forenames><affiliation>INRIA Saclay - Ile de France, MSR - INRIA</affiliation></author></authors><title>Online Learning for Ground Trajectory Prediction</title><categories>cs.AI cs.SY</categories><comments>SESAR 2nd Innovation Days (2012)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a model based on an hybrid system to numerically simulate
the climbing phase of an aircraft. This model is then used within a trajectory
prediction tool. Finally, the Covariance Matrix Adaptation Evolution Strategy
(CMA-ES) optimization algorithm is used to tune five selected parameters, and
thus improve the accuracy of the model. Incorporated within a trajectory
prediction tool, this model can be used to derive the order of magnitude of the
prediction error over time, and thus the domain of validity of the trajectory
prediction. A first validation experiment of the proposed model is based on the
errors along time for a one-time trajectory prediction at the take off of the
flight with respect to the default values of the theoretical BADA model. This
experiment, assuming complete information, also shows the limit of the model. A
second experiment part presents an on-line trajectory prediction, in which the
prediction is continuously updated based on the current aircraft position. This
approach raises several issues, for which improvements of the basic model are
proposed, and the resulting trajectory prediction tool shows statistically
significantly more accurate results than those of the default model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4016</identifier>
 <datestamp>2013-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4016</id><created>2012-12-17</created><updated>2013-12-21</updated><authors><author><keyname>Boyar</keyname><forenames>Joan</forenames></author><author><keyname>Kamali</keyname><forenames>Shahin</forenames></author><author><keyname>Larsen</keyname><forenames>Kim S.</forenames></author><author><keyname>L&#xf3;pez-Ortiz</keyname><forenames>Alejandro</forenames></author></authors><title>Online Bin Packing with Advice</title><categories>cs.DS</categories><comments>19 pages, 1 figure (2 subfigures)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the online bin packing problem under the advice complexity model
where the 'online constraint' is relaxed and an algorithm receives partial
information about the future requests. We provide tight upper and lower bounds
for the amount of advice an algorithm needs to achieve an optimal packing. We
also introduce an algorithm that, when provided with log n + o(log n) bits of
advice, achieves a competitive ratio of 3/2 for the general problem. This
algorithm is simple and is expected to find real-world applications. We
introduce another algorithm that receives 2n + o(n) bits of advice and achieves
a competitive ratio of 4/3 + {\epsilon}. Finally, we provide a lower bound
argument that implies that advice of linear size is required for an algorithm
to achieve a competitive ratio better than 9/8.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4029</identifier>
 <datestamp>2012-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4029</id><created>2012-12-17</created><authors><author><keyname>Laguna</keyname><forenames>M. F.</forenames></author><author><keyname>Abramson</keyname><forenames>G.</forenames></author><author><keyname>Iglesias</keyname><forenames>J. R.</forenames></author></authors><title>Compelled to do the right thing</title><categories>physics.soc-ph cs.SI nlin.AO physics.comp-ph</categories><comments>8 pages, 9 figures, submitted to EPJB</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We use a model of opinion formation to study the consequences of some
mechanisms attempting to enforce the right behaviour in a society. We start
from a model where the possible choices are not equivalent (such is the case
when the agents decide to comply or not with a law) and where an imitation
mechanism allow the agents to change their behaviour based on the influence of
a group of partners. In addition, we consider the existence of two social
constraints: a) an external authority, called monitor, that imposes the correct
behaviour with infinite persuasion and b) an educated group of agents that act
upon their fellows but never change their own opinion, i.e., they exhibit
infinite adamancy. We determine the minimum number of monitors to induce an
effective change in the behaviour of the social group, and the size of the
educated group that produces the same effect. Also, we compare the results for
the cases of random social interactions and agents placed on a network. We have
verified that a small number of monitors are enough to change the behaviour of
the society. This also happens with a relatively small educated group in the
case of random interactions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4034</identifier>
 <datestamp>2012-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4034</id><created>2012-12-17</created><authors><author><keyname>Wunder</keyname><forenames>Gerhard</forenames></author><author><keyname>Kasparick</keyname><forenames>Martin</forenames></author><author><keyname>Brink</keyname><forenames>Stephan ten</forenames></author><author><keyname>Schaich</keyname><forenames>Frank</forenames></author><author><keyname>Wild</keyname><forenames>Thorsten</forenames></author><author><keyname>Gaspar</keyname><forenames>Ivan</forenames></author><author><keyname>Ohlmer</keyname><forenames>Eckhard</forenames></author><author><keyname>Krone</keyname><forenames>Stefan</forenames></author><author><keyname>Michailow</keyname><forenames>Nicola</forenames></author><author><keyname>Navarro</keyname><forenames>Ainoa</forenames></author><author><keyname>Fettweis</keyname><forenames>Gerhard</forenames></author><author><keyname>Ktenas</keyname><forenames>Dimitri</forenames></author><author><keyname>Berg</keyname><forenames>Vincent</forenames></author><author><keyname>Dryjanski</keyname><forenames>Marcin</forenames></author><author><keyname>Pietrzyk</keyname><forenames>Slawomir</forenames></author><author><keyname>Eged</keyname><forenames>Bertalan</forenames></author></authors><title>5GNOW: Challenging the LTE Design Paradigms of Orthogonality and
  Synchronicity</title><categories>cs.IT cs.NI math.IT</categories><comments>Submitted to Workshop on Mobile and Wireless Communication Systems
  for 2020 and beyond (at IEEE VTC 2013, Spring)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  LTE and LTE-Advanced have been optimized to deliver high bandwidth pipes to
wireless users. The transport mechanisms have been tailored to maximize single
cell performance by enforcing strict synchronism and orthogonality within a
single cell and within a single contiguous frequency band. Various emerging
trends reveal major shortcomings of those design criteria: 1) The fraction of
machine-type-communications (MTC) is growing fast. Transmissions of this kind
are suffering from the bulky procedures necessary to ensure strict synchronism.
2) Collaborative schemes have been introduced to boost capacity and coverage
(CoMP), and wireless networks are becoming more and more heterogeneous
following the non-uniform distribution of users. Tremendous efforts must be
spent to collect the gains and to manage such systems under the premise of
strict synchronism and orthogonality. 3) The advent of the Digital Agenda and
the introduction of carrier aggregation are forcing the transmission systems to
deal with fragmented spectrum. 5GNOW is an European research project supported
by the European Commission within FP7 ICT Call 8. It will question the design
targets of LTE and LTE-Advanced having these shortcomings in mind and the
obedience to strict synchronism and orthogonality will be challenged. It will
develop new PHY and MAC layer concepts being better suited to meet the upcoming
needs with respect to service variety and heterogeneous transmission setups.
Wireless transmission networks following the outcomes of 5GNOW will be better
suited to meet the manifoldness of services, device classes and transmission
setups present in envisioned future scenarios like smart cities. The
integration of systems relying heavily on MTC into the communication network
will be eased. The per-user experience will be more uniform and satisfying. To
ensure this 5GNOW will contribute to upcoming 5G standardization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4080</identifier>
 <datestamp>2012-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4080</id><created>2012-12-17</created><authors><author><keyname>Orendorff</keyname><forenames>David</forenames></author><author><keyname>Mjolsness</keyname><forenames>Eric</forenames></author></authors><title>A Hierarchical Exact Accelerated Stochastic Simulation Algorithm</title><categories>q-bio.MN cs.CE cs.DS</categories><comments>22 pages, 3 figures</comments><journal-ref>J. Chem. Phys. 137, 214104 (2012)</journal-ref><doi>10.1063/1.4766353</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new algorithm, &quot;HiER-leap&quot;, is derived which improves on the computational
properties of the ER-leap algorithm for exact accelerated simulation of
stochastic chemical kinetics. Unlike ER-leap, HiER-leap utilizes a hierarchical
or divide-and-conquer organization of reaction channels into tightly coupled
&quot;blocks&quot; and is thereby able to speed up systems with many reaction channels.
Like ER-leap, HiER-leap is based on the use of upper and lower bounds on the
reaction propensities to define a rejection sampling algorithm with inexpensive
early rejection and acceptance steps. But in HiER-leap, large portions of
intra-block sampling may be done in parallel. An accept/reject step is used to
synchronize across blocks. This method scales well when many reaction channels
are present and has desirable asymptotic properties. The algorithm is exact,
parallelizable and achieves a significant speedup over SSA and ER-leap on
certain problems. This algorithm offers a potentially important step towards
efficient in silico modeling of entire organisms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4086</identifier>
 <datestamp>2012-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4086</id><created>2012-12-17</created><authors><author><keyname>de Gevigney</keyname><forenames>Olivier Durand</forenames></author></authors><title>On Frank's conjecture on k-connected orientations</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We disprove a conjecture of Frank stating that each weakly 2k-connected has a
k-vertex-connected orientation. For k at least 3, we also prove that the
problem of deciding whether a graph has a k-vertex-connected orientation is
NP-complete.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4092</identifier>
 <datestamp>2012-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4092</id><created>2012-12-17</created><authors><author><keyname>Kashaf</keyname><forenames>A.</forenames></author><author><keyname>Javaid</keyname><forenames>N.</forenames></author><author><keyname>Khan</keyname><forenames>Z. A.</forenames></author><author><keyname>Khan</keyname><forenames>I. A.</forenames></author></authors><title>TSEP: Threshold-sensitive Stable Election Protocol for WSNs</title><categories>cs.NI</categories><comments>10th IEEE International Conference on Frontiers of Information
  Technology (FIT 12), 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless Sensor Networks (WSNs) are expected to find wide applicability and
increasing deployment in near future. In this paper, we propose a new protocol,
Threshold Sensitive Stable Election Protocol (TSEP), which is reactive protocol
using three levels of heterogeneity. Reactive networks, as opposed to proactive
networks, respond immediately to changes in relevant parameters of interest. We
evaluate performance of our protocol for a simple temperature sensing
application and compare results of protocol with some other protocols LEACH,
DEEC, SEP, ESEP and TEEN. And from simulation results it is observed that
protocol outperforms concerning life time of sensing nodes used.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4093</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4093</id><created>2012-12-17</created><updated>2014-01-16</updated><authors><author><keyname>Choi</keyname><forenames>David</forenames></author><author><keyname>Wolfe</keyname><forenames>Patrick J.</forenames></author></authors><title>Co-clustering separately exchangeable network data</title><categories>math.ST cs.SI math.CO stat.ML stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/13-AOS1173 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS1173</report-no><journal-ref>Annals of Statistics 2014, Vol. 42, No. 1, 29-63</journal-ref><doi>10.1214/13-AOS1173</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article establishes the performance of stochastic blockmodels in
addressing the co-clustering problem of partitioning a binary array into
subsets, assuming only that the data are generated by a nonparametric process
satisfying the condition of separate exchangeability. We provide oracle
inequalities with rate of convergence $\mathcal{O}_P(n^{-1/4})$ corresponding
to profile likelihood maximization and mean-square error minimization, and show
that the blockmodel can be interpreted in this setting as an optimal
piecewise-constant approximation to the generative nonparametric model. We also
show for large sample sizes that the detection of co-clusters in such data
indicates with high probability the existence of co-clusters of equal size and
asymptotically equivalent connectivity in the underlying generative process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4106</identifier>
 <datestamp>2012-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4106</id><created>2012-12-17</created><authors><author><keyname>Shah</keyname><forenames>T.</forenames></author><author><keyname>Javaid</keyname><forenames>N.</forenames></author><author><keyname>Qureshi</keyname><forenames>T. N.</forenames></author></authors><title>Energy Efficient Sleep Awake Aware (EESAA) Intelligent Sensor Network
  Routing Protocol</title><categories>cs.NI</categories><comments>15th IEEE International Multi Topic Conference (INMIC12), 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Wireless Sensor Networks (WSNs), with growing applications in the environment
which are not within human reach have been addressed tremendously in the recent
past. For optimized working of network many routing algorithms have been
proposed, mainly focusing energy efficiency, network lifetime, clustering
processes. Considering homogeneity of network, we proposed Energy Efficient
Sleep Awake Aware (EESAA) intelligent routing protocol for WSNs. In our
proposed technique we evaluate and enhance certain issues like network
stability, network lifetime and cluster head selection process. Utilizing the
concept of characteristical pairing among sensor nodes energy utilization is
optimized. Simulation results show that our proposed protocolnificantly
improved the
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4115</identifier>
 <datestamp>2012-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4115</id><created>2012-12-17</created><authors><author><keyname>Zimmer</keyname><forenames>Stephan</forenames></author><author><keyname>Arrabito</keyname><forenames>Luisa</forenames></author><author><keyname>Glanzman</keyname><forenames>Tom</forenames></author><author><keyname>Johnson</keyname><forenames>Tony</forenames></author><author><keyname>Lavalley</keyname><forenames>Claudia</forenames></author><author><keyname>Tsaregorodtsev</keyname><forenames>Andrei</forenames></author></authors><title>Extending the Fermi-LAT Data Processing Pipeline to the Grid</title><categories>astro-ph.IM cs.DC hep-ex</categories><comments>This is an author-created, un-copyedited version of an article
  accepted for publication in Journal of Physics: Conference Series. IOP
  Publishing Ltd is not responsible for any errors or omissions in this version
  of the manuscript or any version derived from it. The Version of Record is
  available online at http://dx.doi.org/10.1088/1742-6596/396/3/032121</comments><journal-ref>2012 J. Phys.: Conf. Ser. 396 032121</journal-ref><doi>10.1088/1742-6596/396/3/032121</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Data Handling Pipeline (&quot;Pipeline&quot;) has been developed for the Fermi
Gamma-Ray Space Telescope (Fermi) Large Area Telescope (LAT) which launched in
June 2008. Since then it has been in use to completely automate the production
of data quality monitoring quantities, reconstruction and routine analysis of
all data received from the satellite and to deliver science products to the
collaboration and the Fermi Science Support Center. Aside from the
reconstruction of raw data from the satellite (Level 1), data reprocessing and
various event-level analyses are also reasonably heavy loads on the pipeline
and computing resources. These other loads, unlike Level 1, can run
continuously for weeks or months at a time. In addition it receives heavy use
in performing production Monte Carlo tasks.
  The software comprises web-services that allow online monitoring and provides
charts summarizing work flow aspects and performance information. The server
supports communication with several batch systems such as LSF and BQS and
recently also Sun Grid Engine and Condor. This is accomplished through
dedicated job control services that for Fermi are running at SLAC and the other
computing site involved in this large scale framework, the Lyon computing
center of IN2P3. While being different in the logic of a task, we evaluate a
separate interface to the Dirac system in order to communicate with EGI sites
to utilize Grid resources, using dedicated Grid optimized systems rather than
developing our own. (abstract abridged)
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4123</identifier>
 <datestamp>2013-12-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4123</id><created>2012-12-17</created><updated>2013-06-21</updated><authors><author><keyname>Rabah</keyname><forenames>Sleiman</forenames></author><author><keyname>Mokhov</keyname><forenames>Serguei A.</forenames></author><author><keyname>Paquet</keyname><forenames>Joey</forenames></author></authors><title>An Interactive Graph-Based Automation Assistant: A Case Study to Manage
  the GIPSY's Distributed Multi-tier Run-Time System</title><categories>cs.DC</categories><comments>Submitted for publication to RACS</comments><journal-ref>In Proceedings of RACS '13. ACM, pp.387-394. 2013</journal-ref><doi>10.1145/2513228.2513286</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The GIPSY system provides a framework for a distributed multi-tier
demand-driven evaluation of heterogeneous programs, in which certain tiers can
generate demands, while others can respond to demands to work on them. They are
connected through a virtual network that can be flexibly reconfigured at
run-time. Although the demand generator components were originally designed
specifically for the eductive (demand-driven) evaluation of Lucid intensional
programs, the GIPSY's run-time's flexible framework design enables it to
perform the execution of various kinds of programs that can be evaluated using
the demand-driven computational model. Management of the GISPY networks has
become a tedious (although scripted) task that took manual command-line console
to do, which does not scale for large experiments. Therefore a new component
has been designed and developed to allow users to represent, visualize, and
interactively create, configure and seamlessly manage such a network as a
graph. Consequently, this work presents a Graphical GMT Manager, an interactive
graph-based assistant component for the GIPSY network creation and
configuration management. Besides allowing the management of the nodes and
tiers (mapped to hosts where store, workers, and generators reside), it lets
the user to visually control the network parameters and the interconnection
between computational nodes at run-time. In this paper we motivate and present
the key features of this newly implemented graph-based component. We give the
graph representation details, mapping of the graph nodes to tiers, tier groups,
and specific commands. We provide the requirements and design specification of
the tool and its implementation. Then we detail and discuss some experimental
results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4129</identifier>
 <datestamp>2014-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4129</id><created>2012-12-17</created><updated>2014-10-18</updated><authors><author><keyname>Chalermsook</keyname><forenames>Parinya</forenames></author><author><keyname>Laekhanukit</keyname><forenames>Bundit</forenames></author><author><keyname>Nanongkai</keyname><forenames>Danupon</forenames></author></authors><title>Graph Products Revisited: Tight Approximation Hardness of Induced
  Matching, Poset Dimension and More</title><categories>cs.DM cs.CC cs.DS math.CO</categories><comments>Preliminary version is published in SODA 2013</comments><acm-class>F.2; G.2.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Graph product is a fundamental tool with rich applications in both graph
theory and theoretical computer science. It is usually studied in the form
$f(G*H)$ where $G$ and $H$ are graphs, * is a graph product and $f$ is a graph
property. For example, if $f$ is the independence number and * is the
disjunctive product, then the product is known to be multiplicative:
$f(G*H)=f(G)f(H)$.
  In this paper, we study graph products in the following non-standard form:
$f((G\oplus H)*J)$ where $G$, $H$ and $J$ are graphs, $\oplus$ and * are two
different graph products and $f$ is a graph property. We show that if $f$ is
the induced and semi-induced matching number, then for some products $\oplus$
and *, it is subadditive in the sense that $f((G\oplus H)*J)\leq
f(G*J)+f(H*J)$. Moreover, when $f$ is the poset dimension number, it is almost
subadditive.
  As applications of this result (we only need $J=K_2$ here), we obtain tight
hardness of approximation for various problems in discrete mathematics and
computer science: bipartite induced and semi-induced matching (a.k.a. maximum
expanding sequences), poset dimension, maximum feasible subsystem with 0/1
coefficients, unit-demand min-buying and single-minded pricing, donation center
location, boxicity, cubicity, threshold dimension and independent packing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4137</identifier>
 <datestamp>2012-12-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4137</id><created>2012-12-17</created><authors><author><keyname>Richt&#xe1;rik</keyname><forenames>Peter</forenames></author><author><keyname>Tak&#xe1;&#x10d;</keyname><forenames>Martin</forenames></author><author><keyname>Ahipa&#x15f;ao&#x11f;lu</keyname><forenames>Selin Damla</forenames></author></authors><title>Alternating Maximization: Unifying Framework for 8 Sparse PCA
  Formulations and Efficient Parallel Codes</title><categories>stat.ML cs.LG math.OC</categories><comments>20 pages, 5 tables, 6 figures (the paper is accompanied by a release
  of the open source code '24am')</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a multivariate data set, sparse principal component analysis (SPCA)
aims to extract several linear combinations of the variables that together
explain the variance in the data as much as possible, while controlling the
number of nonzero loadings in these combinations. In this paper we consider 8
different optimization formulations for computing a single sparse loading
vector; these are obtained by combining the following factors: we employ two
norms for measuring variance (L2, L1) and two sparsity-inducing norms (L0, L1),
which are used in two different ways (constraint, penalty). Three of our
formulations, notably the one with L0 constraint and L1 variance, have not been
considered in the literature. We give a unifying reformulation which we propose
to solve via a natural alternating maximization (AM) method. We show the the AM
method is nontrivially equivalent to GPower (Journ\'{e}e et al; JMLR
11:517--553, 2010) for all our formulations. Besides this, we provide 24
efficient parallel SPCA implementations: 3 codes (multi-core, GPU and cluster)
for each of the 8 problems. Parallelism in the methods is aimed at i) speeding
up computations (our GPU code can be 100 times faster than an efficient serial
code written in C++), ii) obtaining solutions explaining more variance and iii)
dealing with big data problems (our cluster code is able to solve a 357 GB
problem in about a minute).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4174</identifier>
 <datestamp>2012-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4174</id><created>2012-12-17</created><authors><author><keyname>Scherrer</keyname><forenames>Chad</forenames></author><author><keyname>Tewari</keyname><forenames>Ambuj</forenames></author><author><keyname>Halappanavar</keyname><forenames>Mahantesh</forenames></author><author><keyname>Haglin</keyname><forenames>David</forenames></author></authors><title>Feature Clustering for Accelerating Parallel Coordinate Descent</title><categories>stat.ML cs.DC cs.LG math.OC</categories><comments>Accepted for publication in the proceedings of NIPS (Neural
  Information Processing Systems Foundations) 2012, Lake Tahoe, Nevada</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large-scale L1-regularized loss minimization problems arise in
high-dimensional applications such as compressed sensing and high-dimensional
supervised learning, including classification and regression problems.
High-performance algorithms and implementations are critical to efficiently
solving these problems. Building upon previous work on coordinate descent
algorithms for L1-regularized problems, we introduce a novel family of
algorithms called block-greedy coordinate descent that includes, as special
cases, several existing algorithms such as SCD, Greedy CD, Shotgun, and
Thread-Greedy. We give a unified convergence analysis for the family of
block-greedy algorithms. The analysis suggests that block-greedy coordinate
descent can better exploit parallelism if features are clustered so that the
maximum inner product between features in different blocks is small. Our
theoretical convergence analysis is supported with experimental re- sults using
data from diverse real-world applications. We hope that algorithmic approaches
and convergence analysis we provide will not only advance the field, but will
also encourage researchers to systematically explore the design space of
algorithms for solving large-scale L1-regularization problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4179</identifier>
 <datestamp>2012-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4179</id><created>2012-12-17</created><authors><author><keyname>Yermakova</keyname><forenames>Anya</forenames></author><author><keyname>Baltag</keyname><forenames>Alexandru</forenames></author></authors><title>A Dynamic-Epistemic Logic for Mobile Structured Agents</title><categories>cs.LO math.LO q-bio.MN q-bio.QM</categories><comments>This paper was presented at ECAL'11 and later published in the book
  Integral Biomathics: Tracing the Road to Reality: Results from the First Year
  Project Activities of the INtegral BIOmathics Support Action (INBIOSA)</comments><journal-ref>Integral Biomathics (2012) 129-141</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multi-agent systems have been studied in various contexts of both application
and theory. We take Dynamic Epistemic Logic (DEL), one of the formalisms
designed to reason about such systems, as the foundation of the language we
will build.
  BioAmbient calculus is an extension of \pi-calculus, developed largely for
applications to biomolecular systems. It deals with ambients and their ability
to communicate and to execute concurrent processes while moving.
  In this paper we combine the formalism of Dynamic Epistemic Logic together
with the formalism of BioAmbient Calculus in order to reason about knowledge
maintained and gained upon process transitions. The motivation lies in
developing a language that captures locally available information through
assignment of knowledge, with potential application to biological systems as
well as social, virtual, and others.
  We replace the ambients of BioAmbient Calculus with agents, to which we
attribute knowledge, and explore the parallels of this treatment. The resulting
logic describes the information flow governing mobile structured agents,
organized hierarchically, whose architecture (and local information) may change
due to actions such as communication, merging (of two agents), entering (of an
agent into the inner structure of another agent) and exiting (of an agent from
the structure of another). We show how the main axioms of DEL must be altered
to accommodate the informational effects of the agents' dynamic architecture.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4194</identifier>
 <datestamp>2012-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4194</id><created>2012-12-17</created><authors><author><keyname>Sahneh</keyname><forenames>Faryad Darabi</forenames></author><author><keyname>Scoglio</keyname><forenames>Caterina</forenames></author><author><keyname>Chowdhury</keyname><forenames>Fahmida N.</forenames></author></authors><title>Effect of Coupling on the Epidemic Threshold in Interconnected Complex
  Networks: A Spectral Analysis</title><categories>physics.soc-ph cs.SI math.DS physics.bio-ph</categories><comments>7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In epidemic modeling, the term infection strength indicates the ratio of
infection rate and cure rate. If the infection strength is higher than a
certain threshold -- which we define as the epidemic threshold - then the
epidemic spreads through the population and persists in the long run. For a
single generic graph representing the contact network of the population under
consideration, the epidemic threshold turns out to be equal to the inverse of
the spectral radius of the contact graph. However, in a real world scenario it
is not possible to isolate a population completely: there is always some
interconnection with another network, which partially overlaps with the contact
network. Results for epidemic threshold in interconnected networks are limited
to homogeneous mixing populations and degree distribution arguments. In this
paper, we adopt a spectral approach. We show how the epidemic threshold in a
given network changes as a result of being coupled with another network with
fixed infection strength. In our model, the contact network and the
interconnections are generic. Using bifurcation theory and algebraic graph
theory, we rigorously derive the epidemic threshold in interconnected networks.
These results have implications for the broad field of epidemic modeling and
control. Our analytical results are supported by numerical simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4195</identifier>
 <datestamp>2013-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4195</id><created>2012-12-17</created><updated>2013-01-07</updated><authors><author><keyname>Inuma</keyname><forenames>Manabu</forenames></author><author><keyname>Otsuka</keyname><forenames>Akira</forenames></author></authors><title>Relations among Security Metrics for Template Protection Algorithms</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many biometric template protection algorithms have been proposed mainly in
two approaches: biometric feature transformation and biometric cryptosystem.
Security evaluation of the proposed algorithms are often conducted in various
inconsistent manner. Thus, it is strongly demanded to establish the common
evaluation metrics for easier comparison among many algorithms. Simoens et al.
and Nagar et al. proposed good metrics covering nearly all aspect of
requirements expected for biometric template protection algorithms. One
drawback of the two papers is that they are biased to experimental evaluation
of security of biometric template protection algorithms. Therefore, it was
still difficult mainly for algorithms in biometric cryptosystem to prove their
security according to the proposed metrics. This paper will give a formal
definitions for security metrics proposed by Simoens et al. and Nagar et al. so
that it can be used for the evaluation of both of the two approaches. Further,
this paper will discuss the relations among several notions of security
metrics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4198</identifier>
 <datestamp>2012-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4198</id><created>2012-12-17</created><authors><author><keyname>Marques</keyname><forenames>Antonio G.</forenames></author></authors><title>Underlay Cognitive Radios with Capacity Guarantees for Primary Users</title><categories>cs.IT cs.NI math.IT</categories><comments>31 pages, 12 figures, parts of this paper were presented at CROWNCOM
  2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To use the spectrum efficiently, cognitive radios leverage knowledge of the
channel state information (CSI) to optimize the performance of the secondary
users (SUs) while limiting the interference to the primary users (PUs). The
algorithms in this paper are designed to maximize the weighted ergodic
sum-capacity of SUs, which transmit orthogonally and adhere simultaneously to
constraints limiting: i) the long-term (ergodic) capacity loss caused to each
PU receiver; ii) the long-term interference power at each PU receiver; and iii)
the long-term power at each SU transmitter. Formulations accounting for
short-term counterparts of i) and ii) are also discussed. Although the
long-term capacity constraints are non-convex, the resultant optimization
problem exhibits zero-duality gap and can be efficiently solved in the dual
domain. The optimal allocation schemes (power and rate loadings, frequency
bands to be accessed, and SU links to be activated) are a function of the CSI
of the primary and secondary networks as well as the Lagrange multipliers
associated with the long-term constraints. The optimal resource allocation
algorithms are first designed under the assumption that the CSI is perfect,
then the modifications needed to accommodate different forms of imperfect CSI
(quantized, noisy, and outdated) are analyzed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4210</identifier>
 <datestamp>2013-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4210</id><created>2012-12-17</created><updated>2013-07-10</updated><authors><author><keyname>Jalali</keyname><forenames>Shirin</forenames></author><author><keyname>Maleki</keyname><forenames>Arian</forenames></author></authors><title>From compression to compressed sensing</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Can compression algorithms be employed for recovering signals from their
underdetermined set of linear measurements? Addressing this question is the
first step towards applying compression algorithms for compressed sensing (CS).
In this paper, we consider a family of compression algorithms $\mathcal{C}_r$,
parametrized by rate $r$, for a compact class of signals $\mathcal{Q} \subset
\mathds{R}^n$. The set of natural images and JPEG at different rates are
examples of $\mathcal{Q}$ and $\mathcal{C}_r$, respectively. We establish a
connection between the rate-distortion performance of $\mathcal{C}_r$, and the
number of linear measurements required for successful recovery in CS. We then
propose compressible signal pursuit (CSP) algorithm and prove that, with high
probability, it accurately and robustly recovers signals from an
underdetermined set of linear measurements. We also explore the performance of
CSP in the recovery of infinite dimensional signals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4211</identifier>
 <datestamp>2013-08-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4211</id><created>2012-12-17</created><updated>2013-08-14</updated><authors><author><keyname>Punnen</keyname><forenames>Abraham P.</forenames></author><author><keyname>Taghipour</keyname><forenames>Sara</forenames></author><author><keyname>Karapetyan</keyname><forenames>Daniel</forenames></author><author><keyname>Bhattacharyya</keyname><forenames>Bishnu</forenames></author></authors><title>The quadratic balanced optimization problem</title><categories>math.OC cs.DM math.CO</categories><comments>26 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the quadratic balanced optimization problem (QBOP) which can be
used to model equitable distribution of resources with pairwise interaction.
QBOP is strongly NP-hard even if the family of feasible solutions has a very
simple structure. Several general purpose exact and heuristic algorithms are
presented. Results of extensive computational experiments are reported using
randomly generated quadratic knapsack problems as the test bed. These results
illustrate the efficacy of our exact and heuristic algorithms. We also show
that when the cost matrix is specially structured, QBOP can be solved as a
sequence of linear balanced optimization problems. As a consequence, we have
several polynomially solvable cases of QBOP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4237</identifier>
 <datestamp>2012-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4237</id><created>2012-12-18</created><authors><author><keyname>Kumar</keyname><forenames>S.</forenames></author><author><keyname>Javaid</keyname><forenames>N.</forenames></author><author><keyname>Yousuf</keyname><forenames>Z.</forenames></author><author><keyname>Kumar</keyname><forenames>H.</forenames></author><author><keyname>Khan</keyname><forenames>Z. A.</forenames></author><author><keyname>Qasim</keyname><forenames>U.</forenames></author></authors><title>On Link Availability Probability of Routing Protocols for Urban Scenario
  in VANETs</title><categories>cs.NI</categories><comments>IEEE Conference on Open Systems (ICOS2012)&quot;, Kuala Lumpur, Malaysia,
  2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the link availability probability. We evaluate and
compare the link availability probability for routing protocols; Ad hoc
On-demand Distance vector (AODV), Dynamic Source Routing (DSR) and Fisheye
State Routing (FSR) for different number of connections and node density. A
novel contribution of this work is enhancement in existing parameters of
routing protocols; AODV, DSR and FSR as MOD-AODV, MOD-DSR and MOD-FSR. From the
results, we observe that MOD-DSR and DSR outperform MOD-AODV, AODV, MODOLSR and
OLSR in terms of Packet Delivery Ratio (PDR), Average End-to End Delay (AE2ED),
link availability probability at the cost of high value of Normalized Routing
Overhead (NRO).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4244</identifier>
 <datestamp>2012-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4244</id><created>2012-12-18</created><authors><author><keyname>Sagar</keyname><forenames>S.</forenames></author><author><keyname>Javaid</keyname><forenames>N.</forenames></author><author><keyname>Saqib</keyname><forenames>J.</forenames></author><author><keyname>Khan</keyname><forenames>Z. A.</forenames></author><author><keyname>Qasim</keyname><forenames>U.</forenames></author><author><keyname>Khan</keyname><forenames>M. A.</forenames></author></authors><title>On Probability of Link Availability in Original and Modified AODV, FSR
  and OLSR Using 802.11 and 802.11p</title><categories>cs.NI</categories><comments>IEEE Conference on Open Systems (ICOS2012), Kuala Lumpur, Malaysia,
  2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile Ad-hoc NETworks (MANETs) comprise on wireless mobile nodes that are
communicating with each other without any infrastructure. Vehicular Ad-hoc
NETwork (VANET) is a special type of MANETs in which vehicles with high
mobility need to communicate with each other. In this paper, we present a novel
framework for link availability of paths for static as well as dynamic
networks. Moreover, we evaluate our frame work for routing protocols
performance with different number of nodes in MANETs and in VANETs. We select
three routing protocols namely Ad-hoc On-demand Distance Vector (AODV),
Fish-eye State Routing (FSR) and Optimized Link State Routing (OLSR).
Furthermore, we have also modified default parameters of selected protocols to
check their efficiencies. Performance of these protocols is analyzed using
three performance metrics; Packet Delivery Ratio (PDR), Normalized Routing
Overhead (NRO) and End-to-End Delay (E2ED) against varying scalabilities of
nodes. We perform these simulations with NS-2 using TwoRayGround propagation
model. The SUMO simulator is used to generate a random mobility pattern for
VANETs. From the extensive simulations, we observe that AODV outperforms among
all three protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4246</identifier>
 <datestamp>2012-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4246</id><created>2012-12-18</created><authors><author><keyname>Guillerm</keyname><forenames>Romaric</forenames><affiliation>LAAS</affiliation></author><author><keyname>Demmou</keyname><forenames>Hamid</forenames><affiliation>LAAS</affiliation></author><author><keyname>Sadou</keyname><forenames>Nabil</forenames><affiliation>IETR</affiliation></author></authors><title>Sysml Knowledge base for Designing Dependable Complex System</title><categories>cs.SE</categories><comments>Lambda Mu 17 - Innovation et Ma\^itrise des risques, La Rochelle :
  France (2010)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The work presented in this paper is part of a proposed framework as complete
and rigorous as possible for the design of complex systems. The methodological
framework used is System Engineering, which is a methodological approach to
control the design of complex systems. The practices of this approach are
transcribed in standards, realized by methods and supported by tools. In our
case, the standard EIA-632 was adopted. Specifically, to deal with the
dependability of these complex systems and to improve the processes dealing
with dependability, we have defined a global approach. This approach
incorporates the consideration of dependability in system engineering
processes. The work presented in this paper supports and complements the
overall approach: it is the proposal of an information model based on the SysML
language, allowing the requirements management, including safety requirements
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4247</identifier>
 <datestamp>2012-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4247</id><created>2012-12-18</created><authors><author><keyname>Guillerm</keyname><forenames>Romaric</forenames><affiliation>LAAS</affiliation></author><author><keyname>Demmou</keyname><forenames>Hamid</forenames><affiliation>LAAS</affiliation></author><author><keyname>Sadou</keyname><forenames>Nabil</forenames><affiliation>IETR</affiliation></author></authors><title>Information model for model driven safety requirements management of
  complex systems</title><categories>cs.SE cs.PF</categories><comments>Complex Systems Design &amp; Management (CSDM) 2010, Paris : France
  (2010)</comments><proxy>ccsd</proxy><doi>10.1007/978-3-642-15654-0</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this paper is to propose a rigorous and complete design framework
for complex system based on system engineering (SE) principles. The SE standard
EIA-632 is used to guide the approach. Within this framework, two aspects are
presented. The first one concerns the integration of safety requirements and
management in system engineering process. The objective is to help designers
and engineers in managing safety of complex systems. The second aspect concerns
model driven design through the definition of an information model. This model
is based on SysML (System Modeling Language) to address requirements definition
and their traceability towards the solution and the Verification and Validation
(V&amp;V) elements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4258</identifier>
 <datestamp>2012-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4258</id><created>2012-12-18</created><authors><author><keyname>Millo</keyname><forenames>Jean-Vivien</forenames></author><author><keyname>Ramesh</keyname><forenames>S.</forenames></author><author><keyname>Krishna</keyname><forenames>Shankara Narayanan</forenames></author><author><keyname>Narwane</keyname><forenames>Ganesh Khandu</forenames></author></authors><title>Compositional Verification of Evolving Software Product Lines</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel approach to the design verification of Software
Product Lines(SPL). The proposed approach assumes that the requirements and
designs are modeled as finite state machines with variability information. The
variability information at the requirement and design levels are expressed
differently and at different levels of abstraction. Also the proposed approach
supports verification of SPL in which new features and variability may be added
incrementally. Given the design and requirements of an SPL, the proposed design
verification method ensures that every product at the design level behaviorally
conforms to a product at the requirement level. The conformance procedure is
compositional in the sense that the verification of an entire SPL consisting of
multiple features is reduced to the verification of the individual features.
The method has been implemented and demonstrated in a prototype tool SPLEnD
(SPL Engine for Design Verification) on a couple of fairly large case studies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4269</identifier>
 <datestamp>2013-07-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4269</id><created>2012-12-18</created><updated>2013-07-28</updated><authors><author><keyname>Ibrahimi</keyname><forenames>Morteza</forenames></author><author><keyname>Montanari</keyname><forenames>Andrea</forenames></author><author><keyname>Moore</keyname><forenames>George S</forenames></author></authors><title>Accelerated Time-of-Flight Mass Spectrometry</title><categories>math.OC cs.CE stat.ML</categories><comments>14 pages, 18 figures. This paper is submitted to IEEE Transaction on
  Signal Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a simple modification to the conventional time of flight mass
spectrometry (TOFMS) where a \emph{variable} and (pseudo)-\emph{random} pulsing
rate is used which allows for traces from different pulses to overlap. This
modification requires little alteration to the currently employed hardware.
However, it requires a reconstruction method to recover the spectrum from
highly aliased traces. We propose and demonstrate an efficient algorithm that
can process massive TOFMS data using computational resources that can be
considered modest with today's standards. This approach can be used to improve
duty cycle, speed, and mass resolving power of TOFMS at the same time. We
expect this to extend the applicability of TOFMS to new domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4284</identifier>
 <datestamp>2014-02-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4284</id><created>2012-12-18</created><authors><author><keyname>Rodaro</keyname><forenames>Emanuele</forenames></author><author><keyname>Silva</keyname><forenames>Pedro V.</forenames></author></authors><title>On periodic points of free inverse monoid endomorphisms</title><categories>math.GR cs.FL</categories><comments>18 pages</comments><msc-class>20M35, 68Q85, 54E50</msc-class><journal-ref>IJAC No.23, Issue No. 8, 2013</journal-ref><doi>10.1142/S0218196713500446</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is proved that the periodic point submonoid of a free inverse monoid
endomorphism is always finitely generated. Using Chomsky's hierarchy of
languages, we prove that the fixed point submonoid of an endomorphism of a free
inverse monoid can be represented by a context-sensitive language but, in
general, it cannot be represented by a context-free language.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4287</identifier>
 <datestamp>2012-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4287</id><created>2012-12-18</created><authors><author><keyname>Truchet</keyname><forenames>Charlotte</forenames></author><author><keyname>Richoux</keyname><forenames>Florian</forenames></author><author><keyname>Codognet</keyname><forenames>Philippe</forenames></author></authors><title>Prediction of Parallel Speed-ups for Las Vegas Algorithms</title><categories>cs.DC cs.AI</categories><comments>10 pages, 14 figures, 5 tables. Latex ACM Sigplan format</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a probabilistic model for the parallel execution of Las Vegas
algorithms, i.e., randomized algorithms whose runtime might vary from one
execution to another, even with the same input. This model aims at predicting
the parallel performances (i.e., speedups) by analysis the runtime distribution
of the sequential runs of the algorithm. Then, we study in practice the case of
a particular Las Vegas algorithm for combinatorial optimization, on three
classical problems, and compare with an actual parallel implementation up to
256 cores. We show that the prediction can be quite accurate, matching the
actual speedups very well up to 100 parallel cores and then with a deviation of
about 20% up to 256 cores.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4288</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4288</id><created>2012-12-18</created><updated>2014-08-25</updated><authors><author><keyname>Delahaye</keyname><forenames>Beno&#xee;t</forenames><affiliation>Universit&#xe9; de Nantes, France</affiliation></author><author><keyname>Fahrenberg</keyname><forenames>Uli</forenames><affiliation>Inria / IRISA Rennes, France</affiliation></author><author><keyname>Larsen</keyname><forenames>Kim G.</forenames><affiliation>Aalborg University, Denmark</affiliation></author><author><keyname>Legay</keyname><forenames>Axel</forenames><affiliation>Inria / IRISA Rennes, France</affiliation></author></authors><title>Refinement and Difference for Probabilistic Automata</title><categories>cs.LO</categories><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 10, Issue 3 (August
  26, 2014) lmcs:942</journal-ref><doi>10.2168/LMCS-10(3:11)2014</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies a difference operator for stochastic systems whose
specifications are represented by Abstract Probabilistic Automata (APAs). In
the case refinement fails between two specifications, the target of this
operator is to produce a specification APA that represents all witness PAs of
this failure. Our contribution is an algorithm that allows to approximate the
difference of two APAs with arbitrary precision. Our technique relies on new
quantitative notions of distances between APAs used to assess convergence of
the approximations, as well as on an in-depth inspection of the refinement
relation for APAs. The procedure is effective and not more complex to implement
than refinement checking.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4303</identifier>
 <datestamp>2013-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4303</id><created>2012-12-18</created><updated>2012-12-29</updated><authors><author><keyname>Hegarty</keyname><forenames>Peter</forenames></author></authors><title>On the notion of balance in social network analysis</title><categories>cs.SI math.CO math.PR physics.soc-ph</categories><comments>Version 2: 23 pages, 4 figures. An extra section has been added
  towards the end, to help clarify some things. Some other minor changes</comments><msc-class>91D30</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The notion of &quot;balance&quot; is fundamental for sociologists who study social
networks. In formal mathematical terms, it concerns the distribution of triad
configurations in actual networks compared to random networks of the same edge
density. On reading Charles Kadushin's recent book &quot;Understanding Social
Networks&quot;, we were struck by the amount of confusion in the presentation of
this concept in the early sections of the book. This confusion seems to lie
behind his flawed analysis of a classical empirical data set, namely the karate
club graph of Zachary. Our goal here is twofold. Firstly, we present the notion
of balance in terms which are logically consistent, but also consistent with
the way sociologists use the term. The main message is that the notion can only
be meaningfully applied to undirected graphs. Secondly, we correct the analysis
of triads in the karate club graph. This results in the interesting observation
that the graph is, in a precise sense, quite &quot;unbalanced&quot;. We show that this
lack of balance is characteristic of a wide class of starlike-graphs, and
discuss possible sociological interpretations of this fact, which may be useful
in many other situations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4315</identifier>
 <datestamp>2012-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4315</id><created>2012-12-18</created><authors><author><keyname>Gatti</keyname><forenames>Lorenzo</forenames></author><author><keyname>Guerini</keyname><forenames>Marco</forenames></author></authors><title>Assessing Sentiment Strength in Words Prior Polarities</title><categories>cs.CL</categories><comments>To appear at Coling 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many approaches to sentiment analysis rely on lexica where words are tagged
with their prior polarity - i.e. if a word out of context evokes something
positive or something negative. In particular, broad-coverage resources like
SentiWordNet provide polarities for (almost) every word. Since words can have
multiple senses, we address the problem of how to compute the prior polarity of
a word starting from the polarity of each sense and returning its polarity
strength as an index between -1 and 1. We compare 14 such formulae that appear
in the literature, and assess which one best approximates the human judgement
of prior polarities, with both regression and classification models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4317</identifier>
 <datestamp>2012-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4317</id><created>2012-12-18</created><authors><author><keyname>Biasi</keyname><forenames>Felipe P.</forenames></author><author><keyname>Barreto</keyname><forenames>Paulo S. L. M.</forenames></author><author><keyname>Misoczki</keyname><forenames>Rafael</forenames></author><author><keyname>Ruggiero</keyname><forenames>Wilson V.</forenames></author></authors><title>Scaling efficient code-based cryptosystems for embedded platforms</title><categories>cs.CR</categories><msc-class>94A60, 14G50, 94B35</msc-class><acm-class>E.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a family of highly efficient codes for cryptographic purposes and
dedicated algorithms for their manipulation. Our proposal is especially
tailored for highly constrained platforms, and surpasses certain conventional
and post-quantum proposals (like RSA and NTRU, respectively) according to most
if not all efficiency metrics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4334</identifier>
 <datestamp>2012-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4334</id><created>2012-12-18</created><authors><author><keyname>Chaki</keyname><forenames>Prakash</forenames></author><author><keyname>Nawathe</keyname><forenames>Gouri</forenames></author><author><keyname>Patel</keyname><forenames>Aaqib</forenames></author><author><keyname>Merchant</keyname><forenames>S. N.</forenames></author><author><keyname>Desai</keyname><forenames>U. B.</forenames></author></authors><title>Symbiotic Cognitive Relaying with mobile Secondary nodes in Cognitive
  Radio Networks</title><categories>cs.NI</categories><comments>Ongoing work, will be updated when finished</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a Symbiotic Cognitive Relaying (SCR) scenario, the Secondary users(SU)
nodes can act as multihop relays to assist the communication between Primary
User(PU) nodes in the case of a weak direct link. In return, the SU nodes are
incentivised with the right to carry out SU-SU communication using licensed PU
band for a fixed amount of time, referred to as the 'Time Incentive'. Existing
work on SCR is constrained to a fixed ad-hoc SU network. In this paper, we
introduce mobility in SCR by considering mobile SU nodes while keeping the PU
nodes fixed. This paper uses a specific mobility pattern and routing strategy
for the SU nodes to propose theoretical bounds on the throughput and delay of
PU-PU transmission. We derive analytically the least throughput and maximum
delay possible in our model.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4336</identifier>
 <datestamp>2012-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4336</id><created>2012-12-18</created><authors><author><keyname>Ahmed</keyname><forenames>Syed Hassan</forenames></author><author><keyname>Bouk</keyname><forenames>Safdar H.</forenames></author><author><keyname>Javaid</keyname><forenames>N.</forenames></author><author><keyname>Sasase</keyname><forenames>Iwao</forenames></author></authors><title>Combined Human, Antenna Orientation in Elevation Direction and Ground
  Effect on RSSI in Wireless Sensor Networks</title><categories>cs.NI</categories><comments>10th IEEE International Conference on Frontiers of Information
  Technology (FIT 12), 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we experimentally investigate the combined effect of human,
antenna orientation in elevation direction and the ground effect on the
Received Signal Strength Indicator (RSSI) parameter in the Wireless Sensor
Network (WSN). In experiment, we use MICAz motes and consider different
scenarios where antenna of the transmitter node is tilted in elevation
direction. The motes were placed on the ground to take into account the ground
effect on the RSSI. The effect of one, two and four persons on the RSSI is
recorded. For one and two persons, different walking paces e.g. slow, medium
and fast pace, are analysed. However, in case of four persons, random movement
is carried out between the pair of motes. The experimental results show that
some antenna orientation angles have drastic effect on the RSSI, even without
any human activity. The fluctuation count and range of RSSI in different
scenarios with same walking pace are completely different. Therefore, an
efficient human activity algorithm is need that effectively takes into count
the antenna elevation and other parameters to accurately detect the human
activity in the WSN deployment region.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4347</identifier>
 <datestamp>2012-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4347</id><created>2012-12-18</created><authors><author><keyname>Shin</keyname><forenames>Bonggun</forenames></author><author><keyname>Oh</keyname><forenames>Alice</forenames></author></authors><title>Bayesian Group Nonnegative Matrix Factorization for EEG Analysis</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a generative model of a group EEG analysis, based on appropriate
kernel assumptions on EEG data. We derive the variational inference update rule
using various approximation techniques. The proposed model outperforms the
current state-of-the-art algorithms in terms of common pattern extraction. The
validity of the proposed model is tested on the BCI competition dataset.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4370</identifier>
 <datestamp>2013-10-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4370</id><created>2012-12-18</created><updated>2013-10-22</updated><authors><author><keyname>Disanto</keyname><forenames>Filippo</forenames></author><author><keyname>Imbert</keyname><forenames>Laurent</forenames></author><author><keyname>Philippe</keyname><forenames>Fabrice</forenames></author></authors><title>On the maximal weight of $(p,q)$-ary chain partitions with bounded parts</title><categories>math.NT cs.DM</categories><comments>17 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A $(p,q)$-ary chain is a special type of chain partition of integers with
parts of the form $p^aq^b$ for some fixed integers $p$ and $q$. In this note,
we are interested in the maximal weight of such partitions when their parts are
distinct and cannot exceed a given bound $m$. Characterizing the cases where
the greedy choice fails, we prove that this maximal weight is, as a function of
$m$, asymptotically independent of $\max(p,q)$, and we provide an efficient
algorithm to compute it.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4372</identifier>
 <datestamp>2013-09-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4372</id><created>2012-12-18</created><updated>2013-09-17</updated><authors><author><keyname>Beame</keyname><forenames>Paul</forenames></author><author><keyname>Clifford</keyname><forenames>Raphael</forenames></author><author><keyname>Machmouchi</keyname><forenames>Widad</forenames></author></authors><title>Sliding Windows with Limited Storage</title><categories>cs.CC cs.DS</categories><comments>The results of this paper are superceded by the paper at:
  arXiv:1309.3690</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider time-space tradeoffs for exactly computing frequency moments and
order statistics over sliding windows. Given an input of length 2n-1, the task
is to output the function of each window of length n, giving n outputs in
total. Computations over sliding windows are related to direct sum problems
except that inputs to instances almost completely overlap.
  We show an average case and randomized time-space tradeoff lower bound of TS
in Omega(n^2) for multi-way branching programs, and hence standard RAM and
word-RAM models, to compute the number of distinct elements, F_0, in sliding
windows over alphabet [n]. The same lower bound holds for computing the
low-order bit of F_0 and computing any frequency moment F_k for k not equal to
1. We complement this lower bound with a TS in \tilde O(n^2) deterministic RAM
algorithm for exactly computing F_k in sliding windows.
  We show time-space separations between the complexity of sliding-window
element distinctness and that of sliding-window $F_0\bmod 2$ computation. In
particular for alphabet [n] there is a very simple errorless sliding-window
algorithm for element distinctness that runs in O(n) time on average and uses
O(log{n}) space.
  We show that any algorithm for a single element distinctness instance can be
extended to an algorithm for the sliding-window version of element distinctness
with at most a polylogarithmic increase in the time-space product.
  Finally, we show that the sliding-window computation of order statistics such
as the maximum and minimum can be computed with only a logarithmic increase in
time, but that a TS in Omega(n^2) lower bound holds for sliding-window
computation of order statistics such as the median, a nearly linear increase in
time when space is small.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4373</identifier>
 <datestamp>2012-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4373</id><created>2012-12-18</created><authors><author><keyname>Djedid</keyname><forenames>Mohammed Nadir</forenames></author></authors><title>A trust-based security mechanism for nomadic users in pervasive systems</title><categories>cs.CR cs.AI</categories><comments>7 pages, 1 figure, 2 tables</comments><journal-ref>International Journal of Computer Science Issues IJCSI Journal,
  Volume 9, Issue 5, No 1, September 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The emergence of network technologies and the appearance of new varied
applications in terms of services and resources, has created new security
problems for which existing solutions and mechanisms are inadequate, especially
problems of identification and authentication. In a highly distributed and
pervasive system, a uniform and centralized security management is not an
option. It then becomes necessary to give more autonomy to security systems by
providing them with mechanisms that allows a dynamic and flexible cooperation
and collaboration between the actors in the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4375</identifier>
 <datestamp>2015-04-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4375</id><created>2012-12-18</created><updated>2015-04-20</updated><authors><author><keyname>Geiger</keyname><forenames>Bernhard C.</forenames></author><author><keyname>Temmel</keyname><forenames>Christoph</forenames></author></authors><title>Lumpings of Markov chains, entropy rate preservation, and higher-order
  lumpability</title><categories>cs.IT math.IT math.PR</categories><msc-class>60J10 (60G17 94A17 60G10 65C40)</msc-class><doi>10.1239/jap/1421763331</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A lumping of a Markov chain is a coordinate-wise projection of the chain. We
characterise the entropy rate preservation of a lumping of an aperiodic and
irreducible Markov chain on a finite state space by the random growth rate of
the cardinality of the realisable preimage of a finite-length trajectory of the
lumped chain and by the information needed to reconstruct original trajectories
from their lumped images. Both are purely combinatorial criteria, depending
only on the transition graph of the Markov chain and the lumping function. A
lumping is strongly k-lumpable, iff the lumped process is a k-th order Markov
chain for each starting distribution of the original Markov chain. We
characterise strong k-lumpability via tightness of stationary entropic bounds.
In the sparse setting, we give sufficient conditions on the lumping to both
preserve the entropy rate and be strongly k-lumpable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4441</identifier>
 <datestamp>2012-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4441</id><created>2012-12-18</created><authors><author><keyname>Ahmed</keyname><forenames>Syed Hassan</forenames></author><author><keyname>Bouk</keyname><forenames>Safdar H.</forenames></author><author><keyname>Javaid</keyname><forenames>Nadeem</forenames></author><author><keyname>Sasase</keyname><forenames>Iwao</forenames></author></authors><title>RF Propagation Analysis of MICAz Motes Antenna with Ground Effect</title><categories>cs.NI</categories><comments>15th IEEE International Multi Topic Conference (INMIC12), 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we analyzed the Radio Frequency (RF) propagation
characteristics of monopole antenna in MICAz mote. During the experimental
analysis, two scenarios are considered. In Scenario-I, a pair of MICAz nodes
(one transmitting and one receiving node) are placed on the ground and the RSSI
is measured in presence of the ground effect. In Scenario-II, only the
transmitting node is placed above the ground; however, the receiving node is
placed on the ground. The RSSI is measured by changing the antenna orientation
at different angles and distances between them. The results show that the
ground effect, antenna orientation and distance between the sensor nodes
drastically affect the RSSI.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4444</identifier>
 <datestamp>2012-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4444</id><created>2012-12-16</created><authors><author><keyname>Poyias</keyname><forenames>Kyriakos</forenames></author><author><keyname>Tuosto</keyname><forenames>Emilio</forenames></author></authors><title>Enforcing Architectural Styles in Presence of Unexpected Distributed
  Reconfigurations</title><categories>cs.LO cs.DC cs.NI cs.SE</categories><comments>In Proceedings ICE 2012, arXiv:1212.3458</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 104, 2012, pp. 67-82</journal-ref><doi>10.4204/EPTCS.104.7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Architectural Design Rewriting (ADR, for short) is a rule-based formal
framework for modelling the evolution of architectures of distributed systems.
Rules allow ADR graphs to be refined. After equipping ADR with a simple logic,
we equip rules with pre- and post-conditions; the former constraints the
applicability of the rules while the later specifies properties of the
resulting graphs. We give an algorithm to compute the weakest pre-condition out
of a rule and its post-condition. On top of this algorithm, we design a simple
methodology that allows us to select which rules can be applied at the
architectural level to reconfigure a system so to regain its architectural
style when it becomes compromised by unexpected run-time reconfigurations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4446</identifier>
 <datestamp>2012-12-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4446</id><created>2012-12-17</created><authors><author><keyname>Zaytsev</keyname><forenames>Vadim</forenames></author></authors><title>The Grammar Hammer of 2012</title><categories>cs.FL cs.PL</categories><comments>32 pages</comments><msc-class>68Q42, 68N15, 68T35, 68N19</msc-class><acm-class>D.3.1; F.4.2; F.4.3; I.1.2; I.2.2; I.2.4</acm-class><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This document is a case study in aggressive self-archiving. It collects all
initiatives undertaken by its author in 2012, including unpublished ones,
explains their relevance and relation with one another. Discussed topics
include guided convergence of formal grammars in a broad sense, programmable
grammar transformation operator suites, metasyntactic specifications and
methods of their manipulation, tolerant (soft computing) methods in parsing
theory, megamodelling as modelling linguistic architecture of software systems,
repositories of grammatical knowledge, open notebook computer science, as well
as the number of minor topics (new parsing algorithms, visualisation
techniques, etc). A brief overview of involved venues is also included in the
report.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4455</identifier>
 <datestamp>2015-06-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4455</id><created>2012-12-18</created><updated>2013-05-28</updated><authors><author><keyname>Toni</keyname><forenames>Laura</forenames></author><author><keyname>Maugey</keyname><forenames>Thomas</forenames></author><author><keyname>Frossard</keyname><forenames>Pascal</forenames></author></authors><title>Multi-View Video Packet Scheduling</title><categories>cs.MM cs.NI</categories><doi>10.1109/TMM.2013.2291531</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In multiview applications, multiple cameras acquire the same scene from
different viewpoints and generally produce correlated video streams. This
results in large amounts of highly redundant data. In order to save resources,
it is critical to handle properly this correlation during encoding and
transmission of the multiview data. In this work, we propose a
correlation-aware packet scheduling algorithm for multi-camera networks, where
information from all cameras are transmitted over a bottleneck channel to
clients that reconstruct the multiview images. The scheduling algorithm relies
on a new rate-distortion model that captures the importance of each view in the
scene reconstruction. We propose a problem formulation for the optimization of
the packet scheduling policies, which adapt to variations in the scene content.
Then, we design a low complexity scheduling algorithm based on a trellis search
that selects the subset of candidate packets to be transmitted towards
effective multiview reconstruction at clients. Extensive simulation results
confirm the gain of our scheduling algorithm when inter-source correlation
information is used in the scheduler, compared to scheduling policies with no
information about the correlation or non-adaptive scheduling policies. We
finally show that increasing the optimization horizon in the packet scheduling
algorithm improves the transmission performance, especially in scenarios where
the level of correlation rapidly varies with time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4458</identifier>
 <datestamp>2012-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4458</id><created>2012-12-18</created><updated>2012-12-20</updated><authors><author><keyname>Burger</keyname><forenames>Dan</forenames></author><author><keyname>Stassun</keyname><forenames>Keivan G.</forenames></author><author><keyname>Pepper</keyname><forenames>Joshua</forenames></author><author><keyname>Siverd</keyname><forenames>Robert J.</forenames></author><author><keyname>Paegert</keyname><forenames>Martin A.</forenames></author><author><keyname>De Lee</keyname><forenames>Nathan M.</forenames></author></authors><title>Filtergraph: A Flexible Web Application for Instant Data Visualization
  of Astronomy Datasets</title><categories>astro-ph.IM cs.SE</categories><comments>4 pages, 1 figure. Originally presented at the ADASS XXII Conference
  in Champaign, IL on November 6, 2012. Published in the conference proceedings
  by ASP Conference Series (revised to include URL of web application)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Filtergraph is a web application being developed by the Vanderbilt Initiative
in Data-intensive Astrophysics (VIDA) to flexibly handle a large variety of
astronomy datasets. While current datasets at Vanderbilt are being used to
search for eclipsing binaries and extrasolar planets, this system can be easily
reconfigured for a wide variety of data sources. The user loads a flat-file
dataset into Filtergraph which instantly generates an interactive data portal
that can be easily shared with others. From this portal, the user can
immediately generate scatter plots, histograms, and tables based on the
dataset. Key features of the portal include the ability to filter the data in
real time through user-specified criteria, the ability to select data by
dragging on the screen, and the ability to perform arithmetic operations on the
data in real time. The application is being optimized for speed in the context
of very large datasets: for instance, plot generated from a stellar database of
3.1 million entries render in less than 2 seconds on a standard web server
platform. This web application has been created using the Web2py web framework
based on the Python programming language. Filtergraph is freely available at
http://filtergraph.vanderbilt.edu/.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4483</identifier>
 <datestamp>2014-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4483</id><created>2012-12-18</created><authors><author><keyname>Roversi</keyname><forenames>Luca</forenames></author></authors><title>Extending a system in the calculus of structures with a self-dual
  quantifier</title><categories>cs.LO</categories><comments>29 pages</comments><doi>10.1093/logcom/exu033</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We recall that SBV, a proof system developed under the methodology of deep
inference, extends multiplicative linear logic with the self-dual
non-commutative logical operator Seq. We introduce SBVQ that extends SBV by
adding the self-dual quantifier Sdq. The system SBVQ is consistent because we
prove that (the analogous of) cut elimination holds for it. Its new logical
operator Sdq operationally behaves as a binder, in a way that the interplay
between Seq, and Sdq can model {\beta}-reduction of linear {\lambda}-calculus
inside the cut-free subsystem BVQ of SBVQ. The long term aim is to keep
developing a programme whose goal is to give pure logical accounts of
computational primitives under the proof-search-as-computation analogy, by
means of minimal, and incremental extensions of SBV.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4489</identifier>
 <datestamp>2012-12-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4489</id><created>2012-12-18</created><authors><author><keyname>Dong</keyname><forenames>Jie</forenames></author><author><keyname>Smith</keyname><forenames>David</forenames></author></authors><title>Opportunistic Relaying in Wireless Body Area Networks: Coexistence
  Performance</title><categories>cs.NI cs.PF</categories><comments>6 pages, 9 figures, ICC 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a cooperative two-hop communication scheme, together with
opportunistic relaying (OR), is applied within a mobile wireless body area
network (WBAN). Its effectiveness in interference mitigation is investigated in
a scenario where there are multiple closely-located networks. Due to a typical
WBAN's nature, no coordination is used among different WBANs. A suitable
time-division-multiple-access (TDMA) is adopted as both an intra-network and
also an inter-network access scheme. Extensive on-body and off-body channel
gain measurements are employed to gauge performance, which are overlaid to
simulate a realistic WBAN working environment. It is found that opportunistic
relaying is able to improve the signal-to-interference-and-noise ratio (SINR)
threshold value at outage probability of 10% by an average of 5 dB, and it is
also shown that it can reduce level crossing rate (LCR) significantly at a low
SINR threshold value. Furthermore, this scheme is more efficient when on-body
channels fade less slowly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4490</identifier>
 <datestamp>2012-12-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4490</id><created>2012-12-18</created><authors><author><keyname>Xie</keyname><forenames>Xiaohua</forenames></author><author><keyname>Xu</keyname><forenames>Kai</forenames></author><author><keyname>Mitra</keyname><forenames>Niloy J.</forenames></author><author><keyname>Cohen-Or</keyname><forenames>Daniel</forenames></author><author><keyname>Chen</keyname><forenames>Baoquan</forenames></author></authors><title>Sketch-to-Design: Context-based Part Assembly</title><categories>cs.GR cs.CV</categories><comments>11 pages; Executable: see project webpage</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Designing 3D objects from scratch is difficult, especially when the user
intent is fuzzy without a clear target form. In the spirit of
modeling-by-example, we facilitate design by providing reference and
inspiration from existing model contexts. We rethink model design as navigating
through different possible combinations of part assemblies based on a large
collection of pre-segmented 3D models. We propose an interactive
sketch-to-design system, where the user sketches prominent features of parts to
combine. The sketched strokes are analyzed individually and in context with the
other parts to generate relevant shape suggestions via a design gallery
interface. As the session progresses and more parts get selected, contextual
cues becomes increasingly dominant and the system quickly converges to a final
design. As a key enabler, we use pre-learned part-based contextual information
to allow the user to quickly explore different combinations of parts. Our
experiments demonstrate the effectiveness of our approach for efficiently
designing new variations from existing shapes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4507</identifier>
 <datestamp>2012-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4507</id><created>2012-12-18</created><updated>2012-12-20</updated><authors><author><keyname>Staines</keyname><forenames>Joe</forenames></author><author><keyname>Barber</keyname><forenames>David</forenames></author></authors><title>Variational Optimization</title><categories>stat.ML cs.LG cs.NA</categories><msc-class>65K10</msc-class><acm-class>G.1.6</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss a general technique that can be used to form a differentiable
bound on the optima of non-differentiable or discrete objective functions. We
form a unified description of these methods and consider under which
circumstances the bound is concave. In particular we consider two concrete
applications of the method, namely sparse learning and support vector
classification.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4522</identifier>
 <datestamp>2013-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4522</id><created>2012-12-18</created><updated>2013-09-02</updated><authors><author><keyname>Gong</keyname><forenames>Yunchao</forenames></author><author><keyname>Ke</keyname><forenames>Qifa</forenames></author><author><keyname>Isard</keyname><forenames>Michael</forenames></author><author><keyname>Lazebnik</keyname><forenames>Svetlana</forenames></author></authors><title>A Multi-View Embedding Space for Modeling Internet Images, Tags, and
  their Semantics</title><categories>cs.CV cs.IR cs.LG cs.MM</categories><comments>To Appear: International Journal of Computer Vision</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the problem of modeling Internet images and
associated text or tags for tasks such as image-to-image search, tag-to-image
search, and image-to-tag search (image annotation). We start with canonical
correlation analysis (CCA), a popular and successful approach for mapping
visual and textual features to the same latent space, and incorporate a third
view capturing high-level image semantics, represented either by a single
category or multiple non-mutually-exclusive concepts. We present two ways to
train the three-view embedding: supervised, with the third view coming from
ground-truth labels or search keywords; and unsupervised, with semantic themes
automatically obtained by clustering the tags. To ensure high accuracy for
retrieval tasks while keeping the learning process scalable, we combine
multiple strong visual features and use explicit nonlinear kernel mappings to
efficiently approximate kernel CCA. To perform retrieval, we use a specially
designed similarity function in the embedded space, which substantially
outperforms the Euclidean distance. The resulting system produces compelling
qualitative results and outperforms a number of two-view baselines on retrieval
tasks on three large-scale Internet image datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4527</identifier>
 <datestamp>2012-12-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4527</id><created>2012-12-18</created><authors><author><keyname>Wang</keyname><forenames>Quan</forenames></author></authors><title>GMM-Based Hidden Markov Random Field for Color Image and 3D Volume
  Segmentation</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this project, we first study the Gaussian-based hidden Markov random field
(HMRF) model and its expectation-maximization (EM) algorithm. Then we
generalize it to Gaussian mixture model-based hidden Markov random field. The
algorithm is implemented in MATLAB. We also apply this algorithm to color image
segmentation problems and 3D volume segmentation problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4548</identifier>
 <datestamp>2013-04-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4548</id><created>2012-12-18</created><updated>2013-04-17</updated><authors><author><keyname>Impagliazzo</keyname><forenames>Russell</forenames></author><author><keyname>Paturi</keyname><forenames>Ramamohan</forenames></author><author><keyname>Schneider</keyname><forenames>Stefan</forenames></author></authors><title>A Satisfiability Algorithm for Sparse Depth Two Threshold Circuits</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a nontrivial algorithm for the satisfiability problem for cn-wire
threshold circuits of depth two which is better than exhaustive search by a
factor 2^{sn} where s= 1/c^{O(c^2)}. We believe that this is the first
nontrivial satisfiability algorithm for cn-wire threshold circuits of depth
two. The independently interesting problem of the feasibility of sparse 0-1
integer linear programs is a special case. To our knowledge, our algorithm is
the first to achieve constant savings even for the special case of Integer
Linear Programming. The key idea is to reduce the satisfiability problem to the
Vector Domination Problem, the problem of checking whether there are two
vectors in a given collection of vectors such that one dominates the other
component-wise.
  We also provide a satisfiability algorithm with constant savings for depth
two circuits with symmetric gates where the total weighted fan-in is at most
cn.
  One of our motivations is proving strong lower bounds for TC^0 circuits,
exploiting the connection (established by Williams) between satisfiability
algorithms and lower bounds. Our second motivation is to explore the connection
between the expressive power of the circuits and the complexity of the
corresponding circuit satisfiability problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4560</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4560</id><created>2012-12-18</created><updated>2012-12-23</updated><authors><author><keyname>Pan</keyname><forenames>Victor Y.</forenames></author><author><keyname>Qian</keyname><forenames>Guoliang</forenames></author></authors><title>More on the Power of Randomized Matrix Computations</title><categories>math.NA cs.NA math.PR</categories><comments>17 pages. arXiv admin note: substantial text overlap with
  arXiv:1210.7476</comments><msc-class>15A52, 15A12, 15A06, 65F22, 65F05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A random matrix is likely to be well conditioned, and motivated by this well
known property we employ random matrix multipliers to advance some fundamental
matrix computations. This includes numerical stabilization of Gaussian
elimination with no pivoting as well as block Gaussian elimination,
approximation of the leading and trailing singular spaces of an ill conditioned
matrix, associated with its largest and smallest singular values, respectively,
and approximation of this matrix by low-rank matrices, with further extensions
to computing numerical ranks and the approximation of tensor decomposition. We
formally support the efficiency of the proposed techniques where we employ
Gaussian random multipliers, but our extensive tests have consistently produced
the same outcome where instead we used sparse and structured random
multipliers, defined by much fewer random parameters compared to the number of
their entries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4565</identifier>
 <datestamp>2012-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4565</id><created>2012-12-18</created><updated>2012-12-19</updated><authors><author><keyname>McKelvey</keyname><forenames>Karissa</forenames></author><author><keyname>Menczer</keyname><forenames>Fil</forenames></author></authors><title>Truthy: Enabling the Study of Online Social Networks</title><categories>cs.SI cs.DL physics.soc-ph</categories><comments>ACM Computer Supported Cooperative Work '13, Demonstration</comments><acm-class>H.5; H.3.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The broad adoption of online social networking platforms has made it possible
to study communication networks at an unprecedented scale. Digital trace data
can be compiled into large data sets of online discourse. However, it is a
challenge to collect, store, filter, and analyze large amounts of data, even by
experts in the computational sciences. Here we describe our recent extensions
to Truthy, a system that collects Twitter data to analyze discourse in near
real-time. We introduce several interactive visualizations and analytical tools
with the goal of enabling citizens, journalists, and researchers to understand
and study online social networks at multiple scales.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4589</identifier>
 <datestamp>2013-01-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4589</id><created>2012-12-19</created><updated>2013-01-22</updated><authors><author><keyname>Virmani</keyname><forenames>Deepali</forenames></author><author><keyname>Jain</keyname><forenames>Stabir</forenames></author></authors><title>Clustering Based Topology Control Protocol for Data Delivery in Wireless
  Sensor Networks</title><categories>cs.NI</categories><comments>12 pages, 4 figures; International Journal of Computer Science and
  Issues. January 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The issue of optimizing the limited and often non-renewable energy of sensor
nodes due to its direct impact on network lifetime dominates every aspect of
wireless sensor networks. Existing techniques for optimizing energy consumption
are based on exploiting node redundancy, adaptive radio transmission power and
topology control. Topology control protocols have a significant impact on
network lifetime, available energy and connectivity. In this paper we classify
sensor nodes as strong and weak nodes based on their residual energy as well as
operational lifetime and propose a Clustering based topology control protocol
(CTCP) which extends network lifetime while guarantying the minimum
connectivity. Extensive simulations in Java-Simulator (J-Sim) show that our
proposed protocol outperforms the existing protocols in terms of various
performance metrics life network lifetime, average delay and minimizes energy
utilization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4590</identifier>
 <datestamp>2012-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4590</id><created>2012-12-19</created><authors><author><keyname>Pommaret</keyname><forenames>Jean-Fran&#xe7;ois</forenames><affiliation>CERMICS</affiliation></author></authors><title>Relative parametrization of linear multidimensional systems</title><categories>math.AP cs.SC math.AC math.DG math.RA</categories><comments>Presented for publication in the Springer journal
  MSSP:Multidimensional Systems and Signal Processing</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the last chapter of his book &quot;The Algebraic Theory of Modular Systems &quot;
published in 1916, F. S. Macaulay developped specific techniques for dealing
with &quot; unmixed polynomial ideals &quot; by introducing what he called &quot; inverse
systems &quot;. The purpose of this paper is to extend such a point of view to
differential modules defined by linear multidimensional systems, that is by
linear systems of ordinary differential (OD) or partial differential (PD)
equations of any order, with any number of independent variables, any number of
unknowns and even with variable coefficients in a differential field. The first
and main idea is to replace unmixed polynomial ideals by &quot; pure differential
modules &quot;. The second idea is to notice that a module is 0-pure if and only if
it is torsion-free and thus if and only if it admits an &quot; absolute
parametrization &quot; by means of arbitrary potential like functions, or,
equivalently, if it can be embedded into a free module by means of an &quot;
absolute localization &quot;. The third idea is to refer to a difficult theorem of
algebraic analysis saying that an r-pure module can be embedded into a module
of projective dimension equal to r, that is a module admitting a projective
resolution with exactly r operators. The fourth and final idea is to establish
a link between the use of extension modules for such a purpose and specific
formal properties of the underlying multidimensional system through the use of
involution and a &quot;relative localization &quot; leading to a &quot;relative
parametrization &quot;, that is to the use of potential-like functions satisfying a
kind of &quot;minimum differential constraint &quot; limiting, in some sense, the number
of independent variables appearing in these functions, in a way similar to the
situation met in the Cartan-K\&quot;ahler theorem of analysis. The paper is written
in a rather effective self-contained way and we provide many explicit examples
that should become test examples for a future use of computer algebra.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4608</identifier>
 <datestamp>2012-12-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4608</id><created>2012-12-19</created><authors><author><keyname>Premachandran</keyname><forenames>Vittal</forenames></author><author><keyname>Kakarala</keyname><forenames>Ramakrishna</forenames></author></authors><title>Perceptually Motivated Shape Context Which Uses Shape Interiors</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we identify some of the limitations of current-day shape
matching techniques. We provide examples of how contour-based shape matching
techniques cannot provide a good match for certain visually similar shapes. To
overcome this limitation, we propose a perceptually motivated variant of the
well-known shape context descriptor. We identify that the interior properties
of the shape play an important role in object recognition and develop a
descriptor that captures these interior properties. We show that our method can
easily be augmented with any other shape matching algorithm. We also show from
our experiments that the use of our descriptor can significantly improve the
retrieval rates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4613</identifier>
 <datestamp>2013-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4613</id><created>2012-12-19</created><updated>2013-01-13</updated><authors><author><keyname>Gagie</keyname><forenames>Travis</forenames></author><author><keyname>Hon</keyname><forenames>Wing-Kai</forenames></author><author><keyname>Ku</keyname><forenames>Tsung-Han</forenames></author></authors><title>New Algorithms for Position Heaps</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present several results about position heaps, a relatively new alternative
to suffix trees and suffix arrays. First, we show that, if we limit the maximum
length of patterns to be sought, then we can also limit the height of the heap
and reduce the worst-case cost of insertions and deletions. Second, we show how
to build a position heap in linear time independent of the size of the
alphabet. Third, we show how to augment a position heap such that it supports
access to the corresponding suffix array, and vice versa. Fourth, we introduce
a variant of a position heap that can be simulated efficiently by a compressed
suffix array with a linear number of extra bits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4617</identifier>
 <datestamp>2013-04-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4617</id><created>2012-12-19</created><updated>2013-04-11</updated><authors><author><keyname>Vidyullatha</keyname><forenames>K.</forenames></author><author><keyname>Tejaswi</keyname><forenames>S. V. N. L.</forenames></author><author><keyname>Harish</keyname><forenames>V.</forenames></author><author><keyname>Kumar</keyname><forenames>T. Anil</forenames></author></authors><title>Improved Multiuser Detection in Asynchronous Flat-Fading Non-Gaussian
  Channels</title><categories>cs.SY cs.NI</categories><comments>Appeared in ASID IEEE conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a new M-estimator based multiuser detection in asynchronous
flat-fading non-Gaussian CDMA channels is considered. A new closed-form
expression is derived for the characteristic function of the multiple-access
interference signals. Simulation results are provided to prove the
effectiveness of the derived bit-error probabilities obtained with this
expression in asynchronous flat-fading non-Gaussian CDMA channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4626</identifier>
 <datestamp>2012-12-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4626</id><created>2012-12-19</created><authors><author><keyname>Dikstein</keyname><forenames>Lior</forenames></author><author><keyname>Permuter</keyname><forenames>Haim H.</forenames></author><author><keyname>Shamai</keyname><forenames>Shlomo</forenames></author></authors><title>MAC with Action-Dependent State Information at One Encoder</title><categories>cs.IT math.IT</categories><comments>1. Parts of this paper appeared in the IEEE International Symposium
  on Information Theory (ISIT 2012),Cambridge, MA, US, July 2012 and at the
  IEEE 27th Convention of Electrical and Electronics Engineers in Israel (IEEEI
  2012), Nov. 2012. 2. This work has been supported by the CORNET Consortium
  Israel Ministry for Industry and Commerce</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Problems dealing with the ability to take an action that affects the states
of state-dependent communication channels are of timely interest and
importance. Therefore, we extend the study of action-dependent channels, which
until now focused on point-to-point models, to multiple-access channels (MAC).
In this paper, we consider a two-user, state-dependent MAC, in which one of the
encoders, called the informed encoder, is allowed to take an action that
affects the formation of the channel states. Two independent messages are to be
sent through the channel: a common message known to both encoders and a private
message known only to the informed encoder. In addition, the informed encoder
has access to the sequence of channel states in a non-causal manner. Our
framework generalizes previously evaluated settings of state dependent
point-to-point channels with actions and MACs with common messages. We derive a
single letter characterization of the capacity region for this setting. Using
this general result, we obtain and compute the capacity region for the Gaussian
action-dependent MAC. The unique methods used in solving the Gaussian case are
then applied to obtain the capacity of the Gaussian action-dependent
point-to-point channel; a problem was left open until this work. Finally, we
establish some dualities between action-dependent channel coding and source
coding problems. Specifically, we obtain a duality between the considered MAC
setting and the rate distortion model known as &quot;Successive Refinement with
Actions&quot;. This is done by developing a set of simple duality principles that
enable us to successfully evaluate the outcome of one problem given the other.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4638</identifier>
 <datestamp>2013-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4638</id><created>2012-12-19</created><updated>2013-01-25</updated><authors><author><keyname>Caullery</keyname><forenames>Florian</forenames></author></authors><title>Polynomial functions of degree 20 which are APN infinitely often</title><categories>cs.IT cs.CR math.IT</categories><comments>arXiv admin note: text overlap with arXiv:1101.6033 by other authors</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give all the polynomials functions of degree 20 which are APN over an
infinity of field extensions and show they are all CCZ-equivalent to the
function $x^5$, which is a new step in proving the conjecture of Aubry, McGuire
and Rodier.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4648</identifier>
 <datestamp>2013-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4648</id><created>2012-12-19</created><authors><author><keyname>Krivulin</keyname><forenames>Nikolai K.</forenames></author></authors><title>Algebraic modelling and performance evaluation of acyclic fork-join
  queueing networks</title><categories>math.OC cs.SY</categories><comments>19 pages, 2 figures, 3 tables</comments><msc-class>90B15 (Primary) 15A80, 90B22, 93C65, 68M20 (Secondary)</msc-class><journal-ref>Advances in Stochastic Simulation Methods, 2000, pp. 63-81</journal-ref><doi>10.1007/978-1-4612-1318-5_5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Simple lower and upper bounds on mean cycle time in stochastic acyclic
fork-join queueing networks are derived using a (max,+)-algebra based
representation of network dynamics. The behaviour of the bounds under various
assumptions concerning the service times in the networks is discussed, and
related numerical examples are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4649</identifier>
 <datestamp>2012-12-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4649</id><created>2012-12-19</created><authors><author><keyname>Merhav</keyname><forenames>Neri</forenames></author></authors><title>Exponential error bounds on parameter modulation-estimation for discrete
  memoryless channels</title><categories>cs.IT math.IT</categories><comments>23 pages; 1 figure; submitted to IEEE Trans. on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of modulation and estimation of a random parameter
$U$ to be conveyed across a discrete memoryless channel. Upper and lower bounds
are derived for the best achievable exponential decay rate of a general moment
of the estimation error, $\bE|\hat{U}-U|^\rho$, $\rho\ge 0$, when both the
modulator and the estimator are subjected to optimization. These exponential
error bounds turn out to be intimately related to error exponents of channel
coding and to channel capacity. While in general, there is some gap between the
upper and the lower bound, they asymptotically coincide both for very small and
for very large values of the moment power $\rho$. This means that our
achievability scheme, which is based on simple quantization of $U$ followed by
channel coding, is nearly optimum in both limits. Some additional properties of
the bounds are discussed and demonstrated, and finally, an extension to the
case of a multidimensional parameter vector is outlined, with the principal
conclusion that our upper and lower bound asymptotically coincide also for a
high dimensionality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4650</identifier>
 <datestamp>2014-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4650</id><created>2012-12-19</created><updated>2013-06-21</updated><authors><author><keyname>Gurfinkel</keyname><forenames>Arie</forenames></author><author><keyname>Rollini</keyname><forenames>Simone Fulvio</forenames></author><author><keyname>Sharygina</keyname><forenames>Natasha</forenames></author></authors><title>Interpolation Properties and SAT-based Model Checking</title><categories>cs.LO</categories><doi>10.1007/978-3-319-02444-8_19</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Craig interpolation is a widespread method in verification, with important
applications such as Predicate Abstraction, CounterExample Guided Abstraction
Refinement and Lazy Abstraction With Interpolants. Most state-of-the-art model
checking techniques based on interpolation require collections of interpolants
to satisfy particular properties, to which we refer as &quot;collectives&quot;; they do
not hold in general for all interpolation systems and have to be established
for each particular system and verification environment. Nevertheless, no
systematic approach exists that correlates the individual interpolation systems
and compares the necessary collectives. This paper proposes a uniform
framework, which encompasses (and generalizes) the most common collectives
exploited in verification. We use it for a systematic study of the collectives
and of the constraints they pose on propositional interpolation systems used in
SAT-based model checking.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4653</identifier>
 <datestamp>2013-08-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4653</id><created>2012-12-19</created><updated>2013-08-09</updated><authors><author><keyname>La Guardia</keyname><forenames>Giuliano G.</forenames></author></authors><title>Convolutional Codes Derived From Group Character Codes</title><categories>cs.IT math.IT quant-ph</categories><comments>Accepted for publication in Discrete Math</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  New families of unit memory as well as multi-memory convolutional codes are
constructed algebraically in this paper. These convolutional codes are derived
from the class of group character codes. The proposed codes have basic
generator matrices, consequently, they are non catastrophic. Additionally, the
new code parameters are better than the ones available in the literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4654</identifier>
 <datestamp>2013-11-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4654</id><created>2012-12-19</created><updated>2013-11-05</updated><authors><author><keyname>La Guardia</keyname><forenames>Giuliano G.</forenames></author></authors><title>On Classical and Quantum MDS-Convolutional BCH Codes</title><categories>quant-ph cs.IT math.IT</categories><comments>Accepted for publication in IEEE-Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Several new families of multi-memory classical convolutional
Bose-Chaudhuri-Hocquenghem (BCH) codes as well as families of unit-memory
quantum convolutional codes are constructed in this paper. Our unit-memory
classical and quantum convolutional codes are optimal in the sense that they
attain the classical (quantum) generalized Singleton bound. The constructions
presented in this paper are performed algebraically and not by computational
search.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4658</identifier>
 <datestamp>2012-12-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4658</id><created>2012-12-19</created><authors><author><keyname>Stalio</keyname><forenames>Stefano</forenames></author><author><keyname>Di Carlo</keyname><forenames>Giuseppe</forenames></author><author><keyname>Parlati</keyname><forenames>Sandra</forenames></author><author><keyname>Spinnato</keyname><forenames>Piero</forenames></author></authors><title>Resource management on a VM based computer cluster for scientific
  computing</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the last ten years host virtualization has brought a revolution in the way
almost every activity related to information technology is thought of and
performed. The use of virtualization for HPC and HTC computing, while eagerly
desired, has probably been one of the last steps of this revolution, the
performance loss due to the hardware abstraction layer being the cause that
slowed down a process that has been much faster in other fields. Nowadays the
widespread diffusion of virtualization and of new virtualization techniques
seem to have helped breaking this last barrier and virtual host computing
infrastructures for HPC and HTC are found in many data centers. In this
document the approach adopted at the INFN &quot;Laboratori Nazionali del Gran Sasso&quot;
for providing computational resources via a virtual host based computing
facility is described. Particular evidence is given to the storage layout, to
the middleware architecture and to resource allocation strategies, as these are
issues for which a personalized solution was adopted. Other aspects may be
covered in the future within other documents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4663</identifier>
 <datestamp>2015-02-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4663</id><created>2012-12-19</created><updated>2015-02-24</updated><authors><author><keyname>Raginsky</keyname><forenames>Maxim</forenames></author><author><keyname>Sason</keyname><forenames>Igal</forenames></author></authors><title>Concentration of Measure Inequalities in Information Theory,
  Communications and Coding (Second Edition)</title><categories>cs.IT math.IT math.PR</categories><comments>Foundations and Trends in Communications and Information Theory, vol.
  10, no 1-2, pp. 1-248, 2013. Second edition was published in October 2014.
  ISBN to printed book: 978-1-60198-906-2</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  During the last two decades, concentration inequalities have been the subject
of exciting developments in various areas, including convex geometry,
functional analysis, statistical physics, high-dimensional statistics, pure and
applied probability theory, information theory, theoretical computer science,
and learning theory. This monograph focuses on some of the key modern
mathematical tools that are used for the derivation of concentration
inequalities, on their links to information theory, and on their various
applications to communications and coding. In addition to being a survey, this
monograph also includes various new recent results derived by the authors. The
first part of the monograph introduces classical concentration inequalities for
martingales, as well as some recent refinements and extensions. The power and
versatility of the martingale approach is exemplified in the context of codes
defined on graphs and iterative decoding algorithms, as well as codes for
wireless communication. The second part of the monograph introduces the entropy
method, an information-theoretic technique for deriving concentration
inequalities. The basic ingredients of the entropy method are discussed first
in the context of logarithmic Sobolev inequalities, which underlie the
so-called functional approach to concentration of measure, and then from a
complementary information-theoretic viewpoint based on transportation-cost
inequalities and probability in metric spaces. Some representative results on
concentration for dependent random variables are briefly summarized, with
emphasis on their connections to the entropy method. Finally, we discuss
several applications of the entropy method to problems in communications and
coding, including strong converses, empirical distributions of good channel
codes, and an information-theoretic converse for concentration of measure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4669</identifier>
 <datestamp>2012-12-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4669</id><created>2012-12-19</created><authors><author><keyname>Roversi</keyname><forenames>Luca</forenames></author></authors><title>Communication, and concurrency with logic-based restriction inside a
  calculus of structures</title><categories>cs.LO</categories><comments>32 pages. Submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is well known that we can use structural proof theory to refine, or
generalize, existing paradigmatic computational primitives, or to discover new
ones. Under such a point of view we keep developing a programme whose goal is
establishing a correspondence between proof-search of a logical system and
computations in a process algebra. We give a purely logical account of a
process algebra operation which strictly includes the behavior of restriction
on actions we find in Milner CCS. This is possible inside a logical system in
the Calculus of Structures of Deep Inference endowed with a self-dual
quantifier. Using proof-search of cut-free proofs of such a logical system we
show how to solve reachability problems in a process algebra that subsumes a
significant fragment of Milner CCS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4670</identifier>
 <datestamp>2012-12-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4670</id><created>2012-12-19</created><authors><author><keyname>&#x17d;erovnik</keyname><forenames>Janez</forenames></author><author><keyname>Erve&#x161;</keyname><forenames>Rija</forenames></author></authors><title>Improved upper bounds for vertex and edge fault diameters of Cartesian
  graph bundles</title><categories>math.CO cs.DM</categories><comments>arXiv admin note: substantial text overlap with arXiv:1002.2508</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mixed fault diameter of a graph $G$, $ \D_{(a,b)}(G)$, is the maximal
diameter of $G$ after deletion of any $a$ vertices and any $b$ edges. Special
cases are the (vertex) fault diameter $\D^V_{a} = \D_{(a,0)}$ and the edge
fault diameter $\D^E_{a} = \D_{(0,a)}$. Let $G$ be a Cartesian graph bundle
with fibre $F$ over the base graph $B$. We show that (1) $\D^V_{a+b+1}(G)\leq
\D^V_{a}(F)+\D^V_{b}(B)$ when the graphs $F$ and $B$ are $k_F$-connected and
$k_B$-connected, $0&lt; a &lt; k_F$, $0&lt; b &lt; k_B$, and provided that
$\D_{(a-1,1)}(F)\leq \D^{V}_{a} (F)$ and $\D_{(b-1,1)}(B)\leq \D^{V}_{b} (B)$
and (2) $\D^E_{a+b+1}(G)\leq \D^E_{a}(F)+\D^E_{b}(B)$ when the graphs $F$ and
$B$ are $k_F$-edge connected and $k_B$-edge connected, $0\leq a &lt; k_F$, $0\leq
b &lt; k_B$, and provided that $\D^E_{a}(F)\geq 2$ and $\D^E_{b}(B)\geq 2$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4674</identifier>
 <datestamp>2012-12-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4674</id><created>2012-12-19</created><authors><author><keyname>Kong</keyname><forenames>Hyeok</forenames></author></authors><title>Natural Language Understanding Based on Semantic Relations between
  Sentences</title><categories>cs.CL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we define event expression over sentences of natural language
and semantic relations between events. Based on this definition, we formally
consider text understanding process having events as basic unit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4675</identifier>
 <datestamp>2012-12-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4675</id><created>2012-12-18</created><authors><author><keyname>Han</keyname><forenames>Yufei</forenames><affiliation>INRIA Rocquencourt</affiliation></author><author><keyname>Moutarde</keyname><forenames>Fabien</forenames><affiliation>CAOR</affiliation></author></authors><title>Analysis of Large-scale Traffic Dynamics using Non-negative Tensor
  Factorization</title><categories>cs.LG</categories><comments>ITS World Congress 2012 (2012)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present our work on clustering and prediction of temporal
dynamics of global congestion configurations in large-scale road networks.
Instead of looking into temporal traffic state variation of individual links,
or of small areas, we focus on spatial congestion configurations of the whole
network. In our work, we aim at describing the typical temporal dynamic
patterns of this network-level traffic state and achieving long-term prediction
of the large-scale traffic dynamics, in a unified data-mining framework. To
this end, we formulate this joint task using Non-negative Tensor Factorization
(NTF), which has been shown to be a useful decomposition tools for multivariate
data sequences. Clustering and prediction are performed based on the compact
tensor factorization results. Experiments on large-scale simulated data
illustrate the interest of our method with promising results for long-term
forecast of traffic evolution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4692</identifier>
 <datestamp>2012-12-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4692</id><created>2012-12-19</created><authors><author><keyname>Koundinya</keyname><forenames>Anjan K.</forenames></author><author><keyname>K.</keyname><forenames>Srinath N.</forenames></author><author><keyname>Sharma</keyname><forenames>K. A. K.</forenames></author><author><keyname>Kumar</keyname><forenames>Kiran</forenames></author><author><keyname>N.</keyname><forenames>Madhu M.</forenames></author><author><keyname>Shanbag</keyname><forenames>Kiran U.</forenames></author></authors><title>Map / Reduce Deisgn and Implementation of Apriori Alogirthm for handling
  voluminous data-sets</title><categories>cs.DC</categories><comments>11 pages, 5 figures; Advanced Computing: An International Journal
  (ACIJ), Vol.3, No.6, November 2012</comments><doi>10.5121/acij.2012.3604</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Apriori is one of the key algorithms to generate frequent itemsets. Analyzing
frequent itemset is a crucial step in analysing structured data and in finding
association relationship between items. This stands as an elementary foundation
to supervised learning, which encompasses classifier and feature extraction
methods. Applying this algorithm is crucial to understand the behaviour of
structured data. Most of the structured data in scientific domain are
voluminous. Processing such kind of data requires state of the art computing
machines. Setting up such an infrastructure is expensive. Hence a distributed
environment such as a clustered setup is employed for tackling such scenarios.
Apache Hadoop distribution is one of the cluster frameworks in distributed
environment that helps by distributing voluminous data across a number of nodes
in the framework. This paper focuses on map/reduce design and implementation of
Apriori algorithm for structured data analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4702</identifier>
 <datestamp>2012-12-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4702</id><created>2012-12-19</created><authors><author><keyname>Nasution</keyname><forenames>Mahyuddin K. M.</forenames></author></authors><title>Simple Search Engine Model: Adaptive Properties for Doubleton</title><categories>cs.IR cs.DM</categories><comments>5 pages, nothing, a draf</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the relationship between query and search engine by
exploring the adaptive properties for doubleton as a space of event based on a
simple search engine. We employ set theory for defining doubleton and generate
some properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4703</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4703</id><created>2012-12-19</created><updated>2014-02-14</updated><authors><author><keyname>Michel</keyname><forenames>Lo&#xef;c</forenames></author></authors><title>Semi-explicit Parareal method based on convergence acceleration
  technique</title><categories>cs.SY math.CA math.NA</categories><comments>18 pages, 3 figures</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  The Parareal algorithm is used to solve time-dependent problems considering
multiple solvers that may work in parallel. The key feature is a initial rough
approximation of the solution that is iteratively refined by the parallel
solvers. We report a derivation of the Parareal method that uses a convergence
acceleration technique to improve the accuracy of the solution. Our approach
uses firstly an explicit ODE solver to perform the parallel computations with
different time-steps and then, a decomposition of the solution into specific
convergent series, based on an extrapolation method, allows to refine the
precision of the solution. Our proposed method exploits basic explicit
integration methods, such as for example the explicit Euler scheme, in order to
preserve the simplicity of the global parallel algorithm. The first part of the
paper outlines the proposed method applied to the simple explicit Euler scheme
and then the derivation of the classical Parareal algorithm is discussed and
illustrated with numerical examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4717</identifier>
 <datestamp>2014-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4717</id><created>2012-12-19</created><updated>2014-12-02</updated><authors><author><keyname>Bayraktar</keyname><forenames>Erhan</forenames></author><author><keyname>Kravitz</keyname><forenames>Ross</forenames></author></authors><title>Quickest Detection with Discretely Controlled Observations</title><categories>math.PR cs.IT cs.SY math.IT math.OC</categories><comments>Final version. To appear in Sequential Analysis. Keywords: Bayesian
  changepoint detection; Continuous time; Finitely many sampling rights. 52
  pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a continuous time Bayesian quickest detection problem in which
observation times are a scarce resource. The agent, limited to making a finite
number of discrete observations, must adaptively decide his observation
strategy to minimize detection delay and the probability of false alarm. Under
two different models of observation rights, we establish the existence of
optimal strategies, and formulate an algorithmic approach to the problem via
jump operators. We describe algorithms for these problems, and illustrate them
with some numerical results. As the number of observation rights tends to
infinity, we also show convergence to the classical continuous observation
problem of Shiryaev.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4720</identifier>
 <datestamp>2013-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4720</id><created>2012-12-19</created><updated>2013-03-16</updated><authors><author><keyname>Deza</keyname><forenames>Antoine</forenames></author><author><keyname>Meunier</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames></author><author><keyname>Sarrabezolles</keyname><forenames>Pauline</forenames></author></authors><title>A combinatorial approach to colourful simplicial depth</title><categories>math.CO cs.CG cs.DM</categories><comments>17 pages, 4 figures</comments><msc-class>05C65 (Primary) 52C45, 52A35 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The colourful simplicial depth conjecture states that any point in the convex
hull of each of d+1 sets, or colours, of d+1 points in general position in R^d
is contained in at least d^2+1 simplices with one vertex from each set. We
verify the conjecture in dimension 4 and strengthen the known lower bounds in
higher dimensions. These results are obtained using a combinatorial
generalization of colourful point configurations called octahedral systems. We
present properties of octahedral systems generalizing earlier results on
colourful point configurations and exhibit an octahedral system which can not
arise from a colourful point configuration. The number of octahedral systems is
also given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4731</identifier>
 <datestamp>2013-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4731</id><created>2012-12-19</created><updated>2013-04-29</updated><authors><author><keyname>Chevillard</keyname><forenames>Sylvain</forenames><affiliation>INRIA Sophia Antipolis</affiliation></author><author><keyname>Mezzarobba</keyname><forenames>Marc</forenames><affiliation>Inria Grenoble Rh&#xf4;ne-Alpes / LIP Laboratoire de l'Informatique du Parall&#xe9;lisme</affiliation></author></authors><title>Multiple precision evaluation of the Airy Ai function with reduced
  cancellation</title><categories>cs.SC</categories><proxy>ccsd</proxy><journal-ref>21st IEEE Symposium on Computer Arithmetic (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The series expansion at the origin of the Airy function Ai(x) is alternating
and hence problematic to evaluate for x &gt; 0 due to cancellation. Based on a
method recently proposed by Gawronski, M\&quot;uller, and Reinhard, we exhibit two
functions F and G, both with nonnegative Taylor expansions at the origin, such
that Ai(x) = G(x)/F(x). The sums are now well-conditioned, but the Taylor
coefficients of G turn out to obey an ill-conditioned three-term recurrence. We
use the classical Miller algorithm to overcome this issue. We bound all errors
and our implementation allows an arbitrary and certified accuracy, that can be
used, e.g., for providing correct rounding in arbitrary precision.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4751</identifier>
 <datestamp>2012-12-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4751</id><created>2012-12-19</created><authors><author><keyname>Krause</keyname><forenames>Sebastian M.</forenames></author><author><keyname>Bornholdt</keyname><forenames>Stefan</forenames></author></authors><title>Opinion formation model for markets with a social temperature and fear</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI q-fin.GN</categories><comments>8 pages, 9 figures</comments><journal-ref>Phys. Rev. E 86, 056106 (2012)</journal-ref><doi>10.1103/PhysRevE.86.056106</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the spirit of behavioral finance, we study the process of opinion
formation among investors using a variant of the 2D Voter Model with a tunable
social temperature. Further, a feedback acting on the temperature is
introduced, such that social temperature reacts to market imbalances and thus
becomes time dependent. In this toy market model, social temperature represents
nervousness of agents towards market imbalances representing speculative risk.
We use the knowledge about the discontinuous Generalized Voter Model phase
transition to determine critical fixed points. The system exhibits metastable
phases around these fixed points characterized by structured lattice states,
with intermittent excursions away from the fixed points. The statistical
mechanics of the model is characterized and its relation to dynamics of opinion
formation among investors in real markets is discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4756</identifier>
 <datestamp>2012-12-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4756</id><created>2012-12-19</created><authors><author><keyname>Demaine</keyname><forenames>Erik D.</forenames></author><author><keyname>Demaine</keyname><forenames>Martin L.</forenames></author><author><keyname>Fekete</keyname><forenames>S&#xe1;ndor P.</forenames></author><author><keyname>Patitz</keyname><forenames>Matthew J.</forenames></author><author><keyname>Schweller</keyname><forenames>Robert T.</forenames></author><author><keyname>Winslow</keyname><forenames>Andrew</forenames></author><author><keyname>Woods</keyname><forenames>Damien</forenames></author></authors><title>One Tile to Rule Them All: Simulating Any Turing Machine, Tile Assembly
  System, or Tiling System with a Single Puzzle Piece</title><categories>cs.DS cs.CC cs.CG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we explore the power of tile self-assembly models that extend
the well-studied abstract Tile Assembly Model (aTAM) by permitting tiles of
shapes beyond unit squares. Our main result shows the surprising fact that any
aTAM system, consisting of many different tile types, can be simulated by a
single tile type of a general shape. As a consequence, we obtain a single
universal tile type of a single (constant-size) shape that serves as a
&quot;universal tile machine&quot;: the single universal tile type can simulate any
desired aTAM system when given a single seed assembly that encodes the desired
aTAM system. We also show how to adapt this result to convert any of a variety
of plane tiling systems (such as Wang tiles) into a &quot;nearly&quot; plane tiling
system with a single tile (but with small gaps between the tiles). All of these
results rely on the ability to both rotate and translate tiles; by contrast, we
show that a single nonrotatable tile, of arbitrary shape, can produce
assemblies which either grow infinitely or cannot grow at all, implying
drastically limited computational power.
  On the positive side, we show how to simulate arbitrary cellular automata for
a limited number of steps using a single nonrotatable tile and a linear-size
seed assembly.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4771</identifier>
 <datestamp>2012-12-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4771</id><created>2012-12-19</created><authors><author><keyname>Bremner</keyname><forenames>David</forenames></author><author><keyname>Chan</keyname><forenames>Timothy M.</forenames></author><author><keyname>Demaine</keyname><forenames>Erik D.</forenames></author><author><keyname>Erickson</keyname><forenames>Jeff</forenames></author><author><keyname>Hurtado</keyname><forenames>Ferran</forenames></author><author><keyname>Iacono</keyname><forenames>John</forenames></author><author><keyname>Langerman</keyname><forenames>Stefan</forenames></author><author><keyname>Patrascu</keyname><forenames>Mihai</forenames></author><author><keyname>Taslakian</keyname><forenames>Perouz</forenames></author></authors><title>Necklaces, Convolutions, and X+Y</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give subquadratic algorithms that, given two necklaces each with n beads
at arbitrary positions, compute the optimal rotation of the necklaces to best
align the beads. Here alignment is measured according to the p norm of the
vector of distances between pairs of beads from opposite necklaces in the best
perfect matching. We show surprisingly different results for p = 1, p even, and
p = \infty. For p even, we reduce the problem to standard convolution, while
for p = \infty and p = 1, we reduce the problem to (min, +) convolution and
(median, +) convolution. Then we solve the latter two convolution problems in
subquadratic time, which are interesting results in their own right. These
results shed some light on the classic sorting X + Y problem, because the
convolutions can be viewed as computing order statistics on the antidiagonals
of the X + Y matrix. All of our algorithms run in o(n^2) time, whereas the
obvious algorithms for these problems run in \Theta(n^2) time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4775</identifier>
 <datestamp>2013-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4775</id><created>2012-12-19</created><updated>2013-01-04</updated><authors><author><keyname>Frank</keyname><forenames>Mario</forenames></author><author><keyname>Buhmann</keyname><forenames>Joachim M.</forenames></author><author><keyname>Basin</keyname><forenames>David</forenames></author></authors><title>Role Mining with Probabilistic Models</title><categories>cs.CR cs.LG stat.ML</categories><comments>accepted for publication at ACM Transactions on Information and
  System Security (TISSEC)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Role mining tackles the problem of finding a role-based access control (RBAC)
configuration, given an access-control matrix assigning users to access
permissions as input. Most role mining approaches work by constructing a large
set of candidate roles and use a greedy selection strategy to iteratively pick
a small subset such that the differences between the resulting RBAC
configuration and the access control matrix are minimized. In this paper, we
advocate an alternative approach that recasts role mining as an inference
problem rather than a lossy compression problem. Instead of using combinatorial
algorithms to minimize the number of roles needed to represent the
access-control matrix, we derive probabilistic models to learn the RBAC
configuration that most likely underlies the given matrix.
  Our models are generative in that they reflect the way that permissions are
assigned to users in a given RBAC configuration. We additionally model how
user-permission assignments that conflict with an RBAC configuration emerge and
we investigate the influence of constraints on role hierarchies and on the
number of assignments. In experiments with access-control matrices from
real-world enterprises, we compare our proposed models with other role mining
methods. Our results show that our probabilistic models infer roles that
generalize well to new system users for a wide variety of data, while other
models' generalization abilities depend on the dataset given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4777</identifier>
 <datestamp>2012-12-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4777</id><created>2012-12-19</created><authors><author><keyname>Arora</keyname><forenames>Sanjeev</forenames></author><author><keyname>Ge</keyname><forenames>Rong</forenames></author><author><keyname>Halpern</keyname><forenames>Yoni</forenames></author><author><keyname>Mimno</keyname><forenames>David</forenames></author><author><keyname>Moitra</keyname><forenames>Ankur</forenames></author><author><keyname>Sontag</keyname><forenames>David</forenames></author><author><keyname>Wu</keyname><forenames>Yichen</forenames></author><author><keyname>Zhu</keyname><forenames>Michael</forenames></author></authors><title>A Practical Algorithm for Topic Modeling with Provable Guarantees</title><categories>cs.LG cs.DS stat.ML</categories><comments>26 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Topic models provide a useful method for dimensionality reduction and
exploratory data analysis in large text corpora. Most approaches to topic model
inference have been based on a maximum likelihood objective. Efficient
algorithms exist that approximate this objective, but they have no provable
guarantees. Recently, algorithms have been introduced that provide provable
bounds, but these algorithms are not practical because they are inefficient and
not robust to violations of model assumptions. In this paper we present an
algorithm for topic model inference that is both provable and practical. The
algorithm produces results comparable to the best MCMC implementations while
running orders of magnitude faster.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4779</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4779</id><created>2012-12-19</created><updated>2014-02-17</updated><authors><author><keyname>Cheng</keyname><forenames>Suqi</forenames></author><author><keyname>Shen</keyname><forenames>Huawei</forenames></author><author><keyname>Huang</keyname><forenames>Junming</forenames></author><author><keyname>Zhang</keyname><forenames>Guoqing</forenames></author><author><keyname>Cheng</keyname><forenames>Xueqi</forenames></author></authors><title>StaticGreedy: solving the scalability-accuracy dilemma in influence
  maximization</title><categories>cs.SI cs.DS physics.soc-ph</categories><comments>10 pages, 8 figures, this paper has been published in the proceedings
  of CIKM2013</comments><acm-class>F.2.2; D.2.8</acm-class><doi>10.1145/2505515.2505541</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Influence maximization, defined as a problem of finding a set of seed nodes
to trigger a maximized spread of influence, is crucial to viral marketing on
social networks. For practical viral marketing on large scale social networks,
it is required that influence maximization algorithms should have both
guaranteed accuracy and high scalability. However, existing algorithms suffer a
scalability-accuracy dilemma: conventional greedy algorithms guarantee the
accuracy with expensive computation, while the scalable heuristic algorithms
suffer from unstable accuracy.
  In this paper, we focus on solving this scalability-accuracy dilemma. We
point out that the essential reason of the dilemma is the surprising fact that
the submodularity, a key requirement of the objective function for a greedy
algorithm to approximate the optimum, is not guaranteed in all conventional
greedy algorithms in the literature of influence maximization. Therefore a
greedy algorithm has to afford a huge number of Monte Carlo simulations to
reduce the pain caused by unguaranteed submodularity. Motivated by this
critical finding, we propose a static greedy algorithm, named StaticGreedy, to
strictly guarantee the submodularity of influence spread function during the
seed selection process. The proposed algorithm makes the computational expense
dramatically reduced by two orders of magnitude without loss of accuracy.
Moreover, we propose a dynamical update strategy which can speed up the
StaticGreedy algorithm by 2-7 times on large scale social networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4784</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4784</id><created>2012-12-19</created><updated>2013-03-17</updated><authors><author><keyname>Campos</keyname><forenames>I.</forenames></author><author><keyname>del Castillo</keyname><forenames>E. Fernandez</forenames></author><author><keyname>Heinemeyer</keyname><forenames>S.</forenames></author><author><keyname>Lopez-Garcia</keyname><forenames>A.</forenames></author><author><keyname>Pahlen</keyname><forenames>F. v. d.</forenames></author></authors><title>Phenomenology Tools on Cloud Infrastructures using OpenStack</title><categories>cs.DC hep-lat hep-ph</categories><comments>25 pages, 12 figures; information on memory usage included, as well
  as minor modifications. Version to appear in EPJC</comments><doi>10.1140/epjc/s10052-013-2375-0</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new environment for computations in particle physics
phenomenology employing recent developments in cloud computing. On this
environment users can create and manage &quot;virtual&quot; machines on which the
phenomenology codes/tools can be deployed easily in an automated way. We
analyze the performance of this environment based on &quot;virtual&quot; machines versus
the utilization of &quot;real&quot; physical hardware. In this way we provide a
qualitative result for the influence of the host operating system on the
performance of a representative set of applications for phenomenology
calculations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4788</identifier>
 <datestamp>2012-12-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4788</id><created>2012-12-19</created><authors><author><keyname>Grimm</keyname><forenames>Dominik</forenames></author><author><keyname>Greshake</keyname><forenames>Bastian</forenames></author><author><keyname>Kleeberger</keyname><forenames>Stefan</forenames></author><author><keyname>Lippert</keyname><forenames>Christoph</forenames></author><author><keyname>Stegle</keyname><forenames>Oliver</forenames></author><author><keyname>Sch&#xf6;lkopf</keyname><forenames>Bernhard</forenames></author><author><keyname>Weigel</keyname><forenames>Detlef</forenames></author><author><keyname>Borgwardt</keyname><forenames>Karsten</forenames></author></authors><title>easyGWAS: An integrated interspecies platform for performing genome-wide
  association studies</title><categories>q-bio.GN cs.CE cs.DL stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Motivation: The rapid growth in genome-wide association studies (GWAS) in
plants and animals has brought about the need for a central resource that
facilitates i) performing GWAS, ii) accessing data and results of other GWAS,
and iii) enabling all users regardless of their background to exploit the
latest statistical techniques without having to manage complex software and
computing resources.
  Results: We present easyGWAS, a web platform that provides methods, tools and
dynamic visualizations to perform and analyze GWAS. In addition, easyGWAS makes
it simple to reproduce results of others, validate findings, and access larger
sample sizes through merging of public datasets.
  Availability: Detailed method and data descriptions as well as tutorials are
available in the supplementary materials. easyGWAS is available at
http://easygwas.tuebingen.mpg.de/.
  Contact: dominik.grimm@tuebingen.mpg.de
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4797</identifier>
 <datestamp>2013-05-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4797</id><created>2012-12-19</created><updated>2013-05-28</updated><authors><author><keyname>Kawald</keyname><forenames>Bernd</forenames></author><author><keyname>Lenzner</keyname><forenames>Pascal</forenames></author></authors><title>On Dynamics in Selfish Network Creation</title><categories>cs.GT cs.DS math.DS</categories><comments>36 pages, 16 figures. To appear at SPAA'13</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the dynamic behavior of several variants of the Network Creation
Game, introduced by Fabrikant et al. [PODC'03]. Equilibrium networks in these
models have desirable properties like low social cost and small diameter, which
makes them attractive for the decentralized creation of overlay-networks.
Unfortunately, due to the non-constructiveness of the Nash equilibrium, no
distributed algorithm for finding such networks is known. We treat these games
as sequential-move games and analyze whether (uncoordinated) selfish play
eventually converges to an equilibrium state. Thus, we shed light on one of the
most natural algorithms for this problem: distributed local search, where in
each step some agent performs a myopic selfish improving move.
  We show that fast convergence is guaranteed for all versions of Swap Games,
introduced by Alon et al. [SPAA'10], if the initial network is a tree, and show
that this process can be sped up to an almost optimal number of moves. For
non-tree networks we show the surprising result that even one non-tree edge
suffices to destroy the convergence guarantee and no move policy can enforce
convergence. This answers an open problem from Ehsani et al. [SPAA'11] in the
negative. We extend our negative results to the well-studied original version
and prove that there is no convergence guarantee -- even if all agents play
optimally. Furthermore, we show the quite surprising result that employing
cost-sharing yields even worse dynamic behavior.
  Finally, we contrast our mostly negative theoretical results by a careful
empirical study. Our simulations indicate two positive facts: (1) The
non-convergent behavior seems to be confined to a small set of pathological
instances and is unlikely to show up in practice. (2) In all our simulations we
observed a remarkably fast convergence towards a stable network in O(n) steps,
where n is the number of agents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4799</identifier>
 <datestamp>2015-11-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4799</id><created>2012-12-19</created><updated>2013-10-09</updated><authors><author><keyname>Freer</keyname><forenames>Cameron E.</forenames></author><author><keyname>Roy</keyname><forenames>Daniel M.</forenames></author><author><keyname>Tenenbaum</keyname><forenames>Joshua B.</forenames></author></authors><title>Towards common-sense reasoning via conditional simulation: legacies of
  Turing in Artificial Intelligence</title><categories>cs.AI math.LO stat.ML</categories><comments>51 pages, 6 figures, 1 table. Slight typographical fixes</comments><acm-class>I.2; F.1.2; G.3; F.4.1</acm-class><journal-ref>Turing's Legacy: Developments from Turing's Ideas in Logic, ed.
  Rod Downey, ASL Lecture Notes in Logic 42, Cambridge University Press, 2014</journal-ref><doi>10.1017/CBO9781107338579.007</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of replicating the flexibility of human common-sense reasoning
has captured the imagination of computer scientists since the early days of
Alan Turing's foundational work on computation and the philosophy of artificial
intelligence. In the intervening years, the idea of cognition as computation
has emerged as a fundamental tenet of Artificial Intelligence (AI) and
cognitive science. But what kind of computation is cognition?
  We describe a computational formalism centered around a probabilistic Turing
machine called QUERY, which captures the operation of probabilistic
conditioning via conditional simulation. Through several examples and analyses,
we demonstrate how the QUERY abstraction can be used to cast common-sense
reasoning as probabilistic inference in a statistical model of our observations
and the uncertain structure of the world that generated that experience. This
formulation is a recent synthesis of several research programs in AI and
cognitive science, but it also represents a surprising convergence of several
of Turing's pioneering insights in AI, the foundations of computation, and
statistics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4804</identifier>
 <datestamp>2012-12-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4804</id><created>2012-12-19</created><authors><author><keyname>Glaser</keyname><forenames>S&#xe9;bastien</forenames><affiliation>LIVIC</affiliation></author><author><keyname>Cour</keyname><forenames>Maurice</forenames><affiliation>IBISC</affiliation></author><author><keyname>Nouveliere</keyname><forenames>Lydie</forenames><affiliation>IBISC</affiliation></author><author><keyname>Lambert</keyname><forenames>Alain</forenames><affiliation>IEF</affiliation></author><author><keyname>Nashashibi</keyname><forenames>Fawzi</forenames><affiliation>INRIA Rocquencourt</affiliation></author><author><keyname>Popieul</keyname><forenames>Jean-Christophe</forenames><affiliation>LAMIH</affiliation></author><author><keyname>Mourllion</keyname><forenames>Benjamin</forenames><affiliation>MIPS</affiliation></author></authors><title>Low Speed Automation, a French Initiative</title><categories>cs.RO</categories><comments>TRA (2012)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Nowadays, vehicle safety is constantly increasing thanks to the improvement
of vehicle passive and active safety. However, on a daily usage of the car,
traffic jams remains a problem. With limited space for road infrastructure,
automation of the driving task on specific situation seems to be a possible
solution. The French project ABV, which stands for low speed automation, tries
to demonstrate the feasibility of the concept and to prove the benefits. In
this article, we describe the scientific background of the project and expected
outputs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4846</identifier>
 <datestamp>2012-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4846</id><created>2012-12-19</created><authors><author><keyname>Vigliotti</keyname><forenames>Maria Grazia</forenames></author></authors><title>Operational semantics for product-form solution</title><categories>cs.PF cs.SY</categories><comments>15 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present product-form solutions from the point of view of
stochastic process algebra. In previous work we have shown how to derive
product-form solutions for a formalism called Labelled Markov Automata (LMA).
LMA are very useful as their relation with the Continuous Time Markov Chains is
very direct. The disadvantage of using LMA is that the proofs of properties are
cumbersome. In fact, in LMA it is not possible to use the inductive structure
of the language in a proof. In this paper we consider a simple stochastic
process algebra that has the great advantage of simplifying the proofs. This
simple language has been inspired by PEPA, however, detailed analysis of the
semantics of cooperation will show the differences between the two formalisms.
It will also be shown that the semantics of the cooperation in process algebra
influences the correctness of the derivation of the product-form solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4871</identifier>
 <datestamp>2012-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4871</id><created>2012-12-19</created><authors><author><keyname>Norousi</keyname><forenames>Ramin</forenames></author><author><keyname>Wickles</keyname><forenames>Stephan</forenames></author><author><keyname>Leidig</keyname><forenames>Christoph</forenames></author><author><keyname>Becker</keyname><forenames>Thomas</forenames></author><author><keyname>Schmid</keyname><forenames>Volker J.</forenames></author><author><keyname>Beckmann</keyname><forenames>Roland</forenames></author><author><keyname>Tresch</keyname><forenames>Achim</forenames></author></authors><title>Automatic post-picking using MAPPOS improves particle image detection
  from Cryo-EM micrographs</title><categories>stat.ML cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cryo-electron microscopy (cryo-EM) studies using single particle
reconstruction are extensively used to reveal structural information on
macromolecular complexes. Aiming at the highest achievable resolution, state of
the art electron microscopes automatically acquire thousands of high-quality
micrographs. Particles are detected on and boxed out from each micrograph using
fully- or semi-automated approaches. However, the obtained particles still
require laborious manual post-picking classification, which is one major
bottleneck for single particle analysis of large datasets. We introduce MAPPOS,
a supervised post-picking strategy for the classification of boxed particle
images, as additional strategy adding to the already efficient automated
particle picking routines. MAPPOS employs machine learning techniques to train
a robust classifier from a small number of characteristic image features. In
order to accurately quantify the performance of MAPPOS we used simulated
particle and non-particle images. In addition, we verified our method by
applying it to an experimental cryo-EM dataset and comparing the results to the
manual classification of the same dataset. Comparisons between MAPPOS and
manual post-picking classification by several human experts demonstrated that
merely a few hundred sample images are sufficient for MAPPOS to classify an
entire dataset with a human-like performance. MAPPOS was shown to greatly
accelerate the throughput of large datasets by reducing the manual workload by
orders of magnitude while maintaining a reliable identification of non-particle
images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4898</identifier>
 <datestamp>2014-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4898</id><created>2012-12-19</created><updated>2014-04-22</updated><authors><author><keyname>Zhang</keyname><forenames>Baosen</forenames></author><author><keyname>Rajagopal</keyname><forenames>Ram</forenames></author><author><keyname>Tse</keyname><forenames>David</forenames></author></authors><title>Network Risk Limiting Dispatch: Optimal Control and Price of Uncertainty</title><categories>math.OC cs.IT cs.SY math.IT</categories><comments>To Appear in IEEE transaction on automatic control, shorter version
  presented in Allerton</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Increased uncertainty due to high penetration of renewables imposes
significant costs to the system operators. The added costs depend on several
factors including market design, performance of renewable generation
forecasting and the specific dispatch procedure. Quantifying these costs has
been limited to small sample Monte Carlo approaches applied specific dispatch
algorithms. The computational complexity and accuracy of these approaches has
limited the understanding of tradeoffs between different factors. {In this work
we consider a two-stage stochastic economic dispatch problem. Our goal is to
provide an analytical quantification and an intuitive understanding of the
effects of uncertainties and network congestion on the dispatch procedure and
the optimal cost.} We first consider an uncongested network and calculate the
risk limiting dispatch. In addition, we derive the price of uncertainty, a
number that characterizes the intrinsic impact of uncertainty on the
integration cost of renewables. Then we extend the results to a network where
one link can become congested. Under mild conditions, we calculate price of
uncertainty even in this case. We show that risk limiting dispatch is given by
a set of deterministic equilibrium equations. The dispatch solution yields an
important insight: congested links do not create isolated nodes, even in a
two-node network. In fact, the network can support backflows in congested
links, that are useful to reduce the uncertainty by averaging supply across the
network. We demonstrate the performance of our approach in standard IEEE
benchmark networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4899</identifier>
 <datestamp>2012-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4899</id><created>2012-12-19</created><authors><author><keyname>Fan</keyname><forenames>Pingyi</forenames></author></authors><title>New inequalities of Mill's ratio and Its Application to The Inverse
  Q-function Approximation</title><categories>cs.IT math.IT math.ST stat.TH</categories><comments>12 pages, 4 figures; Submitted to IEEE Trans. Information Theory Dec.
  2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the Mill's ratio estimation problem and get two
new inequalities. Compared to the well known results obtained by Gordon, they
becomes tighter. Furthermore, we also discuss the inverse Q-function
approximation problem and present some useful results on the inverse solution.
Numerical results confirm the validness of our theoretical analysis. In
addition, we also present a conjecture on the bounds of inverse solution on
Q-function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4902</identifier>
 <datestamp>2014-07-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4902</id><created>2012-12-19</created><updated>2013-09-14</updated><authors><author><keyname>Ashraphijuo</keyname><forenames>Mehdi</forenames></author><author><keyname>Aggarwal</keyname><forenames>Vaneet</forenames></author><author><keyname>Wang</keyname><forenames>Xiaodong</forenames></author></authors><title>On the Capacity Region and the Generalized Degrees of Freedom Region for
  the MIMO Interference Channel with Feedback</title><categories>cs.IT math.IT</categories><comments>47 pages, accepted to IEEE Trans. Inf. Th., Sept 2013. arXiv admin
  note: text overlap with arXiv:1102.0267 by other authors</comments><journal-ref>IEEE Trans. Inf. Th., vol.59, no.12, pp.8357--8376, Dec. 2013</journal-ref><doi>10.1109/TIT.2013.2282317</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study the effect of feedback on two-user MIMO interference
channels. The capacity region of MIMO interference channels with feedback is
characterized within a constant number of bits, where this constant is
independent of the channel matrices. Further, it is shown that the capacity
region of a MIMO interference channel with feedback and its reciprocal
interference channel are within a constant number of bits. Finally, the
generalized degrees of freedom region for the MIMO interference channel with
feedback is characterized.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4906</identifier>
 <datestamp>2012-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4906</id><created>2012-12-19</created><authors><author><keyname>Dowty</keyname><forenames>James G.</forenames></author></authors><title>SMML estimators for 1-dimensional continuous data</title><categories>cs.IT math.IT math.ST stat.ML stat.TH</categories><comments>10 pages, 2 tables and 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A method is given for calculating the strict minimum message length (SMML)
estimator for 1-dimensional exponential families with continuous sufficient
statistics. A set of $n$ equations are found that the $n$ cut-points of the
SMML estimator must satisfy. These equations can be solved using Newton's
method and this approach is used to produce new results and to replicate
results that C. S. Wallace obtained using his boundary rules for the SMML
estimator. A rigorous proof is also given that, despite being composed of step
functions, the posterior probability corresponding to the SMML estimator is a
continuous function of the data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4912</identifier>
 <datestamp>2012-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4912</id><created>2012-12-19</created><authors><author><keyname>Wolper</keyname><forenames>James S.</forenames></author></authors><title>A Torelli-like Theorem for Smooth Plane Curves</title><categories>math.AG cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Information-Theoretic Schottky Problem treats the period matrix of a
compact Riemann Surface as a compressible signal. In this case, the period
matrix of a smooth plane curve is characterized by only 4 of its columns, a
significant compression.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4913</identifier>
 <datestamp>2013-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4913</id><created>2012-12-19</created><authors><author><keyname>Khan</keyname><forenames>Sadeque Reza</forenames></author><author><keyname>Khan</keyname><forenames>Siddique Reza</forenames></author><author><keyname>Ferdousi</keyname><forenames>Arifa</forenames></author></authors><title>Voltage Temperature Monitoring System (VTMS) for a BTS Room</title><categories>cs.OH</categories><journal-ref>International Journal of Instrumentation and Control Systems
  (IJICS), Volume: 02, Number:04, page: 01-10, ISSN: 2249 - 1147, year:2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although Cellular communication is getting more and more popular in our
country present days, but its network improvement is hampered by the crysis of
electricity. The recent decision of present Government is that they will not
provide any electricity from the grid to any new BTS rooms of any Celluler
operator companies like Grammen Phone, Robi, Airtel etc. These companies have
to develop their own power stations either by using generators or by developing
solar plants. Now a days most of the BTS rooms, that the cellular operators are
installing with a generator and 48 volt battery backup. So for the
synchronisation of the operation of PDB, Generator and battery, they require a
device called Voltage Temperature Monitoring System or VTMS. It is a
Microcontroller based controlling unit which controlls the operation of
generator and battery when PDB in not available in the BTS room.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4914</identifier>
 <datestamp>2012-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4914</id><created>2012-12-19</created><authors><author><keyname>Zhang</keyname><forenames>Jiang</forenames></author></authors><title>Growing Random Geometric Graph Models of Super-linear Scaling Law</title><categories>physics.soc-ph cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent researches on complex systems highlighted the so-called super-linear
growth phenomenon. As the system size $P$ measured as population in cities or
active users in online communities increases, the total activities $X$ measured
as GDP or number of new patents, crimes in cities generated by these people
also increases but in a faster rate. This accelerating growth phenomenon can be
well described by a super-linear power law $X \propto P^{\gamma}$($\gamma&gt;1$).
However, the explanation on this phenomenon is still lack. In this paper, we
propose a modeling framework called growing random geometric models to explain
the super-linear relationship. A growing network is constructed on an abstract
geometric space. The new coming node can only survive if it just locates on an
appropriate place in the space where other nodes exist, then new edges are
connected with the adjacent nodes whose number is determined by the density of
existing nodes. Thus the total number of edges can grow with the number of
nodes in a faster speed exactly following the super-linear power law. The
models cannot only reproduce a lot of observed phenomena in complex networks,
e.g., scale-free degree distribution and asymptotically size-invariant
clustering coefficient, but also resemble the known patterns of cities, such as
fractal growing, area-population and diversity-population scaling relations,
etc. Strikingly, only one important parameter, the dimension of the geometric
space, can really influence the super-linear growth exponent $\gamma$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4915</identifier>
 <datestamp>2012-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4915</id><created>2012-12-19</created><authors><author><keyname>Xu</keyname><forenames>Ke</forenames></author><author><keyname>Zhong</keyname><forenames>Yifeng</forenames></author><author><keyname>He</keyname><forenames>Huan</forenames></author></authors><title>Can P2P Technology Benefit Eyeball ISPs? A Cooperative Profit
  Distribution Answer</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Peer-to-Peer (P2P) technology has been regarded as a promising way to help
Content Providers (CPs) cost-effectively distribute content. However, under the
traditional Internet pricing mechanism, the fact that most P2P traffic flows
among peers can dramatically decrease the profit of ISPs, who may take actions
against P2P and impede the progress of P2P technology. In this paper, we
develop a mathematical framework to analyze such economic issues. Inspired by
the idea from cooperative game theory, we propose a cooperative
profit-distribution model based on Nash Bargaining Solution (NBS), in which
eyeball ISPs and Peer-assisted CPs (PCPs) form two coalitions respectively and
then compute a fair Pareto point to determine profit distribution. Moreover, we
design a fair and feasible mechanism for profit distribution within each
coalition. We show that such a cooperative method not only guarantees the fair
profit distribution among network participators, but also helps to improve the
economic efficiency of the overall network system. To our knowledge, this is
the first work that systematically studies solutions for P2P caused unbalanced
profit distribution and gives a feasible cooperative method to increase and
fairly share profit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4920</identifier>
 <datestamp>2012-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4920</id><created>2012-12-19</created><authors><author><keyname>Guo</keyname><forenames>Jianya</forenames></author><author><keyname>Mei</keyname><forenames>Xi</forenames></author><author><keyname>Tang</keyname><forenames>Kun</forenames></author></authors><title>Automatic landmark annotation and dense correspondence registration for
  3D human facial images</title><categories>cs.CV q-bio.QM</categories><comments>33 pages, 6 figures, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dense surface registration of three-dimensional (3D) human facial images
holds great potential for studies of human trait diversity, disease genetics,
and forensics. Non-rigid registration is particularly useful for establishing
dense anatomical correspondences between faces. Here we describe a novel
non-rigid registration method for fully automatic 3D facial image mapping. This
method comprises two steps: first, seventeen facial landmarks are automatically
annotated, mainly via PCA-based feature recognition following 3D-to-2D data
transformation. Second, an efficient thin-plate spline (TPS) protocol is used
to establish the dense anatomical correspondence between facial images, under
the guidance of the predefined landmarks. We demonstrate that this method is
robust and highly accurate, even for different ethnicities. The average face is
calculated for individuals of Han Chinese and Uyghur origins. While fully
automatic and computationally efficient, this method enables high-throughput
analysis of human facial feature variation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4930</identifier>
 <datestamp>2013-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4930</id><created>2012-12-20</created><authors><author><keyname>Leukhin</keyname><forenames>Anatolii</forenames></author><author><keyname>Potehin</keyname><forenames>Egor</forenames></author></authors><title>Binary Sequences with Minimum Peak Sidelobe Level up to Length 68</title><categories>cs.OH</categories><comments>10 pages, 3figures,2 tables, submitting to International Worksho on
  Coding and Crypography, WCC 2013, April 15-19, 2013, Bergen, Norway</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Results of an exhaustive search for minimum peak sidelobe level binary
sequences are presented. Several techniques for efficiency implementation of
search algorithm are described. A table of number of non-equivalent optimal
binary sequences with minimum peak sidelobe (MPS) level up to length 68 is
given. This number can be used in prediction of the longest length for a given
sidelobe level of binary sequences. The examples of optimal binary MPS
sequences having high merit factor are shown.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4931</identifier>
 <datestamp>2012-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4931</id><created>2012-12-20</created><authors><author><keyname>Leukhin</keyname><forenames>Anatolii</forenames></author><author><keyname>Moreno</keyname><forenames>Oscar</forenames></author><author><keyname>Tirkel</keyname><forenames>Andrew</forenames></author></authors><title>Secure CDMA Sequences</title><categories>cs.CR</categories><comments>10 pages, 8 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Single sequences like Legendre have high linear complexity. Known CDMA
families of sequences all have low complexities. We present a new method of
constructing CDMA sequence sets with the complexity of the Legendre from new
frequency hop patterns, and compare them with known sequences. These are the
first families whose normalized linear complexities do not asymptote to 0,
verified for lengths up to 6x108. The new constructions in array format are
also useful in watermarking images. We present a conjecture regarding the
recursion polynomials. We also have a method to reverse the process, and from
small Kasami/No-Kumar sequences we obtain a new family of 2n doubly periodic
(2n+1)x(2n-1) frequency hop patterns with correlation 2.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4935</identifier>
 <datestamp>2012-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4935</id><created>2012-12-20</created><authors><author><keyname>Tramboo</keyname><forenames>Shahkar</forenames></author><author><keyname>Humma</keyname></author><author><keyname>Shafi</keyname><forenames>S M</forenames></author><author><keyname>Gul</keyname><forenames>Sumeer</forenames></author></authors><title>A Study on the Open Source Digital Library Software's: Special Reference
  to DSpace, EPrints and Greenstone</title><categories>cs.DL</categories><comments>9 Pages, 3 Figures, 1 Table, &quot;Published with International Journal of
  Computer Applications (IJCA)&quot;</comments><journal-ref>Article: A study on the Open Source Digital Library Softwares:
  Special Reference to DSpace, EPrints and Greenstone. International Journal of
  Computer Applications 59(16):1-9, December 2012. Published by Foundation of
  Computer Science,USA</journal-ref><doi>10.5120/9629-4272</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The richness in knowledge has changed access methods for all stake holders in
retrieving key knowledge and relevant information. This paper presents a study
of three open source digital library management software used to assimilate and
disseminate information to world audience. The methodology followed involves
online survey and study of related software documentation and associated
technical manuals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4940</identifier>
 <datestamp>2012-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4940</id><created>2012-12-20</created><authors><author><keyname>Chernyakova</keyname><forenames>T.</forenames></author><author><keyname>Eldar</keyname><forenames>Y. C.</forenames></author><author><keyname>Amit</keyname><forenames>R.</forenames></author></authors><title>Fourier Domain Beamforming for Medical Ultrasound</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Sonography techniques use multiple transducer elements for tissue
visualization. Signals detected at each element are sampled prior to digital
beamforming. The required sampling rates are up to 4 times the Nyquist rate of
the signal and result in considerable amount of data, that needs to be stored
and processed. A developed technique, based on the finite rate of innovation
model, compressed sensing (CS) and Xampling ideas, allows to reduce the number
of samples needed to reconstruct an image comprised of strong reflectors. A
significant drawback of this method is its inability to treat speckle, which is
of significant importance in medical imaging. Here we build on previous work
and show explicitly how to perform beamforming in the Fourier domain.
Beamforming in frequency exploits the low bandwidth of the beamformed signal
and allows to bypass the oversampling dictated by digital implementation of
beamforming in time. We show that this allows to obtain the same beamformed
image as in standard beamforming but from far fewer samples. Finally, we
present an analysis based CS-technique that allows for further reduction in
sampling rate, using only a portion of the beamformed signal's bandwidth,
namely, sampling the signal at sub-Nyquist rates. We demonstrate our methods on
in vivo cardiac ultrasound data and show that reductions up to 1/25 over
standard beamforming rates are possible.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4950</identifier>
 <datestamp>2012-12-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4950</id><created>2012-12-20</created><authors><author><keyname>Roth</keyname><forenames>Christoph</forenames></author><author><keyname>Benkeser</keyname><forenames>Christian</forenames></author><author><keyname>Studer</keyname><forenames>Christoph</forenames></author><author><keyname>Karakonstantis</keyname><forenames>Georgios</forenames></author><author><keyname>Burg</keyname><forenames>Andreas</forenames></author></authors><title>Data Mapping for Unreliable Memories</title><categories>cs.IT math.IT</categories><comments>Proc. of the IEEE Allerton Conference, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Future digital signal processing (DSP) systems must provide robustness on
algorithm and application level to the presence of reliability issues that come
along with corresponding implementations in modern semiconductor process
technologies. In this paper, we address this issue by investigating the impact
of unreliable memories on general DSP systems. In particular, we propose a
novel framework to characterize the effects of unreliable memories, which
enables us to devise novel methods to mitigate the associated performance loss.
We propose to deploy specifically designed data representations, which have the
capability of substantially improving the system reliability compared to that
realized by conventional data representations used in digital integrated
circuits, such as 2s complement or sign-magnitude number formats. To
demonstrate the efficacy of the proposed framework, we analyze the impact of
unreliable memories on coded communication systems, and we show that the
deployment of optimized data representations substantially improves the
error-rate performance of such systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4959</identifier>
 <datestamp>2012-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4959</id><created>2012-12-20</created><authors><author><keyname>Sayar</keyname><forenames>Imen</forenames></author></authors><title>D'Event-B vers UML/OCL en passant par UML/EM-OCL</title><categories>cs.SE</categories><comments>128 pages, 23 figures, 6 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To overcome the limitations of both approaches classical and formal for the
development of complex software, we proposed a hybrid approach combining the
formal approach (Event-B) and the classical approach (UML/OCL). Upstream phases
of our approach include: Rewriting the requirements document, Refinement
strategy, Abstract specification and Horizontal refinement. We have shown the
feasibility of our approach on a case study: An Electronic Hotel Key System
(SCEH). The problem of transition from the formal (Event-B) to the semi-formal
(UML/OCL) is processed through our extension to OCL (EM-OCL).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4968</identifier>
 <datestamp>2013-09-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4968</id><created>2012-12-20</created><authors><author><keyname>Ispas</keyname><forenames>Adrian</forenames></author><author><keyname>Gong</keyname><forenames>Xitao</forenames></author><author><keyname>Schneider</keyname><forenames>Christian</forenames></author><author><keyname>Ascheid</keyname><forenames>Gerd</forenames></author><author><keyname>Thom&#xe4;</keyname><forenames>Reiner</forenames></author></authors><title>Dual-Polarized Ricean MIMO Channels: Modeling and Performance Assessment</title><categories>cs.IT math.IT</categories><comments>submitted to the IEEE Transactions on Communications</comments><doi>10.1109/TCOMM.2013.082813.120976</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In wireless communication systems, dual-polarized (DP) instead of
single-polarized (SP) multiple-input multiple-output (MIMO) transmission is
used to improve the spectral efficiency under certain conditions on the channel
and the signal-to-noise ratio (SNR). In order to identify these conditions, we
first propose a novel channel model for DP mobile Ricean MIMO channels for
which statistical channel parameters are readily obtained from a moment-based
channel decomposition. Second, we derive an approximation of the mutual
information (MI), which can be expressed as a function of those statistical
channel parameters. Based on this approximation, we characterize the required
SNR for a DP MIMO system to outperform an SP MIMO system in terms of the MI.
Finally, we apply our results to channel measurements at 2.53 GHz. We find
that, using the proposed channel decomposition and the approximation of the MI,
we are able to reproduce the (practically relevant) SNR values above which DP
MIMO systems outperform SP MIMO systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4969</identifier>
 <datestamp>2012-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4969</id><created>2012-12-20</created><authors><author><keyname>Feldmann</keyname><forenames>Michel</forenames></author></authors><title>Polynomial time factoring algorithm using Bayesian arithmetic</title><categories>cs.DS</categories><comments>13 pages, no figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a previous paper, we have shown that any Boolean formula can be encoded as
a linear programming problem in the framework of Bayesian probability theory.
When applied to NP-complete algorithms, this leads to the fundamental
conclusion that P = NP. Now, we implement this concept in elementary arithmetic
and especially in multiplication. This provides a polynomial time deterministic
factoring algorithm, while no such algorithm is known to day. This result
clearly appeals for a revaluation of the current cryptosystems. The Bayesian
arithmetic environment can also be regarded as a toy model for quantum
mechanics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4989</identifier>
 <datestamp>2013-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4989</id><created>2012-12-20</created><updated>2013-01-10</updated><authors><author><keyname>Wozniak</keyname><forenames>Sander</forenames></author><author><keyname>Rossberg</keyname><forenames>Michael</forenames></author><author><keyname>Schaefer</keyname><forenames>Guenter</forenames></author></authors><title>Towards Trustworthy Mobile Social Networking Services for Disaster
  Response</title><categories>cs.SI cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Situational awareness is crucial for effective disaster management. However,
obtaining information about the actual situation is usually difficult and
time-consuming. While there has been some effort in terms of incorporating the
affected population as a source of information, the issue of obtaining
trustworthy information has not yet received much attention. Therefore, we
introduce the concept of witness-based report verification, which enables users
from the affected population to evaluate reports issued by other users. We
present an extensive overview of the objectives to be fulfilled by such a
scheme and provide a first approach considering security and privacy. Finally,
we evaluate the performance of our approach in a simulation study. Our results
highlight synergetic effects of group mobility patterns that are likely in
disaster situations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4991</identifier>
 <datestamp>2012-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4991</id><created>2012-12-20</created><authors><author><keyname>Baldi</keyname><forenames>Marco</forenames></author><author><keyname>Bianchi</keyname><forenames>Marco</forenames></author><author><keyname>Maturo</keyname><forenames>Nicola</forenames></author><author><keyname>Chiaraluce</keyname><forenames>Franco</forenames></author></authors><title>A Physical Layer Secured Key Distribution Technique for IEEE 802.11g
  Wireless Networks</title><categories>cs.IT cs.CR math.IT</categories><comments>9 pages, 7 figures. Accepted for publication in IEEE Wireless
  Communications Letters. Copyright transferred to IEEE</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Key distribution and renewing in wireless local area networks is a crucial
issue to guarantee that unauthorized users are prevented from accessing the
network. In this paper, we propose a technique for allowing an automatic
bootstrap and periodic renewing of the network key by exploiting physical layer
security principles, that is, the inherent differences among transmission
channels. The proposed technique is based on scrambling of groups of
consecutive packets and does not need the use of an initial authentication nor
automatic repeat request protocols. We present a modification of the scrambling
circuits included in the IEEE 802.11g standard which allows for a suitable
error propagation at the unauthorized receiver, thus achieving physical layer
security.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.4999</identifier>
 <datestamp>2012-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.4999</id><created>2012-12-20</created><authors><author><keyname>Seslija</keyname><forenames>Marko</forenames></author><author><keyname>van der Schaft</keyname><forenames>Arjan</forenames></author><author><keyname>Scherpen</keyname><forenames>Jacquelien M. A.</forenames></author></authors><title>Hamiltonian Perspective on Compartmental Reaction-Diffusion Networks</title><categories>cs.SY math.OC nlin.PS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inspired by the recent developments in modeling and analysis of reaction
networks, we provide a geometric formulation of the reversible reaction
networks under the influence of diffusion. Using the graph knowledge of the
underlying reaction network, the obtained reaction-diffusion system is a
distributed-parameter port-Hamiltonian system on a compact spatial domain.
Motivated by the need for computer based design, we offer a spatially
consistent discretization of the PDE system and, in a systematic manner,
recover a compartmental ODE model on a simplicial triangulation of the spatial
domain. Exploring the properties of a balanced weighted Laplacian matrix of the
reaction network and the Laplacian of the simplicial complex, we characterize
the space of equilibrium points and provide a simple stability analysis on the
state space modulo the space of equilibrium points. The paper rules out the
possibility of the persistence of spatial patterns for the compartmental
balanced reaction-diffusion networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5003</identifier>
 <datestamp>2012-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5003</id><created>2012-12-20</created><authors><author><keyname>Caron</keyname><forenames>Pascal</forenames></author><author><keyname>Champarnaud</keyname><forenames>Jean-Marc</forenames></author><author><keyname>Mignot</keyname><forenames>Ludovic</forenames></author></authors><title>A General Framework for the Derivation of Regular Expressions</title><categories>cs.FL</categories><comments>22 pages</comments><msc-class>68Q45</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this paper is to design a theoretical framework that allows us to
perform the computation of regular expression derivatives through a space of
generic structures. Thanks to this formalism, the main properties of regular
expression derivation, such as the finiteness of the set of derivatives, need
only be stated and proved one time, at the top level. Moreover, it is shown how
to construct an alternating automaton associated with the derivation of a
regular expression in this general framework. Finally, Brzozowski's derivation
and Antimirov's derivation turn out to be a particular case of this general
scheme and it is shown how to construct a DFA, a NFA and an AFA for both of
these derivations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5024</identifier>
 <datestamp>2013-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5024</id><created>2012-12-20</created><updated>2013-11-17</updated><authors><author><keyname>Liu</keyname><forenames>Ya-Feng</forenames></author><author><keyname>Dai</keyname><forenames>Yu-Hong</forenames></author></authors><title>On the Complexity of Joint Subcarrier and Power Allocation for
  Multi-User OFDMA Systems</title><categories>cs.IT math.CO math.IT</categories><comments>Accepted for publication in IEEE Transactions on Signal Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a multi-user Orthogonal Frequency Division Multiple Access (OFDMA)
system where multiple users share multiple discrete subcarriers, but at most
one user is allowed to transmit power on each subcarrier. To adapt fast traffic
and channel fluctuations and improve the spectrum efficiency, the system should
have the ability to dynamically allocate subcarriers and power resources to
users. Assuming perfect channel knowledge, two formulations for the joint
subcarrier and power allocation problem are considered in this paper: the first
is to minimize the total transmission power subject to quality of service
constraints and the OFDMA constraint, and the second is to maximize some system
utility function (including the sum-rate utility, the proportional fairness
utility, the harmonic mean utility, and the min-rate utility) subject to the
total transmission power constraint per user and the OFDMA constraint. In spite
of the existence of various heuristics approaches, little is known about the
computational complexity status of the above problem. This paper aims to fill
this theoretical gap, i.e., characterizing the complexity of the joint
subcarrier and power allocation problem for the multi-user OFDMA system. It is
shown in this paper that both formulations of the joint subcarrier and power
allocation problem are strongly NP-hard. The proof is based on a polynomial
time transformation from the so-called 3-dimensional matching problem. Several
subclasses of the problem which can be solved to global optimality or
$\epsilon$-global optimality in polynomial time are also identified. These
complexity results suggest that there are not polynomial time algorithms which
are able to solve the general joint subcarrier and power allocation problem to
global optimality (unless P$=$NP), and determining an approximately optimal
subcarrier and power allocation strategy is more realistic in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5032</identifier>
 <datestamp>2013-09-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5032</id><created>2012-12-20</created><updated>2013-09-20</updated><authors><author><keyname>Bourtsoulatze</keyname><forenames>Eirina</forenames></author><author><keyname>Thomos</keyname><forenames>Nikolaos</forenames></author><author><keyname>Frossard</keyname><forenames>Pascal</forenames></author></authors><title>Distributed Rate Allocation in Inter-Session Network Coding</title><categories>cs.NI cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Multimedia</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we propose a distributed rate allocation algorithm that
minimizes the average decoding delay for multimedia clients in inter-session
network coding systems. We consider a scenario where the users are organized in
a mesh network and each user requests the content of one of the available
sources. We propose a novel distributed algorithm where network users determine
the coding operations and the packet rates to be requested from the parent
nodes, such that the decoding delay is minimized for all the clients. A rate
allocation problem is solved by every user, which seeks the rates that minimize
the average decoding delay for its children and for itself. Since the
optimization problem is a priori non-convex, we introduce the concept of
equivalent packet flows, which permits to estimate the expected number of
packets that every user needs to collect for decoding. We then decompose our
original rate allocation problem into a set of convex subproblems, which are
eventually combined to obtain an effective approximate solution to the delay
minimization problem. The results demonstrate that the proposed scheme
eliminates the bottlenecks and reduces the decoding delay experienced by users
with limited bandwidth resources. We validate the performance of our
distributed rate allocation algorithm in different video streaming scenarios
using the NS-3 network simulator. We show that our system is able to take
benefit of inter-session network coding for simultaneous delivery of video
sessions in networks with path diversity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5035</identifier>
 <datestamp>2012-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5035</id><created>2012-12-20</created><authors><author><keyname>Avrachenkov</keyname><forenames>Konstantin</forenames></author><author><keyname>Basu</keyname><forenames>Prithwish</forenames></author><author><keyname>Neglia</keyname><forenames>Giovanni</forenames></author><author><keyname>Ribeiro</keyname><forenames>Bruno</forenames></author><author><keyname>Towsley</keyname><forenames>Don</forenames></author></authors><title>Online Myopic Network Covering</title><categories>cs.SI physics.soc-ph</categories><comments>Corresponding author: ribeiro@cs.umass.edu</comments><report-no>UMass Technical Report UM-CS-2012-034</report-no><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Efficient marketing or awareness-raising campaigns seek to recruit $n$
influential individuals -- where $n$ is the campaign budget -- that are able to
cover a large target audience through their social connections. So far most of
the related literature on maximizing this network cover assumes that the social
network topology is known. Even in such a case the optimal solution is NP-hard.
In practice, however, the network topology is generally unknown and needs to be
discovered on-the-fly. In this work we consider an unknown topology where
recruited individuals disclose their social connections (a feature known as
{\em one-hop lookahead}). The goal of this work is to provide an efficient
greedy online algorithm that recruits individuals as to maximize the size of
target audience covered by the campaign.
  We propose a new greedy online algorithm, Maximum Expected $d$-Excess Degree
(MEED), and provide, to the best of our knowledge, the first detailed
theoretical analysis of the cover size of a variety of well known network
sampling algorithms on finite networks. Our proposed algorithm greedily
maximizes the expected size of the cover. For a class of random power law
networks we show that MEED simplifies into a straightforward procedure, which
we denote MOD (Maximum Observed Degree). We substantiate our analytical results
with extensive simulations and show that MOD significantly outperforms all
analyzed myopic algorithms. We note that performance may be further improved if
the node degree distribution is known or can be estimated online during the
campaign.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5086</identifier>
 <datestamp>2012-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5086</id><created>2012-12-16</created><authors><author><keyname>Abel</keyname><forenames>Marc W.</forenames></author></authors><title>Practical, scalable alternative session encryption using one-time pads</title><categories>cs.CR</categories><comments>21 pages. To be submitted for publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  When I was smaller, a five megabyte fixed disk cost $5,000, a 300 bps modem
cost hundreds of dollars, and communication links were intercepted by attaching
devices to the target subscriber's local loop. From then to now there have been
three great implosions: the cost of storage, the cost of bandwidth, and the
cost of surveillance. The wake of the first two implosions sheared away most
obstacles to using one-time pads to encrypt data in flight, and the final
imposition -- I mean implosion -- now makes consideration of one-time pads a
practical necessity.
  So far as assurance of confidentiality is concerned, today's block ciphers
and public key cryptosystems flunk the exam. I don't know how to recover an AES
key or compute the discrete logarithm of an elliptic curve element, but there
exists no proof that another cannot. Moreover, encrypted communications can be
recorded and stored for later attack by algorithms and devices yet to be
discovered. Equally concerning is that when a significant &quot;break&quot; is discovered
for solving either puzzle, the safety of an entire planet's communication and
data dissolves like instant pudding.
  The world is unready to ingest so much pudding. We need balance in our
cryptographic diet, and we need that balance now. In this paper, I discredit
many myths concerning one-time pads, discuss practical steps to address
perceived shortcomings, and shatter the notion that secure generation,
distribution, and use of mammoth cryptographic keys cannot be practiced in
every home, church, school, and business. I also discuss my own
implementations, their capabilities and track record, and where they should
lead.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5089</identifier>
 <datestamp>2012-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5089</id><created>2012-12-20</created><authors><author><keyname>El-Zawawy</keyname><forenames>Mohamed A.</forenames></author></authors><title>Dead code elimination based pointer analysis for multithreaded programs</title><categories>cs.SE</categories><comments>19 pages</comments><journal-ref>Journal of the Egyptian Mathematical Society, Volume 20, Issue 1,
  April 2012, Pages 28-37.(Revised version)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new approach for optimizing multitheaded programs with
pointer constructs. The approach has applications in the area of certified code
(proof-carrying code) where a justification or a proof for the correctness of
each optimization is required. The optimization meant here is that of dead code
elimination.
  Towards optimizing multithreaded programs the paper presents a new
operational semantics for parallel constructs like join-fork constructs,
parallel loops, and conditionally spawned threads. The paper also presents a
novel type system for flow-sensitive pointer analysis of multithreaded
programs. This type system is extended to obtain a new type system for
live-variables analysis of multithreaded programs. The live-variables type
system is extended to build the third novel type system, proposed in this
paper, which carries the optimization of dead code elimination. The
justification mentioned above takes the form of type derivation in our
approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5091</identifier>
 <datestamp>2012-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5091</id><created>2012-12-19</created><authors><author><keyname>Tsiang</keyname><forenames>Elaine</forenames></author></authors><title>Maximally Informative Observables and Categorical Perception</title><categories>cs.LG cs.SD</categories><comments>9 pages, 1 figure</comments><report-no>MIMC000</report-no><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We formulate the problem of perception in the framework of information
theory, and prove that categorical perception is equivalent to the existence of
an observable that has the maximum possible information on the target of
perception. We call such an observable maximally informative. Regardless
whether categorical perception is real, maximally informative observables can
form the basis of a theory of perception. We conclude with the implications of
such a theory for the problem of speech perception.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5094</identifier>
 <datestamp>2012-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5094</id><created>2012-12-20</created><authors><author><keyname>El-Zawawy</keyname><forenames>Mohamed A.</forenames></author></authors><title>Recognition of Logically Related Regions Based Heap Abstraction</title><categories>cs.LO cs.SE</categories><comments>15 pages</comments><journal-ref>Journal of the Egyptian Mathematical Society,Volume 20, Issue 2,
  July 2012, Pages 64-71</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel set of algorithms for heap abstraction,
identifying logically related regions of the heap. The targeted regions include
objects that are part of the same component structure (recursive data
structure). The result of the technique outlined in this paper has the form of
a compact normal form (an abstract model) that boosts the efficiency of the
static analysis via speeding its convergence. The result of heap abstraction,
together with some properties of data structures, can be used to enable program
optimizations like static deallocation, pool allocation, region-based garbage
collection, and object co-location.
  More precisely, this paper proposes algorithms for abstracting heap
components with the layout of a singly linked list, a binary tree, a cycle, and
a directed acyclic graph. The termination and correctness of these algorithms
are studied in the paper. Towards presenting the algorithms the paper also
presents concrete and abstract models for heap representations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5095</identifier>
 <datestamp>2012-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5095</id><created>2012-12-20</created><authors><author><keyname>Ghosh</keyname><forenames>T.</forenames></author><author><keyname>Dan</keyname><forenames>P. K.</forenames></author></authors><title>Modelling of Optimal Design of Manufacturing Cell Layout Considering
  Material Flow and Closeness Rating Factors</title><categories>cs.CE</categories><comments>Proceedings of 4th International &amp; 25th AIMTDR Conference, December
  2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Developing a group of machine cells and their corresponding part families to
minimize the inter-cell and intra-cell material flow is the basic objective of
the designing of a cellular manufacturing system (CMS). Afterwards achieving a
competent cell layout is essential in order to minimize the total inter-cell
part travels, which is principally noteworthy. There are plentiful articles of
CMS literature which considered cell formation problems; however cell layout
topic has rarely been addressed. Therefore this research is intended to focus
on an adapted mathematical model of the layout design problem considering
material handling cost and closeness ratings of manufacturing cells. Owing to
the combinatorial class of the said problem, an efficient NP-hard technique
based on Simulated Annealing metaheuristic is proposed henceforth. Some test
problems are solved using the proposed technique. Computational results show
that the proposed metaheuristic approach is extremely effective and efficient
in terms of solution quality and computational complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5097</identifier>
 <datestamp>2012-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5097</id><created>2012-12-15</created><authors><author><keyname>Chermakani</keyname><forenames>Deepak Ponvel</forenames></author></authors><title>NP-Hardness of optimizing the sum of Rational Linear Functions over an
  Asymptotic-Linear-Program</title><categories>cs.CC cs.DM</categories><comments>3 Pages, 4 Theorems, 1 Definition</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We convert, within polynomial-time and sequential processing, an NP-Complete
Problem into a real-variable problem of minimizing a sum of Rational Linear
Functions constrained by an Asymptotic-Linear-Program. The coefficients and
constants in the real-variable problem are 0, 1, -1, K, or -K, where K is the
time parameter that tends to positive infinity. The number of variables,
constraints, and rational linear functions in the objective, of the
real-variable problem is bounded by a polynomial function of the size of the
NP-Complete Problem. The NP-Complete Problem has a feasible solution,
if-and-only-if, the real-variable problem has a feasible optimal objective
equal to zero. We thus show the strong NP-hardness of this real-variable
optimization problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5098</identifier>
 <datestamp>2013-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5098</id><created>2012-12-20</created><updated>2013-04-02</updated><authors><author><keyname>Miller</keyname><forenames>Gary L.</forenames></author><author><keyname>Sheehy</keyname><forenames>Donald R.</forenames></author></authors><title>A New Approach to Output-Sensitive Voronoi Diagrams and Delaunay
  Triangulations</title><categories>cs.CG cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a new algorithm for computing the Voronoi diagram of a set of $n$
points in constant-dimensional Euclidean space.
  The running time of our algorithm is $O(f \log n \log \Delta)$ where $f$ is
the output complexity of the Voronoi diagram and $\Delta$ is the spread of the
input, the ratio of largest to smallest pairwise distances.
  Despite the simplicity of the algorithm and its analysis, it improves on the
state of the art for all inputs with polynomial spread and near-linear output
size.
  The key idea is to first build the Voronoi diagram of a superset of the input
points using ideas from Voronoi refinement mesh generation.
  Then, the extra points are removed in a straightforward way that allows the
total work to be bounded in terms of the output complexity, yielding the output
sensitive bound.
  The removal only involves local flips and is inspired by kinetic data
structures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5100</identifier>
 <datestamp>2012-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5100</id><created>2012-12-20</created><authors><author><keyname>Fontein</keyname><forenames>Felix</forenames></author><author><keyname>Schneider</keyname><forenames>Michael</forenames></author><author><keyname>Wagner</keyname><forenames>Urs</forenames></author></authors><title>A Polynomial Time Version of LLL With Deep Insertions</title><categories>cs.CR math.CO</categories><comments>12 pages, 6 figures</comments><msc-class>68R05, 94A60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lattice reduction algorithms have numerous applications in number theory,
algebra, as well as in cryptanalysis. The most famous algorithm for lattice
reduction is the LLL algorithm. In polynomial time it computes a reduced basis
with provable output quality. One early improvement of the LLL algorithm was
LLL with deep insertions (DeepLLL). The output of this version of LLL has
higher quality in practice but the running time seems to explode. Weaker
variants of DeepLLL, where the insertions are restricted to blocks, behave
nicely in practice concerning the running time. However no proof of polynomial
running time is known. In this paper a new variant of DeepLLL with provably
polynomial running time is presented. We compare the practical behavior of the
new algorithm to classical LLL, BKZ as well as blockwise variants of DeepLLL
regarding both the output quality and running time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5101</identifier>
 <datestamp>2012-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5101</id><created>2012-12-20</created><authors><author><keyname>Sengupta</keyname><forenames>Sourav</forenames></author><author><keyname>Ghosh</keyname><forenames>Tamal</forenames></author><author><keyname>Dan</keyname><forenames>Pranab K</forenames></author><author><keyname>Chattopadhyay</keyname><forenames>Manojit</forenames></author></authors><title>Hybrid Fuzzy-ART based K-Means Clustering Methodology to Cellular
  Manufacturing Using Operational Time</title><categories>cs.LG</categories><comments>Proceedings of International Conference on Operational Excellence for
  Global Competitiveness (ICOEGC 2011)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a new hybrid Fuzzy-ART based K-Means Clustering technique
to solve the part machine grouping problem in cellular manufacturing systems
considering operational time. The performance of the proposed technique is
tested with problems from open literature and the results are compared to the
existing clustering models such as simple K-means algorithm and modified ART1
algorithm using an efficient modified performance measure known as modified
grouping efficiency (MGE) as found in the literature. The results support the
better performance of the proposed algorithm. The Novelty of this study lies in
the simple and efficient methodology to produce quick solutions for shop floor
managers with least computational efforts and time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5106</identifier>
 <datestamp>2013-11-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5106</id><created>2012-12-20</created><authors><author><keyname>Berth&#xe9;</keyname><forenames>Val&#xe9;rie</forenames><affiliation>LIAFA</affiliation></author><author><keyname>Cassaigne</keyname><forenames>Julien</forenames><affiliation>IML</affiliation></author><author><keyname>Steiner</keyname><forenames>Wolfgang</forenames><affiliation>LIAFA</affiliation></author></authors><title>Balance properties of Arnoux-Rauzy words</title><categories>cs.FL math.CO</categories><proxy>ccsd</proxy><journal-ref>International Journal of Algebra and Computation 23, 4 (2013)
  689-704</journal-ref><doi>10.1142/S0218196713400043</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper deals with balances and imbalances in Arnoux-Rauzy words. We
provide sufficient conditions for $C$-balancedness, but our results indicate
that even a characterization of 2-balanced Arnoux-Rauzy words on a 3-letter
alphabet is not immediate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5108</identifier>
 <datestamp>2012-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5108</id><created>2012-12-20</created><authors><author><keyname>Jacquemard</keyname><forenames>Florent</forenames><affiliation>Inria Paris-Rocquencourt, STMS</affiliation></author><author><keyname>Rusinowitch</keyname><forenames>Michael</forenames><affiliation>INRIA Lorraine - LORIA / LIFC</affiliation></author></authors><title>Rewrite Closure and CF Hedge Automata</title><categories>cs.LO cs.DB cs.FL</categories><proxy>ccsd</proxy><journal-ref>7th International Conference on Language and Automata Theory and
  Application (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce an extension of hedge automata called bidimensional context-free
hedge automata. The class of unranked ordered tree languages they recognize is
shown to be preserved by rewrite closure with inverse-monadic rules. We also
extend the parameterized rewriting rules used for modeling the W3C XQuery
Update Facility in previous works, by the possibility to insert a new parent
node above a given node. We show that the rewrite closure of hedge automata
languages with these extended rewriting systems are context-free hedge
languages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5116</identifier>
 <datestamp>2012-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5116</id><created>2012-12-20</created><authors><author><keyname>Dongol</keyname><forenames>Brijesh</forenames></author><author><keyname>Derrick</keyname><forenames>John</forenames></author></authors><title>Proving linearisability via coarse-grained abstraction</title><categories>cs.LO cs.SE</categories><comments>37 pages, 13 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linearisability has become the standard safety criterion for concurrent data
structures ensuring that the effect of a concrete operation takes place after
the execution some atomic statement (often referred to as the linearisation
point). Identification of linearisation points is a non-trivial task and it is
even possible for an operation to be linearised by the execution of other
concurrent operations. This paper presents a method for verifying
linearisability that does not require identification of linearisation points in
the concrete code. Instead, we show that the concrete program is a refinement
of some coarse-grained abstraction. The linearisation points in the abstraction
are straightforward to identify and the linearisability proof itself is simpler
due to the coarse granularity of its atomic statements. The concrete
fine-grained program is a refinement of the coarse-grained program, and hence
is also linearisable because every behaviour of the concrete program is a
possible behaviour its abstraction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5129</identifier>
 <datestamp>2012-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5129</id><created>2012-12-20</created><authors><author><keyname>Zhang</keyname><forenames>Tongguang</forenames></author></authors><title>Cumulative Sum Algorithm for Detecting SYN Flooding Attacks</title><categories>cs.NI cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  SYN flooding attacks generate enormous packets by a large number of agents
and can easily exhaust the computing and communication resources of a victim
within a short period of time. In this paper, we propose a lightweight method
for detecting SYN flooding attack by non-parametric cumulative sum algorithm.
We experiment with real SYN flooding attack data set in order to evaluate our
method. The results show that our method can detect SYN flooding attack very
well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5132</identifier>
 <datestamp>2012-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5132</id><created>2012-12-20</created><authors><author><keyname>De</keyname><forenames>Anindya</forenames></author><author><keyname>Diakonikolas</keyname><forenames>Ilias</forenames></author><author><keyname>Servedio</keyname><forenames>Rocco A.</forenames></author></authors><title>The Inverse Shapley Value Problem</title><categories>cs.GT cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For $f$ a weighted voting scheme used by $n$ voters to choose between two
candidates, the $n$ \emph{Shapley-Shubik Indices} (or {\em Shapley values}) of
$f$ provide a measure of how much control each voter can exert over the overall
outcome of the vote. Shapley-Shubik indices were introduced by Lloyd Shapley
and Martin Shubik in 1954 \cite{SS54} and are widely studied in social choice
theory as a measure of the &quot;influence&quot; of voters. The \emph{Inverse Shapley
Value Problem} is the problem of designing a weighted voting scheme which
(approximately) achieves a desired input vector of values for the
Shapley-Shubik indices. Despite much interest in this problem no provably
correct and efficient algorithm was known prior to our work.
  We give the first efficient algorithm with provable performance guarantees
for the Inverse Shapley Value Problem. For any constant $\eps &gt; 0$ our
algorithm runs in fixed poly$(n)$ time (the degree of the polynomial is
independent of $\eps$) and has the following performance guarantee: given as
input a vector of desired Shapley values, if any &quot;reasonable&quot; weighted voting
scheme (roughly, one in which the threshold is not too skewed) approximately
matches the desired vector of values to within some small error, then our
algorithm explicitly outputs a weighted voting scheme that achieves this vector
of Shapley values to within error $\eps.$ If there is a &quot;reasonable&quot; voting
scheme in which all voting weights are integers at most $\poly(n)$ that
approximately achieves the desired Shapley values, then our algorithm runs in
time $\poly(n)$ and outputs a weighted voting scheme that achieves the target
vector of Shapley values to within error $\eps=n^{-1/8}.$
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5139</identifier>
 <datestamp>2012-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5139</id><created>2012-12-19</created><authors><author><keyname>Zhang</keyname><forenames>Jinjin</forenames></author><author><keyname>Zhu</keyname><forenames>Zhaohui</forenames></author></authors><title>A Modal Characterization of Alternating Approximate Bisimilarity</title><categories>cs.LO</categories><comments>27 pages, 2 figures</comments><acm-class>D.2.4; F.3.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, alternating transition systems are adopted to describe control
systems with disturbances and their finite abstract systems. In order to
capture the equivalence relation between these systems, a notion of alternating
approximate bisimilarity is introduced. This paper aims to establish a modal
characterization for alternating approximate bisimilarity. Moreover, based on
this result, we provide a link between specifications satisfied by the samples
of control systems with disturbances and their finite abstractions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5156</identifier>
 <datestamp>2014-08-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5156</id><created>2012-12-20</created><updated>2014-08-28</updated><authors><author><keyname>Genovese</keyname><forenames>Christopher R.</forenames></author><author><keyname>Perone-Pacifico</keyname><forenames>Marco</forenames></author><author><keyname>Verdinelli</keyname><forenames>Isabella</forenames></author><author><keyname>Wasserman</keyname><forenames>Larry</forenames></author></authors><title>Nonparametric ridge estimation</title><categories>math.ST cs.LG stat.ML stat.TH</categories><comments>Published in at http://dx.doi.org/10.1214/14-AOS1218 the Annals of
  Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical
  Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOS-AOS1218</report-no><journal-ref>Annals of Statistics, Vol. 42, No. 4, 1511-1545 (2014)</journal-ref><doi>10.1214/14-AOS1218</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of estimating the ridges of a density function. Ridge
estimation is an extension of mode finding and is useful for understanding the
structure of a density. It can also be used to find hidden structure in point
cloud data. We show that, under mild regularity conditions, the ridges of the
kernel density estimator consistently estimate the ridges of the true density.
When the data are noisy measurements of a manifold, we show that the ridges are
close and topologically similar to the hidden manifold. To find the estimated
ridges in practice, we adapt the modified mean-shift algorithm proposed by
Ozertem and Erdogmus [J. Mach. Learn. Res. 12 (2011) 1249-1286]. Some numerical
experiments verify that the algorithm is accurate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5170</identifier>
 <datestamp>2012-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5170</id><created>2012-12-19</created><authors><author><keyname>Wang</keyname><forenames>Zhen</forenames></author><author><keyname>Lin</keyname><forenames>Felix Xiaozhu</forenames></author><author><keyname>Zhong</keyname><forenames>Lin</forenames></author><author><keyname>Chishtie</keyname><forenames>Mansoor</forenames></author></authors><title>Guadalupe: a browser design for heterogeneous hardware</title><categories>cs.OH</categories><report-no>Rice University ECE Technical Report 2012-12-19</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile systems are embracing heterogeneous architectures by getting more
types of cores and more specialized cores, which allows applications to be
faster and more efficient. We aim at exploiting the hardware heterogeneity from
the browser without requiring any changes to either the OS or the web
applications. Our design, Guadalupe, can use hardware processing units with
different degrees of capability for matched browser services. It starts with a
weak hardware unit, determines if and when a strong unit is needed, and
seamlessly migrates to the strong one when necessary. Guadalupe not only makes
more computing resources available to mobile web browsing but also improves its
energy proportionality. Based on Chrome for Android and TI OMAP4, We provide a
prototype browser implementation for resource loading and rendering. Compared
to Chrome for Android, we show that Guadalupe browser for rendering can
increase other 3D application's frame rate by up to 767% and save 4.7% of the
entire system's energy consumption. More importantly, by using the two cases,
we demonstrate that Guadalupe creates the great opportunity for many browser
services to get better resource utilization and energy proportionality by
exploiting hardware heterogeneity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5182</identifier>
 <datestamp>2013-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5182</id><created>2012-12-20</created><updated>2012-12-30</updated><authors><author><keyname>Enam</keyname><forenames>Farhana</forenames></author><author><keyname>Rabbani</keyname><forenames>Md. Arif</forenames></author><author><keyname>Islam</keyname><forenames>Md. Ashraful</forenames></author><author><keyname>Sarker</keyname><forenames>Sohag</forenames></author></authors><title>Performance Evaluation of an Orthogonal Frequency Division Multiplexing
  based Wireless Communication System with implementation of Least Mean Square
  Equalization technique</title><categories>cs.IT math.IT</categories><comments>5 pages,4 figures</comments><report-no>IJCSIS-31101218</report-no><journal-ref>International Journal of Computer Science and Information
  Security(IJCSIS), November 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Orthogonal Frequency Division Multiplexing (OFDM) has recently been applied
in wireless communication systems due to its high data rate transmission
capability with high bandwidth efficiency and its robustness to multi-path
delay. Fading is the one of the major aspect which is considered in the
receiver. To cancel the effect of fading, channel estimation and equalization
procedure must be done at the receiver before data demodulation. This paper
mainly deals with pilot based channel estimation techniques for OFDM
communication over frequency selective fading channels. This paper proposes a
specific approach to channel equalization for Orthogonal Frequency Division
Multiplex (OFDM) systems. Inserting an equalizer realized as an adaptive system
before the FFT processing, the influence of variable delay and multi path could
be mitigated in order to remove or reduce considerably the guard interval and
to gain some spectral efficiency. The adaptive algorithm is based on adaptive
filtering with averaging (AFA) for parameter update. Based on the development
of a model of the OFDM system, through extensive computer simulations, we
investigate the performance of the channel equalized system. The results show
much higher convergence and adaptation rate compared to one of the most
frequently used algorithms - Least Mean Squares (LMS).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5188</identifier>
 <datestamp>2015-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5188</id><created>2012-12-20</created><authors><author><keyname>Curto</keyname><forenames>Carina</forenames></author><author><keyname>Itskov</keyname><forenames>Vladimir</forenames></author><author><keyname>Morrison</keyname><forenames>Katherine</forenames></author><author><keyname>Roth</keyname><forenames>Zachary</forenames></author><author><keyname>Walker</keyname><forenames>Judy L.</forenames></author></authors><title>Combinatorial neural codes from a mathematical coding theory perspective</title><categories>q-bio.NC cs.IT math.IT</categories><comments>26 pages, 8 figures</comments><journal-ref>Neural Computation, Vol 25(7), pp. 1891-1925, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Shannon's seminal 1948 work gave rise to two distinct areas of research:
information theory and mathematical coding theory. While information theory has
had a strong influence on theoretical neuroscience, ideas from mathematical
coding theory have received considerably less attention. Here we take a new
look at combinatorial neural codes from a mathematical coding theory
perspective, examining the error correction capabilities of familiar receptive
field codes (RF codes). We find, perhaps surprisingly, that the high levels of
redundancy present in these codes does not support accurate error correction,
although the error-correcting performance of RF codes &quot;catches up&quot; to that of
random comparison codes when a small tolerance to error is introduced. On the
other hand, RF codes are good at reflecting distances between represented
stimuli, while the random comparison codes are not. We suggest that a
compromise in error-correcting capability may be a necessary price to pay for a
neural code whose structure serves not only error correction, but must also
reflect relationships between stimuli.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5194</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5194</id><created>2012-12-20</created><updated>2014-06-20</updated><authors><author><keyname>Halevi</keyname><forenames>Gali</forenames></author><author><keyname>Moed</keyname><forenames>Henk F.</forenames></author></authors><title>International Scientific Migration and Collaboration Patterns Following
  a Bibliometrics Line of Investigation</title><categories>cs.DL</categories><comments>Author copy of a paper published in Scientometrics 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A bibliometric approach is explored to tracking international scientific
migration, based on an analysis of the affiliation countries of authors
publishing in peer reviewed journals indexed in Scopus. The paper introduces a
model that relates base concepts in the study of migration to bibliometric
constructs, and discusses the potentialities and limitations of a bibliometric
approach both with respect to data accuracy and interpretation. Synchronous and
asynchronous analyses are presented for 10 rapidly growing countries and 7
scientifically established countries. Rough error rates of the proposed
indicators are estimated. It is concluded that the bibliometric approach is
promising provided that its outcomes are interpreted with care, based on
insight into the limits and potentialities of the bibliometric approach, and
combined with complementary data, obtained, for instance, from researchers
Curricula Vitae or survey or questionnaire based data. Error rates for units of
assessment with indicator values based on sufficiently large numbers are
estimated to be fairly below 10 per cent, but can be expected to vary
substantially among countries of origin, especially between Asian countries and
Western countries.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5197</identifier>
 <datestamp>2013-05-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5197</id><created>2012-12-20</created><authors><author><keyname>Brown</keyname><forenames>Gavin</forenames></author><author><keyname>Kasprzyk</keyname><forenames>Alexander M.</forenames></author></authors><title>Seven new champion linear codes</title><categories>math.CO cs.IT math.IT</categories><comments>10 pages, 4 figures</comments><msc-class>14G50 (Primary), 52B20, 14M25 (Secondary)</msc-class><journal-ref>LMS J. Comput. Math. 16 (2013) 109-117</journal-ref><doi>10.1112/S1461157013000041</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We exhibit seven linear codes exceeding the current best known minimum
distance d for their dimension k and block length n. Each code is defined over
F_8, and their invariants [n,k,d] are given by [49,13,27], [49,14,26],
[49,16,24], [49,17,23], [49,19,21], [49,25,16] and [49,26,15]. Our method
includes an exhaustive search of all monomial evaluation codes generated by
points in the [0,5]x[0,5] lattice square.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5204</identifier>
 <datestamp>2012-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5204</id><created>2012-12-20</created><authors><author><keyname>Arya</keyname><forenames>Kapil</forenames></author><author><keyname>Denniston</keyname><forenames>Tyler</forenames></author><author><keyname>Visan</keyname><forenames>Ana-Maria</forenames></author><author><keyname>Cooperman</keyname><forenames>Gene</forenames></author></authors><title>FReD: Automated Debugging via Binary Search through a Process Lifetime</title><categories>cs.SE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reversible debuggers have been developed at least since 1970. Such a feature
is useful when the cause of a bug is close in time to the bug manifestation.
When the cause is far back in time, one resorts to setting appropriate
breakpoints in the debugger and beginning a new debugging session. For these
cases when the cause of a bug is far in time from its manifestation, bug
diagnosis requires a series of debugging sessions with which to narrow down the
cause of the bug.
  For such &quot;difficult&quot; bugs, this work presents an automated tool to search
through the process lifetime and locate the cause. As an example, the bug could
be related to a program invariant failing. A binary search through the process
lifetime suffices, since the invariant expression is true at the beginning of
the program execution, and false when the bug is encountered. An algorithm for
such a binary search is presented within the FReD (Fast Reversible Debugger)
software. It is based on the ability to checkpoint, restart and
deterministically replay the multiple processes of a debugging session. It is
based on GDB (a debugger), DMTCP (for checkpoint-restart), and a custom
deterministic record-replay plugin for DMTCP.
  FReD supports complex, real-world multithreaded programs, such as MySQL and
Firefox. Further, the binary search is robust. It operates on multi-threaded
programs, and takes advantage of multi-core architectures during replay.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5210</identifier>
 <datestamp>2013-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5210</id><created>2012-12-20</created><updated>2013-03-31</updated><authors><author><keyname>Saiu</keyname><forenames>Luca</forenames></author></authors><title>GNU epsilon - an extensible programming language</title><categories>cs.PL</categories><comments>172 pages, PhD thesis</comments><msc-class>68N15</msc-class><acm-class>D.3.2; F.3.2; D.3.4; F.3.2; D.3.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reductionism is a viable strategy for designing and implementing practical
programming languages, leading to solutions which are easier to extend,
experiment with and formally analyze. We formally specify and implement an
extensible programming language, based on a minimalistic first-order imperative
core language plus strong abstraction mechanisms, reflection and
self-modification features. The language can be extended to very high levels:
by using Lisp-style macros and code-to-code transforms which automatically
rewrite high-level expressions into core forms, we define closures and
first-class continuations on top of the core. Non-self-modifying programs can
be analyzed and formally reasoned upon, thanks to the language simple
semantics. We formally develop a static analysis and prove a soundness property
with respect to the dynamic semantics. We develop a parallel garbage collector
suitable to multi-core machines to permit efficient execution of parallel
programs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5211</identifier>
 <datestamp>2012-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5211</id><created>2012-12-20</created><authors><author><keyname>Havemann</keyname><forenames>Frank</forenames></author><author><keyname>Scharnhorst</keyname><forenames>Andrea</forenames></author></authors><title>Bibliometric Networks</title><categories>cs.DL cs.SI physics.soc-ph</categories><comments>20 pages, one figure, based on a German text by Havemann and
  Scharnhorst (Bibliometrische Netzwerke, published in: C. Stegbauer and R.
  H\&quot;au{\ss}ling (Eds.), Handbuch Netzwerkforschung, pp. 799-823. VS Verlag
  f\&quot;ur Sozialwissenschaften, 2010)</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This text is based on a translation of a chapter in a handbook about network
analysis (published in German) where we tried to make beginners familiar with
some basic notions and recent developments of network analysis applied to
bibliometric issues (Havemann and Scharnhorst 2010). We have added some recent
references.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5217</identifier>
 <datestamp>2012-12-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5217</id><created>2012-12-20</created><authors><author><keyname>Rodrigues</keyname><forenames>Rui</forenames></author><author><keyname>Couto</keyname><forenames>Paula</forenames></author></authors><title>A Neural Network Approach to ECG Denoising</title><categories>cs.CE cs.NE</categories><comments>15 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an ECG denoising method based on a feed forward neural network
with three hidden layers. Particulary useful for very noisy signals, this
approach uses the available ECG channels to reconstruct a noisy channel. We
tested the method, on all the records from Physionet MIT-BIH Arrhythmia
Database, adding electrode motion artifact noise. This denoising method
improved the perfomance of publicly available ECG analysis programs on noisy
ECG signals. This is an offline method that can be used to remove noise from
very corrupted Holter records.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5238</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5238</id><created>2012-12-20</created><authors><author><keyname>Mocanu</keyname><forenames>Delia</forenames></author><author><keyname>Baronchelli</keyname><forenames>Andrea</forenames></author><author><keyname>Gon&#xe7;alves</keyname><forenames>Bruno</forenames></author><author><keyname>Perra</keyname><forenames>Nicola</forenames></author><author><keyname>Vespignani</keyname><forenames>Alessandro</forenames></author></authors><title>The Twitter of Babel: Mapping World Languages through Microblogging
  Platforms</title><categories>physics.soc-ph cs.CL cs.SI</categories><journal-ref>PLoS One 8, E61981 (2013)</journal-ref><doi>10.1371/journal.pone.0061981</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Large scale analysis and statistics of socio-technical systems that just a
few short years ago would have required the use of consistent economic and
human resources can nowadays be conveniently performed by mining the enormous
amount of digital data produced by human activities. Although a
characterization of several aspects of our societies is emerging from the data
revolution, a number of questions concerning the reliability and the biases
inherent to the big data &quot;proxies&quot; of social life are still open. Here, we
survey worldwide linguistic indicators and trends through the analysis of a
large-scale dataset of microblogging posts. We show that available data allow
for the study of language geography at scales ranging from country-level
aggregation to specific city neighborhoods. The high resolution and coverage of
the data allows us to investigate different indicators such as the linguistic
homogeneity of different countries, the touristic seasonal patterns within
countries and the geographical distribution of different languages in
multilingual regions. This work highlights the potential of geolocalized
studies of open data sources to improve current analysis and develop indicators
for major social phenomena in specific communities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5250</identifier>
 <datestamp>2012-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5250</id><created>2012-12-18</created><authors><author><keyname>Lauret</keyname><forenames>Alfred Jean Philippe</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Boyer</keyname><forenames>Harry</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Riviere</keyname><forenames>Carine</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Bastide</keyname><forenames>Alain</forenames><affiliation>PIMENT</affiliation></author></authors><title>A genetic algorithm applied to the validation of building thermal models</title><categories>cs.NE</categories><proxy>ccsd</proxy><journal-ref>Energy and Buildings 37, 8 (2005) 858-866</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the coupling of a building thermal simulation code with
genetic algorithms (GAs). GAs are randomized search algorithms that are based
on the mechanisms of natural selection and genetics. We show that this coupling
allows the location of defective sub-models of a building thermal model i.e.
parts of model that are responsible for the disagreements between measurements
and model predictions. The method first of all is checked and validated on the
basis of a numerical model of a building taken as reference. It is then applied
to a real building case. The results show that the method could constitute an
efficient tool when checking the model validity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5252</identifier>
 <datestamp>2013-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5252</id><created>2012-12-18</created><updated>2013-04-15</updated><authors><author><keyname>Garde</keyname><forenames>Fran&#xe7;ois</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Boyer</keyname><forenames>Harry</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Celaire</keyname><forenames>Robert</forenames></author></authors><title>Bringing simulation to implementation: Presentation of a global approach
  in the design of passive solar buildings under humid tropical climates</title><categories>cs.CE physics.class-ph</categories><comments>arXiv admin note: text overlap with arXiv:1212.6241</comments><proxy>ccsd</proxy><journal-ref>Solar Energy 71, 2 (2001) 109-120</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In early 1995, a DSM pilot initiative has been launched in the French islands
of Guadeloupe and Reunion through a partnership between several public and
private partners (the French Public Utility EDF, the University of Reunion
Island, low cost housing companies, architects, energy consultants, etc...) to
set up standards to improve thermal design of new residential buildings in
tropical climates. This partnership led to defining optimized bio-climatic
urban planning and architectural designs featuring the use of passive cooling
architectural principles (solar shading, natural ventilation) and components,
as well as energy efficient systems and technologies. The design and sizing of
each architectural component on internal thermal comfort in building has been
assessed with a validated thermal and airflow building simulation software
(CODYRUN). These technical specifications have been edited in a reference
document which has been used to build over 300 new pilot dwellings through the
years 1996-1998 in Reunion Island and in Guadeloupe. An experimental monitoring
has been made in these first ECODOM dwellings in 1998 and 1999. It will result
in experimental validation of impact of the passive cooling strategies on
thermal comfort of occupants leading to modify specifications if necessary. The
paper present all the methodology used for the elaboration of ECODOM, from the
simulations to the experimental results. This follow up is important, as the
setting up of the ECODOM standard will be the first step towards the setting up
of thermal regulations in the French overseas territories, by the year 2002.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5253</identifier>
 <datestamp>2012-12-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5253</id><created>2012-12-18</created><authors><author><keyname>Fakra</keyname><forenames>Ali Hamada</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Miranville</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Boyer</keyname><forenames>Harry</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Guichard</keyname><forenames>St&#xe9;phane</forenames><affiliation>PIMENT</affiliation></author></authors><title>Development of a new model to predict indoor daylighting: Integration in
  CODYRUN software and validation</title><categories>cs.CE</categories><proxy>ccsd</proxy><journal-ref>Energy Conversion and Management 52, 7 (2011) 2724-2734</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many models exist in the scientific literature for determining indoor
daylighting values. They are classified in three categories: numerical,
simplified and empirical models. Nevertheless, each of these categories of
models are not convenient for every application. Indeed, the numerical model
requires high calculation time; conditions of use of the simplified models are
limited, and experimental models need not only important financial resources
but also a perfect control of experimental devices (e.g. scale model), as well
as climatic characteristics of the location (e.g. in situ experiment). In this
article, a new model based on a combination of multiple simplified models is
established. The objective is to improve this category of model. The
originality of our paper relies on the coupling of several simplified models of
indoor daylighting calculations. The accuracy of the simulation code,
introduced into CODYRUN software to simulate correctly indoor illuminance, is
then verified. Besides, the software consists of a numerical building
simulation code, developed in the Physics and Mathematical Engineering
Laboratory for Energy and Environment (P.I.M.E.N.T) at the University of
Reunion. Initially dedicated to the thermal, airflow and hydrous phenomena in
the buildings, the software has been completed for the calculation of indoor
daylighting. New models and algorithms - which rely on a semi-detailed approach
- will be presented in this paper. In order to validate the accuracy of the
integrated models, many test cases have been considered as analytical,
inter-software comparisons and experimental comparisons. In order to prove the
accuracy of the new model - which can properly simulate the illuminance - a
confrontation between the results obtained from the software (developed in this
research paper) and the major made at a given place is described in details. A
new statistical indicator to appreciate the margins of errors - named RSD
(Reliability of Software Degrees) - is also be defined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5255</identifier>
 <datestamp>2012-12-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5255</id><created>2012-12-18</created><authors><author><keyname>Lucas</keyname><forenames>Franck</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Mara</keyname><forenames>Thierry Alex</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Garde</keyname><forenames>Fran&#xe7;ois</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Boyer</keyname><forenames>Harry</forenames><affiliation>PIMENT</affiliation></author></authors><title>A Comparison between CODYRUN and TRNSYS, simulation models for thermal
  buildings behaviour</title><categories>cs.CE</categories><proxy>ccsd</proxy><journal-ref>Word Renewable Energy Congress, Florence : Italy (1998)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Simulation codes of thermal behaviour could significantly improve housing
construction design. Among the existing software, CODYRUN and TRNSYS are
calculations codes of different conceptions. CODYRUN is exclusively dedicated
to housing thermal behaviour, whereas TRNSYS is more generally used on any
thermal system. The purpose of this article is to compare these two instruments
in two different conditions . We will first modelize a mono-zone test cell, and
analyse the results by means of signal treatment methods. Then, we will
modelize a real case of multi-zone housing, representative of housing in wet
tropical climates. We could so evaluate influences of meteorological and
building description data on model errors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5256</identifier>
 <datestamp>2012-12-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5256</id><created>2012-12-18</created><authors><author><keyname>Boyer</keyname><forenames>H.</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Chabriat</keyname><forenames>J. P.</forenames><affiliation>LE2P</affiliation></author><author><keyname>Grondin-Perez</keyname><forenames>B.</forenames><affiliation>LE2P</affiliation></author><author><keyname>Tourrand</keyname><forenames>C.</forenames><affiliation>CETHIL</affiliation></author><author><keyname>Brau</keyname><forenames>J.</forenames><affiliation>CETHIL</affiliation></author></authors><title>Thermal Building Simulation and Computer Generation of Nodal Models</title><categories>cs.CE</categories><proxy>ccsd</proxy><journal-ref>Building and Environment 31, 3 (1996) 207-214</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The designer's preoccupation to reduce the energy needs and get a better
thermal quality of ambiances helped in the development of several packages
simulating the dynamic behaviour of buildings. This paper shows the adaptation
of a method of thermal analysis, the nodal analysis, linked to the case of
building's thermal behaviour. We take successively an interest in the case of
conduction into a wall, in the coupling with superficial exchanges and finally
in the constitution of thermal state models of the building. Big variations
existing from one building to another, it's necessary to build the thermal
model from the building description. This article shows the chosen method in
the case of our thermal simulation program for buildings, CODYRUN
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5260</identifier>
 <datestamp>2012-12-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5260</id><created>2012-12-20</created><authors><author><keyname>Boyer</keyname><forenames>Harry</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Miranville</keyname><forenames>Fr&#xe9;d&#xe9;ric</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Bigot</keyname><forenames>Dimitri</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Guichard</keyname><forenames>St&#xe9;phane</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Idriss</keyname><forenames>Ingar</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Jean</keyname><forenames>Aur&#xe9;lien</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Fakra</keyname><forenames>Ali Hamada</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Soubdhan</keyname><forenames>Ted</forenames></author></authors><title>Heat transfer in buildings : application to air solar heating and Trombe
  wall design</title><categories>cs.CE</categories><comments>Available from:
  http://www.intechopen.com/books/evaporation-condensation-and-heat-transfer/heat-transfer-in-buildings-application-to-solar-air-collector-and-trombe-wall-design</comments><proxy>ccsd</proxy><journal-ref>Evaporation, condensation and heat transfer, InTech (Ed.) (2011)
  227-244</journal-ref><doi>10.5772/23025</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The aim of this paper is to briefly recall heat transfer modes and explain
their integration within a software dedicated to building simulation (CODYRUN).
Detailed elements of the validation of this software are presented and two
applications are finally discussed. One concerns the modeling of a flat plate
air collector and the second focuses on the modeling of Trombe solar walls. In
each case, detailed modeling of heat transfer allows precise understanding of
thermal and energetic behavior of the studied structures. Recent decades have
seen a proliferation of tools for building thermal simulation. These
applications cover a wide spectrum from very simplified steady state models to
dynamic simulation ones, including computational fluid dynamics modules
(Clarke, 2001). These tools are widely available in design offices and
engineering firms. They are often used for the design of HVAC systems and still
subject to detailed research, particularly with respect to the integration of
new fields (specific insulation materials, lighting, pollutants transport,
etc.). Available from:
http://www.intechopen.com/books/evaporation-condensation-and-heat-transfer/heat-transfer-in-buildings-application-to-solar-air-collector-and-trombe-wall-design
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5262</identifier>
 <datestamp>2012-12-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5262</id><created>2012-12-20</created><authors><author><keyname>Boyer</keyname><forenames>Harry</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Garde</keyname><forenames>Fran&#xe7;ois</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Gatina</keyname><forenames>Jean Claude</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Brau</keyname><forenames>Jean</forenames><affiliation>CETHIL</affiliation></author></authors><title>A multimodel approach to building thermal simulation for design and
  research purposes</title><categories>cs.CE</categories><comments>cited By (since 1996) 20</comments><proxy>ccsd</proxy><journal-ref>Energy and Buildings 28, 1 (1998) 71-78</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The designers pre-occupation to reduce energy consumption and to achieve
better thermal ambience levels, has favoured the setting up of numerous
building thermal dynamic simulation programs. The progress in the modelling of
phenomenas and its transfer into the professional field has resulted in various
numerical approaches ranging from softwares dedicated to architects for design
use to tools for laboratory use by the expert thermal researcher. This analysis
shows that each approach tends to fulfil the specific needs of a certain kind
of manipulator only, in the building conception process. Our objective is
notably different as it is a tool which can be used from the very initial stage
of a construction project, to the energy audit for the existing building. In
each of these cases, the objective results, the precision advocated and the
time delay of the results are different parameters which call for a multiple
model approach of the building system
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5263</identifier>
 <datestamp>2012-12-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5263</id><created>2012-12-20</created><authors><author><keyname>Soubdhan</keyname><forenames>Ted</forenames><affiliation>GRER</affiliation></author><author><keyname>Mara</keyname><forenames>Thierry Alex</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Boyer</keyname><forenames>Harry</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Youn&#xe8;s</keyname><forenames>Anis</forenames><affiliation>IMFS</affiliation></author></authors><title>Use of BESTEST procedure to improve a building thermal simulation
  program</title><categories>cs.CE</categories><proxy>ccsd</proxy><journal-ref>World Renewable Energy Congress VI Renewables: The Energy for the
  21st Century World Renewable Energy Congress VI 1-7 July 2000 Brighton, UK,
  Elsevier (Ed.) (2000) 1800-1803</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Validation of building energy simulation programs is of major interest to
both users and modellers. To achieve such a task, it is essential to apply a
methodology based on a priori test and empirical validation. A priori test
consists in verifying that models embedded in a program and their
implementation are correct. this should be achieved before carrying out
experiments. The aim of this report is to present results from the application
of the BESTEST procedure to our code. We will emphasise the way it allows to
find bugs in our program and also how it permits to qualify models of heat
transfer by conduction
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5264</identifier>
 <datestamp>2012-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5264</id><created>2012-12-20</created><authors><author><keyname>Han</keyname><forenames>Yufei</forenames><affiliation>INRIA Rocquencourt</affiliation></author><author><keyname>Moutarde</keyname><forenames>Fabien</forenames><affiliation>CAOR</affiliation></author></authors><title>Statistical Traffic State Analysis in Large-scale Transportation
  Networks Using Locality-Preserving Non-negative Matrix Factorization</title><categories>cs.CE</categories><comments>IET Intelligent Transport Systems (2013)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Statistical traffic data analysis is a hot topic in traffic management and
control. In this field, current research progresses focus on analyzing traffic
flows of individual links or local regions in a transportation network. Less
attention are paid to the global view of traffic states over the entire
network, which is important for modeling large-scale traffic scenes. Our aim is
precisely to propose a new methodology for extracting spatio-temporal traffic
patterns, ultimately for modeling large-scale traffic dynamics, and long-term
traffic forecasting. We attack this issue by utilizing Locality-Preserving
Non-negative Matrix Factorization (LPNMF) to derive low-dimensional
representation of network-level traffic states. Clustering is performed on the
compact LPNMF projections to unveil typical spatial patterns and temporal
dynamics of network-level traffic states. We have tested the proposed method on
simulated traffic data generated for a large-scale road network, and reported
experimental results validate the ability of our approach for extracting
meaningful large-scale space-time traffic patterns. Furthermore, the derived
clustering results provide an intuitive understanding of spatial-temporal
characteristics of traffic flows in the large-scale network, and a basis for
potential long-term forecasting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5265</identifier>
 <datestamp>2012-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5265</id><created>2012-12-20</created><authors><author><keyname>Ghosh</keyname><forenames>Tamal</forenames></author><author><keyname>Dan</keyname><forenames>Pranab K</forenames></author></authors><title>An Effective Machine-Part Grouping Algorithm to Construct Manufacturing
  Cells</title><categories>cs.CE</categories><journal-ref>Proceedings of Conference on Industrial Engineering (NCIE 2011)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The machine-part cell formation problem consists of creating machine cells
and their corresponding part families with the objective of minimizing the
inter-cell and intra-cell movement while maximizing the machine utilization.
This article demonstrates a hybrid clustering approach for the cell formation
problem in cellular manufacturing that conjoins Sorenson s similarity
coefficient based method to form the production cells. Computational results
are shown over the test datasets obtained from the past literature. The hybrid
technique is shown to outperform the other methods proposed in literature and
including powerful soft computing approaches such as genetic algorithms,
genetic programming by exceeding the solution quality on the test problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5271</identifier>
 <datestamp>2015-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5271</id><created>2012-12-20</created><authors><author><keyname>Preen</keyname><forenames>Richard J.</forenames></author><author><keyname>Bull</keyname><forenames>Larry</forenames></author></authors><title>Towards the Evolution of Novel Vertical-Axis Wind Turbines</title><categories>cs.NE cs.AI</categories><comments>14 pages, 11 figures</comments><journal-ref>Proceedings of the 13th Annual UK Workshop on Computational
  Intelligence, UKCI 2013, pp. 74-81. IEEE Computer Society</journal-ref><doi>10.1109/UKCI.2013.6651290</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Renewable and sustainable energy is one of the most important challenges
currently facing mankind. Wind has made an increasing contribution to the
world's energy supply mix, but still remains a long way from reaching its full
potential. In this paper, we investigate the use of artificial evolution to
design vertical-axis wind turbine prototypes that are physically instantiated
and evaluated under approximated wind tunnel conditions. An artificial neural
network is used as a surrogate model to assist learning and found to reduce the
number of fabrications required to reach a higher aerodynamic efficiency,
resulting in an important cost reduction. Unlike in other approaches, such as
computational fluid dynamics simulations, no mathematical formulations are used
and no model assumptions are made.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5275</identifier>
 <datestamp>2012-12-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5275</id><created>2012-12-20</created><authors><author><keyname>Boyer</keyname><forenames>Harry</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Bastide</keyname><forenames>Alain</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Lauret</keyname><forenames>Alfred Jean Philippe</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Lucas</keyname><forenames>Franck</forenames></author></authors><title>A Picard Newton method to solve non linear airflow networks</title><categories>cs.CE</categories><comments>IASTED Modelling and Simulation Conference (MS0 2005), Orangestad :
  Aruba (2005)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In detailled buiding simulation models, airflow modelling and solving are
still open and crucial problems, specially in the case of open buildings as
encountered in tropical climates. As a consequence, wind speed conditioning
indoor thermal comfort or energy needs in case of air conditionning are uneasy
to predict. A first part of the problem is the lack of reliable and usable
large opening elementary modelling and another one concerns the numerical
solving of airflow network. This non linear pressure system is solved by
numerous methods mainly based on Newton Raphson (NR) method. This paper is
adressing this part of the difficulty, in our software CODYRUN. After model
checks, we propose to use Picard method (known also as fixed point) to
initialise zone pressures. A linear system (extracted from the non linear set
of equations) is solved around 10 times at each time step and NR uses this
result for initial values. Known to be uniformly but slowly convergent, this
method appears to be really powerful for the building pressure system. The
comparison of the methods in terms of number of iterations is illustrated using
a real test case experiment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5276</identifier>
 <datestamp>2012-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5276</id><created>2012-12-20</created><authors><author><keyname>Khouadjia</keyname><forenames>Mostepha Redouane</forenames><affiliation>INRIA Saclay - Ile de France</affiliation></author><author><keyname>Schoenauer</keyname><forenames>Marc</forenames><affiliation>INRIA Saclay - Ile de France, LRI</affiliation></author><author><keyname>Vidal</keyname><forenames>Vincent</forenames><affiliation>DCSD</affiliation></author><author><keyname>Dr&#xe9;o</keyname><forenames>Johann</forenames><affiliation>TRT</affiliation></author><author><keyname>Sav&#xe9;ant</keyname><forenames>Pierre</forenames><affiliation>TRT</affiliation></author></authors><title>Multi-Objective AI Planning: Evaluating DAE-YAHSP on a Tunable Benchmark</title><categories>cs.AI</categories><comments>7th International Conference on Evolutionary Multi-Criterion
  Optimization (2013) To appearr. arXiv admin note: text overlap with
  arXiv:0804.3965 by other authors</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  All standard AI planners to-date can only handle a single objective, and the
only way for them to take into account multiple objectives is by aggregation of
the objectives. Furthermore, and in deep contrast with the single objective
case, there exists no benchmark problems on which to test the algorithms for
multi-objective planning. Divide and Evolve (DAE) is an evolutionary planner
that won the (single-objective) deterministic temporal satisficing track in the
last International Planning Competition. Even though it uses intensively the
classical (and hence single-objective) planner YAHSP, it is possible to turn
DAE-YAHSP into a multi-objective evolutionary planner. A tunable benchmark
suite for multi-objective planning is first proposed, and the performances of
several variants of multi-objective DAE-YAHSP are compared on different
instances of this benchmark, hopefully paving the road to further
multi-objective competitions in AI planning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5284</identifier>
 <datestamp>2013-11-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5284</id><created>2012-12-20</created><authors><author><keyname>Perea-Vega</keyname><forenames>Diego</forenames></author><author><keyname>Frigon</keyname><forenames>Jean-Francois</forenames></author><author><keyname>Girard</keyname><forenames>Andre</forenames></author></authors><title>Dual-Based Bounds for Resource Allocation in Zero-forcing Beamforming
  OFDMA-SDMA Systems</title><categories>cs.IT math.IT</categories><comments>Accepted for publication in EURASIP Journal on Wireless
  Communications and Networking. arXiv admin note: substantial text overlap
  with arXiv:1110.1347</comments><journal-ref>EURASIP Journal on Wireless Communications and Networking 2013,
  2013:51</journal-ref><doi>10.1186/1687-1499-2013-51</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider multi-antenna base stations using orthogonal frequency division
multiple access and space division multiple access techniques to serve
single-antenna users. Some users, called real-time users, have minimum rate
requirements and must be served in the current time slot while others, called
non real-time users, do not have strict timing constraints and are served on a
best-effort basis. The resource allocation problem is to find the assignment of
users to subcarriers and the transmit beamforming vectors that maximize the
total user rates subject to power and minimum rate constraints. In general,
this is a nonlinear and non-convex program and the zero-forcing technique used
here makes it integer as well, exact optimal solutions cannot be computed in
reasonable time for realistic cases. For this reason, we present a technique to
compute both upper and lower bounds and show that these are quite close for
some realistic cases.
  First, we formulate the dual problem whose optimum provides an upper bound to
all feasible solutions. We then use a simple method to get a primal-feasible
point starting from the dual optimal solution, which is a lower bound on the
primal optimal solution. Numerical results for several cases show that the two
bounds are close so that the dual method can be used to benchmark any heuristic
used to solve this problem. As an example, we provide numerical results showing
the performance gap of the well-known weight adjustment method and show that
there is considerable room for improvement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5288</identifier>
 <datestamp>2012-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5288</id><created>2012-12-20</created><authors><author><keyname>Nabaee</keyname><forenames>Mahdy</forenames></author><author><keyname>Labeau</keyname><forenames>Fabrice</forenames></author></authors><title>Quantized Network Coding for Correlated Sources</title><categories>cs.IT math.IT</categories><comments>Submitted for IEEE Transactions on Signal Processing</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Non-adaptive joint source network coding of correlated sources is discussed
in this paper. By studying the information flow in the network, we propose
quantized network coding as an alternative for packet forwarding. This
technique has both network coding and distributed source coding advantages,
simultaneously. Quantized network coding is a combination of random linear
network coding in the (infinite) field of real numbers and quantization to cope
with the limited capacity of links. With the aid of the results in the
literature of compressed sensing, we discuss theoretical and practical
feasibility of quantized network coding in lossless networks. We show that, due
to the nature of the field it operates on, quantized network coding can provide
good quality decoding at a sink node with the reception of a reduced number of
packets. Specifically, we discuss the required conditions on local network
coding coefficients, by using restricted isometry property and suggest a
design, which yields in appropriate linear measurements. Finally, our
simulation results show the achieved gain in terms of delivery delay, compared
to conventional routing based packet forwarding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5289</identifier>
 <datestamp>2012-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5289</id><created>2012-12-20</created><authors><author><keyname>Guster</keyname><forenames>D.</forenames></author><author><keyname>Krivulin</keyname><forenames>N. K.</forenames></author></authors><title>Modeling and performance evaluation of computer systems security
  operation</title><categories>cs.CR cs.SY math.OC</categories><comments>Simulation 2001: 4th St. Petersburg Workshop on Simulation, St.
  Petersburg, Russia, June 18-22, 2001; ISBN 5-7997-0304-9</comments><msc-class>68M20 (Primary) 93C65, 15A80, 90B15, 90B22 (Secondary)</msc-class><journal-ref>Proc. 4th St. Petersburg Workshop on Simulation, 2001, pp. 233-238</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A model of computer system security operation is developed based on the
fork-join queueing network formalism. We introduce a security operation
performance measure, and show how it may be used to performance evaluation of
actual systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5291</identifier>
 <datestamp>2012-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5291</id><created>2012-12-20</created><authors><author><keyname>Krivulin</keyname><forenames>N. K.</forenames></author></authors><title>Products of random matrices and queueing system performance evaluation</title><categories>math.OC cs.SY</categories><comments>Simulation 2001: St. Petersburg Workshop on Simulation, St.
  Petersburg, Russia, June 18-22, 2001. ISBN 5-7997-0304-9</comments><msc-class>15A80 (Primary) 68M20, 93C65, 90B15, 37H15 (Secondary)</msc-class><journal-ref>Proc. 4th St. Petersburg Workshop on Simulation, 2001, pp. 304-309</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider (max,+)-algebra products of random matrices, which arise from
performance evaluation of acyclic fork-join queueing networks. A new algebraic
technique to examine properties of the product and investigate its limiting
behaviour is proposed based on an extension of the standard matrix
(max,+)-algebra by endowing it with the ordinary matrix addition as an external
operation. As an application, we derive bounds on the (max,+)-algebra maximal
Lyapunov exponent which can be considered as the cycle time of the networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5300</identifier>
 <datestamp>2014-05-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5300</id><created>2012-12-20</created><updated>2014-05-13</updated><authors><author><keyname>Bai</keyname><forenames>Jingwen</forenames></author><author><keyname>Sabharwal</keyname><forenames>Ashutosh</forenames></author></authors><title>Distributed Full-duplex via Wireless Side Channels: Bounds and Protocols</title><categories>cs.IT math.IT</categories><comments>Published in IEEE Transactions on Wireless Communications, August
  2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study a three-node full-duplex network, where a base
station is engaged in simultaneous up- and downlink communication in the same
frequency band with two half-duplex mobile nodes. To reduce the impact of
inter- node interference between the two mobile nodes on the system capacity,
we study how an orthogonal side-channel between the two mobile nodes can be
leveraged to achieve full-duplex-like multiplexing gains. We propose and
characterize the achievable rates of four distributed full-duplex schemes,
labeled bin-and- cancel, compress-and-cancel, estimate-and-cancel and decode-
and-cancel. Of the four, bin-and-cancel is shown to achieve within 1 bit/s/Hz
of the capacity region for all values of channel parameters. In contrast, the
other three schemes achieve the near-optimal performance only in certain
regimes of channel values. Asymptotic multiplexing gains of all proposed
schemes are derived to show that the side-channel is extremely effective in
regimes where inter-node interference has the highest impact.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5303</identifier>
 <datestamp>2015-07-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5303</id><created>2012-12-20</created><updated>2015-07-24</updated><authors><author><keyname>Spivak</keyname><forenames>David I.</forenames></author><author><keyname>Wisnesky</keyname><forenames>Ryan</forenames></author></authors><title>Relational Foundations For Functorial Data Migration</title><categories>cs.DB math.CT math.LO</categories><msc-class>18A40, 68P15, 03B70</msc-class><acm-class>H.2; H.3.3</acm-class><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  We study the data transformation capabilities associated with schemas that
are presented by directed multi-graphs and path equations. Unlike most
approaches which treat graph-based schemas as abbreviations for relational
schemas, we treat graph-based schemas as categories. A schema $S$ is a
finitely-presented category, and the collection of all $S$-instances forms a
category, $S$-inst. A functor $F$ between schemas $S$ and $T$, which can be
generated from a visual mapping between graphs, induces three adjoint data
migration functors, $\Sigma_F:S$-inst$\to T$-inst, $\Pi_F: S$-inst $\to
T$-inst, and $\Delta_F:T$-inst $\to S$-inst. We present an algebraic query
language FQL based on these functors, prove that FQL is closed under
composition, prove that FQL can be implemented with the
select-project-product-union relational algebra (SPCU) extended with a
key-generation operation, and prove that SPCU can be implemented with FQL.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5315</identifier>
 <datestamp>2012-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5315</id><created>2012-12-20</created><authors><author><keyname>Zeng</keyname><forenames>Xianyi</forenames></author></authors><title>A hybrid finite difference--finite volume approach to solve first-order
  hyperbolic conservation laws with superior accuracy</title><categories>math.NA cs.CE cs.NA</categories><comments>35 pages, 14 figures, submitted to SIAM J Sci. Comput. on 09/12/2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A hybrid finite difference--finite volume (FD-FV) approach for discretization
in space is proposed to solve first-order hyperbolic conservation laws. Unlike
any conventional finite difference method (FDM) or finite volume method (FVM),
this approach uses both cell-averaged values and nodal values as degrees of
freedom (DOF). Consequently it is inherently conservative like FVM and easy to
extend to high-order accuracy in space like FDM. The proposed FD-FV approach
works for arbitrary flux functions, whether convex or non-convex; and it does
not require any exact or approximate Riemann solver hence it is also
computationally economical. Method of lines is adopted for time integration in
present work; in particular, explicit Runge-Kutta methods are employed. It is
theoretically proven and numerically confirmed that in general, the proposed
FD-FV methods possess superior accuracy than conventional FDM or FVM. Linear
stability is studied for general FD-FV schemes -- both space-accurate and
time-stable FD-FV schemes of up to fifth-order accuracy in both space and time
are presented. Numerical examples show that as long as the solutions are
smooth, the proposed FD-FV methods are more efficient than conventional FVM of
the same order, at least when explicit time-integrators are applied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5316</identifier>
 <datestamp>2013-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5316</id><created>2012-12-20</created><updated>2013-06-25</updated><authors><author><keyname>Wilde</keyname><forenames>Mark M.</forenames></author><author><keyname>Datta</keyname><forenames>Nilanjana</forenames></author><author><keyname>Hsieh</keyname><forenames>Min-Hsiu</forenames></author><author><keyname>Winter</keyname><forenames>Andreas</forenames></author></authors><title>Quantum rate distortion coding with auxiliary resources</title><categories>quant-ph cs.IT math.IT</categories><comments>18 pages, 4 figures, IEEE format; v3: accepted into IEEE Transactions
  on Information Theory with minor changes</comments><journal-ref>IEEE Transactions on Information Theory vol. 59, no. 10, pages
  6755-6773 (October 2013)</journal-ref><doi>10.1109/TIT.2013.2271772</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We extend quantum rate distortion theory by considering auxiliary resources
that might be available to a sender and receiver performing lossy quantum data
compression. The first setting we consider is that of quantum rate distortion
coding with the help of a classical side channel. Our result here is that the
regularized entanglement of formation characterizes the quantum rate distortion
function, extending earlier work of Devetak and Berger. We also combine this
bound with the entanglement-assisted bound from our prior work to obtain the
best known bounds on the quantum rate distortion function for an isotropic
qubit source. The second setting we consider is that of quantum rate distortion
coding with quantum side information (QSI) available to the receiver. In order
to prove results in this setting, we first state and prove a quantum reverse
Shannon theorem with QSI (for tensor-power states), which extends the known
tensor-power quantum reverse Shannon theorem. The achievability part of this
theorem relies on the quantum state redistribution protocol, while the converse
relies on the fact that the protocol can cause only a negligible disturbance to
the joint state of the reference and the receiver's QSI. This quantum reverse
Shannon theorem with QSI naturally leads to quantum rate-distortion theorems
with QSI, with or without entanglement assistance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5324</identifier>
 <datestamp>2016-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5324</id><created>2012-12-20</created><updated>2016-03-01</updated><authors><author><keyname>Kauers</keyname><forenames>Manuel</forenames></author><author><keyname>O'Donnell</keyname><forenames>Ryan</forenames></author><author><keyname>Tan</keyname><forenames>Li-Yang</forenames></author><author><keyname>Zhou</keyname><forenames>Yuan</forenames></author></authors><title>Hypercontractive inequalities via SOS, and the Frankl--R\&quot;odl graph</title><categories>cs.CC</categories><license>http://creativecommons.org/licenses/by/4.0/</license><abstract>  Our main result is a formulation and proof of the reverse hypercontractive
inequality in the sum-of-squares (SOS) proof system. As a consequence we show
that for any constant $0 &lt; \gamma \leq 1/4$, the SOS/Lasserre SDP hierarchy at
degree $4\lceil \frac{1}{4\gamma}\rceil$ certifies the statement &quot;the maximum
independent set in the Frankl--R\&quot;odl graph $\mathrm{FR}^{n}_{\gamma}$ has
fractional size~$o(1)$&quot;. Here $\mathrm{FR}^{n}_{\gamma} = (V,E)$ is the graph
with $V = \{0,1\}^n$ and $(x,y) \in E$ whenever $\Delta(x,y) = (1-\gamma)n$ (an
even integer). In particular, we show the degree-$4$ SOS algorithm certifies
the chromatic number lower bound &quot;$\chi(\mathrm{FR}^{n}_{1/4}) = \omega(1)$&quot;,
even though $\mathrm{FR}^{n}_{1/4}$ is the canonical integrality gap instance
for which standard SDP relaxations cannot even certify
&quot;$\chi(\mathrm{FR}^{n}_{1/4}) &gt; 3$&quot;. Finally, we also give an SOS proof of (a
generalization of) the sharp $(2,q)$-hypercontractive inequality for any even
integer $q$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5331</identifier>
 <datestamp>2013-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5331</id><created>2012-12-20</created><updated>2013-01-16</updated><authors><author><keyname>Albaham</keyname><forenames>Ameer Tawfik</forenames></author><author><keyname>Salim</keyname><forenames>Naomie</forenames></author></authors><title>Adapting Voting Techniques for Online Forum Thread Retrieval</title><categories>cs.IR</categories><comments>The original publication is available at
  http://www.springerlink.com/. Fixing minor typos. arXiv admin note: text
  overlap with arXiv:1212.5590</comments><journal-ref>Advanced Machine Learning Technologies and Applications, 2012,
  322, 439-448</journal-ref><doi>10.1007/978-3-642-35326-0_44</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online forums or message boards are rich knowledge-based communities. In
these communities, thread retrieval is an essential tool facilitating
information access. However, the issue on thread search is how to combine
evidence from text units(messages) to estimate thread relevance. In this paper,
we first rank a list of messages, then we score threads by aggregating their
ranked messages' scores. To aggregate the message scores, we adopt several
voting techniques that have been applied in ranking aggregates tasks such as
blog distillation and expert finding. The experimental result shows that many
voting techniques should be preferred over a baseline that treats a thread as a
concatenation of its message texts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5352</identifier>
 <datestamp>2012-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5352</id><created>2012-12-21</created><authors><author><keyname>Chua</keyname><forenames>Kah Keong</forenames></author><author><keyname>Tay</keyname><forenames>Yong Haur</forenames></author></authors><title>On the Adaptability of Neural Network Image Super-Resolution</title><categories>cs.CV</categories><comments>Image Super Resolution, Neural Network, Multilayer Perceptron, Mean
  Squared Error, Peak Signal-to-Noise Ratio, Structural Similarity Index</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we described and developed a framework for Multilayer
Perceptron (MLP) to work on low level image processing, where MLP will be used
to perform image super-resolution. Meanwhile, MLP are trained with different
types of images from various categories, hence analyse the behaviour and
performance of the neural network. The tests are carried out using qualitative
test, in which Mean Squared Error (MSE), Peak Signal-to-Noise Ratio (PSNR) and
Structural Similarity Index (SSIM). The results showed that MLP trained with
single image category can perform reasonably well compared to methods proposed
by other researchers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5353</identifier>
 <datestamp>2012-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5353</id><created>2012-12-21</created><authors><author><keyname>De</keyname><forenames>Minati</forenames></author><author><keyname>Nandy</keyname><forenames>Subhas C.</forenames></author><author><keyname>Roy</keyname><forenames>Sasanka</forenames></author></authors><title>Convex Hull and Linear Programming in Read-only Setup with Limited
  Work-space</title><categories>cs.CG cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Prune-and-search is an important paradigm for solving many important
geometric problems. We show that the general prune-and-search technique can be
implemented where the objects are given in read-only memory. As examples we
consider convex-hull in 2D, and linear programming in 2D and 3D. For the
convex-hull problem, designing sub-quadratic algorithm in a read-only setup
with sub-linear space is an open problem for a long time. We first propose a
simple algorithm for this problem that runs in $O(n^{3/2+\epsilon)}$ time and
$O(n^(1/2))$ space. Next, we consider a restricted version of the problem where
the points in $P$ are given in sorted order with respect to their
$x$-coordinates in a read-only array. For the linear programming problems, the
constraints are given in the read-only array. The last three algorithms use
{\it prune-and-search}, and their time and extra work-space complexities are
$O(n^{1 + \epsilon})$ and $O(\log n)$ respectively, where $\epsilon$ is a small
constant satisfying $\sqrt{\frac{\log\log n}{\log n}} &lt; \epsilon &lt; 1$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5359</identifier>
 <datestamp>2012-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5359</id><created>2012-12-21</created><authors><author><keyname>Dhanalakshmi</keyname><forenames>K.</forenames></author><author><keyname>Inbarani</keyname><forenames>H. Hannah</forenames></author></authors><title>Fuzzy soft rough K-Means clustering approach for gene expression data</title><categories>cs.LG cs.CE</categories><comments>7 pages, IJSER Vol.3 Issue: 10 Oct 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Clustering is one of the widely used data mining techniques for medical
diagnosis. Clustering can be considered as the most important unsupervised
learning technique. Most of the clustering methods group data based on distance
and few methods cluster data based on similarity. The clustering algorithms
classify gene expression data into clusters and the functionally related genes
are grouped together in an efficient manner. The groupings are constructed such
that the degree of relationship is strong among members of the same cluster and
weak among members of different clusters. In this work, we focus on a
similarity relationship among genes with similar expression patterns so that a
consequential and simple analytical decision can be made from the proposed
Fuzzy Soft Rough K-Means algorithm. The algorithm is developed based on Fuzzy
Soft sets and Rough sets. Comparative analysis of the proposed work is made
with bench mark algorithms like K-Means and Rough K-Means and efficiency of the
proposed algorithm is illustrated in this work by using various cluster
validity measures such as DB index and Xie-Beni index.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5374</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5374</id><created>2012-12-21</created><authors><author><keyname>Zheng</keyname><forenames>Zhong</forenames></author><author><keyname>Wei</keyname><forenames>Lu</forenames></author><author><keyname>H&#xe4;m&#xe4;l&#xe4;inen</keyname><forenames>Jyri</forenames></author><author><keyname>Tirkkonen</keyname><forenames>Olav</forenames></author></authors><title>A Blind Time-Reversal Detector in the Presence of Channel Correlation</title><categories>cs.IT cs.PF math.IT</categories><comments>4 pages, 2 figures. Submitted to IEEE Signal Processing Letters</comments><doi>10.1109/LSP.2013.2251880</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A blind target detector using the time reversal transmission is proposed in
the presence of channel correlation. We calculate the exact moments of the test
statistics involved. The derived moments are used to construct an accurate
approximative Likelihood Ratio Test (LRT) based on multivariate Edgeworth
expansion. Performance gain over an existing detector is observed in scenarios
with channel correlation and relatively strong target signal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5378</identifier>
 <datestamp>2014-03-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5378</id><created>2012-12-21</created><updated>2014-03-11</updated><authors><author><keyname>Mitici</keyname><forenames>Mihaela</forenames></author><author><keyname>Onderwater</keyname><forenames>Martijn</forenames></author><author><keyname>de Graaf</keyname><forenames>Maurits</forenames></author><author><keyname>van Ommeren</keyname><forenames>Jan-Kees</forenames></author><author><keyname>van Dijk</keyname><forenames>Nico</forenames></author><author><keyname>Goseling</keyname><forenames>Jasper</forenames></author><author><keyname>Boucherie</keyname><forenames>Richard J.</forenames></author></authors><title>An Optimal Query Assignment for Wireless Sensor Networks</title><categories>math.OC cs.NI</categories><comments>27 pages, 20 figures</comments><msc-class>93E20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A trade-off between two QoS requirements of wireless sensor networks: query
waiting time and validity (age) of the data feeding the queries, is
investigated. We propose a Continuous Time Markov Decision Process with a drift
that trades-off between the two QoS requirements by assigning incoming queries
to the wireless sensor network or to the database. To compute an optimal
assignment policy, we argue, by means of non-standard uniformization, a
discrete time Markov decision process, stochastically equivalent to the initial
continuous process. We determine an optimal query assignment policy for the
discrete time process by means of dynamic programming. Next, we assess
numerically the performance of the optimal policy and show that it outperforms
in terms of average assignment costs three other heuristics, commonly used in
practice. Lastly, the optimality of the our model is confirmed also in the case
of real query traffic, where our proposed policy achieves significant cost
savings compared to the heuristics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5389</identifier>
 <datestamp>2012-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5389</id><created>2012-12-21</created><authors><author><keyname>Stendardo</keyname><forenames>Nabil</forenames></author><author><keyname>Kalousis</keyname><forenames>Alexandros</forenames></author></authors><title>Relationship-aware sequential pattern mining</title><categories>cs.DB stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Relationship-aware sequential pattern mining is the problem of mining
frequent patterns in sequences in which the events of a sequence are mutually
related by one or more concepts from some respective hierarchical taxonomies,
based on the type of the events. Additionally events themselves are also
described with a certain number of taxonomical concepts. We present RaSP an
algorithm that is able to mine relationship-aware patterns over such sequences;
RaSP follows a two stage approach. In the first stage it mines for frequent
type patterns and {\em all} their occurrences within the different sequences.
In the second stage it performs hierarchical mining where for each frequent
type pattern and its occurrences it mines for more specific frequent patterns
in the lower levels of the taxonomies. We test RaSP on a real world medical
application, that provided the inspiration for its development, in which we
mine for frequent patterns of medical behavior in the antibiotic treatment of
microbes and show that it has a very good computational performance given the
complexity of the relationship-aware sequential pattern mining problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5391</identifier>
 <datestamp>2012-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5391</id><created>2012-12-21</created><authors><author><keyname>Jothi</keyname><forenames>G.</forenames></author><author><keyname>Inbarani</keyname><forenames>H. Hannah</forenames></author></authors><title>Soft Set Based Feature Selection Approach for Lung Cancer Images</title><categories>cs.LG cs.CE</categories><comments>7 pages, IJSER Vol.3 Issue . 10 Oct 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Lung cancer is the deadliest type of cancer for both men and women. Feature
selection plays a vital role in cancer classification. This paper investigates
the feature selection process in Computed Tomographic (CT) lung cancer images
using soft set theory. We propose a new soft set based unsupervised feature
selection algorithm. Nineteen features are extracted from the segmented lung
images using gray level co-occurence matrix (GLCM) and gray level different
matrix (GLDM). In this paper, an efficient Unsupervised Soft Set based Quick
Reduct (SSUSQR) algorithm is presented. This method is used to select features
from the data set and compared with existing rough set based unsupervised
feature selection methods. Then K-Means and Self Organizing Map (SOM)
clustering algorithms are used to cluster the data. The performance of the
feature selection algorithms is evaluated based on performance of clustering
techniques. The results show that the proposed method effectively removes
redundant features.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5394</identifier>
 <datestamp>2012-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5394</id><created>2012-12-21</created><authors><author><keyname>Luo</keyname><forenames>Yaming</forenames></author><author><keyname>Zhang</keyname><forenames>Jun</forenames></author><author><keyname>Letaief</keyname><forenames>Khaled B.</forenames></author></authors><title>Optimal Scheduling and Power Allocation for Two-Hop Energy Harvesting
  Communication Systems</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transaction on Wireless Communication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Energy harvesting (EH) has recently emerged as a promising technique for
green communications. To realize its potential, communication protocols need to
be redesigned to combat the randomness of the harvested energy. In this paper,
we investigate how to apply relaying to improve the short-term performance of
EH communication systems. With an EH source and a non-EH half-duplex relay, we
consider two different design objectives: 1) short-term throughput
maximization; and 2) transmission completion time minimization. Both problems
are joint scheduling and power allocation problems, rendered quite challenging
by the half-duplex constraint at the relay. A key finding is that directional
water-filling (DWF), which is the optimal power allocation algorithm for the
single-hop EH system, can serve as guideline for the design of two-hop
communication systems, as it not only determines the value of the optimal
performance, but also forms the basis to derive optimal solutions for both
design problems. Based on a relaxed energy profile along with the DWF
algorithm, we derive key properties of the optimal solutions for both problems
and thereafter propose efficient algorithms. Simulation results will show that
both scheduling and power allocation optimizations are necessary in two-hop EH
communication systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5398</identifier>
 <datestamp>2013-11-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5398</id><created>2012-12-21</created><updated>2013-11-10</updated><authors><author><keyname>Vejdemo-Johansson</keyname><forenames>Mikael</forenames></author></authors><title>Sketches of a platypus: persistent homology and its algebraic
  foundations</title><categories>math.AT cs.CG</categories><comments>22 pages, 4 figures, accepted for publication in an upcoming volume
  of AMS Contemporary Mathematics</comments><msc-class>55N35, 13C60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The subject of persistent homology has vitalized applications of algebraic
topology to point cloud data and to application fields far outside the realm of
pure mathematics. The area has seen several fundamentally important results
that are rooted in choosing a particular algebraic foundational theory to
describe persistent homology, and applying results from that theory to prove
useful and important results.
  In this survey paper, we shall examine the various choices in use, and what
they allow us to prove. We shall also discuss the inherent differences between
the choices people use, and speculate on potential directions of research to
resolve these differences.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5404</identifier>
 <datestamp>2013-06-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5404</id><created>2012-12-21</created><updated>2013-06-19</updated><authors><author><keyname>Wen</keyname><forenames>Chuan</forenames></author><author><keyname>Loe</keyname></author><author><keyname>Jensen</keyname><forenames>Henrik Jeldtoft</forenames></author></authors><title>Edge Union of Networks on the Same Vertex Set</title><categories>physics.soc-ph cond-mat.stat-mech cs.SI</categories><comments>18 pages and 8 figures</comments><journal-ref>2013 J. Phys. A: Math. Theor. 46 245002</journal-ref><doi>10.1088/1751-8113/46/24/245002</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Random networks generators like Erdoes-Renyi, Watts-Strogatz and
Barabasi-Albert models are used as models to study real-world networks. Let
G^1(V,E_1) and G^2(V,E_2) be two such networks on the same vertex set V. This
paper studies the degree distribution and cluster coefficient of the resultant
networks, G(V, E_1 U E_2).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5406</identifier>
 <datestamp>2013-12-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5406</id><created>2012-12-21</created><updated>2013-06-25</updated><authors><author><keyname>Nasir</keyname><forenames>A. A.</forenames></author><author><keyname>Zhou</keyname><forenames>X.</forenames></author><author><keyname>Durrani</keyname><forenames>S.</forenames></author><author><keyname>Kennedy</keyname><forenames>R. A.</forenames></author></authors><title>Relaying Protocols for Wireless Energy Harvesting and Information
  Processing</title><categories>cs.IT math.IT</categories><comments>30 pages, accepted to appear in IEEE Transactions on Wireless
  Communications</comments><journal-ref>IEEE Transactions on Wireless Communications, vol. 12, no. 7, pp.
  3622--3636, July 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An emerging solution for prolonging the lifetime of energy constrained relay
nodes in wireless networks is to avail the ambient radio-frequency (RF) signal
and to simultaneously harvest energy and process information. In this paper, an
amplify-and-forward (AF) relaying network is considered, where an energy
constrained relay node harvests energy from the received RF signal and uses
that harvested energy to forward the source information to the destination.
Based on the time switching and power splitting receiver architectures, two
relaying protocols, namely, i) time switching-based relaying (TSR) protocol and
ii) power splitting-based relaying (PSR) protocol are proposed to enable energy
harvesting and information processing at the relay. In order to determine the
throughput, analytical expressions for the outage probability and the ergodic
capacity are derived for delay-limited and delay-tolerant transmission modes,
respectively. The numerical analysis provides practical insights into the
effect of various system parameters, such as energy harvesting time, power
splitting ratio, source transmission rate, source to relay distance, noise
power, and energy harvesting efficiency, on the performance of wireless energy
harvesting and information processing using AF relay nodes. In particular, the
TSR protocol outperforms the PSR protocol in terms of throughput at relatively
low signal-to-noise-ratios and high transmission rate.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5417</identifier>
 <datestamp>2014-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5417</id><created>2012-12-21</created><authors><author><keyname>Davenport</keyname><forenames>James H.</forenames></author><author><keyname>Bradford</keyname><forenames>Russell</forenames></author><author><keyname>England</keyname><forenames>Matthew</forenames></author><author><keyname>Wilson</keyname><forenames>David</forenames></author></authors><title>Program Verification in the presence of complex numbers, functions with
  branch cuts etc</title><categories>cs.SC</categories><journal-ref>14th International Symposium on Symbolic and Numeric Algorithms
  for Scientific Computing (SYNASC '12), pp. 83--88. IEEE, 2012</journal-ref><doi>10.1109/SYNASC.2012.68</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In considering the reliability of numerical programs, it is normal to &quot;limit
our study to the semantics dealing with numerical precision&quot; (Martel, 2005). On
the other hand, there is a great deal of work on the reliability of programs
that essentially ignores the numerics. The thesis of this paper is that there
is a class of problems that fall between these two, which could be described as
&quot;does the low-level arithmetic implement the high-level mathematics&quot;. Many of
these problems arise because mathematics, particularly the mathematics of the
complex numbers, is more difficult than expected: for example the complex
function log is not continuous, writing down a program to compute an inverse
function is more complicated than just solving an equation, and many algebraic
simplification rules are not universally valid.
  The good news is that these problems are theoretically capable of being
solved, and are practically close to being solved, but not yet solved, in
several real-world examples. However, there is still a long way to go before
implementations match the theoretical possibilities.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5421</identifier>
 <datestamp>2012-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5421</id><created>2012-12-21</created><authors><author><keyname>Zungeru</keyname><forenames>A. M.</forenames></author><author><keyname>Garba</keyname><forenames>A. J.</forenames></author><author><keyname>Kolo</keyname><forenames>J. G.</forenames></author><author><keyname>Ahmed</keyname><forenames>M. S.</forenames></author><author><keyname>Olumide</keyname><forenames>I.</forenames></author></authors><title>Design of a Smart Embedded Uninterrupted Power Supply System for
  Personal Computers</title><categories>cs.SY</categories><comments>20 pages, 15 figures, 4 tables, journal paper</comments><journal-ref>International Journal of Embedded Systems and Applications
  (IJESA), vol. 2(4), pp. 1-19, 2012</journal-ref><doi>10.5121/ijesa.2012.2401</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Digital equipment such as computers, telecommunication systems and
instruments use microprocessors that operate at high frequencies allowing them
to carry out millions or even billions of operations per second. A disturbance
in the electrical supply lasting just a few milliseconds can affect thousands
or millions of basic operations. The result may be malfunctioning and loss of
data with dangerous or costly consequences (e.g. loss of production). That is
why many loads, called sensitive or critical loads, require a supply that is
protected. Many manufacturers of sensitive equipment specify very strict
tolerances, much stricter than those in the distribution system for the supply
of their equipment, one example being Computer Business Equipment Manufacturers
Association for computer equipment against distribution system disturbances.
The design of this uninterrupted power supply (UPS) for personal computer (PC)
is necessitated due to a need for enhanced portability in the design of
personal computer desktop workstations. Apart from its original functionality
as a backup source of power, this design incorporates the unit within the
system unit casing, thereby reducing the number of system components available.
Also, the embedding of this unit removes the untidiness of connecting wires and
makes the whole computer act like a laptop. Not to be left out is the choice of
a microcontroller as an important part of the circuitry. This has eliminated
the weight and space-consuming components that make up an original design. The
singular use of this microcontroller places the UPS under the class of an
advanced technology device.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5423</identifier>
 <datestamp>2015-05-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5423</id><created>2012-12-21</created><updated>2015-05-01</updated><authors><author><keyname>Parambath</keyname><forenames>Shameem A Puthiya</forenames></author></authors><title>Topic Extraction and Bundling of Related Scientific Articles</title><categories>cs.IR cs.DL stat.ML</categories><comments>NeSeFo 2012</comments><report-no>NeSeFo 2012</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Automatic classification of scientific articles based on common
characteristics is an interesting problem with many applications in digital
library and information retrieval systems. Properly organized articles can be
useful for automatic generation of taxonomies in scientific writings, textual
summarization, efficient information retrieval etc. Generating article bundles
from a large number of input articles, based on the associated features of the
articles is tedious and computationally expensive task. In this report we
propose an automatic two-step approach for topic extraction and bundling of
related articles from a set of scientific articles in real-time. For topic
extraction, we make use of Latent Dirichlet Allocation (LDA) topic modeling
techniques and for bundling, we make use of hierarchical agglomerative
clustering techniques.
  We run experiments to validate our bundling semantics and compare it with
existing models in use. We make use of an online crowdsourcing marketplace
provided by Amazon called Amazon Mechanical Turk to carry out experiments. We
explain our experimental setup and empirical results in detail and show that
our method is advantageous over existing ones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5440</identifier>
 <datestamp>2012-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5440</id><created>2012-12-21</created><authors><author><keyname>Zungeru</keyname><forenames>A. M.</forenames></author></authors><title>Development of an Anti-collision Model for Vehicles</title><categories>cs.SY</categories><comments>14 pages, 14 figures, Journal paper</comments><journal-ref>International Journal of Embedded Systems and Applications
  (IJESA), vol. 2(4), pp. 21-34, 2012</journal-ref><doi>10.5121/ijesa.2012.2402</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Anti Collision device is a detection device meant to be incorporated into
cars for the purpose of safety. As opposed to the anti collision devices
present in the market today, this system is not designed to control the
vehicle. Instead, it serves as an alert in the face of imminent collision. The
device is intended to find a way to implement a minimum spacing for cars in
traffic in an affordable way. It would also achieve safety for the passengers
of a moving car. The device is made up of an infrared transmitter and receiver.
Also incorporated into it is an audio visual alarm to work in with the receiver
and effectively alert the driver and/or the passengers. To achieve this design,
555 timers coupled both as astable and monostable circuits were used along with
a 38 KHz Square Pulse generator. The device works by sending out streams of
infrared radiation and when these rays are seen by the other equipped vehicle,
both are meant to take the necessary precaution to avert a collision. The
device would still sound an alarm even though it is not receiving infrared
beams from the oncoming vehicle. This is due to reflection of its own infrared
beams. At the end of the design and testing process, overall system was
implemented with a constructed work, tested working and perfectly functional.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5442</identifier>
 <datestamp>2012-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5442</id><created>2012-12-21</created><authors><author><keyname>Kembellec</keyname><forenames>G&#xe9;rald</forenames><affiliation>DICEN CNAM</affiliation></author><author><keyname>Scopsi</keyname><forenames>Claire</forenames><affiliation>DICEN CNAM</affiliation></author></authors><title>\'Etude compar\'ee de quatre logiciels de gestion de r\'ef\'erences
  bibliographiques libres ou gratuits</title><categories>cs.IR</categories><comments>11 pages</comments><proxy>ccsd</proxy><journal-ref>Documentation et Bibliotheques 58, 4 (2012) 187-197</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article is the result of the analysis of various bibliographic reference
management tools, especially those that are free. The use of editorial tools by
bibliographic editors has evolved rapidly since 2007. But, until recently, free
software has fallen short when it comes to ergonomics or use. The functional
and technical panorama offered by free software is the result of the comparison
of JabRef, Mendeley Desktop, BibDesk and Zotero software undertaken in January
2012 by two research professors affiliated with the Institut national
fran\c{c}ais des techniques de la documentation (INTD).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5443</identifier>
 <datestamp>2014-07-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5443</id><created>2012-12-21</created><updated>2014-07-01</updated><authors><author><keyname>Berger</keyname><forenames>Annabell</forenames></author></authors><title>The Connection between the Number of Realizations for Degree Sequences
  and Majorization</title><categories>math.CO cs.DM</categories><comments>30 pages. There was a mistake an case~3 and case~4 in the proof of
  the result of Proposition 10 (current version). I corrected it. For that I
  added a further result in Proposition 9</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The \emph{graph realization problem} is to find for given nonnegative
integers $a_1,\dots,a_n$ a simple graph (no loops or multiple edges) such that
each vertex $v_i$ has degree $a_i.$ Given pairs of nonnegative integers
$(a_1,b_1),\dots,(a_n,b_n),$ (i) the \emph{bipartite realization problem} ask
whether there is a bipartite graph (no loops or multiple edges) such that
vectors $(a_1,...,a_n)$ and $(b_1,...,b_n)$ correspond to the lists of degrees
in the two partite sets, (ii) the \emph{digraph realization problem} is to find
a digraph (no loops or multiple arcs) such that each vertex $v_i$ has indegree
$a_i$ and outdegree $b_i.$\\ The classic literature provides characterizations
for the existence of such realizations that are strongly related to the concept
of majorization. Aigner and Triesch (1994) extended this approach to a more
general result for graphs, leading to an efficient realization algorithm and a
short and simple proof for the Erd\H{o}s-Gallai Theorem. We extend this
approach to the bipartite realization problem and the digraph realization
problem.\\ Our main result is the connection between majorization and the
number of realizations for a degree list in all three problems. We show: if
degree list $S'$ majorizes $S$ in a certain sense, then $S$ possesses more
realizations than $S'.$ We prove that constant lists possess the largest number
of realizations for fixed $n$ and a fixed number of arcs $m$ when $n$ divides
$m.$ So-called \emph{minconvex lists} for graphs and bipartite graphs or
\emph{opposed minconvex lists} for digraphs maximize the number of realizations
when $n$ does not divide $m$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5449</identifier>
 <datestamp>2012-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5449</id><created>2012-12-21</created><authors><author><keyname>Hidaka</keyname><forenames>Shohei</forenames></author></authors><title>Characterizing Multivariate Information Flows</title><categories>cs.IT math.DS math.IT stat.ME</categories><comments>This manuscript is submitted to Proceedings of the National Academy
  of Sciences of the United States of America</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the crucial steps in scientific studies is to specify dependent
relationships among factors in a system of interest. Given little knowledge of
a system, can we characterize the underlying dependent relationships through
observation of its temporal behaviors? In multivariate systems, there are
potentially many possible dependent structures confusable with each other, and
it may cause false detection of illusory dependency between unrelated factors.
The present study proposes a new information-theoretic measure with
consideration to such potential multivariate relationships. The proposed
measure, called multivariate transfer entropy, is an extension of transfer
entropy, a measure of temporal predictability. In the simulations and empirical
studies, we demonstrated that the proposed measure characterized the latent
dependent relationships in unknown dynamical systems more accurately than its
alternative measure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5454</identifier>
 <datestamp>2013-04-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5454</id><created>2012-12-21</created><authors><author><keyname>David</keyname><forenames>Omid</forenames></author><author><keyname>Gerrah</keyname><forenames>Rabin</forenames></author></authors><title>In Vivo Quantification of Clot Formation in Extracorporeal Circuits</title><categories>cs.CV physics.med-ph</categories><comments>In Proceedings of 20th NextMed Medicine Meets Virtual Reality
  Conference (NextMed / MMVR20), San Diego, CA, February 2013</comments><journal-ref>Studies in Health Technology and Informatics, Vol. 184, pp.
  148--150, January 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Clot formation is a common complication in extracorporeal circuits. In this
paper we describe a novel method for clot formation analysis using image
processing. We assembled a closed extracorporeal circuit and circulated blood
at varying speeds. Blood filters were placed in downstream of the flow, and
clotting agents were added to the circuit. Digital images of the filter were
subsequently taken, and image analysis was applied to calculate the density of
the clot. Our results show a significant correlation between the cumulative
size of the clots, the density measure of the clot based on image analysis, and
flow duration in the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5461</identifier>
 <datestamp>2014-06-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5461</id><created>2012-12-21</created><updated>2014-06-23</updated><authors><author><keyname>Simons</keyname><forenames>Christopher L.</forenames></author><author><keyname>Smith</keyname><forenames>Jim</forenames></author><author><keyname>White</keyname><forenames>Paul</forenames></author></authors><title>Interactive Ant Colony Optimisation (iACO) for Early Lifecycle Software
  Design</title><categories>cs.SE cs.AI</categories><comments>31 pages including appendices</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Software design is crucial to successful software development, yet is a
demanding multi-objective problem for software engineers. In an attempt to
assist the software designer, interactive (i.e. human in-the-loop)
meta-heuristic search techniques such as evolutionary computing have been
applied and show promising results. Recent investigations have also shown that
Ant Colony Optimization (ACO) can outperform evolutionary computing as a
potential search engine for interactive software design. With a limited
computational budget, ACO produces superior candidate design solutions in a
smaller number of iterations. Building on these findings, we propose a novel
interactive ACO (iACO) approach to assist the designer in early lifecycle
software design, in which the search is steered jointly by subjective designer
evaluation as well as machine fitness functions relating the structural
integrity and surrogate elegance of software designs. Results show that iACO is
speedy, responsive and highly effective in enabling interactive, dynamic
multi-objective search in early lifecycle software design. Study participants
rate the iACO search experience as compelling. Results of machine learning of
fitness measure weightings indicate that software design elegance does indeed
play a significant role in designer evaluation of candidate software design. We
conclude that the evenness of the number of attributes and methods among
classes (NAC) is a significant surrogate elegance measure, which in turn
suggests that this evenness of distribution, when combined with structural
integrity, is an implicit but crucial component of effective early lifecycle
software design.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5462</identifier>
 <datestamp>2012-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5462</id><created>2012-12-21</created><authors><author><keyname>Sahai</keyname><forenames>Achaleshwar</forenames></author><author><keyname>Patel</keyname><forenames>Gaurav</forenames></author><author><keyname>Dick</keyname><forenames>Chris</forenames></author><author><keyname>Sabharwal</keyname><forenames>Ashutosh</forenames></author></authors><title>On the Impact of Phase Noise on Active Cancellation in Wireless
  Full-Duplex</title><categories>cs.IT math.IT</categories><comments>35 pages, Submitted to IEEE Transactions on Vehicular Technology, Dec
  2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recent experimental results have shown that full-duplex communication is
possible for short-range communications. However, extending full-duplex to
long-range communication remains a challenge, primarily due to residual
self-interference even with a combination of passive suppression and active
cancellation methods. In this paper, we investigate the root cause of
performance bottlenecks in current full-duplex systems. We first classify all
known full-duplex architectures based on how they compute their cancelling
signal and where the cancelling signal is injected to cancel self-interference.
Based on the classification, we analytically explain several published
experimental results. The key bottleneck in current systems turns out to be the
phase noise in the local oscillators in the transmit and receive chain of the
full-duplex node. As a key by-product of our analysis, we propose signal models
for wideband and MIMO full-duplex systems, capturing all the salient design
parameters, and thus allowing future analytical development of advanced coding
and signal design for full-duplex systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5468</identifier>
 <datestamp>2013-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5468</id><created>2012-12-21</created><updated>2013-07-10</updated><authors><author><keyname>Gon&#xe7;alves</keyname><forenames>Daniel</forenames><affiliation>LIRMM</affiliation></author><author><keyname>Parreau</keyname><forenames>Aline</forenames><affiliation>INRIA Lille - Nord Europe</affiliation></author><author><keyname>Pinlou</keyname><forenames>Alexandre</forenames><affiliation>LIRMM</affiliation></author></authors><title>Locally identifying coloring in bounded expansion classes of graphs</title><categories>math.CO cs.DM</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A proper vertex coloring of a graph is said to be locally identifying if the
sets of colors in the closed neighborhood of any two adjacent non-twin vertices
are distinct. The lid-chromatic number of a graph is the minimum number of
colors used by a locally identifying vertex-coloring. In this paper, we prove
that for any graph class of bounded expansion, the lid-chromatic number is
bounded. Classes of bounded expansion include minor closed classes of graphs.
For these latter classes, we give an alternative proof to show that the
lid-chromatic number is bounded. This leads to an explicit upper bound for the
lid-chromatic number of planar graphs. This answers in a positive way a
question of Esperet et al [L. Esperet, S. Gravier, M. Montassier, P. Ochem and
A. Parreau. Locally identifying coloring of graphs. Electronic Journal of
Combinatorics, 19(2), 2012.].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5473</identifier>
 <datestamp>2012-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5473</id><created>2012-12-21</created><authors><author><keyname>Aschheim</keyname><forenames>Raymond</forenames></author></authors><title>Spin foam with topologically encoded tetrad on trivalent spin networks</title><categories>cs.IT math.IT</categories><comments>4 pages, 17 figures, based on a contributed talk given at LOOPS'11,
  Madrid, Spain</comments><msc-class>68R10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explore discrete approaches in LQG where all fields, the gravitational
tetrad, and the matter and energy fields, are encoded implicitly in a graph
instead of being additional data. Our graph should therefore be richer than a
simple simplicial decomposition. It has to embed geometrical information and
the standard model. We start from Lisi's model. We build a trivalent graph
which is an F4 lattice of 48-valent supernodes, reduced as trivalent subgraphs,
and topologically encoding data. We show it is a solution for EFE with no
matter. We define bosons and half-fermions in two dual basis. They are encoded
by bit exchange in supernodes, operated by Pachner 2-2 move, and rest state can
be restored thanks to information redundancy. Despite its 4 dimensional nature,
our graph is a trivalent spin network, and its history is a pentavalent spin
foam.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5491</identifier>
 <datestamp>2014-10-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5491</id><created>2012-12-21</created><authors><author><keyname>Morandi</keyname><forenames>Benjamin</forenames></author><author><keyname>West</keyname><forenames>Scott</forenames></author><author><keyname>Nanz</keyname><forenames>Sebastian</forenames></author><author><keyname>Gomaa</keyname><forenames>Hassan</forenames></author></authors><title>Concurrent object-oriented development with behavioral design patterns</title><categories>cs.SE</categories><acm-class>D.2.11</acm-class><journal-ref>Proceedings of the 7th European Conference on Software
  Architecture (ECSA'13), volume 7957 of Lecture Notes in Computer Science,
  pages 25-32. Springer, 2013</journal-ref><doi>10.1007/978-3-642-39031-9_3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The development of concurrent applications is challenging because of the
complexity of concurrent designs and the hazards of concurrent programming.
Architectural modeling using the Unified Modeling Language (UML) can support
the development process, but the problem of mapping the model to a concurrent
implementation remains. This paper addresses this problem by defining a scheme
to map concurrent UML designs to a concurrent object-oriented program. Using
the COMET method for the architectural design of concurrent object-oriented
systems, each component and connector is annotated with a stereotype indicating
its behavioral design pattern. For each of these patterns, a reference
implementation is provided using SCOOP, a concurrent object-oriented
programming model. We evaluate this development process using a case study of
an ATM system, obtaining a fully functional implementation based on the
systematic mapping of the individual patterns. Given the strong execution
guarantees of the SCOOP model, which is free of data races by construction,
this development method eliminates a source of intricate concurrent programming
errors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5518</identifier>
 <datestamp>2012-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5518</id><created>2012-12-19</created><authors><author><keyname>Helgesson</keyname><forenames>Peter</forenames></author><author><keyname>Wennberg</keyname><forenames>Bernt</forenames></author></authors><title>The War of Attrition in the Limit of Infinitely Many Players</title><categories>cs.GT math.CA math.PR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The &quot;War of Attrition&quot; is a classical game theoretic model that was first
introduced to mathematically describe certain non-violent animal behavior. The
original setup considers two participating players in a one-shot game competing
for a given prize by waiting. This model has later been extended to several
different models allowing more than two players. One of the first of these
N-player generalizations was due to J. Haigh and C. Cannings (Acta Appl.
Math.14) where two possible models are mainly discussed; one in which the game
starts afresh with new strategies each time a player leaves the game, and one
where the players have to stick with the strategy they chose initially. The
first case is well understood whereas, for the second case, much is still left
open. In this paper we study the asymptotic behavior of these two models as the
number of players tend to infinity and prove that their time evolution coincide
in the limit. We also prove new results concerning the second model in the
N-player setup.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5524</identifier>
 <datestamp>2013-08-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5524</id><created>2012-12-21</created><updated>2013-08-22</updated><authors><author><keyname>Sprangers</keyname><forenames>Olivier</forenames></author><author><keyname>Lopes</keyname><forenames>Gabriel A. D.</forenames></author><author><keyname>Babuska</keyname><forenames>Robert</forenames></author></authors><title>Reinforcement learning for port-Hamiltonian systems</title><categories>cs.SY cs.LG</categories><comments>submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Passivity-based control (PBC) for port-Hamiltonian systems provides an
intuitive way of achieving stabilization by rendering a system passive with
respect to a desired storage function. However, in most instances the control
law is obtained without any performance considerations and it has to be
calculated by solving a complex partial differential equation (PDE). In order
to address these issues we introduce a reinforcement learning approach into the
energy-balancing passivity-based control (EB-PBC) method, which is a form of
PBC in which the closed-loop energy is equal to the difference between the
stored and supplied energies. We propose a technique to parameterize EB-PBC
that preserves the systems's PDE matching conditions, does not require the
specification of a global desired Hamiltonian, includes performance criteria,
and is robust to extra non-linearities such as control input saturation. The
parameters of the control law are found using actor-critic reinforcement
learning, enabling learning near-optimal control policies satisfying a desired
closed-loop energy landscape. The advantages are that near-optimal controllers
can be generated using standard energy shaping techniques and that the
solutions learned can be interpreted in terms of energy shaping and damping
injection, which makes it possible to numerically assess stability using
passivity theory. From the reinforcement learning perspective, our proposal
allows for the class of port-Hamiltonian systems to be incorporated in the
actor-critic framework, speeding up the learning thanks to the resulting
parameterization of the policy. The method has been successfully applied to the
pendulum swing-up problem in simulations and real-life experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5525</identifier>
 <datestamp>2012-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5525</id><created>2012-12-21</created><authors><author><keyname>Lopes</keyname><forenames>G. A. D.</forenames></author><author><keyname>Kersbergen</keyname><forenames>B.</forenames></author><author><keyname>De Schutter</keyname><forenames>B.</forenames></author><author><keyname>Boom</keyname><forenames>T. J. J. van den</forenames></author><author><keyname>Babuska</keyname><forenames>R.</forenames></author></authors><title>Synchronization of a class of cyclic discrete-event systems describing
  legged locomotion</title><categories>cs.SY</categories><comments>Submitted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It has been shown that max-plus linear systems are well suited for
applications in synchronization and scheduling, such as the generation of train
timetables, manufacturing, or traffic. In this paper we show that the same is
true for multi-legged locomotion. In this framework, the max-plus eigenvalue of
the system matrix represents the total cycle time, whereas the max-plus
eigenvector dictates the steady-state behavior. Uniqueness of the
eigenstructure also indicates uniqueness of the resulting behavior. For the
particular case of legged locomotion, the movement of each leg is abstracted to
two-state circuits: swing and stance (leg in flight and on the ground,
respectively). The generation of a gait (a manner of walking) for a multiple
legged robot is then achieved by synchronizing the multiple discrete-event
cycles via the max-plus framework. By construction, different gaits and gait
parameters can be safely interleaved by using different system matrices. In
this paper we address both the transient and steady-state behavior for a class
of gaits by presenting closed-form expressions for the max-plus eigenvalue and
max-plus eigenvector of the system matrix and the coupling time. The
significance of this result is in showing guaranteed robustness to
perturbations and gait switching, and also a systematic methodology for
synthesizing controllers that allow for legged robots to change rhythms fast.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5543</identifier>
 <datestamp>2012-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5543</id><created>2012-12-21</created><authors><author><keyname>Imran</keyname><forenames>Muhammad</forenames></author><author><keyname>Hasbullah</keyname><forenames>Halabi B.</forenames></author><author><keyname>Said</keyname><forenames>Abas Md</forenames></author></authors><title>Personality wireless sensor networks (PWSNs)</title><categories>cs.NI</categories><comments>7th Annual Seminar on Science and Technology, Universiti Malaysia
  Sabah, 2008</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In recent years, WSNs are garnering lot of interest from research community
because of their unique characteristics and potential for enormous range of
applications. Envision for new class of applications are being emerged such as
human augmentation, enhancing social interaction etc. Misunderstanding or
misinterpretation of behaviors from individuals leads to social conflicts.
There are various theories that classify people into different personality
types. Most of the existing theories rely on questionnaires, which is highly
unreliable. Anyone can lead such theories in practice to incorrect
classification intentionally or unintentionally. The objective of this research
is to investigate existing solutions and propose a basic infrastructure for an
automated context-aware psychological classification based on different
parameters. The idea is to use wearable sensors to sense and measure various
human body parameters (i.e. body temperature, blood pressure, perspiration,
brain impulses etc) that coerce human psychological condition. The data
collected from these parameters is transformed in to information, to determine
personality type, mood and psychological condition of interacting parties. This
information is shared among counterparts to better understand each other in
order to avoid potential conflicting situations. We believe that it will help
peoples understand each other, improve their quality of life and minimize
possible conflicting situations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5554</identifier>
 <datestamp>2014-07-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5554</id><created>2012-12-21</created><updated>2014-07-11</updated><authors><author><keyname>Barbier</keyname><forenames>Morgan</forenames><affiliation>GREYC</affiliation></author></authors><title>Re-encoding reformulation and application to Welch-Berlekamp algorithm</title><categories>cs.IT math.IT</categories><proxy>ccsd</proxy><journal-ref>Re-encoding reformulation and application to Welch-Berlekamp
  algorithm (2014) 1782 - 1786</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The main decoding algorithms for Reed-Solomon codes are based on a bivariate
interpolation step, which is expensive in time complexity. Lot of interpolation
methods were proposed in order to decrease the complexity of this procedure,
but they stay still expensive. Then Koetter, Ma and Vardy proposed in 2010 a
technique, called re-encoding, which allows to reduce the practical running
time. However, this trick is only devoted for the Koetter interpolation
algorithm. We propose a reformulation of the re-encoding for any interpolation
methods. The assumption for this reformulation permits only to apply it to the
Welch-Berlekamp algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5558</identifier>
 <datestamp>2012-12-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5558</id><created>2012-12-21</created><authors><author><keyname>Imran</keyname><forenames>Muhammad</forenames></author><author><keyname>khan</keyname><forenames>Asfandyar</forenames></author><author><keyname>Abdullah</keyname><forenames>Azween B.</forenames></author></authors><title>Energy balancing through cluster head selection using K-Theorem in
  homogeneous wireless sensor networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The objective of this paper is to increase life time of homogeneous wireless
sensor networks (WSNs) through minimizing long range communication and energy
balancing. Sensor nodes are resource constrained particularly with limited
energy that is difficult or impossible to replenish. LEACH (Low Energy Adaptive
Clustering Hierarchy) is most well-known cluster based architecture for WSN
that aims to evenly dissipate energy among all sensor nodes. In cluster based
architecture, the role of cluster head is very crucial for the successful
operation of WSN because once the cluster head becomes non functional, the
whole cluster becomes dysfunctional. We have proposed a modified cluster based
WSN architecture by introducing a coordinator node (CN) that is rich in terms
of resources. This CN take up the responsibility of transmitting data to the
base station over longer distances from cluster heads. We have proposed a
cluster head selection algorithm based on K-theorem and other parameters i.e.
residual energy, distance to coordinator node, reliability and degree of
mobility. The K-theorem is used to select candidate cluster heads based on
bunch of sensor nodes in a cluster. We believe that the proposed architecture
and algorithm achieves higher energy efficiency through minimizing
communication and energy balancing. The proposed architecture is more scalable
and proposed algorithm is robust against even/uneven node deployment and node
mobility.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5577</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5577</id><created>2012-12-21</created><updated>2014-12-29</updated><authors><author><keyname>Li</keyname><forenames>Linbo</forenames></author><author><keyname>Mahdavifar</keyname><forenames>Hessam</forenames></author><author><keyname>Kang</keyname><forenames>Inyup</forenames></author></authors><title>A Structured Construction of Optimal Measurement Matrix for Noiseless
  Compressed Sensing via Analog Polarization</title><categories>cs.IT math.IT</categories><comments>26 pages. Rev5 added new results of Theorem 2 (proving that analog
  polar encoding achieves vanishing P_e while achieving the dimension rate
  (1-p) across SANC(p)) and other modifications/additions. Revision 1 done.
  Revision 0: Revised Appendix I, added Appendix IV, and other minor changes.
  Manuscript has been submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a method of structured construction of the optimal
measurement matrix for noiseless compressed sensing (CS), which achieves the
minimum number of measurements which only needs to be as large as the sparsity
of the signal itself to be recovered to guarantee almost error-free recovery,
for sufficiently large dimension. To arrive at the results, we employ a duality
between noiseless CS and analog coding across sparse additive noisy channel
(SANC). Extending Renyi Information Dimension to Mutual Information Dimension
(MID), we show the operational meaning of MID to be the fundamental limit of
asymptotically error-free analog transmission across SANC under linear analog
encoding constraint. We prove that MID polarizes after analog polar
transformation and obeys the same recursive relationship as BEC. We further
prove that analog polar encoding can achieve the fundamental limit of
achievable dimension rate with vanishing Pe across SANC. From the duality, a
structured construction scheme is proposed for the linear measurement matrix
which achieves the minimum measurement requirement for noiseless CS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5589</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5589</id><created>2012-12-20</created><authors><author><keyname>Boyer</keyname><forenames>Harry</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Bastide</keyname><forenames>Alain</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Lauret</keyname><forenames>Alfred Jean Philippe</forenames><affiliation>PIMENT</affiliation></author></authors><title>CODYRUN, outil de simulation et d'aide \`a la conception
  thermo-a\'eraulique de b\^atiments</title><categories>cs.CE</categories><comments>in French</comments><proxy>ccsd</proxy><journal-ref>Journ\'ee th\'ematique SFT-IBPSA 2005, La Rochelle : France (2005)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article presents the CODYRUN software developped by University of La
R\'eunion. It is a multizone thermal software, with detailled airflow and
humidity transfer calculations. One of its specific aspects is that it
constitutes a research tool, a design tool used by the lab and professionnals
and also a teaching tool. After a presentation of the multiple model aspect,
some details of the tree modules associated to physical phenomenons are given.
Elements of validation are exposed in next paraghaph, and then a few details of
the front end.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5590</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5590</id><created>2012-12-21</created><authors><author><keyname>Albaham</keyname><forenames>Ameer Tawfik</forenames></author><author><keyname>Salim</keyname><forenames>Naomie</forenames></author></authors><title>Online Forum Thread Retrieval using Pseudo Cluster Selection and Voting
  Techniques</title><categories>cs.IR</categories><comments>The original publication is available at
  http://www.springerlink.com/. arXiv admin note: substantial text overlap with
  arXiv:1212.5331</comments><journal-ref>Advances in Intelligent Systems and Applications, Volume 1, 2013,
  20, 297-306</journal-ref><doi>10.1007/978-3-642-35452-6_31</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Online forums facilitate knowledge seeking and sharing on the Web. However,
the shared knowledge is not fully utilized due to information overload. Thread
retrieval is one method to overcome information overload. In this paper, we
propose a model that combines two existing approaches: the Pseudo Cluster
Selection and the Voting Techniques. In both, a retrieval system first scores a
list of messages and then ranks threads by aggregating their scored messages.
They differ on what and how to aggregate. The pseudo cluster selection focuses
on input, while voting techniques focus on the aggregation method. Our combined
models focus on the input and the aggregation methods. The result shows that
some combined models are statistically superior to baseline methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5592</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5592</id><created>2012-12-21</created><authors><author><keyname>Boyer</keyname><forenames>Harry</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Gatina</keyname><forenames>Jean Claude</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Brau</keyname><forenames>Jean</forenames><affiliation>CETHIL</affiliation></author></authors><title>Multiple model software for airflow and thermal building simulation. A
  case study under tropical humid climate, in R\'eunion Island</title><categories>cs.CE</categories><comments>Building Simulation Conference 1993, Adela\&quot;ide : Australia (1993)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The first purpose of our work has been to allow -as far as heat transfer
modes, airflow calculation and meteorological data reconstitution are
concerned- the integration of diverse interchangeable physical models in a
single software tool for professional use, CODYRUN. The designer's objectives,
precision requested and calculation time consideration, lead us to design a
structure accepting selective use of models, taking into account multizone
description and airflow patterns. With a building case study in Reunion Island,
we first analyse the sensibility of the thermal model to diffuse radiation
reconstitution on tilted surfaces. Then, a realistic balance between precision
required and calculation time leads us to select detailed models for the zone
of main interest, but to choose simplified models for the other zones.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5593</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5593</id><created>2012-12-21</created><authors><author><keyname>Berthomieu</keyname><forenames>Thierry</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Boyer</keyname><forenames>Harry</forenames><affiliation>PIMENT</affiliation></author></authors><title>Time-variant Linear reduction model approximation : application to
  thermal and airflow building simulation</title><categories>cs.CE</categories><comments>Eighth International IBPSA Conference, Eindhoven : Netherlands
  (2003); Proceedings available at http://www.ibpsa.org/m_bs2003.asp</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Considering the natural ventilation, the thermal behavior of buildings can be
described by a linear time varying model. In this paper, we describe an
implementation of model reduction of linear time varying systems. We show the
consequences of the model reduction on computing time and accuracy. Finally, we
compare experimental measures and simulation results using the initial model or
the reduced model. The reduced model shows negligible difference in accuracy,
and the computing time shortens.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5594</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5594</id><created>2012-12-21</created><authors><author><keyname>Fock</keyname><forenames>Eric</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Mara</keyname><forenames>Thierry Alex</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Lauret</keyname><forenames>Alfred Jean Philippe</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Boyer</keyname><forenames>Harry</forenames><affiliation>PIMENT</affiliation></author></authors><title>Black box modelling of HVAC system : improving the performances of
  neural networks</title><categories>cs.NE cs.CE</categories><comments>Eighth International IBPSA Conference, Eindhoven : Netherlands
  (2003); Proceedings available at http://www.ibpsa.org/m_bs2003.asp</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with neural networks modelling of HVAC systems. In order to
increase the neural networks performances, a method based on sensitivity
analysis is applied. The same technique is also used to compute the relevance
of each input. To avoid the prediction errors in dry coil conditions, a
metamodel for each capacity is derived from the neural networks. The regression
coefficients of the polynomial forms are identified through the use of spectral
analysis. These methods based on sensitivity and spectral analysis lead to an
optimized neural network model, as regard to its architecture and predictions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5596</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5596</id><created>2012-12-21</created><authors><author><keyname>van Wezel</keyname><forenames>Jos</forenames></author><author><keyname>Streit</keyname><forenames>Achim</forenames></author><author><keyname>Jung</keyname><forenames>Christopher</forenames></author><author><keyname>Stotzka</keyname><forenames>Rainer</forenames></author><author><keyname>Halstenberg</keyname><forenames>Silke</forenames></author><author><keyname>Rigoll</keyname><forenames>Fabian</forenames></author><author><keyname>Garcia</keyname><forenames>Ariel</forenames></author><author><keyname>Heiss</keyname><forenames>Andreas</forenames></author><author><keyname>Schwarz</keyname><forenames>Kilian</forenames></author><author><keyname>Gasthuber</keyname><forenames>Martin</forenames></author><author><keyname>Giesler</keyname><forenames>Andr&#xe9;</forenames></author></authors><title>Data Life Cycle Labs, A New Concept to Support Data-Intensive Science</title><categories>cs.DL physics.comp-ph q-bio.QM</categories><comments>8 pages, project white paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many sciences the increasing amounts of data are reaching the limit of
established data handling and processing. With four large research centers of
the German Helmholtz association the Large Scale Data Management and Analysis
(LSDMA) project supports an initial set of scientific projects, initiatives and
instruments to organize and efficiently analyze the increasing amount of data
produced in modern science. LSDMA bridges the gap between data production and
data analysis using a novel approach by combining specific community support
and generic, cross community development. In the Data Life Cycle Labs (DLCL)
experts from the data domain work closely with scientific groups of selected
research domains in joint R&amp;D where community-specific data life cycles are
iteratively optimized, data and meta-data formats are defined and standardized,
simple access and use is established as well as data and scientific insights
are preserved in long-term and open accessible archives.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5599</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5599</id><created>2012-12-21</created><authors><author><keyname>Adelard</keyname><forenames>Laetitia</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Mara</keyname><forenames>Thierry Alex</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Boyer</keyname><forenames>Harry</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Gatina</keyname><forenames>Jean Claude</forenames><affiliation>PIMENT</affiliation></author></authors><title>Elaboration of a new tool for weather data sequences generation</title><categories>cs.CE</categories><comments>Available from http://www.ibpsa.org/m_bs1999.asp; IBPSA'99,
  International Building Performance Association, Tokyo : Japan (1999)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals about the presentation of a new software RUNEOLE used to
provide weather data in buildings physics. RUNEOLE associates three modules
leading to the description, the modelling and the generation of weather data.
The first module is dedicated to the description of each climatic variable
included in the database. Graphic representation is possible (with histograms
for example). Mathematical tools used to compare statistical distributions,
determine daily characteristic evolutions, find typical days, and the
correlations between the different climatic variables have been elaborated in
the second module. Artificial weather datafiles adapted to different simulation
codes are available at the issue of the third module. This tool can then be
used in HVAC system evaluation, or in the study of thermal comfort. The studied
buildings can then be tested under different thermal, aeraulic, and radiative
solicitations, leading to a best understanding of their behaviour for example
in humid climates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5620</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5620</id><created>2012-12-21</created><authors><author><keyname>Pahwa</keyname><forenames>Sakshi</forenames></author><author><keyname>Scoglio</keyname><forenames>Caterina</forenames></author><author><keyname>Schulz</keyname><forenames>Noel</forenames></author></authors><title>Topological Analysis and Mitigation Strategies for Cascading Failures in
  Power Grid Networks</title><categories>physics.soc-ph cs.SI physics.comp-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, there has been a growing concern about the overload status of the
power grid networks, and the increasing possibility of cascading failures. Many
researchers have studied these networks to provide design guidelines for more
robust power grids. Topological analysis is one of the components of system
analysis for its robustness. This paper presents a complex systems analysis of
power grid networks. First, the cascading effect has been simulated on three
well known networks: the IEEE 300 bus test system, the IEEE 118 bus test
system, and the WSCC 179 bus equivalent model. To extend the analysis to a
larger set of networks, we develop a network generator and generate multiple
graphs with characteristics similar to the IEEE test networks but with
different topologies. The generated graphs are then compared to the test
networks to show the effect of topology in determining their robustness with
respect to cascading failures. The generated graphs turn out to be more robust
than the test graphs, showing the importance of topology in the robust design
of power grids. The second part of this paper concerns the discussion of two
novel mitigation strategies for cascading failures: Targeted Load Reduction and
Islanding using Distributed Sources. These new mitigation strategies are
compared with the Homogeneous Load Reduction strategy. Even though the
Homogeneous Load Reduction is simpler to implement, the Targeted Load Reduction
is much more effective. Additionally, an algorithm is presented for the
partitioning of the network for islanding as an effort towards fault isolation
to prevent cascading failures. The results for island formation are better if
the sources are well distributed, else the algorithm leads to the formation of
superislands.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5633</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5633</id><created>2012-12-21</created><authors><author><keyname>Joulin</keyname><forenames>Pierre</forenames></author><author><keyname>Deveaud</keyname><forenames>Romain</forenames></author><author><keyname>SanJuan-Ibekwe</keyname><forenames>Eric</forenames></author><author><keyname>Francony</keyname><forenames>Jean-Marc</forenames></author><author><keyname>Para</keyname><forenames>Fran&#xe7;oise</forenames></author></authors><title>Design, implementation and experiment of a YeSQL Web Crawler</title><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a novel, &quot;focusable&quot;, scalable, distributed web crawler based on
GNU/Linux and PostgreSQL that we designed to be easily extendible and which we
have released under a GNU public licence. We also report a first use case
related to an analysis of Twitter's streams about the french 2012 presidential
elections and the URL's it contains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5636</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5636</id><created>2012-12-21</created><authors><author><keyname>Gal&#xe1;rraga</keyname><forenames>Luis</forenames></author><author><keyname>Hose</keyname><forenames>Katja</forenames></author><author><keyname>Schenkel</keyname><forenames>Ralf</forenames></author></authors><title>Partout: A Distributed Engine for Efficient RDF Processing</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The increasing interest in Semantic Web technologies has led not only to a
rapid growth of semantic data on the Web but also to an increasing number of
backend applications with already more than a trillion triples in some cases.
Confronted with such huge amounts of data and the future growth, existing
state-of-the-art systems for storing RDF and processing SPARQL queries are no
longer sufficient. In this paper, we introduce Partout, a distributed engine
for efficient RDF processing in a cluster of machines. We propose an effective
approach for fragmenting RDF data sets based on a query log, allocating the
fragments to nodes in a cluster, and finding the optimal configuration. Partout
can efficiently handle updates and its query optimizer produces efficient query
execution plans for ad-hoc SPARQL queries. Our experiments show the superiority
of our approach to state-of-the-art approaches for partitioning and distributed
SPARQL query processing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5637</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5637</id><created>2012-12-21</created><authors><author><keyname>Cesa-Bianchi</keyname><forenames>Nicolo'</forenames></author><author><keyname>Gentile</keyname><forenames>Claudio</forenames></author><author><keyname>Vitale</keyname><forenames>Fabio</forenames></author><author><keyname>Zappella</keyname><forenames>Giovanni</forenames></author></authors><title>Random Spanning Trees and the Prediction of Weighted Graphs</title><categories>cs.LG stat.ML</categories><comments>Appeared in ICML 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the problem of sequentially predicting the binary labels on
the nodes of an arbitrary weighted graph. We show that, under a suitable
parametrization of the problem, the optimal number of prediction mistakes can
be characterized (up to logarithmic factors) by the cutsize of a random
spanning tree of the graph. The cutsize is induced by the unknown adversarial
labeling of the graph nodes. In deriving our characterization, we obtain a
simple randomized algorithm achieving in expectation the optimal mistake bound
on any polynomially connected weighted graph. Our algorithm draws a random
spanning tree of the original graph and then predicts the nodes of this tree in
constant expected amortized time and linear space. Experiments on real-world
datasets show that our method compares well to both global (Perceptron) and
local (label propagation) methods, while being generally faster in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5645</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5645</id><created>2012-12-21</created><authors><author><keyname>Tandon</keyname><forenames>Rajat</forenames></author></authors><title>Algorithm to Compute Squares of 1st N Natural Numbers Without Using
  Multiplication</title><categories>cs.DS cs.CC</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Processors may find some elementary operations to be faster than the others.
Although an operation may be conceptually as simple as some other operation,
the processing speeds of the two can vary. A clever programmer will always try
to choose the faster instructions for the job. This paper presents an algorithm
to display squares of 1st N natural numbers without using multiplication (*
operator). Instead, the same work can be done using addition (+ operator). The
results can also be used to compute the sum of those squares. If we compare the
normal method of computing the squares of 1st N natural numbers with this
method, we can conclude that the algorithm discussed in the paper is more
optimized in terms of time complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5649</identifier>
 <datestamp>2013-10-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5649</id><created>2012-12-21</created><updated>2013-10-28</updated><authors><author><keyname>Kharitonov</keyname><forenames>Daniel</forenames></author></authors><title>Multi-attribute value functions in energy-aware network control</title><categories>cs.NI</categories><comments>preprint</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper evaluates and classifies existing and emerging energy-control
technologies for computer networks based on their relative value functions.
Using formal decision analysis methods, we demonstrate the impact of
risk-benefit dimensions on technology certain equivalent and deployment
perspective. We demonstrate how energy control solutions can be cost-effective
or unsustainable depending on network type and operator risk tolerance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5650</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5650</id><created>2012-12-21</created><authors><author><keyname>Zhou</keyname><forenames>Ke</forenames></author><author><keyname>Zha</keyname><forenames>Hongyuan</forenames></author><author><keyname>Xue</keyname><forenames>Gui-Rong</forenames></author><author><keyname>Yu</keyname><forenames>Yong</forenames></author></authors><title>Learning the Gain Values and Discount Factors of DCG</title><categories>cs.IR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Evaluation metrics are an essential part of a ranking system, and in the past
many evaluation metrics have been proposed in information retrieval and Web
search. Discounted Cumulated Gains (DCG) has emerged as one of the evaluation
metrics widely adopted for evaluating the performance of ranking functions used
in Web search. However, the two sets of parameters, gain values and discount
factors, used in DCG are determined in a rather ad-hoc way. In this paper we
first show that DCG is generally not coherent, meaning that comparing the
performance of ranking functions using DCG very much depends on the particular
gain values and discount factors used. We then propose a novel methodology that
can learn the gain values and discount factors from user preferences over
rankings. Numerical simulations illustrate the effectiveness of our proposed
methods. Please contact the authors for the full version of this work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5656</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5656</id><created>2012-12-22</created><authors><author><keyname>Tang</keyname><forenames>Zhongwei</forenames></author><author><keyname>von Gioi</keyname><forenames>Rafael Grompone</forenames></author><author><keyname>Monasse</keyname><forenames>Pascal</forenames></author><author><keyname>Morel</keyname><forenames>Jean-Michel</forenames></author></authors><title>High-precision camera distortion measurements with a &quot;calibration harp&quot;</title><categories>cs.CV</categories><journal-ref>JOSA A, Vol. 29, Issue 10, pp. 2134-2143 (2012)</journal-ref><doi>10.1364/JOSAA.29.002134</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the high precision measurement of the distortion of a
digital camera from photographs. Traditionally, this distortion is measured
from photographs of a flat pattern which contains aligned elements.
Nevertheless, it is nearly impossible to fabricate a very flat pattern and to
validate its flatness. This fact limits the attainable measurable precisions.
In contrast, it is much easier to obtain physically very precise straight lines
by tightly stretching good quality strings on a frame. Taking literally
&quot;plumb-line methods&quot;, we built a &quot;calibration harp&quot; instead of the classic flat
patterns to obtain a high precision measurement tool, demonstrably reaching
2/100 pixel precisions. The harp is complemented with the algorithms computing
automatically from harp photographs two different and complementary lens
distortion measurements. The precision of the method is evaluated on images
corrected by state-of-the-art distortion correction algorithms, and by popular
software. Three applications are shown: first an objective and reliable
measurement of the result of any distortion correction. Second, the harp
permits to control state-of-the art global camera calibration algorithms: It
permits to select the right distortion model, thus avoiding internal
compensation errors inherent to these methods. Third, the method replaces
manual procedures in other distortion correction methods, makes them fully
automatic, and increases their reliability and precision.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5663</identifier>
 <datestamp>2013-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5663</id><created>2012-12-22</created><authors><author><keyname>Barbier</keyname><forenames>Morgan</forenames><affiliation>GREYC</affiliation></author><author><keyname>Pernet</keyname><forenames>Cl&#xe9;ment</forenames><affiliation>INRIA Grenoble Rh&#xf4;ne-Alpes / LIG Laboratoire d'Informatique de Grenoble</affiliation></author><author><keyname>Quintin</keyname><forenames>Guillaume</forenames><affiliation>LIX</affiliation></author></authors><title>On the decoding of quasi-BCH codes</title><categories>cs.IT math.IT</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we investigate the structure of quasi-BCH codes. In the first
part of this paper we show that quasi-BCH codes can be derived from
Reed-Solomon codes over square matrices extending the known relation about
classical BCH and Reed-Solomon codes. This allows us to adapt the
Welch-Berlekamp algorithm to quasi-BCH codes. In the second part of this paper
we show that quasi-BCH codes can be seen as subcodes of interleaved
Reed-Solomon codes over finite fields. This provides another approach for
decoding quasi-BCH codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5664</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5664</id><created>2012-12-22</created><authors><author><keyname>Adelard</keyname><forenames>Laetitia</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Garde</keyname><forenames>Fran&#xe7;ois</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Pignolet-Tardan</keyname><forenames>Florence</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Boyer</keyname><forenames>Harry</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Gatina</keyname><forenames>Jean Claude</forenames><affiliation>PIMENT</affiliation></author></authors><title>Weather sequences for predicting HVAC system behaviour in residential
  units located in tropical climates</title><categories>cs.CE</categories><comments>Available from http://www.ibpsa.org/m_bs1997.asp. arXiv admin note:
  text overlap with arXiv:1212.3930</comments><proxy>ccsd</proxy><journal-ref>IBPSA Building Simulation 97, Prague : Czech Republic (1997)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The purpose of our research deals with the description of a methodology for
the definition of specific weather sequences and their influence on the energy
needs of HVAC system. We'll apply the method on the tropical Reunion Island.
The methodological approach based on a detailed analysis of weather sequences
leads to a classification of climatic situations that can be applied to the
site. These sequences have been used to simulate buildings and air handling
systems thanks to a thermal simulation code, CODYRUN. Results bring to the
light how necessary it is to have coherent meteorological data for this kind of
simulation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5665</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5665</id><created>2012-12-22</created><authors><author><keyname>Garde</keyname><forenames>Fran&#xe7;ois</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Boyer</keyname><forenames>Harry</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Pignolet</keyname><forenames>Florence</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Lucas</keyname><forenames>Franck</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Brau</keyname><forenames>Jean</forenames><affiliation>CETHIL</affiliation></author></authors><title>Multiple model approach and experimental validation of a residential
  air-to-air heat pump</title><categories>cs.CE</categories><proxy>ccsd</proxy><journal-ref>Clima 2000, Li\`ege : Belgium (1997)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The beginning of this work is the achievement of a design tool, which is a
multiple model software called &quot; CODYRUN &quot;, suitable for professionnals and
usable by researchers. The original aspect of this software is that the
designer has at his disposal a wide panel of choices between different heat
transfer models More precisely, it consists in a multizone software integrating
both natural ventilation and moisture tranfers . This software is developed on
PC micro computer and gets advantage of the Microsoft WINDOWS front-end. Most
of time, HVAC systems and specially domestic air conditioners, are taken into
account in a very simplified way, or in a elaborated one. On one side,they are
just supposed to supply the demand of cooling loads with an ideal control loop
(no delay between the sollicitations and the time response of the system), The
available outputs are initially the hourly cooling and heating consumptions
without integrating the real caracteristics of the HVAC system This paper is
also following the same multiple model approach than for the building modelling
by defining different modelling levels for the air conditionning systems, from
a very simplified one to a detailled one. An experimental validation is
achieved in order to compare the sensitivity of each defined model and to point
out the interaction between the thermal behaviour of the envelop and the
electrical system consumption. For validation purposes, we will describe the
data acquisition system. and the used real size test cell located in the
University of Reunion island, Indian Ocean.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5667</identifier>
 <datestamp>2013-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5667</id><created>2012-12-22</created><updated>2013-01-27</updated><authors><author><keyname>Fareed</keyname><forenames>Muhammad Mehboob</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author></authors><title>Efficient Incremental Relaying</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel relaying scheme which improves the spectral efficiency of
cooperative diversity systems by utilizing limited feedback from destination.
Our scheme capitalizes on the fact that relaying is only required when direct
transmission suffers deep fading. We calculate the packet error rate for the
proposed efficient incremental relaying scheme with both amplify and forward
and decode and forward relaying. Numerical results are also presented to verify
their analytical counterparts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5668</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5668</id><created>2012-12-22</created><authors><author><keyname>Ahrens</keyname><forenames>Benedikt</forenames></author></authors><title>Initial Semantics for Reduction Rules</title><categories>math.LO cs.LO</categories><comments>Extended version of arXiv:1206.4547, proves a variant of a result of
  PhD thesis arXiv:1206.4556</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give an algebraic characterization of the syntax and operational semantics
of a class of simply--typed languages, such as the language PCF: we
characterize simply--typed syntax with variable binding and equipped with
reduction rules via a universal property, namely as the initial object of some
category of models. For this purpose, we employ techniques developed in two
previous works: in a first work we model syntactic translations between
languages over different sets of types as initial morphisms in a category of
models. In a second work we characterize untyped syntax with reduction rules as
initial object in a category of models. In the present work, we show that the
techniques used there are modular enough to be combined: we thus characterize
simply--typed syntax with reduction rules as initial object in a category. The
universal property yields an operator which allows to specify translations ---
that are semantically faithful by construction --- between languages over
possibly different sets of types.
  We specify a language by a 2-signature, that is, a signature on two levels:
the syntactic level specifies the types and terms of the language, and
associates a type to each term. The semantic level specifies, through
inequations, reduction rules on the terms of the language. To any given
2-signature we associate a category of models. We prove that this category has
an initial object, which integrates the types and terms freely generated by the
2--signature, and the reduction relation on those terms generated by its
inequations. We call this object the (programming) language generated by the
2--signature.
  This paper is an extended version of an article published in the proceedings
of WoLLIC 2012.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5679</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5679</id><created>2012-12-22</created><authors><author><keyname>Fan</keyname><forenames>Yun</forenames></author><author><keyname>Ling</keyname><forenames>San</forenames></author><author><keyname>Liu</keyname><forenames>Hongwei</forenames></author><author><keyname>Shen</keyname><forenames>Jing</forenames></author><author><keyname>Xing</keyname><forenames>Chaoping</forenames></author></authors><title>Cumulative Distance Enumerators of Random Codes and their Thresholds</title><categories>cs.IT math.IT</categories><msc-class>94B65, 94B99, 82B26</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cumulative weight enumerators of random linear codes are introduced, their
asymptotic properties are studied, and very sharp thresholds are exhibited; as
a consequence, it is shown that the asymptotic Gilbert-Varshamov bound is a
very sharp threshold point for the density of the linear codes whose relative
distance is greater than a given positive number. For arbitrary random codes,
similar settings and results are exhibited; in particular, the very sharp
threshold point for the density of the codes whose relative distance is greater
than a given positive number is located at half the asymptotic
Gilbert-Varshamov bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5687</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5687</id><created>2012-12-22</created><updated>2013-12-13</updated><authors><author><keyname>La Guardia</keyname><forenames>Giuliano G.</forenames></author></authors><title>On the Construction of Nonbinary Quantum BCH Codes</title><categories>quant-ph cs.IT math.IT</categories><journal-ref>IEEE Transactions on Information Theory 60(3), 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Four quantum code constructions generating several new families of good
nonbinary quantum nonprimitive non-narrow-sense Bose-Chaudhuri-Hocquenghem
(BCH) codes are presented in this paper. The first two ones are based on
Calderbank-Shor-Steane (CSS) construction derived from two nonprimitive BCH
codes, not necessarily self-orthogonal. The third one is based on nonbinary
Steane's enlargement of CSS codes applied to suitable sub-families of
nonprimitive non-narrow-sense BCH codes. The fourth construction is derived
from suitable sub-families of Hermitian self-orthogonal nonprimitive
non-narrow-sense BCH codes. These constructions generate new families of
quantum BCH codes whose parameters are better than the ones available in the
literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5692</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5692</id><created>2012-12-22</created><authors><author><keyname>Benton</keyname><forenames>Nick</forenames></author><author><keyname>Hofmann</keyname><forenames>Martin</forenames></author><author><keyname>Nigam</keyname><forenames>Vivek</forenames></author></authors><title>Abstract Effects and Proof-Relevant Logical Relations</title><categories>cs.PL cs.LO</categories><acm-class>D.3.3; F.3.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a novel variant of logical relations that maps types not merely
to partial equivalence relations on values, as is commonly done, but rather to
a proof-relevant generalisation thereof, namely setoids. The objects of a
setoid establish that values inhabit semantic types, whilst its morphisms are
understood as proofs of semantic equivalence. The transition to proof-relevance
solves two well-known problems caused by the use of existential quantification
over future worlds in traditional Kripke logical relations: failure of
admissibility, and spurious functional dependencies. We illustrate the novel
format with two applications: a direct-style validation of Pitts and Stark's
equivalences for &quot;new&quot; and a denotational semantics for a region-based effect
system that supports type abstraction in the sense that only externally visible
effects need to be tracked; non-observable internal modifications, such as the
reorganisation of a search tree or lazy initialisation, can count as `pure' or
`read only'. This `fictional purity' allows clients of a module soundly to
validate more effect-based program equivalences than would be possible with
traditional effect systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5701</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5701</id><created>2012-12-22</created><authors><author><keyname>Zeiler</keyname><forenames>Matthew D.</forenames></author></authors><title>ADADELTA: An Adaptive Learning Rate Method</title><categories>cs.LG</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel per-dimension learning rate method for gradient descent
called ADADELTA. The method dynamically adapts over time using only first order
information and has minimal computational overhead beyond vanilla stochastic
gradient descent. The method requires no manual tuning of a learning rate and
appears robust to noisy gradient information, different model architecture
choices, various data modalities and selection of hyperparameters. We show
promising results compared to other methods on the MNIST digit classification
task using a single machine and on a large scale voice dataset in a distributed
cluster environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5711</identifier>
 <datestamp>2016-01-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5711</id><created>2012-12-22</created><updated>2013-03-29</updated><authors><author><keyname>Cohen</keyname><forenames>Andrew R.</forenames><affiliation>Electrical and Computer Engineering, Drexel University, Philadelphia</affiliation></author><author><keyname>Vitanyi</keyname><forenames>Paul M. B.</forenames><affiliation>CWI and University of Amsterdam</affiliation></author></authors><title>Normalized Compression Distance of Multisets with Applications</title><categories>cs.CV cs.IT math.IT physics.data-an</categories><comments>LaTeX 28 pages, 3 figures. This version is changed from the
  preliminary version to the final version. Updates of the theory. How to
  compute it, special recepies for classification, more applications and better
  results (see abstract and especially the detailed results in the paper). The
  title was changed to reflect this. In v4 corrected the proof of Theorem III-7</comments><acm-class>I.5.3; H.3.3; E.4; J.3</acm-class><journal-ref>IEEE Trans. Pattern Analysis and Machine Intelligence, 37:8(2015),
  1602-1614</journal-ref><doi>10.1109/TPAMI.2014.2375175</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Normalized compression distance (NCD) is a parameter-free, feature-free,
alignment-free, similarity measure between a pair of finite objects based on
compression. However, it is not sufficient for all applications. We propose an
NCD of finite multisets (a.k.a. multiples) of finite objects that is also a
metric. Previously, attempts to obtain such an NCD failed. We cover the entire
trajectory from theoretical underpinning to feasible practice. The new NCD for
multisets is applied to retinal progenitor cell classification questions and to
related synthetically generated data that were earlier treated with the
pairwise NCD. With the new method we achieved significantly better results.
Similarly for questions about axonal organelle transport. We also applied the
new NCD to handwritten digit recognition and improved classification accuracy
significantly over that of pairwise NCD by incorporating both the pairwise and
NCD for multisets. In the analysis we use the incomputable Kolmogorov
complexity that for practical purposes is approximated from above by the length
of the compressed version of the file involved, using a real-world compression
program.
  Index Terms--- Normalized compression distance, multisets or multiples,
pattern recognition, data mining, similarity, classification, Kolmogorov
complexity, retinal progenitor cells, synthetic data, organelle transport,
handwritten character recognition
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5720</identifier>
 <datestamp>2013-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5720</id><created>2012-12-22</created><updated>2013-01-10</updated><authors><author><keyname>Yu</keyname><forenames>Yen-Yun</forenames></author><author><keyname>Fletcher</keyname><forenames>P. Thomas</forenames></author><author><keyname>Awate</keyname><forenames>Suyash P.</forenames></author></authors><title>Hierarchical Graphical Models for Multigroup Shape Analysis using
  Expectation Maximization with Sampling in Kendall's Shape Space</title><categories>cs.CV</categories><comments>9 pages, 7 figures, International Conference on Machine Learning 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a novel framework for multi-group shape analysis relying
on a hierarchical graphical statistical model on shapes within a population.The
framework represents individual shapes as point setsmodulo translation,
rotation, and scale, following the notion in Kendall shape space.While
individual shapes are derived from their group shape model, each group shape
model is derived from a single population shape model. The hierarchical model
follows the natural organization of population data and the top level in the
hierarchy provides a common frame of reference for multigroup shape analysis,
e.g. classification and hypothesis testing. Unlike typical shape-modeling
approaches, the proposed model is a generative model that defines a joint
distribution of object-boundary data and the shape-model variables.
Furthermore, it naturally enforces optimal correspondences during the process
of model fitting and thereby subsumes the so-called correspondence problem. The
proposed inference scheme employs an expectation maximization (EM) algorithm
that treats the individual and group shape variables as hidden random variables
and integrates them out before estimating the parameters (population mean and
variance and the group variances). The underpinning of the EM algorithm is the
sampling of pointsets, in Kendall shape space, from their posterior
distribution, for which we exploit a highly-efficient scheme based on
Hamiltonian Monte Carlo simulation. Experiments in this paper use the fitted
hierarchical model to perform (1) hypothesis testing for comparison between
pairs of groups using permutation testing and (2) classification for image
retrieval. The paper validates the proposed framework on simulated data and
demonstrates results on real data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5764</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5764</id><created>2012-12-23</created><authors><author><keyname>Ghoneim</keyname><forenames>Ayman</forenames></author><author><keyname>Williamson</keyname><forenames>Robert C.</forenames></author></authors><title>Strategy-Proof Prediction Markets</title><categories>cs.GT cs.MA</categories><comments>9 pages</comments><acm-class>J.4; I.2.11</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Prediction markets aggregate agents' beliefs regarding a future event, where
each agent is paid based on the accuracy of its reported belief when compared
to the realized outcome. Agents may strategically manipulate the market (e.g.,
delay reporting, make false reports) aiming for higher expected payments, and
hence the accuracy of the market's aggregated information will be in question.
In this study, we present a general belief model that captures how agents
influence each other beliefs, and show that there are three necessary and
sufficient conditions for agents to behave truthfully in scoring rule based
markets (SRMs). Given that these conditions are restrictive and difficult to
satisfy in real-life, we present novel strategy-proof SRMs where agents are
truthful while dismissing all these conditions. Although achieving such a
strong form of truthfulness increases the worst-case loss in the new markets,
we show that this is the minimum loss required to dismiss these conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5765</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5765</id><created>2012-12-23</created><authors><author><keyname>Li</keyname><forenames>Quan</forenames></author><author><keyname>Scruggs</keyname><forenames>Jeffrey T.</forenames></author></authors><title>Stochastic Subspace Identification: Valid Model, Asymptotics and Model
  Error Bounds</title><categories>cs.SY math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates the ability of the stochastic subspace identification
technique to return a valid model from finite measurement data, its asymptotic
properties as the data set becomes large, and asymptotic error bounds of the
identified model (in terms of $\mathcal{H}_2$ and $\mathcal{H}_{\infty}$
norms). First, a new and straightforward LMI-based approach is proposed, which
returns a valid identified model even in cases where the system poles are very
close to unit circle and there is insufficient data to accurately estimate the
covariance matrices. The approach, which is demonstrated by numerical examples,
provides an altenative to other techniques which often fail under these
circumstances. Then, an explicit expression for the variance of the
asymptotically normally distributed sample output covariance matrices and
block-Hankel matrix are derived. From this result, together with perturbation
techniques, error bounds for the state-space matrices in the innovations model
are derived, for a given confidence level. This result is in turn used to
derive several error bounds for the identified transfer functions, for a given
confidence level. One is an explicit $\mathcal{H}_2$ bound. Additionally, two
$\mathcal{H}_{\infty}$ error bounds are derived; one via perturbation analysis,
and the other via an LMI-based technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5766</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5766</id><created>2012-12-23</created><authors><author><keyname>Devanur</keyname><forenames>Nikhil R.</forenames></author><author><keyname>Ha</keyname><forenames>Bach Q.</forenames></author><author><keyname>Hartline</keyname><forenames>Jason D.</forenames></author></authors><title>Prior-free Auctions for Budgeted Agents</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider prior-free auctions for revenue and welfare maximization when
agents have a common budget. The abstract environments we consider are ones
where there is a downward-closed and symmetric feasibility constraint on the
probabilities of service of the agents. These environments include position
auctions where slots with decreasing click-through rates are auctioned to
advertisers. We generalize and characterize the envy-free benchmark from
Hartline and Yan (2011) to settings with budgets and characterize the optimal
envy-free outcomes for both welfare and revenue. We give prior-free mechanisms
that approximate these benchmarks. A building block in our mechanism is a
clinching auction for position auction environments. This auction is a
generalization of the multi-unit clinching auction of Dobzinski et al. (2008)
and a special case of the polyhedral clinching auction of Goel et al. (2012).
For welfare maximization, we show that this clinching auction is a good
approximation to the envy-free optimal welfare for position auction
environments. For profit maximization, we generalize the random sampling profit
extraction auction from Fiat et al. (2002) for digital goods to give a
10.0-approximation to the envy-free optimal revenue in symmetric,
downward-closed environments. The profit maximization question is of interest
even without budgets and our mechanism is a 7.5-approximation which improving
on the 30.4 bound of Ha and Hartline (2012).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5768</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5768</id><created>2012-12-23</created><updated>2014-02-07</updated><authors><author><keyname>Olshevsky</keyname><forenames>Alex</forenames></author></authors><title>Consensus with Ternary Messages</title><categories>math.OC cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a protocol for real-valued average consensus by networks of agents
which exchange only a single message from the ternary alphabet {-1,0,1} between
neighbors at each step. Our protocol works on time-varying undirected graphs
subject to a connectivity condition, has a worst-case convergence time which is
polynomial in the number of agents and the initial values, and requires no
global knowledge about the graph topologies on the part of each node to
implement except for knowing an upper bound on the degrees of its neighbors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5776</identifier>
 <datestamp>2013-03-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5776</id><created>2012-12-23</created><updated>2013-03-04</updated><authors><author><keyname>El-Dosuky</keyname><forenames>M. A.</forenames></author><author><keyname>Rashad</keyname><forenames>M. Z.</forenames></author><author><keyname>Hamza</keyname><forenames>T. T.</forenames></author><author><keyname>EL-Bassiouny</keyname><forenames>A. H.</forenames></author></authors><title>Improving problem solving by exploiting the concept of symmetry</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the concept of symmetry and its role in problem solving. This
paper first defines precisely the elements that constitute a &quot;problem&quot; and its
&quot;solution,&quot; and gives several examples to illustrate these definitions. Given
precise definitions of problems, it is relatively straightforward to construct
a search process for finding solutions. Finally this paper attempts to exploit
the concept of symmetry in improving problem solving.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5777</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5777</id><created>2012-12-23</created><authors><author><keyname>El-Dosuky</keyname><forenames>M. A.</forenames></author><author><keyname>Rashad</keyname><forenames>M. Z.</forenames></author><author><keyname>Hamza</keyname><forenames>T. T.</forenames></author><author><keyname>EL-Bassiouny</keyname><forenames>A. H.</forenames></author></authors><title>Collaborating Robotics Using Nature-Inspired Meta-Heuristics</title><categories>cs.NE cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper introduces collaborating robots which provide the possibility of
enhanced task performance, high reliability and decreased. Collaborating-bots
are a collection of mobile robots able to self-assemble and to self-organize in
order to solve problems that cannot be solved by a single robot. These robots
combine the power of swarm intelligence with the flexibility of
self-reconfiguration as aggregate Collaborating-bots can dynamically change
their structure to match environmental variations. Collaborating robots are
more than just networks of independent agents, they are potentially
reconfigurable networks of communicating agents capable of coordinated sensing
and interaction with the environment. Robots are going to be an important part
of the future. Collaborating robots are limited in individual capability, but
robots deployed in large numbers can represent a strong force similar to a
colony of ants or swarm of bees. We present a mechanism for collaborating
robots based on swarm intelligence such as Ant colony optimization and Particle
swarm Optimization
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5782</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5782</id><created>2012-12-23</created><authors><author><keyname>Goseling</keyname><forenames>Jasper</forenames></author><author><keyname>Gastpar</keyname><forenames>Michael</forenames></author><author><keyname>Weber</keyname><forenames>Jos H.</forenames></author></authors><title>Random Access with Physical-layer Network Coding</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Leveraging recent progress in physical-layer network coding we propose a new
approach to random access: When packets collide, it is possible to recover a
linear combination of the packets at the receiver. Over many rounds of
transmission, the receiver can thus obtain many linear combinations and
eventually recover all original packets. This is by contrast to slotted ALOHA
where packet collisions lead to complete erasures. The throughput of the
proposed strategy is derived and shown to be significantly superior to the best
known strategies, including multipacket reception.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5789</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5789</id><created>2012-12-23</created><authors><author><keyname>Rif&#xe0;</keyname><forenames>J.</forenames></author><author><keyname>Solov'eva</keyname><forenames>F. I.</forenames></author><author><keyname>Villanueva</keyname><forenames>M.</forenames></author></authors><title>Self-embeddings of Hamming Steiner triple systems of small order and APN
  permutations</title><categories>cs.IT math.IT</categories><comments>Submitted to Design, Codes and Cryptography</comments><msc-class>94B15, 94A60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The classification, up to isomorphism, of all self-embedding monomial power
permutations of Hamming Steiner triple systems of order n=2^m-1 for small m, m
&lt; 23, is given. As far as we know, for m in {5,7,11,13,17,19}, all given
self-embeddings in closed surfaces are new. Moreover, they are cyclic for all m
and nonorientable at least for all m &lt; 21. For any non prime m, the
nonexistence of such self-embeddings in a closed surface is proven.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5791</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5791</id><created>2012-12-23</created><updated>2013-01-09</updated><authors><author><keyname>Xu</keyname><forenames>Kui</forenames></author><author><keyname>Ma</keyname><forenames>Wenfeng</forenames></author><author><keyname>Wu</keyname><forenames>Lianguo</forenames></author><author><keyname>Xie</keyname><forenames>Wei</forenames></author><author><keyname>Zhang</keyname><forenames>Dongmei</forenames></author><author><keyname>Xu</keyname><forenames>Youyun</forenames></author></authors><title>Carrier Frequency Offset Estimation Approach for Multicarrier
  Transmission on Hexagonal Time-Frequency Lattice</title><categories>cs.IT math.IT</categories><comments>6 pages. The paper has been accepted for publication at the IEEE
  Wireless Communications and Networking Conference (WCNC) 2013, Shanghai,
  China, Apr. 2013. Copyright transferred to IEEE</comments><doi>10.1109/TSP.2013.2284153</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper, a novel carrier frequency offset estimation approach,
including preamble structure, carrier frequency offset estimation algorithm, is
proposed for hexagonal multi-carrier transmission (HMCT) system. The
closed-form Cramer-Rao lower bound of the proposed carrier frequency offset
estimation scheme is given. Theoretical analyses and simulation results show
that the proposed preamble structure and carrier frequency offset estimation
algorithm for HMCT system obtains an approximation to the Cramer-Rao lower
bound mean square error (MSE) performance over the doubly dispersive (DD)
propagation channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5792</identifier>
 <datestamp>2013-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5792</id><created>2012-12-23</created><updated>2013-01-09</updated><authors><author><keyname>Xu</keyname><forenames>Kui</forenames></author><author><keyname>Xu</keyname><forenames>Youyun</forenames></author><author><keyname>Xia</keyname><forenames>Xiaochen</forenames></author><author><keyname>Zhang</keyname><forenames>Dongmei</forenames></author></authors><title>On Max-SINR Receiver for Hexagonal Multicarrier Transmission Over Doubly
  Dispersive Channel</title><categories>cs.IT math.IT</categories><comments>6 pages. The paper has been published in Proc. IEEE GLOBECOM 2012.
  Copyright transferred to IEEE. arXiv admin note: text overlap with
  arXiv:1212.5791</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper, a novel receiver for Hexagonal Multicarrier Transmission (HMT)
system based on the maximizing Signal-to-Interference-plus-Noise Ratio
(Max-SINR) criterion is proposed. Theoretical analysis shows that the prototype
pulse of the proposed Max-SINR receiver should adapt to the root mean square
(RMS) delay spread of the doubly dispersive (DD) channel with exponential power
delay profile and U-shape Doppler spectrum. Simulation results show that the
proposed Max-SINR receiver outperforms traditional projection scheme and
obtains an approximation to the theoretical upper bound SINR performance within
the full range of channel spread factor. Meanwhile, the SINR performance of the
proposed prototype pulse is robust to the estimation error between the
estimated value and the real value of time delay spread.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5796</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5796</id><created>2012-12-23</created><authors><author><keyname>Warnke</keyname><forenames>Lutz</forenames></author></authors><title>On the method of typical bounded differences</title><categories>math.CO cs.DM math.PR</categories><comments>25 pages</comments><msc-class>60C05 (Primary) 60F10, 60B99 (Secondary)</msc-class><journal-ref>Combinator. Probab. Comp. 25 (2015) 269-299</journal-ref><doi>10.1017/S0963548315000103</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Concentration inequalities are fundamental tools in probabilistic
combinatorics and theoretical computer science for proving that random
functions are near their means. Of particular importance is the case where f(X)
is a function of independent random variables X=(X_1, ..., X_n). Here the well
known bounded differences inequality (also called McDiarmid's or
Hoeffding-Azuma inequality) establishes sharp concentration if the function f
does not depend too much on any of the variables. One attractive feature is
that it relies on a very simple Lipschitz condition (L): it suffices to show
that |f(X)-f(X')| \leq c_k whenever X,X' differ only in X_k. While this is easy
to check, the main disadvantage is that it considers worst-case changes c_k,
which often makes the resulting bounds too weak to be useful.
  In this paper we prove a variant of the bounded differences inequality which
can be used to establish concentration of functions f(X) where (i) the typical
changes are small although (ii) the worst case changes might be very large. One
key aspect of this inequality is that it relies on a simple condition that (a)
is easy to check and (b) coincides with heuristic considerations why
concentration should hold. Indeed, given an event \Gamma that holds with very
high probability, we essentially relax the Lipschitz condition (L) to
situations where \Gamma occurs. The point is that the resulting typical changes
c_k are often much smaller than the worst case ones.
  To illustrate its application we consider the reverse H-free process, where H
is 2-balanced. We prove that the final number of edges in this process is
concentrated, and also determine its likely value up to constant factors. This
answers a question of Bollob\'as and Erd\H{o}s.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5801</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5801</id><created>2012-12-23</created><authors><author><keyname>Bassil</keyname><forenames>Youssef</forenames></author></authors><title>Image Steganography Method Based on Brightness Adjustment</title><categories>cs.CR cs.MM</categories><comments>LACSC Lebanese Association for Computational Sciences,
  http://www.lacsc.org. arXiv admin note: text overlap with arXiv:1212.2064</comments><journal-ref>Advances in Computer Science and its Applications (ACSA), Vol. 2,
  No. 2, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Steganography is an information hiding technique in which secret data are
secured by covering them into a computer carrier file without damaging the file
or changing its size. The difference between steganography and cryptography is
that steganography is a stealthy method of communication that only the
communicating parties are aware of; while, cryptography is an overt method of
communication that anyone is aware of, despite its payload is scribbled.
Typically, an irrecoverable steganography algorithm is the algorithm that makes
it hard for malicious third parties to discover how it works and how to recover
the secret data out of the carrier file. One popular way to achieve
irrecoverability is to digitally process the carrier file after hiding the
secret data into it. However, such process is irreversible as it would destroy
the concealed data. This paper proposes a new image steganography method for
textual data, as well as for any form of digital data, based on adjusting the
brightness of the carrier image after covering the secret data into it. The
algorithm used is parameterized as it can be configured using three different
parameters defined by the communicating parties. They include the amount of
brightness to apply on the carrier image after the completion of the covering
process, the color channels whose brightness should be adjusted, and the bytes
that should carry in the secret data. The novelty of the proposed method is
that it embeds bits of the secret data into the three LSBs of the bytes that
compose the carrier image in such a way that does not destroy the secret data
when restoring back the original brightness of the carrier image. The
simulation conducted proved that the proposed algorithm is valid and correct.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5815</identifier>
 <datestamp>2013-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5815</id><created>2012-12-23</created><updated>2012-12-28</updated><authors><author><keyname>Stumper</keyname><forenames>Jean-Francois</forenames></author><author><keyname>D&#xf6;tlinger</keyname><forenames>Alexander</forenames></author><author><keyname>Kennel</keyname><forenames>Ralph</forenames></author></authors><title>Classical Model Predictive Control of a Permanent Magnet Synchronous
  Motor</title><categories>cs.SY math.OC</categories><comments>Preprint of: European Power Electronics and Drives Journal (EPE J.),
  Vol. 22, No. 3, pages 24 - 31, 2012. Invited paper</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A model predictive control (MPC) scheme for a permanent-magnet synchronous
motor (PMSM) is presented. The torque controller optimizes a quadratic cost
consisting of control error and machine losses repeatedly, accounting the
voltage and current limitations. The scheme extensively relies on optimization,
to meet the runtime limitation, a suboptimal algorithm based on differential
flatness, continuous parameterization and linear programming is introduced.
  The multivariable controller exploits cross-coupling effects in the
long-range constrained predictive control strategy. The optimization results in
fast and smooth torque dynamics while inherently using field-weakening to
improve the power efficiency and the current dynamics in high speed operation.
As distinctive MPC feature, constraint handling is improved, instead of just
saturating the control input, field weakening is applied dynamically to bypass
the voltage limitation. The performance of the scheme is demonstrated by
experimental and numerical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5829</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5829</id><created>2012-12-23</created><authors><author><keyname>Dhillon</keyname><forenames>Harpreet S.</forenames></author><author><keyname>Ganti</keyname><forenames>Radha Krishna</forenames></author><author><keyname>Andrews</keyname><forenames>Jeffrey G.</forenames></author></authors><title>Modeling Non-Uniform UE Distributions in Downlink Cellular Networks</title><categories>cs.IT math.IT stat.AP</categories><comments>Submitted to IEEE Wireless Communications Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A recent way to model and analyze downlink cellular networks is by using
random spatial models. Assuming user equipment (UE) distribution to be uniform,
the analysis is performed at a typical UE located at the origin. While this
method of sampling UEs provides statistics averaged over the UE locations, it
is not possible to sample cell interior and cell edge UEs separately. This
complicates the problem of analyzing deployment scenarios involving non-uniform
distribution of UEs, especially when the locations of the UEs and the base
stations (BSs) are dependent. To facilitate this separation, we propose a new
tractable method of sampling UEs by conditionally thinning the BS point process
and show that the resulting framework can be used as a tractable generative
model to study cellular networks with non-uniform UE distribution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5841</identifier>
 <datestamp>2013-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5841</id><created>2012-12-23</created><updated>2013-01-01</updated><authors><author><keyname>Zinovyev</keyname><forenames>Andrei</forenames></author><author><keyname>Mirkes</keyname><forenames>Evgeny</forenames></author></authors><title>Data complexity measured by principal graphs</title><categories>cs.LG cs.IT math.IT</categories><comments>Computers and Mathematics with Applications, in press</comments><doi>10.1016/j.camwa.2012.12.009</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How to measure the complexity of a finite set of vectors embedded in a
multidimensional space? This is a non-trivial question which can be approached
in many different ways. Here we suggest a set of data complexity measures using
universal approximators, principal cubic complexes. Principal cubic complexes
generalise the notion of principal manifolds for datasets with non-trivial
topologies. The type of the principal cubic complex is determined by its
dimension and a grammar of elementary graph transformations. The simplest
grammar produces principal trees.
  We introduce three natural types of data complexity: 1) geometric (deviation
of the data's approximator from some &quot;idealized&quot; configuration, such as
deviation from harmonicity); 2) structural (how many elements of a principal
graph are needed to approximate the data), and 3) construction complexity (how
many applications of elementary graph transformations are needed to construct
the principal object starting from the simplest one).
  We compute these measures for several simulated and real-life data
distributions and show them in the &quot;accuracy-complexity&quot; plots, helping to
optimize the accuracy/complexity ratio. We discuss various issues connected
with measuring data complexity. Software for computing data complexity measures
from principal cubic complexes is provided as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5855</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5855</id><created>2012-12-23</created><authors><author><keyname>Rhim</keyname><forenames>Joong Bum</forenames></author><author><keyname>Goyal</keyname><forenames>Vivek K.</forenames></author></authors><title>Keep Ballots Secret: On the Futility of Social Learning in Decision
  Making by Voting</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that social learning is not useful in a model of team binary decision
making by voting, where each vote carries equal weight. Specifically, we
consider Bayesian binary hypothesis testing where agents have any
conditionally-independent observation distribution and their local decisions
are fused by any L-out-of-N fusion rule. The agents make local decisions
sequentially, with each allowed to use its own private signal and all precedent
local decisions. Though social learning generally occurs in that precedent
local decisions affect an agent's belief, optimal team performance is obtained
when all precedent local decisions are ignored. Thus, social learning is
futile, and secret ballots are optimal. This contrasts with typical studies of
social learning because we include a fusion center rather than concentrating on
the performance of the latest-acting agents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5860</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5860</id><created>2012-12-23</created><authors><author><keyname>Zhu</keyname><forenames>Shenghuo</forenames></author></authors><title>A short note on the tail bound of Wishart distribution</title><categories>math.ST cs.LG stat.TH</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the tail bound of the emperical covariance of multivariate normal
distribution. Following the work of (Gittens &amp; Tropp, 2011), we provide a tail
bound with a small constant.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5863</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5863</id><created>2012-12-23</created><authors><author><keyname>Momma</keyname><forenames>Michinari</forenames></author><author><keyname>Chi</keyname><forenames>Yun</forenames></author><author><keyname>Lin</keyname><forenames>Yuanqing</forenames></author><author><keyname>Zhu</keyname><forenames>Shenghuo</forenames></author><author><keyname>Yang</keyname><forenames>Tianbao</forenames></author></authors><title>Influence Analysis in the Blogosphere</title><categories>cs.SI physics.soc-ph</categories><report-no>NEC Labs TR 2009-L111</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we analyze influence in the blogosphere. Recently, influence
analysis has become an increasingly important research topic, as online
communities, such as social networks and e-commerce sites, playing a more and
more significant role in our daily life. However, so far few studies have
succeeded in extracting influence from online communities in a satisfactory
way. One of the challenges that limited previous researches is that it is
difficult to capture user behaviors. Consequently, the influence among users
could only be inferred in an indirect and heuristic way, which is inaccurate
and noise-prone. In this study, we conduct an extensive investigation in regard
to influence among bloggers at a Japanese blog web site, BIGLOBE. By processing
the log files of the web servers, we are able to accurately extract the
activities of BIGLOBE members in terms of writing their blog posts and reading
other member's posts. Based on these activities, we propose a principled
framework to detect influence among the members with high confidence level.
From the extracted influence, we conduct in-depth analysis on how influence
varies over different topics and how influence varies over different members.
We also show the potentials of leveraging the extracted influence to make
personalized recommendation in BIGLOBE. To our best knowledge, this is one of
the first studies that capture and analyze influence in the blogosphere in such
a large scale.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5877</identifier>
 <datestamp>2013-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5877</id><created>2012-12-24</created><updated>2013-03-28</updated><authors><author><keyname>Karrenbauer</keyname><forenames>Andreas</forenames></author><author><keyname>W&#xf6;ll</keyname><forenames>Dominik</forenames></author></authors><title>Blinking Molecule Tracking</title><categories>cs.CV cs.DM</categories><comments>12th International Symposium on Experimental Algorithms 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss a method for tracking individual molecules which globally
optimizes the likelihood of the connections between molecule positions fast and
with high reliability even for high spot densities and blinking molecules. Our
method works with cost functions which can be freely chosen to combine costs
for distances between spots in space and time and which can account for the
reliability of positioning a molecule. To this end, we describe a top-down
polyhedral approach to the problem of tracking many individual molecules. This
immediately yields an effective implementation using standard linear
programming solvers. Our method can be applied to 2D and 3D tracking.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5880</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5880</id><created>2012-12-24</created><updated>2013-04-07</updated><authors><author><keyname>Wolff</keyname><forenames>Ran</forenames></author></authors><title>Local Thresholding in General Network Graphs</title><categories>cs.DC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Local thresholding algorithms were first presented more than a decade ago and
have since been applied to a variety of data mining tasks in peer-to-peer
systems, wireless sensor networks, and in grid systems. One critical assumption
made by those algorithms has always been cycle-free routing. The existence of
even one cycle may lead all peers to the wrong outcome. Outside the lab,
unfortunately, cycle freedom is not easy to achieve.
  This work is the first to lift the requirement of cycle freedom by presenting
a local thresholding algorithm suitable for general network graphs. The
algorithm relies on a new repositioning of the problem in weighted vector
arithmetics, on a new stopping rule, whose proof does not require that the
network be cycle free, and on new methods for balance correction when the
stopping rule fails.
  The new stopping and update rules permit calculation of the very same
functions that were calculable using previous algorithms, which do assume cycle
freedom. The algorithm is implemented on a standard peer-to-peer simulator and
is validated for networks of up to 80,000 peers, organized in three different
topologies, which are representative of the topology of major current
distributed systems: the Internet, structured peer-to-peer systems, and
wireless sensor networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5882</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5882</id><created>2012-12-24</created><authors><author><keyname>Baum</keyname><forenames>Marcus</forenames></author><author><keyname>Hanebeck</keyname><forenames>Uwe D.</forenames></author></authors><title>The Kernel-SME Filter for Multiple Target Tracking</title><categories>cs.SY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel method called Kernel-SME filter for tracking multiple
targets when the association of the measurements to the targets is unknown. The
method is a further development of the Symmetric Measurement Equation (SME)
filter, which removes the data association uncertainty of the original
measurement equation with the help of a symmetric transformation. The
underlying idea of the Kernel-SME filter is to construct a symmetric
transformation by means of mapping the measurements to a Gaussian mixture. This
transformation is scalable to a large number of targets and allows for deriving
a Gaussian state estimator that has a cubic time complexity in the number of
targets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5895</identifier>
 <datestamp>2014-04-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5895</id><created>2012-12-24</created><authors><author><keyname>Alviano</keyname><forenames>Mario</forenames></author><author><keyname>Faber</keyname><forenames>Wolfgang</forenames></author><author><keyname>Woltran</keyname><forenames>Stefan</forenames></author></authors><title>Complexity of super-coherence problems in ASP</title><categories>cs.LO cs.CC</categories><comments>22 pages, 1 figure, journal paper</comments><msc-class>68Q17</msc-class><acm-class>I.2.3; F.2.2; F.4.1</acm-class><doi>10.1017/S147106841300001X</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Adapting techniques from database theory in order to optimize Answer Set
Programming (ASP) systems, and in particular the grounding components of ASP
systems, is an important topic in ASP. In recent years, the Magic Set method
has received some interest in this setting, and a variant of it, called DMS,
has been proposed for ASP. However, this technique has a caveat, because it is
not correct (in the sense of being query-equivalent) for all ASP programs. In
recent work, a large fragment of ASP programs, referred to as super-coherent
programs, has been identified, for which DMS is correct. The fragment contains
all programs which possess at least one answer set, no matter which set of
facts is added to them. Two open question remained: How complex is it to
determine whether a given program is super-coherent? Does the restriction to
super-coherent programs limit the problems that can be solved? Especially the
first question turned out to be quite difficult to answer precisely. In this
paper, we formally prove that deciding whether a propositional program is
super-coherent is \Pi^P_3-complete in the disjunctive case, while it is
\Pi^P_2-complete for normal programs. The hardness proofs are the difficult
part in this endeavor: We proceed by characterizing the reductions by the
models and reduct models which the ASP programs should have, and then provide
instantiations that meet the given specifications. Concerning the second
question, we show that all relevant ASP reasoning tasks can be transformed into
tasks over super-coherent programs, even though this transformation is more of
theoretical than practical interest.
  To appear in Theory and Practice of Logic Programming (TPLP).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5921</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5921</id><created>2012-12-24</created><authors><author><keyname>Carreira-Perpi&#xf1;&#xe1;n</keyname><forenames>Miguel &#xc1;.</forenames></author><author><keyname>Wang</keyname><forenames>Weiran</forenames></author></authors><title>Distributed optimization of deeply nested systems</title><categories>cs.LG cs.NE math.OC stat.ML</categories><comments>21 pages, 7 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In science and engineering, intelligent processing of complex signals such as
images, sound or language is often performed by a parameterized hierarchy of
nonlinear processing layers, sometimes biologically inspired. Hierarchical
systems (or, more generally, nested systems) offer a way to generate complex
mappings using simple stages. Each layer performs a different operation and
achieves an ever more sophisticated representation of the input, as, for
example, in an deep artificial neural network, an object recognition cascade in
computer vision or a speech front-end processing. Joint estimation of the
parameters of all the layers and selection of an optimal architecture is widely
considered to be a difficult numerical nonconvex optimization problem,
difficult to parallelize for execution in a distributed computation
environment, and requiring significant human expert effort, which leads to
suboptimal systems in practice. We describe a general mathematical strategy to
learn the parameters and, to some extent, the architecture of nested systems,
called the method of auxiliary coordinates (MAC). This replaces the original
problem involving a deeply nested function with a constrained problem involving
a different function in an augmented space without nesting. The constrained
problem may be solved with penalty-based methods using alternating optimization
over the parameters and the auxiliary coordinates. MAC has provable
convergence, is easy to implement reusing existing algorithms for single
layers, can be parallelized trivially and massively, applies even when
parameter derivatives are not available or not desirable, and is competitive
with state-of-the-art nonlinear optimizers even in the serial computation
setting, often providing reasonable models within a few iterations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5932</identifier>
 <datestamp>2013-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5932</id><created>2012-12-24</created><updated>2012-12-27</updated><authors><author><keyname>Lahti</keyname><forenames>Leo</forenames></author><author><keyname>Torrente</keyname><forenames>Aurora</forenames></author><author><keyname>Elo</keyname><forenames>Laura L.</forenames></author><author><keyname>Brazma</keyname><forenames>Alvis</forenames></author><author><keyname>Rung</keyname><forenames>Johan</forenames></author></authors><title>Fully scalable online-preprocessing algorithm for short oligonucleotide
  microarray atlases</title><categories>q-bio.QM cs.CE cs.LG q-bio.GN stat.AP stat.ML</categories><comments>20 pages, 3 figures, 1 supplementary PDF</comments><journal-ref>Leo Lahti, Aurora Torrente, Laura L. Elo, Alvis Brazma, Johan
  Rung. A fully scalable online pre-processing algorithm for short
  oligonucleotide microarray atlases. Nucleic Acids Research, Online April 5,
  2013</journal-ref><doi>10.1093/nar/gkt229</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Accumulation of standardized data collections is opening up novel
opportunities for holistic characterization of genome function. The limited
scalability of current preprocessing techniques has, however, formed a
bottleneck for full utilization of contemporary microarray collections. While
short oligonucleotide arrays constitute a major source of genome-wide profiling
data, scalable probe-level preprocessing algorithms have been available only
for few measurement platforms based on pre-calculated model parameters from
restricted reference training sets. To overcome these key limitations, we
introduce a fully scalable online-learning algorithm that provides tools to
process large microarray atlases including tens of thousands of arrays. Unlike
the alternatives, the proposed algorithm scales up in linear time with respect
to sample size and is readily applicable to all short oligonucleotide
platforms. This is the only available preprocessing algorithm that can learn
probe-level parameters based on sequential hyperparameter updates at small,
consecutive batches of data, thus circumventing the extensive memory
requirements of the standard approaches and opening up novel opportunities to
take full advantage of contemporary microarray data collections. Moreover,
using the most comprehensive data collections to estimate probe-level effects
can assist in pinpointing individual probes affected by various biases and
provide new tools to guide array design and quality control. The implementation
is freely available in R/Bioconductor at
http://www.bioconductor.org/packages/devel/bioc/html/RPA.html
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5943</identifier>
 <datestamp>2013-09-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5943</id><created>2012-12-24</created><updated>2013-09-09</updated><authors><author><keyname>Thij</keyname><forenames>Marijn ten</forenames></author><author><keyname>Volkovich</keyname><forenames>Yana</forenames></author><author><keyname>Laniado</keyname><forenames>David</forenames></author><author><keyname>Kaltenbrunner</keyname><forenames>Andreas</forenames></author></authors><title>Modeling page-view dynamics on Wikipedia</title><categories>cs.CY cs.SI physics.data-an physics.soc-ph</categories><comments>6 pages, 13 figures Proceedings of the European Conference on Complex
  Systems (ECCS 2013)</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  We introduce a model for predicting page-view dynamics of promoted content.
The regularity of the content promotion process on Wikipedia provides excellent
experimental conditions which favour detailed modelling. We show that the
popularity of an article featured on Wikipedia's main page decays exponentially
in time if the circadian cycles of the users are taken into account. Our model
can be explained as the result of individual Poisson processes and is validated
through empirical measurements. It provides a simpler explanation for the
evolution of content popularity than previous studies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5951</identifier>
 <datestamp>2014-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5951</id><created>2012-12-24</created><updated>2014-02-07</updated><authors><author><keyname>Ceccherini-Silberstein</keyname><forenames>Tullio</forenames></author><author><keyname>Coornaert</keyname><forenames>Michel</forenames></author><author><keyname>Fiorenzi</keyname><forenames>Francesca</forenames></author><author><keyname>Sunic</keyname><forenames>Zoran</forenames></author></authors><title>Cellular automata between sofic tree shifts</title><categories>cs.FL cs.DM</categories><journal-ref>Theoret. Comput. Sci. 506 (2013), 79-101</journal-ref><doi>10.1016/j.tcs.2013.07.007</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the sofic tree shifts of $A^{\Sigma^*}$, where $\Sigma^*$ is a
regular rooted tree of finite rank. In particular, we give their
characterization in terms of unrestricted Rabin automata. We show that if $X
\subset A^{\Sigma^*}$ is a sofic tree shift, then the configurations in $X$
whose orbit under the shift action is finite are dense in $X$, and, as a
consequence of this, we deduce that every injective cellular automata
$\tau\colon X \to X$ is surjective. Moreover, a characterization of sofic tree
shifts in terms of general Rabin automata is given.
  We present an algorithm for establishing whether two unrestricted Rabin
automata accept the same sofic tree shift or not. This allows us to prove the
decidability of the surjectivity problem for cellular automata between sofic
tree shifts. We also prove the decidability of the injectivity problem for
cellular automata defined on a tree shift of finite type.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5956</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5956</id><created>2012-12-24</created><authors><author><keyname>Wang</keyname><forenames>Jingxin K.</forenames></author><author><keyname>Ding</keyname><forenames>Jianrui</forenames></author><author><keyname>Niu</keyname><forenames>Tian</forenames></author></authors><title>Interoperability and Standardization of Intercloud Cloud Computing</title><categories>cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cloud computing is getting mature, and the interoperability and
standardization of the clouds is still waiting to be solved. This paper
discussed the interoperability among clouds about message transmission, data
transmission and virtual machine transfer. Starting from IEEE Pioneering Cloud
Computing Initiative, this paper discussed about standardization of the cloud
computing, especially intercloud cloud computing. This paper also discussed the
standardization from the market-oriented view.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5959</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5959</id><created>2012-12-24</created><authors><author><keyname>Nalchigar</keyname><forenames>Soroosh</forenames></author><author><keyname>Weber</keyname><forenames>Ingmar</forenames></author></authors><title>A Large-Scale Study of Online Shopping Behavior</title><categories>cs.CY</categories><acm-class>H.1.2; H.3.m</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The continuous growth of electronic commerce has stimulated great interest in
studying online consumer behavior. Given the significant growth in online
shopping, better understanding of customers allows better marketing strategies
to be designed. While studies of online shopping attitude are widespread in the
literature, studies of browsing habits differences in relation to online
shopping are scarce.
  This research performs a large scale study of the relationship between
Internet browsing habits of users and their online shopping behavior. Towards
this end, we analyze data of 88,637 users who have bought more in total half a
milion products from the retailer sites Amazon and Walmart. Our results
indicate that even coarse-grained Internet browsing behavior has predictive
power in terms of what users will buy online. Furthermore, we discover both
surprising (e.g., &quot;expensive products do not come with more effort in terms of
purchase&quot;) and expected (e.g., &quot;the more loyal a user is to an online shop, the
less effort they spend shopping&quot;) facts.
  Given the lack of large-scale studies linking online browsing and online
shopping behavior, we believe that this work is of general interest to people
working in related areas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5969</identifier>
 <datestamp>2013-08-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5969</id><created>2012-12-24</created><updated>2013-08-19</updated><authors><author><keyname>Bruggeman</keyname><forenames>Jeroen</forenames></author></authors><title>The Strength of Varying Tie Strength</title><categories>physics.soc-ph cs.SI</categories><comments>This paper replaces an earlier one with the title &quot;Network Diversity
  and Economic Development&quot; [arXiv:1011.0208]</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  ``The Strength of Weak Ties&quot; argument (Granovetter 1973) says that the most
valuable information is best collected through bridging ties with other social
circles than one's own, and that those ties tend to be weak. Aral and Van
Alstyne (2011) added that to access complex information, actors need strong
ties (``high bandwidth&quot;) instead. These insights I integrate and generalize by
pointing at actors' benefits and costs. Weak ties are well-suited for
relatively simple information at low costs, whereas for complex information,
the best outcomes are expected for those actors who vary their bandwidths along
with the value of information accessed. To support my claim I use all patents
in the USA (two million) over the period 1975---1999.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5981</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5981</id><created>2012-12-24</created><updated>2013-03-10</updated><authors><author><keyname>Azimi-Tafreshi</keyname><forenames>N.</forenames></author><author><keyname>Dorogovtsev</keyname><forenames>S. N.</forenames></author><author><keyname>Mendes</keyname><forenames>J. F. F.</forenames></author></authors><title>Core organization of directed complex networks</title><categories>cond-mat.dis-nn cs.SI math-ph math.MP physics.soc-ph</categories><comments>10 pages, 7 figures</comments><journal-ref>Phys. Rev. E 87, 032815 (2013)</journal-ref><doi>10.1103/PhysRevE.87.032815</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recursive removal of leaves (dead end vertices) and their neighbors from
an undirected network results, when this pruning algorithm stops, in a
so-called core of the network. This specific subgraph should be distinguished
from $k$-cores, which are principally different subgraphs in networks. If the
vertex mean degree of a network is sufficiently large, the core is a giant
cluster containing a finite fraction of vertices. We find that generalization
of this pruning algorithm to directed networks provides a significantly more
complex picture of cores. By implementing a rate equation approach to this
pruning procedure for directed uncorrelated networks, we identify a set of
cores progressively embedded into each other in a network and describe their
birth points and structure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.5983</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.5983</id><created>2012-12-24</created><authors><author><keyname>Song</keyname><forenames>Yun</forenames></author><author><keyname>Li</keyname><forenames>Zhihui</forenames></author></authors><title>An ideal multi-secret sharing scheme based on minimal privileged
  coalitions</title><categories>cs.CR</categories><comments>13pages</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  How to construct an ideal multi-secret sharing scheme for general access
structures is difficult. In this paper, we solve an open problem proposed by
Spiez et al.recently [Finite Fields and Their Application, 2011(17) 329-342],
namely to design an algorithm of privileged coalitions of any length if such
coalitions exist. Furthermore, in terms of privileged coalitions, we show that
most of the existing multi-secret sharing schemes based on Shamir threshold
secret sharing are not perfect by analyzing Yang et al.'s scheme and Pang et
al.'s scheme. Finally, based on the algorithm mentioned above, we devise an
ideal multi-secret sharing scheme for families of access structures, which
possesses more vivid authorized sets than that of the threshold scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6009</identifier>
 <datestamp>2013-02-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6009</id><created>2012-12-25</created><updated>2013-02-21</updated><authors><author><keyname>Patterson</keyname><forenames>Stacy</forenames></author><author><keyname>Eldar</keyname><forenames>Yonina C.</forenames></author><author><keyname>Keidar</keyname><forenames>Idit</forenames></author></authors><title>Distributed Sparse Signal Recovery For Sensor Networks</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a distributed algorithm for sparse signal recovery in sensor
networks based on Iterative Hard Thresholding (IHT). Every agent has a set of
measurements of a signal x, and the objective is for the agents to recover x
from their collective measurements at a minimal communication cost and with low
computational complexity. A naive distributed implementation of IHT would
require global communication of every agent's full state in each iteration. We
find that we can dramatically reduce this communication cost by leveraging
solutions to the distributed top-K problem in the database literature.
Evaluations show that our algorithm requires up to three orders of magnitude
less total bandwidth than the best-known distributed basis pursuit method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6018</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6018</id><created>2012-12-25</created><authors><author><keyname>Ross</keyname><forenames>Gordon J.</forenames></author><author><keyname>Adams</keyname><forenames>Niall M.</forenames></author><author><keyname>Tasoulis</keyname><forenames>Dimitris K.</forenames></author><author><keyname>Hand</keyname><forenames>David J.</forenames></author></authors><title>Exponentially Weighted Moving Average Charts for Detecting Concept Drift</title><categories>stat.ML cs.LG stat.AP</categories><journal-ref>Pattern Recognition Letters, 33(2) 191-198, 2012</journal-ref><doi>10.1016/j.patrec.2011.08.019</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Classifying streaming data requires the development of methods which are
computationally efficient and able to cope with changes in the underlying
distribution of the stream, a phenomenon known in the literature as concept
drift. We propose a new method for detecting concept drift which uses an
Exponentially Weighted Moving Average (EWMA) chart to monitor the
misclassification rate of an streaming classifier. Our approach is modular and
can hence be run in parallel with any underlying classifier to provide an
additional layer of concept drift detection. Moreover our method is
computationally efficient with overhead O(1) and works in a fully online manner
with no need to store data points in memory. Unlike many existing approaches to
concept drift detection, our method allows the rate of false positive
detections to be controlled and kept constant over time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6027</identifier>
 <datestamp>2014-09-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6027</id><created>2012-12-25</created><updated>2014-09-05</updated><authors><author><keyname>Khandwawala</keyname><forenames>Mustafa</forenames></author><author><keyname>Sundaresan</keyname><forenames>Rajesh</forenames></author></authors><title>Belief propagation for optimal edge cover in the random complete graph</title><categories>math.PR cs.DM cs.IT math.IT</categories><comments>Published in at http://dx.doi.org/10.1214/13-AAP981 the Annals of
  Applied Probability (http://www.imstat.org/aap/) by the Institute of
  Mathematical Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AAP-AAP981</report-no><journal-ref>Annals of Applied Probability 2014, Vol. 24, No. 6, 2414-2454</journal-ref><doi>10.1214/13-AAP981</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We apply the objective method of Aldous to the problem of finding the
minimum-cost edge cover of the complete graph with random independent and
identically distributed edge costs. The limit, as the number of vertices goes
to infinity, of the expected minimum cost for this problem is known via a
combinatorial approach of Hessler and W\&quot;{a}stlund. We provide a proof of this
result using the machinery of the objective method and local weak convergence,
which was used to prove the $\zeta(2)$ limit of the random assignment problem.
A proof via the objective method is useful because it provides us with more
information on the nature of the edge's incident on a typical root in the
minimum-cost edge cover. We further show that a belief propagation algorithm
converges asymptotically to the optimal solution. This can be applied in a
computational linguistics problem of semantic projection. The belief
propagation algorithm yields a near optimal solution with lesser complexity
than the known best algorithms designed for optimality in worst-case settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6030</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6030</id><created>2012-12-25</created><authors><author><keyname>Krivulin</keyname><forenames>Nikolai K.</forenames></author></authors><title>Bounds on the state vector growth rate in stochastic dynamical systems</title><categories>math.OC cs.SY</categories><comments>The 5th St. Petersburg Workshop on Simulation, St. Petersburg,
  Russia, June 26-July 2, 2005</comments><msc-class>93E25 (Primary) 15A80, 93C65, 90B15, 37H15 (Secondary)</msc-class><journal-ref>Proc. 5th St. Petersburg Workshop on Simulation, 2005, pp. 391-396</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A stochastic dynamical system represented through a linear vector equation in
idempotent algebra is considered. We propose simple bounds on the mean growth
rate of the system state vector, and give an analysis of absolute error of a
bound. As an illustration, numerical results of evaluation of the bounds for a
test system are also presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6031</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6031</id><created>2012-12-25</created><authors><author><keyname>Bernstein</keyname><forenames>Alexander V.</forenames></author><author><keyname>Kuleshov</keyname><forenames>Alexander P.</forenames></author></authors><title>Tangent Bundle Manifold Learning via Grassmann&amp;Stiefel Eigenmaps</title><categories>cs.LG</categories><comments>25 pages, 6 figures</comments><msc-class>68T05</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the ultimate goals of Manifold Learning (ML) is to reconstruct an
unknown nonlinear low-dimensional manifold embedded in a high-dimensional
observation space by a given set of data points from the manifold. We derive a
local lower bound for the maximum reconstruction error in a small neighborhood
of an arbitrary point. The lower bound is defined in terms of the distance
between tangent spaces to the original manifold and the estimated manifold at
the considered point and reconstructed point, respectively. We propose an
amplification of the ML, called Tangent Bundle ML, in which the proximity not
only between the original manifold and its estimator but also between their
tangent spaces is required. We present a new algorithm that solves this problem
and gives a new solution for the ML also.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6036</identifier>
 <datestamp>2013-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6036</id><created>2012-12-25</created><authors><author><keyname>Mei</keyname><forenames>Gang</forenames></author><author><keyname>Tipper</keyname><forenames>John C.</forenames></author><author><keyname>Xu</keyname><forenames>Nengxiong</forenames></author></authors><title>T-Base: A Triangle-Based Iterative Algorithm for Smoothing Quadrilateral
  Meshes</title><categories>cs.CG</categories><comments>Proceedings of the 2012 International Conference on Information
  Technology and Software Engineering Lecture Notes in Electrical Engineering
  Volume 212, 2013, pp 305-315</comments><journal-ref>Lecture Notes in Electrical EngineeringVolume 212, 2013, pp
  305-315</journal-ref><doi>10.1007/978-3-642-34531-9_32</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a novel approach named TBase for smoothing planar and surface
quadrilateral meshes. Our motivation is that the best shape of quadrilateral
element (square) can be virtually divided into a pair of equilateral right
triangles by any of its diagonals. When move a node to smooth a quadrilateral,
it is optimal to make a pair of triangles divided by a diagonal be equilateral
right triangles separately. The finally smoothed position is obtained by
weighting all individual optimal positions. Three variants are produced
according to the determination of weights. Tests by the TBase are given and
compared with Laplacian smoothing: The Vari.1 of TBase is effectively identical
to Laplacian smoothing for planar quad meshes, while Vari.2 is the best. For
the quad mesh on underlying parametric surface and interpolation surface,
Vari.2 and Vari.1 are best, respectively.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6038</identifier>
 <datestamp>2013-06-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6038</id><created>2012-12-25</created><authors><author><keyname>Mei</keyname><forenames>Gang</forenames></author><author><keyname>Tipper</keyname><forenames>John C.</forenames></author><author><keyname>Xu</keyname><forenames>Nengxiong</forenames></author></authors><title>Ear-clipping Based Algorithms of Generating High-quality Polygon
  Triangulation</title><categories>cs.CG</categories><comments>Proceedings of the 2012 International Conference on Information
  Technology and Software Engineering Lecture Notes in Electrical Engineering
  Volume 212, 2013, pp 979-988</comments><journal-ref>Lecture Notes in Electrical EngineeringVolume 212, 2013, pp
  979-988</journal-ref><doi>10.1007/978-3-642-34531-9_105</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A basic and an improved ear clipping based algorithm for triangulating simple
polygons and polygons with holes are presented. In the basic version, the ear
with smallest interior angle is always selected to be cut in order to create
fewer sliver triangles. To reduce sliver triangles in further, a bound of angle
is set to determine whether a newly formed triangle has sharp angles, and edge
swapping is accepted when the triangle is sharp. To apply the two algorithms on
polygons with holes, &quot;Bridge&quot; edges are created to transform a polygon with
holes to a degenerate polygon which can be triangulated by the two algorithms.
Applications show that the basic algorithm can avoid creating sliver triangles
and obtain better triangulations than the traditional ear clipping algorithm,
and the improved algorithm can in further reduce sliver triangles effectively.
Both of the algorithms run in O(n2) time and O(n) space.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6039</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6039</id><created>2012-12-25</created><authors><author><keyname>Chen</keyname><forenames>Danny Z.</forenames></author><author><keyname>Wang</keyname><forenames>Haitao</forenames></author></authors><title>Weak Visibility Queries of Line Segments in Simple Polygons</title><categories>cs.CG cs.DS</categories><comments>16 pages, 9 figures. A preliminary version of this paper appeared in
  ISAAC 2012 and we have improved results in this full version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a simple polygon P in the plane, we present new algorithms and data
structures for computing the weak visibility polygon from any query line
segment in P. We build a data structure in O(n) time and O(n) space that can
compute the visibility polygon for any query line segment s in O(k log n) time,
where k is the size of the visibility polygon of s and n is the number of
vertices of P. Alternatively, we build a data structure in O(n^3) time and
O(n^3) space that can compute the visibility polygon for any query line segment
in O(k + log n) time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6040</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6040</id><created>2012-12-25</created><authors><author><keyname>Kadry</keyname><forenames>Seifedine</forenames></author><author><keyname>ElShalkamy</keyname><forenames>Maha</forenames></author></authors><title>Toward New Vision in Teaching Calculus</title><categories>cs.CY</categories><comments>IERI Procedia, Elsevier. 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Usually the first course in mathematics is calculus. Its a core course in the
curriculum of the Business, Engineering and the Sciences. However many students
face difficulties to learn calculus. These difficulties are often caused by the
prior fear of mathematics. The students today cant live without using computer
technology. The uses of computer for teaching and learning can transform the
boring traditional methodology of teach to more active and attractive method.
In this paper, we will show how we can use Excel in teaching calculus to
improve our students learning and understanding through different types of
applications ranging from Business to Engineering. The effectiveness of the
proposed methodology was tested on a random sample of 45 students from
different majors over a period of two semesters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6041</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6041</id><created>2012-12-25</created><authors><author><keyname>Kadry</keyname><forenames>Seifedine</forenames></author><author><keyname>Claver</keyname><forenames>Jimbo</forenames></author></authors><title>XML parser GUI using .NET Technology</title><categories>cs.SE</categories><comments>IERI Procedia, Elsevier. 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The purpose of this paper is to implement software that can save time,
effort, and facilitate XML and XSL programming. The XML parser helps the
programmer to determine whether the XML document is Well-formed or not, by
specifying if any the positions of the errors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6043</identifier>
 <datestamp>2013-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6043</id><created>2012-12-25</created><authors><author><keyname>Mei</keyname><forenames>Gang</forenames></author><author><keyname>Tipper</keyname><forenames>John C.</forenames></author><author><keyname>Xu</keyname><forenames>Nengxiong</forenames></author></authors><title>An Algorithm for Finding Convex Hulls of Planar Point Sets</title><categories>cs.CG</categories><comments>Proceedings of IEEE Conference, ICCSNT 2012, in Press</comments><journal-ref>2012 2nd International Conference on , vol., no., pp.888,891,
  29-31 Dec. 2012</journal-ref><doi>10.1109/ICCSNT.2012.6526070</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents an alternate choice of computing the convex hulls (CHs)
for planar point sets. We firstly discard the interior points and then sort the
remaining vertices by x- / y- coordinates separately, and later create a group
ofquadrilaterals (e-Quads) recursively according to the sequences ofthe sorted
lists of points. Finally, the desired CH is built based on a simple polygon
derived from all e-Quads. Besides the preprocessing for original planar point
sets, this algorithm has another mechanism of discarding interior point when
form e-Quads and assemble the simple polygon. Compared with three popular CH
algorithms, the proposed algorithm can generate CHs faster thanthe three but
has a penalty in space cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6045</identifier>
 <datestamp>2013-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6045</id><created>2012-12-25</created><authors><author><keyname>Mei</keyname><forenames>Gang</forenames></author><author><keyname>Tipper</keyname><forenames>John C.</forenames></author><author><keyname>Xu</keyname><forenames>Nengxiong</forenames></author></authors><title>A Hybrid Approach for Optimizing Planar Triangular Meshes</title><categories>cs.CG</categories><comments>Proceedings of IEEE Conference, ICCSNT 2012, in Press</comments><journal-ref>2012 2nd International Conference on , vol., no., pp.968,972,
  29-31 Dec. 2012</journal-ref><doi>10.1109/ICCSNT.2012.6526088</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Modified Direct Method (MDM) is an iterative scheme based on Jacobi
iterations for smoothing planar meshes [4]. The basic idea behind MDM is to
make any triangular element be as close to an equilateral triangle as possible.
Basedon the MDM, a length-weighted MDM is proposed and then combined with edge
swapping. In length-weighted MDM, weights of each neighboring node of a
smoothed node are determined by the length of its opposite edge. Also, the MDM,
Laplacian smoothing and length-weighted MDM are all combined with edge
swapping, and then implemented and compared on both structured and unstructured
triangular meshes. Examples show that length-weighted MDM is better than the
MDM and Laplacian smoothing for structured mesh but worse for unstructured
mesh. The hybrid approach of combining length-weighted MDM and edge swapping is
much better and can obtain more even optimized meshes than other two hybrid
approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6046</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6046</id><created>2012-12-25</created><authors><author><keyname>Kadry</keyname><forenames>Seifedine</forenames></author><author><keyname>Joumaa</keyname><forenames>Chibli</forenames></author></authors><title>Green WSUS</title><categories>cs.OH</categories><comments>International Journal of Applied Science and Technology, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The new era of information and communication technology (ICT) calls for a
greater understanding of the environmental impacts of recent technology. With
increasing energy cost and growing environmental concerns, green IT is
receiving more and more attention. Network and system design play a crucial
role in both computing and telecommunication systems. Significant part of this
energy cost goes to system update by downloading regularly patches and bug
fixes to solve security problems and to assure that the operating system and
other systems function properly. This paper describes a new design of Windows
Server Update Services (WSUS), system responsible of downloads of the mentioned
patches and updates from Microsoft Update website and then distributes them to
computers on a network. The general idea behind our proposed design is simple.
Instead of the periodical check done by the WSUS servers to ensure update form
Microsoft main servers, we rather propose to reverse the scenario in order to
reduce energy consumption. In the proposed design, the Microsoft main server(s)
sends signal to all WSUS servers to inform them about new updates. Once the
signal received, WSUS can contact the main server to start downloading.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6048</identifier>
 <datestamp>2013-09-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6048</id><created>2012-12-25</created><authors><author><keyname>Mei</keyname><forenames>Gang</forenames></author><author><keyname>Tipper</keyname><forenames>John C.</forenames></author><author><keyname>Xu</keyname><forenames>Nengxiong</forenames></author></authors><title>Discrete Surface Modeling Based on Google Earth: A Case Study</title><categories>cs.GR physics.geo-ph</categories><comments>Proceedings of IEEE Conference, ICCSNT 2012, in Press</comments><journal-ref>2012 2nd International Conference on , vol., no., pp.1137,1141,
  29-31 Dec. 2012</journal-ref><doi>10.1109/ICCSNT.2012.6526125</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Google Earth (GE) has become a powerful tool for geological, geophysical and
geographical modeling; yet GE can be accepted to acquire elevation data of
terrain. In this paper, we present a real study case of building the discrete
surface model (DSM) at Haut-Barr Castle in France based on the elevation data
of terrain points extracted from GE using the COM API. We first locate the
position of Haut-Barr Castle and determine the region of the study area, then
extract elevation data of terrain at Haut-Barr, and thirdly create a planar
triangular mesh that covers the study area and finally generate the desired DSM
by calculating the elevation of vertices in the planar mesh via interpolating
with Universal Kriging (UK) and Inverse Distance Weighting (IDW). The generated
DSM can reflect the features of the ground surface at Haut-Barr well, and can
be used for constructingthe Sealed Engineering Geological Model (SEGM) in
further step.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6050</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6050</id><created>2012-12-25</created><authors><author><keyname>Al-Taie</keyname><forenames>Mohammed</forenames></author><author><keyname>Kadry</keyname><forenames>Seifedine</forenames></author></authors><title>Applying Social Network Analysis to Analyze a Web-Based Community</title><categories>cs.SI cs.CY</categories><comments>International Journal of Advanced Computer Science and
  Applications(IJACSA). 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with a very renowned website (that is Book-Crossing) from
two angles: The first angle focuses on the direct relations between users and
books. Many things can be inferred from this part of analysis such as who is
more interested in book reading than others and why? Which books are most
popular and which users are most active and why? The task requires the use of
certain social network analysis measures (e.g. degree centrality). What does it
mean when two users like the same book? Is it the same when other two users
have one thousand books in common? Who is more likely to be a friend of whom
and why? Are there specific people in the community who are more qualified to
establish large circles of social relations? These questions (and of course
others) were answered through the other part of the analysis, which will take
us to probe the potential social relations between users in this community.
Although these relationships do not exist explicitly, they can be inferred with
the help of affiliation network analysis and techniques such as m-slice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6051</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6051</id><created>2012-12-25</created><authors><author><keyname>Bakari</keyname><forenames>Wided</forenames></author><author><keyname>Ali</keyname><forenames>Mouez</forenames></author><author><keyname>Ben-Abdallah</keyname><forenames>Hanene</forenames></author></authors><title>Automatic approach for generating ETL operators</title><categories>cs.DB</categories><comments>in French</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article addresses the generation of the ETL
operators(Extract-Transform-Load) for supplying a Data Warehouse from a
relational data source. As a first step, we add new rules to those proposed by
the authors of [1], these rules deal with the combination of ETL operators. In
a second step, we propose an automatic approach based on model transformations
to generate the ETL operations needed for loading a data warehouse. This
approach offers the possibility to set some designer requirements for loading.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6053</identifier>
 <datestamp>2013-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6053</id><created>2012-12-25</created><updated>2013-11-12</updated><authors><author><keyname>Krivulin</keyname><forenames>Nikolai K.</forenames></author><author><keyname>Guster</keyname><forenames>Dennis</forenames></author><author><keyname>Hall</keyname><forenames>Charles</forenames></author></authors><title>On parallel implementation of a discrete optimization random search
  algorithm</title><categories>cs.DC math.OC</categories><comments>15 pages, 2 figures 1, table. ISSN 1109-2750</comments><msc-class>90C26 (Primary) 68W10, 65C05, 68T20 (Secondary)</msc-class><journal-ref>WSEAS Transactions on Computers, 2005, Vol. 4, No 9, pp. 1122-1129</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A random search algorithm intended to solve discrete optimization problems is
considered. We outline the main components of the algorithm, and then describe
it in more detail. We show how the algorithm can be implemented on parallel
computer systems. A performance analysis of both serial and parallel versions
of the algorithm is given, and related results of solving test problems are
discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6054</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6054</id><created>2012-12-25</created><authors><author><keyname>Alkafagee</keyname><forenames>Mohammad</forenames></author><author><keyname>Kadry</keyname><forenames>Seifedine</forenames></author></authors><title>New design of Robotics Remote lab</title><categories>cs.RO</categories><comments>(IJACSA) International Journal of Advanced Computer Science and
  Applications, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Robotic Remote Laboratory controls the Robot labs via the Internet and
applies the Robot experiment in easy and advanced way. If we want to enhance
the RRL system, we must study requirements of the Robot experiment in a deeply
way. One of key requirements of the Robot experiment is the Control algorithm
that includes all important activities to affect the Robot; one of them relates
the path or obstacle. Our goal is to produce a new design of the RRL includes a
new treatment to the Control algorithm depends on isolation one of the Control
algorithm's activities that relates the paths in a separated algorithm, i.e.,
design the (Path planning algorithm) is independent of the original Control
algorithm. This aim can be achieved by depending on the light to produce the
Light obstacle. To apply the Light obstacle, we need to hardware (Light control
server and Light arms) and soft ware (path planning algorithm).The NXT 2.0
Robot will sense the Light obstacle depending on the Light sensor of it. The
new design has two servers, one for the path (Light control server) and other
for the other activities of the Control algorithm (Robot control server).The
website of the new design includes three main parts (Lab Reservation, Open Lab,
Download Simulation).We proposed a set of scenarios for organizing the
reservation of the Remote Lab. Additionally, we developed an appropriate
software to simulate the Robot and to practice it before usage the Remote lab.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6055</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6055</id><created>2012-12-25</created><authors><author><keyname>Kadry</keyname><forenames>Seifedine</forenames></author><author><keyname>Abdallah</keyname><forenames>Ayman</forenames></author><author><keyname>Joumaa</keyname><forenames>Chibli</forenames></author></authors><title>On The Optimization of Dijkstras Algorithm</title><categories>cs.DS</categories><comments>Informatics in Control, Automation and Robotics, Volume 2,
  Springer-Verlag Berlin Heidelberg 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose some amendment on Dijkstras algorithm in order to
optimize it by reducing the number of iterations. The main idea is to solve the
problem where more than one node satisfies the condition of the second step in
the traditional Dijkstras algorithm. After application of the proposed
modifications, the maximum number of iterations of Dijkstras algorithm is less
than the number of the graphs nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6056</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6056</id><created>2012-12-25</created><authors><author><keyname>Abdallah</keyname><forenames>Ayman</forenames></author><author><keyname>Kadry</keyname><forenames>Seifedine</forenames></author><author><keyname>Joumaa</keyname><forenames>Chibli</forenames></author></authors><title>Design and Performance Study of Smart Antenna Systems for WIMAX
  Applications</title><categories>cs.OH</categories><comments>Informatics in Control, Automation and Robotics, Volume 2, LNEE 133,
  Springer-Verlag Berlin Heidelberg 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose an approach that uses homodyne receivers to design
smart antenna systems. The receivers functions are to detect angles of arrivals
of seven incoming RF signals using MUSIC or ESPRIT algorithms. The
characteristics of each algorithm are critical for the systems precision as
well as receivers types. Results are deduced from the simulation of each
system, using the Advanced Design System (ADS) and MATLAB. These are compared
to results deduced from real systems in the WIMAX (3.5GHz) domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6058</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6058</id><created>2012-12-25</created><authors><author><keyname>Gao</keyname><forenames>Xinwei</forenames></author><author><keyname>Zhang</keyname><forenames>Jian</forenames></author><author><keyname>Jiang</keyname><forenames>Feng</forenames></author><author><keyname>Fan</keyname><forenames>Xiaopeng</forenames></author><author><keyname>Ma</keyname><forenames>Siwei</forenames></author><author><keyname>Zhao</keyname><forenames>Debin</forenames></author></authors><title>High Quality Image Interpolation via Local Autoregressive and Nonlocal
  3-D Sparse Regularization</title><categories>cs.MM cs.CV</categories><comments>4 pages, 5 figures, 2 tables, to be published at IEEE Visual
  Communications and Image Processing (VCIP) 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a novel image interpolation algorithm, which is
formulated via combining both the local autoregressive (AR) model and the
nonlocal adaptive 3-D sparse model as regularized constraints under the
regularization framework. Estimating the high-resolution image by the local AR
regularization is different from these conventional AR models, which weighted
calculates the interpolation coefficients without considering the rough
structural similarity between the low-resolution (LR) and high-resolution (HR)
images. Then the nonlocal adaptive 3-D sparse model is formulated to regularize
the interpolated HR image, which provides a way to modify these pixels with the
problem of numerical stability caused by AR model. In addition, a new
Split-Bregman based iterative algorithm is developed to solve the above
optimization problem iteratively. Experiment results demonstrate that the
proposed algorithm achieves significant performance improvements over the
traditional algorithms in terms of both objective quality and visual perception
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6059</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6059</id><created>2012-12-25</created><authors><author><keyname>Naik</keyname><forenames>Priyanka</forenames></author><author><keyname>Sanyal</keyname><forenames>Sugata</forenames></author></authors><title>Prover and Verifier Based Password Protection: PVBPP</title><categories>cs.CR</categories><comments>8 pages, 1 figure, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In today's world password are mostly used for authentication. This makes them
prone to various kinds of attacks like dictionary attacks. A dictionary attack
is a method of breaking the password by systematically entering every word in a
dictionary as a password. This attack leads to an overload on the server
leading to denial of service attack. This paper presents a protocol to reduce
the rate of dictionary attack by using a prover and a verifier system. This
system makes it difficult for the attacker to prove it as a valid user by
becoming computationally intensive. The rate of attempts is also reduced and
thus restricting the Denial of Service attack.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6069</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6069</id><created>2012-12-25</created><authors><author><keyname>Krivulin</keyname><forenames>N. K.</forenames></author></authors><title>Evaluation of Lyapunov exponent in generalized linear dynamical models
  of queueing networks</title><categories>math.OC cs.SY</categories><comments>The 6th Vienna Conference on Mathematical Modelling, February 11-13,
  2009, Vienna University of Technology, Vienna; Proc. MATHMOD 09 Vienna Full
  Papers CD Volume, ARGESIM/ASIM, Vienna, 2009. ISBN 978-3-901608-35-3</comments><msc-class>90B22 (Primary) 15A80, 93C65, 37H15, 90B15 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of evaluation of Lyapunov exponent in queueing network analysis
is considered based on models and methods of idempotent algebra. General
existence conditions for Lyapunov exponent to exist in generalized linear
stochastic dynamic systems are given, and examples of evaluation of the
exponent for systems with matrices of particular types are presented. A method
which allow one to get the exponent is proposed based on some appropriate
decomposition of the system matrix. A general approach to modeling of a wide
class of queueing networks is taken to provide for models in the form of
stochastic dynamic systems. It is shown how to find the mean service cycle time
for the networks through the evaluation of Lyapunov exponent for their
associated dynamic systems. As an illustration, the mean service time is
evaluated for some systems including open and closed tandem queues with finite
and infinite buffers, fork-join networks, and systems with round-robin routing.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6074</identifier>
 <datestamp>2015-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6074</id><created>2012-12-25</created><updated>2015-05-20</updated><authors><author><keyname>Yona</keyname><forenames>Yair</forenames></author><author><keyname>Feder</keyname><forenames>Meir</forenames></author></authors><title>On the Diversity-Multiplexing Tradeoff of Unconstrained Multiple-Access
  Channels</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work the optimal diversity-multiplexing tradeoff (DMT) is
investigated for the multiple-input multiple-output fading multiple-access
channels with no power constraints (infinite constellations). For K users
(K&gt;1), M transmit antennas for each user, and N receive antennas, infinite
constellations in general and lattices in particular are shown to attain the
optimal DMT of finite constellations for the case N equals or greater than
(K+1)M-1, i.e., user limited regime. On the other hand for N&lt;(K+1)M-1 it is
shown that infinite constellations can not attain the optimal DMT. This is in
contrast to the point-to-point case in which infinite constellations are DMT
optimal for any M and N. In general, this work shows that when the network is
heavily loaded, i.e. K&gt;max(1,(N-M+1)/M), taking into account the shaping region
in the decoding process plays a crucial role in pursuing the optimal DMT. By
investigating the cases where infinite constellations are optimal and
suboptimal, this work also gives a geometrical interpretation to the DMT of
infinite constellations in multiple-access channels.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6079</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6079</id><created>2012-12-25</created><authors><author><keyname>Krivulin</keyname><forenames>Nikolai</forenames></author></authors><title>Evaluation of the Lyapunov exponent for generalized linear second-order
  exponential systems</title><categories>math.OC cs.SY math.PR</categories><comments>The 6th St. Petersburg Workshop on Simulation, St. Petersburg,
  Russia, June 28-July 4, 2009</comments><msc-class>68M20 (Primary) 90B15, 37H15, 93C65, 15A80 (Secondary)</msc-class><journal-ref>Proc. 6th St. Petersburg Workshop on Simulation, Volume II, 2009,
  pp. 875-881</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider generalized linear stochastic dynamical systems with second-order
state transition matrices. The entries of the matrix are assumed to be either
independent and exponentially distributed or equal to zero. We give an overview
of new results on evaluation of asymptotic growth rate of the system state
vector, which is called the Lyapunov exponent of the system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6080</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6080</id><created>2012-12-25</created><authors><author><keyname>Adel</keyname><forenames>Hidri</forenames></author><author><keyname>Souad</keyname><forenames>Meddeb</forenames></author><author><keyname>Alaqeeli</keyname><forenames>Abdulqadir</forenames></author><author><keyname>Hamid</keyname><forenames>Amiri</forenames></author></authors><title>Beamforming Techniques for Multichannel audio Signal Separation</title><categories>cs.OH</categories><comments>9 pages, 7 Figures</comments><journal-ref>JDCTA: International Journal of Digital Content Technology and its
  Applications, Vol. 6, No. 20, pp. 659-667, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Beamforming is a signal processing technique. It has been studied in many
areas such as radar, sonar, seismology and wireless communications, to name but
a few. It can be used for a myriad of purposes, such as detecting the presence
of a signal, estimating the direction of arrival, and enhancing a desired
signal from its measurements corrupted by noise, competing sources and
reverberation. Actually, Beamforming has been adopted by the audio research
society, mostly to separate or extract speech for noisy environment.
Beamforming techniques basically approach the problem from a spatial point of
view. A microphone array is used to form a spatial filter which can extract a
signal from a specific direction and reduce the contamination of signals from
other directions. In this paper we survey some Beamforming techniques used for
multichannel audio signal separation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6085</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6085</id><created>2012-12-25</created><authors><author><keyname>Krivulin</keyname><forenames>Nikolai</forenames></author></authors><title>Algebraic solutions to multidimensional minimax location problems with
  Chebyshev distance</title><categories>math.OC cs.DM</categories><comments>International Conference on Applied and Computational Mathematics
  (ICACM'11), Lanzarote, Canary Islands, Spain, May 27-29, 2011, WSEAS Press.
  ISBN 978-1-61804-002-2</comments><msc-class>15A80 (Primary) 90B85, 65K05, 90C47, 15A18 (Secondary)</msc-class><journal-ref>Recent Researches in Applied and Computational Mathematics:
  Intern. Conf. on Applied and Computational Mathematics (ICACM'11), 2011, pp.
  157-162</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multidimensional minimax single facility location problems with Chebyshev
distance are examined within the framework of idempotent algebra. A new
algebraic solution based on an extremal property of the eigenvalues of
irreducible matrices is given. The solution reduces both unconstrained and
constrained location problems to evaluation of the eigenvalue and eigenvectors
of an appropriate matrix.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6086</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6086</id><created>2012-12-25</created><authors><author><keyname>Nouh</keyname><forenames>Said</forenames></author><author><keyname>Aylaj</keyname><forenames>Bouchaib</forenames></author><author><keyname>Belkasmi</keyname><forenames>Mostafa</forenames></author></authors><title>A Method to determine Partial Weight Enumerator for Linear Block Codes</title><categories>cs.IT math.IT</categories><comments>Computer Engineering and Intelligent Systems Vol 3, No.11, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a fast and efficient method to find partial weight
enumerator (PWE) for binary linear block codes by using the error impulse
technique and Monte Carlo method. This PWE can be used to compute an upper
bound of the error probability for the soft decision maximum likelihood decoder
(MLD). As application of this method we give partial weight enumerators and
analytical performances of the BCH(130,66), BCH(103,47) and BCH(111,55)
shortened codes; the first code is obtained by shortening the binary primitive
BCH (255,191,17) code and the two other codes are obtained by shortening the
binary primitive BCH(127,71,19) code. The weight distributions of these three
codes are unknown at our knowledge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6089</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6089</id><created>2012-12-25</created><authors><author><keyname>Krivulin</keyname><forenames>Nikolai</forenames></author></authors><title>Algebraic solution to a constrained rectilinear minimax location problem
  on the plane</title><categories>math.OC cs.DM</categories><comments>2011 International Conference on Multimedia Technology (ICMT), 26-28
  July 2011, Hangzhou, China. ISBN 978-1-61284-771-9</comments><msc-class>15A80 (Primary) 90B85, 65K05, 90C47, 15A18 (Secondary)</msc-class><journal-ref>2011 International Conference on Multimedia Technology (ICMT),
  IEEE, 2011, pp. 6212-6220</journal-ref><doi>10.1109/ICMT.2011.6002526</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a constrained minimax single facility location problem on the
plane with rectilinear distance. The feasible set of location points is
restricted to rectangles with sides oriented at a 45 degrees angle to the axes
of Cartesian coordinates. To solve the problem, an algebraic approach based on
an extremal property of eigenvalues of irreducible matrices in idempotent
algebra is applied. A new algebraic solution is given that reduces the problem
to finding eigenvalues and eigenvectors of appropriately defined matrices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6094</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6094</id><created>2012-12-25</created><authors><author><keyname>Huang</keyname><forenames>Chang</forenames></author><author><keyname>Zhu</keyname><forenames>Shenghuo</forenames></author><author><keyname>Yu</keyname><forenames>Kai</forenames></author></authors><title>Large Scale Strongly Supervised Ensemble Metric Learning, with
  Applications to Face Verification and Retrieval</title><categories>cs.CV</categories><comments>8 pages</comments><report-no>NEC Labs, 2011-TR115</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Learning Mahanalobis distance metrics in a high- dimensional feature space is
very difficult especially when structural sparsity and low rank are enforced to
improve com- putational efficiency in testing phase. This paper addresses both
aspects by an ensemble metric learning approach that consists of sparse block
diagonal metric ensembling and join- t metric learning as two consecutive
steps. The former step pursues a highly sparse block diagonal metric by
selecting effective feature groups while the latter one further exploits
correlations between selected feature groups to obtain an accurate and low rank
metric. Our algorithm considers all pairwise or triplet constraints generated
from training samples with explicit class labels, and possesses good scala-
bility with respect to increasing feature dimensionality and growing data
volumes. Its applications to face verification and retrieval outperform
existing state-of-the-art methods in accuracy while retaining high efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6098</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6098</id><created>2012-12-25</created><authors><author><keyname>Krivulin</keyname><forenames>Nikolai</forenames></author></authors><title>Evaluation of the mean cycle time in stochastic discrete event dynamic
  systems</title><categories>math.OC cs.SY math.PR</categories><comments>The 6th International Conference on Queueing Theory and Network
  Applications (QTNA'11), Aug. 23-26, 2011, Seoul, Korea; ACM, New York, ISBN
  978-1-4503-0758-1</comments><msc-class>37H15 (Primary) 90B15, 93C65, 60K10, 15A80 (Secondary)</msc-class><acm-class>G.3</acm-class><journal-ref>Proc. 6th Intern. Conf. on Queueing Theory and Network
  Applications (QTNA'11), 2011, pp. 93-100</journal-ref><doi>10.1145/2021216.2021230</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider stochastic discrete event dynamic systems that have time
evolution represented with two-dimensional state vectors through a vector
equation that is linear in terms of an idempotent semiring. The state
transitions are governed by second-order random matrices that are assumed to be
independent and identically distributed. The problem of interest is to evaluate
the mean growth rate of state vector, which is also referred to as the mean
cycle time of the system, under various assumptions on the matrix entries. We
give an overview of early results including a solution for systems determined
by matrices with independent entries having a common exponential distribution.
It is shown how to extend the result to the cases when the entries have
different exponential distributions and when some of the entries are replaced
by zero. Finally, the mean cycle time is calculated for systems with matrices
that have one random entry, whereas the other entries in the matrices can be
arbitrary nonnegative and zero constants. The random entry is always assumed to
have exponential distribution except for one case of a matrix with zero row
when the particular form of the matrix makes it possible to obtain a solution
that does not rely on exponential distribution assumptions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6102</identifier>
 <datestamp>2014-09-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6102</id><created>2012-12-25</created><updated>2013-03-12</updated><authors><author><keyname>Chaffin</keyname><forenames>Benjamin</forenames></author><author><keyname>Linderman</keyname><forenames>John P.</forenames></author><author><keyname>Sloane</keyname><forenames>N. J. A.</forenames></author><author><keyname>Wilks</keyname><forenames>Allan R.</forenames></author></authors><title>On Curling Numbers of Integer Sequences</title><categories>math.CO cs.DM</categories><comments>25 pages, one figure, 14 tables. This paper is a sequel to the paper
  arXiv:0912.2382. Feb 17 2013: added list of OEIS sequences that are
  mentioned. March 12 2013: A number of small improvements</comments><msc-class>68R15 (Primary) 11B37 (Secondary)</msc-class><journal-ref>J. Integer Sequences 16 (2013), #13.4.3</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a finite nonempty sequence S of integers, write it as XY^k, where Y^k
is a power of greatest exponent that is a suffix of S: this k is the curling
number of S. The Curling Number Conjecture is that if one starts with any
initial sequence S, and extends it by repeatedly appending the curling number
of the current sequence, the sequence will eventually reach 1. The conjecture
remains open. In this paper we discuss the special case when S consists just of
2's and 3's. Even this case remains open, but we determine how far a sequence
of n 2's and 3's can extend before reaching a 1, conjecturally for n &lt;= 80. We
investigate several related combinatorial problems, such as finding c(n,k), the
number of binary sequences of length n and curling number k, and t(n,i), the
number of sequences of length n which extend for i steps before reaching a 1. A
number of interesting combinatorial problems remain unsolved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6103</identifier>
 <datestamp>2014-06-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6103</id><created>2012-12-25</created><updated>2014-06-17</updated><authors><author><keyname>Hibi</keyname><forenames>Takayuki</forenames></author><author><keyname>Nishiyama</keyname><forenames>Kenta</forenames></author><author><keyname>Takayama</keyname><forenames>Nobuki</forenames></author></authors><title>Pfaffian Systems of A-Hypergeometric Equations I: Bases of Twisted
  Cohomology Groups</title><categories>math.CA cs.SC math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is the third revision. We study bases of Pfaffian systems for
$A$-hypergeometric system. Gr\&quot;obner deformations give bases. These bases also
give those for twisted cohomology groups. For hypergeometric system associated
to a class of order polytopes, these bases have a combinatorial description.
The size of the bases associated to a subclass of the order polytopes have the
growth rate of the polynomial order. Bases associated to two chain posets and
bouquets are studied.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6104</identifier>
 <datestamp>2014-02-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6104</id><created>2012-12-25</created><updated>2014-02-12</updated><authors><author><keyname>Teutsch</keyname><forenames>Jason</forenames></author></authors><title>Short lists for shortest descriptions in short time</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Is it possible to find a shortest description for a binary string? The
well-known answer is &quot;no, Kolmogorov complexity is not computable.&quot; Faced with
this barrier, one might instead seek a short list of candidates which includes
a laconic description. Remarkably such approximations exist. This paper
presents an efficient algorithm which generates a polynomial-size list
containing an optimal description for a given input string. Along the way, we
employ expander graphs and randomness dispersers to obtain an Explicit Online
Matching Theorem for bipartite graphs and a refinement of Muchnik's Conditional
Complexity Theorem. Our main result extends recent work by Bauwens, Mahklin,
Vereschchagin, and Zimand.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6106</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6106</id><created>2012-12-25</created><authors><author><keyname>Krivulin</keyname><forenames>Nikolai</forenames></author></authors><title>A tropical extremal problem with nonlinear objective function and linear
  inequality constraints</title><categories>math.OC cs.DM</categories><comments>The 6th WSEAS European Computing Conference (ECC'12), Prague, Czech
  Republic, September 24-26, 2012; Advances in Computer Science: Proc. 6th
  WSEAS European Computing Conf. (ECC '12), WSEAS Press. ISBN
  978-1-61804-126-5; RACES 5, ISSN 1790-5109</comments><msc-class>15A80 (Primary) 90B85, 90C47, 65K05, 15A18 (Secondary)</msc-class><journal-ref>Recent Advances in Computer Engineering Series, Vol. 5, 2012, pp.
  216-221</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a multidimensional extremal problem formulated in terms of
tropical mathematics. The problem is to minimize a nonlinear objective
function, which is defined on a finite-dimensional semimodule over an
idempotent semifield, subject to linear inequality constraints. An efficient
solution approach is developed which reduces the problem to that of solving a
linear inequality with an extended set of unknown variables. We use the
approach to obtain a complete solution to the problem in a closed form under
quite general assumptions. To illustrate the obtained results, a
two-dimensional problem is examined and its numerical solution is given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6107</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6107</id><created>2012-12-25</created><authors><author><keyname>Krivulin</keyname><forenames>Nikolai</forenames></author></authors><title>A solution of a tropical linear vector equation</title><categories>math.OC cs.DM</categories><comments>The 6th WSEAS European Computing Conference (ECC'12), Prague, Czech
  Republic, September 24-26, 2012; Advances in Computer Science: Proc. 6th
  WSEAS European Computing Conf. (ECC'12), WSEAS Press. ISBN 978-1-61804-126-5.
  RACES 5, ISSN 1790-5109</comments><msc-class>15A80 (Primary) 15A06, 90C47 (Secondary)</msc-class><journal-ref>Recent Advances in Computer Engineering Series, Vol. 5, 2012, pp.
  244-249</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A linear vector equation is considered defined in terms of idempotent
mathematics. To solve the equation, we apply an approach that is based on the
analysis of distances between vectors in idempotent vector spaces and reduces
the solution of the equation to that of a tropical optimization problem. Based
on the approach, existence and uniqueness conditions are established for the
solution, and a general solution to the equation is given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6110</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6110</id><created>2012-12-25</created><authors><author><keyname>Konoshima</keyname><forenames>Makiko</forenames></author><author><keyname>Noma</keyname><forenames>Yui</forenames></author></authors><title>Hyperplane Arrangements and Locality-Sensitive Hashing with Lift</title><categories>cs.LG cs.IR stat.ML</categories><comments>9 pages, 7 figures</comments><acm-class>H.3.3; H.3.m</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Locality-sensitive hashing converts high-dimensional feature vectors, such as
image and speech, into bit arrays and allows high-speed similarity calculation
with the Hamming distance. There is a hashing scheme that maps feature vectors
to bit arrays depending on the signs of the inner products between feature
vectors and the normal vectors of hyperplanes placed in the feature space. This
hashing can be seen as a discretization of the feature space by hyperplanes. If
labels for data are given, one can determine the hyperplanes by using learning
algorithms. However, many proposed learning methods do not consider the
hyperplanes' offsets. Not doing so decreases the number of partitioned regions,
and the correlation between Hamming distances and Euclidean distances becomes
small. In this paper, we propose a lift map that converts learning algorithms
without the offsets to the ones that take into account the offsets. With this
method, the learning methods without the offsets give the discretizations of
spaces as if it takes into account the offsets. For the proposed method, we
input several high-dimensional feature data sets and studied the relationship
between the statistical characteristics of data, the number of hyperplanes, and
the effect of the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6147</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6147</id><created>2012-12-26</created><authors><author><keyname>Jain</keyname><forenames>Paridhi</forenames></author><author><keyname>Kumaraguru</keyname><forenames>Ponnurangam</forenames></author></authors><title>Finding Nemo: Searching and Resolving Identities of Users Across Online
  Social Networks</title><categories>cs.SI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An online user joins multiple social networks in order to enjoy different
services. On each joined social network, she creates an identity and
constitutes its three major dimensions namely profile, content and connection
network. She largely governs her identity formulation on any social network and
therefore can manipulate multiple aspects of it. With no global identifier to
mark her presence uniquely in the online domain, her online identities remain
unlinked, isolated and difficult to search. Earlier research has explored the
above mentioned dimensions, to search and link her multiple identities with an
assumption that the considered dimensions have been least disturbed across her
identities. However, majority of the approaches are restricted to exploitation
of one or two dimensions. We make a first attempt to deploy an integrated
system (Finding Nemo) which uses all the three dimensions of an identity to
search for a user on multiple social networks. The system exploits a known
identity on one social network to search for her identities on other social
networks. We test our system on two most popular and distinct social networks -
Twitter and Facebook. We show that the integrated system gives better accuracy
than the individual algorithms. We report experimental findings in the report.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6148</identifier>
 <datestamp>2013-10-02</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6148</id><created>2012-12-26</created><updated>2013-09-30</updated><authors><author><keyname>Fulek</keyname><forenames>Radoslav</forenames></author><author><keyname>T&#xf3;th</keyname><forenames>Csaba D.</forenames></author></authors><title>Universal point sets for planar three-tree</title><categories>cs.CG math.CO</categories><comments>revision</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For every $n\in \mathbb{N}$, we present a set $S_n$ of $O(n^{3/2}\log n)$
points in the plane such that every planar 3-tree with $n$ vertices has a
straight-line embedding in the plane in which the vertices are mapped to a
subset of $S_n$. This is the first subquadratic upper bound on the size of
universal point sets for planar 3-trees, as well as for the class of 2-trees
and serial parallel graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6167</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6167</id><created>2012-12-26</created><authors><author><keyname>Beninel</keyname><forenames>Farid</forenames></author><author><keyname>Bouaguel</keyname><forenames>Waad</forenames></author><author><keyname>Belmufti</keyname><forenames>Ghazi</forenames></author></authors><title>Transfer Learning Using Logistic Regression in Credit Scoring</title><categories>cs.LG cs.CE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The credit scoring risk management is a fast growing field due to consumer's
credit requests. Credit requests, of new and existing customers, are often
evaluated by classical discrimination rules based on customers information.
However, these kinds of strategies have serious limits and don't take into
account the characteristics difference between current customers and the future
ones. The aim of this paper is to measure credit worthiness for non customers
borrowers and to model potential risk given a heterogeneous population formed
by borrowers customers of the bank and others who are not. We hold on previous
works done in generalized gaussian discrimination and transpose them into the
logistic model to bring out efficient discrimination rules for non customers'
subpopulation.
  Therefore we obtain several simple models of connection between parameters of
both logistic models associated respectively to the two subpopulations. The
German credit data set is selected to experiment and to compare these models.
Experimental results show that the use of links between the two subpopulations
improve the classification accuracy for the new loan applicants.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6176</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6176</id><created>2012-12-26</created><authors><author><keyname>Chechik</keyname><forenames>Shiri</forenames></author><author><keyname>Johnson</keyname><forenames>M. P.</forenames></author><author><keyname>Parter</keyname><forenames>Merav</forenames></author><author><keyname>Peleg</keyname><forenames>David</forenames></author></authors><title>Secluded Connectivity Problems</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Consider a setting where possibly sensitive information sent over a path in a
network is visible to every {neighbor} of the path, i.e., every neighbor of
some node on the path, thus including the nodes on the path itself. The
exposure of a path $P$ can be measured as the number of nodes adjacent to it,
denoted by $N[P]$. A path is said to be secluded if its exposure is small. A
similar measure can be applied to other connected subgraphs, such as Steiner
trees connecting a given set of terminals. Such subgraphs may be relevant due
to considerations of privacy, security or revenue maximization. This paper
considers problems related to minimum exposure connectivity structures such as
paths and Steiner trees. It is shown that on unweighted undirected $n$-node
graphs, the problem of finding the minimum exposure path connecting a given
pair of vertices is strongly inapproximable, i.e., hard to approximate within a
factor of $O(2^{\log^{1-\epsilon}n})$ for any $\epsilon&gt;0$ (under an
appropriate complexity assumption), but is approximable with ratio
$\sqrt{\Delta}+3$, where $\Delta$ is the maximum degree in the graph. One of
our main results concerns the class of bounded-degree graphs, which is shown to
exhibit the following interesting dichotomy. On the one hand, the minimum
exposure path problem is NP-hard on node-weighted or directed bounded-degree
graphs (even when the maximum degree is 4). On the other hand, we present a
polynomial algorithm (based on a nontrivial dynamic program) for the problem on
unweighted undirected bounded-degree graphs. Likewise, the problem is shown to
be polynomial also for the class of (weighted or unweighted) bounded-treewidth
graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6177</identifier>
 <datestamp>2013-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6177</id><created>2012-12-26</created><updated>2013-01-05</updated><authors><author><keyname>Ainsworth</keyname><forenames>Scott G.</forenames></author><author><keyname>AlSum</keyname><forenames>Ahmed</forenames></author><author><keyname>SalahEldeen</keyname><forenames>Hany</forenames></author><author><keyname>Weigle</keyname><forenames>Michele C.</forenames></author><author><keyname>Nelson</keyname><forenames>Michael L.</forenames></author></authors><title>How Much of the Web Is Archived?</title><categories>cs.DL cs.IR</categories><comments>This is the long version of the short paper by the same title
  published at JCDL'11. 10 pages, 5 figures, 7 tables. Version 2 includes minor
  typographical corrections</comments><acm-class>H.3.7</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Although the Internet Archive's Wayback Machine is the largest and most
well-known web archive, there have been a number of public web archives that
have emerged in the last several years. With varying resources, audiences and
collection development policies, these archives have varying levels of overlap
with each other. While individual archives can be measured in terms of number
of URIs, number of copies per URI, and intersection with other archives, to
date there has been no answer to the question &quot;How much of the Web is
archived?&quot; We study the question by approximating the Web using sample URIs
from DMOZ, Delicious, Bitly, and search engine indexes; and, counting the
number of copies of the sample URIs exist in various public web archives. Each
sample set provides its own bias. The results from our sample sets indicate
that range from 35%-90% of the Web has at least one archived copy, 17%-49% has
between 2-5 copies, 1%-8% has 6-10 copies, and 8%-63% has more than 10 copies
in public web archives. The number of URI copies varies as a function of time,
but no more than 31.3% of URIs are archived more than once per month.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6183</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6183</id><created>2012-12-26</created><authors><author><keyname>Deng</keyname><forenames>Xiaojie</forenames></author><author><keyname>Zhang</keyname><forenames>Yu</forenames></author><author><keyname>Deng</keyname><forenames>Yuxin</forenames></author><author><keyname>Zhong</keyname><forenames>Farong</forenames></author></authors><title>The Buffered \pi-Calculus: A Model for Concurrent Languages</title><categories>cs.LO cs.FL</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Message-passing based concurrent languages are widely used in developing
large distributed and coordination systems. This paper presents the buffered
$\pi$-calculus --- a variant of the $\pi$-calculus where channel names are
classified into buffered and unbuffered: communication along buffered channels
is asynchronous, and remains synchronous along unbuffered channels. We show
that the buffered $\pi$-calculus can be fully simulated in the polyadic
$\pi$-calculus with respect to strong bisimulation. In contrast to the
$\pi$-calculus which is hard to use in practice, the new language enables easy
and clear modeling of practical concurrent languages. We encode two real-world
concurrent languages in the buffered $\pi$-calculus: the (core) Go language and
the (Core) Erlang. Both encodings are fully abstract with respect to weak
bisimulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6193</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6193</id><created>2012-12-26</created><authors><author><keyname>Sawant</keyname><forenames>Uma</forenames></author><author><keyname>Chakrabarti</keyname><forenames>Soumen</forenames></author></authors><title>Learning Joint Query Interpretation and Response Ranking</title><categories>cs.IR</categories><comments>11 pages, 14 figures</comments><acm-class>H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Thanks to information extraction and semantic Web efforts, search on
unstructured text is increasingly refined using semantic annotations and
structured knowledge bases. However, most users cannot become familiar with the
schema of knowledge bases and ask structured queries. Interpreting free-format
queries into a more structured representation is of much current interest. The
dominant paradigm is to segment or partition query tokens by purpose
(references to types, entities, attribute names, attribute values, relations)
and then launch the interpreted query on structured knowledge bases. Given that
structured knowledge extraction is never complete, here we use a data
representation that retains the unstructured text corpus, along with structured
annotations (mentions of entities and relationships) on it. We propose two new,
natural formulations for joint query interpretation and response ranking that
exploit bidirectional flow of information between the knowledge base and the
corpus.One, inspired by probabilistic language models, computes expected
response scores over the uncertainties of query interpretation. The other is
based on max-margin discriminative learning, with latent variables representing
those uncertainties. In the context of typed entity search, both formulations
bridge a considerable part of the accuracy gap between a generic query that
does not constrain the type at all, and the upper bound where the &quot;perfect&quot;
target entity type of each query is provided by humans. Our formulations are
also superior to a two-stage approach of first choosing a target type using
recent query type prediction techniques, and then launching a type-restricted
entity search query.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6196</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6196</id><created>2012-12-20</created><authors><author><keyname>Khan</keyname><forenames>Sadeque Reza</forenames></author></authors><title>Development of Low Cost Private Office Access Control System (OACS)</title><categories>cs.OH</categories><comments>ISSN: 1839-5171</comments><journal-ref>IJESA, Volume 02, Number 02, page 01-07, 2012</journal-ref><doi>10.5121/ijesa.2012.2201</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the years, access control systems have become more and more
sophisticated and several security measures have been employed to combat the
menace of insecurity of lives and property. This is done by preventing
unauthorized entrance into buildings through entrance doors using conventional
and electronic locks, discrete access code, and biometric methods such as the
finger prints, thumb prints, the iris and facial recognition. We have designed
a flexible and low cost modular system based on integration of keypad, magnetic
lock and a controller. PIC 16F876A which is an 8-bit Microcontroller, is used
here as a main controller. An advanced simulation based compiler Flowcode V4 is
used to develop the software part in this project.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6207</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6207</id><created>2012-12-21</created><authors><author><keyname>Sorudeykin</keyname><forenames>Kirill A.</forenames></author></authors><title>Irrespective Priority-Based Regular Properties of High-Intensity Virtual
  Environments</title><categories>cs.AI</categories><comments>4 pages, 2 figures; ISBN: 978-1-4673-2984-2</comments><journal-ref>20th Telecommunications Forum TELFOR 2012, 2012, pp. 510-513</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We have a lot of relation to the encoding and the Theory of Information, when
considering thinking. This is a natural process and, at once, the complex thing
we investigate. This always was a challenge - to understand how our mind works,
and we are trying to find some universal models for this. A lot of ways have
been considered so far, but we are looking for Something, we seek for
approaches. And the goal is to find a consistent, noncontradictory view, which
should at once be enough flexible in any dimensions to allow to represent
various kinds of processes and environments, matters of different nature and
diverse objects. Developing of such a model is the destination of this article.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6209</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6209</id><created>2012-12-26</created><authors><author><keyname>Deng</keyname><forenames>Yi</forenames></author><author><keyname>Coen</keyname><forenames>Philip</forenames></author><author><keyname>Sun</keyname><forenames>Mingzhai</forenames></author><author><keyname>Shaevitz</keyname><forenames>Joshua W.</forenames></author></authors><title>Efficient Multiple Object Tracking Using Mutually Repulsive Active
  Membranes</title><categories>q-bio.QM cs.CV physics.bio-ph</categories><comments>18 pages, 6 figures, 1 table</comments><doi>10.1371/journal.pone.0065769</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Studies of social and group behavior in interacting organisms require
high-throughput analysis of the motion of a large number of individual
subjects. Computer vision techniques offer solutions to specific tracking
problems, and allow automated and efficient tracking with minimal human
intervention. In this work, we adopt the open active contour model to track the
trajectories of moving objects at high density. We add repulsive interactions
between open contours to the original model, treat the trajectories as an
extrusion in the temporal dimension, and show applications to two tracking
problems. The walking behavior of Drosophila is studied at different population
density and gender composition. We demonstrate that individual male flies have
distinct walking signatures, and that the social interaction between flies in a
mixed gender arena is gender specific. We also apply our model to studies of
trajectories of gliding Myxococcus xanthus bacteria at high density. We examine
the individual gliding behavioral statistics in terms of the gliding speed
distribution. Using these two examples at very distinctive spatial scales, we
illustrate the use of our algorithm on tracking both short rigid bodies
(Drosophila) and long flexible objects (Myxococcus xanthus). Our repulsive
active membrane model reaches error rates better than $5\times 10^{-6}$ per fly
per second for Drosophila tracking and comparable results for Myxococcus
xanthus.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6216</identifier>
 <datestamp>2013-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6216</id><created>2012-12-26</created><updated>2013-01-06</updated><authors><author><keyname>Amoozgar</keyname><forenames>Masoud</forenames></author><author><keyname>Khashabi</keyname><forenames>Daniel</forenames></author><author><keyname>Heydarian</keyname><forenames>Milad</forenames></author><author><keyname>Nokhbeh</keyname><forenames>Mohammad</forenames></author><author><keyname>Shouraki</keyname><forenames>Saeed Bagheri</forenames></author></authors><title>Generating Motion Patterns Using Evolutionary Computation in Digital
  Soccer</title><categories>cs.AI cs.RO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Dribbling an opponent player in digital soccer environment is an important
practical problem in motion planning. It has special complexities which can be
generalized to most important problems in other similar Multi Agent Systems. In
this paper, we propose a hybrid computational geometry and evolutionary
computation approach for generating motion trajectories to avoid a mobile
obstacle. In this case an opponent agent is not only an obstacle but also one
who tries to harden dribbling procedure. One characteristic of this approach is
reducing process cost of online stage by transferring it to offline stage which
causes increment in agents' performance. This approach breaks the problem into
two offline and online stages. During offline stage the goal is to find desired
trajectory using evolutionary computation and saving it as a trajectory plan. A
trajectory plan consists of nodes which approximate information of each
trajectory plan. In online stage, a linear interpolation along with Delaunay
triangulation in xy-plan is applied to trajectory plan to retrieve desired
action.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6225</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6225</id><created>2012-12-26</created><authors><author><keyname>Pang</keyname><forenames>Jong-Shi</forenames></author><author><keyname>Scutari</keyname><forenames>Gesualdo</forenames></author></authors><title>Joint Sensing and Power Allocation in Nonconvex Cognitive Radio Games:
  Quasi-Nash Equilibria</title><categories>cs.IT math.IT</categories><comments>to appear on IEEE Trans. on Signal Processing, 2013</comments><doi>10.1109/TSP.2013.2239993</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a novel class of Nash problems for Cognitive Radio
(CR) networks composed of multiple primary users (PUs) and secondary users
(SUs) wherein each SU (player) competes against the others to maximize his own
opportunistic throughput by choosing jointly the sensing duration, the
detection thresholds, and the vector power allocation over a multichannel link.
In addition to power budget constraints, several (deterministic or
probabilistic) interference constraints can be accommodated in the proposed
general formulation, such as constraints on the maximum individual/aggregate
(probabilistic) interference tolerable from the PUs. To keep the optimization
as decentralized as possible, global interference constraints, when present,
are imposed via pricing; the prices are thus additional variables to be
optimized. The resulting players' optimization problems are nonconvex and there
are price clearance conditions associated with the nonconvex global
interference constraints to be satisfied by the equilibria of the game, which
make the analysis of the proposed game a challenging task; none of classical
results in the game theory literature can be successfully applied. To deal with
the nonconvexity of the game, we introduce a relaxed equilibrium concept, the
Quasi-Nash Equilibrium (QNE), and study its main properties, performance, and
connection with local Nash equilibria. Quite interestingly, the proposed game
theoretical formulations yield a considerable performance improvement with
respect to current centralized and decentralized designs of CR systems, which
validates the concept of QNE.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6235</identifier>
 <datestamp>2013-12-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6235</id><created>2012-12-26</created><updated>2013-12-14</updated><authors><author><keyname>Scutari</keyname><forenames>Gesualdo</forenames></author><author><keyname>Facchinei</keyname><forenames>Francisco</forenames></author><author><keyname>Pang</keyname><forenames>Jong-Shi</forenames></author><author><keyname>Palomar</keyname><forenames>Daniel P.</forenames></author></authors><title>Real and Complex Monotone Communication Games</title><categories>cs.GT cs.IT math.IT</categories><comments>to appear on IEEE Transactions in Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Noncooperative game-theoretic tools have been increasingly used to study many
important resource allocation problems in communications, networking, smart
grids, and portfolio optimization. In this paper, we consider a general class
of convex Nash Equilibrium Problems (NEPs), where each player aims to solve an
arbitrary smooth convex optimization problem. Differently from most of current
works, we do not assume any specific structure for the players' problems, and
we allow the optimization variables of the players to be matrices in the
complex domain. Our main contribution is the design of a novel class of
distributed (asynchronous) best-response- algorithms suitable for solving the
proposed NEPs, even in the presence of multiple solutions. The new methods,
whose convergence analysis is based on Variational Inequality (VI) techniques,
can select, among all the equilibria of a game, those that optimize a given
performance criterion, at the cost of limited signaling among the players. This
is a major departure from existing best-response algorithms, whose convergence
conditions imply the uniqueness of the NE. Some of our results hinge on the use
of VI problems directly in the complex domain; the study of these new kind of
VIs also represents a noteworthy innovative contribution. We then apply the
developed methods to solve some new generalizations of SISO and MIMO games in
cognitive radios and femtocell systems, showing a considerable performance
improvement over classical pure noncooperative schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6241</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6241</id><created>2012-12-21</created><authors><author><keyname>Garde</keyname><forenames>Fran&#xe7;ois</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Boyer</keyname><forenames>Harry</forenames><affiliation>PIMENT</affiliation></author><author><keyname>Celaire</keyname><forenames>Robert</forenames></author><author><keyname>Seauve</keyname><forenames>Laurent</forenames></author></authors><title>Building design in tropical climates. Elaboration of the ECODOM standard
  in the french tropical islands</title><categories>cs.OH</categories><comments>ISES'99, International Solar Energy Society, J\'erusalem : Israel
  (1999). arXiv admin note: substantial text overlap with arXiv:1212.3925,
  arXiv:1212.5252</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper deals with the elaboration of global quality standards for natural
and low energy cooling in french tropical island buildings. Electric load
profiles of tropical islands in developed countries are characterised by
morning, midday and evening peaks arising from all year round high power demand
in the commercial and residential sectors, mostly due to air conditioning
appliances and bad thermal conception of the building. In early 1995, a DSM
pilot initiative has been launched in the french islands of Guadeloupe and
Reunion through a partnership between the French Public Utility EDF,
institutions involved in energy conservation, environment preservation (ADEME)
and construction quality improvment, the University of Reunion Island and
several other public and private partners (low cost housing institutions,
architects, energy consultant, etc...) to set up a standard in the thermal
conception of buildings in tropical climates. This has led to definition of
optimized bioclimatic urban planning and architectural design, the use of
passive cooling architectural components, natural ventilation and energy
efficient systems. The impact of each technical solution on the thermal comfort
within the building was evaluated with an airflow and thermal building
simulation software (CODYRUN). These technical solutions have been edited in a
pedagogical reference document and have been implemented in 300 new pilot
dwelling projects through the year 1996 in Reunion Island and in Guadeloupe
island. An experimental follow up is still in process in the first ECODOM
dwellings for an experimental validation of the impact of the passive cooling
solutions on the comfort of the occupants and to modify them if necessary.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6246</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6246</id><created>2012-12-26</created><authors><author><keyname>Wang</keyname><forenames>Chunyi</forenames></author><author><keyname>Neal</keyname><forenames>Radford M.</forenames></author></authors><title>Gaussian Process Regression with Heteroscedastic or Non-Gaussian
  Residuals</title><categories>stat.ML cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Gaussian Process (GP) regression models typically assume that residuals are
Gaussian and have the same variance for all observations. However, applications
with input-dependent noise (heteroscedastic residuals) frequently arise in
practice, as do applications in which the residuals do not have a Gaussian
distribution. In this paper, we propose a GP Regression model with a latent
variable that serves as an additional unobserved covariate for the regression.
This model (which we call GPLC) allows for heteroscedasticity since it allows
the function to have a changing partial derivative with respect to this
unobserved covariate. With a suitable covariance function, our GPLC model can
handle (a) Gaussian residuals with input-dependent variance, or (b)
non-Gaussian residuals with input-dependent variance, or (c) Gaussian residuals
with constant variance. We compare our model, using synthetic datasets, with a
model proposed by Goldberg, Williams and Bishop (1998), which we refer to as
GPLV, which only deals with case (a), as well as a standard GP model which can
handle only case (c). Markov Chain Monte Carlo methods are developed for both
modelsl. Experiments show that when the data is heteroscedastic, both GPLC and
GPLV give better results (smaller mean squared error and negative
log-probability density) than standard GP regression. In addition, when the
residual are Gaussian, our GPLC model is generally nearly as good as GPLV,
while when the residuals are non-Gaussian, our GPLC model is better than GPLV.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6250</identifier>
 <datestamp>2012-12-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6250</id><created>2012-12-26</created><authors><author><keyname>Song</keyname><forenames>Miao</forenames></author></authors><title>Computer-Assisted Interactive Documentary and Performance Arts in
  Illimitable Space</title><categories>cs.MM cs.CY cs.GR cs.HC</categories><comments>PhD thesis copy; 272 pages, 83 figures, 6 algorithms</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This major component of the research described in this thesis is 3D computer
graphics, specifically the realistic physics-based softbody simulation and
haptic responsive environments. Minor components include advanced
human-computer interaction environments, non-linear documentary storytelling,
and theatre performance. The journey of this research has been unusual because
it requires a researcher with solid knowledge and background in multiple
disciplines; who also has to be creative and sensitive in order to combine the
possible areas into a new research direction. [...] It focuses on the advanced
computer graphics and emerges from experimental cinematic works and theatrical
artistic practices. Some development content and installations are completed to
prove and evaluate the described concepts and to be convincing. [...] To
summarize, the resulting work involves not only artistic creativity, but
solving or combining technological hurdles in motion tracking, pattern
recognition, force feedback control, etc., with the available documentary
footage on film, video, or images, and text via a variety of devices [....] and
programming, and installing all the needed interfaces such that it all works in
real-time. Thus, the contribution to the knowledge advancement is in solving
these interfacing problems and the real-time aspects of the interaction that
have uses in film industry, fashion industry, new age interactive theatre,
computer games, and web-based technologies and services for entertainment and
education. It also includes building up on this experience to integrate Kinect-
and haptic-based interaction, artistic scenery rendering, and other forms of
control. This research work connects all the research disciplines, seemingly
disjoint fields of research, such as computer graphics, documentary film,
interactive media, and theatre performance together.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6259</identifier>
 <datestamp>2013-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6259</id><created>2012-12-23</created><authors><author><keyname>Bassil</keyname><forenames>Youssef</forenames></author></authors><title>Image Steganography based on a Parameterized Canny Edge Detection
  Algorithm</title><categories>cs.CR</categories><comments>LACSC Lebanese Association for Computational Sciences, International
  Journal of Computer Applications, Vol 60, No.4, 2012, http://www.lacsc.org.
  arXiv admin note: text overlap with arXiv:1212.2064, arXiv:1212.5801</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Steganography is the science of hiding digital information in such a way that
no one can suspect its existence. Unlike cryptography which may arouse
suspicions, steganography is a stealthy method that enables data communication
in total secrecy. Steganography has many requirements, the foremost one is
irrecoverability which refers to how hard it is for someone apart from the
original communicating parties to detect and recover the hidden data out of the
secret communication. A good strategy to guaranteeirrecoverability is to cover
the secret data not usinga trivial method based on a predictable algorithm, but
using a specific random pattern based on a mathematical algorithm. This paper
proposes an image steganography technique based on theCanny edge detection
algorithm.It is designed to hide secret data into a digital image within the
pixels that make up the boundaries of objects detected in the image. More
specifically, bits of the secret data replace the three LSBs of every color
channel of the pixels detected by the Canny edge detection algorithm as part of
the edges in the carrier image. Besides, the algorithm is parameterized by
three parameters: The size of the Gaussian filter, a low threshold value, and a
high threshold value. These parameters can yield to different outputs for the
same input image and secret data. As a result, discovering the inner-workings
of the algorithm would be considerably ambiguous, misguiding steganalysts from
the exact location of the covert data. Experiments showed a simulation tool
codenamed GhostBit, meant to cover and uncover secret data using the proposed
algorithm. As future work, examining how other image processing techniques such
as brightness and contrast adjustment can be taken advantage of in
steganography with the purpose ofgiving the communicating parties more
preferences tomanipulate their secret communication.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6273</identifier>
 <datestamp>2013-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6273</id><created>2012-12-26</created><authors><author><keyname>Cabibihan</keyname><forenames>John-John</forenames></author><author><keyname>So</keyname><forenames>Wing-Chee</forenames></author><author><keyname>Pramanik</keyname><forenames>Soumo</forenames></author></authors><title>Human-Recognizable Robotic Gestures</title><categories>cs.RO cs.AI cs.HC</categories><comments>21 pages, 5 figures</comments><journal-ref>Autonomous Mental Development, IEEE Transactions, 2012, 4(4),
  305-314</journal-ref><doi>10.1109/TAMD.2012.2208962</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  For robots to be accommodated in human spaces and in humans daily activities,
robots should be able to understand messages from the human conversation
partner. In the same light, humans must also understand the messages that are
being communicated by robots, including the non-verbal ones. We conducted a
web-based video study wherein participants gave interpretations on the iconic
gestures and emblems that were produced by an anthropomorphic robot. Out of the
15 gestures presented, we found 6 robotic gestures that can be accurately
recognized by the human observer. These were nodding, clapping, hugging,
expressing anger, walking, and flying. We reviewed these gestures for their
meaning from literatures in human and animal behavior. We conclude by
discussing the possible implications of these gestures for the design of social
robots that are aimed to have engaging interactions with humans.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6276</identifier>
 <datestamp>2013-04-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6276</id><created>2012-12-26</created><authors><author><keyname>Basterrech</keyname><forenames>Sebasti&#xe1;n</forenames></author><author><keyname>Rubino</keyname><forenames>Gerardo</forenames></author></authors><title>Echo State Queueing Network: a new reservoir computing learning tool</title><categories>cs.NE cs.AI cs.LG</categories><comments>Proceedings of the 10th IEEE Consumer Communications and Networking
  Conference (CCNC), Las Vegas, USA, 2013</comments><msc-class>92B20, 90B22, 90B20, 37M10</msc-class><acm-class>I.2; D.4.8; F.1.1</acm-class><doi>10.1109/CCNC.2013.6488435</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the last decade, a new computational paradigm was introduced in the field
of Machine Learning, under the name of Reservoir Computing (RC). RC models are
neural networks which a recurrent part (the reservoir) that does not
participate in the learning process, and the rest of the system where no
recurrence (no neural circuit) occurs. This approach has grown rapidly due to
its success in solving learning tasks and other computational applications.
Some success was also observed with another recently proposed neural network
designed using Queueing Theory, the Random Neural Network (RandNN). Both
approaches have good properties and identified drawbacks. In this paper, we
propose a new RC model called Echo State Queueing Network (ESQN), where we use
ideas coming from RandNNs for the design of the reservoir. ESQNs consist in
ESNs where the reservoir has a new dynamics inspired by recurrent RandNNs. The
paper positions ESQNs in the global Machine Learning area, and provides
examples of their use and performances. We show on largely used benchmarks that
ESQNs are very accurate tools, and we illustrate how they compare with standard
ESNs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6296</identifier>
 <datestamp>2013-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6296</id><created>2012-12-26</created><authors><author><keyname>Mutiara</keyname><forenames>A. B.</forenames></author><author><keyname>Muslim</keyname><forenames>A.</forenames></author><author><keyname>Oswari</keyname><forenames>T.</forenames></author><author><keyname>Miharja</keyname><forenames>R. A.</forenames></author></authors><title>A Model of OpenEHR Based Electronic Medical Record In Indonesia</title><categories>cs.CY</categories><comments>10 pages, 5 figures; European Journal of Scientific Research (EJSR),
  Vol. 90 Issues 3, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For the realization of the vision and mission of Healthy Indonesia 2015, we
need a health service with a broad and comprehensive scope.To provide health
services, it can be realized by creating an integrated information system
applications such as creating an electronic medical record that has the ability
to process and store patient medical data. The specifications used medical
record is an open specification contained in OpenEHR that includes information
and service model for electronic medical records, demographics, and the
archetype which allows software developers taking the logical structure as a
universal functional interface, so it can facilitate the process of information
by the recipient. It is because of using the interface with
appropriate-purposed data presentation and data on computer screen of the same
users. The purpose of this paper is to create an electronic website for the
medical record by using OpenEHR specifications for easy accessing, processing
and storing the medical records by the actors that play a role in the data
processing of medical records. With this application it is expected to be
useful for data processing and health information gathering, thus to improve
the quality of services that will impact the improved performance of the
hospital management. The improved performance of the hospital management will
become a supporter of the vision and mission Healthy Indonesia 2015.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6298</identifier>
 <datestamp>2013-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6298</id><created>2012-12-26</created><authors><author><keyname>Refianti</keyname><forenames>R.</forenames></author><author><keyname>Mutiara</keyname><forenames>A. B.</forenames></author><author><keyname>Gunawan</keyname><forenames>H.</forenames></author></authors><title>Design of Intelligent Agents Based System for Commodity Market
  Simulation with JADE</title><categories>cs.MA cs.AI</categories><comments>13 pages, 11 figures; European Journal of Scientific Research (EJSR),
  Vol. 92 Issue 3, Desember 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A market of potato commodity for industry scale usage is engaging several
types of actors. They are farmers, middlemen, and industries. A multi-agent
system has been built to simulate these actors into agent entities, based on
manually given parameters within a simulation scenario file. Each type of
agents has its own fuzzy logic representing actual actors' knowledge, to be
used to interpreting values and take appropriated decision of it while on
simulation. The system will simulate market activities with programmed
behaviors then produce the results as spreadsheet and chart graph files. These
results consist of each agent's yearly finance and commodity data. The system
will also predict each of next value from these outputs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6299</identifier>
 <datestamp>2013-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6299</id><created>2012-12-26</created><authors><author><keyname>Fitriyani</keyname><forenames>Y.</forenames></author><author><keyname>Mutiara</keyname><forenames>A. B.</forenames></author><author><keyname>Refianti</keyname><forenames>R.</forenames></author></authors><title>YAGI Antenna Design for Signal Phone Jammer</title><categories>cs.OH</categories><comments>6 pages, 10 figures; Journal of Theoretical and Applied Information
  Technology (JATIT), Vol.43 No.1, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile phone is one of the most widely used today in mobile communications.
This technology is very useful for communication but this raises several
problems in a situation where silence is required such as in libraries, places
of worship, classrooms and others. Mobile phone jammer is a device that used to
block the incoming signal to a mobile phone from the base station. If the
mobile phone jammer is turned on then it can not form the incoming or outgoing
calls even sms. In this research, we designed a Yagi antenna (900MHz) to expand
the range of jamming because Yagi has a great gain. Results of impedance by
gamma match are 50.16 Om. Obtained the value of VSWR Yagi is 1.46:1 and jamming
distance that can be taken approximately 16 meters, It is different from the
jamming distance of helical antenna on a mobile phone jammer itself is about 4
meters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6303</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6303</id><created>2012-12-27</created><authors><author><keyname>Saha</keyname><forenames>Sangeet</forenames></author><author><keyname>pal</keyname><forenames>Chandrajit</forenames></author><author><keyname>paul</keyname><forenames>Rourab</forenames></author><author><keyname>Maity</keyname><forenames>Satyabrata</forenames></author><author><keyname>Sau</keyname><forenames>Suman</forenames></author></authors><title>A brief experience on journey through hardware developments for image
  processing and its applications on Cryptography</title><categories>cs.AR cs.CR cs.CV</categories><comments>In the proceedings of 100th Indian Science Congress,03-07
  January,Kolkata</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The importance of embedded applications on image and video
processing,communication and cryptography domain has been taking a larger space
in current research era. Improvement of pictorial information for betterment of
human perception like deblurring, de-noising in several fields such as
satellite imaging, medical imaging etc are renewed research thrust.
Specifically we would like to elaborate our experience on the significance of
computer vision as one of the domains where hardware implemented algorithms
perform far better than those implemented through software. So far embedded
design engineers have successfully implemented their designs by means of
Application Specific Integrated Circuits (ASICs) and/or Digital Signal
Processors (DSP), however with the advancement of VLSI technology a very
powerful hardware device namely the Field Programmable Gate Array (FPGA)
combining the key advantages of ASICs and DSPs was developed which have the
possibility of reprogramming making them a very attractive device for rapid
prototyping.Communication of image and video data in multiple FPGA is no longer
far away from the thrust of secured transmission among them, and then the
relevance of cryptography is indeed unavoidable. This paper shows how the
Xilinx hardware development platform as well Mathworks Matlab can be used to
develop hardware based computer vision algorithms and its corresponding crypto
transmission channel between multiple FPGA platform from a system level
approach, making it favourable for developing a hardware-software co-design
environment.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6316</identifier>
 <datestamp>2013-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6316</id><created>2012-12-27</created><authors><author><keyname>Olteanu</keyname><forenames>Madalina</forenames><affiliation>SAMM</affiliation></author><author><keyname>Villa-Vialaneix</keyname><forenames>Nathalie</forenames><affiliation>SAMM</affiliation></author><author><keyname>Cottrell</keyname><forenames>Marie</forenames><affiliation>SAMM</affiliation></author></authors><title>On-line relational SOM for dissimilarity data</title><categories>stat.ML cs.LG</categories><comments>WSOM 2012, Santiago : Chile (2012)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In some applications and in order to address real world situations better,
data may be more complex than simple vectors. In some examples, they can be
known through their pairwise dissimilarities only. Several variants of the Self
Organizing Map algorithm were introduced to generalize the original algorithm
to this framework. Whereas median SOM is based on a rough representation of the
prototypes, relational SOM allows representing these prototypes by a virtual
combination of all elements in the data set. However, this latter approach
suffers from two main drawbacks. First, its complexity can be large. Second,
only a batch version of this algorithm has been studied so far and it often
provides results having a bad topographic organization. In this article, an
on-line version of relational SOM is described and justified. The algorithm is
tested on several datasets, including categorical data and graphs, and compared
with the batch version and with other SOM algorithms for non vector data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6323</identifier>
 <datestamp>2013-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6323</id><created>2012-12-27</created><authors><author><keyname>Hu</keyname><forenames>Pili</forenames></author><author><keyname>Lau</keyname><forenames>Wing Cheong</forenames></author></authors><title>Localized Algorithm of Community Detection on Large-Scale Decentralized
  Social Networks</title><categories>cs.SI physics.soc-ph stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite the overwhelming success of the existing Social Networking Services
(SNS), their centralized ownership and control have led to serious concerns in
user privacy, censorship vulnerability and operational robustness of these
services. To overcome these limitations, Distributed Social Networks (DSN) have
recently been proposed and implemented. Under these new DSN architectures, no
single party possesses the full knowledge of the entire social network. While
this approach solves the above problems, the lack of global knowledge for the
DSN nodes makes it much more challenging to support some common but critical
SNS services like friends discovery and community detection. In this paper, we
tackle the problem of community detection for a given user under the constraint
of limited local topology information as imposed by common DSN architectures.
By considering the Personalized Page Rank (PPR) approach as an ink spilling
process, we justify its applicability for decentralized community detection
using limited local topology information.Our proposed PPR-based solution has a
wide range of applications such as friends recommendation, targeted
advertisement, automated social relationship labeling and sybil defense. Using
data collected from a large-scale SNS in practice, we demonstrate our adapted
version of PPR can significantly outperform the basic PR as well as two other
commonly used heuristics. The inclusion of a few manually labeled friends in
the Escape Vector (EV) can boost the performance considerably (64.97% relative
improvement in terms of Area Under the ROC Curve (AUC)).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6325</identifier>
 <datestamp>2013-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6325</id><created>2012-12-27</created><authors><author><keyname>Takada</keyname><forenames>Masaaki</forenames></author><author><keyname>Hori</keyname><forenames>Yutaka</forenames></author><author><keyname>Hara</keyname><forenames>Shinji</forenames></author></authors><title>Existence of Oscillations in Cyclic Gene Regulatory Networks with Time
  Delay</title><categories>cs.SY math.OC q-bio.MN</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is concerned with conditions for the existence of oscillations in
gene regulatory networks with negative cyclic feedback, where time delays in
transcription, translation and translocation process are explicitly considered.
The primary goal of this paper is to propose systematic analysis tools that are
useful for a broad class of cyclic gene regulatory networks, and to provide
novel biological insights. To this end, we adopt a simplified model that is
suitable for capturing the essence of a large class of gene regulatory
networks. It is first shown that local instability of the unique equilibrium
state results in oscillations based on a Poincare-Bendixson type theorem. Then,
a graphical existence condition, which is equivalent to the local instability
of a unique equilibrium, is derived. Based on the graphical condition, the
existence condition is analytically presented in terms of biochemical
parameters. This allows us to find the dimensionless parameters that primarily
affect the existence of oscillations, and to provide biological insights. The
analytic conditions and biological insights are illustrated with two existing
biochemical networks, Repressilator and the Hes7 gene regulatory networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6326</identifier>
 <datestamp>2013-04-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6326</id><created>2012-12-27</created><updated>2013-04-26</updated><authors><author><keyname>Demidov</keyname><forenames>Denis</forenames></author><author><keyname>Ahnert</keyname><forenames>Karsten</forenames></author><author><keyname>Rupp</keyname><forenames>Karl</forenames></author><author><keyname>Gottschling</keyname><forenames>Peter</forenames></author></authors><title>Programming CUDA and OpenCL: A Case Study Using Modern C++ Libraries</title><categories>cs.MS cs.DC physics.comp-ph</categories><comments>21 pages, 4 figures, submitted to SIAM Journal of Scientific
  Computing and accepted</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a comparison of several modern C++ libraries providing high-level
interfaces for programming multi- and many-core architectures on top of CUDA or
OpenCL. The comparison focuses on the solution of ordinary differential
equations and is based on odeint, a framework for the solution of systems of
ordinary differential equations. Odeint is designed in a very flexible way and
may be easily adapted for effective use of libraries such as Thrust, MTL4,
VexCL, or ViennaCL, using CUDA or OpenCL technologies. We found that CUDA and
OpenCL work equally well for problems of large sizes, while OpenCL has higher
overhead for smaller problems. Furthermore, we show that modern high-level
libraries allow to effectively use the computational resources of many-core
GPUs or multi-core CPUs without much knowledge of the underlying technologies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6327</identifier>
 <datestamp>2013-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6327</id><created>2012-12-27</created><authors><author><keyname>Brodnik</keyname><forenames>Andrej</forenames></author><author><keyname>Grgurovi&#x10d;</keyname><forenames>Marko</forenames></author></authors><title>Speeding up shortest path algorithms</title><categories>cs.DS cs.CC</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given an arbitrary, non-negatively weighted, directed graph $G=(V,E)$ we
present an algorithm that computes all pairs shortest paths in time
$\mathcal{O}(m^* n + m \lg n + nT_\psi(m^*, n))$, where $m^*$ is the number of
different edges contained in shortest paths and $T_\psi(m^*, n)$ is a running
time of an algorithm to solve a single-source shortest path problem (SSSP).
This is a substantial improvement over a trivial $n$ times application of
$\psi$ that runs in $\mathcal{O}(nT_\psi(m,n))$. In our algorithm we use $\psi$
as a black box and hence any improvement on $\psi$ results also in improvement
of our algorithm.
  Furthermore, a combination of our method, Johnson's reweighting technique and
topological sorting results in an $\mathcal{O}(m^*n + m \lg n)$ all-pairs
shortest path algorithm for arbitrarily-weighted directed acyclic graphs.
  In addition, we also point out a connection between the complexity of a
certain sorting problem defined on shortest paths and SSSP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6331</identifier>
 <datestamp>2014-10-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6331</id><created>2012-12-27</created><authors><author><keyname>Liang</keyname><forenames>Xiao</forenames></author><author><keyname>Zhao</keyname><forenames>Jichang</forenames></author><author><keyname>Dong</keyname><forenames>Li</forenames></author><author><keyname>Xu</keyname><forenames>Ke</forenames></author></authors><title>Modeling collective human mobility: Understanding exponential law of
  intra-urban movement</title><categories>physics.soc-ph cs.SI</categories><comments>24 pages, 12 figures</comments><journal-ref>Scientific Reports 3, 2983, 2013</journal-ref><doi>10.1038/srep02983</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It is very important to understand urban mobility patterns because most trips
are concentrated in urban areas. In the paper, a new model is proposed to model
collective human mobility in urban areas. The model can be applied to predict
individual flows not only in intra-city but also in countries or a larger
range. Based on the model, it can be concluded that the exponential law of
distance distribution is attributed to decreasing exponentially of average
density of human travel demands. Since the distribution of human travel demands
only depends on urban planning, population distribution, regional functions and
so on, it illustrates that these inherent properties of cities are impetus to
drive collective human movements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6350</identifier>
 <datestamp>2013-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6350</id><created>2012-12-27</created><authors><author><keyname>Basterrech</keyname><forenames>Sebasti&#xe1;n</forenames></author><author><keyname>Rubino</keyname><forenames>Gerardo</forenames></author><author><keyname>Varela</keyname><forenames>Mart&#xed;n</forenames></author></authors><title>Single-sided Real-time PESQ Score Estimation</title><categories>cs.SD cs.MM</categories><comments>In Proceeding of Measurement of Speech, Audio and Video Quality in
  Networks (MESAQIN'09), Prague, Czech Republic, June 2009, pp. 94-99</comments><msc-class>82C32, 62P30, 62M20</msc-class><acm-class>C.4; D.4.4; I.5.1; B.4</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For several years now, the ITU-T's Perceptual Evaluation of Speech Quality
(PESQ) has been the reference for objective speech quality assessment. It is
widely deployed in commercial QoE measurement products, and it has been well
studied in the literature. While PESQ does provide reasonably good correlation
with subjective scores for VoIP applications, the algorithm itself is not
usable in a real-time context, since it requires a reference signal, which is
usually not available in normal conditions. In this paper we provide an
alternative technique for estimating PESQ scores in a single-sided fashion,
based on the Pseudo Subjective Quality Assessment (PSQA) technique.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6354</identifier>
 <datestamp>2013-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6354</id><created>2012-12-27</created><authors><author><keyname>Haider</keyname><forenames>Sajjad</forenames></author><author><keyname>Yasin</keyname><forenames>Mehboob</forenames></author><author><keyname>Hussain</keyname><forenames>Naveed</forenames></author><author><keyname>Imran</keyname><forenames>Muhammad</forenames></author></authors><title>LNOS - Live Network Operating System</title><categories>cs.NI cs.OS</categories><comments>6th CIIT Workshop on Research in Computing (CWRC, 2007)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Operating Systems exists since existence of computers, and have been evolving
continuously from time to time. In this paper we have reviewed a relatively new
or unexplored topic of Live OS. From networking perspective, Live OS is used
for establishing Clusters, Firewalls and as Network security assessment tool
etc. Our proposed concept is that a Live OS can be established or configured
for an organizations specific network requirements with respect to their
servers. An important server failure due to hardware or software could take
time for remedy of the problem, so for that situation a preconfigured server in
the form of Live OS on CD/DVD/USB can be used as an immediate solution. In a
network of ten nodes, we stopped the server machine and with necessary
adjustments, Live OS replaced the server in less than five minutes. Live OS in
a network environment is a quick replacement of the services that are failed
due to server failure (hardware or software). It is a cost effective solution
for low budget networks. The life of Live OS starts when we boot it from
CD/DVD/USB and remains in action for that session. As soon as the machine is
rebooted, any work done for that session is gone, (in case we do not store any
information on permanent storage media). Live CD/DVD/USB is normally used on
systems where we do not have Operating Systems installed. A Live OS can also be
used on systems where we already have an installed OS. On the basis of
functionality a Live OS can be used for many purposes and has some typical
advantages that are not available on other operating systems. Vendors are
releasing different distributions of Live OS and is becoming their sole
identity in a particular domain like Networks, Security, Education or
Entertainment etc. There can be many aspects of Live OS, but Linux based Live
OS and their use in the field of networks is the main focus of this paper.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6355</identifier>
 <datestamp>2013-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6355</id><created>2012-12-27</created><authors><author><keyname>Jiang</keyname><forenames>Xiang</forenames></author><author><keyname>Pauly</keyname><forenames>Arno</forenames></author></authors><title>Efficient Decomposition of Bimatrix Games</title><categories>cs.GT</categories><msc-class>91A05, 68W99, 91-08</msc-class><acm-class>F.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Exploiting the algebraic structure of the set of bimatrix games, a
divide-and-conquer algorithm for finding Nash equilibria is proposed. The
algorithm is fixed-parameter tractable with the size of the largest irreducible
component of a game as parameter. An implementation of the algorithm is shown
to yield a significant performance increase on inputs with small parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6371</identifier>
 <datestamp>2014-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6371</id><created>2012-12-27</created><authors><author><keyname>Li</keyname><forenames>Shuxing</forenames></author><author><keyname>Hu</keyname><forenames>Sihuang</forenames></author><author><keyname>Feng</keyname><forenames>Tao</forenames></author><author><keyname>Ge</keyname><forenames>Gennian</forenames></author></authors><title>The Weight Distribution of a Class of Cyclic Codes Related to Hermitian
  Forms Graphs</title><categories>cs.IT math.CO math.IT</categories><comments>4 pages</comments><journal-ref>Information Theory, IEEE Transactions on (Volume:59 , Issue: 5 ),
  May 2013</journal-ref><doi>10.1109/TIT.2013.2242957</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The determination of weight distribution of cyclic codes involves evaluation
of Gauss sums and exponential sums. Despite of some cases where a neat
expression is available, the computation is generally rather complicated. In
this note, we determine the weight distribution of a class of reducible cyclic
codes whose dual codes may have arbitrarily many zeros. This goal is achieved
by building an unexpected connection between the corresponding exponential sums
and the spectrums of Hermitian forms graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6372</identifier>
 <datestamp>2013-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6372</id><created>2012-12-27</created><authors><author><keyname>Wang</keyname><forenames>Xiaoshu</forenames></author></authors><title>URI Identity and Web Architecture Revisited</title><categories>cs.OH</categories><comments>14 pages, originally published at http://dfdf.inesc-id.pt/tr/web-arch</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This document reexamined the URI's identity issue and the debate regarding
the nature of &quot;information resource&quot;. By making emphasis on the abstract nature
of resource and the role of URI as an interface to the web, this article
presented an alternative viewpoint about the architecture of the web that would
allow us to objectively and consistently treat all kinds of resources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6375</identifier>
 <datestamp>2013-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6375</id><created>2012-12-27</created><authors><author><keyname>V&#xe1;squez</keyname><forenames>Oscar C.</forenames></author></authors><title>Energy in computing systems with speed scaling: optimization and
  mechanisms design</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a simple scheduling game for the speed scaling model. Players want
their job to complete early, which however generates a big energy consumption.
We address the game from the mechanism design side, and by charging the energy
usage to the players we seek for a good compromize between quality of service
and energy usage.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6382</identifier>
 <datestamp>2014-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6382</id><created>2012-12-27</created><updated>2014-01-01</updated><authors><author><keyname>Babu</keyname><forenames>Jasine</forenames></author><author><keyname>Basavaraju</keyname><forenames>Manu</forenames></author><author><keyname>Chandran</keyname><forenames>L. Sunil</forenames></author><author><keyname>Rajendraprasad</keyname><forenames>Deepak</forenames></author></authors><title>2-connecting Outerplanar Graphs without Blowing Up the Pathwidth</title><categories>cs.DM cs.DS math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a connected outerplanar graph G of pathwidth p, we give an algorithm to
add edges to G to get a supergraph of G, which is 2-vertex-connected,
outerplanar and of pathwidth O(p). This settles an open problem raised by
Biedl, in the context of computing minimum height planar straight line drawings
of outerplanar graphs, with their vertices placed on a two dimensional grid. In
conjunction with the result of this paper, the constant factor approximation
algorithm for this problem obtained by Biedl for 2-vertex-connected outerplanar
graphs will work for all outer planar graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6383</identifier>
 <datestamp>2015-04-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6383</id><created>2012-12-27</created><authors><author><keyname>Burattin</keyname><forenames>Andrea</forenames></author><author><keyname>Sperduti</keyname><forenames>Alessandro</forenames></author><author><keyname>van der Aalst</keyname><forenames>Wil M. P.</forenames></author></authors><title>Heuristics Miners for Streaming Event Data</title><categories>cs.DB</categories><acm-class>H.2.8; F.1.2</acm-class><doi>10.1109/CEC.2014.6900341</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  More and more business activities are performed using information systems.
These systems produce such huge amounts of event data that existing systems are
unable to store and process them. Moreover, few processes are in steady-state
and due to changing circumstances processes evolve and systems need to adapt
continuously. Since conventional process discovery algorithms have been defined
for batch processing, it is difficult to apply them in such evolving
environments. Existing algorithms cannot cope with streaming event data and
tend to generate unreliable and obsolete results.
  In this paper, we discuss the peculiarities of dealing with streaming event
data in the context of process mining. Subsequently, we present a general
framework for defining process mining algorithms in settings where it is
impossible to store all events over an extended period or where processes
evolve while being analyzed. We show how the Heuristics Miner, one of the most
effective process discovery algorithms for practical applications, can be
modified using this framework. Different stream-aware versions of the
Heuristics Miner are defined and implemented in ProM. Moreover, experimental
results on artificial and real logs are reported.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6388</identifier>
 <datestamp>2013-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6388</id><created>2012-12-27</created><authors><author><keyname>Baayen</keyname><forenames>Jorn H.</forenames></author></authors><title>Trajectory tracking control of kites with system delay</title><categories>cs.SY math.OC</categories><comments>10 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A previously published algorithm for trajectory tracking control of tethered
wings, i.e. kites, is updated in light of recent experimental evidence. The
algorithm is, furthermore, analyzed in the framework of delay differential
equations. It is shown how the presence of system delay influences the
stability of the control system, and a methodology is derived for gain
selection using the Lambert W function. The validity of the methodology is
demonstrated with simulation results. The analysis sheds light on previously
poorly understood stability problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6437</identifier>
 <datestamp>2013-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6437</id><created>2012-12-27</created><authors><author><keyname>Scutari</keyname><forenames>Gesualdo</forenames></author><author><keyname>Pang</keyname><forenames>Jong-Shi</forenames></author></authors><title>Joint Sensing and Power Allocation in Nonconvex Cognitive Radio Games:
  Nash Equilibria and Distributed Algorithms</title><categories>cs.IT math.IT</categories><comments>to appear IEEE Transactions on Information Theory, 2013. arXiv admin
  note: text overlap with arXiv:1212.6225</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose a novel class of Nash problems for Cognitive Radio
(CR) networks, modeled as Gaussian frequency-selective interference channels,
wherein each secondary user (SU) competes against the others to maximize his
own opportunistic throughput by choosing jointly the sensing duration, the
detection thresholds, and the vector power allocation. The proposed general
formulation allows to accommodate several (transmit) power and
(deterministic/probabilistic) interference constraints, such as constraints on
the maximum individual and/or aggregate (probabilistic) interference tolerable
at the primary receivers. To keep the optimization as decentralized as
possible, global (coupling) interference constraints are imposed by penalizing
each SU with a set of time-varying prices based upon his contribution to the
total interference; the prices are thus additional variable to optimize. The
resulting players' optimization problems are nonconvex; moreover, there are
possibly price clearing conditions associated with the global constraints to be
satisfied by the solution. All this makes the analysis of the proposed games a
challenging task; none of classical results in the game theory literature can
be successfully applied. The main contribution of this paper is to develop a
novel optimization-based theory for studying the proposed nonconvex games; we
provide a comprehensive analysis of the existence and uniqueness of a standard
Nash equilibrium, devise alternative best-response based algorithms, and
establish their convergence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6456</identifier>
 <datestamp>2013-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6456</id><created>2012-12-27</created><authors><author><keyname>Zhang</keyname><forenames>Guo-Qing</forenames></author><author><keyname>Cheng</keyname><forenames>Su-Qi</forenames></author><author><keyname>Zhang</keyname><forenames>Guo-Qiang</forenames></author></authors><title>A universal assortativity measure for network analysis</title><categories>physics.soc-ph cs.SI physics.data-an</categories><comments>This manuscript was submitted to Physical Review E on September
  6,2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Characterizing the connectivity tendency of a network is a fundamental
problem in network science. The traditional and well-known assortativity
coefficient is calculated on a per-network basis, which is of little use to
partial connection tendency of a network. This paper proposes a universal
assortativity coefficient(UAC), which is based on the unambiguous definition of
each individual edge's contribution to the global assortativity coefficient
(GAC). It is able to reveal the connection tendency of microscopic, mesoscopic,
macroscopic structures and any given part of a network. Applying UAC to real
world networks, we find that, contrary to the popular expectation, most
networks (notably the AS-level Internet topology) have markedly more
assortative edges/nodes than dissortaive ones despite their global
dissortativity. Consequently, networks can be categorized along two
dimensions--single global assortativity and local assortativity statistics.
Detailed anatomy of the AS-level Internet topology further illustrates how UAC
can be used to decipher the hidden patterns of connection tendencies on
different scales.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6458</identifier>
 <datestamp>2014-08-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6458</id><created>2012-12-27</created><updated>2014-08-21</updated><authors><author><keyname>Alagic</keyname><forenames>Gorjan</forenames></author><author><keyname>Jeffery</keyname><forenames>Stacey</forenames></author><author><keyname>Jordan</keyname><forenames>Stephen P.</forenames></author></authors><title>Partial-indistinguishability obfuscation using braids</title><categories>cs.CR quant-ph</categories><comments>21 pages,Proceedings of TQC 2014</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An obfuscator is an algorithm that translates circuits into
functionally-equivalent similarly-sized circuits that are hard to understand.
Efficient obfuscators would have many applications in cryptography. Until
recently, theoretical progress has mainly been limited to no-go results. Recent
works have proposed the first efficient obfuscation algorithms for classical
logic circuits, based on a notion of indistinguishability against
polynomial-time adversaries. In this work, we propose a new notion of
obfuscation, which we call partial-indistinguishability. This notion is based
on computationally universal groups with efficiently computable normal forms,
and appears to be incomparable with existing definitions. We describe universal
gate sets for both classical and quantum computation, in which our definition
of obfuscation can be met by polynomial-time algorithms. We also discuss some
potential applications to testing quantum computers. We stress that the
cryptographic security of these obfuscators, especially when composed with
translation from other gate sets, remains an open question.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6465</identifier>
 <datestamp>2013-09-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6465</id><created>2012-12-28</created><updated>2013-09-09</updated><authors><author><keyname>Zhang</keyname><forenames>Xiaojie</forenames></author><author><keyname>Siegel</keyname><forenames>Paul H.</forenames></author></authors><title>Quantized Iterative Message Passing Decoders with Low Error Floor for
  LDPC Codes</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The error floor phenomenon observed with LDPC codes and their graph-based,
iterative, message-passing (MP) decoders is commonly attributed to the
existence of error-prone substructures -- variously referred to as near
codewords, trapping sets, absorbing sets, or pseudocodewords -- in a Tanner
graph representation of the code. Many approaches have been proposed to lower
the error floor by designing new LDPC codes with fewer such substructures or by
modifying the decoding algorithm. Using a theoretical analysis of iterative MP
decoding in an idealized trapping set scenario, we show that a contributor to
the error floors observed in the literature may be the imprecise implementation
of decoding algorithms and, in particular, the message quantization rules used.
We then propose a new quantization method -- (q+1)-bit quasi-uniform
quantization -- that efficiently increases the dynamic range of messages,
thereby overcoming a limitation of conventional quantization schemes. Finally,
we use the quasi-uniform quantizer to decode several LDPC codes that suffer
from high error floors with traditional fixed-point decoder implementations.
The performance simulation results provide evidence that the proposed
quantization scheme can, for a wide variety of codes, significantly lower error
floors with minimal increase in decoder complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6478</identifier>
 <datestamp>2013-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6478</id><created>2012-12-28</created><authors><author><keyname>Vaiter</keyname><forenames>Samuel</forenames><affiliation>CEREMADE</affiliation></author><author><keyname>Deledalle</keyname><forenames>Charles</forenames><affiliation>CEREMADE</affiliation></author><author><keyname>Peyr&#xe9;</keyname><forenames>Gabriel</forenames><affiliation>CEREMADE</affiliation></author><author><keyname>Fadili</keyname><forenames>Jalal</forenames><affiliation>GREYC</affiliation></author><author><keyname>Dossal</keyname><forenames>Charles</forenames><affiliation>IMB</affiliation></author></authors><title>The degrees of freedom of the Group Lasso for a General Design</title><categories>cs.IT math.IT</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we are concerned with regression problems where covariates can
be grouped in nonoverlapping blocks, and where only a few of them are assumed
to be active. In such a situation, the group Lasso is an at- tractive method
for variable selection since it promotes sparsity of the groups. We study the
sensitivity of any group Lasso solution to the observations and provide its
precise local parameterization. When the noise is Gaussian, this allows us to
derive an unbiased estimator of the degrees of freedom of the group Lasso. This
result holds true for any fixed design, no matter whether it is under- or
overdetermined. With these results at hand, various model selec- tion criteria,
such as the Stein Unbiased Risk Estimator (SURE), are readily available which
can provide an objectively guided choice of the optimal group Lasso fit.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6492</identifier>
 <datestamp>2013-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6492</id><created>2012-12-28</created><authors><author><keyname>Wu</keyname><forenames>Changzhi</forenames></author><author><keyname>Li</keyname><forenames>Chaojie</forenames></author><author><keyname>Gao</keyname><forenames>David Yang</forenames></author></authors><title>Canonical Primal-Dual Method for Solving Non-convex Minimization
  Problems</title><categories>cs.NA cs.DS math.OC</categories><comments>21 pages, 6 figures and 4 tables</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new primal-dual algorithm is presented for solving a class of non-convex
minimization problems. This algorithm is based on canonical duality theory such
that the original non-convex minimization problem is first reformulated as a
convex-concave saddle point optimization problem, which is then solved by a
quadratically perturbed primal-dual method. %It is proved that the popular SDP
method is indeed a special case of the canonical duality theory. Numerical
examples are illustrated. Comparing with the existing results, the proposed
algorithm can achieve better performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6500</identifier>
 <datestamp>2013-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6500</id><created>2012-12-28</created><authors><author><keyname>Kufleitner</keyname><forenames>Manfred</forenames></author><author><keyname>Lauser</keyname><forenames>Alexander</forenames></author></authors><title>Quantifier Alternation in Two-Variable First-Order Logic with Successor
  Is Decidable</title><categories>cs.LO cs.FL</categories><comments>Accepted at STACS 2013</comments><acm-class>F.4.1; F.4.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the quantifier alternation hierarchy within two-variable
first-order logic FO^2[&lt;,suc] over finite words with linear order and binary
successor predicate. We give a single identity of omega-terms for each level of
this hierarchy. This shows that it is decidable for a given regular language
and a non-negative integer m, whether the language is definable by a formula in
FO^2[&lt;,suc] which has at most m quantifier alternations. We also consider the
alternation hierarchy of unary temporal logic TL[X,F,Y,P] defined by the
maximal number of nested negations. This hierarchy coincides with the
FO^2[&lt;,suc] alternation hierarchy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6510</identifier>
 <datestamp>2013-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6510</id><created>2012-12-28</created><authors><author><keyname>Derbel</keyname><forenames>Houda</forenames><affiliation>LIFL, INRIA Lille - Nord Europe</affiliation></author><author><keyname>Derbel</keyname><forenames>Bilel</forenames><affiliation>LIFL, INRIA Lille - Nord Europe</affiliation></author></authors><title>On Neighborhood Tree Search</title><categories>cs.OH</categories><comments>Genetic and Evolutionary Computation Conference (GECCO'12) (2012)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the neighborhood tree induced by alternating the use of different
neighborhood structures within a local search descent. We investigate the issue
of designing a search strategy operating at the neighborhood tree level by
exploring different paths of the tree in a heuristic way. We show that allowing
the search to 'backtrack' to a previously visited solution and resuming the
iterative variable neighborhood descent by 'pruning' the already explored
neighborhood branches leads to the design of effective and efficient search
heuristics. We describe this idea by discussing its basic design components
within a generic algorithmic scheme and we propose some simple and intuitive
strategies to guide the search when traversing the neighborhood tree. We
conduct a thorough experimental analysis of this approach by considering two
different problem domains, namely, the Total Weighted Tardiness Problem
(SMTWTP), and the more sophisticated Location Routing Problem (LRP). We show
that independently of the considered domain, the approach is highly
competitive. In particular, we show that using different branching and
backtracking strategies when exploring the neighborhood tree allows us to
achieve different trade-offs in terms of solution quality and computing cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6519</identifier>
 <datestamp>2013-03-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6519</id><created>2012-12-28</created><updated>2013-03-23</updated><authors><author><keyname>Mani</keyname><forenames>A.</forenames></author></authors><title>Dialectics of Knowledge Representation in a Granular Rough Set Theory</title><categories>cs.AI cs.LO</categories><comments>Enlarged version of Refereed Conference Paper. 18 pp 1 figure. (An
  extended version is to appear soon)</comments><msc-class>06Axx, 03G25, 03B60, 08B20, 68U35</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The concepts of rough and definite objects are relatively more determinate
than those of granules and granulation in general rough set theory (RST) [1].
Representation of rough objects can however depend on the dialectical relation
between granulation and definiteness. In this research, we make this exact in
the context of RST over proto-transitive approximation spaces. This approach
can be directly extended to many other types of RST. These are used for
formulating an extended concept of knowledge interpretation (KI)(relative the
situation for classical RST) and the problem of knowledge representation (KR)
is solved. These will be of direct interest in granular KR in RST as developed
by the present author [2] and of rough objects in general. In [3], these have
already been used for five different semantics by the present author. This is
an extended version of [4] with key examples and more results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6521</identifier>
 <datestamp>2013-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6521</id><created>2012-12-28</created><authors><author><keyname>Koutn&#xed;k</keyname><forenames>Jan</forenames></author><author><keyname>Schmidhuber</keyname><forenames>Juergen</forenames></author><author><keyname>Gomez</keyname><forenames>Faustino</forenames></author></authors><title>A Frequency-Domain Encoding for Neuroevolution</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Neuroevolution has yet to scale up to complex reinforcement learning tasks
that require large networks. Networks with many inputs (e.g. raw video) imply a
very high dimensional search space if encoded directly. Indirect methods use a
more compact genotype representation that is transformed into networks of
potentially arbitrary size. In this paper, we present an indirect method where
networks are encoded by a set of Fourier coefficients which are transformed
into network weight matrices via an inverse Fourier-type transform. Because
there often exist network solutions whose weight matrices contain regularity
(i.e. adjacent weights are correlated), the number of coefficients required to
represent these networks in the frequency domain is much smaller than the
number of weights (in the same way that natural images can be compressed by
ignore high-frequency components). This &quot;compressed&quot; encoding is compared to
the direct approach where search is conducted in the weight space on the
high-dimensional octopus arm task. The results show that representing networks
in the frequency domain can reduce the search-space dimensionality by as much
as two orders of magnitude, both accelerating convergence and yielding more
general solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6526</identifier>
 <datestamp>2014-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6526</id><created>2012-12-28</created><updated>2013-09-10</updated><authors><author><keyname>Alvarado</keyname><forenames>Alex</forenames></author><author><keyname>Brannstrom</keyname><forenames>Fredrik</forenames></author><author><keyname>Agrell</keyname><forenames>Erik</forenames></author><author><keyname>Koch</keyname><forenames>Tobias</forenames></author></authors><title>High-SNR Asymptotics of Mutual Information for Discrete Constellations
  with Applications to BICM</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Trans. Inf. Theory</comments><journal-ref>IEEE Trans. Inf. Theory, vol. 60, no. 2, pp. 1061-1076, Feb. 2014</journal-ref><doi>10.1109/TIT.2013.2291865</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Asymptotic expressions of the mutual information between any discrete input
and the corresponding output of the scalar additive white Gaussian noise
channel are presented in the limit as the signal-to-noise ratio (SNR) tends to
infinity. Asymptotic expressions of the symbol-error probability (SEP) and the
minimum mean-square error (MMSE) achieved by estimating the channel input given
the channel output are also developed. It is shown that for any input
distribution, the conditional entropy of the channel input given the output,
MMSE and SEP have an asymptotic behavior proportional to the Gaussian
Q-function. The argument of the Q-function depends only on the minimum
Euclidean distance (MED) of the constellation and the SNR, and the
proportionality constants are functions of the MED and the probabilities of the
pairs of constellation points at MED. The developed expressions are then
generalized to study the high-SNR behavior of the generalized mutual
information (GMI) for bit-interleaved coded modulation (BICM). By means of
these asymptotic expressions, the long-standing conjecture that Gray codes are
the binary labelings that maximize the BICM-GMI at high SNR is proven. It is
further shown that for any equally spaced constellation whose size is a power
of two, there always exists an anti-Gray code giving the lowest BICM-GMI at
high SNR.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6527</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6527</id><created>2012-12-28</created><authors><author><keyname>Bann</keyname><forenames>Eugene Yuta</forenames></author></authors><title>Discovering Basic Emotion Sets via Semantic Clustering on a Twitter
  Corpus</title><categories>cs.AI cs.CL</categories><comments>University of Bath BSc(Hons) Computer Science Dissertation, 105 Pages</comments><report-no>CSBU-2013-01</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A plethora of words are used to describe the spectrum of human emotions, but
how many emotions are there really, and how do they interact? Over the past few
decades, several theories of emotion have been proposed, each based around the
existence of a set of 'basic emotions', and each supported by an extensive
variety of research including studies in facial expression, ethology, neurology
and physiology. Here we present research based on a theory that people transmit
their understanding of emotions through the language they use surrounding
emotion keywords. Using a labelled corpus of over 21,000 tweets, six of the
basic emotion sets proposed in existing literature were analysed using Latent
Semantic Clustering (LSC), evaluating the distinctiveness of the semantic
meaning attached to the emotional label. We hypothesise that the more distinct
the language is used to express a certain emotion, then the more distinct the
perception (including proprioception) of that emotion is, and thus more
'basic'. This allows us to select the dimensions best representing the entire
spectrum of emotion. We find that Ekman's set, arguably the most frequently
used for classifying emotions, is in fact the most semantically distinct
overall. Next, taking all analysed (that is, previously proposed) emotion terms
into account, we determine the optimal semantically irreducible basic emotion
set using an iterative LSC algorithm. Our newly-derived set (Accepting,
Ashamed, Contempt, Interested, Joyful, Pleased, Sleepy, Stressed) generates a
6.1% increase in distinctiveness over Ekman's set (Angry, Disgusted, Joyful,
Sad, Scared). We also demonstrate how using LSC data can help visualise
emotions. We introduce the concept of an Emotion Profile and briefly analyse
compound emotions both visually and mathematically.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6531</identifier>
 <datestamp>2013-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6531</id><created>2012-12-28</created><authors><author><keyname>Tayeb</keyname><forenames>Sabria Hadj</forenames></author><author><keyname>Noureddine</keyname><forenames>Myriam</forenames></author></authors><title>Domain ontology and multi-criteria analysis for enterprise modeling</title><categories>cs.CY</categories><comments>Paper published in IJCSI International Journal of Computer Science
  Issues, Vol. 9, Issue 2, No 2, March 2012 ISSN (Online): 1694-0814</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Knowing that an enterprise is a complex reality, it is necessary to develop a
modeling framework allowing the description of system structure and dynamics
that alter the structure. The concept of enterprise modeling addresses this
need and many techniques have emerged. Our goal is to provide leaders of
Algerian enterprise an overview of modeling techniques. Thus these managers may
elect, in collaboration with the University, the modeling technique best suited
to their requirements. TWe believe that this could be a step towards an
effective reorganization of the enterprise leading. TThis article proposes a
domain ontology and multi-criteria analysis in the frame of modeling
enterprise. Our approach is based on two stages using the Prot\'eg\'e tool for
the technique representation and the PROMETHEE method for their evaluation. The
result is a ranking between the different techniques, which allows selecting
the most appropriate methodology according to the criteria for a given
Tenterprise.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6542</identifier>
 <datestamp>2013-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6542</id><created>2012-12-28</created><authors><author><keyname>Beyer</keyname><forenames>Dirk</forenames></author><author><keyname>L&#xf6;we</keyname><forenames>Stefan</forenames></author></authors><title>Explicit-Value Analysis Based on CEGAR and Interpolation</title><categories>cs.SE cs.PL</categories><comments>12 pages, 5 figures, 3 tables, 4 algorithms</comments><report-no>MIP-1205</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Abstraction, counterexample-guided refinement, and interpolation are
techniques that are essential to the success of predicate-based program
analysis. These techniques have not yet been applied together to explicit-value
program analysis. We present an approach that integrates abstraction and
interpolation-based refinement into an explicit-value analysis, i.e., a program
analysis that tracks explicit values for a specified set of variables (the
precision). The algorithm uses an abstract reachability graph as central data
structure and a path-sensitive dynamic approach for precision adjustment. We
evaluate our algorithm on the benchmark set of the Competition on Software
Verification 2012 (SV-COMP'12) to show that our new approach is highly
competitive. In addition, we show that combining our new approach with an
auxiliary predicate analysis scores significantly higher than the SV-COMP'12
winner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6550</identifier>
 <datestamp>2013-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6550</id><created>2012-12-28</created><authors><author><keyname>Martins</keyname><forenames>Andre F. T.</forenames></author><author><keyname>Figueiredo</keyname><forenames>Mario A. T.</forenames></author><author><keyname>Aguiar</keyname><forenames>Pedro M. Q.</forenames></author><author><keyname>Smith</keyname><forenames>Noah A.</forenames></author><author><keyname>Xing</keyname><forenames>Eric P.</forenames></author></authors><title>Alternating Directions Dual Decomposition</title><categories>cs.AI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose AD3, a new algorithm for approximate maximum a posteriori (MAP)
inference on factor graphs based on the alternating directions method of
multipliers. Like dual decomposition algorithms, AD3 uses worker nodes to
iteratively solve local subproblems and a controller node to combine these
local solutions into a global update. The key characteristic of AD3 is that
each local subproblem has a quadratic regularizer, leading to a faster
consensus than subgradient-based dual decomposition, both theoretically and in
practice. We provide closed-form solutions for these AD3 subproblems for binary
pairwise factors and factors imposing first-order logic constraints. For
arbitrary factors (large or combinatorial), we introduce an active set method
which requires only an oracle for computing a local MAP configuration, making
AD3 applicable to a wide range of problems. Experiments on synthetic and
realworld problems show that AD3 compares favorably with the state-of-the-art.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6551</identifier>
 <datestamp>2013-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6551</id><created>2012-12-28</created><authors><author><keyname>Gortler</keyname><forenames>Steven J.</forenames></author><author><keyname>Thurston</keyname><forenames>Dylan P.</forenames></author></authors><title>Measurement Isomorphism of Graphs</title><categories>math.MG cs.DM math.CO</categories><msc-class>52C25, 05C60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The d-measurement set of a graph is its set of possible squared edge lengths
over all d-dimensional embeddings. In this note, we define a new notion of
graph isomorphism called d-measurement isomorphism. Two graphs are
d-measurement isomorphic if there is agreement in their d-measurement sets. A
natural question to ask is &quot;what can be said about two graphs that are
d-measurement isomorphic?&quot; In this note, we show that this property coincides
with the 2-isomorphism property studied by Whitney.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6556</identifier>
 <datestamp>2015-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6556</id><created>2012-12-28</created><updated>2015-03-18</updated><authors><author><keyname>Chatterjee</keyname><forenames>Krishnendu</forenames></author><author><keyname>Prabhu</keyname><forenames>Vinayak S.</forenames></author></authors><title>Quantitative Timed Simulation Functions and Refinement Metrics for Timed
  Systems (Full Version)</title><categories>cs.SY cs.GT</categories><comments>Added some corrections from the Journal version</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce quantatitive timed refinement and timed simulation (directed)
metrics, incorporating zenoness check s, for timed systems. These metrics
assign positive real numbers between zero and infinity which quantify the
\emph{timing mismatches} between two timed systems, amongst non-zeno runs. We
quantify timing mismatches in three ways: (1) the maximal timing mismatch that
can arise, (2) the &quot;steady-state&quot; maximal timing mismatches, where initial
transient timing mismatches are ignored; and (3) the (long-run) average timing
mismatches amongst two systems. These three kinds of mismatches constitute
three important types of timing differences. Our event times are the
\emph{global times}, measured from the start of the system execution, not just
the time durations of individual steps. We present algorithms over timed
automata for computing the three quantitative simulation distances to within
any desired degree of accuracy. In order to compute the values of the
quantitative simulation distances, we use a game theoretic formulation. We
introduce two new kinds of objectives for two player games on finite-state game
graphs: (1) eventual debit-sum level objectives, and (2) average debit-sum
level objectives. We present algorithms for computing the optimal values for
these objectives in graph games, and then use these algorithms to compute the
values of the timed simulation distances over timed automata.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6567</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6567</id><created>2012-12-28</created><updated>2013-03-18</updated><authors><author><keyname>Grohe</keyname><forenames>Martin</forenames><affiliation>RWTH Aachen University, Germany</affiliation></author><author><keyname>Gru&#xdf;ien</keyname><forenames>Berit</forenames><affiliation>Humboldt-Universit&#xe4;t zu Berlin</affiliation></author><author><keyname>Hernich</keyname><forenames>Andr&#xe9;</forenames><affiliation>Humboldt-Universit&#xe4;t zu Berlin</affiliation></author><author><keyname>Laubner</keyname><forenames>Bastian</forenames><affiliation>Humboldt-Universit&#xe4;t zu Berlin</affiliation></author></authors><title>L-Recursion and a new Logic for Logarithmic Space</title><categories>cs.LO cs.CC</categories><comments>44 pages, 10 figures. A preliminary version of this article appeared
  in the Proceedings of the 25th International Workshop on Computer Science
  Logic (CSL '11)</comments><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 9, Issue 1 (March 13,
  2013) lmcs:938</journal-ref><doi>10.2168/LMCS-9(1:11)2013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We extend first-order logic with counting by a new operator that allows it to
formalise a limited form of recursion which can be evaluated in logarithmic
space. The resulting logic LREC has a data complexity in LOGSPACE, and it
defines LOGSPACE-complete problems like deterministic reachability and Boolean
formula evaluation. We prove that LREC is strictly more expressive than
deterministic transitive closure logic with counting and incomparable in
expressive power with symmetric transitive closure logic STC and transitive
closure logic (with or without counting). LREC is strictly contained in
fixed-point logic with counting FPC. We also study an extension LREC= of LREC
that has nicer closure properties and is more expressive than both LREC and
STC, but is still contained in FPC and has a data complexity in LOGSPACE. Our
main results are that LREC captures LOGSPACE on the class of directed trees and
that LREC= captures LOGSPACE on the class of interval graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6574</identifier>
 <datestamp>2013-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6574</id><created>2012-12-28</created><authors><author><keyname>&#xd6;lveczky</keyname><forenames>Peter Csaba</forenames><affiliation>University of Oslo</affiliation></author><author><keyname>Artho</keyname><forenames>Cyrille</forenames><affiliation>AIST</affiliation></author></authors><title>Proceedings First International Workshop on Formal Techniques for
  Safety-Critical Systems</title><categories>cs.LO cs.SE cs.SY</categories><proxy>EPTCS</proxy><acm-class>D.2.4</acm-class><journal-ref>EPTCS 105, 2012</journal-ref><doi>10.4204/EPTCS.105</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This volume contains the proceedings of the First International Workshop of
Formal Techniques for Safety-Critical Systems (FTSCS 2012), held in Kyoto on
November 12, 2012, as a satellite event of the ICFEM conference.
  The aim of this workshop is to bring together researchers and engineers
interested in the application of (semi-)formal methods to improve the quality
of safety-critical computer systems. FTSCS is particularly interested in
industrial applications of formal methods. Topics include:
  - the use of formal methods for safety-critical and QoS-critical systems,
including avionics, automotive, and medical systems; - methods, techniques and
tools to support automated analysis, certification, debugging, etc.; - analysis
methods that address the limitations of formal methods in industry; - formal
analysis support for modeling languages used in industry, such as AADL,
Ptolemy, SysML, SCADE, Modelica, etc.; and - code generation from validated
models.
  The workshop received 25 submissions; 21 of these were regular papers and 4
were tool/work-in-progress/position papers. Each submission was reviewed by
three referees; based on the reviews and extensive discussions, the program
committee selected nine regular papers, which are included in this volume. Our
program also included an invited talk by Ralf Huuck.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6576</identifier>
 <datestamp>2014-09-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6576</id><created>2012-12-28</created><updated>2014-09-06</updated><authors><author><keyname>Lewitzka</keyname><forenames>Steffen</forenames></author></authors><title>Denotational semantics for modal systems S3--S5 extended by axioms for
  propositional quantifiers and identity</title><categories>cs.LO</categories><comments>32 pages. This version of the article has been accepted for
  publication in STUDIA LOGICA. The final publication is available at Springer
  via http://dx.doi.org/10.1007/s11225-014-9577-9</comments><msc-class>03B45</msc-class><doi>10.1007/s11225-014-9577-9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are logics where necessity is defined by means of a given identity
connective: $\square\varphi := \varphi\equiv\top$ ($\top$ is a tautology). On
the other hand, in many standard modal logics the concept of propositional
identity (PI) $\varphi\equiv\psi$ can be defined by strict equivalence (SE)
$\square(\varphi\leftrightarrow\psi)$. All these approaches to modality involve
a principle that we call the Collapse Axiom (CA): &quot;There is only one necessary
proposition.&quot; In this paper, we consider a notion of PI which relies on the
identity axioms of Suszko's non-Fregean logic $\mathit{SCI}$. Then $S3$ proves
to be the smallest Lewis modal system where PI can be defined as SE. We extend
$S3$ to a non-Fregean logic with propositional quantifiers such that necessity
and PI are integrated as non-interdefinable concepts. CA is not valid and PI
refines SE. Models are expansions of $\mathit{SCI}$-models. We show that
$\mathit{SCI}$-models are Boolean prealgebras, and vice-versa. This associates
Non-Fregean Logic with research on Hyperintensional Semantics. PI equals SE iff
models are Boolean algebras and CA holds. A representation result establishes a
connection to Fine's approach to propositional quantifiers and shows that our
theories are \textit{conservative} extensions of $S3$--$S5$, respectively. If
we exclude the Barcan formula and a related axiom, then the resulting systems
are still complete w.r.t. a simpler denotational semantics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6582</identifier>
 <datestamp>2015-05-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6582</id><created>2012-12-28</created><updated>2015-05-10</updated><authors><author><keyname>Pedarsani</keyname><forenames>Ramtin</forenames></author><author><keyname>Walrand</keyname><forenames>Jean</forenames></author></authors><title>Stability of Multiclass Queueing Networks under Longest-Queue and
  Longest-Dominating-Queue Scheduling</title><categories>math.PR cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the stability of robust scheduling policies for multiclass
queueing networks. These are open networks with arbitrary routing matrix and
several disjoint groups of queues in which at most one queue can be served at a
time. The arrival and potential service processes and routing decisions at the
queues are independent, stationary and ergodic. A scheduling policy is called
robust if it does not depend on the arrival and service rates nor on the
routing probabilities. A policy is called throughput-optimal if it makes the
system stable whenever the parameters are such that the system can be stable.
We propose two robust polices: longest-queue scheduling and a new policy called
longest-dominating-queue scheduling. We show that longest-queue scheduling is
throughput-optimal for two groups of two queues. We also prove the
throughput-optimality of longest-dominating-queue scheduling when the network
topology is acyclic, for an arbitrary number of groups and queues.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6592</identifier>
 <datestamp>2013-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6592</id><created>2012-12-29</created><authors><author><keyname>Rhim</keyname><forenames>Joong Bum</forenames></author><author><keyname>Goyal</keyname><forenames>Vivek K</forenames></author></authors><title>Social Teaching: Being Informative vs. Being Right in Sequential
  Decision Making</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that it can be suboptimal for Bayesian decision-making agents
employing social learning to use correct prior probabilities as their initial
beliefs. We consider sequential Bayesian binary hypothesis testing where each
individual agent makes a binary decision based on an initial belief, a private
signal, and the decisions of all earlier-acting agents---with the actions of
precedent agents causing updates of the initial belief. Each agent acts to
minimize Bayes risk, with all agents sharing the same Bayes costs for Type I
(false alarm) and Type II (missed detection) errors. The effect of the set of
initial beliefs on the decision-making performance of the last agent is
studied. The last agent makes the best decision when the initial beliefs are
inaccurate. When the private signals are described by Gaussian likelihoods, the
optimal initial beliefs are not haphazard but rather follow a systematic
pattern: the earlier-acting agents should act as if the prior probability is
larger than it is in reality when the true prior probability is small, and vice
versa. We interpret this as being open minded toward the unlikely hypothesis.
The early-acting agents face a trade-off between making a correct decision and
being maximally informative to the later-acting agents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6602</identifier>
 <datestamp>2013-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6602</id><created>2012-12-29</created><authors><author><keyname>Zhang</keyname><forenames>Haizhang</forenames></author></authors><title>Multidimensional Analytic Signals and the Bedrosian Identity</title><categories>cs.IT math.CA math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The analytic signal method via the Hilbert transform is a key tool in signal
analysis and processing, especially in the time-frquency analysis. Imaging and
other applications to multidimensional signals call for extension of the method
to higher dimensions. We justify the usage of partial Hilbert transforms to
define multidimensional analytic signals from both engineering and mathematical
perspectives. The important associated Bedrosian identity $T(fg)=fTg$ for
partial Hilbert transforms $T$ are then studied. Characterizations and several
necessity theorems are established. We also make use of the identity to
construct basis functions for the time-frequency analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6607</identifier>
 <datestamp>2013-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6607</id><created>2012-12-29</created><authors><author><keyname>Zhang</keyname><forenames>Jinjin</forenames></author><author><keyname>Zhu</keyname><forenames>Zhaohui</forenames></author><author><keyname>Yang</keyname><forenames>Jianfei</forenames></author></authors><title>A control strategy algorithm for finite alternating transition systems</title><categories>math.OC cs.LO</categories><comments>21 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, there has been an increasing interest in the formal analysis and
design of control systems. In this area, in order to reduce the complexity and
scale of control systems, finite abstractions of control systems are introduced
and explored. Amongst, Pola and Tabuada construct finite alternating transition
systems as approximate finite abstractions for control systems with disturbance
inputs [SIAM Journal on Control and Optimization, Vol. 48, 2009, 719-733].
Given linear temporal logical formulas as specifications, this paper provides a
control strategy algorithm to find control strategies of Pola and Tabuada's
abstractions enforcing specifications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6609</identifier>
 <datestamp>2013-10-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6609</id><created>2012-12-29</created><updated>2013-01-15</updated><authors><author><keyname>Holub</keyname><forenames>&#x160;t&#x11b;p&#xe1;n</forenames></author></authors><title>On an algorithm for multiperiodic words</title><categories>cs.FL math.CO</categories><comments>changes in the introduction, missing references added</comments><msc-class>68R15</msc-class><journal-ref>Acta Polytechnica 53(4) (2013) 344-346</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider an algorithm by Tijdeman and Zamboni constructing a word of a
given length that has a given set of periods, and the richest possible
alphabet. We show that this algorithm can be easily stated and its correctness
briefly proved using the class equivalence approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6626</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6626</id><created>2012-12-29</created><authors><author><keyname>de Lamare</keyname><forenames>Rodrigo C.</forenames></author><author><keyname>Diniz</keyname><forenames>Paulo S. R.</forenames></author></authors><title>Blind Adaptive Interference Suppression Based on Set-Membership
  Constrained Constant-Modulus Algorithms with Time-Varying Bounds</title><categories>cs.IT math.IT</categories><comments>10 figures, IEEE Transactions on Signal Processing, 2012</comments><doi>10.1109/TSP.2012.2229995</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work presents blind constrained constant modulus (CCM) adaptive
algorithms based on the set-membership filtering (SMF) concept and incorporates
dynamic bounds {for interference suppression} applications. We develop
stochastic gradient and recursive least squares type algorithms based on the
CCM design criterion in accordance with the specifications of the SMF concept.
We also propose a blind framework that includes channel and amplitude
estimators that take into account parameter estimation dependency, multiple
access interference (MAI) and inter-symbol interference (ISI) to address the
important issue of bound specification in multiuser communications. A
convergence and tracking analysis of the proposed algorithms is carried out
along with the development of analytical expressions to predict their
performance. Simulations for a number of scenarios of interest with a DS-CDMA
system show that the proposed algorithms outperform previously reported
techniques with a smaller number of parameter updates and a reduced risk of
overbounding or underbounding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6627</identifier>
 <datestamp>2013-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6627</id><created>2012-12-29</created><authors><author><keyname>Shen</keyname><forenames>Yulong</forenames></author><author><keyname>Jiang</keyname><forenames>Xiaohong</forenames></author><author><keyname>Ma</keyname><forenames>Jianfeng</forenames></author></authors><title>Exploring Relay Cooperation Scheme for Load-Balance Control in Two-hop
  Secure Communication System</title><categories>cs.IT cs.CR cs.NI math.IT</categories><comments>5 pages, 1 figure. arXiv admin note: substantial text overlap with
  arXiv:1212.0287, arXiv:1211.7075</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work considers load-balance control among the relays under the secure
transmission protocol via relay cooperation in two-hop wireless networks
without the information of both eavesdropper channels and locations. The
available two-hop secure transmission protocols in physical layer secrecy
framework cannot provide a flexible load-balance control, which may
significantly limit their application scopes. This paper proposes a secure
transmission protocol in case that the path-loss is identical between all pairs
of nodes, in which the relay is randomly selected from the first $k$ preferable
assistant relays. This protocol enables load-balance among relays to be
flexibly controlled by a proper setting of the parameter $k$, and covers the
available works as special cases, like ones with the optimal relay selection
($k=1$) and ones with the random relay selection ($k = n$, i.e. the number of
system nodes). The theoretic analysis is further provided to determine the
maximum number of eavesdroppers one network can tolerate by applying the
proposed protocol to ensure a desired performance in terms of the secrecy
outage probability and transmission outage probability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6632</identifier>
 <datestamp>2013-04-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6632</id><created>2012-12-29</created><updated>2013-04-24</updated><authors><author><keyname>Velner</keyname><forenames>Yaron</forenames></author></authors><title>The Complexity of Infinitely Repeated Alternating Move Games</title><categories>cs.GT</categories><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  We consider infinite duration alternating move games. These games were
previously studied by Roth, Balcan, Kalai and Mansour. They presented an FPTAS
for computing an approximated equilibrium, and conjectured that there is a
polynomial algorithm for finding an exact equilibrium. We extend their study in
two directions: (1) We show that finding an exact equilibrium, even for
two-player zero-sum games, is polynomial time equivalent to finding a winning
strategy for a (two-player) mean-payoff game on graphs. The existence of a
polynomial algorithm for the latter is a long standing open question in
computer science. Our hardness result for two-player games suggests that
two-player alternating move games are harder to solve than two-player
simultaneous move games, while the work of Roth et al., suggests that for
$k\geq 3$, $k$-player games are easier to analyze in the alternating move
setting. (2) We show that optimal equilibriums (with respect to the social
welfare metric) can be obtained by pure strategies, and we present an FPTAS for
computing a pure approximated equilibrium that is $\delta$-optimal with respect
to the social welfare metric. This result extends the previous work by
presenting an FPTAS that finds a much more desirable approximated equilibrium.
We also show that if there is a polynomial algorithm for mean-payoff games on
graphs, then there is a polynomial algorithm that computes an optimal exact
equilibrium, and hence, (two-player) mean-payoff games on graphs are
inter-reducible with $k$-player alternating move games, for any $k\geq 2$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6636</identifier>
 <datestamp>2014-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6636</id><created>2012-12-29</created><updated>2014-01-15</updated><authors><author><keyname>Koutris</keyname><forenames>Paraschos</forenames></author><author><keyname>Suciu</keyname><forenames>Dan</forenames></author></authors><title>A Dichotomy on the Complexity of Consistent Query Answering for Atoms
  with Simple Keys</title><categories>cs.DB</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of consistent query answering under primary key
violations. In this setting, the relations in a database violate the key
constraints and we are interested in maximal subsets of the database that
satisfy the constraints, which we call repairs. For a boolean query Q, the
problem CERTAINTY(Q) asks whether every such repair satisfies the query or not;
the problem is known to be always in coNP for conjunctive queries. However,
there are queries for which it can be solved in polynomial time. It has been
conjectured that there exists a dichotomy on the complexity of CERTAINTY(Q) for
conjunctive queries: it is either in PTIME or coNP-complete. In this paper, we
prove that the conjecture is indeed true for the case of conjunctive queries
without self-joins, where each atom has as a key either a single attribute
(simple key) or all attributes of the atom.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6640</identifier>
 <datestamp>2013-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6640</id><created>2012-12-29</created><authors><author><keyname>Nikolaev</keyname><forenames>Andrey</forenames></author></authors><title>Exploring mutexes, the Oracle RDBMS retrial spinlocks</title><categories>cs.DB cs.DC cs.PF</categories><comments>Proceedings of International Conference on Informatics MEDIAS2012.
  Cyprus, Limassol, May 7--14, 2012. ISBN 978-5-88835-023-2. 12 pages, 15
  figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Spinlocks are widely used in database engines for processes synchronization.
KGX mutexes is new retrial spinlocks appeared in contemporary Oracle versions
for submicrosecond synchronization. The mutex contention is frequently observed
in highly concurrent OLTP environments.
  This work explores how Oracle mutexes operate, spin, and sleep. It develops
predictive mathematical model and discusses parameters and statistics related
to mutex performance tuning, as well as results of contention experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6641</identifier>
 <datestamp>2014-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6641</id><created>2012-12-29</created><updated>2014-06-02</updated><authors><author><keyname>Boldo</keyname><forenames>Sylvie</forenames><affiliation>LRI, INRIA Saclay - &#xce;le-de-France</affiliation></author><author><keyname>Cl&#xe9;ment</keyname><forenames>Fran&#xe7;ois</forenames><affiliation>Inria Paris-Rocquencourt</affiliation></author><author><keyname>Filli&#xe2;tre</keyname><forenames>Jean-Christophe</forenames><affiliation>LRI, INRIA Saclay - &#xce;le-de-France</affiliation></author><author><keyname>Mayero</keyname><forenames>Micaela</forenames><affiliation>LIPN</affiliation></author><author><keyname>Melquiond</keyname><forenames>Guillaume</forenames><affiliation>LRI, INRIA Saclay - &#xce;le-de-France</affiliation></author><author><keyname>Weis</keyname><forenames>Pierre</forenames><affiliation>Inria Paris-Rocquencourt</affiliation></author></authors><title>Trusting Computations: a Mechanized Proof from Partial Differential
  Equations to Actual Program</title><categories>math.NA cs.LO</categories><comments>N&amp;deg; RR-8197 (2012). arXiv admin note: text overlap with
  arXiv:1112.1795</comments><proxy>ccsd</proxy><report-no>RR-8197</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Computer programs may go wrong due to exceptional behaviors, out-of-bound
array accesses, or simply coding errors. Thus, they cannot be blindly trusted.
Scientific computing programs make no exception in that respect, and even bring
specific accuracy issues due to their massive use of floating-point
computations. Yet, it is uncommon to guarantee their correctness. Indeed, we
had to extend existing methods and tools for proving the correct behavior of
programs to verify an existing numerical analysis program. This C program
implements the second-order centered finite difference explicit scheme for
solving the 1D wave equation. In fact, we have gone much further as we have
mechanically verified the convergence of the numerical scheme in order to get a
complete formal proof covering all aspects from partial differential equations
to actual numerical results. To the best of our knowledge, this is the first
time such a comprehensive proof is achieved.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6643</identifier>
 <datestamp>2014-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6643</id><created>2012-12-29</created><updated>2014-01-18</updated><authors><author><keyname>Stavrou</keyname><forenames>Photios A.</forenames></author><author><keyname>Charalambous</keyname><forenames>Charalambos D.</forenames></author></authors><title>Nonanticipative Rate Distortion Function and Filtering Theory: A weak
  Convergence Approach</title><categories>cs.IT cs.SY math.IT</categories><comments>26 pages and 4 figures, part of this work is published in European
  Control Conference (ECC' 13)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper the relation between nonanticipative rate distortion function
(RDF) and Bayesian filtering theory is further investigated on general Polish
spaces. The relation is established via an optimization on the space of
conditional distributions of the so-called directed information subject to
fidelity constraints. Existence of the optimal reproduction distribution of the
nonanticipative RDF is shown using the topology of weak convergence of
probability measures. Subsequently, we use the solution of the nonanticipative
RDF to present the realization of a multidimensional partially observable
source over a scalar Gaussian channel. We show that linear encoders are
optimal, establishing joint source-channel coding in real-time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6646</identifier>
 <datestamp>2013-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6646</id><created>2012-12-29</created><authors><author><keyname>de Lamare</keyname><forenames>Rodrigo C.</forenames></author><author><keyname>Sampaio-Neto</keyname><forenames>Raimundo</forenames></author></authors><title>Blind Adaptive MIMO Receivers for Space-Time Block-Coded DS-CDMA Systems
  in Multipath Channels Using the Constant Modulus Criterion</title><categories>cs.IT math.IT</categories><comments>4 figures, IEEE Transactions on Communications, 2010</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose blind adaptive multi-input multi-output (MIMO) linear receivers
for DS-CDMA systems using multiple transmit antennas and space-time block codes
(STBC) in multipath channels. A space-time code-constrained constant modulus
(CCM) design criterion based on constrained optimization techniques is
considered and recursive least squares (RLS) adaptive algorithms are developed
for estimating the parameters of the linear receivers. A blind space-time
channel estimation method for MIMO DS-CDMA systems with STBC based on a
subspace approach is also proposed along with an efficient RLS algorithm.
Simulations for a downlink scenario assess the proposed algorithms in several
situations against existing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6659</identifier>
 <datestamp>2013-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6659</id><created>2012-12-29</created><authors><author><keyname>Pelossof</keyname><forenames>Raphael</forenames></author><author><keyname>Ying</keyname><forenames>Zhiliang</forenames></author></authors><title>Focus of Attention for Linear Predictors</title><categories>stat.ML cs.AI cs.LG</categories><comments>9 pages, 4 figures. arXiv admin note: substantial text overlap with
  arXiv:1105.0382</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a method to stop the evaluation of a prediction process when the
result of the full evaluation is obvious. This trait is highly desirable in
prediction tasks where a predictor evaluates all its features for every example
in large datasets. We observe that some examples are easier to classify than
others, a phenomenon which is characterized by the event when most of the
features agree on the class of an example. By stopping the feature evaluation
when encountering an easy- to-classify example, the predictor can achieve
substantial gains in computation. Our method provides a natural attention
mechanism for linear predictors where the predictor concentrates most of its
computation on hard-to-classify examples and quickly discards easy-to-classify
ones. By modifying a linear prediction algorithm such as an SVM or AdaBoost to
include our attentive method we prove that the average number of features
computed is O(sqrt(n log 1/sqrt(delta))) where n is the original number of
features, and delta is the error rate incurred due to early stopping. We
demonstrate the effectiveness of Attentive Prediction on MNIST, Real-sim,
Gisette, and synthetic datasets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6663</identifier>
 <datestamp>2013-10-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6663</id><created>2012-12-29</created><updated>2013-10-24</updated><authors><author><keyname>Lim</keyname><forenames>Lek-Heng</forenames></author><author><keyname>Comon</keyname><forenames>Pierre</forenames></author></authors><title>Blind Multilinear Identification</title><categories>cs.IT math.IT</categories><comments>20 pages, to appear in IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We discuss a technique that allows blind recovery of signals or blind
identification of mixtures in instances where such recovery or identification
were previously thought to be impossible: (i) closely located or highly
correlated sources in antenna array processing, (ii) highly correlated
spreading codes in CDMA radio communication, (iii) nearly dependent spectra in
fluorescent spectroscopy. This has important implications --- in the case of
antenna array processing, it allows for joint localization and extraction of
multiple sources from the measurement of a noisy mixture recorded on multiple
sensors in an entirely deterministic manner. In the case of CDMA, it allows the
possibility of having a number of users larger than the spreading gain. In the
case of fluorescent spectroscopy, it allows for detection of nearly identical
chemical constituents. The proposed technique involves the solution of a
bounded coherence low-rank multilinear approximation problem. We show that
bounded coherence allows us to establish existence and uniqueness of the
recovered solution. We will provide some statistical motivation for the
approximation problem and discuss greedy approximation bounds. To provide the
theoretical underpinnings for this technique, we develop a corresponding theory
of sparse separable decompositions of functions, including notions of rank and
nuclear norm that specialize to the usual ones for matrices and operators but
apply to also hypermatrices and tensors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6680</identifier>
 <datestamp>2015-06-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6680</id><created>2012-12-29</created><updated>2013-06-21</updated><authors><author><keyname>Bouwmeester</keyname><forenames>Henricus</forenames></author><author><keyname>Dougherty</keyname><forenames>Andrew</forenames></author><author><keyname>Knyazev</keyname><forenames>Andrew V.</forenames></author></authors><title>Nonsymmetric multigrid preconditioning for conjugate gradient methods</title><categories>cs.NA math.NA</categories><comments>7 pages</comments><report-no>TR2013-027</report-no><journal-ref>Procedia Computer Science, v. 51, pp. 276-285, 2015</journal-ref><doi>10.1016/j.procs.2015.05.241</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We numerically analyze the possibility of turning off post-smoothing
(relaxation) in geometric multigrid when used as a preconditioner in conjugate
gradient linear and eigenvalue solvers for the 3D Laplacian. The geometric
Semicoarsening Multigrid (SMG) method is provided by the hypre parallel
software package. We solve linear systems using two variants (standard and
flexible) of the preconditioned conjugate gradient (PCG) and preconditioned
steepest descent (PSD) methods. The eigenvalue problems are solved using the
locally optimal block preconditioned conjugate gradient (LOBPCG) method
available in hypre through BLOPEX software. We observe that turning off the
post-smoothing in SMG dramatically slows down the standard PCG-SMG. For
flexible PCG and LOBPCG, our numerical results show that post-smoothing can be
avoided, resulting in overall acceleration, due to the high costs of smoothing
and relatively insignificant decrease in convergence speed. We numerically
demonstrate for linear systems that PSD-SMG and flexible PCG-SMG converge
similarly if SMG post-smoothing is off. We experimentally show that the effect
of acceleration is independent of memory interconnection. A theoretical
justification is provided.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6686</identifier>
 <datestamp>2013-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6686</id><created>2012-12-29</created><authors><author><keyname>Xia</keyname><forenames>Xiaochen</forenames></author><author><keyname>Xu</keyname><forenames>Youyun</forenames></author><author><keyname>Xu</keyname><forenames>Kui</forenames></author><author><keyname>Zhang</keyname><forenames>Dongmei</forenames></author><author><keyname>Li</keyname><forenames>Ning</forenames></author></authors><title>Outage Performance of AF-based Time Division Broadcasting Protocol in
  the Presence of Co-channel Interference</title><categories>cs.IT math.IT</categories><comments>6pages, 4figures, accepted by WCNC 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we investigate the outage performance of time division
broadcasting (TDBC) protocol in independent but non-identical Rayleigh
flat-fading channels, where all nodes are interfered by a finite number of
co-channel interferers. We assume that the relay operates in the
amplified-and-forward mode. A tight lower bound as well as the asymptotic
expression of the outage probability is obtained in closed-form. Through both
theoretic analyses and simulation results, we show that the achievable
diversity of TDBC protocol is zero in the interference-limited scenario.
Moreover, we study the impacts of interference power, number of interferers and
relay placement on the outage probability. Finally, the correctness of our
analytic results is validated via computer simulations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6725</identifier>
 <datestamp>2013-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6725</id><created>2012-12-30</created><authors><author><keyname>Kobayashi</keyname><forenames>Koji</forenames></author></authors><title>Solvability of HornSAT and CNFSAT</title><categories>cs.CC</categories><comments>3 pages, English and Japanese (see Other formats - Source)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This article describes the solvability of HornSAT and CNFSAT.
  Unsatisfiable HornCNF have partially ordered set that is made by causation of
each clauses. In this partially ordered set, Truth value assignment that is
false in each clauses become simply connected space. Therefore, if we reduce
CNFSAT to HornSAT, we must make such partially ordered set in HornSAT. But
CNFSAT have correlations of each clauses, the partially ordered set is not in
polynomial size.
  Therefore, we cannot reduce CNFSAT to HornSAT in polynomial size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6734</identifier>
 <datestamp>2013-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6734</id><created>2012-12-30</created><updated>2013-06-11</updated><authors><author><keyname>Schwarz</keyname><forenames>Stefan</forenames></author><author><keyname>Ikuno</keyname><forenames>Josep Colom</forenames></author><author><keyname>&#x160;imko</keyname><forenames>Michal</forenames></author><author><keyname>Taranetz</keyname><forenames>Martin</forenames></author><author><keyname>Wang</keyname><forenames>Qi</forenames></author><author><keyname>Rupp</keyname><forenames>Markus</forenames></author></authors><title>Pushing the Limits of LTE: A Survey on Research Enhancing the Standard</title><categories>cs.IT math.IT</categories><comments>The final version of the manuscript is available at:
  http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=6514821&amp;isnumber=6336544</comments><journal-ref>Schwarz, S.; Ikuno, J.C.; Simko, M.; Taranetz, M.; Wang, Q.; Rupp,
  M., &quot;Pushing the Limits of LTE: A Survey on Research Enhancing the Standard,&quot;
  Access, IEEE , vol.1, no., pp.51,62, 2013</journal-ref><doi>10.1109/ACCESS.2013.2260371</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cellular networks are an essential part of todays communication
infrastructure. The ever-increasing demand for higher data-rates calls for a
close cooperation between researchers and industry/standardization experts
which hardly exists in practice. In this article we give an overview about our
efforts in trying to bridge this gap. Our research group provides a
standard-compliant open-source simulation platform for 3GPP LTE that enables
reproducible research in a well-defined environment. We demonstrate that much
innovative research under the confined framework of a real-world standard is
still possible, sometimes even encouraged. With examplary samples of our
research work we investigate on the potential of several important research
areas under typical practical conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6745</identifier>
 <datestamp>2015-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6745</id><created>2012-12-30</created><updated>2015-08-25</updated><authors><author><keyname>Zenil</keyname><forenames>Hector</forenames></author><author><keyname>Soler-Toscano</keyname><forenames>Fernando</forenames></author><author><keyname>Delahaye</keyname><forenames>Jean-Paul</forenames></author><author><keyname>Gauvrit</keyname><forenames>Nicolas</forenames></author></authors><title>Two-Dimensional Kolmogorov Complexity and Validation of the Coding
  Theorem Method by Compressibility</title><categories>cs.CC cs.IT math.IT</categories><comments>39 pages, 13 figures. Forthcoming in PeerJ Computer Science (this
  version is as it was accepted with minor changes to the figures enumeration,
  also a one-letter typo corrected)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a measure based upon the fundamental theoretical concept in
algorithmic information theory that provides a natural approach to the problem
of evaluating $n$-dimensional complexity by using an $n$-dimensional
deterministic Turing machine. The technique is interesting because it provides
a natural algorithmic process for symmetry breaking generating complex
$n$-dimensional structures from perfectly symmetric and fully deterministic
computational rules producing a distribution of patterns as described by
algorithmic probability. Algorithmic probability also elegantly connects the
frequency of occurrence of a pattern with its algorithmic complexity, hence
effectively providing estimations to the complexity of the generated patterns.
Experiments to validate estimations of algorithmic complexity based on these
concepts are presented, showing that the measure is stable in the face of some
changes in computational formalism and that results are in agreement with the
results obtained using lossless compression algorithms when both methods
overlap in their range of applicability. We then use the output frequency of
the set of 2-dimensional Turing machines to classify the algorithmic complexity
of the space-time evolutions of Elementary Cellular Automata.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6751</identifier>
 <datestamp>2013-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6751</id><created>2012-12-30</created><authors><author><keyname>Miller</keyname><forenames>Russell</forenames></author><author><keyname>Schoutens</keyname><forenames>Hans</forenames></author></authors><title>Computably Categorical Fields via Fermat's Last Theorem</title><categories>math.LO cs.LO</categories><comments>to appear in the journal Computability</comments><msc-class>03C57 (Primary), 03D45, 12L05 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We construct a computable, computably categorical field of infinite
transcendence degree over the rational numbers, using the Fermat polynomials
and assorted results from algebraic geometry. We also show that this field has
an intrinsically computable (infinite) transcendence basis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6773</identifier>
 <datestamp>2013-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6773</id><created>2012-12-30</created><authors><author><keyname>Mingers</keyname><forenames>John</forenames></author><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author></authors><title>Identifying Research Fields within Business and Management: A Journal
  Cross-Citation Analysis</title><categories>cs.DL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A discipline such as business and management (B&amp;M) is very broad and has many
fields within it, ranging from fairly scientific ones such as management
science or economics to softer ones such as information systems. There are at
least two reasons why it is important to identify these sub-fields accurately.
Firstly, for the purpose of normalizing citation data as it is well known that
citation rates vary significantly between different disciplines. Secondly,
because journal rankings and lists tend to split their classifications into
different subjects, for example the the Association of Business Schools (ABS)
list, which is a standard in the UK, has 22 different fields. Unfortunately, at
the moment these are created in an ad hoc manner with no underlying rigour. The
purpose of this paper is to identify possible sub-fields in B&amp;M rigorously
based on actual citation patterns. We have examined 450 journals in B&amp;M which
are included in the ISI Web of Science (WoS) and analysed the cross-citation
rates between them enabling us to generate sets of coherent and consistent
sub-fields that minimise the extent to which journals appear in several
categories. Implications and limitations of the analysis are discussed
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6781</identifier>
 <datestamp>2013-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6781</id><created>2012-12-30</created><authors><author><keyname>Dadush</keyname><forenames>Daniel</forenames></author><author><keyname>Kun</keyname><forenames>Gabor</forenames></author></authors><title>Lattice Sparsification and the Approximate Closest Vector Problem</title><categories>cs.DS cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a deterministic algorithm for solving the (1+eps)-approximate Closest
Vector Problem (CVP) on any n dimensional lattice and any norm in
2^{O(n)}(1+1/eps)^n time and 2^n poly(n) space. Our algorithm builds on the
lattice point enumeration techniques of Micciancio and Voulgaris (STOC 2010)
and Dadush, Peikert and Vempala (FOCS 2011), and gives an elegant,
deterministic alternative to the &quot;AKS Sieve&quot; based algorithms for (1+eps)-CVP
(Ajtai, Kumar, and Sivakumar; STOC 2001 and CCC 2002). Furthermore, assuming
the existence of a poly(n)-space and 2^{O(n)} time algorithm for exact CVP in
the l_2 norm, the space complexity of our algorithm can be reduced to
polynomial.
  Our main technical contribution is a method for &quot;sparsifying&quot; any input
lattice while approximately maintaining its metric structure. To this end, we
employ the idea of random sublattice restrictions, which was first employed by
Khot (FOCS 2003) for the purpose of proving hardness for Shortest Vector
Problem (SVP) under l_p norms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6806</identifier>
 <datestamp>2013-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6806</id><created>2012-12-30</created><authors><author><keyname>Colbaugh</keyname><forenames>Richard</forenames></author><author><keyname>Glass</keyname><forenames>Kristin</forenames></author><author><keyname>Bauer</keyname><forenames>Travis</forenames></author></authors><title>Leveraging Sociological Models for Predictive Analytics</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is considerable interest in developing techniques for predicting human
behavior, for instance to enable emerging contentious situations to be forecast
or the nature of ongoing but hidden activities to be inferred. A promising
approach to this problem is to identify and collect appropriate empirical data
and then apply machine learning methods to these data to generate the
predictions. This paper shows the performance of such learning algorithms often
can be improved substantially by leveraging sociological models in their
development and implementation. In particular, we demonstrate that
sociologically-grounded learning algorithms outperform gold-standard methods in
three important and challenging tasks: 1.) inferring the (unobserved) nature of
relationships in adversarial social networks, 2.) predicting whether nascent
social diffusion events will go viral, and 3.) anticipating and defending
future actions of opponents in adversarial settings. Significantly, the new
algorithms perform well even when there is limited data available for their
training and execution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6808</identifier>
 <datestamp>2013-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6808</id><created>2012-12-30</created><authors><author><keyname>Colbaugh</keyname><forenames>Richard</forenames></author><author><keyname>Glass</keyname><forenames>Kristin</forenames></author></authors><title>Early Warning Analysis for Social Diffusion Events</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There is considerable interest in developing predictive capabilities for
social diffusion processes, for instance to permit early identification of
emerging contentious situations, rapid detection of disease outbreaks, or
accurate forecasting of the ultimate reach of potentially viral ideas or
behaviors. This paper proposes a new approach to this predictive analytics
problem, in which analysis of meso-scale network dynamics is leveraged to
generate useful predictions for complex social phenomena. We begin by deriving
a stochastic hybrid dynamical systems (S-HDS) model for diffusion processes
taking place over social networks with realistic topologies; this modeling
approach is inspired by recent work in biology demonstrating that S-HDS offer a
useful mathematical formalism with which to represent complex, multi-scale
biological network dynamics. We then perform formal stochastic reachability
analysis with this S-HDS model and conclude that the outcomes of social
diffusion processes may depend crucially upon the way the early dynamics of the
process interacts with the underlying network's community structure and
core-periphery structure. This theoretical finding provides the foundations for
developing a machine learning algorithm that enables accurate early warning
analysis for social diffusion events. The utility of the warning algorithm, and
the power of network-based predictive metrics, are demonstrated through an
empirical investigation of the propagation of political memes over social media
networks. Additionally, we illustrate the potential of the approach for
security informatics applications through case studies involving early warning
analysis of large-scale protests events and politically-motivated cyber
attacks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6810</identifier>
 <datestamp>2013-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6810</id><created>2012-12-30</created><authors><author><keyname>Glass</keyname><forenames>Kristin</forenames></author><author><keyname>Colbaugh</keyname><forenames>Richard</forenames></author></authors><title>Web Analytics for Security Informatics</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An enormous volume of security-relevant information is present on the Web,
for instance in the content produced each day by millions of bloggers
worldwide, but discovering and making sense of these data is very challenging.
This paper considers the problem of exploring and analyzing the Web to realize
three fundamental objectives: 1.) security relevant information discovery; 2.)
target situational awareness, typically by making (near) real-time inferences
concerning events and activities from available observations; and 3.)
predictive analysis, to include providing early warning for crises and forming
predictions regarding likely outcomes of emerging issues and contemplated
interventions. The proposed approach involves collecting and integrating three
types of Web data, textual, relational, and temporal, to perform assessments
and generate insights that would be difficult or impossible to obtain using
standard methods. We demonstrate the efficacy of the framework by summarizing a
number of successful real-world deployments of the methodology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6813</identifier>
 <datestamp>2013-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6813</id><created>2012-12-30</created><authors><author><keyname>Zhu</keyname><forenames>Zhaohui</forenames></author><author><keyname>Zhang</keyname><forenames>Yan</forenames></author><author><keyname>Zhang</keyname><forenames>Jinjin</forenames></author></authors><title>Merging Process Algebra and Action-based Computation Tree Logic</title><categories>cs.LO cs.SE</categories><comments>64 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Process algebra and temporal logic are two popular paradigms for the
specification, verification and systematic development of reactive and
concurrent systems. These two approaches take different standpoint for looking
at specifications and verifications, and offer complementary advantages. In
order to mix algebraic and logic styles of specification in a uniform
framework, the notion of a logic labelled transition system (LLTS) has been
presented and explored by Luttgen and Vogler. This paper intends to propose a
LLTS-oriented process calculus which, in addition to usual process-algebraic
operators, involves logic connectives (conjunction and disjunction) and
standard temporal operators (always and unless). This calculus preserves usual
properties of these logic operators, allows one to freely mix operational and
logic operators, and supports compositional reasoning. Moreover, the links
between this calculus and Action-based Computation Tree Logic (ACTL) including
characteristic formulae of process terms, characteristic processes of ACTL
formulae and Galois connection are explored.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6817</identifier>
 <datestamp>2013-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6817</id><created>2012-12-30</created><authors><author><keyname>Acharya</keyname><forenames>Anish</forenames></author><author><keyname>Mitra</keyname><forenames>Debatri</forenames></author><author><keyname>Halder</keyname><forenames>Kaushik</forenames></author></authors><title>Stability Analysis Of Delayed System Using Bodes Integral</title><categories>cs.SY</categories><comments>5 pages, 5 figures, 2013 International Conference on Computer
  Communication and Informatics (ICCCI-2013), Jan 2013, Coimbatore, INDIA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The PID controller parameters can be adjusted in such a manner that it gives
the desired frequency response and the results are found using the Bodes
integral formula in order to adjust the slope of the nyquist curve in a desired
manner. The same idea is applied for plants with time delay . The same has also
been done in a new approach . The delay term is approximated as a transfer
function using Pade approximation and then the Bode integral is used to
determine the controller parameters. Both the methodologies are demonstrated
with MATLAB simulation of representative plants and accompanying PID
controllers. A proper comparison of the two methodologies is also done. The PID
controller parameters are also tuned using a real coded Genetic Algorithm (GA)
and a proper comparison is done between the three methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6831</identifier>
 <datestamp>2013-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6831</id><created>2012-12-31</created><authors><author><keyname>Xiao</keyname><forenames>Mingyu</forenames></author><author><keyname>Nagamochi</keyname><forenames>Hiroshi</forenames></author></authors><title>An Exact Algorithm for TSP in Degree-3 Graphs via Circuit Procedure and
  Amortization on Connectivity Structure</title><categories>cs.DS</categories><comments>24 pages and 4 figures</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The paper presents an O^*(1.2312^n)-time and polynomial-space algorithm for
the traveling salesman problem in an n-vertex graph with maximum degree 3. This
improves the previous time bounds of O^*(1.251^n) by Iwama and Nakashima and
O^*(1.260^n) by Eppstein. Our algorithm is a simple branch-and-search
algorithm. The only branch rule is designed on a cut-circuit structure of a
graph induced by unprocessed edges. To improve a time bound by a simple
analysis on measure and conquer, we introduce an amortization scheme over the
cut-circuit structure by defining the measure of an instance to be the sum of
not only weights of vertices but also weights of connected components of the
induced graph.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6837</identifier>
 <datestamp>2013-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6837</id><created>2012-12-31</created><authors><author><keyname>Nguyen</keyname><forenames>Hai</forenames></author><author><keyname>Kemp</keyname><forenames>Charles C.</forenames></author></authors><title>Autonomously Learning to Visually Detect Where Manipulation Will Succeed</title><categories>cs.RO cs.AI cs.CV</categories><comments>15 pages, 10 figures. Submitted to the Autonomous Robots Journal
  Special Issue &quot;Beyond Grasping - Modern Approaches for Dexterous
  Manipulation&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Visual features can help predict if a manipulation behavior will succeed at a
given location. For example, the success of a behavior that flips light
switches depends on the location of the switch. Within this paper, we present
methods that enable a mobile manipulator to autonomously learn a function that
takes an RGB image and a registered 3D point cloud as input and returns a 3D
location at which a manipulation behavior is likely to succeed. Given a pair of
manipulation behaviors that can change the state of the world between two sets
(e.g., light switch up and light switch down), classifiers that detect when
each behavior has been successful, and an initial hint as to where one of the
behaviors will be successful, the robot autonomously trains a pair of support
vector machine (SVM) classifiers by trying out the behaviors at locations in
the world and observing the results. When an image feature vector associated
with a 3D location is provided as input to one of the SVMs, the SVM predicts if
the associated manipulation behavior will be successful at the 3D location. To
evaluate our approach, we performed experiments with a PR2 robot from Willow
Garage in a simulated home using behaviors that flip a light switch, push a
rocker-type light switch, and operate a drawer. By using active learning, the
robot efficiently learned SVMs that enabled it to consistently succeed at these
tasks. After training, the robot also continued to learn in order to adapt in
the event of failure.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6844</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6844</id><created>2012-12-31</created><authors><author><keyname>Kwon</keyname><forenames>Keehang</forenames></author><author><keyname>Hur</keyname><forenames>Sungwoo</forenames></author><author><keyname>Park</keyname><forenames>Mi-Young</forenames></author></authors><title>Improving Robustness via Disjunctive Statements in Imperative
  Programming</title><categories>cs.PL</categories><journal-ref>IEICE transaction on information and system, vol.E96-D, no. 9,
  Sep, 2013</journal-ref><doi>10.1587/transinf.E96.D.2036</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  To deal with failures as simply as possible, we propose a new foun- dation
for the core (untyped) C, which is based on a new logic called task logic or
imperative logic. We then introduce a sequential-disjunctive statement of the
form S : R. This statement has the following semantics: execute S and R
sequentially. It is considered a success if at least one of S;R is a success.
This statement is useful for dealing with inessential errors without explicitly
catching them.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6846</identifier>
 <datestamp>2013-01-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6846</id><created>2012-12-31</created><updated>2013-01-10</updated><authors><author><keyname>Kale</keyname><forenames>Sagar</forenames></author></authors><title>Maximizing a Nonnegative, Monotone, Submodular Function Constrained to
  Matchings</title><categories>cs.DS cs.AI cs.CC cs.LG stat.ML</categories><comments>Withdrawn because the main result is implied by a more general result
  about p-independence-system (which generalize matchings) in the paper by
  Calinescu, Chekuri, Pal, and Vondrak, Maximizing a Monotone Submodular
  Function Subject to a Matroid Constraint, SIAM J. Comput., 2011, Vol 40, No
  6, pp. 1740-1766</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Submodular functions have many applications. Matchings have many
applications. The bitext word alignment problem can be modeled as the problem
of maximizing a nonnegative, monotone, submodular function constrained to
matchings in a complete bipartite graph where each vertex corresponds to a word
in the two input sentences and each edge represents a potential word-to-word
translation. We propose a more general problem of maximizing a nonnegative,
monotone, submodular function defined on the edge set of a complete graph
constrained to matchings; we call this problem the CSM-Matching problem.
CSM-Matching also generalizes the maximum-weight matching problem, which has a
polynomial-time algorithm; however, we show that it is NP-hard to approximate
CSM-Matching within a factor of e/(e-1) by reducing the max k-cover problem to
it. Our main result is a simple, greedy, 3-approximation algorithm for
CSM-Matching. Then we reduce CSM-Matching to maximizing a nonnegative,
monotone, submodular function over two matroids, i.e., CSM-2-Matroids.
CSM-2-Matroids has a (2+epsilon)-approximation algorithm - called LSV2. We show
that we can find a (4+epsilon)-approximate solution to CSM-Matching using LSV2.
We extend this approach to similar problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6848</identifier>
 <datestamp>2013-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6848</id><created>2012-12-31</created><updated>2013-04-22</updated><authors><author><keyname>Crowston</keyname><forenames>R.</forenames></author><author><keyname>Gutin</keyname><forenames>G.</forenames></author><author><keyname>Jones</keyname><forenames>M.</forenames></author><author><keyname>Muciaccia</keyname><forenames>G.</forenames></author></authors><title>Maximum Balanced Subgraph Problem Parameterized Above Lower Bound</title><categories>cs.DS cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider graphs without loops or parallel edges in which every edge is
assigned + or -. Such a signed graph is balanced if its vertex set can be
partitioned into parts $V_1$ and $V_2$ such that all edges between vertices in
the same part have sign + and all edges between vertices of different parts
have sign $-$ (one of the parts may be empty). It is well-known that every
connected signed graph with $n$ vertices and $m$ edges has a balanced subgraph
with at least $\frac{m}{2} + \frac{n-1}{4}$ edges and this bound is tight. We
consider the following parameterized problem: given a connected signed graph
$G$ with $n$ vertices and $m$ edges, decide whether $G$ has a balanced subgraph
with at least $\frac{m}{2} + \frac{n-1}{4}+\frac{k}{4}$ edges, where $k$ is the
parameter.
  We obtain an algorithm for the problem of runtime $8^k(kn)^{O(1)}$. We also
prove that for each instance $(G,k)$ of the problem, in polynomial time, we can
either solve $(G,k)$ or produce an equivalent instance $(G',k')$ such that
$k'\le k$ and $|V(G')|=O(k^3)$. Our first result generalizes a result of
Crowston, Jones and Mnich (ICALP 2012) on the corresponding parameterization of
Max Cut (when every edge of $G$ has sign $-$). Our second result generalizes
and significantly improves the corresponding result of Crowston, Jones and
Mnich: they showed that $|V(G')|=O(k^5)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6855</identifier>
 <datestamp>2014-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6855</id><created>2012-12-31</created><authors><author><keyname>Kretz</keyname><forenames>Tobias</forenames></author></authors><title>Multi-Directional Flow as Touch-Stone to Assess Models of Pedestrian
  Dynamics</title><categories>physics.soc-ph cs.MA</categories><comments>Contribution to 92nd Annual Meeting of the Transportation Research
  Board 2013. Presentation Number 13-1160</comments><journal-ref>in Annual Meeting of the Transportation Research Board, 13-1160,
  (2013)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For simulation models of pedestrian dynamics there are always the issues of
calibration and validation. These are usually done by comparing measured
properties of the dynamics found in observation, experiments and simulation in
certain scenarios. For this the scenarios first need to be sensitive to
parameter changes of a particular model or - if models are compared -
differences between models. Second it is helpful if the exhibited differences
can be expressed in quantities which are as simple as possible ideally a single
number. Such a scenario is proposed in this contribution together with
evaluation measures. In an example evaluation of a particular model it is shown
that the proposed evaluation measures are very sensitive to parameter changes
and therefore summarize differences effects of parameter changes and
differences between models efficiently, sometimes in a single number. It is
shown how the symmetry which exists in the achiral geometry of the proposed
example scenario is broken in particular simulation runs exhibiting chiral
dynamics, while in the statistics of 1,000 simulation runs there is a symmetry
between left- and right-chiral dynamics. In the course of the symmetry breaking
differences between models and parameter settings are amplified which is the
origin of the high sensitivity of the scenario against parameter changes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6856</identifier>
 <datestamp>2013-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6856</id><created>2012-12-31</created><authors><author><keyname>Altman</keyname><forenames>Eitan</forenames></author><author><keyname>De Pellegrini</keyname><forenames>Francesco</forenames></author><author><keyname>El-Azouzi</keyname><forenames>Rachid</forenames></author><author><keyname>Miorandi</keyname><forenames>Daniele</forenames></author><author><keyname>Jimenez</keyname><forenames>Tania</forenames></author></authors><title>Emergence of Equilibria from Individual Strategies in Online Content
  Diffusion</title><categories>cs.GT cs.NI cs.SI</categories><msc-class>91A80</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social scientists have observed that human behavior in society can often be
modeled as corresponding to a threshold type policy. A new behavior would
propagate by a procedure in which an individual adopts the new behavior if the
fraction of his neighbors or friends having adopted the new behavior exceeds
some threshold. In this paper we study the question of whether the emergence of
threshold policies may be modeled as a result of some rational process which
would describe the behavior of non-cooperative rational members of some social
network. We focus on situations in which individuals take the decision whether
to access or not some content, based on the number of views that the content
has. Our analysis aims at understanding not only the behavior of individuals,
but also the way in which information about the quality of a given content can
be deduced from view counts when only part of the viewers that access the
content are informed about its quality. In this paper we present a game
formulation for the behavior of individuals using a meanfield model: the number
of individuals is approximated by a continuum of atomless players and for which
the Wardrop equilibrium is the solution concept. We derive conditions on the
problem's parameters that result indeed in the emergence of threshold
equilibria policies. But we also identify some parameters in which other
structures are obtained for the equilibrium behavior of individuals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6857</identifier>
 <datestamp>2013-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6857</id><created>2012-12-31</created><authors><author><keyname>Bagan</keyname><forenames>Guillaume</forenames></author><author><keyname>Bonifati</keyname><forenames>Angela</forenames></author><author><keyname>Groz</keyname><forenames>Benoit</forenames></author></authors><title>A Trichotomy for Regular Simple Path Queries on Graphs</title><categories>cs.DB cs.DM</categories><comments>15 pages, conference submission</comments><msc-class>05CXX</msc-class><acm-class>E.2; F.2.2; G.2.2; H.2.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Regular path queries (RPQs) select nodes connected by some path in a graph.
The edge labels of such a path have to form a word that matches a given regular
expression. We investigate the evaluation of RPQs with an additional constraint
that prevents multiple traversals of the same nodes. Those regular simple path
queries (RSPQs) find several applications in practice, yet they quickly become
intractable, even for basic languages such as (aa)* or a*ba*.
  In this paper, we establish a comprehensive classification of regular
languages with respect to the complexity of the corresponding regular simple
path query problem. More precisely, we identify the fragment that is maximal in
the following sense: regular simple path queries can be evaluated in polynomial
time for every regular language L that belongs to this fragment and evaluation
is NP-complete for languages outside this fragment. We thus fully characterize
the frontier between tractability and intractability for RSPQs, and we refine
our results to show the following trichotomy: Evaluations of RSPQs is either
AC0, NL-complete or NP-complete in data complexity, depending on the regular
language L. The fragment identified also admits a simple characterization in
terms of regular expressions.
  Finally, we also discuss the complexity of the following decision problem:
decide, given a language L, whether finding a regular simple path for L is
tractable. We consider several alternative representations of L: DFAs, NFAs or
regular expressions, and prove that this problem is NL-complete for the first
representation and PSPACE-complete for the other two. As a conclusion we extend
our results from edge-labeled graphs to vertex-labeled graphs and vertex-edge
labeled graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6879</identifier>
 <datestamp>2015-03-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6879</id><created>2012-12-31</created><updated>2015-03-24</updated><authors><author><keyname>Chalopin</keyname><forenames>J&#xe9;r&#xe9;mie</forenames></author><author><keyname>Chepoi</keyname><forenames>Victor</forenames></author><author><keyname>Osajda</keyname><forenames>Damian</forenames></author></authors><title>On two conjectures of Maurer concerning basis graphs of matroids</title><categories>math.CO cs.DM</categories><comments>28 pages</comments><msc-class>05B35, 05C12, 57M10, 57M20</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We characterize 2-dimensional complexes associated canonically with basis
graphs of matroids as simply connected triangle-square complexes satisfying
some local conditions. This proves a version of a (disproved) conjecture by
Stephen Maurer (Conjecture 3 of S. Maurer, Matroid basis graphs I, JCTB 14
(1973), 216-240). We also establish Conjecture 1 from the same paper about the
redundancy of the conditions in the characterization of basis graphs. We
indicate positive-curvature-like aspects of the local properties of the studied
complexes. We characterize similarly the corresponding 2-dimensional complexes
of even $\Delta$-matroids.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6882</identifier>
 <datestamp>2015-12-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6882</id><created>2012-12-31</created><updated>2015-12-29</updated><authors><author><keyname>Nittoor</keyname><forenames>Vivek S.</forenames></author></authors><title>Regular Bipartite Graphs And Their Properties</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new notation for representing labeled regular bipartite graphs
of arbitrary degree. Several enumeration problems for labeled and unlabeled
regular bipartite graphs have been introduced. A general algorithm for
enumerating all non-isomorphic 2-regular bipartite graphs for a specified
number of vertices has been described and a mathematical proof has been
provided for its completeness. An abstraction of m Symmetric Permutation Tree
in order to visualize a labeled r-Regular Bipartite Graph with 2m vertices and
enumerate its automorphism group has been introduced. An algorithm to generate
the partition associated with two compatible permutations has been introduced.
The relationship between Automorphism Group and permutation enumeration problem
has been used to derive formulae for the number of compatible permutations
corresponding to a specified partition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6883</identifier>
 <datestamp>2013-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6883</id><created>2012-12-31</created><updated>2013-01-22</updated><authors><author><keyname>Nittoor</keyname><forenames>Vivek S.</forenames></author><author><keyname>Suda</keyname><forenames>Reiji</forenames></author></authors><title>Partition Parameters for Girth Maximum (m, r) BTUs</title><categories>cs.DM math.CO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes the calculation of the optimal partition parameters such
that the girth maximum (m, r) Balanced Tanner Unit lies in family of BTUs
specified by them using a series of proved results and thus creates a framework
for specifying a search problem for finding the girth maximum (m, r) BTU.
Several open questions for girth maximum (m, r) BTU have been raised.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6903</identifier>
 <datestamp>2013-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6903</id><created>2012-12-31</created><authors><author><keyname>Hidri</keyname><forenames>Adel</forenames></author><author><keyname>Meddeb</keyname><forenames>Souad</forenames></author><author><keyname>Amiri</keyname><forenames>Hamid</forenames></author></authors><title>About Multichannel Speech Signal Extraction and Separation Techniques</title><categories>cs.SD</categories><comments>10 pages, 11 Figures. arXiv admin note: substantial text overlap with
  arXiv:1212.6080</comments><journal-ref>Journal of Signal and Information Processing, Vol. 3 No. 2, 2012,
  pp. 238-247</journal-ref><doi>10.4236/jsip.2012.32032</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The extraction of a desired speech signal from a noisy environment has become
a challenging issue. In the recent years, the scientific community has
particularly focused on multichannel techniques which are dealt with in this
review. In fact, this study tries to classify these multichannel techniques
into three main ones: Beamforming, Independent Com-ponent Analysis (ICA) and
Time Frequency (T-F) masking. This paper also highlights their advantages and
drawbacks. However these previously mentioned techniques could not afford
satisfactory results. This fact leads to the idea that a combination of those
techniques, which is depicted along this study, may probably provide more
efficient results. In-deed, giving the fact that those approaches are still be
considered as being not totally efficient, has led us to review these mentioned
above in the hope that further researches will provide this domain with
suitable innovations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6907</identifier>
 <datestamp>2013-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6907</id><created>2012-12-31</created><authors><author><keyname>Mukhopadhyay</keyname><forenames>Abhisek</forenames></author><author><keyname>Ranjan</keyname><forenames>Priya</forenames></author></authors><title>Nonlinear Instabilities in D2TCP-II</title><categories>cs.NI nlin.CD</categories><comments>Seven Pages, TIME-E conference at Bandung Indonesia</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the era of heavy-duty transmission control protocols (TCP), adapted for
extremely hi-bandwidth datacenters, the fundamental question of stable
interaction with either proposed active queue management(AQM) or popularly
discussed Random Early Detection (RED) remains a hotly debated issue. While
there are claims of &quot;oscillation&quot; only dynamical behavior, there are equally
large number of claims which demonstrate the chaotic nature of different
flavors of TCP and their AQM interaction. In this work, we provide a sound and
analytical mathematical model of DTCP/D2TCP and study their interaction with
threshold based packet marking policy. Our work shows that for a simple
scenario this interaction is chaotic in nature and has large variability in
dynamical behavior over orders of magnitude changes in parameter range as
demonstrated by bifurcation diagrams. We conclude with numerical simulation
evidence that chaotic behavior of protocols is inherent in their design which
they inherit from their early vanilla TCP days, and it has serious implications
for data-center throughput, load batching and collapse in Incast kind of
scenario.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6922</identifier>
 <datestamp>2013-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6922</id><created>2012-12-31</created><authors><author><keyname>Hassim</keyname><forenames>Yana Mazwin Mohmad</forenames></author><author><keyname>Ghazali</keyname><forenames>Rozaida</forenames></author></authors><title>Training a Functional Link Neural Network Using an Artificial Bee Colony
  for Solving a Classification Problems</title><categories>cs.NE cs.LG</categories><comments>6 pages, 3 figures, 4 tables</comments><journal-ref>Journal of Computing, Volume 4, Issue 9 (2012), 110-115</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Artificial Neural Networks have emerged as an important tool for
classification and have been widely used to classify a non-linear separable
pattern. The most popular artificial neural networks model is a Multilayer
Perceptron (MLP) as it is able to perform classification task with significant
success. However due to the complexity of MLP structure and also problems such
as local minima trapping, over fitting and weight interference have made neural
network training difficult. Thus, the easy way to avoid these problems is to
remove the hidden layers. This paper presents the ability of Functional Link
Neural Network (FLNN) to overcome the complexity structure of MLP by using
single layer architecture and propose an Artificial Bee Colony (ABC)
optimization for training the FLNN. The proposed technique is expected to
provide better learning scheme for a classifier in order to get more accurate
classification result
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6923</identifier>
 <datestamp>2013-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6923</id><created>2012-12-31</created><authors><author><keyname>Allison</keyname><forenames>John</forenames></author><author><keyname>Garnier</keyname><forenames>Laurent</forenames></author><author><keyname>Kimura</keyname><forenames>Akinori</forenames></author><author><keyname>Perl</keyname><forenames>Joseph</forenames></author></authors><title>The Geant4 Visualisation System - a multi-driver graphics system</title><categories>cs.GR hep-ex</categories><comments>22 pages, 15 figures. Submitted to the International Journal of
  Modeling, Simulation, and Scientific Computing</comments><report-no>MAN/HEP/2012/19</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  From the beginning the Geant4 Visualisation System was designed to support
several simultaneous graphics systems written to common abstract interfaces.
Today it has matured into a powerful diagnostic and presentational tool. It
comes with a library of models that may be added to the current scene and which
include the representation of the Geant4 geometry hierarchy, simulated
trajectories and user-written hits and digitisations. The workhorse is the
OpenGL suite of drivers for X, Xm, Qt and Win32. There is an Open Inventor
driver. Scenes can be exported in special graphics formats for offline viewing
in the DAWN, VRML, HepRApp and gMocren browsers. PostScript can be generated
through OpenGL, Open Inventor, DAWN and HepRApp. Geant4's own tracking
algorithms are used by the Ray Tracer. Not all drivers support all features but
all drivers bring added functionality of some sort. This paper describes the
interfaces and details the individual drivers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6925</identifier>
 <datestamp>2016-02-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6925</id><created>2012-12-31</created><updated>2016-02-08</updated><authors><author><keyname>Guruswami</keyname><forenames>Venkatesan</forenames></author><author><keyname>Onak</keyname><forenames>Krzysztof</forenames></author></authors><title>Superlinear lower bounds for multipass graph processing</title><categories>cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove $n^{1+\Omega(1/p)}/p^{O(1)}$ lower bounds for the space complexity
of $p$-pass streaming algorithms solving the following problems on $n$-vertex
graphs:
  * testing if an undirected graph has a perfect matching (this implies lower
bounds for computing a maximum matching or even just the maximum matching
size),
  * testing if two specific vertices are at distance at most $2(p+1)$ in an
undirected graph,
  * testing if there is a directed path from $s$ to $t$ for two specific
vertices $s$ and $t$ in a directed graph.
  Prior to our result, it was known that these problems require $\Omega(n^2)$
space in one pass, but no $n^{1+\Omega(1)}$ lower bound was known for any $p\ge
2$.
  These streaming results follow from a communication complexity lower bound
for a communication game in which the players hold two graphs on the same set
of vertices. The task of the players is to find out whether the sets of
vertices at distance exactly $p+1$ from a specific vertex intersect. The game
requires a significant amount of communication only if the players are forced
to speak in a specific difficult order. This is reminiscent of lower bounds for
communication problems such as indexing and pointer chasing. Among other
things, our line of attack requires proving an information cost lower bound for
a decision version of the classic pointer chasing problem and a direct sum type
theorem for the disjunction of several instances of this problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6930</identifier>
 <datestamp>2013-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6930</id><created>2012-12-31</created><authors><author><keyname>Khisti</keyname><forenames>Ashish</forenames></author><author><keyname>Liu</keyname><forenames>Tie</forenames></author></authors><title>Private Broadcasting over Independent Parallel Channels</title><categories>cs.IT math.IT</categories><comments>Submitted to IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study private broadcasting of two messages to two groups of receivers over
independent parallel channels. One group consists of an arbitrary number of
receivers interested in a common message, whereas the other group has only one
receiver. Each message must be kept confidential from the receiver(s) in the
other group. Each of the sub-channels is degraded, but the order of receivers
on each channel can be different. While corner points of the capacity region
were characterized in earlier works, we establish the capacity region and show
the optimality of a superposition strategy. For the case of parallel Gaussian
channels, we show that a Gaussian input distribution is optimal. We also
discuss an extension of our setup to broadcasting over a block-fading channel
and demonstrate significant performance gains using the proposed scheme over a
baseline time-sharing scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6933</identifier>
 <datestamp>2013-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6933</id><created>2012-12-31</created><updated>2013-01-14</updated><authors><author><keyname>Moukalled</keyname><forenames>H. J.</forenames></author></authors><title>On Automation and Medical Image Interpretation, With Applications for
  Laryngeal Imaging</title><categories>cs.CV</categories><comments>18 pages, 9 figures, 41 references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Indeed, these are exciting times. We are in the heart of a digital
renaissance. Automation and computer technology allow engineers and scientists
to fabricate processes that amalgamate quality of life. We anticipate much
growth in medical image interpretation and understanding, due to the influx of
computer technologies. This work should serve as a guide to introduce the
reader to core themes in theoretical computer science, as well as imaging
applications for understanding vocal-fold vibrations. In this work, we motivate
the use of automation, review some mathematical models of computation. We
present a proof of a classical problem in image analysis that cannot be
automated by means of algorithms. Furthermore, discuss some applications for
processing medical images of the vocal folds, and discuss some of the
exhilarating directions the art of automation will take vocal-fold image
interpretation and quite possibly other areas of biomedical image analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6935</identifier>
 <datestamp>2013-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6935</id><created>2012-12-31</created><authors><author><keyname>Camerani</keyname><forenames>Giorgio</forenames></author></authors><title>The ODD EVEN DELTA problem is #P-hard</title><categories>cs.CC</categories><comments>3 pages</comments><acm-class>F.1.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let G=(V,E) be a graph. Let k &lt; |V| be an integer. Let O_k be the number of
edge induced subgraphs of G having k vertices and an odd number of edges. Let
E_k be the number of edge induced subgraphs of G having k vertices and an even
number of edges. Let D_k = O_k - E_k. The ODD EVEN DELTA problem consists in
computing D_k, given G and k. We show that such problem is #P-hard, even on
3-regular bipartite planar graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6937</identifier>
 <datestamp>2015-07-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6937</id><created>2012-12-31</created><updated>2013-08-09</updated><authors><author><keyname>Colcombet</keyname><forenames>Thomas</forenames><affiliation>Liafa/CNRS/Universit&#xe9; Denis Diderot</affiliation></author></authors><title>Regular Cost Functions, Part I: Logic and Algebra over Words</title><categories>cs.FL</categories><comments>47 pages</comments><proxy>LMCS</proxy><journal-ref>Logical Methods in Computer Science, Volume 9, Issue 3 (August 13,
  2013) lmcs:1221</journal-ref><doi>10.2168/LMCS-9(3:3)2013</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The theory of regular cost functions is a quantitative extension to the
classical notion of regularity. A cost function associates to each input a
non-negative integer value (or infinity), as opposed to languages which only
associate to each input the two values &quot;inside&quot; and &quot;outside&quot;. This theory is a
continuation of the works on distance automata and similar models. These models
of automata have been successfully used for solving the star-height problem,
the finite power property, the finite substitution problem, the relative
inclusion star-height problem and the boundedness problem for monadic-second
order logic over words. Our notion of regularity can be -- as in the classical
theory of regular languages -- equivalently defined in terms of automata,
expressions, algebraic recognisability, and by a variant of the monadic
second-order logic. These equivalences are strict extensions of the
corresponding classical results. The present paper introduces the cost monadic
logic, the quantitative extension to the notion of monadic second-order logic
we use, and show that some problems of existence of bounds are decidable for
this logic. This is achieved by introducing the corresponding algebraic
formalism: stabilisation monoids.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6952</identifier>
 <datestamp>2013-04-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6952</id><created>2012-12-31</created><updated>2013-04-02</updated><authors><author><keyname>Shah</keyname><forenames>Nihar B.</forenames></author></authors><title>On Minimizing Data-read and Download for Storage-Node Recovery</title><categories>cs.IT math.IT</categories><comments>IEEE Communications Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of efficient recovery of the data stored in any
individual node of a distributed storage system, from the rest of the nodes.
Applications include handling failures and degraded reads. We measure
efficiency in terms of the amount of data-read and the download required. To
minimize the download, we focus on the minimum bandwidth setting of the
'regenerating codes' model for distributed storage. Under this model, the
system has a total of n nodes, and the data stored in any node must be
(efficiently) recoverable from any d of the other (n-1) nodes. Lower bounds on
the two metrics under this model were derived previously; it has also been
shown that these bounds are achievable for the amount of data-read and download
when d=n-1, and for the amount of download alone when d&lt;n-1.
  In this paper, we complete this picture by proving the converse result, that
when d&lt;n-1, these lower bounds are strictly loose with respect to the amount of
read required. The proof is information-theoretic, and hence applies to
non-linear codes as well. We also show that under two (practical) relaxations
of the problem setting, these lower bounds can be met for both read and
download simultaneously.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6958</identifier>
 <datestamp>2013-01-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6958</id><created>2012-12-31</created><authors><author><keyname>Gordon</keyname><forenames>Geoffrey J.</forenames></author></authors><title>Fast Solutions to Projective Monotone Linear Complementarity Problems</title><categories>cs.LG math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new interior-point potential-reduction algorithm for solving
monotone linear complementarity problems (LCPs) that have a particular special
structure: their matrix $M\in{\mathbb R}^{n\times n}$ can be decomposed as
$M=\Phi U + \Pi_0$, where the rank of $\Phi$ is $k&lt;n$, and $\Pi_0$ denotes
Euclidean projection onto the nullspace of $\Phi^\top$. We call such LCPs
projective. Our algorithm solves a monotone projective LCP to relative accuracy
$\epsilon$ in $O(\sqrt n \ln(1/\epsilon))$ iterations, with each iteration
requiring $O(nk^2)$ flops. This complexity compares favorably with
interior-point algorithms for general monotone LCPs: these algorithms also
require $O(\sqrt n \ln(1/\epsilon))$ iterations, but each iteration needs to
solve an $n\times n$ system of linear equations, a much higher cost than our
algorithm when $k\ll n$. Our algorithm works even though the solution to a
projective LCP is not restricted to lie in any low-rank subspace.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1212.6964</identifier>
 <datestamp>2015-04-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1212.6964</id><created>2012-12-31</created><updated>2014-03-13</updated><authors><author><keyname>Kliuchnikov</keyname><forenames>Vadym</forenames></author><author><keyname>Maslov</keyname><forenames>Dmitri</forenames></author><author><keyname>Mosca</keyname><forenames>Michele</forenames></author></authors><title>Practical approximation of single-qubit unitaries by single-qubit
  quantum Clifford and T circuits</title><categories>quant-ph cs.ET</categories><comments>11 pages, 9 figures</comments><doi>10.1109/TC.2015.2409842</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an algorithm, along with its implementation that finds T-optimal
approximations of single-qubit Z-rotations using quantum circuits consisting of
Clifford and T gates. Our algorithm is capable of handling errors in
approximation down to size $10^{-15}$, resulting in optimal single-qubit
circuit designs required for implementation of scalable quantum algorithms. Our
implementation along with the experimental results are available in the public
domain.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0001</identifier>
 <datestamp>2013-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0001</id><created>2012-12-27</created><authors><author><keyname>Daimiwal</keyname><forenames>Nivedita</forenames></author><author><keyname>Sundhararajan</keyname><forenames>M.</forenames></author><author><keyname>Shriram</keyname><forenames>Revati</forenames></author></authors><title>Applications of fMRI for Brain Mapping</title><categories>cs.ET q-bio.NC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Brain-mapping techniques have proven to be vital in understanding the
molecular, cellular, and functional mechanisms of the brain. Normal anatomical
imaging can provide structural information on certain abnormalities in the
brain. However there are many neurological disorders for which only structure
studies are not sufficient. In such cases it is required to investigate the
functional organization of the brain. Further it is necessary to study the
brain functions under normal as well as diseased conditions. Brain mapping
techniques can help in deriving useful and important information on these
issues. Brain functions and brain area responsible for the particular
activities like motor, sensory speech and memory process could be investigated.
The authors provide an overview of various Brain Mapping techniques and fMRI
signal processing methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0006</identifier>
 <datestamp>2013-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0006</id><created>2012-12-30</created><authors><author><keyname>Colbaugh</keyname><forenames>Richard</forenames></author><author><keyname>Glass</keyname><forenames>Kristin</forenames></author><author><keyname>Johnson</keyname><forenames>Curtis</forenames></author></authors><title>Predictive Non-equilibrium Social Science</title><categories>cs.SI physics.soc-ph</categories><comments>arXiv admin note: substantial text overlap with arXiv:1212.6806</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Non-Equilibrium Social Science (NESS) emphasizes dynamical phenomena, for
instance the way political movements emerge or competing organizations
interact. This paper argues that predictive analysis is an essential element of
NESS, occupying a central role in its scientific inquiry and representing a key
activity of practitioners in domains such as economics, public policy, and
national security. We begin by clarifying the distinction between models which
are useful for prediction and the much more common explanatory models studied
in the social sciences. We then investigate a challenging real-world predictive
analysis case study, and find evidence that the poor performance of standard
prediction methods does not indicate an absence of human predictability but
instead reflects (1.) incorrect assumptions concerning the predictive utility
of explanatory models, (2.) misunderstanding regarding which features of social
dynamics actually possess predictive power, and (3.) practical difficulties
exploiting predictive representations.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0014</identifier>
 <datestamp>2014-03-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0014</id><created>2012-12-31</created><updated>2014-02-20</updated><authors><author><keyname>Krotov</keyname><forenames>Denis</forenames><affiliation>Sobolev Institute of Mathematics, Novosibirsk, Russia</affiliation></author><author><keyname>Potapov</keyname><forenames>Vladimir</forenames><affiliation>Sobolev Institute of Mathematics, Novosibirsk, Russia</affiliation></author></authors><title>Propelinear 1-perfect codes from quadratic functions</title><categories>cs.IT cs.DM math.CO math.IT</categories><comments>4 IEEE pages. v2: minor revision, + upper bound (Sect. III.B),
  +remarks (Sect. V.A); v3: minor revision, + length 15 (Sect. V.B)</comments><msc-class>94B25</msc-class><journal-ref>IEEE Trans. Inf. Theory 60(4) 2014, 2065-2068</journal-ref><doi>10.1109/TIT.2014.2303158</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Perfect codes obtained by the Vasil'ev--Sch\&quot;onheim construction from a
linear base code and quadratic switching functions are transitive and,
moreover, propelinear. This gives at least $\exp(cN^2)$ propelinear $1$-perfect
codes of length $N$ over an arbitrary finite field, while an upper bound on the
number of transitive codes is $\exp(C(N\ln N)^2)$. Keywords: perfect code,
propelinear code, transitive code, automorphism group, Boolean function.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0015</identifier>
 <datestamp>2013-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0015</id><created>2012-12-31</created><authors><author><keyname>Weller</keyname><forenames>Adrian</forenames></author><author><keyname>Jebara</keyname><forenames>Tony</forenames></author></authors><title>Bethe Bounds and Approximating the Global Optimum</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Inference in general Markov random fields (MRFs) is NP-hard, though
identifying the maximum a posteriori (MAP) configuration of pairwise MRFs with
submodular cost functions is efficiently solvable using graph cuts. Marginal
inference, however, even for this restricted class, is in #P. We prove new
formulations of derivatives of the Bethe free energy, provide bounds on the
derivatives and bracket the locations of stationary points, introducing a new
technique called Bethe bound propagation. Several results apply to pairwise
models whether associative or not. Applying these to discretized
pseudo-marginals in the associative case we present a polynomial time
approximation scheme for global optimization provided the maximum degree is
$O(\log n)$, and discuss several extensions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0026</identifier>
 <datestamp>2013-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0026</id><created>2012-12-31</created><authors><author><keyname>Scoville</keyname><forenames>John</forenames></author></authors><title>Bounding Lossy Compression using Lossless Codes at Reduced Precision</title><categories>cs.MM cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An alternative approach to two-part 'critical compression' is presented.
Whereas previous results were based on summing a lossless code at reduced
precision with a lossy-compressed error or noise term, the present approach
uses a similar lossless code at reduced precision to establish absolute bounds
which constrain an arbitrary lossy data compression algorithm applied to the
original data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0027</identifier>
 <datestamp>2013-10-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0027</id><created>2012-12-31</created><updated>2013-09-29</updated><authors><author><keyname>Uppman</keyname><forenames>Hannes</forenames></author></authors><title>Three-Element Min-Sol and Conservative Min-Cost-Hom</title><categories>cs.CC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Thapper and Zivny [STOC'13] recently classified the complexity of VCSP for
all finite-valued constraint languages. However, the complexity of VCSPs for
constraint languages that are not finite-valued remains poorly understood. In
this paper we study the complexity of two such VCSPs, namely Min-Cost-Hom and
Min-Sol. We obtain a full classification for the complexity of Min-Sol on
domains that contain at most three elements and for the complexity of
conservative Min-Cost-Hom on arbitrary finite domains. Our results answer a
question raised by Takhanov [STACS'10, COCOON'10].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0035</identifier>
 <datestamp>2013-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0035</id><created>2012-12-31</created><authors><author><keyname>Shparlinski</keyname><forenames>Igor</forenames></author></authors><title>On the Product of Small Elkies Primes</title><categories>math.NT cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given an elliptic curve $E$ over a finite field $\F_q$ of $q$ elements, we
say that an odd prime $\ell \nmid q$ is an Elkies prime for $E$ if $t_E^2 - 4q$
is a quadratic residue modulo $\ell$, where $t_E = q+1 - #E(\F_q)$ and
$#E(\F_q)$ is the number of $\F_q$-rational points on $E$. These primes are
used in the presently most efficient algorithm to compute $#E(\F_q)$. In
particular, the bound $L_q(E)$ such that the product of all Elkies primes for
$E$ up to $L_q(E)$ exceeds $4q^{1/2}$ is a crucial parameter of this algorithm.
We show that there are infinitely many pairs $(p, E)$ of primes $p$ and curves
$E$ over $\F_p$ with $L_p(E) \ge c \log p \log \log \log p$ for some absolute
constant $c&gt;0$, while a naive heuristic estimate suggests that $L_p(E) \sim
\log p$. This complements recent results of Galbraith and Satoh (2002),
conditional under the Generalised Riemann Hypothesis, and of Shparlinski and
Sutherland (2012), unconditional for almost all pairs $(p,E)$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0037</identifier>
 <datestamp>2013-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0037</id><created>2012-12-31</created><authors><author><keyname>Huuck</keyname><forenames>Ralf</forenames><affiliation>NICTA</affiliation></author></authors><title>Formal Verification, Engineering and Business Value</title><categories>cs.SE</categories><comments>In Proceedings FTSCS 2012, arXiv:1212.6574</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 105, 2012, pp. 1-4</journal-ref><doi>10.4204/EPTCS.105.1</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How to apply automated verification technology such as model checking and
static program analysis to millions of lines of embedded C/C++ code? How to
package this technology in a way that it can be used by software developers and
engineers, who might have no background in formal verification? And how to
convince business managers to actually pay for such a software? This work
addresses a number of those questions. Based on our own experience on
developing and distributing the Goanna source code analyzer for detecting
software bugs and security vulnerabilities in C/C++ code, we explain the
underlying technology of model checking, static analysis and SMT solving, steps
involved in creating industrial-proof tools.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0038</identifier>
 <datestamp>2013-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0038</id><created>2012-12-31</created><authors><author><keyname>Bae</keyname><forenames>Kyungmin</forenames><affiliation>University of Illinois at Urbana-Champaign</affiliation></author><author><keyname>Krisiloff</keyname><forenames>Joshua</forenames><affiliation>University of Illinois at Urbana-Champaign</affiliation></author><author><keyname>Meseguer</keyname><forenames>Jos&#xe9;</forenames><affiliation>University of Illinois at Urbana-Champaign</affiliation></author><author><keyname>&#xd6;lveczky</keyname><forenames>Peter Csaba</forenames><affiliation>University of Oslo</affiliation></author></authors><title>PALS-Based Analysis of an Airplane Multirate Control System in Real-Time
  Maude</title><categories>cs.LO cs.SE</categories><comments>In Proceedings FTSCS 2012, arXiv:1212.6574</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 105, 2012, pp. 5-21</journal-ref><doi>10.4204/EPTCS.105.2</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Distributed cyber-physical systems (DCPS) are pervasive in areas such as
aeronautics and ground transportation systems, including the case of
distributed hybrid systems. DCPS design and verification is quite challenging
because of asynchronous communication, network delays, and clock skews.
Furthermore, their model checking verification typically becomes unfeasible due
to the huge state space explosion caused by the system's concurrency. The PALS
(&quot;physically asynchronous, logically synchronous&quot;) methodology has been
proposed to reduce the design and verification of a DCPS to the much simpler
task of designing and verifying its underlying synchronous version. The
original PALS methodology assumes a single logical period, but Multirate PALS
extends it to deal with multirate DCPS in which components may operate with
different logical periods. This paper shows how Multirate PALS can be applied
to formally verify a nontrivial multirate DCPS. We use Real-Time Maude to
formally specify a multirate distributed hybrid system consisting of an
airplane maneuvered by a pilot who turns the airplane according to a specified
angle through a distributed control system. Our formal analysis revealed that
the original design was ineffective in achieving a smooth turning maneuver, and
led to a redesign of the system that satisfies the desired correctness
properties. This shows that the Multirate PALS methodology is not only
effective for formal DCPS verification, but can also be used effectively in the
DCPS design process, even before properties are verified.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0039</identifier>
 <datestamp>2013-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0039</id><created>2012-12-31</created><authors><author><keyname>Champion</keyname><forenames>Adrien</forenames><affiliation>Onera / Rockwell Collins France</affiliation></author><author><keyname>Delmas</keyname><forenames>R&#xe9;mi</forenames><affiliation>Onera</affiliation></author><author><keyname>Dierkes</keyname><forenames>Michael</forenames><affiliation>Rockwell Collins France</affiliation></author></authors><title>Generating Property-Directed Potential Invariants By Backward Analysis</title><categories>cs.LO cs.CE cs.SE</categories><comments>In Proceedings FTSCS 2012, arXiv:1212.6574</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 105, 2012, pp. 22-38</journal-ref><doi>10.4204/EPTCS.105.3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper addresses the issue of lemma generation in a k-induction-based
formal analysis of transition systems, in the linear real/integer arithmetic
fragment. A backward analysis, powered by quantifier elimination, is used to
output preimages of the negation of the proof objective, viewed as unauthorized
states, or gray states. Two heuristics are proposed to take advantage of this
source of information. First, a thorough exploration of the possible
partitionings of the gray state space discovers new relations between state
variables, representing potential invariants. Second, an inexact exploration
regroups and over-approximates disjoint areas of the gray state space, also to
discover new relations between state variables. k-induction is used to isolate
the invariants and check if they strengthen the proof objective. These
heuristics can be used on the first preimage of the backward exploration, and
each time a new one is output, refining the information on the gray states. In
our context of critical avionics embedded systems, we show that our approach is
able to outperform other academic or commercial tools on examples of interest
in our application field. The method is introduced and motivated through two
main examples, one of which was provided by Rockwell Collins, in a
collaborative formal verification framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0040</identifier>
 <datestamp>2013-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0040</id><created>2012-12-31</created><authors><author><keyname>Hui</keyname><forenames>Peter</forenames><affiliation>Pacific Northwest National Laboratory</affiliation></author><author><keyname>Chikkagoudar</keyname><forenames>Satish</forenames><affiliation>Pacific Northwest National Laboratory</affiliation></author></authors><title>A Formal Model For Real-Time Parallel Computation</title><categories>cs.LO cs.DC</categories><comments>In Proceedings FTSCS 2012, arXiv:1212.6574</comments><proxy>EPTCS</proxy><acm-class>D.2.4</acm-class><journal-ref>EPTCS 105, 2012, pp. 39-55</journal-ref><doi>10.4204/EPTCS.105.4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The imposition of real-time constraints on a parallel computing environment-
specifically high-performance, cluster-computing systems- introduces a variety
of challenges with respect to the formal verification of the system's timing
properties. In this paper, we briefly motivate the need for such a system, and
we introduce an automaton-based method for performing such formal verification.
We define the concept of a consistent parallel timing system: a hybrid system
consisting of a set of timed automata (specifically, timed Buchi automata as
well as a timed variant of standard finite automata), intended to model the
timing properties of a well-behaved real-time parallel system. Finally, we give
a brief case study to demonstrate the concepts in the paper: a parallel matrix
multiplication kernel which operates within provable upper time bounds. We give
the algorithm used, a corresponding consistent parallel timing system, and
empirical results showing that the system operates under the specified timing
constraints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0041</identifier>
 <datestamp>2013-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0041</id><created>2012-12-31</created><authors><author><keyname>Matsubara</keyname><forenames>Masahiro</forenames><affiliation>Hitachi, Ltd.</affiliation></author><author><keyname>Sakurai</keyname><forenames>Kohei</forenames><affiliation>Hitachi, Ltd.</affiliation></author><author><keyname>Narisawa</keyname><forenames>Fumio</forenames><affiliation>Hitachi, Ltd.</affiliation></author><author><keyname>Enshoiwa</keyname><forenames>Masushi</forenames><affiliation>Hitachi Advanced Digital, Inc.</affiliation></author><author><keyname>Yamane</keyname><forenames>Yoshio</forenames><affiliation>Hitachi Advanced Digital, Inc.</affiliation></author><author><keyname>Yamanaka</keyname><forenames>Hisamitsu</forenames><affiliation>Hitachi Automotive Systems, Ltd.</affiliation></author></authors><title>Model Checking with Program Slicing Based on Variable Dependence Graphs</title><categories>cs.LO cs.SE</categories><comments>In Proceedings FTSCS 2012, arXiv:1212.6574</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 105, 2012, pp. 56-68</journal-ref><doi>10.4204/EPTCS.105.5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In embedded control systems, the potential risks of software defects have
been increasing because of software complexity which leads to, for example,
timing related problems. These defects are rarely found by tests or
simulations. To detect such defects, we propose a modeling method which can
generate software models for model checking with a program slicing technique
based on a variable dependence graph. We have applied the proposed method to
one case in automotive control software and demonstrated the effectiveness of
the method. Furthermore, we developed a software tool to automate model
generation and achieved a 35% decrease in total verification time on model
checking.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0042</identifier>
 <datestamp>2013-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0042</id><created>2012-12-31</created><authors><author><keyname>Park</keyname><forenames>Mingyu</forenames><affiliation>Kyungpook National University</affiliation></author><author><keyname>Byun</keyname><forenames>Taejoon</forenames><affiliation>Kyungpook National University</affiliation></author><author><keyname>Choi</keyname><forenames>Yunja</forenames><affiliation>Kyungpook National University</affiliation></author></authors><title>Property-based Code Slicing for Efficient Verification of OSEK/VDX
  Operating Systems</title><categories>cs.SE</categories><comments>In Proceedings FTSCS 2012, arXiv:1212.6574</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 105, 2012, pp. 69-84</journal-ref><doi>10.4204/EPTCS.105.6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Testing is a de-facto verification technique in industry, but insufficient
for identifying subtle issues due to its optimistic incompleteness. On the
other hand, model checking is a powerful technique that supports
comprehensiveness, and is thus suitable for the verification of safety-critical
systems. However, it generally requires more knowledge and cost more than
testing. This work attempts to take advantage of both techniques to achieve
integrated and efficient verification of OSEK/VDX-based automotive operating
systems. We propose property-based environment generation and model extraction
techniques using static code analysis, which can be applied to both model
checking and testing. The technique is automated and applied to an
OSEK/VDX-based automotive operating system, Trampoline. Comparative experiments
using random testing and model checking for the verification of assertions in
the Trampoline kernel code show how our environment generation and abstraction
approach can be utilized for efficient fault-detection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0043</identifier>
 <datestamp>2013-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0043</id><created>2012-12-31</created><authors><author><keyname>Shaikh</keyname><forenames>Siraj</forenames><affiliation>Coventry University</affiliation></author><author><keyname>Krishnan</keyname><forenames>Padmanabhan</forenames><affiliation>Bond University</affiliation></author></authors><title>A Framework for Analysing Driver Interactions with Semi-Autonomous
  Vehicles</title><categories>cs.HC cs.RO cs.SY</categories><comments>In Proceedings FTSCS 2012, arXiv:1212.6574</comments><proxy>EPTCS</proxy><acm-class>H.1.2</acm-class><journal-ref>EPTCS 105, 2012, pp. 85-99</journal-ref><doi>10.4204/EPTCS.105.7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Semi-autonomous vehicles are increasingly serving critical functions in
various settings from mining to logistics to defence. A key characteristic of
such systems is the presence of the human (drivers) in the control loop. To
ensure safety, both the driver needs to be aware of the autonomous aspects of
the vehicle and the automated features of the vehicle built to enable safer
control. In this paper we propose a framework to combine empirical models
describing human behaviour with the environment and system models. We then
analyse, via model checking, interaction between the models for desired safety
properties. The aim is to analyse the design for safe vehicle-driver
interaction. We demonstrate the applicability of our approach using a case
study involving semi-autonomous vehicles where the driver fatigue are factors
critical to a safe journey.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0044</identifier>
 <datestamp>2013-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0044</id><created>2012-12-31</created><authors><author><keyname>Wang</keyname><forenames>Chen-Wei</forenames><affiliation>McMaster Centre for Software Certification, McMaster University</affiliation></author><author><keyname>Davies</keyname><forenames>Jim</forenames><affiliation>Department of Computer Science, University of Oxford</affiliation></author></authors><title>Formal Model-Driven Engineering: Generating Data and Behavioural
  Components</title><categories>cs.SE cs.LO cs.MS</categories><comments>In Proceedings FTSCS 2012, arXiv:1212.6574</comments><proxy>EPTCS</proxy><acm-class>D.3.1; F.3.1</acm-class><journal-ref>EPTCS 105, 2012, pp. 100-117</journal-ref><doi>10.4204/EPTCS.105.8</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Model-driven engineering is the automatic production of software artefacts
from abstract models of structure and functionality. By targeting a specific
class of system, it is possible to automate aspects of the development process,
using model transformations and code generators that encode domain knowledge
and implementation strategies. Using this approach, questions of correctness
for a complex, software system may be answered through analysis of abstract
models of lower complexity, under the assumption that the transformations and
generators employed are themselves correct. This paper shows how formal
techniques can be used to establish the correctness of model transformations
used in the generation of software components from precise object models. The
source language is based upon existing, formal techniques; the target language
is the widely-used SQL notation for database programming. Correctness is
established by giving comparable, relational semantics to both languages, and
checking that the transformations are semantics-preserving.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0045</identifier>
 <datestamp>2013-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0045</id><created>2012-12-31</created><authors><author><keyname>Wang</keyname><forenames>Mengying</forenames><affiliation>ECNU</affiliation></author><author><keyname>Lu</keyname><forenames>Yang</forenames><affiliation>SJTU</affiliation></author></authors><title>A Timed Calculus for Mobile Ad Hoc Networks</title><categories>cs.LO cs.SE</categories><comments>In Proceedings FTSCS 2012, arXiv:1212.6574</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 105, 2012, pp. 118-134</journal-ref><doi>10.4204/EPTCS.105.9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a timed calculus for Mobile Ad Hoc Networks embodying the
peculiarities of local broadcast, node mobility and communication interference.
We present a Reduction Semantics and a Labelled Transition Semantics and prove
the equivalence between them. We then apply our calculus to model and study
some MAC-layer protocols with special emphasis on node mobility and
communication interference.
  A main purpose of the semantics is to describe the various forms of
interference while nodes change their locations in the network. Such
interference only occurs when a node is simultaneously reached by more than one
ongoing transmission over the same channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0046</identifier>
 <datestamp>2013-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0046</id><created>2012-12-31</created><authors><author><keyname>Wang</keyname><forenames>Zheng</forenames><affiliation>East China Normal University</affiliation></author><author><keyname>Pu</keyname><forenames>Geguang</forenames><affiliation>East China Normal University</affiliation></author><author><keyname>Li</keyname><forenames>Jianwen</forenames><affiliation>East China Normal University</affiliation></author><author><keyname>He</keyname><forenames>Jifeng</forenames><affiliation>East China Normal University</affiliation></author><author><keyname>Qin</keyname><forenames>Shengchao</forenames><affiliation>University of Teesside</affiliation></author><author><keyname>Larsen</keyname><forenames>Kim G.</forenames><affiliation>Aalborg University of Denmark</affiliation></author><author><keyname>Madsen</keyname><forenames>Jan</forenames><affiliation>Technical University of Denmark</affiliation></author><author><keyname>Gu</keyname><forenames>Bin</forenames><affiliation>Beijing Institute of Control Engineering</affiliation></author></authors><title>MDM: A Mode Diagram Modeling Framework</title><categories>cs.LO cs.SE</categories><comments>In Proceedings FTSCS 2012, arXiv:1212.6574</comments><proxy>EPTCS</proxy><journal-ref>EPTCS 105, 2012, pp. 135-149</journal-ref><doi>10.4204/EPTCS.105.10</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Periodic control systems used in spacecrafts and automotives are usually
period-driven and can be decomposed into different modes with each mode
representing a system state observed from outside. Such systems may also
involve intensive computing in their modes. Despite the fact that such control
systems are widely used in the above-mentioned safety-critical embedded
domains, there is lack of domain-specific formal modelling languages for such
systems in the relevant industry. To address this problem, we propose a formal
visual modeling framework called mode diagram as a concise and precise way to
specify and analyze such systems. To capture the temporal properties of
periodic control systems, we provide, along with mode diagram, a property
specification language based on interval logic for the description of concrete
temporal requirements the engineers are concerned with. The statistical model
checking technique can then be used to verify the mode diagram models against
desired properties. To demonstrate the viability of our approach, we have
applied our modelling framework to some real life case studies from industry
and helped detect two design defects for some spacecraft control systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0047</identifier>
 <datestamp>2013-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0047</id><created>2012-12-31</created><authors><author><keyname>Towfic</keyname><forenames>Zaid J.</forenames></author><author><keyname>Chen</keyname><forenames>Jianshu</forenames></author><author><keyname>Sayed</keyname><forenames>Ali H.</forenames></author></authors><title>On Distributed Online Classification in the Midst of Concept Drifts</title><categories>math.OC cs.DC cs.LG cs.SI physics.soc-ph</categories><comments>19 pages, 14 figures, to appear in Neurocomputing, 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we analyze the generalization ability of distributed online
learning algorithms under stationary and non-stationary environments. We derive
bounds for the excess-risk attained by each node in a connected network of
learners and study the performance advantage that diffusion strategies have
over individual non-cooperative processing. We conduct extensive simulations to
illustrate the results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0048</identifier>
 <datestamp>2013-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0048</id><created>2012-12-31</created><authors><author><keyname>Kamada</keyname><forenames>Yukihiro</forenames></author><author><keyname>Miyasaki</keyname><forenames>Kiyonori</forenames></author></authors><title>Generating High-Order Threshold Functions with Multiple Thresholds</title><categories>cs.NE</categories><comments>7 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we consider situations in which a given logical function is
realized by a multithreshold threshold function. In such situations, constant
functions can be easily obtained from multithreshold threshold functions, and
therefore, we can show that it becomes possible to optimize a class of
high-order neural networks. We begin by proposing a generating method for
threshold functions in which we use a vector that determines the boundary
between the linearly separable function and the high-order threshold function.
By applying this method to high-order threshold functions, we show that
functions with the same weight as, but a different threshold than, a threshold
function generated by the generation process can be easily obtained. We also
show that the order of the entire network can be extended while maintaining the
structure of given functions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0051</identifier>
 <datestamp>2014-04-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0051</id><created>2012-12-31</created><authors><author><keyname>Chen</keyname><forenames>Licheng</forenames></author><author><keyname>Lu</keyname><forenames>Tianyue</forenames></author><author><keyname>Wang</keyname><forenames>Yanan</forenames></author><author><keyname>Chen</keyname><forenames>Mingyu</forenames></author><author><keyname>Ruan</keyname><forenames>Yuan</forenames></author><author><keyname>Cui</keyname><forenames>Zehan</forenames></author><author><keyname>Huang</keyname><forenames>Yongbing</forenames></author><author><keyname>Chen</keyname><forenames>Mingyang</forenames></author><author><keyname>Zhang</keyname><forenames>Jiutian</forenames></author><author><keyname>Bao</keyname><forenames>Yungang</forenames></author></authors><title>MIMS: Towards a Message Interface based Memory System</title><categories>cs.AR</categories><journal-ref>Journal of Computer Science and Technology (JCST), 2014, V29(2):
  255-272</journal-ref><doi>10.1007/s11390-014-1428-7</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Memory system is often the main bottleneck in chipmultiprocessor (CMP)
systems in terms of latency, bandwidth and efficiency, and recently
additionally facing capacity and power problems in an era of big data. A lot of
research works have been done to address part of these problems, such as
photonics technology for bandwidth, 3D stacking for capacity, and NVM for power
as well as many micro-architecture level innovations. Many of them need a
modification of current memory architecture, since the decades-old synchronous
memory architecture (SDRAM) has become an obstacle to adopt those advances.
However, to the best of our knowledge, none of them is able to provide a
universal memory interface that is scalable enough to cover all these problems.
  In this paper, we argue that a message-based interface should be adopted to
replace the traditional bus-based interface in memory system. A novel message
interface based memory system (MIMS) is proposed. The key innovation of MIMS is
that processor and memory system communicate through a universal and flexible
message interface. Each message packet could contain multiple memory requests
or commands along with various semantic information. The memory system is more
intelligent and active by equipping with a local buffer scheduler, which is
responsible to process packet, schedule memory requests, and execute specific
commands with the help of semantic information. The experimental results by
simulator show that, with accurate granularity message, the MIMS would improve
performance by 53.21%, while reducing energy delay product (EDP) by 55.90%, the
effective bandwidth utilization is improving by 62.42%. Furthermore, combining
multiple requests in a packet would reduce link overhead and provide
opportunity for address compression.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0068</identifier>
 <datestamp>2013-02-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0068</id><created>2013-01-01</created><updated>2013-02-18</updated><authors><author><keyname>Bresler</keyname><forenames>Guy</forenames></author><author><keyname>Bresler</keyname><forenames>Ma'ayan</forenames></author><author><keyname>Tse</keyname><forenames>David</forenames></author></authors><title>Optimal Assembly for High Throughput Shotgun Sequencing</title><categories>q-bio.GN cs.DS cs.IT math.IT q-bio.QM</categories><comments>26 pages, 18 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a framework for the design of optimal assembly algorithms for
shotgun sequencing under the criterion of complete reconstruction. We derive a
lower bound on the read length and the coverage depth required for
reconstruction in terms of the repeat statistics of the genome. Building on
earlier works, we design a de Brujin graph based assembly algorithm which can
achieve very close to the lower bound for repeat statistics of a wide range of
sequenced genomes, including the GAGE datasets. The results are based on a set
of necessary and sufficient conditions on the DNA sequence and the reads for
reconstruction. The conditions can be viewed as the shotgun sequencing analogue
of Ukkonen-Pevzner's necessary and sufficient conditions for Sequencing by
Hybridization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0079</identifier>
 <datestamp>2013-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0079</id><created>2013-01-01</created><authors><author><keyname>Kaspi</keyname><forenames>Yonatan</forenames></author><author><keyname>Merhav</keyname><forenames>Neri</forenames></author></authors><title>Zero-Delay and Causal Single-User and Multi-User Lossy Source Coding
  with Decoder Side Information</title><categories>cs.IT math.IT</categories><comments>Submitted to the IEEE Transactions on Information Theory</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider zero-delay single-user and multi-user source coding with average
distortion constraint and decoder side information. The zero-delay constraint
translates into causal (sequential) encoder and decoder pairs as well as the
use of instantaneous codes. For the single-user setting, we show that optimal
performance is attained by time sharing at most two scalar encoder-decoder
pairs, that use zero-error side information codes. Side information lookahead
is shown to useless in this setting. We show that the restriction to causal
encoding functions is the one that causes the performance degradation, compared
to unrestricted systems, and not the sequential decoders or instantaneous
codes. Furthermore, we show that even without delay constraints, if either the
encoder or decoder are restricted a-priori to be scalar, the performance loss
cannot be compensated by the other component, which can be scalar as well
without further loss. Finally, we show that the multi-terminal source coding
problem can be solved in the zero-delay regime and the rate-distortion region
is given.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0080</identifier>
 <datestamp>2013-03-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0080</id><created>2013-01-01</created><updated>2013-03-01</updated><authors><author><keyname>Xing</keyname><forenames>Chengwen</forenames></author><author><keyname>Li</keyname><forenames>Shuo</forenames></author><author><keyname>Fei</keyname><forenames>Zesong</forenames></author><author><keyname>Kuang</keyname><forenames>Jingming</forenames></author></authors><title>How to Understand LMMSE Transceiver Design for MIMO Systems From
  Quadratic Matrix Programming</title><categories>cs.IT math.IT</categories><comments>31 pages, 4 figures, Accepted by IET Communications</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, a unified linear minimum mean-square-error (LMMSE) transceiver
design framework is investigated, which is suitable for a wide range of
wireless systems. The unified design is based on an elegant and powerful
mathematical programming technology termed as quadratic matrix programming
(QMP). Based on QMP it can be observed that for different wireless systems,
there are certain common characteristics which can be exploited to design LMMSE
transceivers e.g., the quadratic forms. It is also discovered that evolving
from a point-to-point MIMO system to various advanced wireless systems such as
multi-cell coordinated systems, multi-user MIMO systems, MIMO cognitive radio
systems, amplify-and-forward MIMO relaying systems and so on, the quadratic
nature is always kept and the LMMSE transceiver designs can always be carried
out via iteratively solving a number of QMP problems. A comprehensive framework
on how to solve QMP problems is also given. The work presented in this paper is
likely to be the first shoot for the transceiver design for the future
ever-changing wireless systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0082</identifier>
 <datestamp>2013-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0082</id><created>2013-01-01</created><authors><author><keyname>Catak</keyname><forenames>F. Ozgur</forenames></author><author><keyname>Balaban</keyname><forenames>M. Erdal</forenames></author></authors><title>CloudSVM : Training an SVM Classifier in Cloud Computing Systems</title><categories>cs.LG cs.DC</categories><comments>13 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In conventional method, distributed support vector machines (SVM) algorithms
are trained over pre-configured intranet/internet environments to find out an
optimal classifier. These methods are very complicated and costly for large
datasets. Hence, we propose a method that is referred as the Cloud SVM training
mechanism (CloudSVM) in a cloud computing environment with MapReduce technique
for distributed machine learning applications. Accordingly, (i) SVM algorithm
is trained in distributed cloud storage servers that work concurrently; (ii)
merge all support vectors in every trained cloud node; and (iii) iterate these
two steps until the SVM converges to the optimal classifier function. Large
scale data sets are not possible to train using SVM algorithm on a single
computer. The results of this study are important for training of large scale
data sets for machine learning applications. We provided that iterative
training of splitted data set in cloud computing environment using SVM will
converge to a global optimal classifier in finite iteration size.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0087</identifier>
 <datestamp>2013-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0087</id><created>2013-01-01</created><authors><author><keyname>Zhang</keyname><forenames>Tian</forenames></author><author><keyname>Chen</keyname><forenames>Wei</forenames></author><author><keyname>Cao</keyname><forenames>Zhigang</forenames></author></authors><title>Opportunistic DF-AF Selection Relaying with Optimal Relay Selection in
  Nakagami-m Fading Environments</title><categories>cs.IT math.IT</categories><comments>Proc. IEEE ICCC'12, Beijing, China, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An opportunistic DF-AF selection relaying scheme with maximal received
signal-to-noise ratio (SNR) at the destination is investigated in this paper.
The outage probability of the opportunistic DF-AF selection relaying scheme
over Nakagami-m fading channels is analyzed, and a closed-form solution is
obtained. We perform asymptotic analysis of the outage probability in high SNR
domain. The coding gain and the diversity order are obtained. For the purpose
of comparison, the asymptotic analysis of opportunistic AF scheme in Nakagami-m
fading channels is also performed by using the Squeeze Theorem. In addition, we
prove that compared with the opportunistic DF scheme and opportunistic AF
scheme, the opportunistic DF-AF selection relaying scheme has better outage
performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0091</identifier>
 <datestamp>2014-07-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0091</id><created>2013-01-01</created><updated>2014-07-21</updated><authors><author><keyname>Bayraktar</keyname><forenames>Erhan</forenames></author><author><keyname>Yao</keyname><forenames>Song</forenames></author></authors><title>On the Robust Optimal Stopping Problem</title><categories>math.PR cs.SY math.OC q-fin.PR</categories><comments>Final Version, 49 pages. A more concise version is going to appear in
  the SIAM Journal on Control and Optimization</comments><msc-class>60G40, 93E20, 49L20, 91A15, 60G44, 91B28</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study a robust optimal stopping problem with respect to a set $\cP$ of
mutually singular probabilities. This can be interpreted as a zero-sum
controller-stopper game in which the stopper is trying to maximize its pay-off
while an adverse player wants to minimize this payoff by choosing an evaluation
criteria from $\cP$. We show that the \emph{upper Snell envelope $\ol{Z}$} of
the reward process $Y$ is a supermartingale with respect to an appropriately
defined nonlinear expectation $\ul{\sE}$, and $\ol{Z}$ is further an
$\ul{\sE}-$martingale up to the first time $\t^*$ when $\ol{Z}$ meets $Y$.
Consequently, $\t^*$ is the optimal stopping time for the robust optimal
stopping problem and the corresponding zero-sum game has a value. Although the
result seems similar to the one obtained in the classical optimal stopping
theory, the mutual singularity of probabilities and the game aspect of the
problem give rise to major technical hurdles, which we circumvent using some
new methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0093</identifier>
 <datestamp>2015-12-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0093</id><created>2013-01-01</created><updated>2014-11-12</updated><authors><author><keyname>Liu</keyname><forenames>Jingbo</forenames></author><author><keyname>Jin</keyname><forenames>Jian</forenames></author><author><keyname>Gu</keyname><forenames>Yuantao</forenames></author></authors><title>Robustness of Sparse Recovery via $F$-minimization: A Topological
  Viewpoint</title><categories>cs.IT math.IT</categories><comments>39 pages</comments><journal-ref>IEEE Transactions on Information Theory, (Volume:61 , Issue: 7 ),
  pages: 3996 - 4014, July 2015</journal-ref><doi>10.1109/TIT.2015.2438302</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A recent trend in compressed sensing is to consider non-convex optimization
techniques for sparse recovery. The important case of $F$-minimization has
become of particular interest, for which the exact reconstruction condition
(ERC) in the noiseless setting can be precisely characterized by the null space
property (NSP). However, little work has been done concerning its robust
reconstruction condition (RRC) in the noisy setting. We look at the null space
of the measurement matrix as a point on the Grassmann manifold, and then study
the relation between the ERC and RRC sets, denoted as $\Omega_J$ and
$\Omega_J^r$, respectively. It is shown that $\Omega_J^r$ is the interior of
$\Omega_J$, from which a previous result of the equivalence of ERC and RRC for
$\ell_p$-minimization follows easily as a special case. Moreover, when $F$ is
non-decreasing, it is shown that
$\overline{\Omega}_J\setminus\interior(\Omega_J)$ is a set of measure zero and
of the first category. As a consequence, the probabilities of ERC and RRC are
the same if the measurement matrix $\mathbf{A}$ is randomly generated according
to a continuous distribution. Quantitatively, if the null space
$\mathcal{N}(\bf A)$ lies in the &quot;$d$-interior&quot; of $\Omega_J$, then RRC will be
satisfied with the robustness constant
$C=\frac{2+2d}{d\sigma_{\min}(\mathbf{A}^{\top})}$; and conversely if RRC holds
with $C=\frac{2-2d}{d\sigma_{\max}(\mathbf{A}^{\top})}$, then $\mathcal{N}(\bf
A)$ must lie in $d$-interior of $\Omega_J$. We also present several rules for
comparing the performances of different cost functions. Finally, these results
are capitalized to derive achievable tradeoffs between the measurement rate and
robustness with the aid of Gordon's escape through the mesh theorem or a
connection between NSP and the restricted eigenvalue condition.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0094</identifier>
 <datestamp>2013-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0094</id><created>2013-01-01</created><authors><author><keyname>de Lamare</keyname><forenames>Rodrigo C.</forenames></author></authors><title>Joint Iterative Power Allocation and Linear Interference Suppression
  Algorithms in Cooperative DS-CDMA Networks</title><categories>cs.IT math.IT</categories><comments>9 figures; IET Communications, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work presents joint iterative power allocation and interference
suppression algorithms for spread spectrum networks which employ multiple hops
and the amplify-and-forward cooperation strategy for both the uplink and the
downlink. We propose a joint constrained optimization framework that considers
the allocation of power levels across the relays subject to individual and
global power constraints and the design of linear receivers for interference
suppression. We derive constrained linear minimum mean-squared error (MMSE)
expressions for the parameter vectors that determine the optimal power levels
across the relays and the linear receivers. In order to solve the proposed
optimization problems, we develop cost-effective algorithms for adaptive joint
power allocation, and estimation of the parameters of the receiver and the
channels. An analysis of the optimization problem is carried out and shows that
the problem can have its convexity enforced by an appropriate choice of the
power constraint parameter, which allows the algorithms to avoid problems with
local minima. A study of the complexity and the requirements for feedback
channels of the proposed algorithms is also included for completeness.
Simulation results show that the proposed algorithms obtain significant gains
in performance and capacity over existing non-cooperative and cooperative
schemes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0097</identifier>
 <datestamp>2013-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0097</id><created>2013-01-01</created><authors><author><keyname>de Lamare</keyname><forenames>Rodrigo C.</forenames></author><author><keyname>Diniz</keyname><forenames>Paulo S. R.</forenames></author></authors><title>Set-Membership Adaptive Algorithms based on Time-Varying Error Bounds
  for Interference Suppression</title><categories>cs.IT math.IT</categories><comments>7 figures; IEEE Transactions on Vehicular Technology, 2009. arXiv
  admin note: text overlap with arXiv:1212.6626</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work presents set-membership adaptive algorithms based on time-varying
error bounds for CDMA interference suppression. We introduce a modified family
of set-membership adaptive algorithms for parameter estimation with
time-varying error bounds. The algorithms considered include modified versions
of the set-membership normalized least mean squares (SM-NLMS), the affine
projection (SM-AP) and the bounding ellipsoidal adaptive constrained (BEACON)
recursive least-squares technique. The important issue of error bound
specification is addressed in a new framework that takes into account parameter
estimation dependency, multi-access and inter-symbol interference for DS-CDMA
communications. An algorithm for tracking and estimating the interference power
is proposed and analyzed. This algorithm is then incorporated into the proposed
time-varying error bound mechanisms. Computer simulations show that the
proposed algorithms are capable of outperforming previously reported techniques
with a significantly lower number of parameter updates and a reduced risk of
overbounding or underbounding.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0101</identifier>
 <datestamp>2013-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0101</id><created>2013-01-01</created><updated>2013-01-28</updated><authors><author><keyname>Andreeva</keyname><forenames>Julia</forenames></author><author><keyname>Iglesias</keyname><forenames>Carlos Borrego</forenames></author><author><keyname>Campana</keyname><forenames>Simone</forenames></author><author><keyname>Di Girolamo</keyname><forenames>Alessandro</forenames></author><author><keyname>Dzhunov</keyname><forenames>Ivan</forenames></author><author><keyname>Curull</keyname><forenames>Xavier Espinal</forenames></author><author><keyname>Gayazov</keyname><forenames>Stavro</forenames></author><author><keyname>Magradze</keyname><forenames>Erekle</forenames></author><author><keyname>Nowotka</keyname><forenames>Michal Maciej</forenames></author><author><keyname>Rinaldi</keyname><forenames>Lorenzo</forenames></author><author><keyname>Saiz</keyname><forenames>Pablo</forenames></author><author><keyname>Schovancova</keyname><forenames>Jaroslava</forenames></author><author><keyname>Stewart</keyname><forenames>Graeme Andrew</forenames></author><author><keyname>Wright</keyname><forenames>Michael</forenames></author></authors><title>Automating ATLAS Computing Operations using the Site Status Board</title><categories>cs.DC hep-ex</categories><comments>The paper has been withdrawn by the author</comments><journal-ref>Journal of Physics: Conference Series 396 (2012) 032072</journal-ref><doi>10.1088/1742-6596/396/3/032072</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The automation of operations is essential to reduce manpower costs and
improve the reliability of the system. The Site Status Board (SSB) is a
framework which allows Virtual Organizations to monitor their computing
activities at distributed sites and to evaluate site performance. The ATLAS
experiment intensively uses the SSB for the distributed computing shifts, for
estimating data processing and data transfer efficiencies at a particular site,
and for implementing automatic exclusion of sites from computing activities, in
case of potential problems. The ATLAS SSB provides a real-time aggregated
monitoring view and keeps the history of the monitoring metrics. Based on this
history, usability of a site from the perspective of ATLAS is calculated. The
paper will describe how the SSB is integrated in the ATLAS operations and
computing infrastructure and will cover implementation details of the ATLAS SSB
sensors and alarm system, based on the information in the SSB. It will
demonstrate the positive impact of the use of the SSB on the overall
performance of ATLAS computing activities and will overview future plans.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0103</identifier>
 <datestamp>2013-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0103</id><created>2013-01-01</created><authors><author><keyname>Marcus</keyname><forenames>Shoshana</forenames></author><author><keyname>Sokol</keyname><forenames>Dina</forenames></author></authors><title>2D Lyndon Words and Applications</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A Lyndon word is a primitive string which is lexicographically smallest among
cyclic permutations of its characters. Lyndon words are used for constructing
bases in free Lie algebras, constructing de Bruijn sequences, finding the
lexicographically smallest or largest substring in a string, and succinct
suffix-prefix matching of highly periodic strings. In this paper, we extend the
concept of the Lyndon word to two dimensions. We introduce the 2D Lyndon word
and use it to capture 2D horizontal periodicity of a matrix in which each row
is highly periodic, and to efficiently solve 2D horizontal suffix-prefix
matching among a set of patterns. This yields a succinct and efficient
algorithm for 2D dictionary matching.
  We present several algorithms that compute the 2D Lyndon word that represents
a matrix. The final algorithm achieves linear time complexity even when the
least common multiple of the periods of the rows is exponential in the matrix
width.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0104</identifier>
 <datestamp>2013-10-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0104</id><created>2013-01-01</created><authors><author><keyname>Tamar</keyname><forenames>Aviv</forenames></author><author><keyname>Di Castro</keyname><forenames>Dotan</forenames></author><author><keyname>Mannor</keyname><forenames>Shie</forenames></author></authors><title>Policy Evaluation with Variance Related Risk Criteria in Markov Decision
  Processes</title><categories>cs.LG stat.ML</categories><journal-ref>JMLR Workshop and Conference Proceedings 28 (3): 495-503, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we extend temporal difference policy evaluation algorithms to
performance criteria that include the variance of the cumulative reward. Such
criteria are useful for risk management, and are important in domains such as
finance and process control. We propose both TD(0) and LSTD(lambda) variants
with linear function approximation, prove their convergence, and demonstrate
their utility in a 4-dimensional continuous state space problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0114</identifier>
 <datestamp>2013-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0114</id><created>2013-01-01</created><authors><author><keyname>Tarau</keyname><forenames>Paul</forenames></author></authors><title>Tree-based Arithmetic and Compressed Representations of Giant Numbers</title><categories>cs.PL cs.DM cs.DS cs.MS</categories><comments>UNPUBLISHED DRAFT, 26 pages, 2 figures, literate Haskell code
  included</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Can we do arithmetic in a completely different way, with a radically
different data structure? Could this approach provide practical benefits, like
operations on giant numbers while having an average performance similar to
traditional bitstring representations?
  While answering these questions positively, our tree based representation
described in this paper comes with a few extra benefits: it compresses giant
numbers such that, for instance, the largest known prime number as well as its
related perfect number are represented as trees of small sizes. The same also
applies to Fermat numbers and important computations like exponentiation of two
become constant time operations.
  At the same time, succinct representations of sparse sets, multisets and
sequences become possible through bijections to our tree-represented natural
numbers.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0123</identifier>
 <datestamp>2014-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0123</id><created>2013-01-01</created><updated>2014-05-29</updated><authors><author><keyname>Chiplunkar</keyname><forenames>Ashish</forenames></author><author><keyname>Vishwanathan</keyname><forenames>Sundar</forenames></author></authors><title>On Randomized Memoryless Algorithms for the Weighted $k$-server Problem</title><categories>cs.DS</categories><comments>Published at the 54th Annual IEEE Symposium on Foundations of
  Computer Science (FOCS 2013)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The weighted $k$-server problem is a generalization of the $k$-server problem
in which the cost of moving a server of weight $\beta_i$ through a distance $d$
is $\beta_i\cdot d$. The weighted server problem on uniform spaces models
caching where caches have different write costs. We prove tight bounds on the
performance of randomized memoryless algorithms for this problem on uniform
metric spaces. We prove that there is an $\alpha_k$-competitive memoryless
algorithm for this problem, where $\alpha_k=\alpha_{k-1}^2+3\alpha_{k-1}+1$;
$\alpha_1=1$. On the other hand we also prove that no randomized memoryless
algorithm can have competitive ratio better than $\alpha_k$.
  To prove the upper bound of $\alpha_k$ we develop a framework to bound from
above the competitive ratio of any randomized memoryless algorithm for this
problem. The key technical contribution is a method for working with potential
functions defined implicitly as the solution of a linear system. The result is
robust in the sense that a small change in the probabilities used by the
algorithm results in a small change in the upper bound on the competitive
ratio. The above result has two important implications. Firstly this yields an
$\alpha_k$-competitive memoryless algorithm for the weighted $k$-server problem
on uniform spaces. This is the first competitive algorithm for $k&gt;2$ which is
memoryless. Secondly, this helps us prove that the Harmonic algorithm, which
chooses probabilities in inverse proportion to weights, has a competitive ratio
of $k\alpha_k$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0127</identifier>
 <datestamp>2013-08-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0127</id><created>2013-01-01</created><updated>2013-08-02</updated><authors><author><keyname>Srivastava</keyname><forenames>Madhur</forenames></author><author><keyname>Singh</keyname><forenames>Satish K.</forenames></author><author><keyname>Panigrahi</keyname><forenames>Prasanta K.</forenames></author></authors><title>A Semi-automated Statistical Algorithm for Object Separation</title><categories>cs.CV</categories><comments>21 pages, 9 figures, The final publication is available at
  link.springer.com</comments><doi>10.1007/s00034-013-9613-4</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explicate a semi-automated statistical algorithm for object identification
and segregation in both gray scale and color images. The algorithm makes
optimal use of the observation that definite objects in an image are typically
represented by pixel values having narrow Gaussian distributions about
characteristic mean values. Furthermore, for visually distinct objects, the
corresponding Gaussian distributions have negligible overlap with each other
and hence the Mahalanobis distance between these distributions are large. These
statistical facts enable one to sub-divide images into multiple thresholds of
variable sizes, each segregating similar objects. The procedure incorporates
the sensitivity of human eye to the gray pixel values into the variable
threshold size, while mapping the Gaussian distributions into localized
\delta-functions, for object separation. The effectiveness of this recursive
statistical algorithm is demonstrated using a wide variety of images.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0128</identifier>
 <datestamp>2013-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0128</id><created>2013-01-01</created><authors><author><keyname>Tarau</keyname><forenames>Paul</forenames></author></authors><title>Binary Tree Arithmetic with Generalized Constructors</title><categories>cs.MS cs.DM cs.PL</categories><comments>UNPUBLISHED DRAFt, 7 pages, 1 figure</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe arithmetic computations in terms of operations on some well known
free algebras (S1S, S2S and ordered rooted binary trees) while emphasizing the
common structure present in all them when seen as isomorphic with the set of
natural numbers.
  Constructors and deconstructors seen through an initial algebra semantics are
generalized to recursively defined functions obeying similar laws.
  Implementation using Scala's apply and unapply are discussed together with an
application to a realistic arbitrary size arithmetic package written in Scala,
based on the free algebra of rooted ordered binary trees, which also supports
rational number operations through an extension to signed rationals of the
Calkin-Wilf bijection.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0129</identifier>
 <datestamp>2013-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0129</id><created>2013-01-01</created><authors><author><keyname>Tarau</keyname><forenames>Paul</forenames></author></authors><title>On Two Infinite Families of Pairing Bijections</title><categories>cs.MS cs.DM cs.PL</categories><comments>UNPUBLISHED DRAFT</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe two general mechanisms for producing pairing bijections
(bijective functions defined from N x N to N).
  The first mechanism, using n-adic valuations results in parameterized
algorithms generating a countable family of distinct pairing bijections.
  The second mechanism, using characteristic functions of subsets of N provides
2^N distinct pairing bijections.
  Mechanisms to combine such pairing functions and their application to
generate families of permutations of N are also described.
  The paper uses a small subset of the functional language Haskell to provide
type checked executable specifications of all the functions defined in a
literate programming style. The self-contained Haskell code extracted from the
paper is available at http://logic.cse.unt.edu/tarau/research/2012/infpair.hs .
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0140</identifier>
 <datestamp>2014-04-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0140</id><created>2013-01-01</created><authors><author><keyname>Poncet</keyname><forenames>Paul</forenames></author></authors><title>The idempotent Radon--Nikodym theorem has a converse statement</title><categories>math.FA cs.IT math.IT</categories><comments>13 pages</comments><journal-ref>Information Sciences 271 (2014) 115-124</journal-ref><doi>10.1016/j.ins.2014.02.074</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Idempotent integration is an analogue of the Lebesgue integration where
$\sigma$-additive measures are replaced by $\sigma$-maxitive measures. It has
proved useful in many areas of mathematics such as fuzzy set theory,
optimization, idempotent analysis, large deviation theory, or extreme value
theory. Existence of Radon--Nikodym derivatives, which turns out to be crucial
in all of these applications, was proved by Sugeno and Murofushi. Here we show
a converse statement to this idempotent version of the Radon--Nikodym theorem,
i.e. we characterize the $\sigma$-maxitive measures that have the
Radon--Nikodym property.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0142</identifier>
 <datestamp>2013-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0142</id><created>2013-01-01</created><authors><author><keyname>Lopez-Paz</keyname><forenames>David</forenames></author><author><keyname>Hern&#xe1;ndez-Lobato</keyname><forenames>Jos&#xe9; Miguel</forenames></author><author><keyname>Sch&#xf6;lkopf</keyname><forenames>Bernhard</forenames></author></authors><title>Semi-Supervised Domain Adaptation with Non-Parametric Copulas</title><categories>stat.ML cs.LG</categories><comments>9 pages, Appearing on Advances in Neural Information Processing
  Systems 25</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A new framework based on the theory of copulas is proposed to address semi-
supervised domain adaptation problems. The presented method factorizes any
multivariate density into a product of marginal distributions and bivariate
cop- ula functions. Therefore, changes in each of these factors can be detected
and corrected to adapt a density model accross different learning domains.
Impor- tantly, we introduce a novel vine copula model, which allows for this
factorization in a non-parametric manner. Experimental results on regression
problems with real-world data illustrate the efficacy of the proposed approach
when compared to state-of-the-art techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0148</identifier>
 <datestamp>2013-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0148</id><created>2013-01-01</created><authors><author><keyname>Papapetrou</keyname><forenames>Maria</forenames></author><author><keyname>Kugiumtzis</keyname><forenames>Dimitris</forenames></author></authors><title>Markov Chain Order estimation with Conditional Mutual Information</title><categories>physics.data-an cs.IT math.IT stat.ME</categories><comments>16 pages, 3 figures; M. Papapetrou, D. Kugiumtzis, Markov chain order
  estimation with conditional mutual information, Physica A: Statistical
  Mechanics and its Applications, Available online 26 December 2012, ISSN
  0378-4371</comments><doi>10.1016/j.physa.2012.12.017.</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the Conditional Mutual Information (CMI) for the estimation of
the Markov chain order. For a Markov chain of $K$ symbols, we define CMI of
order $m$, $I_c(m)$, as the mutual information of two variables in the chain
being $m$ time steps apart, conditioning on the intermediate variables of the
chain. We find approximate analytic significance limits based on the estimation
bias of CMI and develop a randomization significance test of $I_c(m)$, where
the randomized symbol sequences are formed by random permutation of the
components of the original symbol sequence. The significance test is applied
for increasing $m$ and the Markov chain order is estimated by the last order
for which the null hypothesis is rejected. We present the appropriateness of
CMI-testing on Monte Carlo simulations and compare it to the Akaike and
Bayesian information criteria, the maximal fluctuation method (Peres-Shields
estimator) and a likelihood ratio test for increasing orders using
$\phi$-divergence. The order criterion of CMI-testing turns out to be superior
for orders larger than one, but its effectiveness for large orders depends on
data availability. In view of the results from the simulations, we interpret
the estimated orders by the CMI-testing and the other criteria on genes and
intergenic regions of DNA chains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0152</identifier>
 <datestamp>2013-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0152</id><created>2013-01-01</created><authors><author><keyname>Cahan</keyname><forenames>Dodge</forenames></author><author><keyname>McCabe-Dansted</keyname><forenames>John</forenames></author><author><keyname>Slinko</keyname><forenames>Arkadii</forenames></author></authors><title>Nonconvergent Electoral Equilibria under Scoring Rules: Beyond Plurality</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We use Hotelling's spatial model of competition to investigate the
position-taking behaviour of political candidates under a class of electoral
systems known as scoring rules. In a scoring rule election, voters rank all the
candidates running for office, following which the candidates are assigned
points according to a vector of nonincreasing scores. Convergent Nash
equilibria in which all candidates adopt the same policy were characterised by
Cox (1987). Here, we investigate nonconvergent equilibria, where candidates
adopt divergent policies. We identify a number of classes of scoring rules
exhibiting a range of different equilibrium properties. For some of these,
nonconvergent equilibria do not exist. For others, nonconvergent equilibria in
which candidates cluster at positions spread across the issue space are
observed. In particular, we prove that the class of convex rules does not have
Nash equilibria (convergent or nonconvergent) with the exception of some
derivatives of Borda rule. Finally, we examine the special cases of four-,
five- and six- candidate elections. In the former two cases, we provide a
complete characterisation of nonconvergent equilibria.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0157</identifier>
 <datestamp>2013-02-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0157</id><created>2013-01-01</created><updated>2013-02-01</updated><authors><author><keyname>Perera</keyname><forenames>Charith</forenames></author><author><keyname>Zaslavsky</keyname><forenames>Arkady</forenames></author><author><keyname>Christen</keyname><forenames>Peter</forenames></author><author><keyname>Salehi</keyname><forenames>Ali</forenames></author><author><keyname>Georgakopoulos</keyname><forenames>Dimitrios</forenames></author></authors><title>Capturing Sensor Data from Mobile Phones using Global Sensor Network
  Middleware</title><categories>cs.NI</categories><comments>Proceedings of the IEEE 23rd International Symposium on Personal
  Indoor and Mobile Radio Communications (PIMRC), Sydney, Australia, September,
  2012</comments><doi>10.1109/PIMRC.2012.6362778</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Mobile phones play increasingly bigger role in our everyday lives. Today,
most smart phones comprise a wide variety of sensors which can sense the
physical environment. The Internet of Things vision encompasses participatory
sensing which is enabled using mobile phones based sensing and reasoning. In
this research, we propose and demonstrate our DAM4GSN architecture to capture
sensor data using sensors built into the mobile phones. Specifically, we
combine an open source sensor data stream processing engine called 'Global
Sensor Network (GSN)' with the Android platform to capture sensor data. To
achieve this goal, we proposed and developed a prototype application that can
be installed on Android devices as well as a AndroidWrapper as a GSN middleware
component. The process and the difficulty of manually connecting sensor devices
to sensor data processing middleware systems are examined. We evaluated the
performance of the system based on power consumption of the mobile client.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0159</identifier>
 <datestamp>2013-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0159</id><created>2013-01-01</created><authors><author><keyname>Zaslavsky</keyname><forenames>Arkady</forenames></author><author><keyname>Perera</keyname><forenames>Charith</forenames></author><author><keyname>Georgakopoulos</keyname><forenames>Dimitrios</forenames></author></authors><title>Sensing as a Service and Big Data</title><categories>cs.CY cs.NI</categories><comments>Proceedings of the International Conference on Advances in Cloud
  Computing (ACC), Bangalore, India, July, 2012</comments><report-no>ISSN/ISBN: 9788173717789</report-no><journal-ref>Proceedings of the International Conference on Advances in Cloud
  Computing (ACC), Bangalore, India, July, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Internet of Things (IoT) will comprise billions of devices that can sense,
communicate, compute and potentially actuate. Data streams coming from these
devices will challenge the traditional approaches to data management and
contribute to the emerging paradigm of big data. This paper discusses emerging
Internet of Things (IoT) architecture, large scale sensor network applications,
federating sensor networks, sensor data and related context capturing
techniques, challenges in cloud-based management, storing, archiving and
processing of sensor data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0167</identifier>
 <datestamp>2013-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0167</id><created>2013-01-01</created><authors><author><keyname>Mamatha</keyname><forenames>H. R.</forenames></author><author><keyname>Karthik</keyname><forenames>S.</forenames></author><author><keyname>Srikanta</keyname><forenames>Murthy K.</forenames></author></authors><title>Classifier Fusion Method to Recognize Handwritten Kannada Numerals</title><categories>cs.CV</categories><comments>6 pages having 3 tables and 9 figures. Published in ICECT 2012
  conference</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Optical Character Recognition (OCR) is one of the important fields in image
processing and pattern recognition domain. Handwritten character recognition
has always been a challenging task. Only a little work can be traced towards
the recognition of handwritten characters for the south Indian languages.
Kannada is one such south Indian language which is also one of the official
language of India. Accurate recognition of Kannada characters is a challenging
task because of the high degree of similarity between the characters. Hence,
good quality features are to be extracted and better classifiers are needed to
improve the accuracy of the OCR for Kannada characters. This paper explores the
effectiveness of feature extraction method like run length count (RLC) and
directional chain code (DCC) for the recognition of handwritten Kannada
numerals. In this paper, a classifier fusion method is implemented to improve
the recognition rate. For the classifier fusion, we have considered K-nearest
neighbour (KNN) and Linear classifier (LC). The novelty of this method is to
achieve better accuracy with few features using classifier fusion approach.
Proposed method achieves an average recognition rate of 96%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0169</identifier>
 <datestamp>2013-02-14</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0169</id><created>2013-01-02</created><updated>2013-02-13</updated><authors><author><keyname>Mangipudi</keyname><forenames>Easwar Vivek</forenames></author><author><keyname>Ramaiyan</keyname><forenames>Venkatesh</forenames></author><author><keyname>Bhashyam</keyname><forenames>Srikrishna</forenames></author></authors><title>Cross-Layer Strategies for Throughput Maximization in Data Aggregating
  Wireless Networks</title><categories>cs.NI</categories><comments>5 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a data aggregating wireless network where all nodes have data to
send to a single destination node, the sink. We consider a linear placement of
nodes with the sink at one end. The nodes communicate directly to the sink
(single hop transmission) and we assume that the nodes are scheduled one at a
time by a central scheduler (possibly the sink). The wireless nodes are power
limited and our network objective (notion of fairness) is to maximize the
minimum throughput of the nodes subject to the node power constraints. In this
work, we consider network designs that permit adapting node transmission time,
node transmission power and node placements, and study cross- layer strategies
that seek to maximize the network throughput. Using simulations, we
characterize the performance of the dif- ferent strategies and comment on their
applicability for various network scenarios.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0170</identifier>
 <datestamp>2015-04-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0170</id><created>2013-01-02</created><authors><author><keyname>Hori</keyname><forenames>Yutaka</forenames></author><author><keyname>Hara</keyname><forenames>Shinji</forenames></author></authors><title>Noise-Induced Spatial Pattern Formation in Stochastic Reaction-Diffusion
  Systems</title><categories>q-bio.QM cs.SY q-bio.PE</categories><journal-ref>Proceedings of IEEE Conference on Decision and Control, pp.
  1053-1058, 2012</journal-ref><doi>10.1109/CDC.2012.6426152</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is concerned with stochastic reaction-diffusion kinetics governed
by the reaction-diffusion master equation. Specifically, the primary goal of
this paper is to provide a mechanistic basis of Turing pattern formation that
is induced by intrinsic noise. To this end, we first derive an approximate
reaction-diffusion system by using linear noise approximation. We show that the
approximated system has a certain structure that is associated with a coupled
dynamic multi-agent system. This observation then helps us derive an efficient
computation tool to examine the spatial power spectrum of the intrinsic noise.
We numerically demonstrate that the result is quite effective to analyze
noise-induced Turing pattern. Finally, we illustrate the theoretical mechanism
behind the noise-induced pattern formation with a H2 norm interpretation of the
multi-agent system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0173</identifier>
 <datestamp>2013-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0173</id><created>2013-01-02</created><authors><author><keyname>Doreswamy</keyname></author></authors><title>Knowledge Discovery System For Fiber Reinforced Polymer Matrix Composite
  Laminate</title><categories>cs.AI cs.CE</categories><comments>International Journal of Computing, Vol. 2, Issue 7, pp. 121-130,
  July 2010. (ISSN 2151-9617)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper Knowledge Discovery System (KDS) is proposed and implemented
for the extraction of knowledge-mean stiffness of a polymer composite material
in which when fibers are placed at different orientations. Cosine amplitude
method is implemented for retrieving compatible polymer matrix and
reinforcement fiber which is coming under predicted fiber class, from the
polymer and reinforcement database respectively, based on the design
requirements. Fuzzy classification rules to classify fibers into short, medium
and long fiber classes are derived based on the fiber length and the computed
or derive critical length of fiber. Longitudinal and Transverse module of
Polymer Matrix Composite consisting of seven layers with different fiber volume
fractions and different fibers orientations at 0,15,30,45,60,75 and 90 degrees
are analyzed through Rule-of Mixture material design model. The analysis
results are represented in different graphical steps and have been measured
with statistical parameters. This data mining application implemented here has
focused the mechanical problems of material design and analysis. Therefore,
this system is an expert decision support system for optimizing the materials
performance for designing light-weight and strong, and cost effective polymer
composite materials.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0176</identifier>
 <datestamp>2013-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0176</id><created>2013-01-02</created><authors><author><keyname>Doreswamy</keyname></author><author><keyname>Vanajakshi</keyname><forenames>M. N.</forenames></author></authors><title>Similarity Measuring Approuch for Engineering Materials Selection</title><categories>cs.AI cs.CE</categories><comments>International Journal of Computational Intelligence Systems (IJCIS),
  Vol.3, Issue 1, April 2010, pp.115-122. (ISSN: 1875-6883)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Advanced engineering materials design involves the exploration of massive
multidimensional feature spaces, the correlation of materials properties and
the processing parameters derived from disparate sources. The search for
alternative materials or processing property strategies, whether through
analytical, experimental or simulation approaches, has been a slow and arduous
task, punctuated by infrequent and often expected discoveries. A few systematic
efforts have been made to analyze the trends in data as a basis for
classifications and predictions. This is particularly due to the lack of large
amounts of organized data and more importantly the challenging of shifting
through them in a timely and efficient manner. The application of recent
advances in Data Mining on materials informatics is the state of art of
computational and experimental approaches for materials discovery. In this
paper similarity based engineering materials selection model is proposed and
implemented to select engineering materials based on the composite materials
constraints. The result reviewed from this model is sustainable for effective
decision making in advanced engineering materials design applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0178</identifier>
 <datestamp>2013-10-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0178</id><created>2013-01-02</created><authors><author><keyname>Hanif</keyname><forenames>Muhammad Fainan</forenames></author><author><keyname>Tran</keyname><forenames>Le-Nam</forenames></author><author><keyname>T&#xf6;lli</keyname><forenames>Antti</forenames></author><author><keyname>Juntti</keyname><forenames>Markku</forenames></author><author><keyname>Glisic</keyname><forenames>Savo</forenames></author></authors><title>Efficient Solutions for Weighted Sum Rate Maximization in Multicellular
  Networks With Channel Uncertainties</title><categories>cs.IT math.IT</categories><comments>31 pages, 8 figures. Submitted for possible publication</comments><journal-ref>IEEE Transactions on Signal Processing, vol.61, no.22,
  pp.5659--5674, Nov., 2013</journal-ref><doi>10.1109/TSP.2013.2278815</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The important problem of weighted sum rate maximization (WSRM) in a
multicellular environment is intrinsically sensitive to channel estimation
errors. In this paper, we study ways to maximize the weighted sum rate in a
linearly precoded multicellular downlink system where the receivers are
equipped with a single antenna. With perfect channel information available at
the base stations, we first present a novel fast converging algorithm that
solves the WSRM problem. Then, the assumption is relaxed to the case where the
error vectors in the channel estimates are assumed to lie in an uncertainty set
formed by the intersection of finite ellipsoids. As our main contributions, we
present two procedures to solve the intractable nonconvex robust designs based
on the worst case principle. The proposed iterative algorithms solve the
semidefinite programs in each of their steps and provably converge to a locally
optimal solution of the robust WSRM problem. The proposed approaches are
numerically compared against each other to ascertain their robustness towards
channel estimation imperfections. The results clearly indicate the performance
gain compared to the case when channel uncertainties are ignored in the design
process. For certain scenarios, we also quantify the gap between the proposed
approximations and exact solutions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0179</identifier>
 <datestamp>2013-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0179</id><created>2013-01-02</created><authors><author><keyname>Doreswamy</keyname></author><author><keyname>Hemanth</keyname><forenames>K. S.</forenames></author></authors><title>A Novel Design Specification Distance(DSD) Based K-Mean Clustering
  Performace Evluation on Engineering Materials Database</title><categories>cs.LG</categories><comments>International Journal of Computer Applications Vol.55(15), pp.26-33,
  October 2012. Published by Foundation of Computer Science, New York, USA</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Organizing data into semantically more meaningful is one of the fundamental
modes of understanding and learning. Cluster analysis is a formal study of
methods for understanding and algorithm for learning. K-mean clustering
algorithm is one of the most fundamental and simple clustering algorithms. When
there is no prior knowledge about the distribution of data sets, K-mean is the
first choice for clustering with an initial number of clusters. In this paper a
novel distance metric called Design Specification (DS) distance measure
function is integrated with K-mean clustering algorithm to improve cluster
accuracy. The K-means algorithm with proposed distance measure maximizes the
cluster accuracy to 99.98% at P = 1.525, which is determined through the
iterative procedure. The performance of Design Specification (DS) distance
measure function with K - mean algorithm is compared with the performances of
other standard distance functions such as Euclidian, squared Euclidean, City
Block, and Chebshew similarity measures deployed with K-mean algorithm.The
proposed method is evaluated on the engineering materials database. The
experiments on cluster analysis and the outlier profiling show that these is an
excellent improvement in the performance of the proposed method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0181</identifier>
 <datestamp>2013-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0181</id><created>2013-01-02</created><authors><author><keyname>Kocan</keyname><forenames>Fatih</forenames></author></authors><title>A nonenumerative algorithm to find the k longest (shortest) paths in a
  DAG</title><categories>cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we present a novel and efficient algorithm to find the k
longest (shortest) paths between sources and sinks in a directed acyclic graph
(DAG). The algorithm does not enumerate paths therefore it is especially useful
for very large k values. It is based on the Valued-Sum-of-Product (VSOP) tool,
which is an extension of Zero-suppressed Binary Decision Diagrams (ZBDDs). We
assessed the performance of this algorithm with a DAG model of a path-intensive
combinational circuit, viz. c6288, that has \sim10^{20} paths. We found that it
took about 64 minutes to compute all paths in this DAG along with their
lengths.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0188</identifier>
 <datestamp>2013-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0188</id><created>2013-01-02</created><authors><author><keyname>Virmani</keyname><forenames>Deepali</forenames></author><author><keyname>Jain</keyname><forenames>Satbir</forenames></author></authors><title>Reliable Robust and Real-Time Communication Protocol for Data Delivery
  in Wireless sensor Networks</title><categories>cs.NI</categories><comments>15 pages, 6 figures</comments><journal-ref>International Journal of Information Technology and Knowledge
  Management 2011, Volume 4, No. 2, pp. 595-601</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  WSNs can be considered a distributed control system designed to react to
sensor information with an effective and timely action. For this reason, in
WSNs it is important to provide real-time coordination and communication to
guarantee timely execution of the right actions. In this paper a new
communication protocol RRRT to support robust real-time and reliable event data
delivery with minimum energy consumption and with congestion avoidance in WSNs
is proposed. The proposed protocol uses the fault tolerant optimal path for
data delivery. The proposed solution dynamically adjust their protocol
configurations to adapt to the heterogeneous characteristics of WSNs.
Specifically, the interactions between contention resolution and congestion
control mechanisms as well as the physical layer effects in WSNs are
investigated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0189</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0189</id><created>2013-01-02</created><updated>2013-05-19</updated><authors><author><keyname>Hu</keyname><forenames>Haibo</forenames></author><author><keyname>Guo</keyname><forenames>Jinli</forenames></author><author><keyname>Liu</keyname><forenames>Xuan</forenames></author></authors><title>A generalized theory of preferential linking</title><categories>physics.soc-ph cs.SI</categories><comments>18 pages, 6 figures, 2 tables</comments><doi>10.1016/j.physa.2014.08.026</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are diverse mechanisms driving the evolution of social networks. A key
open question dealing with understanding their evolution is: How various
preferential linking mechanisms produce networks with different features? In
this paper we first empirically study preferential linking phenomena in an
evolving online social network, find and validate the linear preference. We
propose an analyzable model which captures the real growth process of the
network and reveals the underlying mechanism dominating its evolution.
Furthermore based on preferential linking we propose a generalized model
reproducing the evolution of online social networks, present unified analytical
results describing network characteristics for 27 preference scenarios, and
explore the relation between preferential linking mechanism and network
features. We find that within the framework of preferential linking analytical
degree distributions can only be the combinations of finite kinds of functions
which are related to rational, logarithmic and inverse tangent functions, and
extremely complex network structure will emerge even for very simple sublinear
preferential linking. This work not only provides a verifiable origin for the
emergence of various network characteristics in social networks, but bridges
the micro individuals' behaviors and the global organization of social
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0207</identifier>
 <datestamp>2013-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0207</id><created>2013-01-02</created><authors><author><keyname>Agnihotri</keyname><forenames>Samar</forenames></author><author><keyname>Venkatachalapathy</keyname><forenames>Rajesh</forenames></author></authors><title>Worst-case Asymmetric Distributed Source Coding</title><categories>cs.IT math.IT</categories><comments>22 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider a worst-case asymmetric distributed source coding problem where
an information sink communicates with $N$ correlated information sources to
gather their data. A data-vector $\bar{x} = (x_1, ..., x_N) \sim {\mathcal P}$
is derived from a discrete and finite joint probability distribution ${\mathcal
P} = p(x_1, ..., x_N)$ and component $x_i$ is revealed to the $i^{\textrm{th}}$
source, $1 \le i \le N$. We consider an asymmetric communication scenario where
only the sink is assumed to know distribution $\mathcal P$. We are interested
in computing the minimum number of bits that the sources must send, in the
worst-case, to enable the sink to losslessly learn any $\bar{x}$ revealed to
the sources.
  We propose a novel information measure called information ambiguity to
perform the worst-case information-theoretic analysis and prove its various
properties. Then, we provide interactive communication protocols to solve the
above problem in two different communication scenarios. We also investigate the
role of block-coding in the worst-case analysis of distributed compression
problem and prove that it offers almost no compression advantage compared to
the scenarios where this problem is addressed, as in this paper, with only a
single instance of data-vector.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0213</identifier>
 <datestamp>2014-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0213</id><created>2013-01-02</created><updated>2013-11-07</updated><authors><author><keyname>Arildsen</keyname><forenames>Thomas</forenames></author><author><keyname>Larsen</keyname><forenames>Torben</forenames></author></authors><title>Compressed Sensing with Linear Correlation Between Signal and
  Measurement Noise</title><categories>cs.IT math.IT</categories><comments>37 pages, 5 figures. Accepted for publication in EURASIP Signal
  Processing Accompanying Matlab code available at:
  https://github.com/ThomasA/cs-correlated-noise</comments><doi>10.1016/j.sigpro.2013.10.021</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Existing convex relaxation-based approaches to reconstruction in compressed
sensing assume that noise in the measurements is independent of the signal of
interest. We consider the case of noise being linearly correlated with the
signal and introduce a simple technique for improving compressed sensing
reconstruction from such measurements. The technique is based on a linear model
of the correlation of additive noise with the signal. The modification of the
reconstruction algorithm based on this model is very simple and has negligible
additional computational cost compared to standard reconstruction algorithms,
but is not known in existing literature. The proposed technique reduces
reconstruction error considerably in the case of linearly correlated
measurements and noise. Numerical experiments confirm the efficacy of the
technique. The technique is demonstrated with application to low-rate
quantization of compressed measurements, which is known to introduce correlated
noise, and improvements in reconstruction error compared to ordinary Basis
Pursuit De-Noising of up to approximately 7 dB are observed for 1 bit/sample
quantization. Furthermore, the proposed method is compared to Binary Iterative
Hard Thresholding which it is demonstrated to outperform in terms of
reconstruction error for sparse signals with a number of non-zero coefficients
greater than approximately 1/10th of the number of compressed measurements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0216</identifier>
 <datestamp>2013-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0216</id><created>2013-01-02</created><authors><author><keyname>Hrn&#x10d;&#xed;&#x159;</keyname><forenames>Jan</forenames></author><author><keyname>Rovatsos</keyname><forenames>Michael</forenames></author></authors><title>Applying Strategic Multiagent Planning to Real-World Travel Sharing
  Problems</title><categories>cs.AI</categories><comments>7th International Workshop on Agents in Traffic and Transportation,
  AAMAS, 2012</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Travel sharing, i.e., the problem of finding parts of routes which can be
shared by several travellers with different points of departure and
destinations, is a complex multiagent problem that requires taking into account
individual agents' preferences to come up with mutually acceptable joint plans.
In this paper, we apply state-of-the-art planning techniques to real-world
public transportation data to evaluate the feasibility of multiagent planning
techniques in this domain. The potential application value of improving travel
sharing technology has great application value due to its ability to reduce the
environmental impact of travelling while providing benefits to travellers at
the same time. We propose a three-phase algorithm that utilises performant
single-agent planners to find individual plans in a simplified domain first,
then merges them using a best-response planner which ensures resulting
solutions are individually rational, and then maps the resulting plan onto the
full temporal planning domain to schedule actual journeys. The evaluation of
our algorithm on real-world, multi-modal public transportation data for the
United Kingdom shows linear scalability both in the scenario size and in the
number of agents, where trade-offs have to be made between total cost
improvement, the percentage of feasible timetables identified for journeys, and
the prolongation of these journeys. Our system constitutes the first
implementation of strategic multiagent planning algorithms in large-scale
domains and provides insights into the engineering process of translating
general domain-independent multiagent planning algorithms to real-world
applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0239</identifier>
 <datestamp>2013-01-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0239</id><created>2013-01-02</created><updated>2013-01-03</updated><authors><author><keyname>Aldecoa</keyname><forenames>Rodrigo</forenames></author><author><keyname>Mar&#xed;n</keyname><forenames>Ignacio</forenames></author></authors><title>Surprise maximization reveals the community structure of complex
  networks</title><categories>cs.SI cond-mat.stat-mech physics.soc-ph q-bio.MN</categories><comments>8 pages, 6 figures, 2 extra tables. Accepted for publication in
  Scientific Reports</comments><journal-ref>Scientific Reports 3, 1060 (2013)</journal-ref><doi>10.1038/srep01060</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  How to determine the community structure of complex networks is an open
question. It is critical to establish the best strategies for community
detection in networks of unknown structure. Here, using standard synthetic
benchmarks, we show that none of the algorithms hitherto developed for
community structure characterization perform optimally. Significantly,
evaluating the results according to their modularity, the most popular measure
of the quality of a partition, systematically provides mistaken solutions.
However, a novel quality function, called Surprise, can be used to elucidate
which is the optimal division into communities. Consequently, we show that the
best strategy to find the community structure of all the networks examined
involves choosing among the solutions provided by multiple algorithms the one
with the highest Surprise value. We conclude that Surprise maximization
precisely reveals the community structure of complex networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0254</identifier>
 <datestamp>2013-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0254</id><created>2012-12-27</created><updated>2013-05-03</updated><authors><author><keyname>Clark</keyname><forenames>Andrew</forenames></author></authors><title>Group theory, group actions, evolutionary algorithms, and global
  optimization</title><categories>cs.NE math.DS math.OC math.RA</categories><comments>19 pages</comments><msc-class>90-02, 90-08, 90B99, 37D05</msc-class><acm-class>F.2.1; G.1.6; G.2.0; I.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we use group, action and orbit to understand how evolutionary
solve nonconvex optimization problems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0259</identifier>
 <datestamp>2013-06-28</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0259</id><created>2013-01-02</created><updated>2013-06-27</updated><authors><author><keyname>Klimek</keyname><forenames>Peter</forenames></author><author><keyname>Thurner</keyname><forenames>Stefan</forenames></author></authors><title>Triadic closure dynamics drives scaling-laws in social multiplex
  networks</title><categories>physics.soc-ph cs.SI physics.data-an</categories><comments>5 pages, 3 figures</comments><journal-ref>New J. Phys. 15 063008 (2013)</journal-ref><doi>10.1088/1367-2630/15/6/063008</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Social networks exhibit scaling-laws for several structural characteristics,
such as the degree distribution, the scaling of the attachment kernel, and the
clustering coefficients as a function of node degree. A detailed understanding
if and how these scaling laws are inter-related is missing so far, let alone
whether they can be understood through a common, dynamical principle. We
propose a simple model for stationary network formation and show that the three
mentioned scaling relations follow as natural consequences of triadic closure.
The validity of the model is tested on multiplex data from a well studied
massive multiplayer online game. We find that the three scaling exponents
observed in the multiplex data for the friendship, communication and trading
networks can simultaneously be explained by the model. These results suggest
that triadic closure could be identified as one of the fundamental dynamical
principles in social multiplex network formation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0265</identifier>
 <datestamp>2013-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0265</id><created>2013-01-02</created><authors><author><keyname>Ghezaiel</keyname><forenames>Wajdi</forenames></author><author><keyname>Slimane</keyname><forenames>Amel Ben</forenames></author><author><keyname>Braiek</keyname><forenames>Ezzedine Ben</forenames></author></authors><title>Usable Speech Assignment for Speaker Identification under Co-Channel
  Situation</title><categories>cs.SD</categories><comments>International Journal of Computer Applications (IJCA)</comments><journal-ref>International Journal of Computer Applications 59(18):7-11, 2012</journal-ref><doi>10.5120/9646-4381</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Usable speech criteria are proposed to extract minimally corrupted speech for
speaker identification (SID) in co-channel speech. In co-channel speech, either
speaker can randomly appear as the stronger speaker or the weaker one at a
time. Hence, the extracted usable segments are separated in time and need to be
organized into speaker streams for SID. In this paper, we focus to organize
extracted usable speech segment into a single stream for the same speaker by
speaker assignment system. For this, we develop model-based speaker assignment
method based on posterior probability and exhaustive search algorithm.
Evaluation of this method is performed on TIMIT database. The system is
evaluated on co-channel speech and results show a significant improvement.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0278</identifier>
 <datestamp>2013-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0278</id><created>2013-01-02</created><authors><author><keyname>Ghezaiel</keyname><forenames>Wajdi</forenames></author><author><keyname>Rahmouni</keyname><forenames>Amel Ben Slimane</forenames></author><author><keyname>Braiek</keyname><forenames>Ezzedine Ben</forenames></author></authors><title>Evaluation of a Multi-Resolution Dyadic Wavelet Transform Method for
  usable Speech Detection</title><categories>cs.SD</categories><journal-ref>WASET Journal, 2011 Vol.79 P. 829-833</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many applications of speech communication and speaker identification suffer
from the problem of co-channel speech. This paper deals with a multi-resolution
dyadic wavelet transform method for usable segments of co-channel speech
detection that could be processed by a speaker identification system.
Evaluation of this method is performed on TIMIT database referring to the
Target to Interferer Ratio measure. Co-channel speech is constructed by mixing
all possible gender speakers. Results do not show much difference for different
mixtures. For the overall mixtures 95.76% of usable speech is correctly
detected with false alarms of 29.65%.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0282</identifier>
 <datestamp>2013-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0282</id><created>2013-01-02</created><authors><author><keyname>Hod</keyname><forenames>Rani</forenames></author><author><keyname>Naor</keyname><forenames>Alon</forenames></author></authors><title>Component Games on Regular Graphs</title><categories>math.CO cs.DM</categories><comments>10 pages</comments><msc-class>91A24, 68R10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the (1:b) Maker-Breaker component game, played on the edge set of a
d-regular graph. Maker's aim in this game is to build a large connected
component, while Breaker's aim is to not let him do so. For all values of
Breaker's bias b, we determine whether Breaker wins (on any d-regular graph) or
Maker wins (on almost every d-regular graph) and provide explicit winning
strategies for both players.
  To this end, we prove an extension of a theorem by Gallai-Hasse-Roy-Vitaver
about graph orientations without long directed simple paths.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0289</identifier>
 <datestamp>2013-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0289</id><created>2012-12-24</created><authors><author><keyname>Prakash</keyname><forenames>Aaditya</forenames></author></authors><title>Reconstructing Self Organizing Maps as Spider Graphs for better visual
  interpretation of large unstructured datasets</title><categories>cs.GR stat.ML</categories><comments>9 pages, 8 figures</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  Self-Organizing Maps (SOM) are popular unsupervised artificial neural network
used to reduce dimensions and visualize data. Visual interpretation from
Self-Organizing Maps (SOM) has been limited due to grid approach of data
representation, which makes inter-scenario analysis impossible. The paper
proposes a new way to structure SOM. This model reconstructs SOM to show
strength between variables as the threads of a cobweb and illuminate
inter-scenario analysis. While Radar Graphs are very crude representation of
spider web, this model uses more lively and realistic cobweb representation to
take into account the difference in strength and length of threads. This model
allows for visualization of highly unstructured dataset with large number of
dimensions, common in Bigdata sources.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0297</identifier>
 <datestamp>2013-01-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0297</id><created>2013-01-02</created><authors><author><keyname>Vaezi</keyname><forenames>Mojtaba</forenames></author><author><keyname>Labeau</keyname><forenames>Fabrice</forenames></author></authors><title>Wyner-Ziv Coding in the Real Field Based on BCH-DFT Codes</title><categories>cs.IT math.IT</categories><comments>23 pages, 7 figures, submitted to IEEE Transactions on Signal
  Processing. arXiv admin note: text overlap with arXiv:1111.0654</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show how real-number codes can be used to compress correlated sources and
establish a new framework for distributed lossy source coding, in which we
quantize compressed sources instead of compressing quantized sources. This
change in the order of binning and quantization blocks makes it possible to
model correlation between continuous-valued sources more realistically and
compensate for the quantization error when the sources are completely
correlated. We focus on the asymmetric case, i.e., lossy source coding with
side information at the decoder, also known as Wyner-Ziv coding. The encoding
and decoding procedures are described in detail for discrete Fourier transform
(DFT) codes, both for syndrome- and parity-based approaches. We also extend the
parity-based approach to the case where the transmission channel is noisy and
perform distributed joint source-channel coding in this context. The proposed
system is well suited for low-delay communications. Furthermore, the
mean-squared reconstruction error (MSE) is shown to be less than or close to
the quantization error level, the ideal case in coding based on binary codes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0302</identifier>
 <datestamp>2013-01-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0302</id><created>2013-01-02</created><updated>2013-01-18</updated><authors><author><keyname>Shakarian</keyname><forenames>Paulo</forenames></author><author><keyname>Simari</keyname><forenames>Gerardo I.</forenames></author><author><keyname>Schroeder</keyname><forenames>Robert</forenames></author></authors><title>MANCaLog: A Logic for Multi-Attribute Network Cascades (Technical
  Report)</title><categories>cs.AI cs.LO cs.MA cs.SI physics.soc-ph</categories><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  The modeling of cascade processes in multi-agent systems in the form of
complex networks has in recent years become an important topic of study due to
its many applications: the adoption of commercial products, spread of disease,
the diffusion of an idea, etc. In this paper, we begin by identifying a
desiderata of seven properties that a framework for modeling such processes
should satisfy: the ability to represent attributes of both nodes and edges, an
explicit representation of time, the ability to represent non-Markovian
temporal relationships, representation of uncertain information, the ability to
represent competing cascades, allowance of non-monotonic diffusion, and
computational tractability. We then present the MANCaLog language, a formalism
based on logic programming that satisfies all these desiderata, and focus on
algorithms for finding minimal models (from which the outcome of cascades can
be obtained) as well as how this formalism can be applied in real world
scenarios. We are not aware of any other formalism in the literature that meets
all of the above requirements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0303</identifier>
 <datestamp>2013-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0303</id><created>2013-01-02</created><updated>2013-01-21</updated><authors><author><keyname>Dujmovic</keyname><forenames>Vida</forenames></author><author><keyname>Morin</keyname><forenames>Pat</forenames></author><author><keyname>Sheffer</keyname><forenames>Adam</forenames></author></authors><title>Crossings in Grid Drawings</title><categories>math.CO cs.CG</categories><comments>16 pages, 3 figures; improved lower-bound in 3-D</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We prove crossing number inequalities for geometric graphs whose vertex sets
are taken from a d-dimensional grid of volume N and give applications of these
inequalities to counting the number of non-crossing geometric graphs that can
be drawn on such grids.
  In particular, we show that any geometric graph with m &gt;= 8N edges and with
vertices on a 3D integer grid of volume N, has \Omega((m^2/n)\log(m/n))
crossings. In d-dimensions, with d &gt;= 4, this bound becomes \Omega(m^2/n). We
provide matching upper bounds for all d. Finally, for d &gt;= 4 the upper bound
implies that the maximum number of crossing-free geometric graphs with vertices
on some d-dimensional grid of volume N is n^\Theta(n). In 3 dimensions it
remains open to improve the trivial bounds, namely, the 2^\Omega(n) lower bound
and the n^O(n) upper bound.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0306</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0306</id><created>2013-01-02</created><updated>2014-03-05</updated><authors><author><keyname>Vinogradova</keyname><forenames>Julia</forenames></author><author><keyname>Couillet</keyname><forenames>Romain</forenames></author><author><keyname>Hachem</keyname><forenames>Walid</forenames></author></authors><title>Statistical Inference in Large Antenna Arrays under Unknown Noise
  Pattern</title><categories>cs.IT math.IT</categories><comments>25 pages, 5 figures</comments><journal-ref>IEEE Transactions on Signal Processing, vol. 61, no. 22, pp.
  5633-5645, 2013</journal-ref><doi>10.1109/TSP.2013.2280443</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article, a general information-plus-noise transmission model is
assumed, the receiver end of which is composed of a large number of sensors and
is unaware of the noise pattern. For this model, and under reasonable
assumptions, a set of results is provided for the receiver to perform
statistical eigen-inference on the information part. In particular, we
introduce new methods for the detection, counting, and the power and subspace
estimation of multiple sources composing the information part of the
transmission. The theoretical performance of some of these techniques is also
discussed. An exemplary application of these methods to array processing is
then studied in greater detail, leading in particular to a novel MUSIC-like
algorithm assuming unknown noise covariance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0313</identifier>
 <datestamp>2014-04-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0313</id><created>2013-01-01</created><authors><author><keyname>Kak</keyname><forenames>Subhash</forenames></author></authors><title>The Piggy Bank Cryptographic Trope</title><categories>cs.CR</categories><comments>7 pages, 6 figures</comments><journal-ref>Infocommunications Journal 6: 22-25 (2014)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents applications of the trope of the locked and sealed
piggy-bank into which the secret can be easily inserted but from which it
cannot be withdrawn without opening the box. We present a basic two-pass
cryptographic scheme that can serve as template for a variety of
implementations. Together with the sealed piggy-bank is sent a coded letter
that lists and certifies the contents of the box. We show how this idea can
help increase the security of cryptographic protocols for classical systems as
well as those based on &quot;single-state&quot; systems. More specifically, we propose
the use of a hashing digest (instead of the coded letter) to detect loss of key
bits to the eavesdropper and use in communication systems where error
correction is an important issue.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0315</identifier>
 <datestamp>2013-01-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0315</id><created>2013-01-02</created><authors><author><keyname>Naik</keyname><forenames>Priyanka</forenames></author><author><keyname>Sanyal</keyname><forenames>Sugata</forenames></author></authors><title>Increasing Security in Cloud Environment</title><categories>cs.CR</categories><comments>8 pages, 1 table</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The concept of cloud computing was introduced to meet the increase in demand
for new application for a project, and to provide a large storage facility
whenever or wherever a user needs it. The cloud system facility helped many
industries as well as individual users to get authentic software at a very low
cost. But with this new system comes the major concern of security, as the
connection to the cloud is through the web and the data and application
availability need to be handled for each client. The paper describes the
various security measures that can be added in isolation or in combination for
securing data transmission, server and client.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0340</identifier>
 <datestamp>2015-06-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0340</id><created>2013-01-02</created><updated>2014-02-14</updated><authors><author><keyname>Bruner</keyname><forenames>Marie-Louise</forenames></author><author><keyname>Lackner</keyname><forenames>Martin</forenames></author></authors><title>The computational landscape of permutation patterns</title><categories>cs.CC math.CO</categories><comments>23 pages, to appear in Journal of Pure and Applied Mathematics,
  Special Issue for Permutation Patterns 2012</comments><journal-ref>Pure Mathematics and Applications, 24 (2): 83-101, 2013</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the last years, different types of patterns in permutations have been
studied: vincular, bivincular and mesh patterns, just to name a few. Every type
of permutation pattern naturally defines a corresponding computational problem:
Given a pattern P and a permutation T (the text), is P contained in T? In this
paper we draw a map of the computational landscape of permutation pattern
matching with different types of patterns. We provide a classical complexity
analysis and investigate the impact of the pattern length on the computational
hardness. Furthermore, we highlight several directions in which the study of
computational aspects of permutation patterns could evolve.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0344</identifier>
 <datestamp>2013-01-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0344</id><created>2013-01-02</created><authors><author><keyname>Rossi</keyname><forenames>Lorenzo</forenames></author><author><keyname>Chakareski</keyname><forenames>Jacob</forenames></author><author><keyname>Frossard</keyname><forenames>Pascal</forenames></author><author><keyname>Colonnese</keyname><forenames>Stefania</forenames></author></authors><title>A Poisson Hidden Markov Model for Multiview Video Traffic</title><categories>cs.MM cs.NI</categories><comments>11 pages, 21 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Multiview video has recently emerged as a means to improve user experience in
novel multimedia services. We propose a new stochastic model to characterize
the traffic generated by a Multiview Video Coding (MVC) variable bit rate
source. To this aim, we resort to a Poisson Hidden Markov Model (P-HMM), in
which the first (hidden) layer represents the evolution of the video activity
and the second layer represents the frame sizes of the multiple encoded views.
We propose a method for estimating the model parameters in long MVC sequences.
We then present extensive numerical simulations assessing the model's ability
to produce traffic with realistic characteristics for a general class of MVC
sequences. We then extend our framework to network applications where we show
that our model is able to accurately describe the sender and receiver buffers
behavior in MVC transmission. Finally, we derive a model of user behavior for
interactive view selection, which, in conjunction with our traffic model, is
able to accurately predict actual network load in interactive multiview
services.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0363</identifier>
 <datestamp>2013-01-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0363</id><created>2013-01-02</created><authors><author><keyname>Srihari</keyname><forenames>Sriganesh</forenames></author><author><keyname>Leong</keyname><forenames>Hon Wai</forenames></author></authors><title>Employing functional interactions for characterization and detection of
  sparse complexes from yeast PPI networks</title><categories>cs.CE q-bio.MN</categories><comments>18 pages, 9 Tables, 1 Figure</comments><msc-class>92-08</msc-class><journal-ref>Int J Bioinform Res Appl. 2012, 8(3-4):286-304</journal-ref><doi>10.1504/IJBRA.2012.048962</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the last few years, several computational techniques have been devised
to recover protein complexes from the protein interaction (PPI) networks of
organisms. These techniques model &quot;dense&quot; subnetworks within PPI networks as
complexes. However, our comprehensive evaluations revealed that these
techniques fail to reconstruct many 'gold standard' complexes that are &quot;sparse&quot;
in the networks (only 71 recovered out of 123 known yeast complexes embedded in
a network of 9704 interactions among 1622 proteins). In this work, we propose a
novel index called Component-Edge (CE) score to quantitatively measure the
notion of &quot;complex derivability&quot; from PPI networks. Using this index, we
theoretically categorize complexes as &quot;sparse&quot; or &quot;dense&quot; with respect to a
given network. We then devise an algorithm SPARC that selectively employs
functional interactions to improve the CE scores of predicted complexes, and
thereby elevates many of the &quot;sparse&quot; complexes to &quot;dense&quot;. This empowers
existing methods to detect these &quot;sparse&quot; complexes. We demonstrate that our
approach is effective in reconstructing significantly many complexes missed
previously (104 recovered out of the 123 known complexes or ~47% improvement).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0369</identifier>
 <datestamp>2013-01-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0369</id><created>2013-01-02</created><authors><author><keyname>Chen</keyname><forenames>Bocong</forenames></author><author><keyname>Fan</keyname><forenames>Yun</forenames></author><author><keyname>Lin</keyname><forenames>Liren</forenames></author><author><keyname>Liu</keyname><forenames>Hongwei</forenames></author></authors><title>Constacyclic Codes over Finite Fields</title><categories>cs.IT math.IT math.NT</categories><journal-ref>Finite Fields and Their Applications 18(2012) 1217-1231</journal-ref><doi>10.1016/j.ffa.2012.10.001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An equivalence relation called isometry is introduced to classify
constacyclic codes over a finite field; the polynomial generators of
constacyclic codes of length $\ell^tp^s$ are characterized, where $p$ is the
characteristic of the finite field and $\ell$ is a prime different from $p$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0373</identifier>
 <datestamp>2013-01-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0373</id><created>2013-01-02</created><authors><author><keyname>Xu</keyname><forenames>Guangwu</forenames></author><author><keyname>Xu</keyname><forenames>Zhiqiang</forenames></author></authors><title>Compressed Sensing Matrices from Fourier Matrices</title><categories>cs.IT math.IT math.NA</categories><comments>17 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The class of Fourier matrices is of special importance in compressed sensing
(CS). This paper concerns deterministic construction of compressed sensing
matrices from Fourier matrices. By using Katz' character sum estimation, we are
able to design a deterministic procedure to select rows from a Fourier matrix
to form a good compressed sensing matrix for sparse recovery. The sparsity
bound in our construction is similar to that of binary CS matrices constructed
by DeVore which greatly improves previous results for CS matrices from Fourier
matrices. Our approach also provides more flexibilities in terms of the
dimension of CS matrices. As a consequence, our construction yields an
approximately mutually unbiased bases from Fourier matrices which is of
particular interest to quantum information theory. This paper also contains a
useful improvement to Katz' character sum estimation for quadratic extensions,
with an elementary and transparent proof. Some numerical examples are included.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0379</identifier>
 <datestamp>2013-01-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0379</id><created>2013-01-03</created><updated>2013-01-17</updated><authors><author><keyname>Arvind</keyname><forenames>Vikraman</forenames></author></authors><title>The Parameterized Complexity of some Permutation Group Problems</title><categories>cs.CC cs.DM math.CO</categories><comments>In the revised version the FPT algorithm for computing a base of size
  k is only for cyclic permutation groups and not for all abelian permutation
  groups</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study the parameterized complexity of two well-known
permutation group problems which are NP-complete. 1. Given a permutation group
G=&lt;S&gt;, subgroup of $S_n$, and a parameter $k$, find a permutation $\pi$ in G
such that $|{i\in [n]\mid \pi(i)\ne i}|$ is at least $k$. This generalizes the
well-known NP-complete problem of finding a fixed-point free permutation in G.
(this is the case when $k=n$). We show that this problem with parameter $k$ is
fixed parameter tractable. In the process, we give a simple deterministic
polynomial-time algorithm for finding a fixed point free element in a
transitive permutation group, answering an open question of Cameron. 2. Next we
consider the problem of computing a base for a permutation group G=&lt;S&gt;. A base
for G is a subset B of $[n]$ such that the subgroup of G that fixes B pointwise
is trivial. This problem is known to be NP-complete. We show that it is fixed
parameter tractable for the case of cyclic permutation groups and for
permutation groups of constant orbit size. For more general classes of
permutation groups we do not know whether the problem is in FPT or is
W[1]-hard.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0384</identifier>
 <datestamp>2013-01-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0384</id><created>2013-01-03</created><authors><author><keyname>Bao</keyname><forenames>Vo Nguyen Quoc</forenames></author><author><keyname>Thanh</keyname><forenames>Tran Thien</forenames></author><author><keyname>Nguyen</keyname><forenames>Tuan Duc</forenames></author><author><keyname>Vu</keyname><forenames>Thanh Dinh</forenames></author></authors><title>Spectrum Sharing-based Multi-hop Decode-and-Forward Relay Networks under
  Interference Constraints: Performance Analysis and Relay Position
  Optimization</title><categories>cs.IT math.IT</categories><comments>11 pages, 8 figures, accepted on &quot;Journal of Communications and
  Networks&quot;</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The exact closed-form expressions for outage probability and bit error rate
of spectrum sharing-based multi-hop decodeand- forward (DF) relay networks in
non-identical Rayleigh fading channels are derived. We also provide the
approximate closed-form expression for the system ergodic capacity. Utilizing
these tractable analytical formulas, we can study the impact of key network
parameters on the performance of cognitivemulti-hop relay networks under
interference constraints. Using a linear network model, we derive an optimum
relay position scheme by numerically solving an optimization problem of
balancing average signal-to-noise ratio (SNR) of each hop. The numerical
results show that the optimal scheme leads to SNR performance gains of more
than 1 dB. All the analytical expressions are verified by Monte-Carlo
simulations confirming the advantage ofmultihop DF relaying networks in
cognitive environments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0387</identifier>
 <datestamp>2013-10-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0387</id><created>2013-01-03</created><updated>2013-10-20</updated><authors><author><keyname>Chen</keyname><forenames>ShengYao</forenames></author><author><keyname>Xi</keyname><forenames>Feng</forenames></author><author><keyname>Liu</keyname><forenames>Zhong</forenames></author></authors><title>Chaotic Analog-to-Information Conversion with Chaotic State Modulation</title><categories>cs.IT math.IT</categories><comments>24 pages, 10 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Chaotic compressive sensing is a nonlinear framework for compressive sensing.
Along the framework, this paper proposes a chaotic analog-to-information
converter, chaotic modulation, to acquire and reconstruct band-limited sparse
analog signals at sub-Nyquist rate. In the chaotic modulation, the sparse
signal is randomized through state modulation of continuous-time chaotic system
and one state output is sampled as compressive measurements. The reconstruction
is achieved through the estimation of the sparse coefficients with principle of
chaotic impulsive synchronization and Lp-norm regularized nonlinear least
squares. The concept of supreme local Lyapunov exponents (SLLE) is introduced
to study the reconstructablity. It is found that the sparse signals are
reconstructable, if the largest SLLE of the error dynamical system is negative.
As examples, the Lorenz system and Liu system excited by the sparse multi-tone
signals are taken to illustrate the principle and the performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0401</identifier>
 <datestamp>2013-01-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0401</id><created>2013-01-03</created><authors><author><keyname>Fu</keyname><forenames>Hu</forenames></author><author><keyname>Hartline</keyname><forenames>Jason</forenames></author><author><keyname>Hoy</keyname><forenames>Darrell</forenames></author></authors><title>Prior-independent Auctions for Risk-averse Agents</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study simple and approximately optimal auctions for agents with a
particular form of risk-averse preferences. We show that, for symmetric agents,
the optimal revenue (given a prior distribution over the agent preferences) can
be approximated by the first-price auction (which is prior independent), and,
for asymmetric agents, the optimal revenue can be approximated by an auction
with simple form. These results are based on two technical methods. The first
is for upper-bounding the revenue from a risk-averse agent. The second gives a
payment identity for mechanisms with pay-your-bid semantics.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0427</identifier>
 <datestamp>2013-04-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0427</id><created>2013-01-03</created><updated>2013-04-28</updated><authors><author><keyname>Manin</keyname><forenames>Yuri I.</forenames></author></authors><title>Zipf's law and L. Levin's probability distributions</title><categories>cs.IT math.IT</categories><comments>19 pages</comments><msc-class>97M99</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Zipf's law in its basic incarnation is an empirical probability distribution
governing the frequency of usage of words in a language. As Terence Tao
recently remarked, it still lacks a convincing and satisfactory mathematical
explanation.
  In this paper I suggest that at least in certain situations, Zipf's law can
be explained as a special case of the a priori distribution introduced and
studied by L. Levin. The Zipf ranking corresponding to diminishing probability
appears then as the ordering determined by the growing Kolmogorov complexity.
  One argument justifying this assertion is the appeal to a recent
interpretation by Yu. Manin and M. Marcolli of asymptotic bounds for
error--correcting codes in terms of phase transition. In the respective
partition function, Kolmogorov complexity of a code plays the role of its
energy.
  This version contains minor corrections and additions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0432</identifier>
 <datestamp>2013-01-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0432</id><created>2013-01-03</created><authors><author><keyname>Mahmood</keyname><forenames>F.</forenames></author><author><keyname>Kunwar</keyname><forenames>F.</forenames></author></authors><title>A Self-Organizing Neural Scheme for Door Detection in Different
  Environments</title><categories>cs.CV</categories><comments>Page No 13-18, 7 figures, Published with International Journal of
  Computer Applications (IJCA)</comments><journal-ref>International Journal of Computer Applications 60(9):13-18, 2012</journal-ref><doi>10.5120/9719-3679</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Doors are important landmarks for indoor mobile robot navigation and also
assist blind people to independently access unfamiliar buildings. Most existing
algorithms of door detection are limited to work for familiar environments
because of restricted assumptions about color, texture and shape. In this paper
we propose a novel approach which employs feature based classification and uses
the Kohonen Self-Organizing Map (SOM) for the purpose of door detection.
Generic and stable features are used for the training of SOM that increase the
performance significantly: concavity, bottom-edge intensity profile and door
edges. To validate the robustness and generalizability of our method, we
collected a large dataset of real world door images from a variety of
environments and different lighting conditions. The algorithm achieves more
than 95% detection which demonstrates that our door detection method is generic
and robust with variations of color, texture, occlusions, lighting condition,
scales, and viewpoints.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0435</identifier>
 <datestamp>2013-01-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0435</id><created>2013-01-03</created><authors><author><keyname>Mahmood</keyname><forenames>F.</forenames></author><author><keyname>Haider</keyname><forenames>Syed. M. B.</forenames></author><author><keyname>Kunwar</keyname><forenames>F.</forenames></author></authors><title>Investigating the performance of Correspondence Algorithms in Vision
  based Driver-assistance in Indoor Environment</title><categories>cs.CV cs.RO</categories><comments>7 pages, 9 figures,Published with International Journal of Computer
  Applications (IJCA)</comments><journal-ref>IJCA 60(9):6-12, 2012</journal-ref><doi>10.5120/9718-3663</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents the experimental comparison of fourteen stereo matching
algorithms in variant illumination conditions. Different adaptations of global
and local stereo matching techniques are chosen for evaluation The variant
strength and weakness of the chosen correspondence algorithms are explored by
employing the methodology of the prediction error strategy. The algorithms are
gauged on the basis of their performance on real world data set taken in
various indoor lighting conditions and at different times of the day
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0450</identifier>
 <datestamp>2014-02-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0450</id><created>2013-01-03</created><updated>2014-02-24</updated><authors><author><keyname>Wang</keyname><forenames>Guanghui</forenames></author><author><keyname>Wang</keyname><forenames>Tao</forenames></author><author><keyname>Liu</keyname><forenames>Guizhen</forenames></author></authors><title>Long properly colored cycles in edge colored complete graphs</title><categories>math.CO cs.DM</categories><comments>8 pages</comments><journal-ref>Discrete Mathematics, 324 (2014) 56--61</journal-ref><doi>10.1016/j.disc.2014.02.003</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Let $K_{n}^{c}$ denote a complete graph on $n$ vertices whose edges are
colored in an arbitrary way. Let $\Delta^{\mathrm{mon}} (K_{n}^{c})$ denote the
maximum number of edges of the same color incident with a vertex of
$K_{n}^{c}$. A properly colored cycle (path) in $K_{n}^{c}$ is a cycle (path)
in which adjacent edges have distinct colors. B. Bollob\'{a}s and P. Erd\&quot;{o}s
(1976) proposed the following conjecture: if $\Delta^{\mathrm{mon}}
(K_{n}^{c})&lt;\lfloor \frac{n}{2} \rfloor$, then $K_{n}^{c}$ contains a properly
colored Hamiltonian cycle. Li, Wang and Zhou proved that if
$\Delta^{\mathrm{mon}} (K_{n}^{c})&lt; \lfloor \frac{n}{2} \rfloor$, then
$K_{n}^{c}$ contains a properly colored cycle of length at least $\lceil
\frac{n+2}{3}\rceil+1$. In this paper, we improve the bound to $\lceil
\frac{n}{2}\rceil + 2$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0460</identifier>
 <datestamp>2013-01-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0460</id><created>2013-01-03</created><authors><author><keyname>Wang</keyname><forenames>Tao</forenames></author><author><keyname>Wang</keyname><forenames>Ping</forenames></author><author><keyname>Yu</keyname><forenames>Qinglin</forenames></author></authors><title>On Murty-Simon Conjecture II</title><categories>math.CO cs.DM</categories><comments>9 pages, submitted for publication on May 10, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A graph is diameter two edge-critical if its diameter is two and the deletion
of any edge increases the diameter. Murty and Simon conjectured that the number
of edges in a diameter two edge-critical graph on $n$ vertices is at most
$\lfloor \frac{n^{2}}{4} \rfloor$ and the extremal graph is the complete
bipartite graph $K_{\lfloor \frac{n}{2} \rfloor, \lceil \frac{n}{2} \rceil}$.
In the series papers [7-9], the Murty-Simon Conjecture stated by Haynes et al.
is not the original conjecture, indeed, it is only for the diameter two
edge-critical graphs of even order. In this paper, we completely prove the
Murty-Simon Conjecture for the graphs whose complements have vertex
connectivity $\ell$, where $\ell = 1, 2, 3$; and for the graphs whose
complements have an independent vertex cut of cardinality at least three.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0476</identifier>
 <datestamp>2013-01-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0476</id><created>2013-01-03</created><authors><author><keyname>Andrews</keyname><forenames>Matthew</forenames></author><author><keyname>Zhang</keyname><forenames>Lisa</forenames></author></authors><title>Energy-Delay Tradeoffs in a Load-Balanced Router</title><categories>cs.NI</categories><comments>Proceedings of the 50th Annual Allerton Conference on Communication,
  Control, and Computing - 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Load-Balanced Router architecture has received a lot of attention because
it does not require centralized scheduling at the internal switch fabrics. In
this paper we reexamine the architecture, motivated by its potential to turn
off multiple components and thereby conserve energy in the presence of low
traffic.
  We perform a detailed analysis of the queue and delay performance of a
Load-Balanced Router under a simple random routing algorithm. We calculate
probabilistic bounds for queue size and delay, and show that the probabilities
drop exponentially with increasing queue size or delay. We also demonstrate a
tradeoff in energy consumption against the queue and delay performance.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0503</identifier>
 <datestamp>2013-01-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0503</id><created>2013-01-03</created><authors><author><keyname>Castella</keyname><forenames>Quim</forenames></author><author><keyname>Sutton</keyname><forenames>Charles</forenames></author></authors><title>Word Storms: Multiples of Word Clouds for Visual Comparison of Documents</title><categories>cs.IR cs.DL cs.HC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Word clouds are a popular tool for visualizing documents, but they are not a
good tool for comparing documents, because identical words are not presented
consistently across different clouds. We introduce the concept of word storms,
a visualization tool for analysing corpora of documents. A word storm is a
group of word clouds, in which each cloud represents a single document,
juxtaposed to allow the viewer to compare and contrast the documents. We
present a novel algorithm that creates a coordinated word storm, in which words
that appear in multiple documents are placed in the same location, using the
same color and orientation, in all of the corresponding clouds. In this way,
similar documents are represented by similar-looking word clouds, making them
easier to compare and contrast visually. We evaluate the algorithm in two ways:
first, an automatic evaluation based on document classification; and second, a
user study. The results confirm that unlike standard word clouds, a coordinated
word storm better allows for visual comparison of documents.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0523</identifier>
 <datestamp>2013-01-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0523</id><created>2013-01-03</created><authors><author><keyname>L'Orphelin</keyname><forenames>Cyril</forenames><affiliation>CC IN2P3</affiliation></author><author><keyname>Cordier</keyname><forenames>H&#xe9;l&#xe8;ne</forenames><affiliation>CC IN2P3</affiliation></author><author><keyname>Reynaud</keyname><forenames>Sylvain</forenames><affiliation>CC IN2P3</affiliation></author><author><keyname>Lins</keyname><forenames>Marcos</forenames><affiliation>CC IN2P3</affiliation></author><author><keyname>Loikkanen</keyname><forenames>Sinika</forenames><affiliation>CC IN2P3</affiliation></author><author><keyname>Lequeux</keyname><forenames>Olivier</forenames><affiliation>CC IN2P3</affiliation></author><author><keyname>Veyre</keyname><forenames>Pierre</forenames><affiliation>CC IN2P3</affiliation></author></authors><title>EELA Operations: A standalone regional dashboard implementation</title><categories>cs.SE cs.DC</categories><comments>Second EELA-2 Conference, CHORONI : Venezuela, Bolivarian Republic Of
  (2009)</comments><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Grid operators in EGEE use a dedicated dashboard as their central operational
tool, stable and scalable for the last 5 years despite continuous upgrade from
specifications by users, monitoring tools or data providers. In EGEE-III,
regionalisation of operations led the tool developers to conceive a standalone
instance of this tool. Hereby, we will present the concept and the EELA-II
implementation. Indeed, there-engineering of this tool led to an easily
deployable package that canconnect to EELA-II specific information sources such
as EVENTUM, EELAGOCDB like or SAM EELA instance through the three components of
thepackage: the generic and scalable data access mechanism, Lavoisier;
thewidely spread php framework Symfony, for configuration flexibility and a
Mysql database.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0528</identifier>
 <datestamp>2013-01-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0528</id><created>2013-01-03</created><authors><author><keyname>Huang</keyname><forenames>Yingsong</forenames></author><author><keyname>Mao</keyname><forenames>Shiwen</forenames></author><author><keyname>Nelms</keyname><forenames>R. M.</forenames></author></authors><title>Adaptive Electricity Scheduling in Microgrids</title><categories>cs.SY</categories><comments>12 pages, extended technical report</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Microgrid (MG) is a promising component for future smart grid (SG)
deployment. The balance of supply and demand of electric energy is one of the
most important requirements of MG management. In this paper, we present a novel
framework for smart energy management based on the concept of
quality-of-service in electricity (QoSE). Specifically, the resident
electricity demand is classified into basic usage and quality usage. The basic
usage is always guaranteed by the MG, while the quality usage is controlled
based on the MG state. The microgrid control center (MGCC) aims to minimize the
MG operation cost and maintain the outage probability of quality usage, i.e.,
QoSE, below a target value, by scheduling electricity among renewable energy
resources, energy storage systems, and macrogrid. The problem is formulated as
a constrained stochastic programming problem. The Lyapunov optimization
technique is then applied to derive an adaptive electricity scheduling
algorithm by introducing the QoSE virtual queues and energy storage virtual
queues. The proposed algorithm is an online algorithm since it does not require
any statistics and future knowledge of the electricity supply, demand and price
processes. We derive several &quot;hard&quot; performance bounds for the proposed
algorithm, and evaluate its performance with trace-driven simulations. The
simulation results demonstrate the efficacy of the proposed electricity
scheduling algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0534</identifier>
 <datestamp>2015-03-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0534</id><created>2013-01-03</created><updated>2013-01-17</updated><authors><author><keyname>de Rooij</keyname><forenames>Steven</forenames></author><author><keyname>van Erven</keyname><forenames>Tim</forenames></author><author><keyname>Gr&#xfc;nwald</keyname><forenames>Peter D.</forenames></author><author><keyname>Koolen</keyname><forenames>Wouter M.</forenames></author></authors><title>Follow the Leader If You Can, Hedge If You Must</title><categories>cs.LG stat.ML</categories><comments>under submission</comments><journal-ref>Journal of Machine Learning Research, vol 15, p. 1281-1316, 2014</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Follow-the-Leader (FTL) is an intuitive sequential prediction strategy that
guarantees constant regret in the stochastic setting, but has terrible
performance for worst-case data. Other hedging strategies have better
worst-case guarantees but may perform much worse than FTL if the data are not
maximally adversarial. We introduce the FlipFlop algorithm, which is the first
method that provably combines the best of both worlds.
  As part of our construction, we develop AdaHedge, which is a new way of
dynamically tuning the learning rate in Hedge without using the doubling trick.
AdaHedge refines a method by Cesa-Bianchi, Mansour and Stoltz (2007), yielding
slightly improved worst-case guarantees. By interleaving AdaHedge and FTL, the
FlipFlop algorithm achieves regret within a constant factor of the FTL regret,
without sacrificing AdaHedge's worst-case guarantees.
  AdaHedge and FlipFlop do not need to know the range of the losses in advance;
moreover, unlike earlier methods, both have the intuitive property that the
issued weights are invariant under rescaling and translation of the losses. The
losses are also allowed to be negative, in which case they may be interpreted
as gains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0541</identifier>
 <datestamp>2014-12-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0541</id><created>2013-01-03</created><updated>2014-12-26</updated><authors><author><keyname>Krawczyk</keyname><forenames>Tomasz</forenames></author><author><keyname>Pawlik</keyname><forenames>Arkadiusz</forenames></author><author><keyname>Walczak</keyname><forenames>Bartosz</forenames></author></authors><title>Coloring triangle-free rectangle overlap graphs with $O(\log\log n)$
  colors</title><categories>cs.CG cs.DM math.CO</categories><comments>Minor revision</comments><msc-class>05C62, 05C15</msc-class><journal-ref>Discrete Comput.Geom. 53 (2015) 199-220</journal-ref><doi>10.1007/s00454-014-9640-3</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, it was proved that triangle-free intersection graphs of $n$ line
segments in the plane can have chromatic number as large as $\Theta(\log\log
n)$. Essentially the same construction produces $\Theta(\log\log n)$-chromatic
triangle-free intersection graphs of a variety of other geometric
shapes---those belonging to any class of compact arc-connected sets in
$\mathbb{R}^2$ closed under horizontal scaling, vertical scaling, and
translation, except for axis-parallel rectangles. We show that this
construction is asymptotically optimal for intersection graphs of boundaries of
axis-parallel rectangles, which can be alternatively described as overlap
graphs of axis-parallel rectangles. That is, we prove that triangle-free
rectangle overlap graphs have chromatic number $O(\log\log n)$, improving on
the previous bound of $O(\log n)$. To this end, we exploit a relationship
between off-line coloring of rectangle overlap graphs and on-line coloring of
interval overlap graphs. Our coloring method decomposes the graph into a
bounded number of subgraphs with a tree-like structure that &quot;encodes&quot;
strategies of the adversary in the on-line coloring problem. Then, these
subgraphs are colored with $O(\log\log n)$ colors using a combination of
techniques from on-line algorithms (first-fit) and data structure design
(heavy-light decomposition).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0542</identifier>
 <datestamp>2013-05-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0542</id><created>2013-01-03</created><updated>2013-05-29</updated><authors><author><keyname>Demanet</keyname><forenames>Laurent</forenames></author><author><keyname>Zhang</keyname><forenames>Xiangxiong</forenames></author></authors><title>Eventual linear convergence of the Douglas Rachford iteration for basis
  pursuit</title><categories>math.NA cs.IT math.IT math.OC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We provide a simple analysis of the Douglas-Rachford splitting algorithm in
the context of $\ell^1$ minimization with linear constraints, and quantify the
asymptotic linear convergence rate in terms of principal angles between
relevant vector spaces. In the compressed sensing setting, we show how to bound
this rate in terms of the restricted isometry constant. More general iterative
schemes obtained by $\ell^2$-regularization and over-relaxation including the
dual split Bregman method are also treated, which answers the question how to
choose the relaxation and soft-thresholding parameters to accelerate the
asymptotic convergence rate. We make no attempt at characterizing the transient
regime preceding the onset of linear convergence.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0550</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0550</id><created>2012-12-12</created><authors><author><keyname>Ali</keyname><forenames>Ayesha R.</forenames></author><author><keyname>Richardson</keyname><forenames>Thomas S.</forenames></author></authors><title>Markov Equivalence Classes for Maximal Ancestral Graphs</title><categories>cs.AI stat.ME</categories><comments>Appears in Proceedings of the Eighteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2002)</comments><proxy>auai</proxy><report-no>UAI-P-2002-PG-1-9</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Ancestral graphs are a class of graphs that encode conditional independence
relations arising in DAG models with latent and selection variables,
corresponding to marginalization and conditioning. However, for any ancestral
graph, there may be several other graphs to which it is Markov equivalent. We
introduce a simple representation of a Markov equivalence class of ancestral
graphs, thereby facilitating model search. \ More specifically, we define a
join operation on ancestral graphs which will associate a unique graph with a
Markov equivalence class. We also extend the separation criterion for ancestral
graphs (which is an extension of d-separation) and provide a proof of the
pairwise Markov property for joined ancestral graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0551</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0551</id><created>2012-12-12</created><authors><author><keyname>Anguelov</keyname><forenames>Dragomir</forenames></author><author><keyname>Biswas</keyname><forenames>Rahul</forenames></author><author><keyname>Koller</keyname><forenames>Daphne</forenames></author><author><keyname>Limketkai</keyname><forenames>Benson</forenames></author><author><keyname>Thrun</keyname><forenames>Sebastian</forenames></author></authors><title>Learning Hierarchical Object Maps Of Non-Stationary Environments with
  mobile robots</title><categories>cs.LG cs.RO stat.ML</categories><comments>Appears in Proceedings of the Eighteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2002)</comments><proxy>auai</proxy><report-no>UAI-P-2002-PG-10-17</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Building models, or maps, of robot environments is a highly active research
area; however, most existing techniques construct unstructured maps and assume
static environments. In this paper, we present an algorithm for learning object
models of non-stationary objects found in office-type environments. Our
algorithm exploits the fact that many objects found in office environments look
alike (e.g., chairs, recycling bins). It does so through a two-level
hierarchical representation, which links individual objects with generic shape
templates of object classes. We derive an approximate EM algorithm for learning
shape parameters at both levels of the hierarchy, using local occupancy grid
maps for representing shape. Additionally, we develop a Bayesian model
selection algorithm that enables the robot to estimate the total number of
objects and object templates in the environment. Experimental results using a
real robot equipped with a laser range finder indicate that our approach
performs well at learning object-based maps of simple office environments. The
approach outperforms a previously developed non-hierarchical algorithm that
models objects but lacks class templates.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0552</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0552</id><created>2012-12-12</created><authors><author><keyname>Aron</keyname><forenames>Ionut</forenames></author><author><keyname>Van Hentenryck</keyname><forenames>Pascal</forenames></author></authors><title>A constraint satisfaction approach to the robust spanning tree problem
  with interval data</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2002)</comments><proxy>auai</proxy><report-no>UAI-P-2002-PG-18-25</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Robust optimization is one of the fundamental approaches to deal with
uncertainty in combinatorial optimization. This paper considers the robust
spanning tree problem with interval data, which arises in a variety of
telecommunication applications. It proposes a constraint satisfaction approach
using a combinatorial lower bound, a pruning component that removes infeasible
and suboptimal edges, as well as a search strategy exploring the most uncertain
edges first. The resulting algorithm is shown to produce very dramatic
improvements over the mathematical programming approach of Yaman et al. and to
enlarge considerably the class of problems amenable to effective solutions
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0553</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0553</id><created>2012-12-12</created><authors><author><keyname>Auvray</keyname><forenames>Vincent</forenames></author><author><keyname>Wehenkel</keyname><forenames>Louis</forenames></author></authors><title>On the Construction of the Inclusion Boundary Neighbourhood for Markov
  Equivalence Classes of Bayesian Network Structures</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2002)</comments><proxy>auai</proxy><report-no>UAI-P-2002-PG-26-35</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of learning Markov equivalence classes of Bayesian network
structures may be solved by searching for the maximum of a scoring metric in a
space of these classes. This paper deals with the definition and analysis of
one such search space. We use a theoretically motivated neighbourhood, the
inclusion boundary, and represent equivalence classes by essential graphs. We
show that this search space is connected and that the score of the neighbours
can be evaluated incrementally. We devise a practical way of building this
neighbourhood for an essential graph that is purely graphical and does not
explicitely refer to the underlying independences. We find that its size can be
intractable, depending on the complexity of the essential graph of the
equivalence class. The emphasis is put on the potential use of this space with
greedy hill -climbing search
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0554</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0554</id><created>2012-12-12</created><authors><author><keyname>Bach</keyname><forenames>Francis R.</forenames></author><author><keyname>Jordan</keyname><forenames>Michael I.</forenames></author></authors><title>Tree-dependent Component Analysis</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Eighteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2002)</comments><proxy>auai</proxy><report-no>UAI-P-2002-PG-36-44</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a generalization of independent component analysis (ICA), where
instead of looking for a linear transform that makes the data components
independent, we look for a transform that makes the data components well fit by
a tree-structured graphical model. Treating the problem as a semiparametric
statistical problem, we show that the optimal transform is found by minimizing
a contrast function based on mutual information, a function that directly
extends the contrast function used for classical ICA. We provide two
approximations of this contrast function, one using kernel density estimation,
and another using kernel generalized variance. This tree-dependent component
analysis framework leads naturally to an efficient general multivariate density
estimation technique where only bivariate density estimation needs to be
performed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0555</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0555</id><created>2012-12-12</created><authors><author><keyname>Benferhat</keyname><forenames>Salem</forenames></author><author><keyname>Dubois</keyname><forenames>Didier</forenames></author><author><keyname>Kaci</keyname><forenames>Souhila</forenames></author><author><keyname>Prade</keyname><forenames>Henri</forenames></author></authors><title>Bipolar Possibilistic Representations</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2002)</comments><proxy>auai</proxy><report-no>UAI-P-2002-PG-45-52</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, it has been emphasized that the possibility theory framework allows
us to distinguish between i) what is possible because it is not ruled out by
the available knowledge, and ii) what is possible for sure. This distinction
may be useful when representing knowledge, for modelling values which are not
impossible because they are consistent with the available knowledge on the one
hand, and values guaranteed to be possible because reported from observations
on the other hand. It is also of interest when expressing preferences, to point
out values which are positively desired among those which are not rejected.
This distinction can be encoded by two types of constraints expressed in terms
of necessity measures and in terms of guaranteed possibility functions, which
induce a pair of possibility distributions at the semantic level. A consistency
condition should ensure that what is claimed to be guaranteed as possible is
indeed not impossible. The present paper investigates the representation of
this bipolar view, including the case when it is stated by means of conditional
measures, or by means of comparative context-dependent constraints. The
interest of this bipolar framework, which has been recently stressed for
expressing preferences, is also pointed out in the representation of diagnostic
knowledge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0556</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0556</id><created>2012-12-12</created><authors><author><keyname>Blei</keyname><forenames>David</forenames></author><author><keyname>Bagnell</keyname><forenames>J Andrew</forenames></author><author><keyname>McCallum</keyname><forenames>Andrew</forenames></author></authors><title>Learning with Scope, with Application to Information Extraction and
  Classification</title><categories>cs.LG cs.IR stat.ML</categories><comments>Appears in Proceedings of the Eighteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2002)</comments><proxy>auai</proxy><report-no>UAI-P-2002-PG-53-60</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In probabilistic approaches to classification and information extraction, one
typically builds a statistical model of words under the assumption that future
data will exhibit the same regularities as the training data. In many data
sets, however, there are scope-limited features whose predictive power is only
applicable to a certain subset of the data. For example, in information
extraction from web pages, word formatting may be indicative of extraction
category in different ways on different web pages. The difficulty with using
such features is capturing and exploiting the new regularities encountered in
previously unseen data. In this paper, we propose a hierarchical probabilistic
model that uses both local/scope-limited features, such as word formatting, and
global features, such as word content. The local regularities are modeled as an
unobserved random parameter which is drawn once for each local data set. This
random parameter is estimated during the inference process and then used to
perform classification with both the local and global features--- a procedure
which is akin to automatically retuning the classifier to the local
regularities on each newly encountered web page. Exact inference is intractable
and we present approximations via point estimates and variational methods.
Empirical results on large collections of web data demonstrate that this method
significantly improves performance from traditional models of global features
alone.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0557</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0557</id><created>2012-12-12</created><authors><author><keyname>Bonet</keyname><forenames>Blai</forenames></author><author><keyname>Pearl</keyname><forenames>Judea</forenames></author></authors><title>Qualitative MDPs and POMDPs: An Order-Of-Magnitude Approximation</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2002)</comments><proxy>auai</proxy><report-no>UAI-P-2002-PG-61-68</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a qualitative theory of Markov Decision Processes (MDPs) and
Partially Observable MDPs that can be used to model sequential decision making
tasks when only qualitative information is available. Our approach is based
upon an order-of-magnitude approximation of both probabilities and utilities,
similar to epsilon-semantics. The result is a qualitative theory that has close
ties with the standard maximum-expected-utility theory and is amenable to
general planning techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0558</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0558</id><created>2012-12-12</created><authors><author><keyname>Brafman</keyname><forenames>Ronen I.</forenames></author><author><keyname>Domshlak</keyname><forenames>Carmel</forenames></author></authors><title>Introducing Variable Importance Tradeoffs into CP-Nets</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2002)</comments><proxy>auai</proxy><report-no>UAI-P-2002-PG-69-76</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ability to make decisions and to assess potential courses of action is a
corner-stone of many AI applications, and usually this requires explicit
information about the decision-maker s preferences. IN many applications,
preference elicitation IS a serious bottleneck.The USER either does NOT have
the time, the knowledge, OR the expert support required TO specify complex
multi - attribute utility functions. IN such cases, a method that IS based ON
intuitive, yet expressive, preference statements IS required. IN this paper we
suggest the USE OF TCP - nets, an enhancement OF CP - nets, AS a tool FOR
representing, AND reasoning about qualitative preference statements.We present
AND motivate this framework, define its semantics, AND show how it can be used
TO perform constrained optimization.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0559</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0559</id><created>2012-12-12</created><authors><author><keyname>Bresina</keyname><forenames>John</forenames></author><author><keyname>Dearden</keyname><forenames>Richard</forenames></author><author><keyname>Meuleau</keyname><forenames>Nicolas</forenames></author><author><keyname>Ramkrishnan</keyname><forenames>Sailesh</forenames></author><author><keyname>Smith</keyname><forenames>David</forenames></author><author><keyname>Washington</keyname><forenames>Richard</forenames></author></authors><title>Planning under Continuous Time and Resource Uncertainty: A Challenge for
  AI</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2002)</comments><proxy>auai</proxy><report-no>UAI-P-2002-PG-77-84</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We outline a class of problems, typical of Mars rover operations, that are
problematic for current methods of planning under uncertainty. The existing
methods fail because they suffer from one or more of the following limitations:
1) they rely on very simple models of actions and time, 2) they assume that
uncertainty is manifested in discrete action outcomes, 3) they are only
practical for very small problems. For many real world problems, these
assumptions fail to hold. In particular, when planning the activities for a
Mars rover, none of the above assumptions is valid: 1) actions can be
concurrent and have differing durations, 2) there is uncertainty concerning
action durations and consumption of continuous resources like power, and 3)
typical daily plans involve on the order of a hundred actions. This class of
problems may be of particular interest to the UAI community because both
classical and decision-theoretic planning techniques may be useful in solving
it. We describe the rover problem, discuss previous work on planning under
uncertainty, and present a detailed, but very small, example illustrating some
of the difficulties of finding good plans.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0560</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0560</id><created>2012-12-12</created><authors><author><keyname>Brito</keyname><forenames>Carlos</forenames></author><author><keyname>Pearl</keyname><forenames>Judea</forenames></author></authors><title>Generalized Instrumental Variables</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2002)</comments><proxy>auai</proxy><report-no>UAI-P-2002-PG-85-93</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper concerns the assessment of direct causal effects from a
combination of: (i) non-experimental data, and (ii) qualitative domain
knowledge. Domain knowledge is encoded in the form of a directed acyclic graph
(DAG), in which all interactions are assumed linear, and some variables are
presumed to be unobserved. We provide a generalization of the well-known method
of Instrumental Variables, which allows its application to models with few
conditional independeces.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0561</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0561</id><created>2012-12-12</created><authors><author><keyname>Chickering</keyname><forenames>David Maxwell</forenames></author><author><keyname>Meek</keyname><forenames>Christopher</forenames></author></authors><title>Finding Optimal Bayesian Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2002)</comments><proxy>auai</proxy><report-no>UAI-P-2002-PG-94-102</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we derive optimality results for greedy Bayesian-network
search algorithms that perform single-edge modifications at each step and use
asymptotically consistent scoring criteria. Our results extend those of Meek
(1997) and Chickering (2002), who demonstrate that in the limit of large
datasets, if the generative distribution is perfect with respect to a DAG
defined over the observable variables, such search algorithms will identify
this optimal (i.e. generative) DAG model. We relax their assumption about the
generative distribution, and assume only that this distribution satisfies the
{em composition property} over the observable variables, which is a more
realistic assumption for real domains. Under this assumption, we guarantee that
the search algorithms identify an {em inclusion-optimal} model; that is, a
model that (1) contains the generative distribution and (2) has no sub-model
that contains this distribution. In addition, we show that the composition
property is guaranteed to hold whenever the dependence relationships in the
generative distribution can be characterized by paths between singleton
elements in some generative graphical model (e.g. a DAG, a chain graph, or a
Markov network) even when the generative model includes unobserved variables,
and even when the observed data is subject to selection bias.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0562</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0562</id><created>2012-12-12</created><authors><author><keyname>Corduneanu</keyname><forenames>Adrian</forenames></author><author><keyname>Jaakkola</keyname><forenames>Tommi S.</forenames></author></authors><title>Continuation Methods for Mixing Heterogenous Sources</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Eighteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2002)</comments><proxy>auai</proxy><report-no>UAI-P-2002-PG-111-118</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A number of modern learning tasks involve estimation from heterogeneous
information sources. This includes classification with labeled and unlabeled
data as well as other problems with analogous structure such as competitive
(game theoretic) problems. The associated estimation problems can be typically
reduced to solving a set of fixed point equations (consistency conditions). We
introduce a general method for combining a preferred information source with
another in this setting by evolving continuous paths of fixed points at
intermediate allocations. We explicitly identify critical points along the
unique paths to either increase the stability of estimation or to ensure a
significant departure from the initial source. The homotopy continuation
approach is guaranteed to terminate at the second source, and involves no
combinatorial effort. We illustrate the power of these ideas both in
classification tasks with labeled and unlabeled data, as well as in the context
of a competitive (min-max) formulation of DNA sequence motif discovery.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0563</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0563</id><created>2012-12-12</created><authors><author><keyname>Davies</keyname><forenames>Scott</forenames></author><author><keyname>Moore</keyname><forenames>Andrew</forenames></author></authors><title>Interpolating Conditional Density Trees</title><categories>cs.LG cs.AI stat.ML</categories><comments>Appears in Proceedings of the Eighteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2002)</comments><proxy>auai</proxy><report-no>UAI-P-2002-PG-119-127</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Joint distributions over many variables are frequently modeled by decomposing
them into products of simpler, lower-dimensional conditional distributions,
such as in sparsely connected Bayesian networks. However, automatically
learning such models can be very computationally expensive when there are many
datapoints and many continuous variables with complex nonlinear relationships,
particularly when no good ways of decomposing the joint distribution are known
a priori. In such situations, previous research has generally focused on the
use of discretization techniques in which each continuous variable has a single
discretization that is used throughout the entire network. \ In this paper, we
present and compare a wide variety of tree-based algorithms for learning and
evaluating conditional density estimates over continuous variables. These trees
can be thought of as discretizations that vary according to the particular
interactions being modeled; however, the density within a given leaf of the
tree need not be assumed constant, and we show that such nonuniform leaf
densities lead to more accurate density estimation. We have developed Bayesian
network structure-learning algorithms that employ these tree-based conditional
density representations, and we show that they can be used to practically learn
complex joint probability models over dozens of continuous variables from
thousands of datapoints. We focus on finding models that are simultaneously
accurate, fast to learn, and fast to evaluate once they are learned.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0564</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0564</id><created>2012-12-12</created><authors><author><keyname>Dechter</keyname><forenames>Rina</forenames></author><author><keyname>Kask</keyname><forenames>Kalev</forenames></author><author><keyname>Mateescu</keyname><forenames>Robert</forenames></author></authors><title>Iterative Join-Graph Propagation</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2002)</comments><proxy>auai</proxy><report-no>UAI-P-2002-PG-128-136</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The paper presents an iterative version of join-tree clustering that applies
the message passing of join-tree clustering algorithm to join-graphs rather
than to join-trees, iteratively. It is inspired by the success of Pearl's
belief propagation algorithm as an iterative approximation scheme on one hand,
and by a recently introduced mini-clustering i. success as an anytime
approximation method, on the other. The proposed Iterative Join-graph
Propagation IJGP belongs to the class of generalized belief propagation
methods, recently proposed using analogy with algorithms in statistical
physics. Empirical evaluation of this approach on a number of problem classes
demonstrates that even the most time-efficient variant is almost always
superior to IBP and MC i, and is sometimes more accurate by as much as several
orders of magnitude.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0565</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0565</id><created>2012-12-12</created><authors><author><keyname>Dom</keyname><forenames>Byron E</forenames></author></authors><title>An Information-Theoretic External Cluster-Validity Measure</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Eighteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2002)</comments><proxy>auai</proxy><report-no>UAI-P-2002-PG-137-145</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose a measure of clustering quality or accuracy that is
appropriate in situations where it is desirable to evaluate a clustering
algorithm by somehow comparing the clusters it produces with ``ground truth'
consisting of classes assigned to the patterns by manual means or some other
means in whose veracity there is confidence. Such measures are refered to as
``external'. Our measure also has the characteristic of allowing clusterings
with different numbers of clusters to be compared in a quantitative and
principled way. Our evaluation scheme quantitatively measures how useful the
cluster labels of the patterns are as predictors of their class labels. In
cases where all clusterings to be compared have the same number of clusters,
the measure is equivalent to the mutual information between the cluster labels
and the class labels. In cases where the numbers of clusters are different,
however, it computes the reduction in the number of bits that would be required
to encode (compress) the class labels if both the encoder and decoder have free
acccess to the cluster labels. To achieve this encoding the estimated
conditional probabilities of the class labels given the cluster labels must
also be encoded. These estimated probabilities can be seen as a model for the
class labels and their associated code length as a model cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0566</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0566</id><created>2012-12-12</created><authors><author><keyname>Eiter</keyname><forenames>Thomas</forenames></author><author><keyname>Lukasiewicz</keyname><forenames>Thomas</forenames></author></authors><title>Causes and Explanations in the Structural-Model Approach: Tractable
  Cases</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2002)</comments><proxy>auai</proxy><report-no>UAI-P-2002-PG-146-153</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we continue our research on the algorithmic aspects of Halpern
and Pearl's causes and explanations in the structural-model approach. To this
end, we present new characterizations of weak causes for certain classes of
causal models, which show that under suitable restrictions deciding causes and
explanations is tractable. To our knowledge, these are the first explicit
tractability results for the structural-model approach.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0567</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0567</id><created>2012-12-12</created><authors><author><keyname>Finney</keyname><forenames>Sarah</forenames></author><author><keyname>Gardiol</keyname><forenames>Natalia</forenames></author><author><keyname>Kaelbling</keyname><forenames>Leslie Pack</forenames></author><author><keyname>Oates</keyname><forenames>Tim</forenames></author></authors><title>The Thing That We Tried Didn't Work Very Well : Deictic Representation
  in Reinforcement Learning</title><categories>cs.LG cs.AI</categories><comments>Appears in Proceedings of the Eighteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2002)</comments><proxy>auai</proxy><report-no>UAI-P-2002-PG-154-161</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most reinforcement learning methods operate on propositional representations
of the world state. Such representations are often intractably large and
generalize poorly. Using a deictic representation is believed to be a viable
alternative: they promise generalization while allowing the use of existing
reinforcement-learning methods. Yet, there are few experiments on learning with
deictic representations reported in the literature. In this paper we explore
the effectiveness of two forms of deictic representation and a na\&quot;{i}ve
propositional representation in a simple blocks-world domain. We find,
empirically, that the deictic representations actually worsen learning
performance. We conclude with a discussion of possible causes of these results
and strategies for more effective learning in domains with objects.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0568</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0568</id><created>2012-12-12</created><authors><author><keyname>Geiger</keyname><forenames>Dan</forenames></author><author><keyname>Meek</keyname><forenames>Christopher</forenames></author><author><keyname>Sturmfels</keyname><forenames>Bernd</forenames></author></authors><title>Factorization of Discrete Probability Distributions</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2002)</comments><proxy>auai</proxy><report-no>UAI-P-2002-PG-162-169</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We formulate necessary and sufficient conditions for an arbitrary discrete
probability distribution to factor according to an undirected graphical model,
or a log-linear model, or other more general exponential models. This result
generalizes the well known Hammersley-Clifford Theorem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0569</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0569</id><created>2012-12-12</created><authors><author><keyname>Giang</keyname><forenames>Phan H.</forenames></author><author><keyname>Shenoy</keyname><forenames>Prakash P.</forenames></author></authors><title>Statistical Decisions Using Likelihood Information Without Prior
  Probabilities</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2002)</comments><proxy>auai</proxy><report-no>UAI-P-2002-PG-170-178</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a decision-theoretic approach to statistical inference
that satisfies the likelihood principle (LP) without using prior information.
Unlike the Bayesian approach, which also satisfies LP, we do not assume
knowledge of the prior distribution of the unknown parameter. With respect to
information that can be obtained from an experiment, our solution is more
efficient than Wald's minimax solution.However, with respect to information
assumed to be known before the experiment, our solution demands less input than
the Bayesian solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0570</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0570</id><created>2012-12-12</created><authors><author><keyname>Goodman</keyname><forenames>Joshua</forenames></author></authors><title>Reduction of Maximum Entropy Models to Hidden Markov Models</title><categories>cs.AI cs.CL</categories><comments>Appears in Proceedings of the Eighteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2002)</comments><proxy>auai</proxy><report-no>UAI-P-2002-PG-179-186</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that maximum entropy (maxent) models can be modeled with certain
kinds of HMMs, allowing us to construct maxent models with hidden variables,
hidden state sequences, or other characteristics. The models can be trained
using the forward-backward algorithm. While the results are primarily of
theoretical interest, unifying apparently unrelated concepts, we also give
experimental results for a maxent model with a hidden variable on a word
disambiguation task; the model outperforms standard techniques.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0571</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0571</id><created>2012-12-12</created><authors><author><keyname>Guestrin</keyname><forenames>Carlos E.</forenames></author><author><keyname>Gordon</keyname><forenames>Geoffrey</forenames></author></authors><title>Distributed Planning in Hierarchical Factored MDPs</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2002)</comments><proxy>auai</proxy><report-no>UAI-P-2002-PG-197-206</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a principled and efficient planning algorithm for collaborative
multiagent dynamical systems. All computation, during both the planning and the
execution phases, is distributed among the agents; each agent only needs to
model and plan for a small part of the system. Each of these local subsystems
is small, but once they are combined they can represent an exponentially larger
problem. The subsystems are connected through a subsystem hierarchy.
Coordination and communication between the agents is not imposed, but derived
directly from the structure of this hierarchy. A globally consistent plan is
achieved by a message passing algorithm, where messages correspond to natural
local reward functions and are computed by local linear programs; another
message passing algorithm allows us to execute the resulting policy. When two
portions of the hierarchy share the same structure, our algorithm can reuse
plans and messages to speed up computation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0572</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0572</id><created>2012-12-12</created><authors><author><keyname>Heskes</keyname><forenames>Tom</forenames></author><author><keyname>Zoeter</keyname><forenames>Onno</forenames></author></authors><title>Expectation Propogation for approximate inference in dynamic Bayesian
  networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2002)</comments><proxy>auai</proxy><report-no>UAI-P-2002-PG-216-223</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe expectation propagation for approximate inference in dynamic
Bayesian networks as a natural extension of Pearl s exact belief
propagation.Expectation propagation IS a greedy algorithm, converges IN many
practical cases, but NOT always.We derive a DOUBLE - loop algorithm, guaranteed
TO converge TO a local minimum OF a Bethe free energy.Furthermore, we show that
stable fixed points OF (damped) expectation propagation correspond TO local
minima OF this free energy, but that the converse need NOT be the CASE .We
illustrate the algorithms BY applying them TO switching linear dynamical
systems AND discuss implications FOR approximate inference IN general Bayesian
networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0573</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0573</id><created>2012-12-12</created><authors><author><keyname>Horvitz</keyname><forenames>Eric J.</forenames></author><author><keyname>Koch</keyname><forenames>Paul</forenames></author><author><keyname>Kadie</keyname><forenames>Carl</forenames></author><author><keyname>Jacobs</keyname><forenames>Andy</forenames></author></authors><title>Coordinates: Probabilistic Forecasting of Presence and Availability</title><categories>cs.HC cs.AI</categories><comments>Appears in Proceedings of the Eighteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2002)</comments><proxy>auai</proxy><report-no>UAI-P-2002-PG-224-233</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present methods employed in Coordinate, a prototype service that supports
collaboration and communication by learning predictive models that provide
forecasts of users s AND availability.We describe how data IS collected about
USER activity AND proximity FROM multiple devices, IN addition TO analysis OF
the content OF users, the time of day, and day of week. We review applications
of presence forecasting embedded in the Priorities application and then present
details of the Coordinate service that was informed by the earlier efforts.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0574</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0574</id><created>2012-12-12</created><authors><author><keyname>Jensen</keyname><forenames>Finn Verner</forenames></author><author><keyname>Vomlelova</keyname><forenames>Marta</forenames></author></authors><title>Unconstrained Influence Diagrams</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2002)</comments><proxy>auai</proxy><report-no>UAI-P-2002-PG-234-241</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We extend the language of influence diagrams to cope with decision scenarios
where the order of decisions and observations is not determined. As the
ordering of decisions is dependent on the evidence, a step-strategy of such a
scenario is a sequence of dependent choices of the next action. A strategy is a
step-strategy together with selection functions for decision actions. The
structure of a step-strategy can be represented as a DAG with nodes labeled
with action variables. We introduce the concept of GS-DAG: a DAG incorporating
an optimal step-strategy for any instantiation. We give a method for
constructing GS-DAGs, and we show how to use a GS-DAG for determining an
optimal strategy. Finally we discuss how analysis of relevant past can be used
to reduce the size of the GS-DAG.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0575</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0575</id><created>2012-12-12</created><updated>2015-05-16</updated><authors><author><keyname>Kadie</keyname><forenames>Carl</forenames></author><author><keyname>Meek</keyname><forenames>Christopher</forenames></author><author><keyname>Heckerman</keyname><forenames>David</forenames></author></authors><title>CFW: A Collaborative Filtering System Using Posteriors Over Weights Of
  Evidence</title><categories>cs.IR cs.AI</categories><comments>Appears in Proceedings of the Eighteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2002)</comments><proxy>Martijn de Jongh</proxy><report-no>UAI-P-2002-PG-242-250</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe CFW, a computationally efficient algorithm for collaborative
filtering that uses posteriors over weights of evidence. In experiments on real
data, we show that this method predicts as well or better than other methods in
situations where the size of the user query is small. The new approach works
particularly well when the user s query CONTAINS low frequency(unpopular)
items.The approach complements that OF dependency networks which perform well
WHEN the size OF the query IS large.Also IN this paper, we argue that the USE
OF posteriors OVER weights OF evidence IS a natural way TO recommend similar
items collaborative - filtering task.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0576</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0576</id><created>2012-12-12</created><authors><author><keyname>Kayaalp</keyname><forenames>Mehmet</forenames></author><author><keyname>Cooper</keyname><forenames>Gregory F.</forenames></author></authors><title>A Bayesian Network Scoring Metric That Is Based On Globally Uniform
  Parameter Priors</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2002)</comments><proxy>auai</proxy><report-no>UAI-P-2002-PG-251-258</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a new Bayesian network (BN) scoring metric called the Global
Uniform (GU) metric. This metric is based on a particular type of default
parameter prior. Such priors may be useful when a BN developer is not willing
or able to specify domain-specific parameter priors. The GU parameter prior
specifies that every prior joint probability distribution P consistent with a
BN structure S is considered to be equally likely. Distribution P is consistent
with S if P includes just the set of independence relations defined by S. We
show that the GU metric addresses some undesirable behavior of the BDeu and K2
Bayesian network scoring metrics, which also use particular forms of default
parameter priors. A closed form formula for computing GU for special classes of
BNs is derived. Efficiently computing GU for an arbitrary BN remains an open
problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0577</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0577</id><created>2012-12-12</created><authors><author><keyname>Kearns</keyname><forenames>Michael</forenames></author><author><keyname>Mansour</keyname><forenames>Yishay</forenames></author></authors><title>Efficient Nash Computation in Large Population Games with Bounded
  Influence</title><categories>cs.GT cs.AI</categories><comments>Appears in Proceedings of the Eighteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2002)</comments><proxy>auai</proxy><report-no>UAI-P-2002-PG-259-266</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce a general representation of large-population games in which each
player s influence ON the others IS centralized AND limited, but may otherwise
be arbitrary.This representation significantly generalizes the class known AS
congestion games IN a natural way.Our main results are provably correct AND
efficient algorithms FOR computing AND learning approximate Nash equilibria IN
this general framework.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0578</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0578</id><created>2012-12-12</created><authors><author><keyname>Kocka</keyname><forenames>Tomas</forenames></author><author><keyname>Zhang</keyname><forenames>Nevin Lianwen</forenames></author></authors><title>Dimension Correction for Hierarchical Latent Class Models</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Eighteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2002)</comments><proxy>auai</proxy><report-no>UAI-P-2002-PG-267-274</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Model complexity is an important factor to consider when selecting among
graphical models. When all variables are observed, the complexity of a model
can be measured by its standard dimension, i.e. the number of independent
parameters. When hidden variables are present, however, standard dimension
might no longer be appropriate. One should instead use effective dimension
(Geiger et al. 1996). This paper is concerned with the computation of effective
dimension. First we present an upper bound on the effective dimension of a
latent class (LC) model. This bound is tight and its computation is easy. We
then consider a generalization of LC models called hierarchical latent class
(HLC) models (Zhang 2002). We show that the effective dimension of an HLC model
can be obtained from the effective dimensions of some related LC models. We
also demonstrate empirically that using effective dimension in place of
standard dimension improves the quality of models learned from data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0579</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0579</id><created>2012-12-12</created><authors><author><keyname>Kutin</keyname><forenames>Samuel</forenames></author><author><keyname>Niyogi</keyname><forenames>Partha</forenames></author></authors><title>Almost-everywhere algorithmic stability and generalization error</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Eighteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2002)</comments><proxy>auai</proxy><report-no>UAI-P-2002-PG-275-282</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We explore in some detail the notion of algorithmic stability as a viable
framework for analyzing the generalization error of learning algorithms. We
introduce the new notion of training stability of a learning algorithm and show
that, in a general setting, it is sufficient for good bounds on generalization
error. In the PAC setting, training stability is both necessary and sufficient
for learnability.\ The approach based on training stability makes no reference
to VC dimension or VC entropy. There is no need to prove uniform convergence,
and generalization error is bounded directly via an extended McDiarmid
inequality. As a result it potentially allows us to deal with a broader class
of learning algorithms than Empirical Risk Minimization. \ We also explore the
relationships among VC dimension, generalization error, and various notions of
stability. Several examples of learning algorithms are considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0580</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0580</id><created>2012-12-12</created><authors><author><keyname>Lagoudakis</keyname><forenames>Michail</forenames></author><author><keyname>Parr</keyname><forenames>Ron</forenames></author></authors><title>Value Function Approximation in Zero-Sum Markov Games</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2002)</comments><proxy>auai</proxy><report-no>UAI-P-2002-PG-283-292</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper investigates value function approximation in the context of
zero-sum Markov games, which can be viewed as a generalization of the Markov
decision process (MDP) framework to the two-agent case. We generalize error
bounds from MDPs to Markov games and describe generalizations of reinforcement
learning algorithms to Markov games. We present a generalization of the optimal
stopping problem to a two-player simultaneous move Markov game. For this
special problem, we provide stronger bounds and can guarantee convergence for
LSTD and temporal difference learning with linear value function approximation.
We demonstrate the viability of value function approximation for Markov games
by using the Least squares policy iteration (LSPI) algorithm to learn good
policies for a soccer domain and a flow control problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0581</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0581</id><created>2012-12-12</created><authors><author><keyname>Leisink</keyname><forenames>Martijn</forenames></author><author><keyname>Kappen</keyname><forenames>Hilbert</forenames></author></authors><title>General Lower Bounds based on Computer Generated Higher Order Expansions</title><categories>cs.NA</categories><comments>Appears in Proceedings of the Eighteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2002)</comments><proxy>auai</proxy><report-no>UAI-P-2002-PG-293-300</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this article we show the rough outline of a computer algorithm to generate
lower bounds on the exponential function of (in principle) arbitrary precision.
We implemented this to generate all necessary analytic terms for the Boltzmann
machine partition function thus leading to lower bounds of any order. It turns
out that the extra variational parameters can be optimized analytically. We
show that bounds upto nineth order are still reasonably calculable in practical
situations. The generated terms can also be used as extra correction terms
(beyond TAP) in mean field expansions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0582</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0582</id><created>2012-12-12</created><authors><author><keyname>Lerner</keyname><forenames>Uri</forenames></author><author><keyname>Moses</keyname><forenames>Brooks</forenames></author><author><keyname>Scott</keyname><forenames>Maricia</forenames></author><author><keyname>McIlraith</keyname><forenames>Sheila</forenames></author><author><keyname>Koller</keyname><forenames>Daphne</forenames></author></authors><title>Monitoring a Complez Physical System using a Hybrid Dynamic Bayes Net</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2002)</comments><proxy>auai</proxy><report-no>UAI-P-2002-PG-301-310</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The Reverse Water Gas Shift system (RWGS) is a complex physical system
designed to produce oxygen from the carbon dioxide atmosphere on Mars. If sent
to Mars, it would operate without human supervision, thus requiring a reliable
automated system for monitoring and control. The RWGS presents many challenges
typical of real-world systems, including: noisy and biased sensors, nonlinear
behavior, effects that are manifested over different time granularities, and
unobservability of many important quantities. In this paper we model the RWGS
using a hybrid (discrete/continuous) Dynamic Bayesian Network (DBN), where the
state at each time slice contains 33 discrete and 184 continuous variables. We
show how the system state can be tracked using probabilistic inference over the
model. We discuss how to deal with the various challenges presented by the
RWGS, providing a suite of techniques that are likely to be useful in a wide
range of applications. In particular, we describe a general framework for
dealing with nonlinear behavior using numerical integration techniques,
extending the successful Unscented Filter. We also show how to use a
fixed-point computation to deal with effects that develop at different time
scales, specifically rapid changes occurring during slowly changing processes.
We test our model using real data collected from the RWGS, demonstrating the
feasibility of hybrid DBNs for monitoring complex real-world physical systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0583</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0583</id><created>2012-12-12</created><authors><author><keyname>Madani</keyname><forenames>Omid</forenames></author></authors><title>Polynomial Value Iteration Algorithms for Detrerminstic MDPs</title><categories>cs.AI cs.DS</categories><comments>Appears in Proceedings of the Eighteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2002)</comments><proxy>auai</proxy><report-no>UAI-P-2002-PG-311-318</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Value iteration is a commonly used and empirically competitive method in
solving many Markov decision process problems. However, it is known that value
iteration has only pseudo-polynomial complexity in general. We establish a
somewhat surprising polynomial bound for value iteration on deterministic
Markov decision (DMDP) problems. We show that the basic value iteration
procedure converges to the highest average reward cycle on a DMDP problem in
heta(n^2) iterations, or heta(mn^2) total time, where n denotes the number of
states, and m the number of edges. We give two extensions of value iteration
that solve the DMDP in heta(mn) time. We explore the analysis of policy
iteration algorithms and report on an empirical study of value iteration
showing that its convergence is much faster on random sparse graphs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0584</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0584</id><created>2012-12-12</created><authors><author><keyname>Marthi</keyname><forenames>Bhaskara</forenames></author><author><keyname>Pasula</keyname><forenames>Hanna</forenames></author><author><keyname>Russell</keyname><forenames>Stuart</forenames></author><author><keyname>Peres</keyname><forenames>Yuval</forenames></author></authors><title>Decayed MCMC Filtering</title><categories>cs.AI cs.LG cs.SY</categories><comments>Appears in Proceedings of the Eighteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2002)</comments><proxy>auai</proxy><report-no>UAI-P-2002-PG-319-326</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Filtering---estimating the state of a partially observable Markov process
from a sequence of observations---is one of the most widely studied problems in
control theory, AI, and computational statistics. Exact computation of the
posterior distribution is generally intractable for large discrete systems and
for nonlinear continuous systems, so a good deal of effort has gone into
developing robust approximation algorithms. This paper describes a simple
stochastic approximation algorithm for filtering called {em decayed MCMC}. The
algorithm applies Markov chain Monte Carlo sampling to the space of state
trajectories using a proposal distribution that favours flips of more recent
state variables. The formal analysis of the algorithm involves a generalization
of standard coupling arguments for MCMC convergence. We prove that for any
ergodic underlying Markov process, the convergence time of decayed MCMC with
inverse-polynomial decay remains bounded as the length of the observation
sequence grows. We show experimentally that decayed MCMC is at least
competitive with other approximation algorithms such as particle filtering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0585</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0585</id><created>2012-12-12</created><authors><author><keyname>McBurney</keyname><forenames>Peter</forenames></author><author><keyname>Parsons</keyname><forenames>Simon</forenames></author></authors><title>Formalizing Scenario Analysis</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2002)</comments><proxy>auai</proxy><report-no>UAI-P-2002-PG-327-334</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a formal treatment of scenarios in the context of a dialectical
argumentation formalism for qualitative reasoning about uncertain propositions.
Our formalism extends prior work in which arguments for and against uncertain
propositions were presented and compared in interaction spaces called Agoras.
We now define the notion of a scenario in this framework and use it to define a
set of qualitative uncertainty labels for propositions across a collection of
scenarios. This work is intended to lead to a formal theory of scenarios and
scenario analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0586</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0586</id><created>2012-12-12</created><authors><author><keyname>Meek</keyname><forenames>Christopher</forenames></author><author><keyname>Thiesson</keyname><forenames>Bo</forenames></author><author><keyname>Heckerman</keyname><forenames>David</forenames></author></authors><title>Staged Mixture Modelling and Boosting</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Eighteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2002)</comments><proxy>auai</proxy><report-no>UAI-P-2002-PG-335-343</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we introduce and evaluate a data-driven staged mixture
modeling technique for building density, regression, and classification models.
Our basic approach is to sequentially add components to a finite mixture model
using the structural expectation maximization (SEM) algorithm. We show that our
technique is qualitatively similar to boosting. This correspondence is a
natural byproduct of the fact that we use the SEM algorithm to sequentially fit
the mixture model. Finally, in our experimental evaluation, we demonstrate the
effectiveness of our approach on a variety of prediction and density estimation
tasks using real-world data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0587</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0587</id><created>2012-12-12</created><authors><author><keyname>Mettu</keyname><forenames>Ramgopal</forenames></author><author><keyname>Plaxton</keyname><forenames>Greg</forenames></author></authors><title>Optimal Time Bounds for Approximate Clustering</title><categories>cs.DS cs.LG stat.ML</categories><comments>Appears in Proceedings of the Eighteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2002)</comments><proxy>auai</proxy><report-no>UAI-P-2002-PG-344-351</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Clustering is a fundamental problem in unsupervised learning, and has been
studied widely both as a problem of learning mixture models and as an
optimization problem. In this paper, we study clustering with respect the
emph{k-median} objective function, a natural formulation of clustering in which
we attempt to minimize the average distance to cluster centers. One of the main
contributions of this paper is a simple but powerful sampling technique that we
call emph{successive sampling} that could be of independent interest. We show
that our sampling procedure can rapidly identify a small set of points (of size
just O(klog{n/k})) that summarize the input points for the purpose of
clustering. Using successive sampling, we develop an algorithm for the k-median
problem that runs in O(nk) time for a wide range of values of k and is
guaranteed, with high probability, to return a solution with cost at most a
constant factor times optimal. We also establish a lower bound of Omega(nk) on
any randomized constant-factor approximation algorithm for the k-median problem
that succeeds with even a negligible (say 1/100) probability. Thus we establish
a tight time bound of Theta(nk) for the k-median problem for a wide range of
values of k. The best previous upper bound for the problem was O(nk), where the
O-notation hides polylogarithmic factors in n and k. The best previous lower
bound of O(nk) applied only to deterministic k-median algorithms. While we
focus our presentation on the k-median objective, all our upper bounds are
valid for the k-means objective as well. In this context our algorithm compares
favorably to the widely used k-means heuristic, which requires O(nk) time for
just one iteration and provides no useful approximation guarantees.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0588</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0588</id><created>2012-12-12</created><authors><author><keyname>Minka</keyname><forenames>Thomas P.</forenames></author><author><keyname>Lafferty</keyname><forenames>John</forenames></author></authors><title>Expectation-Propogation for the Generative Aspect Model</title><categories>cs.LG cs.IR stat.ML</categories><comments>Appears in Proceedings of the Eighteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2002)</comments><proxy>auai</proxy><report-no>UAI-P-2002-PG-352-359</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The generative aspect model is an extension of the multinomial model for text
that allows word probabilities to vary stochastically across documents.
Previous results with aspect models have been promising, but hindered by the
computational difficulty of carrying out inference and learning. This paper
demonstrates that the simple variational methods of Blei et al (2001) can lead
to inaccurate inferences and biased learning for the generative aspect model.
We develop an alternative approach that leads to higher accuracy at comparable
cost. An extension of Expectation-Propagation is used for inference and then
embedded in an EM algorithm for learning. Experimental results are presented
for both synthetic and real data sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0589</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0589</id><created>2012-12-12</created><authors><author><keyname>Moore</keyname><forenames>Andrew</forenames></author><author><keyname>Schneider</keyname><forenames>Jeff</forenames></author></authors><title>Real-valued All-Dimensions search: Low-overhead rapid searching over
  subsets of attributes</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2002)</comments><proxy>auai</proxy><report-no>UAI-P-2002-PG-360-369</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is about searching the combinatorial space of contingency tables
during the inner loop of a nonlinear statistical optimization. Examples of this
operation in various data analytic communities include searching for nonlinear
combinations of attributes that contribute significantly to a regression
(Statistics), searching for items to include in a decision list (machine
learning) and association rule hunting (Data Mining).
  This paper investigates a new, efficient approach to this class of problems,
called RADSEARCH (Real-valued All-Dimensions-tree Search). RADSEARCH finds the
global optimum, and this gives us the opportunity to empirically evaluate the
question: apart from algorithmic elegance what does this attention to
optimality buy us?
  We compare RADSEARCH with other recent successful search algorithms such as
CN2, PRIM, APriori, OPUS and DenseMiner. Finally, we introduce RADREG, a new
regression algorithm for learning real-valued outputs based on RADSEARCHing for
high-order interactions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0590</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0590</id><created>2012-12-12</created><authors><author><keyname>Ng</keyname><forenames>Brenda</forenames></author><author><keyname>Peshkin</keyname><forenames>Leonid</forenames></author><author><keyname>Pfeffer</keyname><forenames>Avi</forenames></author></authors><title>Factored Particles for Scalable Monitoring</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2002)</comments><proxy>auai</proxy><report-no>UAI-P-2002-PG-370-377</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Exact monitoring in dynamic Bayesian networks is intractable, so approximate
algorithms are necessary. This paper presents a new family of approximate
monitoring algorithms that combine the best qualities of the particle filtering
and Boyen-Koller methods. Our algorithms maintain an approximate representation
the belief state in the form of sets of factored particles, that correspond to
samples of clusters of state variables. Empirical results show that our
algorithms outperform both ordinary particle filtering and the Boyen-Koller
algorithm on large systems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0591</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0591</id><created>2012-12-12</created><authors><author><keyname>Nodelman</keyname><forenames>Uri</forenames></author><author><keyname>Shelton</keyname><forenames>Christian R.</forenames></author><author><keyname>Koller</keyname><forenames>Daphne</forenames></author></authors><title>Continuous Time Bayesian Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2002)</comments><proxy>auai</proxy><report-no>UAI-P-2002-PG-378-387</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present a language for finite state continuous time Bayesian
networks (CTBNs), which describe structured stochastic processes that evolve
over continuous time. The state of the system is decomposed into a set of local
variables whose values change over time. The dynamics of the system are
described by specifying the behavior of each local variable as a function of
its parents in a directed (possibly cyclic) graph. The model specifies, at any
given point in time, the distribution over two aspects: when a local variable
changes its value and the next value it takes. These distributions are
determined by the variable s CURRENT value AND the CURRENT VALUES OF its
parents IN the graph.More formally, each variable IS modelled AS a finite state
continuous time Markov process whose transition intensities are functions OF
its parents.We present a probabilistic semantics FOR the language IN terms OF
the generative model a CTBN defines OVER sequences OF events.We list types OF
queries one might ask OF a CTBN, discuss the conceptual AND computational
difficulties associated WITH exact inference, AND provide an algorithm FOR
approximate inference which takes advantage OF the structure within the
process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0592</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0592</id><created>2012-12-12</created><authors><author><keyname>Park</keyname><forenames>James D.</forenames></author></authors><title>MAP Complexity Results and Approximation Methods</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2002)</comments><proxy>auai</proxy><report-no>UAI-P-2002-PG-388-396</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  MAP is the problem of finding a most probable instantiation of a set of
nvariables in a Bayesian network, given some evidence. MAP appears to be a
significantly harder problem than the related problems of computing the
probability of evidence Pr, or MPE a special case of MAP. Because of the
complexity of MAP, and the lack of viable algorithms to approximate it,MAP
computations are generally avoided by practitioners. This paper investigates
the complexity of MAP. We show that MAP is complete for NP. We also provide
negative complexity results for elimination based algorithms. It turns out that
MAP remains hard even when MPE, and Pr are easy. We show that MAP is NPcomplete
when the networks are restricted to polytrees, and even then can not be
effectively approximated. Because there is no approximation algorithm with
guaranteed results, we investigate best effort approximations. We introduce a
generic MAP approximation framework. As one instantiation of it, we implement
local search coupled with belief propagation BP to approximate MAP. We show how
to extract approximate evidence retraction information from belief propagation
which allows us to perform efficient local search. This allows MAP
approximation even on networks that are too complex to even exactly solve the
easier problems of computing Pr or MPE. Experimental results indicate that
using BP and local search provides accurate MAP estimates in many cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0593</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0593</id><created>2012-12-12</created><authors><author><keyname>Pavlenko</keyname><forenames>Tatjana</forenames></author><author><keyname>von Rosen</keyname><forenames>Dietrich</forenames></author></authors><title>Bayesian Network Classifiers in a High Dimensional Framework</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Eighteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2002)</comments><proxy>auai</proxy><report-no>UAI-P-2002-PG-397-404</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a growing dimension asymptotic formalism. The perspective in this
paper is classification theory and we show that it can accommodate
probabilistic networks classifiers, including naive Bayes model and its
augmented version. When represented as a Bayesian network these classifiers
have an important advantage: The corresponding discriminant function turns out
to be a specialized case of a generalized additive model, which makes it
possible to get closed form expressions for the asymptotic misclassification
probabilities used here as a measure of classification accuracy. Moreover, in
this paper we propose a new quantity for assessing the discriminative power of
a set of features which is then used to elaborate the augmented naive Bayes
classifier. The result is a weighted form of the augmented naive Bayes that
distributes weights among the sets of features according to their
discriminative power. We derive the asymptotic distribution of the sample based
discriminative power and show that it is seriously overestimated in a high
dimensional case. We then apply this result to find the optimal, in a sense of
minimum misclassification probability, type of weighting.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0594</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0594</id><created>2012-12-12</created><authors><author><keyname>Pennock</keyname><forenames>David M</forenames></author><author><keyname>Debnath</keyname><forenames>Sandip</forenames></author><author><keyname>Glover</keyname><forenames>Eric</forenames></author><author><keyname>Giles</keyname><forenames>C. Lee</forenames></author></authors><title>Modelling Information Incorporation in Markets, with Application to
  Detecting and Explaining Events</title><categories>cs.AI q-fin.GN</categories><comments>Appears in Proceedings of the Eighteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2002)</comments><proxy>auai</proxy><report-no>UAI-P-2002-PG-405-413</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a model of how information flows into a market, and derive
algorithms for automatically detecting and explaining relevant events. We
analyze data from twenty-two &quot;political stock markets&quot; (i.e., betting markets
on political outcomes) on the Iowa Electronic Market (IEM). We prove that,
under certain efficiency assumptions, prices in such betting markets will on
average approach the correct outcomes over time, and show that IEM data
conforms closely to the theory. We present a simple model of a betting market
where information is revealed over time, and show a qualitative correspondence
between the model and real market data. We also present an algorithm for
automatically detecting significant events and generating semantic explanations
of their origin. The algorithm operates by discovering significant changes in
vocabulary on online news sources (using expected entropy loss) that align with
major price spikes in related betting markets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0595</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0595</id><created>2012-12-12</created><authors><author><keyname>Porter</keyname><forenames>Ryan</forenames></author><author><keyname>Ronen</keyname><forenames>Amir</forenames></author><author><keyname>Shoham</keyname><forenames>Yoav</forenames></author><author><keyname>Tennenholtz</keyname><forenames>Moshe</forenames></author></authors><title>Mechanism Design with Execution Uncertainty</title><categories>cs.GT</categories><comments>Appears in Proceedings of the Eighteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2002)</comments><proxy>auai</proxy><report-no>UAI-P-2002-PG-414-421</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We introduce the notion of fault tolerant mechanism design, which extends the
standard game theoretic framework of mechanism design to allow for uncertainty
about execution. Specifically, we define the problem of task allocation in
which the private information of the agents is not only their costs to attempt
the tasks, but also their probabilities of failure. For several different
instances of this setting we present technical results, including positive ones
in the form of mechanisms that are incentive compatible, individually rational
and efficient, and negative ones in the form of impossibility theorems.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0596</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0596</id><created>2012-12-12</created><authors><author><keyname>Renooij</keyname><forenames>Silja</forenames></author><author><keyname>van der Gaag</keyname><forenames>Linda C.</forenames></author></authors><title>From Qualitative to Quantitative Probabilistic Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2002)</comments><proxy>auai</proxy><report-no>UAI-P-2002-PG-422-429</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Quantification is well known to be a major obstacle in the construction of a
probabilistic network, especially when relying on human experts for this
purpose. The construction of a qualitative probabilistic network has been
proposed as an initial step in a network s quantification, since the
qualitative network can be used TO gain preliminary insight IN the projected
networks reasoning behaviour. We extend on this idea and present a new type of
network in which both signs and numbers are specified; we further present an
associated algorithm for probabilistic inference. Building upon these
semi-qualitative networks, a probabilistic network can be quantified and
studied in a stepwise manner. As a result, modelling inadequacies can be
detected and amended at an early stage in the quantification process.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0597</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0597</id><created>2012-12-12</created><authors><author><keyname>da Rocha</keyname><forenames>Jose Carlos Ferreira</forenames></author><author><keyname>Cozman</keyname><forenames>Fabio Gagliardi</forenames></author></authors><title>Inference with Seperately Specified Sets of Probabilities in Credal
  Networks</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2002)</comments><proxy>auai</proxy><report-no>UAI-P-2002-PG-430-437</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present new algorithms for inference in credal networks --- directed
acyclic graphs associated with sets of probabilities. Credal networks are here
interpreted as encoding strong independence relations among variables. We first
present a theory of credal networks based on separately specified sets of
probabilities. We also show that inference with polytrees is NP-hard in this
setting. We then introduce new techniques that reduce the computational effort
demanded by inference, particularly in polytrees, by exploring separability of
credal sets.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0598</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0598</id><created>2012-12-12</created><authors><author><keyname>Rusakov</keyname><forenames>Dmitry</forenames></author><author><keyname>Geiger</keyname><forenames>Dan</forenames></author></authors><title>Asymptotic Model Selection for Naive Bayesian Networks</title><categories>cs.AI cs.LG</categories><comments>Appears in Proceedings of the Eighteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2002)</comments><proxy>auai</proxy><report-no>UAI-P-2002-PG-438-445</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop a closed form asymptotic formula to compute the marginal
likelihood of data given a naive Bayesian network model with two hidden states
and binary features. This formula deviates from the standard BIC score. Our
work provides a concrete example that the BIC score is generally not valid for
statistical models that belong to a stratified exponential family. This stands
in contrast to linear and curved exponential families, where the BIC score has
been proven to provide a correct approximation for the marginal likelihood.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0599</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0599</id><created>2012-12-12</created><authors><author><keyname>Schapire</keyname><forenames>Robert E.</forenames></author></authors><title>Advances in Boosting (Invited Talk)</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Eighteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2002)</comments><proxy>auai</proxy><report-no>UAI-P-2002-PG-446-452</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Boosting is a general method of generating many simple classification rules
and combining them into a single, highly accurate rule. In this talk, I will
review the AdaBoost boosting algorithm and some of its underlying theory, and
then look at how this theory has helped us to face some of the challenges of
applying AdaBoost in two domains: In the first of these, we used boosting for
predicting and modeling the uncertainty of prices in complicated, interacting
auctions. The second application was to the classification of caller utterances
in a telephone spoken-dialogue system where we faced two challenges: the need
to incorporate prior knowledge to compensate for initially insufficient data;
and a later need to filter the large stream of unlabeled examples being
collected to select the ones whose labels are likely to be most informative.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0600</identifier>
 <datestamp>2015-05-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0600</id><created>2012-12-12</created><updated>2015-05-16</updated><authors><author><keyname>Shani</keyname><forenames>Guy</forenames></author><author><keyname>Brafman</keyname><forenames>Ronen I.</forenames></author><author><keyname>Heckerman</keyname><forenames>David</forenames></author></authors><title>An MDP-based Recommender System</title><categories>cs.LG cs.AI cs.IR</categories><comments>Appears in Proceedings of the Eighteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2002)</comments><proxy>Martijn de Jongh</proxy><report-no>UAI-P-2002-PG-453-460</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Typical Recommender systems adopt a static view of the recommendation process
and treat it as a prediction problem. We argue that it is more appropriate to
view the problem of generating recommendations as a sequential decision problem
and, consequently, that Markov decision processes (MDP) provide a more
appropriate model for Recommender systems. MDPs introduce two benefits: they
take into account the long-term effects of each recommendation, and they take
into account the expected value of each recommendation. To succeed in practice,
an MDP-based Recommender system must employ a strong initial model; and the
bulk of this paper is concerned with the generation of such a model. In
particular, we suggest the use of an n-gram predictive model for generating the
initial MDP. Our n-gram model induces a Markov-chain model of user behavior
whose predictive accuracy is greater than that of existing predictive models.
We describe our predictive model in detail and evaluate its performance on real
data. In addition, we show how the model can be used in an MDP-based
Recommender system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0601</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0601</id><created>2012-12-12</created><authors><author><keyname>Shelton</keyname><forenames>Christian R.</forenames></author></authors><title>Reinforcement Learning with Partially Known World Dynamics</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Eighteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2002)</comments><proxy>auai</proxy><report-no>UAI-P-2002-PG-461-468</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Reinforcement learning would enjoy better success on real-world problems if
domain knowledge could be imparted to the algorithm by the modelers. Most
problems have both hidden state and unknown dynamics. Partially observable
Markov decision processes (POMDPs) allow for the modeling of both.
Unfortunately, they do not provide a natural framework in which to specify
knowledge about the domain dynamics. The designer must either admit to knowing
nothing about the dynamics or completely specify the dynamics (thereby turning
it into a planning problem). We propose a new framework called a partially
known Markov decision process (PKMDP) which allows the designer to specify
known dynamics while still leaving portions of the environment s dynamics
unknown.The model represents NOT ONLY the environment dynamics but also the
agents knowledge of the dynamics. We present a reinforcement learning algorithm
for this model based on importance sampling. The algorithm incorporates
planning based on the known dynamics and learning about the unknown dynamics.
Our results clearly demonstrate the ability to add domain knowledge and the
resulting benefits for learning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0602</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0602</id><created>2012-12-12</created><authors><author><keyname>Steck</keyname><forenames>Harald</forenames></author><author><keyname>Jaakkola</keyname><forenames>Tommi S.</forenames></author></authors><title>Unsupervised Active Learning in Large Domains</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Eighteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2002)</comments><proxy>auai</proxy><report-no>UAI-P-2002-PG-469-476</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Active learning is a powerful approach to analyzing data effectively. We show
that the feasibility of active learning depends crucially on the choice of
measure with respect to which the query is being optimized. The standard
information gain, for example, does not permit an accurate evaluation with a
small committee, a representative subset of the model space. We propose a
surrogate measure requiring only a small committee and discuss the properties
of this new measure. We devise, in addition, a bootstrap approach for committee
selection. The advantages of this approach are illustrated in the context of
recovering (regulatory) network models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0603</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0603</id><created>2012-12-12</created><authors><author><keyname>Takikawa</keyname><forenames>Masami</forenames></author><author><keyname>D'Ambrosio</keyname><forenames>Bruce</forenames></author><author><keyname>Wright</keyname><forenames>Ed</forenames></author></authors><title>Real-Time Inference with Large-Scale Temporal Bayes Nets</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2002)</comments><proxy>auai</proxy><report-no>UAI-P-2002-PG-477-484</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An increasing number of applications require real-time reasoning under
uncertainty with streaming input. The temporal (dynamic) Bayes net formalism
provides a powerful representational framework for such applications. However,
existing exact inference algorithms for dynamic Bayes nets do not scale to the
size of models required for real world applications which often contain
hundreds or even thousands of variables for each time slice. In addition,
existing algorithms were not developed with real-time processing in mind. We
have developed a new computational approach to support real-time exact
inference in large temporal Bayes nets. Our approach tackles scalability by
recognizing that the complexity of the inference depends on the number of
interface nodes between time slices and by exploiting the distinction between
static and dynamic nodes in order to reduce the number of interface nodes and
to factorize their joint probability distribution. We approach the real-time
issue by organizing temporal Bayes nets into static representations, and then
using the symbolic probabilistic inference algorithm to derive analytic
expressions for the static representations. The parts of these expressions that
do not change at each time step are pre-computed. The remaining parts are
compiled into efficient procedural code so that the memory and CPU resources
required by the inference are small and fixed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0604</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0604</id><created>2012-12-12</created><authors><author><keyname>Taskar</keyname><forenames>Ben</forenames></author><author><keyname>Abbeel</keyname><forenames>Pieter</forenames></author><author><keyname>Koller</keyname><forenames>Daphne</forenames></author></authors><title>Discriminative Probabilistic Models for Relational Data</title><categories>cs.LG cs.AI stat.ML</categories><comments>Appears in Proceedings of the Eighteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2002)</comments><proxy>auai</proxy><report-no>UAI-P-2002-PG-485-492</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In many supervised learning tasks, the entities to be labeled are related to
each other in complex ways and their labels are not independent. For example,
in hypertext classification, the labels of linked pages are highly correlated.
A standard approach is to classify each entity independently, ignoring the
correlations between them. Recently, Probabilistic Relational Models, a
relational version of Bayesian networks, were used to define a joint
probabilistic model for a collection of related entities. In this paper, we
present an alternative framework that builds on (conditional) Markov networks
and addresses two limitations of the previous approach. First, undirected
models do not impose the acyclicity constraint that hinders representation of
many important relational dependencies in directed models. Second, undirected
models are well suited for discriminative training, where we optimize the
conditional likelihood of the labels given the features, which generally
improves classification accuracy. We show how to train these models
effectively, and how to use approximate probabilistic inference over the
learned model for collective classification of multiple related entities. We
provide experimental results on a webpage classification task, showing that
accuracy can be significantly improved by modeling relational dependencies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0605</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0605</id><created>2012-12-12</created><authors><author><keyname>Tatikonda</keyname><forenames>Sekhar</forenames></author><author><keyname>Jordan</keyname><forenames>Michael I.</forenames></author></authors><title>Loopy Belief Propogation and Gibbs Measures</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2002)</comments><proxy>auai</proxy><report-no>UAI-P-2002-PG-493-500</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We address the question of convergence in the loopy belief propagation (LBP)
algorithm. Specifically, we relate convergence of LBP to the existence of a
weak limit for a sequence of Gibbs measures defined on the LBP s associated
computation tree.Using tools FROM the theory OF Gibbs measures we develop
easily testable sufficient conditions FOR convergence.The failure OF
convergence OF LBP implies the existence OF multiple phases FOR the associated
Gibbs specification.These results give new insight INTO the mechanics OF the
algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0606</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0606</id><created>2012-12-12</created><authors><author><keyname>Thiebaux</keyname><forenames>Sylvie</forenames></author><author><keyname>Kabanza</keyname><forenames>Froduald</forenames></author><author><keyname>Slanley</keyname><forenames>John</forenames></author></authors><title>Anytime State-Based Solution Methods for Decision Processes with
  non-Markovian Rewards</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2002)</comments><proxy>auai</proxy><report-no>UAI-P-2002-PG-501-510</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A popular approach to solving a decision process with non-Markovian rewards
(NMRDP) is to exploit a compact representation of the reward function to
automatically translate the NMRDP into an equivalent Markov decision process
(MDP) amenable to our favorite MDP solution method. The contribution of this
paper is a representation of non-Markovian reward functions and a translation
into MDP aimed at making the best possible use of state-based anytime
algorithms as the solution method. By explicitly constructing and exploring
only parts of the state space, these algorithms are able to trade computation
time for policy quality, and have proven quite effective in dealing with large
MDPs. Our representation extends future linear temporal logic (FLTL) to express
rewards. Our translation has the effect of embedding model-checking in the
solution method. It results in an MDP of the minimal size achievable without
stepping outside the anytime framework, and consequently in better policies by
the deadline.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0607</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0607</id><created>2012-12-12</created><authors><author><keyname>Thrun</keyname><forenames>Sebastian</forenames></author></authors><title>Particle Filters in Robotics (Invited Talk)</title><categories>cs.RO cs.AI</categories><comments>Appears in Proceedings of the Eighteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2002)</comments><proxy>auai</proxy><report-no>UAI-P-2002-PG-511-518</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This presentation will introduce the audience to a new, emerging body of
research on sequential Monte Carlo techniques in robotics. In recent years,
particle filters have solved several hard perceptual robotic problems. Early
successes were limited to low-dimensional problems, such as the problem of
robot localization in environments with known maps. More recently, researchers
have begun exploiting structural properties of robotic domains that have led to
successful particle filter applications in spaces with as many as 100,000
dimensions. The presentation will discuss specific tricks necessary to make
these techniques work in real - world domains,and also discuss open challenges
for researchers IN the UAI community.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0608</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0608</id><created>2012-12-12</created><authors><author><keyname>Tian</keyname><forenames>Jin</forenames></author><author><keyname>Pearl</keyname><forenames>Judea</forenames></author></authors><title>On the Testable Implications of Causal Models with Hidden Variables</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2002)</comments><proxy>auai</proxy><report-no>UAI-P-2002-PG-519-527</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The validity OF a causal model can be tested ONLY IF the model imposes
constraints ON the probability distribution that governs the generated data. IN
the presence OF unmeasured variables, causal models may impose two types OF
constraints : conditional independencies, AS READ through the d - separation
criterion, AND functional constraints, FOR which no general criterion IS
available.This paper offers a systematic way OF identifying functional
constraints AND, thus, facilitates the task OF testing causal models AS well AS
inferring such models FROM data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0609</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0609</id><created>2012-12-12</created><authors><author><keyname>Vomlel</keyname><forenames>Jirka</forenames></author></authors><title>Exploiting Functional Dependence in Bayesian Network Inference</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2002)</comments><proxy>auai</proxy><report-no>UAI-P-2002-PG-528-535</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an efficient method for Bayesian network inference in models with
functional dependence. We generalize the multiplicative factorization method
originally designed by Takikawa and D Ambrosio(1999) FOR models WITH
independence OF causal influence.Using a hidden variable, we transform a
probability potential INTO a product OF two - dimensional potentials.The
multiplicative factorization yields more efficient inference. FOR example, IN
junction tree propagation it helps TO avoid large cliques. IN ORDER TO keep
potentials small, the number OF states OF the hidden variable should be
minimized.We transform this problem INTO a combinatorial problem OF minimal
base IN a particular space.We present an example OF a computerized adaptive
test, IN which the factorization method IS significantly more efficient than
previous inference methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0610</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0610</id><created>2012-12-12</created><authors><author><keyname>Wainwright</keyname><forenames>Martin</forenames></author><author><keyname>Jaakkola</keyname><forenames>Tommi S.</forenames></author><author><keyname>Willsky</keyname><forenames>Alan</forenames></author></authors><title>A New Class of Upper Bounds on the Log Partition Function</title><categories>cs.LG stat.ML</categories><comments>Appears in Proceedings of the Eighteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2002)</comments><proxy>auai</proxy><report-no>UAI-P-2002-PG-536-543</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Bounds on the log partition function are important in a variety of contexts,
including approximate inference, model fitting, decision theory, and large
deviations analysis. We introduce a new class of upper bounds on the log
partition function, based on convex combinations of distributions in the
exponential domain, that is applicable to an arbitrary undirected graphical
model. In the special case of convex combinations of tree-structured
distributions, we obtain a family of variational problems, similar to the Bethe
free energy, but distinguished by the following desirable properties: i. they
are cnvex, and have a unique global minimum; and ii. the global minimum gives
an upper bound on the log partition function. The global minimum is defined by
stationary conditions very similar to those defining fixed points of belief
propagation or tree-based reparameterization Wainwright et al., 2001. As with
BP fixed points, the elements of the minimizing argument can be used as
approximations to the marginals of the original model. The analysis described
here can be extended to structures of higher treewidth e.g., hypertrees,
thereby making connections with more advanced approximations e.g., Kikuchi and
variants Yedidia et al., 2001; Minka, 2001.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0611</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0611</id><created>2012-12-12</created><authors><author><keyname>Wakker</keyname><forenames>Peter P.</forenames></author></authors><title>Decision Principles to justify Carnap's Updating Method and to Suggest
  Corrections of Probability Judgments (Invited Talks)</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2002)</comments><proxy>auai</proxy><report-no>UAI-P-2002-PG-544-551</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper uses decision-theoretic principles to obtain new insights into the
assessment and updating of probabilities. First, a new foundation of
Bayesianism is given. It does not require infinite atomless uncertainties as
did Savage s classical result, AND can therefore be applied TO ANY finite
Bayesian network.It neither requires linear utility AS did de Finetti s
classical result, AND r ntherefore allows FOR the empirically AND normatively
desirable risk r naversion.Finally, BY identifying AND fixing utility IN an
elementary r nmanner, our result can readily be applied TO identify methods OF
r nprobability updating.Thus, a decision - theoretic foundation IS given r nto
the computationally efficient method OF inductive reasoning r ndeveloped BY
Rudolf Carnap.Finally, recent empirical findings ON r nprobability assessments
are discussed.It leads TO suggestions FOR r ncorrecting biases IN probability
assessments, AND FOR an alternative r nto the Dempster - Shafer belief
functions that avoids the reduction TO r ndegeneracy after multiple updatings.r
n
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0612</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0612</id><created>2012-12-12</created><authors><author><keyname>Wang</keyname><forenames>Yang</forenames></author><author><keyname>Tan</keyname><forenames>Tele</forenames></author></authors><title>Adaptive Foreground and Shadow Detection inImage Sequences</title><categories>cs.CV</categories><comments>Appears in Proceedings of the Eighteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2002)</comments><proxy>auai</proxy><report-no>UAI-P-2002-PG-552-559</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper presents a novel method of foreground segmentation that
distinguishes moving objects from their moving cast shadows in monocular image
sequences. The models of background, edge information, and shadow are set up
and adaptively updated. A Bayesian belief network is proposed to describe the
relationships among the segmentation label, background, intensity, and edge
information. The notion of Markov random field is used to encourage the spatial
connectivity of the segmented regions. The solution is obtained by maximizing
the posterior possibility density of the segmentation field.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0613</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0613</id><created>2012-12-12</created><authors><author><keyname>Wiegerinck</keyname><forenames>Wim</forenames></author><author><keyname>Heskes</keyname><forenames>Tom</forenames></author></authors><title>IPF for Discrete Chain Factor Graphs</title><categories>cs.LG cs.AI stat.ML</categories><comments>Appears in Proceedings of the Eighteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2002)</comments><proxy>auai</proxy><report-no>UAI-P-2002-PG-560-567</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Iterative Proportional Fitting (IPF), combined with EM, is commonly used as
an algorithm for likelihood maximization in undirected graphical models. In
this paper, we present two iterative algorithms that generalize upon IPF. The
first one is for likelihood maximization in discrete chain factor graphs, which
we define as a wide class of discrete variable models including undirected
graphical models and Bayesian networks, but also chain graphs and sigmoid
belief networks. The second one is for conditional likelihood maximization in
standard undirected models and Bayesian networks. In both algorithms, the
iteration steps are expressed in closed form. Numerical simulations show that
the algorithms are competitive with state of the art methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0614</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0614</id><created>2012-12-12</created><authors><author><keyname>Yoon</keyname><forenames>Sung Wook</forenames></author><author><keyname>Fern</keyname><forenames>Alan</forenames></author><author><keyname>Givan</keyname><forenames>Robert</forenames></author></authors><title>Inductive Policy Selection for First-Order MDPs</title><categories>cs.AI</categories><comments>Appears in Proceedings of the Eighteenth Conference on Uncertainty in
  Artificial Intelligence (UAI2002)</comments><proxy>auai</proxy><report-no>UAI-P-2002-PG-568-576</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We select policies for large Markov Decision Processes (MDPs) with compact
first-order representations. We find policies that generalize well as the
number of objects in the domain grows, potentially without bound. Existing
dynamic-programming approaches based on flat, propositional, or first-order
representations either are impractical here or do not naturally scale as the
number of objects grows without bound. We implement and evaluate an alternative
approach that induces first-order policies using training data constructed by
solving small problem instances using PGraphplan (Blum &amp; Langford, 1999). Our
policies are represented as ensembles of decision lists, using a taxonomic
concept language. This approach extends the work of Martin and Geffner (2000)
to stochastic domains, ensemble learning, and a wider variety of problems.
Empirically, we find &quot;good&quot; policies for several stochastic first-order MDPs
that are beyond the scope of previous approaches. We also discuss the
application of this work to the relational reinforcement-learning problem.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0633</identifier>
 <datestamp>2013-07-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0633</id><created>2013-01-03</created><authors><author><keyname>Kazemi</keyname><forenames>Sanaz</forenames></author><author><keyname>Yatawatta</keyname><forenames>Sarod</forenames></author><author><keyname>Zaroubi</keyname><forenames>Saleem</forenames></author></authors><title>Clustered Calibration: An Improvement to Radio Interferometric Direction
  Dependent Self-Calibration</title><categories>astro-ph.IM cs.CE stat.AP</categories><comments>18 pages, 21 figures, Accepted 2013 January 2. Abstract abridged</comments><journal-ref>Monthly Notices of the Royal Astronomical Society, Volume 430,
  Issue 2, p.1457-1472, 2013</journal-ref><doi>10.1093/mnras/stt018</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The new generation of radio synthesis arrays, such as LOFAR and SKA, have
been designed to surpass existing arrays in terms of sensitivity, angular
resolution and frequency coverage. This evolution has led to the development of
advanced calibration techniques that ensure the delivery of accurate results at
the lowest possible computational cost. However, the performance of such
calibration techniques is still limited by the compact, bright sources in the
sky, used as calibrators. It is important to have a bright enough source that
is well distinguished from the background noise level in order to achieve
satisfactory results in calibration. We present &quot;clustered calibration&quot; as a
modification to traditional radio interferometric calibration, in order to
accommodate faint sources that are almost below the background noise level into
the calibration process. The main idea is to employ the information of the
bright sources' measured signals as an aid to calibrate fainter sources that
are nearby the bright sources. In the case where we do not have bright enough
sources, a source cluster could act as a bright source that can be
distinguished from background noise. We construct a number of source clusters
assuming that the signals of the sources belonging to a single cluster are
corrupted by almost the same errors, and each cluster is calibrated as a single
source, using the combined coherencies of its sources simultaneously. This
upgrades the power of an individual faint source by the effective power of its
cluster. We give performance analysis of clustered calibration to show the
superiority of this approach compared to the traditional unclustered
calibration. We also provide analytical criteria to choose the optimum number
of clusters for a given observation in an efficient manner.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0647</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0647</id><created>2013-01-03</created><authors><author><keyname>Mani</keyname><forenames>A.</forenames></author></authors><title>Algebraic Semantics of Similarity-Based Bitten Rough Set Theory</title><categories>math.LO cs.LO cs.MA</categories><comments>Almost the same as my published paper in FI</comments><msc-class>03G25, 06F99, 08A55</msc-class><journal-ref>Mani, A.: Algebraic semantics of similarity-based bitten rough set
  theory. Fundamenta Informaticae 97 (2009) 177--197</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We develop two algebraic semantics for bitten rough set theory (\cite{SW})
over similarity spaces and their abstract granular versions. Connections with
choice based generalized rough semantics developed in \cite{AM69} by the
present author and general cover based rough set theories are also considered.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0648</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0648</id><created>2013-01-03</created><authors><author><keyname>Alkhalaf</keyname><forenames>Salem</forenames></author><author><keyname>Drew</keyname><forenames>Steve</forenames></author><author><keyname>Nguyen</keyname><forenames>Anne</forenames></author></authors><title>Validation of the IS Impact Model for Measuring the Impact of e-Learning
  Systems in KSA Universities: Student Perspective</title><categories>cs.CY</categories><comments>6 Pages, International Journal of Advanced Computer Science and
  Applications (IJACSA) 2012 Vol. 3, No. 5, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The IS-Impact Measurement Model, developed by Gable, Sedera and Chan in 2008,
represents the to-date and expected stream of net profits from a given
information system (IS), as perceived by all major user classes. Although this
model has been stringently validated in previous studies, its generalizability
and verified effectiveness are enhanced through this new application in
e-learning. This paper focuses on the re-validation of the findings of the
IS-Impact Model in two universities in the Kingdom of Saudi Arabia (KSA). Among
the users of 2 universities e-learning systems, 528 students were recruited. A
formative validation measurement with SmartPLS, a graphical structural equation
modeling tool was used to analyse the collected data. On the basis of the
SmartPLS results, as well as with the aid of data-supported IS impact
measurements and dimensions, we confirmed the validity of the IS-Impact Model
for assessing the effect of e-learning systems in KSA universities. The newly
constructed model is more understandable, its use was proved as robust and
applicable to various circumstances.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0667</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0667</id><created>2013-01-03</created><authors><author><keyname>Luo</keyname><forenames>Zhaohua</forenames></author></authors><title>Algebraic Logic, I Quantifier Theories and Completeness Theorems</title><categories>cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Algebraic logic studies algebraic theories related to proposition and
first-order logic. A new algebraic approach to first-order logic is sketched in
this paper. We introduce the notion of a quantifier theory, which is a functor
from the category of a monad of sets to the category of Boolean algebras,
together with a uniquely determined system of quantifiers. A striking feature
of this approach is that Cayley's Completeness Theorem and Godel's Completeness
Theorem can be stated and proved in a much simpler fashion for quantifier
theories. Both theorems are due to Halmos for polyadic algebras. We also
present a simple transparent treatment of ultraproducts of models of a
quantifier theory.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0669</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0669</id><created>2013-01-03</created><authors><author><keyname>Zhang</keyname><forenames>Guanghui</forenames></author><author><keyname>Chen</keyname><forenames>Bocong</forenames></author></authors><title>Constacyclic Codes over $F_p+vF_p$</title><categories>cs.IT math.IT</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we study constacyclic codes over $F_p+vF_p$, where $p$ is an
odd prime and $v^2=v$. The polynomial generators of all constacyclic codes over
$F_p+vF_p$ are characterized and their dual codes are also determined.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0683</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0683</id><created>2013-01-04</created><authors><author><keyname>Demichev</keyname><forenames>A.</forenames></author><author><keyname>Ilyin</keyname><forenames>V.</forenames></author><author><keyname>Kryukov</keyname><forenames>A.</forenames></author><author><keyname>Polyakov</keyname><forenames>S.</forenames></author></authors><title>A Quality and Cost Approach for Comparison of Small-World Networks</title><categories>cs.SI physics.soc-ph</categories><comments>27 pages, 16 figures, 1 table</comments><journal-ref>J. Inter. Net. 14: 1350008, 2013</journal-ref><doi>10.1142/S0219265913500084</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose an approach based on analysis of cost-quality tradeoffs for
comparison of efficiency of various algorithms for small-world network
construction. A number of both known in the literature and original algorithms
for complex small-world networks construction are shortly reviewed and
compared. The networks constructed on the basis of these algorithms have basic
structure of 1D regular lattice with additional shortcuts providing the
small-world properties. It is shown that networks proposed in this work have
the best cost-quality ratio in the considered class.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0700</identifier>
 <datestamp>2013-03-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0700</id><created>2013-01-04</created><updated>2013-03-17</updated><authors><author><keyname>Chepuri</keyname><forenames>Sundeep Prabhakar</forenames></author><author><keyname>Leus</keyname><forenames>Geert</forenames></author><author><keyname>van der Veen</keyname><forenames>Alle-Jan</forenames></author></authors><title>Position and Orientation Estimation of a Rigid Body: Rigid Body
  Localization</title><categories>cs.IT math.IT</categories><comments>4 pages and 1 reference page; 3 Figures; In Proc. of ICASSP 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Rigid body localization refers to a problem of estimating the position of a
rigid body along with its orientation using anchors. We consider a setup in
which a few sensors are mounted on a rigid body. The absolute position of the
rigid body is not known, but, the relative position of the sensors or the
topology of the sensors on the rigid body is known. We express the absolute
position of the sensors as an affine function of the Stiefel manifold and
propose a simple least-squares (LS) estimator as well as a constrained total
least-squares (CTLS) estimator to jointly estimate the orientation and the
position of the rigid body. To account for the perturbations of the sensors, we
also propose a constrained total least-squares (CTLS) estimator. Analytical
closed-form solutions for the proposed estimators are provided. Simulations are
used to corroborate and analyze the performance of the proposed estimators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0701</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0701</id><created>2013-01-04</created><authors><author><keyname>Prasath</keyname><forenames>R. Rajendra</forenames></author><author><keyname>&#xd6;zt&#xfc;rk</keyname><forenames>Pinar</forenames></author></authors><title>Similarity Assessment through blocking and affordance assignment in
  Textual CBR</title><categories>cs.IR cs.AI</categories><comments>10 pages, 3 figures, WebCBR 2010, Alessandria, Italy</comments><journal-ref>in: Proc. of the Reasoning from Experiences on the Web (WebCBR
  2010), pp. 151-160, July 2010</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It has been conceived that children learn new objects through their
affordances, that is, the actions that can be taken on them. We suggest that
web pages also have affordances defined in terms of the users' information need
they meet. An assumption of the proposed approach is that different parts of a
text may not be equally important / relevant to a given query. Judgment on the
relevance of a web document requires, therefore, a thorough look into its
parts, rather than treating it as a monolithic content. We propose a method to
extract and assign affordances to texts and then use these affordances to
retrieve the corresponding web pages. The overall approach presented in the
paper relies on case-based representations that bridge the queries to the
affordances of web documents. We tested our method on the tourism domain and
the results are promising.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0702</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0702</id><created>2013-01-04</created><authors><author><keyname>Chepuri</keyname><forenames>Sundeep Prabhakar</forenames></author><author><keyname>Leus</keyname><forenames>Geert</forenames></author><author><keyname>van der Veen</keyname><forenames>Alle-Jan</forenames></author></authors><title>Joint localization and clock synchronization for wireless sensor
  networks</title><categories>cs.IT math.IT</categories><comments>5 pages, 4 figures, to appear in proc. of Asilomar 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A fully-asynchronous network with one target sensor and a few anchors (nodes
with known locations) is considered. Localization and synchronization are
traditionally treated as two separate problems. In this paper, localization and
synchronization is studied under a unified framework. We present a new model in
which time-stamps obtained either via two-way communication between the nodes
or with a broadcast based protocol can be used in a simple estimator based on
least-squares (LS) to jointly estimate the position of the target node as well
as all the unknown clock-skews and clock-offsets. The Cram\'er-Rao lower bound
(CRLB) is derived for the considered problem and is used as a benchmark to
analyze the performance of the proposed estimator.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0722</identifier>
 <datestamp>2015-12-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0722</id><created>2013-01-04</created><updated>2015-12-03</updated><authors><author><keyname>Gerdjikov</keyname><forenames>Stefan</forenames></author><author><keyname>Mihov</keyname><forenames>Stoyan</forenames></author><author><keyname>Mitankin</keyname><forenames>Petar</forenames></author><author><keyname>Schulz</keyname><forenames>Klaus U.</forenames></author></authors><title>Good parts first - a new algorithm for approximate search in lexica and
  string databases</title><categories>cs.CL cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a new efficient method for approximate search in electronic
lexica. Given an input string (the pattern) and a similarity threshold, the
algorithm retrieves all entries of the lexicon that are sufficiently similar to
the pattern. Search is organized in subsearches that always start with an exact
partial match where a substring of the input pattern is aligned with a
substring of a lexicon word. Afterwards this partial match is extended stepwise
to larger substrings. For aligning further parts of the pattern with
corresponding parts of lexicon entries, more errors are tolerated at each
subsequent step. For supporting this alignment order, which may start at any
part of the pattern, the lexicon is represented as a structure that enables
immediate access to any substring of a lexicon word and permits the extension
of such substrings in both directions. Experimental evaluations of the
approximate search procedure are given that show significant efficiency
improvements compared to existing techniques. Since the technique can be used
for large error bounds it offers interesting possibilities for approximate
search in special collections of &quot;long&quot; strings, such as phrases, sentences, or
book ti
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0725</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0725</id><created>2013-01-04</created><authors><author><keyname>Senelle</keyname><forenames>Mathieu</forenames></author><author><keyname>Garcia-Diez</keyname><forenames>Silvia</forenames></author><author><keyname>Mantrach</keyname><forenames>Amin</forenames></author><author><keyname>Shimbo</keyname><forenames>Masashi</forenames></author><author><keyname>Saerens</keyname><forenames>Marco</forenames></author><author><keyname>Fouss</keyname><forenames>Fran&#xe7;ois</forenames></author></authors><title>The Sum-over-Forests density index: identifying dense regions in a graph</title><categories>cs.LG stat.ML</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work introduces a novel nonparametric density index defined on graphs,
the Sum-over-Forests (SoF) density index. It is based on a clear and intuitive
idea: high-density regions in a graph are characterized by the fact that they
contain a large amount of low-cost trees with high outdegrees while low-density
regions contain few ones. Therefore, a Boltzmann probability distribution on
the countable set of forests in the graph is defined so that large (high-cost)
forests occur with a low probability while short (low-cost) forests occur with
a high probability. Then, the SoF density index of a node is defined as the
expected outdegree of this node in a non-trivial tree of the forest, thus
providing a measure of density around that node. Following the matrix-forest
theorem, and a statistical physics framework, it is shown that the SoF density
index can be easily computed in closed form through a simple matrix inversion.
Experiments on artificial and real data sets show that the proposed index
performs well on finding dense regions, for graphs of various origins.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0730</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0730</id><created>2013-01-04</created><authors><author><keyname>Benkhelifa</keyname><forenames>Fatma</forenames></author><author><keyname>Rezki</keyname><forenames>Zouheir</forenames></author><author><keyname>Alouini</keyname><forenames>Mohamed-Slim</forenames></author></authors><title>On the Low SNR Capacity of Maximum Ratio Combining over Rician Fading
  Channels with Full Channel State Information</title><categories>cs.IT math.IT</categories><comments>4 pages, 3 figures, accepted for publication to Wireless
  Communication Letters</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this letter, we study the ergodic capacity of a maximum ratio combining
(MRC) Rician fading channel with full channel state information (CSI) at the
transmitter and at the receiver. We focus on the low Signal-to-Noise Ratio
(SNR) regime and we show that the capacity scales as (L Omega/(K+L)) SNR
log(1/SNR), where Omega is the expected channel gain per branch, K is the
Rician fading factor, and L is the number of diversity branches. We show that
one-bit CSI feedback at the transmitter is enough to achieve this capacity
using an on-off power control scheme. Our framework can be seen as a
generalization of recently established results regarding the fading-channels
capacity characterization in the low-SNR regime.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0745</identifier>
 <datestamp>2013-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0745</id><created>2013-01-04</created><updated>2013-01-15</updated><authors><author><keyname>Chen</keyname><forenames>Danny Z.</forenames></author><author><keyname>Li</keyname><forenames>Jian</forenames></author><author><keyname>Liang</keyname><forenames>Hongyu</forenames></author><author><keyname>Wang</keyname><forenames>Haitao</forenames></author></authors><title>Matroid and Knapsack Center Problems</title><categories>cs.DS cs.DM</categories><comments>A preliminary version of this paper is accepted to IPCO 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the classic $k$-center problem, we are given a metric graph, and the
objective is to open $k$ nodes as centers such that the maximum distance from
any vertex to its closest center is minimized. In this paper, we consider two
important generalizations of $k$-center, the matroid center problem and the
knapsack center problem. Both problems are motivated by recent content
distribution network applications. Our contributions can be summarized as
follows:
  1. We consider the matroid center problem in which the centers are required
to form an independent set of a given matroid. We show this problem is NP-hard
even on a line. We present a 3-approximation algorithm for the problem on
general metrics. We also consider the outlier version of the problem where a
given number of vertices can be excluded as the outliers from the solution. We
present a 7-approximation for the outlier version.
  2. We consider the (multi-)knapsack center problem in which the centers are
required to satisfy one (or more) knapsack constraint(s). It is known that the
knapsack center problem with a single knapsack constraint admits a
3-approximation. However, when there are at least two knapsack constraints, we
show this problem is not approximable at all. To complement the hardness
result, we present a polynomial time algorithm that gives a 3-approximate
solution such that one knapsack constraint is satisfied and the others may be
violated by at most a factor of $1+\epsilon$. We also obtain a 3-approximation
for the outlier version that may violate the knapsack constraint by
$1+\epsilon$.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0748</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0748</id><created>2013-01-04</created><authors><author><keyname>Charousset</keyname><forenames>Dominik</forenames></author><author><keyname>Schmidt</keyname><forenames>Thomas C.</forenames></author></authors><title>libcppa - Designing an Actor Semantic for C++11</title><categories>cs.PL</categories><comments>10 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Parallel hardware makes concurrency mandatory for efficient program
execution. However, writing concurrent software is both challenging and
error-prone. C++11 provides standard facilities for multiprogramming, such as
atomic operations with acquire/release semantics and RAII mutex locking, but
these primitives remain too low-level. Using them both correctly and
efficiently still requires expert knowledge and hand-crafting. The actor model
replaces implicit communication by sharing with an explicit message passing
mechanism. It applies to concurrency as well as distribution, and a lightweight
actor model implementation that schedules all actors in a properly
pre-dimensioned thread pool can outperform equivalent thread-based
applications. However, the actor model did not enter the domain of native
programming languages yet besides vendor-specific island solutions. With the
open source library libcppa, we want to combine the ability to build reliable
and distributed systems provided by the actor model with the performance and
resource-efficiency of C++11.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0759</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0759</id><created>2013-01-04</created><authors><author><keyname>Poncet</keyname><forenames>Paul</forenames></author></authors><title>Pruning a poset with veins</title><categories>cs.DM math.CO</categories><comments>8 pages</comments><msc-class>06A05, 06A06</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We recall some abstract connectivity concepts, and apply them to special
chains in partially ordered sets, called veins, that are defined as
order-convex chains that are contained in every maximal chain they meet. Veins
enable us to define a new partial order on the same underlying set, called the
pruning order. The associated pruned poset is simpler than the initial poset,
but irreducible, coirreducible, and doubly-irreducible elements are preserved
by the operation of pruning.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0762</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0762</id><created>2013-01-04</created><authors><author><keyname>Cordier</keyname><forenames>H&#xe9;l&#xe8;ne</forenames><affiliation>CC IN2P3</affiliation></author><author><keyname>L'Orphelin</keyname><forenames>Cyril</forenames><affiliation>CC IN2P3</affiliation></author><author><keyname>Reynaud</keyname><forenames>Sylvain</forenames><affiliation>CC IN2P3</affiliation></author><author><keyname>Lequeux</keyname><forenames>Olivier</forenames><affiliation>CC IN2P3</affiliation></author><author><keyname>Loikkanen</keyname><forenames>Sinika</forenames><affiliation>CC IN2P3</affiliation></author><author><keyname>Veyre</keyname><forenames>Pierre</forenames><affiliation>CC IN2P3</affiliation></author></authors><title>From EGEE OPerations Portal towards EGI OPerations Portal</title><categories>cs.SE cs.DC</categories><comments>arXiv admin note: text overlap with arXiv:1301.0523</comments><proxy>ccsd</proxy><journal-ref>ISGC 2010, Taipei : Taiwan, Province Of China (2010)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  EGEE to EGI structure based on NGIs evolution induces a large move from the
operations that will rely on a sustainable and largely decentralized model. One
of the key evolutions for the challenge in the regionalisation relies in the
scalability and the flexibility required regarding information source types and
information handling. For 5 years, we have developed and maintained a
standard-based component that allows us to address both theses issues. This
open-source tool, named Lavoisier, has been a critical success factor for the
operations dashboard, one of the Operations Portal main features. Indeed, it
enables coherent efficient and reliable data handling which is customizable and
scalable, as Lavoisier is an extensible service designed to provide a unified
view of data collected from multiple heterogeneous data sources. Data views are
represented and accessed as XML documents through standard languages such as
XSLT, XPath. Moreover, scalability and reliability are enforced by a caching
mechanism adaptable to specific data sources and use-cases. We will namely
expose how the concept and the implementation enable clear roles separation
between plug-in developer, service configuration administrator or end-user.
Also, maintainability of the portal code has increased dramatically as the
latter is now independent from the data sources technology or from the cache
management policies. Finally, integration of data has recently been simplified
as the service administrator proceeds now through web interfaces
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0763</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0763</id><created>2013-01-04</created><authors><author><keyname>Pasquini</keyname><forenames>Lorenzo</forenames></author></authors><title>Improved QFT algorithm for power-of-two FFT</title><categories>cs.DS cs.MS</categories><msc-class>F.2.1, G.4</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper shows that it is possible to improve the computational cost, the
memory requirements and the accuracy of Quick Fourier Transform (QFT) algorithm
for power-of-two FFT (Fast Fourier Transform) just introducing a slight
modification in this algorithm. The new algorithm requires the same number of
additions and multiplications of split-radix 3add/3mul, one of the most
appreciated FFT algorithms appeared in the literature, but employing only half
of the trigonometric constants. These results can elevate the QFT approach to
the level of most used FFT procedures. A new quite general way to describe FFT
algorithms, based on signal types and on a particular notation, is also
proposed and used, highligting its advantages.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0775</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0775</id><created>2013-01-04</created><authors><author><keyname>Zarmehri</keyname><forenames>Mohammad Nozari</forenames></author><author><keyname>Aguiar</keyname><forenames>Ana</forenames></author></authors><title>Supporting Sensing Application in Vehicular Networks</title><categories>cs.NI</categories><comments>7 pages, 9 figures, 2 tables</comments><journal-ref>ACM MobiCom Workshop on Challenged Networks, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This research aims at using vehicular ad-hoc networks as infra-structure for
an urban cyber-physical system in order to gather data about a city. In this
scenario, all nodes are data sources and there is a gateway as ultimate
destination for all packets. Because of the volatility of the network
connections and uncertainty of actual node placement, we argue that a
broadcast-based protocol is the most adequate solution, despite the high
overhead. The Urban Data Collector (UDC) protocol has been proposed which uses
a distributed election of the forwarding node among the nodes receiving the
packet: nodes that are nearer to the gateway have shorter timers and a higher
forwarding probabilities. The performance of the UDC protocol has been
evaluated with different suppression levels in terms of the amount of collected
data from each road segment using NS-3, and our results show that UDC can
achieve significantly higher sensing accuracy than to other broadcast-based
protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0785</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0785</id><created>2013-01-04</created><authors><author><keyname>Aldar</keyname><forenames>Dilip S</forenames></author></authors><title>Adaptive Intelligent Cooperative Spectrum Sensing In Cognitive Radio</title><categories>cs.NE</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Radio Spectrum is most precious and scarce resource and must be utilized
efficiently and effectively. Cognitive radio is the promising solutions for the
optimum utilization of the scared natural resource. The spectrum owned by the
primary user should be shared among the secondary user, but primary user should
not be interfered by the secondary user. In order to utilize the primary user
spectrum, secondary user must detect accurately, the existence of primary in
the band of interest. In cooperative spectrum sensing, the channel between the
secondary users and the cognitive radio base station is non stationary and
causes interference in the decision in decision fusion and in information in
information due to multipath fading. In this paper neural network based
cooperative spectrum sensing method is proposed, the performance of proposed
method is evaluated and observed that, the neural network based scheme
performance improve significantly over the AND,OR and Majority rule
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0793</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0793</id><created>2013-01-04</created><authors><author><keyname>Moseley</keyname><forenames>Benjamin</forenames></author><author><keyname>Pruhs</keyname><forenames>Kirk</forenames></author><author><keyname>Stein</keyname><forenames>Cliff</forenames></author></authors><title>The Complexity of Scheduling for p-norms of Flow and Stretch</title><categories>cs.DS</categories><comments>Conference version accepted to IPCO 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider computing optimal k-norm preemptive schedules of jobs that arrive
over time. In particular, we show that computing the optimal k-norm of flow
schedule, is strongly NP-hard for k in (0, 1) and integers k in (1, infinity).
Further we show that computing the optimal k-norm of stretch schedule, is
strongly NP-hard for k in (0, 1) and integers k in (1, infinity).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0801</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0801</id><created>2013-01-04</created><updated>2013-01-31</updated><authors><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author><author><keyname>Wagner</keyname><forenames>Caroline</forenames></author><author><keyname>Park</keyname><forenames>Han Woo</forenames></author><author><keyname>Adams</keyname><forenames>Jonathan</forenames></author></authors><title>International Collaboration in Science: The Global Map and the Network</title><categories>cs.DL</categories><comments>El Profesional de la Informaci\'on (2013, in press)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The network of international co-authorship relations has been dominated by
certain European nations and the USA, but this network is rapidly expanding at
the global level. Between 40 and 50 countries appear in the center of the
international network in 2011, and almost all (201) nations are nowadays
involved in international collaboration. In this brief communication, we
present both a global map with the functionality of a Google Map (zooming,
etc.) and network maps based on normalized relations. These maps reveal
complementary aspects of the network. International collaboration in the
generation of knowledge claims (that is, the context of discovery) changes the
structural layering of the sciences. Previously, validation was at the global
level and discovery more dependent on local contexts. This changing
relationship between the geographical and intellectual dimensions of the
sciences also has implications for national science policies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0802</identifier>
 <datestamp>2015-01-30</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0802</id><created>2013-01-04</created><updated>2015-01-29</updated><authors><author><keyname>Nguyen</keyname><forenames>XuanLong</forenames></author></authors><title>Borrowing strength in hierarchical Bayes: convergence of the Dirichlet
  base measure</title><categories>math.ST cs.LG math.PR stat.TH</categories><comments>62 pages, revised and expanded from the previous versions. To appear
  in Bernoulli</comments><report-no>Technical report 532, Department of Statistics, University of
  Michigan</report-no><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper studies posterior concentration behavior of the base probability
measure $G$ of a Dirichlet measure $\mathcal{D}_{\alpha G}$, given observations
associated with $m$ Dirichlet processes sampled from $\mathcal{D}_{\alpha G}$,
as $m$ and the number of observations $m\times n$ tend to infinity. The base
measure itself is endowed with another Dirichlet prior, a construction known as
the hierarchical Dirichlet processes (Teh et al, 2006). Convergence rates are
established in transportation distances (i.e. Wasserstein metrics) under
various geometrically sparse conditions on the support of the true base
measure. As a consequence of the theory we demonstrate the benefit of
&quot;borrowing strength&quot; in the inference of multiple groups of data --- a
heuristic argument commonly used to motivate hierarchical modeling. In certain
settings, the gain in efficiency due to the latent hierarchy can be dramatic,
improving from a standard nonparametric rate to a parametric rate of
convergence. Tools developed include transportation distances for nonparametric
Bayesian hierarchies of random measures, the existence of tests for Dirichlet
measures, and geometric properties of the support of Dirichlet measures.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0803</identifier>
 <datestamp>2013-03-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0803</id><created>2013-01-04</created><updated>2013-03-05</updated><authors><author><keyname>Liu</keyname><forenames>Zhen</forenames></author><author><keyname>He</keyname><forenames>Jia-Lin</forenames></author><author><keyname>Srivastava</keyname><forenames>Jaideep</forenames></author></authors><title>Cliques in complex networks reveal link formation and community
  evolution</title><categories>cs.SI physics.soc-ph</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Missing link prediction in indirected and un-weighted network is an open and
challenge problem which has been studied intensively in recent years. In this
paper, we studied the relationships between community structure and link
formation and proposed a Fast Block probabilistic Model(FBM). In accordance
with the experiments on four real world networks, we have yielded very good
accuracy of missing link prediction and huge improvement in computing
efficiency compared to conventional methods. By analyzing the mechanism of link
formation, we also discovered that clique structure plays a significant role to
help us understand how links grow in communities. Therefore, we summarized
three principles which are proved to be able to well explain the mechanism of
link formation and network evolution from the theory of graph topology.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0805</identifier>
 <datestamp>2016-01-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0805</id><created>2013-01-04</created><authors><author><keyname>Wiedermann</keyname><forenames>Marc</forenames></author><author><keyname>Donges</keyname><forenames>Jonathan F.</forenames></author><author><keyname>Heitzig</keyname><forenames>Jobst</forenames></author><author><keyname>Kurths</keyname><forenames>J&#xfc;rgen</forenames></author></authors><title>Node-weighted interacting network measures improve the representation of
  real-world complex systems</title><categories>physics.soc-ph cs.SI physics.data-an</categories><comments>7 pages, 5 figures</comments><doi>10.1209/0295-5075/102/28007</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Network theory provides a rich toolbox consisting of methods, measures, and
models for studying the structure and dynamics of complex systems found in
nature, society, or technology. Recently, it has been pointed out that many
real-world complex systems are more adequately mapped by networks of
interacting or interdependent networks, e.g., a power grid showing
interdependency with a communication network. Additionally, in many real-world
situations it is reasonable to include node weights into complex network
statistics to reflect the varying size or importance of subsystems that are
represented by nodes in the network of interest. E.g., nodes can represent
vastly different surface area in climate networks, volume in brain networks or
economic capacity in trade networks. In this letter, combining both ideas, we
derive a novel class of statistical measures for analysing the structure of
networks of interacting networks with heterogeneous node weights. Using a
prototypical spatial network model, we show that the newly introduced
node-weighted interacting network measures indeed provide an improved
representation of the underlying system's properties as compared to their
unweighted analogues. We apply our method to study the complex network
structure of cross-boundary trade between European Union (EU) and non-EU
countries finding that it provides important information on trade balance and
economic robustness.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0820</identifier>
 <datestamp>2013-01-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0820</id><created>2013-01-04</created><authors><author><keyname>Klivans</keyname><forenames>Adam</forenames></author><author><keyname>Meka</keyname><forenames>Raghu</forenames></author></authors><title>Moment-Matching Polynomials</title><categories>cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give a new framework for proving the existence of low-degree, polynomial
approximators for Boolean functions with respect to broad classes of
non-product distributions. Our proofs use techniques related to the classical
moment problem and deviate significantly from known Fourier-based methods,
which require the underlying distribution to have some product structure.
  Our main application is the first polynomial-time algorithm for agnostically
learning any function of a constant number of halfspaces with respect to any
log-concave distribution (for any constant accuracy parameter). This result was
not known even for the case of learning the intersection of two halfspaces
without noise. Additionally, we show that in the &quot;smoothed-analysis&quot; setting,
the above results hold with respect to distributions that have sub-exponential
tails, a property satisfied by many natural and well-studied distributions in
machine learning.
  Given that our algorithms can be implemented using Support Vector Machines
(SVMs) with a polynomial kernel, these results give a rigorous theoretical
explanation as to why many kernel methods work so well in practice.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0834</identifier>
 <datestamp>2013-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0834</id><created>2013-01-04</created><authors><author><keyname>Saeed</keyname><forenames>Fahad</forenames></author><author><keyname>Pisitkun</keyname><forenames>Trairak</forenames></author><author><keyname>Knepper</keyname><forenames>Mark A.</forenames></author><author><keyname>Hoffert</keyname><forenames>Jason D.</forenames></author></authors><title>An Efficient Algorithm for Clustering of Large-Scale Mass Spectrometry
  Data</title><categories>cs.DS q-bio.GN q-bio.QM</categories><comments>4 pages, 4 figures, Bioinformatics and Biomedicine (BIBM), 2012 IEEE
  International Conference on</comments><journal-ref>IEEE Proceedings publications 2012</journal-ref><doi>10.1109/BIBM.2012.6392738</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  High-throughput spectrometers are capable of producing data sets containing
thousands of spectra for a single biological sample. These data sets contain a
substantial amount of redundancy from peptides that may get selected multiple
times in a LC-MS/MS experiment. In this paper, we present an efficient
algorithm, CAMS (Clustering Algorithm for Mass Spectra) for clustering mass
spectrometry data which increases both the sensitivity and confidence of
spectral assignment. CAMS utilizes a novel metric, called F-set, that allows
accurate identification of the spectra that are similar. A graph theoretic
framework is defined that allows the use of F-set metric efficiently for
accurate cluster identifications. The accuracy of the algorithm is tested on
real HCD and CID data sets with varying amounts of peptides. Our experiments
show that the proposed algorithm is able to cluster spectra with very high
accuracy in a reasonable amount of time for large spectral data sets. Thus, the
algorithm is able to decrease the computational time by compressing the data
sets while increasing the throughput of the data by interpreting low S/N
spectra.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0849</identifier>
 <datestamp>2013-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0849</id><created>2013-01-04</created><authors><author><keyname>Kirrage</keyname><forenames>James</forenames></author><author><keyname>Rathnayake</keyname><forenames>Asiri</forenames></author><author><keyname>Thielecke</keyname><forenames>Hayo</forenames></author></authors><title>Static Analysis for Regular Expression Denial-of-Service Attacks</title><categories>cs.PL cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Regular expressions are a concise yet expressive language for expressing
patterns. For instance, in networked software, they are used for input
validation and intrusion detection. Yet some widely deployed regular expression
matchers based on backtracking are themselves vulnerable to denial-of-service
attacks, since their runtime can be exponential for certain input strings. This
paper presents a static analysis for detecting such vulnerable regular
expressions. The running time of the analysis compares favourably with tools
based on fuzzing, that is, randomly generating inputs and measuring how long
matching them takes. Unlike fuzzers, the analysis pinpoints the source of the
vulnerability and generates possible malicious inputs for programmers to use in
security testing. Moreover, the analysis has a firm theoretical foundation in
abstract machines. Testing the analysis on two large repositories of regular
expressions shows that the analysis is able to find significant numbers of
vulnerable regular expressions in a matter of seconds.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0859</identifier>
 <datestamp>2013-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0859</id><created>2013-01-04</created><authors><author><keyname>Dhillon</keyname><forenames>Harpreet S.</forenames></author><author><keyname>Huang</keyname><forenames>Howard C.</forenames></author><author><keyname>Viswanathan</keyname><forenames>Harish</forenames></author><author><keyname>Valenzuela</keyname><forenames>Reinaldo A.</forenames></author></authors><title>Power-Efficient System Design for Cellular-Based Machine-to-Machine
  Communications</title><categories>cs.IT cs.NI math.IT</categories><comments>submitted to IEEE Transactions on Wireless Communications, Jan. 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The growing popularity of Machine-to-Machine (M2M) communications in cellular
networks is driving the need to optimize networks based on the characteristics
of M2M, which are significantly different from the requirements that current
networks are designed to meet. First, M2M requires large number of short
sessions as opposed to small number of long lived sessions required by the
human generated traffic. Second, M2M constitutes a number of battery operated
devices that are static in locations such as basements and tunnels, and need to
transmit at elevated powers compared to the traditional devices. Third,
replacing or recharging batteries of such devices may not be feasible. All
these differences highlight the importance of a systematic framework to study
the power and energy optimal system design in the regime of interest for M2M,
which is the main focus of this paper. For a variety of coordinated and
uncoordinated transmission strategies, we derive results for the optimal
transmit power, energy per bit, and the maximum load supported by the base
station, leading to the following design guidelines: (i) frequency division
multiple access (FDMA), including equal bandwidth allocation, is sum-power
optimal in the asymptotically low spectral efficiency regime, (ii) while FDMA
is the best practical strategy overall, uncoordinated code division multiple
access (CDMA) is almost as good when the base station is lightly loaded, (iii)
the value of optimization within FDMA is in general not significant in the
regime of interest for M2M.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0862</identifier>
 <datestamp>2013-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0862</id><created>2013-01-04</created><authors><author><keyname>Apon</keyname><forenames>Daniel</forenames></author></authors><title>On Lower Bound Methods for Tree-like Cutting Plane Proofs</title><categories>cs.CC</categories><comments>5 pages, manuscript</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In the book Boolean Function Complexity by Stasys Jukna, two lower bound
techniques for Tree-like Cutting Plane proofs (henceforth, &quot;Tree-CP proofs&quot;)
using Karchmer-Widgerson type communication games (henceforth, &quot;KW games&quot;) are
presented: The first, applicable to Tree-CP proofs with bounded coefficients,
translates Omega(t) deterministic lower bounds on KW games to 2^Omega(t/log n)
lower bounds on Tree-CP proof size. The second, applicable to Tree-CP proofs
with unbounded coefficients, translates Omega(t) randomized lower bounds on KW
games to 2^Omega(t/log^2 n) lower bounds on Tree-CP proof size.
  The textbook proof in the latter case uses a O(log^2 n)-bit randomized
protocol for the GreaterThan function. However, Nisan mentioned using the ideas
of Feige, et al. to construct a O(log n + log(1/epsilon))-bit randomized
protocol for GreaterThan. Nisan did not explicitly give the proof, though later
results in his paper assume such a protocol.
  In this short exposition, we present the full O(log n + log(1/epsilon))-bit
randomized protocol for the GreaterThan function based on the ideas of Feige,
et al. for &quot;noisy binary search.&quot; As an application, we show how to translate
Omega(t) randomized lower bounds on KW games to 2^Omega(t/log n) lower bounds
on Tree-CP proof size in the unbounded coefficient case. This equates
randomness with coefficient size for the Tree-CP/KW game lower bound method.
  We believe that, while the O(log n + log(1/epsilon))-bit randomized protocol
for GreaterThan is a &quot;known&quot; result, the explicit connection to Tree-CP proof
size lower bounds given here is new.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0875</identifier>
 <datestamp>2015-10-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0875</id><created>2013-01-05</created><authors><author><keyname>Tallapragada</keyname><forenames>Pavankumar</forenames></author><author><keyname>Chopra</keyname><forenames>Nikhil</forenames></author></authors><title>On Event Triggered Tracking for Nonlinear Systems</title><categories>cs.SY math.OC</categories><comments>8 pages, 3 figures. Includes proofs for all results</comments><journal-ref>IEEE Transactions on Automatic Control, vol. 58, no. 9, pages
  2343-2348, 2013</journal-ref><doi>10.1109/TAC.2013.2251794</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we study an event based control algorithm for trajectory
tracking in nonlinear systems. The desired trajectory is modelled as the
solution of a reference system with an exogenous input and it is assumed that
the desired trajectory and the exogenous input to the reference system are
uniformly bounded. Given a continuous-time control law that guarantees global
uniform asymptotic tracking of the desired trajectory, our algorithm provides
an event based controller that not only guarantees uniform ultimate boundedness
of the tracking error, but also ensures non-accumulation of inter-execution
times. In the case that the derivative of the exogenous input to the reference
system is also uniformly bounded, an arbitrarily small ultimate bound can be
designed. If the exogenous input to the reference system is piecewise
continuous and not differentiable everywhere then the achievable ultimate bound
is constrained and the result is local, though with a known region of
attraction. The main ideas in the paper are illustrated through simulations of
trajectory tracking by a nonlinear system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0878</identifier>
 <datestamp>2013-02-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0878</id><created>2013-01-05</created><updated>2013-02-17</updated><authors><author><keyname>Ailon</keyname><forenames>Nir</forenames></author><author><keyname>Rauhut</keyname><forenames>Holger</forenames></author></authors><title>Fast and RIP-optimal transforms</title><categories>cs.NA cs.IT math.IT</categories><acm-class>F.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study constructions of $k \times n$ matrices $A$ that both (1) satisfy the
restricted isometry property (RIP) at sparsity $s$ with optimal parameters, and
(2) are efficient in the sense that only $O(n\log n)$ operations are required
to compute $Ax$ given a vector $x$. Our construction is based on repeated
application of independent transformations of the form $DH$, where $H$ is a
Hadamard or Fourier transform and $D$ is a diagonal matrix with random
$\{+1,-1\}$ elements on the diagonal, followed by any $k \times n$ matrix of
orthonormal rows (e.g.\ selection of $k$ coordinates). We provide guarantees
(1) and (2) for a larger regime of parameters for which such constructions were
previously unknown. Additionally, our construction does not suffer from the
extra poly-logarithmic factor multiplying the number of observations $k$ as a
function of the sparsity $s$, as present in the currently best known RIP
estimates for partial random Fourier matrices and other classes of structured
random matrices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0884</identifier>
 <datestamp>2013-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0884</id><created>2013-01-05</created><authors><author><keyname>Godbole</keyname><forenames>vaibhav</forenames></author></authors><title>Performance Analysis of Clustering Protocol Using Fuzzy Logic for
  Wireless Sensor Network</title><categories>cs.NI</categories><journal-ref>IAES International Journal of Artificial Intelligence (IJ-AI) Vol.
  1, No. 3, September 2012, pp. 103-111 ISSN: 2252-8938</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to gather information more efficiently, wireless sensor networks
(WSNs) are partitioned into clusters. Most proposed clustering algorithms do
not consider the location of the base station. This situation causes hot spot
problems in multi-hop WSNs. In this paper, we analyze a fuzzy clustering
algorithm (FCA) which aims to prolong the lifetime of WSNs. This algorithm
adjusts the cluster-head radius considering the residual energy and distance to
the base station parameters of the sensor nodes. This helps to decrease the
intra-cluster work of the sensor nodes which are closer to the base station or
have lower battery level. Fuzzy logic is utilized for handling the
uncertainties in cluster-head radius estimation. We compare this algorithm with
the low energy adaptive clustering hierarchy (LEACH) algorithm according to the
parameters of first node dies half of the nodes alive and energy-efficiency
metrics. Our simulation results show that the fuzzy clustering approach
performs better than LEACH. Therefore, the FCA is a stable and energy-efficient
clustering algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0886</identifier>
 <datestamp>2013-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0886</id><created>2013-01-05</created><authors><author><keyname>Godbole</keyname><forenames>Vaibhav</forenames></author></authors><title>Performance Analysis of Bio-Inspired Routing Protocols based on Random
  Waypoint Mobility Model</title><categories>cs.NI</categories><comments>arXiv admin note: substantial text overlap with arXiv:1205.1604 by
  other authors without attribution</comments><journal-ref>Defence S &amp; T Technical Bulletin, Science &amp; Research Technology
  Institute for Defence (STRIDE), Vol. 5, No. 2, November 2012, pp. 114-134,
  ISSN: 1985-6571</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A mobile ad hoc network (MANET) is a non-centralised, multihop, wireless
network that lacks a common infrastructure and hence it needs
self-organisation. The biggest challenge in MANETs is to find a path between
communicating nodes, which is the MANET routing problem. Biology-inspired
techniques such as ant colony optimisation (ACO) which have proven to be very
adaptable in other problem domains, have been applied to the MANET routing
problem as it forms a good fit to the problem. The general characteristics of
these biological systems, which include their capability for self-organisation,
self-healing and local decision making, make them suitable for routing in
MANETs. In this paper, we discuss a few ACO based protocols, namely AntNet,
hybrid ACO (AntHocNet), ACO based routing algorithm (ARA), imProved ant colony
optimisation routing algorithm for mobile ad hoc NETworks (PACONET), ACO based
on demand distance vector (Ant-AODV) and ACO based dynamic source routing
(Ant-DSR), and determine their performance in terms of quality of service (QoS)
parameters, such as end-to-end delay and packet delivery ratio, using Network
Simulator 2 (NS2). We also compare them with well known protocols, ad hoc on
demand distance vector (AODV) and dynamic source routing (DSR), based on the
random waypoint mobility model. The simulation results show how this
biology-inspired approach helps in improving QoS parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0901</identifier>
 <datestamp>2013-11-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0901</id><created>2013-01-05</created><authors><author><keyname>Krzakala</keyname><forenames>Florent</forenames></author><author><keyname>M&#xe9;zard</keyname><forenames>Marc</forenames></author><author><keyname>Zdeborov&#xe1;</keyname><forenames>Lenka</forenames></author></authors><title>Compressed Sensing under Matrix Uncertainty: Optimum Thresholds and
  Robust Approximate Message Passing</title><categories>cs.IT cond-mat.stat-mech math.IT math.ST stat.TH</categories><comments>5 pages, 4 figures</comments><journal-ref>Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE
  International Conference on, pages 5519 - 5523</journal-ref><doi>10.1109/ICASSP.2013.6638719</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In compressed sensing one measures sparse signals directly in a compressed
form via a linear transform and then reconstructs the original signal. However,
it is often the case that the linear transform itself is known only
approximately, a situation called matrix uncertainty, and that the measurement
process is noisy. Here we present two contributions to this problem: first, we
use the replica method to determine the mean-squared error of the Bayes-optimal
reconstruction of sparse signals under matrix uncertainty. Second, we consider
a robust variant of the approximate message passing algorithm and demonstrate
numerically that in the limit of large systems, this algorithm matches the
optimal performance in a large region of parameters.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0902</identifier>
 <datestamp>2013-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0902</id><created>2013-01-05</created><authors><author><keyname>Nasre</keyname><forenames>Meghana</forenames></author></authors><title>Popular Matchings -- structure and cheating strategies</title><categories>cs.DS cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the cheating strategies for the popular matchings problem. The
popular matchings problem can be defined as follows: Let G = (A U P, E) be a
bipartite graph where A denotes a set of agents, P denotes a set of posts and
the edges in E are ranked. Each agent ranks a subset of posts in an order of
preference, possibly involving ties. A matching M is popular if there exists no
matching M' such that the number of agents that prefer M' to M exceeds the
number of agents that prefer M to M'. Consider a centralized market where
agents submit their preferences and a central authority matches agents to posts
according to the notion of popularity. Since a popular matching need not be
unique, we assume that the central authority chooses an arbitrary popular
matching. Let a1 be the sole manipulative agent who is aware of the true
preference lists of all other agents. The goal of a1 is to falsify her
preference list to get better always, that is, to improve the set of posts that
she gets matched to as opposed to what she got when she was truthful. We show
that the optimal cheating strategy for a single agent to get better always can
be computed in O(\sqrt{n}m) time when preference lists are allowed to contain
ties and in O(m+n) time when preference lists are all strict. Here n = |A| +
|P| and m = |E|.
  To compute the cheating strategies, we develop a switching graph
characterization of the popular matchings problem involving ties. The switching
graph characterization was studied for the case of strict lists by McDermid and
Irving (J. Comb. Optim. 2011) and it was open for the case of ties. The
switching graph characterization for the case of ties is of independent
interest and answers a part of the open questions posed by McDermid and Irving.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0917</identifier>
 <datestamp>2013-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0917</id><created>2013-01-05</created><authors><author><keyname>Chen</keyname><forenames>Shaoshi</forenames></author><author><keyname>Jaroschek</keyname><forenames>Maximilian</forenames></author><author><keyname>Kauers</keyname><forenames>Manuel</forenames></author><author><keyname>Singer</keyname><forenames>Michael F.</forenames></author></authors><title>Desingularization Explains Order-Degree Curves for Ore Operators</title><categories>cs.SC</categories><acm-class>I.1.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Desingularization is the problem of finding a left multiple of a given Ore
operator in which some factor of the leading coefficient of the original
operator is removed. An order-degree curve for a given Ore operator is a curve
in the $(r,d)$-plane such that for all points $(r,d)$ above this curve, there
exists a left multiple of order $r$ and degree $d$ of the given operator. We
give a new proof of a desingularization result by Abramov and van Hoeij for the
shift case, and show how desingularization implies order-degree curves which
are extremely accurate in examples.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0926</identifier>
 <datestamp>2013-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0926</id><created>2013-01-05</created><authors><author><keyname>Simeone</keyname><forenames>Osvaldo</forenames></author></authors><title>Source Coding with in-Block Memory and Causally Controllable Side
  Information</title><categories>cs.IT math.IT</categories><comments>This document is a longer version of a paper submitted to ISIT 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The recently proposed set-up of source coding with a side information
&quot;vending machine&quot; allows the decoder to select actions in order to control the
quality of the side information. The actions can depend on the message received
from the encoder and on the previously measured samples of the side
information, and are cost constrained. Moreover, the final estimate of the
source by the decoder is a function of the encoder's message and depends
causally on the side information sequence. Previous work by Permuter and
Weissman has characterized the rate-distortion-cost function in the special
case in which the source and the &quot;vending machine&quot; are memoryless. In this
work, motivated by the related channel coding model introduced by Kramer, the
rate-distortion-cost function characterization is extended to a model with
in-block memory. Various special cases are studied including block-feedforward
and side information repeat request models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0929</identifier>
 <datestamp>2013-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0929</id><created>2013-01-05</created><authors><author><keyname>Fister</keyname><forenames>Iztok</forenames></author><author><keyname>Mernik</keyname><forenames>Marjan</forenames></author><author><keyname>Brest</keyname><forenames>Janez</forenames></author></authors><title>Hybridization of Evolutionary Algorithms</title><categories>cs.NE</categories><journal-ref>Iztok Fister, Marjan Mernik and Janez Brest (2011). Hybridization
  of Evolutionary Algorithms, Evolutionary Algorithms, Eisuke Kita (Ed.), ISBN:
  978-953-307-171-8, InTech</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Evolutionary algorithms are good general problem solver but suffer from a
lack of domain specific knowledge. However, the problem specific knowledge can
be added to evolutionary algorithms by hybridizing. Interestingly, all the
elements of the evolutionary algorithms can be hybridized. In this chapter, the
hybridization of the three elements of the evolutionary algorithms is
discussed: the objective function, the survivor selection operator and the
parameter settings. As an objective function, the existing heuristic function
that construct the solution of the problem in traditional way is used. However,
this function is embedded into the evolutionary algorithm that serves as a
generator of new solutions. In addition, the objective function is improved by
local search heuristics. The new neutral selection operator has been developed
that is capable to deal with neutral solutions, i.e. solutions that have the
different representation but expose the equal values of objective function. The
aim of this operator is to directs the evolutionary search into a new
undiscovered regions of the search space. To avoid of wrong setting of
parameters that control the behavior of the evolutionary algorithm, the
self-adaptation is used. Finally, such hybrid self-adaptive evolutionary
algorithm is applied to the two real-world NP-hard problems: the graph
3-coloring and the optimization of markers in the clothing industry. Extensive
experiments shown that these hybridization improves the results of the
evolutionary algorithms a lot. Furthermore, the impact of the particular
hybridizations is analyzed in details as well.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0930</identifier>
 <datestamp>2013-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0930</id><created>2013-01-05</created><authors><author><keyname>Saha</keyname><forenames>Sayan</forenames></author><author><keyname>Das</keyname><forenames>Saptarshi</forenames></author><author><keyname>Pakhira</keyname><forenames>Anindya</forenames></author><author><keyname>Mukherjee</keyname><forenames>Sumit</forenames></author><author><keyname>Pan</keyname><forenames>Indranil</forenames></author></authors><title>Comparative Studies on Decentralized Multiloop PID Controller Design
  Using Evolutionary Algorithms</title><categories>cs.SY cs.NE</categories><comments>6 pages, 9 figures</comments><journal-ref>Engineering and Systems (SCES), 2012 Students Conference on;
  Allahabad, March 2012</journal-ref><doi>10.1109/SCES.2012.6199122</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Decentralized PID controllers have been designed in this paper for
simultaneous tracking of individual process variables in multivariable systems
under step reference input. The controller design framework takes into account
the minimization of a weighted sum of Integral of Time multiplied Squared Error
(ITSE) and Integral of Squared Controller Output (ISCO) so as to balance the
overall tracking errors for the process variables and required variation in the
corresponding manipulated variables. Decentralized PID gains are tuned using
three popular Evolutionary Algorithms (EAs) viz. Genetic Algorithm (GA),
Evolutionary Strategy (ES) and Cultural Algorithm (CA). Credible simulation
comparisons have been reported for four benchmark 2x2 multivariable processes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0932</identifier>
 <datestamp>2013-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0932</id><created>2013-01-05</created><authors><author><keyname>Mahfudz</keyname><forenames>Sufianto</forenames></author><author><keyname>Nasution</keyname><forenames>Mahyuddin K. M.</forenames></author><author><keyname>Nasution</keyname><forenames>Sawaluddin</forenames></author></authors><title>Knowledge Sharing: A Model</title><categories>cs.SI cs.AI</categories><comments>5 pages, a draf for ICOCSIM 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We know anything because we learn about it, there is anything we ever share
about it, but now a lot of media that can represent how it happened as
infrastructure of the knowledge sharing. This paper aims to introduce a model
for understanding a problem in knowledge sharing based on interaction.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0935</identifier>
 <datestamp>2013-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0935</id><created>2013-01-05</created><authors><author><keyname>Lee</keyname><forenames>Chung-Pi</forenames></author><author><keyname>Lin</keyname><forenames>Shih-Chun</forenames></author><author><keyname>Su</keyname><forenames>Hsuan-Jung</forenames></author><author><keyname>Poor</keyname><forenames>H. Vincent</forenames></author></authors><title>Multi-user lattice coding for the multiple-access relay channel</title><categories>cs.IT math.IT</categories><comments>32 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the multi-antenna multiple access relay channel (MARC),
in which multiple users transmit messages to a common destination with the
assistance of a relay. In a variety of MARC settings, the dynamic decode and
forward (DDF) protocol is very useful due to its outstanding rate performance.
However, the lack of good structured codebooks so far hinders practical
applications of DDF for MARC. In this work, two classes of structured MARC
codes are proposed: 1) one-to-one relay-mapper aided multiuser lattice coding
(O-MLC), and 2) modulo-sum relay-mapper aided multiuser lattice coding
(MS-MLC). The former enjoys better rate performance, while the latter provides
more flexibility to tradeoff between the complexity of the relay mapper and the
rate performance. It is shown that, in order to approach the rate performance
achievable by an unstructured codebook with maximum-likelihood decoding, it is
crucial to use a new K-stage coset decoder for structured O-MLC, instead of the
one-stage decoder proposed in previous works. However, if O-MLC is decoded with
the one-stage decoder only, it can still achieve the optimal DDF
diversity-multiplexing gain tradeoff in the high signal-to-noise ratio regime.
As for MS-MLC, its rate performance can approach that of the O-MLC by
increasing the complexity of the modulo-sum relay-mapper. Finally, for
practical implementations of both O-MLC and MS-MLC, practical short length
lattice codes with linear mappers are designed, which facilitate efficient
lattice decoding. Simulation results show that the proposed coding schemes
outperform existing schemes in terms of outage probabilities in a variety of
channel settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0939</identifier>
 <datestamp>2013-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0939</id><created>2013-01-05</created><authors><author><keyname>Fister</keyname><forenames>Iztok</forenames></author><author><keyname>Mernik</keyname><forenames>Marjan</forenames></author><author><keyname>Filipi&#x10d;</keyname><forenames>Bogdan</forenames></author></authors><title>Graph 3-coloring with a hybrid self-adaptive evolutionary algorithm</title><categories>cs.NE</categories><doi>10.1007/s10589-012-9496-5</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a hybrid self-adaptive evolutionary algorithm for graph
coloring that is hybridized with the following novel elements: heuristic
genotype-phenotype mapping, a swap local search heuristic, and a neutral
survivor selection operator. This algorithm was compared with the evolutionary
algorithm with the SAW method of Eiben et al., the Tabucol algorithm of Hertz
and de Werra, and the hybrid evolutionary algorithm of Galinier and Hao. The
performance of these algorithms were tested on a test suite consisting of
randomly generated 3-colorable graphs of various structural features, such as
graph size, type, edge density, and variability in sizes of color classes.
Furthermore, the test graphs were generated including the phase transition
where the graphs are hard to color. The purpose of the extensive experimental
work was threefold: to investigate the behavior of the tested algorithms in the
phase transition, to identify what impact hybridization with the DSatur
traditional heuristic has on the evolutionary algorithm, and to show how graph
structural features influence the performance of the graph-coloring algorithms.
The results indicate that the performance of the hybrid self-adaptive
evolutionary algorithm is comparable with, or better than, the performance of
the hybrid evolutionary algorithm which is one of the best graph-coloring
algorithms today. Moreover, the fact that all the considered algorithms
performed poorly on flat graphs confirms that this type of graphs is really the
hardest to color.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0954</identifier>
 <datestamp>2013-07-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0954</id><created>2013-01-05</created><authors><author><keyname>Krishnan</keyname><forenames>Narayanan</forenames></author><author><keyname>Yates</keyname><forenames>Roy D.</forenames></author><author><keyname>Mandayam</keyname><forenames>Narayan B.</forenames></author></authors><title>Cellular Systems with Many Antennas: Large System Analysis under Pilot
  Contamination</title><categories>cs.IT math.IT</categories><comments>5 pages, 4 figures</comments><doi>10.1109/Allerton.2012.6483357</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Base stations with a large number of transmit antennas have the potential to
serve a large number of users simultaneously at higher rates. They also promise
a lower power consumption due to coherent combining at the receiver. However,
the receiver processing in the uplink relies on the channel estimates which are
known to suffer from pilot interference. In this work, we perform an uplink
large system analysis of multi-cell multi-antenna system when the receiver
employs a matched filtering with a pilot contaminated estimate. We find the
asymptotic Signal to Interference plus Noise Ratio (SINR) as the number of
antennas and number of users per base station grow large while maintaining a
fixed ratio. To do this, we make use of the similarity of the uplink received
signal in a multi-antenna system to the representation of the received signal
in CDMA systems. The asymptotic SINR expression explicitly captures the effect
of pilot contamination and that of interference averaging. This also explains
the SINR performance of receiver processing schemes at different regimes such
as instances when the number of antennas are comparable to number of users as
well as when antennas exceed greatly the number of users. Finally, we also
propose that the adaptive MMSE symbol detection scheme, which does not require
the explicit channel knowledge, can be employed for cellular systems with large
number of antennas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0955</identifier>
 <datestamp>2013-02-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0955</id><created>2013-01-05</created><updated>2013-02-05</updated><authors><author><keyname>Martelot</keyname><forenames>Erwan Le</forenames></author><author><keyname>Hankin</keyname><forenames>Chris</forenames></author></authors><title>Fast Multi-Scale Community Detection based on Local Criteria within a
  Multi-Threaded Algorithm</title><categories>cs.DS cs.SI physics.soc-ph</categories><comments>arXiv admin note: text overlap with arXiv:1204.1002</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Many systems can be described using graphs, or networks. Detecting
communities in these networks can provide information about the underlying
structure and functioning of the original systems. Yet this detection is a
complex task and a large amount of work was dedicated to it in the past decade.
One important feature is that communities can be found at several scales, or
levels of resolution, indicating several levels of organisations. Therefore
solutions to the community structure may not be unique. Also networks tend to
be large and hence require efficient processing. In this work, we present a new
algorithm for the fast detection of communities across scales using a local
criterion. We exploit the local aspect of the criterion to enable parallel
computation and improve the algorithm's efficiency further. The algorithm is
tested against large generated multi-scale networks and experiments demonstrate
its efficiency and accuracy.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0957</identifier>
 <datestamp>2013-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0957</id><created>2013-01-05</created><authors><author><keyname>Viswanatha</keyname><forenames>Kumar</forenames></author><author><keyname>Ramaswamy</keyname><forenames>Sharadh</forenames></author><author><keyname>Saxena</keyname><forenames>Ankur</forenames></author><author><keyname>Akyol</keyname><forenames>Emrah</forenames></author><author><keyname>Rose</keyname><forenames>Kenneth</forenames></author></authors><title>On Large Scale Distributed Compression and Dispersive Information
  Routing for Networks</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper considers the problem of distributed source coding for a large
network. A major obstacle that poses an existential threat to practical
deployment of conventional approaches to distributed coding is the exponential
growth of the decoder complexity with the number of sources and the encoding
rates. This growth in complexity renders many traditional approaches
impractical even for moderately sized networks. In this paper, we propose a new
decoding paradigm for large scale distributed compression wherein the decoder
complexity is explicitly controlled during the design. Central to our approach
is a module called the &quot;bit-subset selector&quot; whose role is to judiciously
extract an appropriate subset of the received bits for decoding per individual
source. We propose a practical design strategy, based on deterministic
annealing (DA) for the joint design of the system components, that enables
direct optimization of the decoder complexity-distortion trade-off, and thereby
the desired scalability. We also point out the direct connections between the
problem of large scale distributed compression and a related problem in sensor
networks, namely, dispersive information routing of correlated sources. This
allows us to extend the design principles proposed in the context of large
scale distributed compression to design efficient routers for minimum cost
communication of correlated sources across a network. Experiments on both real
and synthetic data-sets provide evidence for substantial gains over
conventional approaches.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0958</identifier>
 <datestamp>2013-03-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0958</id><created>2013-01-05</created><authors><author><keyname>Gilio</keyname><forenames>Angelo</forenames></author><author><keyname>Sanfilippo</keyname><forenames>Giuseppe</forenames></author></authors><title>Probabilistic entailment in the setting of coherence: The role of quasi
  conjunction and inclusion relation</title><categories>math.PR cs.AI math.ST stat.TH</categories><journal-ref>International Journal of Approximate Reasoning, vol. 54, no. 4,
  pp. 513-525, 2013, http://dx.doi.org/10.1016/j.ijar.2012.11.001</journal-ref><doi>10.1016/j.ijar.2012.11.001</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, by adopting a coherence-based probabilistic approach to
default reasoning, we focus the study on the logical operation of quasi
conjunction and the Goodman-Nguyen inclusion relation for conditional events.
We recall that quasi conjunction is a basic notion for defining consistency of
conditional knowledge bases. By deepening some results given in a previous
paper we show that, given any finite family of conditional events F and any
nonempty subset S of F, the family F p-entails the quasi conjunction C(S);
then, given any conditional event E|H, we analyze the equivalence between
p-entailment of E|H from F and p-entailment of E|H from C(S), where S is some
nonempty subset of F. We also illustrate some alternative theorems related with
p-consistency and p-entailment. Finally, we deepen the study of the connections
between the notions of p-entailment and inclusion relation by introducing for a
pair (F,E|H) the (possibly empty) class K of the subsets S of F such that C(S)
implies E|H. We show that the class K satisfies many properties; in particular
K is additive and has a greatest element which can be determined by applying a
suitable algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0963</identifier>
 <datestamp>2013-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0963</id><created>2013-01-05</created><authors><author><keyname>tahyudin</keyname><forenames>Imam</forenames></author></authors><title>Analysis of Influence of Internet Retail Service Quality (IRSQ) to
  Consumer Online Shopping Satisfaction at www.kebanaran.com</title><categories>cs.CY</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The purpose of this research was to determine the influence of Internet
Retail Service Quality (IRSQ) (website performance, access, security,
sensation, and information) to the satisfaction www.kebanaran.com online
shoppers. The method of analysis used was path analysis. Based on the research
results influence IRSQ variables (performance, access, sensation, and
information security), performance variables (X1), access (X2) and sensation
(X3) had no significant effect on satisfaction (Y). It showsthat the online
shopping website www.kebanaran.com already apply standard terms online stores
in general, such as membership, has a return policy, a unique craft product
offerings, the choice of language, the choice of currency, the chatroom
facility, the product ctalogue about images from different angles and so forth,
so that consumers be sure to purchase products through the online shopping
website www.kebanaran.com. Security variable (X4) and information (X5) has a
significant effect on satisfaction (Y). This shows that security is applied and
the importance of information for consumers such as information availability,
quality productsinformation, accurate product information is essential so that
consumers do not hesitate to deal transaction use online shopping website
www.kebanaran.com.
  Keyword: Service Quality, Satisfaction, Online Shop
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0965</identifier>
 <datestamp>2013-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0965</id><created>2013-01-05</created><authors><author><keyname>Monteiro</keyname><forenames>Romeu</forenames></author><author><keyname>Sargento</keyname><forenames>Susana</forenames></author><author><keyname>Viriyasitavat</keyname><forenames>Wantanee</forenames></author><author><keyname>Tonguz</keyname><forenames>Ozan K.</forenames></author></authors><title>Improving VANET Protocols via Network Science</title><categories>cs.NI</categories><comments>Proceedings of the 2012 IEEE Vehicular Networking Conference (VNC),
  Korea, November 2012</comments><journal-ref>R. Monteiro, S. Sargento, W. Viriyasitavat, and O. K. Tonguz,
  &quot;Improving VANET Protocols via Network Science&quot;, in Proceedings of the 2012
  IEEE Vehicular Networking Conference (VNC), Korea, November 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Developing routing protocols for Vehicular Ad Hoc Networks (VANETs) is a
significant challenge in these large, self- organized and distributed networks.
We address this challenge by studying VANETs from a network science perspective
to develop solutions that act locally but influence the network performance
globally. More specifically, we look at snapshots from highway and urban VANETs
of different sizes and vehicle densities, and study parameters such as the node
degree distribution, the clustering coefficient and the average shortest path
length, in order to better understand the networks' structure and compare it to
structures commonly found in large real world networks such as small-world and
scale-free networks. We then show how to use this information to improve
existing VANET protocols. As an illustrative example, it is shown that, by
adding new mechanisms that make use of this information, the overhead of the
urban vehicular broadcasting (UV-CAST) protocol can be reduced substantially
with no significant performance degradation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0967</identifier>
 <datestamp>2014-05-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0967</id><created>2013-01-05</created><updated>2014-05-19</updated><authors><author><keyname>Zeng</keyname><forenames>Xianyi</forenames></author></authors><title>A general approach to enhance slope limiters on non-uniform rectilinear
  grids</title><categories>math.NA cs.NA</categories><comments>25 pages, 31 figures</comments><msc-class>65M06, 65M08, 65M12, 35L65</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Most slope limiter functions in high-resolution finite volume methods to
solve hyperbolic conservation laws are designed assuming one-dimensional
uniform grids, and they are also used to compute slope limiters in computations
on non-uniform rectilinear grids. However, this strategy may lead to either
loss of total variation diminishing (TVD) stability for 1D linear problems or
the loss of formal second-order accuracy if the grid is highly non-uniform.
This is especially true when the limiter function is not piecewise linear.
Numerical evidences are provided to support this argument for two popular
finite volume strategies: MUSCL in space and method of lines in time
(MUSCL-MOL), and capacity-form differencing. In order to deal with this issue,
this paper presents a general approach to study and enhance the slope limiter
functions for highly non-uniform grids in the MUSCL-MOL framework. This
approach extends the classical reconstruct-evolve-project procedure to general
grids, and it gives sufficient conditions for a slope limiter function leading
to a TVD stable, formal second-order accuracy in space, and symmetry preserving
numerical scheme on arbitrary grids. Several widely used limiter functions,
including the smooth ones by van Leer and van Albada, are enhanced to satisfy
these conditions. These properties are confirmed by solving various
one-dimensional and two-dimensional benchmark problems using the enhanced
limiters on highly non-uniform rectilinear grids.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0975</identifier>
 <datestamp>2015-09-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0975</id><created>2013-01-05</created><updated>2015-09-24</updated><authors><author><keyname>Zhang</keyname><forenames>Yan-Yu</forenames></author><author><keyname>Yu</keyname><forenames>Hong-Yi</forenames></author><author><keyname>Zhu</keyname><forenames>Yi-Jun</forenames></author><author><keyname>Wang</keyname><forenames>Jin-Long</forenames></author></authors><title>Multiple layer Phase Shift Linear Space-time Block Code for High-speed
  Visible Light Communications</title><categories>cs.IT math.IT</categories><comments>This paper has been withdrawn by the author due to a crucial error in
  the performance analysis</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this letter, we consider intensity modulation/direct detection (IM/DD)
channel in the visible light communication (VLC) systems with multiple
transmitter phosphor-based white light emitting diodes (LED) and single
receiver avalanche photo diode (APD). We proposed a Multiple Layer Phase Shift
Linear Space-time Block Code (MLPS-LSTBC). We show that our proposed code for
VLC has the following main features: (a) The symbol transmission rate is
$N/(N+M-1)$, where $N$ is the number of transmitter LED and $M$ denotes the
number of shift intervals contained by a single codeword per layer; (b)
zero-forcing receiver can transform the virtual MIMO matrix channel into
parallel sub-channels even without channel state information at the receiver
side (CSIR); (c) Our MLPS-LSTBC can asymptotically enhance the spectral
efficiency by $\min (M\text{,}N)$, which is attractive for LED-based VLC with
limited electrical modulation bandwidth. By simulations, we achieve the record
data rate of 1.5 Gb/s with the bit error rate performance below the FEC limit
of $2\times10^{-3}$ via multiple 100-MBaud transmission of OOK signal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0977</identifier>
 <datestamp>2013-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0977</id><created>2013-01-06</created><authors><author><keyname>Yildirim</keyname><forenames>Hilmi</forenames></author><author><keyname>Chaoji</keyname><forenames>Vineet</forenames></author><author><keyname>Zaki</keyname><forenames>Mohammed J.</forenames></author></authors><title>DAGGER: A Scalable Index for Reachability Queries in Large Dynamic
  Graphs</title><categories>cs.DB cs.DS</categories><comments>11 pages, 7 figures, 2 tables</comments><acm-class>H.3.3</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  With the ubiquity of large-scale graph data in a variety of application
domains, querying them effectively is a challenge. In particular, reachability
queries are becoming increasingly important, especially for containment,
subsumption, and connectivity checks. Whereas many methods have been proposed
for static graph reachability, many real-world graphs are constantly evolving,
which calls for dynamic indexing. In this paper, we present a fully dynamic
reachability index over dynamic graphs. Our method, called DAGGER, is a
light-weight index based on interval labeling, that scales to million node
graphs and beyond. Our extensive experimental evaluation on real-world and
synthetic graphs confirms its effectiveness over baseline methods.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0980</identifier>
 <datestamp>2013-04-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0980</id><created>2013-01-06</created><updated>2013-03-29</updated><authors><author><keyname>Chee</keyname><forenames>Yeow Meng</forenames></author><author><keyname>Ling</keyname><forenames>San</forenames></author><author><keyname>Wang</keyname><forenames>Huaxiong</forenames></author><author><keyname>Zhang</keyname><forenames>Liang Feng</forenames></author></authors><title>Upper Bounds on Matching Families in $\mathbb{Z}_{pq}^n$</title><categories>cs.IT math.IT</categories><comments>10 pages</comments><msc-class>94B65</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  \textit{Matching families} are one of the major ingredients in the
construction of {\em locally decodable codes} (LDCs) and the best known
constructions of LDCs with a constant number of queries are based on matching
families. The determination of the largest size of any matching family in
$\mathbb{Z}_m^n$, where $\mathbb{Z}_m$ is the ring of integers modulo $m$, is
an interesting problem. In this paper, we show an upper bound of
$O((pq)^{0.625n+0.125})$ for the size of any matching family in
$\mathbb{Z}_{pq}^n$, where $p$ and $q$ are two distinct primes. Our bound is
valid when $n$ is a constant, $p\rightarrow \infty$ and $p/q\rightarrow 1$. Our
result improves an upper bound of Dvir {\it et al.}
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0995</identifier>
 <datestamp>2013-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0995</id><created>2013-01-06</created><authors><author><keyname>Liang</keyname><forenames>Hongyu</forenames></author><author><keyname>Lou</keyname><forenames>Tiancheng</forenames></author><author><keyname>Tan</keyname><forenames>Haisheng</forenames></author><author><keyname>Wang</keyname><forenames>Yuexuan</forenames></author><author><keyname>Yu</keyname><forenames>Dongxiao</forenames></author></authors><title>On the Complexity of Connectivity in Cognitive Radio Networks Through
  Spectrum Assignment</title><categories>cs.DS</categories><comments>A preliminary version of this paper appeared in ALGOSENSORS 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cognitive Radio Networks (CRNs) are considered as a promising solution to the
spectrum shortage problem in wireless communication. In this paper, we initiate
the first systematic study on the algorithmic complexity of the connectivity
problem in CRNs through spectrum assignments. We model the network of secondary
users (SUs) as a potential graph, where two nodes having an edge between them
are connected as long as they choose a common available channel. In the general
case, where the potential graph is arbitrary and the SUs may have different
number of antennae, we prove that it is NP-complete to determine whether the
network is connectable even if there are only two channels. For the special
case where the number of channels is constant and all the SUs have the same
number of antennae, which is more than one but less than the number of
channels, the problem is also NP-complete. For the special cases in which the
potential graph is complete, a tree, or a graph with bounded treewidth, we
prove the problem is NP-complete and fixed-parameter tractable (FPT) when
parameterized by the number of channels. Exact algorithms are also derived to
determine the connectability of a given cognitive radio network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0997</identifier>
 <datestamp>2013-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0997</id><created>2013-01-06</created><authors><author><keyname>Perrot</keyname><forenames>K&#xe9;vin</forenames></author><author><keyname>R&#xe9;mila</keyname><forenames>Eric</forenames></author></authors><title>Kadanoff Sand Piles, following the snowball</title><categories>cs.DM math.CO</categories><comments>Research report. 26 pages, 9 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper is about cubic sand grains moving around on nicely packed columns
in one dimension (the physical sand pile is two dimensional, but the support of
sand columns is one dimensional). The Kadanoff Sand Pile Model is a discrete
dynamical system describing the evolution of a finite number of stacked grains
--as they would fall from an hourglass-- to a stable configuration. Grains move
according to the repeated application of a simple local rule until reaching a
stable configuration from which no rule can be applied, namely a fixed point.
  The main interest of the model relies in the difficulty of understanding its
behavior, despite the simplicity of the rule. We are interested in describing
the shape of fixed point configurations according to the number of initially
stacked sand grains. In this paper, we prove the emergence of a wavy shape on
fixed points, i.e., a regular pattern is (nearly) periodically repeated on
fixed points. Interestingly, the regular pattern does not cover the entire
fixed point, but eventually emerges from a seemingly highly disordered segment.
Fortunately, the relative size of the part of fixed points non-covered by the
pattern repetition is asymptotically null.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.0998</identifier>
 <datestamp>2013-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.0998</id><created>2013-01-06</created><authors><author><keyname>Bakshi</keyname><forenames>Sambit</forenames></author><author><keyname>Mehrotra</keyname><forenames>Hunny</forenames></author><author><keyname>Majhi</keyname><forenames>Banshidhar</forenames></author></authors><title>Stratified SIFT Matching for Human Iris Recognition</title><categories>cs.CV</categories><comments>7 pages</comments><msc-class>68U10</msc-class><journal-ref>Proceedings of International Conference on Advances in Computing
  and Communications (ACC-2011)</journal-ref><doi>10.1007/978-3-642-22720-2_17</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes an efficient three fold stratified SIFT matching for iris
recognition. The objective is to filter wrongly paired conventional SIFT
matches. In Strata I, the keypoints from gallery and probe iris images are
paired using traditional SIFT approach. Due to high image similarity at
different regions of iris there may be some impairments. These are detected and
filtered by finding gradient of paired keypoints in Strata II. Further, the
scaling factor of paired keypoints is used to remove impairments in Strata III.
The pairs retained after Strata III are likely to be potential matches for iris
recognition. The proposed system performs with an accuracy of 96.08% and 97.15%
on publicly available CASIAV3 and BATH databases respectively. This marks
significant improvement of accuracy and FAR over the existing SIFT matching for
iris.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1002</identifier>
 <datestamp>2015-09-15</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1002</id><created>2013-01-06</created><updated>2015-09-14</updated><authors><author><keyname>Sarikaya</keyname><forenames>Yunus</forenames></author><author><keyname>Koksal</keyname><forenames>C. Emre</forenames></author><author><keyname>Ercetin</keyname><forenames>Ozgur</forenames></author></authors><title>Dynamic Network Control for Confidential Multi-hop Communications</title><categories>cs.IT cs.SY math.IT</categories><comments>15 pages, 7 figures</comments><journal-ref>IEEE/ACM Transactions on Networking, vol. PP, no. 99, pp. 1-15,
  2015</journal-ref><doi>10.1109/TNET.2015.2414945</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of resource allocation and control of multihop
networks in which multiple source-destination pairs communicate confidential
messages, to be kept confidential from the intermediate nodes. We pose the
problem as that of network utility maximization, into which confidentiality is
incorporated as an additional quality of service constraint. We develop a
simple, and yet provably optimal dynamic control algorithm that combines flow
control, routing and end-to-end secrecy-encoding. In order to achieve
confidentiality, our scheme exploits multipath diversity and temporal diversity
due to channel variability. Our end-to-end dynamic encoding scheme encodes
confidential messages across multiple packets, to be combined at the ultimate
destination for recovery. We first develop an optimal dynamic policy for the
case in which the number of blocks across which secrecy encoding is performed
is asymptotically large. Next, we consider encoding across a finite number of
packets, which eliminates the possibility of achieving perfect secrecy. For
this case, we develop a dynamic policy to choose the encoding rates for each
message, based on the instantaneous channel state information, queue states and
secrecy outage requirements. By numerical analysis, we observe that the
proposed scheme approaches the optimal rates asymptotically with increasing
block size. Finally, we address the consequences of practical implementation
issues such as infrequent queue updates and de-centralized scheduling. We
demonstrate the efficacy of our policies by numerical studies under various
network conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1003</identifier>
 <datestamp>2013-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1003</id><created>2013-01-06</created><authors><author><keyname>Wijsen</keyname><forenames>Jef</forenames></author></authors><title>Charting the Tractability Frontier of Certain Conjunctive Query
  Answering</title><categories>cs.DB cs.LO</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An uncertain database is defined as a relational database in which primary
keys need not be satisfied. A repair (or possible world) of such database is
obtained by selecting a maximal number of tuples without ever selecting two
distinct tuples with the same primary key value. For a Boolean query q, the
decision problem CERTAINTY(q) takes as input an uncertain database db and asks
whether q is satisfied by every repair of db. Our main focus is on acyclic
Boolean conjunctive queries without self-join. Previous work has introduced the
notion of (directed) attack graph of such queries, and has proved that
CERTAINTY(q) is first-order expressible if and only if the attack graph of q is
acyclic. The current paper investigates the boundary between tractability and
intractability of CERTAINTY(q). We first classify cycles in attack graphs as
either weak or strong, and then prove among others the following. If the attack
graph of a query q contains a strong cycle, then CERTAINTY(q) is coNP-complete.
If the attack graph of q contains no strong cycle and every weak cycle of it is
terminal (i.e., no edge leads from a vertex in the cycle to a vertex outside
the cycle), then CERTAINTY(q) is in P. We then partially address the only
remaining open case, i.e., when the attack graph contains some nonterminal
cycle and no strong cycle. Finally, we establish a relationship between the
complexities of CERTAINTY(q) and evaluating q on probabilistic databases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1013</identifier>
 <datestamp>2013-01-29</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1013</id><created>2013-01-06</created><updated>2013-01-28</updated><authors><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author><author><keyname>Rafols</keyname><forenames>Ismael</forenames></author><author><keyname>Chen</keyname><forenames>Chaomei</forenames></author></authors><title>Interactive Overlays of Journals and the Measurement of
  Interdisciplinarity on the basis of Aggregated Journal-Journal Citations</title><categories>cs.DL</categories><comments>Journal of the American Society for Information Science and
  Technology (in press)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using &quot;Analyze Results&quot; at the Web of Science, one can directly generate
overlays onto global journal maps of science. The maps are based on the 10,000+
journals contained in the Journal Citation Reports (JCR) of the Science and
Social Science Citation Indices (2011). The disciplinary diversity of the
retrieval is measured in terms of Rao-Stirling's &quot;quadratic entropy.&quot; Since
this indicator of interdisciplinarity is normalized between zero and one, the
interdisciplinarity can be compared among document sets and across years, cited
or citing. The colors used for the overlays are based on Blondel et al.'s
(2008) community-finding algorithms operating on the relations journals
included in JCRs. The results can be exported from VOSViewer with different
options such as proportional labels, heat maps, or cluster density maps. The
maps can also be web-started and/or animated (e.g., using PowerPoint). The
&quot;citing&quot; dimension of the aggregated journal-journal citation matrix was found
to provide a more comprehensive description than the matrix based on the cited
archive. The relations between local and global maps and their different
functions in studying the sciences in terms of journal litteratures are further
discussed: local and global maps are based on different assumptions and can be
expected to serve different purposes for the explanation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1018</identifier>
 <datestamp>2013-09-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1018</id><created>2013-01-06</created><updated>2013-09-21</updated><authors><author><keyname>Shechtman</keyname><forenames>Yoav</forenames></author><author><keyname>Beck</keyname><forenames>Amir</forenames></author><author><keyname>Eldar</keyname><forenames>Yonina C.</forenames></author></authors><title>GESPAR: Efficient Phase Retrieval of Sparse Signals</title><categories>cs.IT math.IT</categories><comments>Generalized to non-Fourier measurements, added 2D simulations, and a
  theorem for convergence to stationary point</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider the problem of phase retrieval, namely, recovery of a signal from
the magnitude of its Fourier transform, or of any other linear transform. Due
to the loss of the Fourier phase information, this problem is ill-posed.
Therefore, prior information on the signal is needed in order to enable its
recovery. In this work we consider the case in which the signal is known to be
sparse, i.e., it consists of a small number of nonzero elements in an
appropriate basis. We propose a fast local search method for recovering a
sparse signal from measurements of its Fourier transform (or other linear
transform) magnitude which we refer to as GESPAR: GrEedy Sparse PhAse
Retrieval. Our algorithm does not require matrix lifting, unlike previous
approaches, and therefore is potentially suitable for large scale problems such
as images. Simulation results indicate that GESPAR is fast and more accurate
than existing techniques in a variety of settings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1026</identifier>
 <datestamp>2013-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1026</id><created>2013-01-06</created><authors><author><keyname>Gaborit</keyname><forenames>Philippe</forenames></author><author><keyname>Ruatta</keyname><forenames>Olivier</forenames></author><author><keyname>Schrek</keyname><forenames>Julien</forenames></author></authors><title>On the complexity of the Rank Syndrome Decoding problem</title><categories>cs.CR</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we propose two new generic attacks on the Rank Syndrome
Decoding (RSD) problem
  Let $C$ be a random $[n,k]$ rank code over $GF(q^m)$ and let $y=x+e$ be a
received word such that $x \in C$ and the $Rank(e)=r$. The first attack is
combinatorial and permits to recover an error $e$ of rank weight $r$ in
$min(O((n-k)^3m^3q^{r\lfloor\frac{km}{n}\rfloor},
O((n-k)^3m^3q^{(r-1)\lfloor\frac{(k+1)m}{n}\rfloor}))$ operations on $GF(q)$.
This attack dramatically improves on previous attack by introducing the length
$n$ of the code in the exponent of the complexity, which was not the case in
previous generic attacks. which can be considered The second attack is based on
a algebraic attacks: based on the theory of $q$-polynomials introduced by Ore
we propose a new algebraic setting for the RSD problem that permits to consider
equations and unknowns in the extension field $GF(q^m)$ rather than in $GF(q)$
as it is usually the case. We consider two approaches to solve the problem in
this new setting. Linearization technics show that if $n \ge (k+1)(r+1)-1$ the
RSD problem can be solved in polynomial time, more generally we prove that if
$\lceil \frac{(r+1)(k+1)-(n+1)}{r} \rceil \le k$, the problem can be solved
with an average complexity $O(r^3k^3q^{r\lceil \frac{(r+1)(k+1)-(n+1)}{r}
\rceil})$. We also consider solving with \grob bases for which which we discuss
theoretical complexity, we also consider consider hybrid solving with \grob
bases on practical parameters. As an example of application we use our new
attacks on all proposed recent cryptosystems which reparation the GPT
cryptosystem, we break all examples of published proposed parameters, some
parameters are broken in less than 1 s in certain cases.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1027</identifier>
 <datestamp>2013-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1027</id><created>2013-01-06</created><updated>2013-11-21</updated><authors><author><keyname>Khuzani</keyname><forenames>Masoud Badiei</forenames></author><author><keyname>Mitran</keyname><forenames>Patrick</forenames></author></authors><title>On online energy harvesting in multiple access communication systems</title><categories>cs.IT math.IT</categories><comments>34 pages, 4 figures, updated bibliography and assorted corrections
  (v2)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate performance limits of a multiple access communication system
with energy harvesting nodes where the utility function is taken to be the
long-term average sum-throughput. We assume a causal structure for energy
arrivals and study the problem in the continuous time regime. For this setting,
we first characterize a storage dam model that captures the dynamics of a
battery with energy harvesting and variable transmission power. Using this
model, we next establish an upper bound on the throughput problem as a function
of battery capacity. We also formulate a non-linear optimization problem to
determine optimal achievable power policies for transmitters. Applying a
calculus of variation technique, we then derive Euler-Lagrange equations as
necessary conditions for optimum power policies in terms of a system of coupled
partial integro-differential equations (PIDEs). Based on a Gauss-Seidel
algorithm, we devise an iterative algorithm to solve these equations. We also
propose a fixed-point algorithm for the symmetric multiple access setting in
which the statistical descriptions of energy harvesters are identical. Along
with the analysis and to support our iterative algorithms, comprehensive
numerical results are also obtained.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1061</identifier>
 <datestamp>2013-01-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1061</id><created>2013-01-06</created><updated>2013-01-18</updated><authors><author><keyname>Jiang</keyname><forenames>Nan</forenames></author><author><keyname>Yang</keyname><forenames>Yang</forenames></author><author><keyname>H&#xf8;st-Madsen</keyname><forenames>Anders</forenames></author><author><keyname>Xiong</keyname><forenames>Zixiang</forenames></author></authors><title>On the Minimum Energy of Sending Gaussian Multiterminal Sources over the
  Gaussian MAC</title><categories>cs.IT math.IT</categories><comments>Under revision</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this work, we investigate the minimum energy of transmitting correlated
sources over the Gaussian multiple-access channel (MAC). Compared to other
works on joint source-channel coding, we consider the general scenario where
the source and channel bandwidths are not naturally matched. In particular, we
proposed the use of hybrid digital-analog coding over to improve the
transmission energy efficiency. Different models of correlated sources are
studied. We first consider lossless transmission of binary sources over the
MAC. We then treat lossy transmission of Gaussian sources over the Gaussian
MAC, including CEO sources and multiterminal sources. In all cases, we show
that hybrid transmission achieves the best known energy efficiency.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1064</identifier>
 <datestamp>2013-08-27</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1064</id><created>2013-01-06</created><updated>2013-08-26</updated><authors><author><keyname>Fagiano</keyname><forenames>Lorenzo</forenames></author><author><keyname>Zgraggen</keyname><forenames>Aldo U.</forenames></author><author><keyname>Morari</keyname><forenames>Manfred</forenames></author><author><keyname>Khammash</keyname><forenames>Mustafa</forenames></author></authors><title>Automatic crosswind flight of tethered wings for airborne wind energy:
  modeling, control design and experimental results</title><categories>cs.SY math.OC</categories><comments>This manuscript is a preprint of a paper accepted for publication on
  the IEEE Transactions on Control Systems Technology and is subject to IEEE
  Copyright. The copy of record is available at IEEEXplore library:
  http://ieeexplore.ieee.org/</comments><doi>10.1109/TCST.2013.2279592</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An approach to control tethered wings for airborne wind energy is proposed. A
fixed length of the lines is considered, and the aim of the control system is
to obtain figure-eight crosswind trajectories. The proposed technique is based
on the notion of the wing's &quot;velocity angle&quot; and, in contrast with most
existing approaches, it does not require a measurement of the wind speed or of
the effective wind at the wing's location. Moreover, the proposed approach
features few parameters, whose effects on the system's behavior are very
intuitive, hence simplifying tuning procedures. A simplified model of the
steering dynamics of the wing is derived from first-principle laws, compared
with experimental data and used for the control design. The control algorithm
is divided into a low-level loop for the velocity angle and a high-level
guidance strategy to achieve the desired flight patterns. The robustness of the
inner loop is verified analytically, and the overall control system is tested
experimentally on a small-scale prototype, with varying wind conditions and
using different wings.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1065</identifier>
 <datestamp>2014-01-31</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1065</id><created>2013-01-06</created><updated>2013-06-04</updated><authors><author><keyname>Rudzi&#x144;ski</keyname><forenames>Adam</forenames></author></authors><title>Effective number of samples and pseudo-random nonlinear distortions in
  digital OFDM coded signal</title><categories>physics.data-an cs.IT math.IT</categories><journal-ref>Circuits Syst. Signal Process. 33, 197 (2014)</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper concerns theoretical modeling of degradation of signal with OFDM
coding caused by pseudo-random nonlinear distortions introduced by an
analog-to-digital or digital-to-analog converter. A new quantity, effective
number of samples, is defined and used for derivation of accurate expressions
for autocorrelation function and the total power of the distortions. The
derivation is based on probabilistic model of the signal and its transition
probability. It is shown, that for digital (discrete and quantized) signals the
effective number of samples replaces the total number of samples and is the
proper quantity defining their properties.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1071</identifier>
 <datestamp>2013-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1071</id><created>2013-01-06</created><authors><author><keyname>Benson</keyname><forenames>Austin R.</forenames></author><author><keyname>Gleich</keyname><forenames>David F.</forenames></author><author><keyname>Demmel</keyname><forenames>James</forenames></author></authors><title>Direct QR factorizations for tall-and-skinny matrices in MapReduce
  architectures</title><categories>cs.DC cs.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The QR factorization and the SVD are two fundamental matrix decompositions
with applications throughout scientific computing and data analysis. For
matrices with many more rows than columns, so-called &quot;tall-and-skinny
matrices,&quot; there is a numerically stable, efficient, communication-avoiding
algorithm for computing the QR factorization. It has been used in traditional
high performance computing and grid computing environments. For MapReduce
environments, existing methods to compute the QR decomposition use a
numerically unstable approach that relies on indirectly computing the Q factor.
In the best case, these methods require only two passes over the data. In this
paper, we describe how to compute a stable tall-and-skinny QR factorization on
a MapReduce architecture in only slightly more than 2 passes over the data. We
can compute the SVD with only a small change and no difference in performance.
We present a performance comparison between our new direct TSQR method, a
standard unstable implementation for MapReduce (Cholesky QR), and the classic
stable algorithm implemented for MapReduce (Householder QR). We find that our
new stable method has a large performance advantage over the Householder QR
method. This holds both in a theoretical performance model as well as in an
actual implementation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1084</identifier>
 <datestamp>2013-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1084</id><created>2013-01-06</created><authors><author><keyname>Perera</keyname><forenames>Charith</forenames></author><author><keyname>Zaslavsky</keyname><forenames>Arkady</forenames></author><author><keyname>Christen</keyname><forenames>Peter</forenames></author><author><keyname>Georgakopoulos</keyname><forenames>Dimitrios</forenames></author></authors><title>CA4IOT Context Awareness for Internet of Things</title><categories>cs.SE cs.NI</categories><journal-ref>Proceedings of the IEEE International Conference on Green
  Computing and Communications, Conference on Internet of Things, and
  Conference on Cyber,Physical and Social Computing (iThings/CPSCom/GreenCom),
  Besancon, France, November,2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Internet of Things (IoT) will connect billions of sensors deployed around the
world together. This will create an ideal opportunity to build a
sensing-as-a-service platform. Due to large number of sensor deployments, there
would be number of sensors that can be used to sense and collect similar
information. Further, due to advances in sensor hardware technology, new
methods and measurements will be introduced continuously. In the IoT paradigm,
selecting the most appropriate sensors which can provide relevant sensor data
to address the problems at hand among billions of possibilities would be a
challenge for both technical and non-technical users. In this paper, we propose
the Context Awareness for Internet of Things (CA4IOT) architecture to help
users by automating the task of selecting the sensors according to the
problems/tasks at hand. We focus on automated configuration of filtering,
fusion and reasoning mechanisms that can be applied to the collected sensor
data streams using selected sensors. Our objective is to allow the users to
submit their problems, so our proposed architecture understands them and
produces more comprehensive and meaningful information than the raw sensor data
streams generated by individual sensors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1085</identifier>
 <datestamp>2013-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1085</id><created>2013-01-06</created><authors><author><keyname>Perera</keyname><forenames>Charith</forenames></author><author><keyname>Zaslavsky</keyname><forenames>Arkady</forenames></author><author><keyname>Christen</keyname><forenames>Peter</forenames></author><author><keyname>Salehi</keyname><forenames>Ali</forenames></author><author><keyname>Georgakopoulos</keyname><forenames>Dimitrios</forenames></author></authors><title>Connecting Mobile Things to Global Sensor Network Middleware using
  System-generated Wrappers</title><categories>cs.SE cs.NI</categories><journal-ref>Proceedings of the 11th ACM International Workshop on Data
  Engineering for Wireless and Mobile Access (ACM SIGMOD/PODS-Workshop-MobiDE),
  Scottsdale, Arizona, USA, May, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Internet of Things (IoT) will create a cyberphysical world where all the
things around us are connected to the Inter net, sense and produce &quot;big data&quot;
that has to be stored, processed and communicated with minimum human
intervention. With the ever increasing emergence of new sensors, interfaces and
mobile devices, the grand challenge is to keep up with this race in developing
software drivers and wrappers for IoT things. In this paper, we examine the
approaches that automate the process of developing middleware drivers/wrappers
for the IoT things. We propose ASCM4GSN architecture to address this challenge
efficiently and effectively. We demonstrate the proposed approach using Global
Sensor Network (GSN) middleware which exemplifies a cluster of data streaming
engines. The ASCM4GSN architecture significantly speeds up the wrapper
development and sensor configuration process as demonstrated for Android mobile
phone based sensors as well as for Sun SPOT sensors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1107</identifier>
 <datestamp>2015-02-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1107</id><created>2013-01-06</created><updated>2015-02-10</updated><authors><author><keyname>Avron</keyname><forenames>Haim</forenames></author><author><keyname>Druinsky</keyname><forenames>Alex</forenames></author><author><keyname>Toledo</keyname><forenames>Sivan</forenames></author></authors><title>Spectral Condition-Number Estimation of Large Sparse Matrices</title><categories>cs.NA math.NA</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We describe a Krylov-subspace method for estimating the spectral condition
number of a real matrix A or indicating that it is numerically rank deficient.
The main difficulty in estimating the condition number is the estimation of the
smallest singular value \sigma_{\min} of A. Our method estimates this value by
solving a consistent linear least-squares problem with a known solution using a
specific Krylov-subspace method called LSQR. In this method, the forward error
tends to concentrate in the direction of a singular vector corresponding to
\sigma_{\min}. Extensive experiments show that the method is reliable. It is
often much faster than a dense SVD and it can sometimes estimate the condition
number when running a dense SVD would be impractical due to the computational
cost or the memory requirements. The method uses very little memory (it
inherits this property from LSQR) and it works equally well on square and
rectangular matrices.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1134</identifier>
 <datestamp>2013-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1134</id><created>2013-01-07</created><authors><author><keyname>Kaniezhil</keyname><forenames>R.</forenames></author><author><keyname>Chandrasekar</keyname><forenames>Dr. C.</forenames></author></authors><title>Performance Analysis of Wireless Network with Opportunistic Spectrum
  Sharing via Cognitive Radio Nodes</title><categories>cs.NI</categories><comments>10 Pages, Journal of Electronic Science and Technology, Vol. 10, No.
  4, December 2012. arXiv admin note: text overlap with arXiv:1210.3435; and
  with arXiv:1201.1964 by other authors without attribution</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Cognitive radio (CR) is found to be an emerging key for efficient spectrum
utilization. In this paper, spectrum sharing among service providers with the
help of cognitive radio has been investigated. The technique of spectrum
sharing among service providers to share the licensed spectrum of licensed
service providers in a dynamic manner is considered. The performance of the
wireless network with opportunistic spectrum sharing techniques is analyzed.
Thus, the spectral utilization and efficiency of sensing is increased, the
interference is minimized, and the call blockage is reduced.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1153</identifier>
 <datestamp>2013-07-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1153</id><created>2013-01-07</created><updated>2013-07-10</updated><authors><author><keyname>Ben-Zwi</keyname><forenames>Oren</forenames></author><author><keyname>Lavi</keyname><forenames>Ron</forenames></author><author><keyname>Newman</keyname><forenames>Ilan</forenames></author></authors><title>Ascending auctions and Walrasian equilibrium</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a family of submodular valuation classes that generalizes gross
substitute. We show that Walrasian equilibrium always exist for one class in
this family, and there is a natural ascending auction which finds it. We prove
some new structural properties on gross-substitute auctions which, in turn,
show that the known ascending auctions for this class (Gul-Stacchetti and
Ausbel) are, in fact, identical. We generalize these two auctions, and provide
a simple proof that they terminate in a Walrasian equilibrium.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1166</identifier>
 <datestamp>2013-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1166</id><created>2013-01-07</created><authors><author><keyname>Feng</keyname><forenames>Tao</forenames></author><author><keyname>Severini</keyname><forenames>Simone</forenames></author></authors><title>Quantum channels from association schemes</title><categories>quant-ph cs.IT math.IT</categories><comments>6 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose in this note the study of quantum channels from association
schemes. This is done by interpreting the $(0,1)$-matrices of a scheme as the
Kraus operators of a channel. Working in the framework of one-shot zero-error
information theory, we give bounds and closed formulas for various independence
numbers of the relative non-commutative (confusability) graphs, or,
equivalently, graphical operator systems. We use pseudocyclic association
schemes as an example. In this case, we show that the unitary
entanglement-assisted independence number grows at least quadratically faster,
with respect to matrix size, than the independence number. The latter parameter
was introduced by Beigi and Shor as a generalization of the one-shot Shannon
capacity, in analogy with the corresponding graph-theoretic notion.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1195</identifier>
 <datestamp>2013-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1195</id><created>2013-01-07</created><authors><author><keyname>Grigoriev</keyname><forenames>Dima</forenames></author><author><keyname>Shpilrain</keyname><forenames>Vladimir</forenames></author></authors><title>Tropical cryptography</title><categories>cs.CR math.RA</categories><comments>9 pages</comments><msc-class>15A80, 94A60</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We employ tropical algebras as platforms for several cryptographic schemes
that would be vulnerable to linear algebra attacks were they based on &quot;usual&quot;
algebras as platforms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1215</identifier>
 <datestamp>2015-03-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1215</id><created>2013-01-07</created><authors><author><keyname>Schaetz</keyname><forenames>Sebastian</forenames></author><author><keyname>Uecker</keyname><forenames>Martin</forenames></author></authors><title>A Multi-GPU Programming Library for Real-Time Applications</title><categories>cs.DC cs.PF</categories><comments>15 pages, 10 figures</comments><journal-ref>Algorithms and Architectures for Parallel Processing 7439 (2012)
  114-128</journal-ref><doi>10.1007/978-3-642-33078-0_9</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present MGPU, a C++ programming library targeted at single-node multi-GPU
systems. Such systems combine disproportionate floating point performance with
high data locality and are thus well suited to implement real-time algorithms.
We describe the library design, programming interface and implementation
details in light of this specific problem domain. The core concepts of this
work are a novel kind of container abstraction and MPI-like communication
methods for intra-system communication. We further demonstrate how MGPU is used
as a framework for porting existing GPU libraries to multi-device
architectures. Putting our library to the test, we accelerate an iterative
non-linear image reconstruction algorithm for real-time magnetic resonance
imaging using multiple GPUs. We achieve a speed-up of about 1.7 using 2 GPUs
and reach a final speed-up of 2.1 with 4 GPUs. These promising results lead us
to conclude that multi-GPU systems are a viable solution for real-time MRI
reconstruction as well as signal-processing applications in general.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1218</identifier>
 <datestamp>2014-01-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1218</id><created>2013-01-07</created><updated>2014-01-22</updated><authors><author><keyname>Riondato</keyname><forenames>Matteo</forenames></author><author><keyname>Vandin</keyname><forenames>Fabio</forenames></author></authors><title>Finding the True Frequent Itemsets</title><categories>cs.LG cs.DB cs.DS stat.ML</categories><comments>13 pages, Extended version of work appeared in SIAM International
  Conference on Data Mining, 2014</comments><acm-class>H.2.8</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Frequent Itemsets (FIs) mining is a fundamental primitive in data mining. It
requires to identify all itemsets appearing in at least a fraction $\theta$ of
a transactional dataset $\mathcal{D}$. Often though, the ultimate goal of
mining $\mathcal{D}$ is not an analysis of the dataset \emph{per se}, but the
understanding of the underlying process that generated it. Specifically, in
many applications $\mathcal{D}$ is a collection of samples obtained from an
unknown probability distribution $\pi$ on transactions, and by extracting the
FIs in $\mathcal{D}$ one attempts to infer itemsets that are frequently (i.e.,
with probability at least $\theta$) generated by $\pi$, which we call the True
Frequent Itemsets (TFIs). Due to the inherently stochastic nature of the
generative process, the set of FIs is only a rough approximation of the set of
TFIs, as it often contains a huge number of \emph{false positives}, i.e.,
spurious itemsets that are not among the TFIs. In this work we design and
analyze an algorithm to identify a threshold $\hat{\theta}$ such that the
collection of itemsets with frequency at least $\hat{\theta}$ in $\mathcal{D}$
contains only TFIs with probability at least $1-\delta$, for some
user-specified $\delta$. Our method uses results from statistical learning
theory involving the (empirical) VC-dimension of the problem at hand. This
allows us to identify almost all the TFIs without including any false positive.
We also experimentally compare our method with the direct mining of
$\mathcal{D}$ at frequency $\theta$ and with techniques based on widely-used
standard bounds (i.e., the Chernoff bounds) of the binomial distribution, and
show that our algorithm outperforms these methods and achieves even better
results than what is guaranteed by the theoretical analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1223</identifier>
 <datestamp>2014-04-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1223</id><created>2013-01-07</created><updated>2014-04-08</updated><authors><author><keyname>Asyhari</keyname><forenames>A. Taufiq</forenames></author><author><keyname>Koch</keyname><forenames>Tobias</forenames></author><author><keyname>F&#xe0;bregas</keyname><forenames>Albert Guill&#xe9;n i</forenames></author></authors><title>Nearest Neighbor Decoding and Pilot-Aided Channel Estimation for Fading
  Channels</title><categories>cs.IT math.IT</categories><comments>47 pages, 5 figures. arXiv admin note: text overlap with
  arXiv:1107.1640, arXiv:1103.0205</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the information rates of non-coherent, stationary, Gaussian,
multiple-input multiple-output (MIMO) flat-fading channels that are achievable
with nearest neighbor decoding and pilot-aided channel estimation. In
particular, we investigate the behavior of these achievable rates in the limit
as the signal- to-noise ratio (SNR) tends to infinity by analyzing the capacity
pre-log, which is defined as the limiting ratio of the capacity to the
logarithm of the SNR as the SNR tends to infinity. We demonstrate that a scheme
estimating the channel using pilot symbols and detecting the message using
nearest neighbor decoding (while assuming that the channel estimation is
perfect) essentially achieves the capacity pre-log of non-coherent
multiple-input single-output flat-fading channels, and it essentially achieves
the best so far known lower bound on the capacity pre-log of non-coherent MIMO
flat-fading channels. We then extend our analysis to the multiple-access
channel.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1254</identifier>
 <datestamp>2013-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1254</id><created>2013-01-07</created><authors><author><keyname>Hall</keyname><forenames>Eric C.</forenames></author><author><keyname>Willett</keyname><forenames>Rebecca M.</forenames></author></authors><title>Dynamical Models and Tracking Regret in Online Convex Programming</title><categories>stat.ML cs.LG</categories><comments>To appear in ICML 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper describes a new online convex optimization method which
incorporates a family of candidate dynamical models and establishes novel
tracking regret bounds that scale with the comparator's deviation from the best
dynamical model in this family. Previous online optimization methods are
designed to have a total accumulated loss comparable to that of the best
comparator sequence, and existing tracking or shifting regret bounds scale with
the overall variation of the comparator sequence. In many practical scenarios,
however, the environment is nonstationary and comparator sequences with small
variation are quite weak, resulting in large losses. The proposed Dynamic
Mirror Descent method, in contrast, can yield low regret relative to highly
variable comparator sequences by both tracking the best dynamical model and
forming predictions based on that model. This concept is demonstrated
empirically in the context of sequential compressive observations of a dynamic
scene and tracking a dynamic social network.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1261</identifier>
 <datestamp>2013-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1261</id><created>2012-12-16</created><authors><author><keyname>Suresh</keyname><forenames>M.</forenames></author><author><keyname>Dutta</keyname><forenames>Maheswar</forenames></author><author><keyname>Purushothaman</keyname><forenames>S.</forenames></author></authors><title>Application of polynomial vector (pv) processing to improve the
  estimation performance of bio diesel in variable compression ratio diesel
  engine</title><categories>cs.OH</categories><comments>8 pages, 15 figures, 3 tables</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  This paper presents the implementation of polynomial vector back propagation
algorithm (PVBPA) for estimating the power, torque, specific fuel consumption
and presence of carbon monoxide, hydrocarbons in the emission of a direct
injection diesel engine. Experimental readings were obtained using the
biodiesel prepared form the waste low quality cooking oil collected from the
canteen of Sri Sairam Engineering College, India.. This waste cooking oil was
due to the preparation of varieties of food (vegetables fried and non
vegetarian). Over more than a week, trans esterification was done in chemical
lab and the biodiesel was obtained. The biodiesel was mixed in proportions of
10%, 20%, 30%, 40%, 50% with remaining combinations of the diesel supplied by
the Indian government. Variable compression ratio (VCR) diesel engine with
single cylinder, four stroke diesel type was used. The outputs of the engine as
power, torque and specific fuel consumption were obtained from the
computational facility attached to the engine. The data collected for different
input conditions of the engine was further used to train (PVBPA). The trained
PVBPA network was further used to predict the power, torque and brake specific
fuel consumption (SFC) for different speed, biodiesel and diesel combinations
and full load condition. The estimation performance of the PVBPA network is
discussed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1262</identifier>
 <datestamp>2013-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1262</id><created>2012-12-16</created><authors><author><keyname>Kalmukov</keyname><forenames>Yordan</forenames></author></authors><title>Protecting Online Documents from an Unauthorized External Access (in
  Bulgarian)</title><categories>cs.CR cs.DL</categories><comments>in Bulgarian</comments><journal-ref>Conference Proceedings of &quot;Days of Science 2011&quot;, Volume 2, pp.
  442-451, Veliko Tarnovo, Bulgaria, 2011, ISSN: 1314-2283</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The modern multi-tier web applications and information systems store and
process various types of data. Some of them are stored in a database,
controlled by an external database management system, while other data are
stored directly within the server's file system. The database is secured by the
database management system itself, but it is a programmer's responsibility to
design and develop a security protection of the files managed by the
information system. This paper summarizes the existing and suggests new rules
for design and implementation of an in-depth security protection of file
resources, published on the Internet, from an unauthorized external access. The
paper is in Bulgarian.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1268</identifier>
 <datestamp>2013-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1268</id><created>2013-01-07</created><authors><author><keyname>Banerjee</keyname><forenames>Abhik</forenames></author><author><keyname>Gauthier</keyname><forenames>Vincent</forenames></author><author><keyname>Labiod</keyname><forenames>Houda</forenames></author><author><keyname>Afifi</keyname><forenames>Hossam</forenames></author></authors><title>Cooperation Optimized Design for Information Dissemination in Vehicular
  Networks using Evolutionary Game Theory</title><categories>cs.NI cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present an evolutionary game theoretic approach to study node cooperation
behavior in wireless ad hoc networks. Evolutionary game theory (EGT) has been
used to study the conditions governing the growth of cooperation behavior in
biological and social networks. We propose a model of node cooperation behavior
in dynamic wireless networks such as vehicular networks. Our work is motivated
by the fact that, similar to existing EGT studies, node behavior in dynamic
wireless networks is characterized by decision making that only depends on the
immediate neighborhood. We adapt our model to study cooperation behavior in the
context of information dissemination in wireless networks. We obtain conditions
that determine whether a network evolves to a state of complete cooperation
from all nodes. Finally, we use our model to study the evolution of cooperation
behavior and its impact on content downloading in vehicular networks, taking
into consideration realistic network conditions.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1275</identifier>
 <datestamp>2015-04-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1275</id><created>2013-01-07</created><updated>2013-09-24</updated><authors><author><keyname>Allodi</keyname><forenames>Luca</forenames></author><author><keyname>Massacci</keyname><forenames>Fabio</forenames></author></authors><title>My Software has a Vulnerability, should I worry?</title><categories>cs.CR</categories><comments>12 pages, 4 figures</comments><journal-ref>ACM TISSEC Vol 17 Issue 1, 2014</journal-ref><doi>10.1145/2630069</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  (U.S) Rule-based policies to mitigate software risk suggest to use the CVSS
score to measure the individual vulnerability risk and act accordingly: an HIGH
CVSS score according to the NVD (National (U.S.) Vulnerability Database) is
therefore translated into a &quot;Yes&quot;. A key issue is whether such rule is
economically sensible, in particular if reported vulnerabilities have been
actually exploited in the wild, and whether the risk score do actually match
the risk of actual exploitation.
  We compare the NVD dataset with two additional datasets, the EDB for the
white market of vulnerabilities (such as those present in Metasploit), and the
EKITS for the exploits traded in the black market. We benchmark them against
Symantec's threat explorer dataset (SYM) of actual exploit in the wild. We
analyze the whole spectrum of CVSS submetrics and use these characteristics to
perform a case-controlled analysis of CVSS scores (similar to those used to
link lung cancer and smoking) to test its reliability as a risk factor for
actual exploitation.
  We conclude that (a) fixing just because a high CVSS score in NVD only yields
negligible risk reduction, (b) the additional existence of proof of concepts
exploits (e.g. in EDB) may yield some additional but not large risk reduction,
(c) fixing in response to presence in black markets yields the equivalent risk
reduction of wearing safety belt in cars (you might also die but still..). On
the negative side, our study shows that as industry we miss a metric with high
specificity (ruling out vulns for which we shouldn't worry).
  In order to address the feedback from BlackHat 2013's audience, the final
revision (V3) provides additional data in Appendix A detailing how the control
variables in the study affect the results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1279</identifier>
 <datestamp>2013-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1279</id><created>2013-01-07</created><authors><author><keyname>Bodas</keyname><forenames>Shreeshankar</forenames></author><author><keyname>Sadiq</keyname><forenames>Bilal</forenames></author></authors><title>Polynomial-complexity, Low-delay Scheduling for SCFDMA-based Wireless
  Uplink Networks (Technical Report)</title><categories>cs.NI cs.IT math.IT</categories><comments>Longer version of a paper to be presented at Infocom mini-conference,
  2013</comments><acm-class>C.2.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Uplink scheduling/resource allocation under the single-carrier FDMA
constraint is investigated, taking into account the queuing dynamics at the
transmitters. Under the single-carrier constraint, the problem of MaxWeight
scheduling, as well as that of determining if a given number of packets can be
served from all the users, are shown to be NP-complete. Finally, a
matching-based scheduling algorithm is presented that requires only a
polynomial number of computations per timeslot, and in the case of a system
with large bandwidth and user population, provably provides a good delay
(small-queue) performance, even under the single-carrier constraint.
  In summary, the results in first part of the paper support the recent push to
remove SCFDMA from the Standards, whereas those in the second part present a
way of working around the single-carrier constraint if it remains in the
Standards.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1294</identifier>
 <datestamp>2013-11-04</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1294</id><created>2013-01-07</created><updated>2013-10-31</updated><authors><author><keyname>Liang</keyname><forenames>Guanfeng</forenames></author><author><keyname>Kozat</keyname><forenames>Ulas C.</forenames></author></authors><title>FAST CLOUD: Pushing the Envelope on Delay Performance of Cloud Storage
  with Coding</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Our paper presents solutions that can significantly improve the delay
performance of putting and retrieving data in and out of cloud storage. We
first focus on measuring the delay performance of a very popular cloud storage
service Amazon S3. We establish that there is significant randomness in service
times for reading and writing small and medium size objects when assigned
distinct keys. We further demonstrate that using erasure coding, parallel
connections to storage cloud and limited chunking (i.e., dividing the object
into a few smaller objects) together pushes the envelope on service time
distributions significantly (e.g., 76%, 80%, and 85% reductions in mean, 90th,
and 99th percentiles for 2 Mbyte files) at the expense of additional storage
(e.g., 1.75x). However, chunking and erasure coding increase the load and hence
the queuing delays while reducing the supportable rate region in number of
requests per second per node. Thus, in the second part of our paper we focus on
analyzing the delay performance when chunking, FEC, and parallel connections
are used together. Based on this analysis, we develop load adaptive algorithms
that can pick the best code rate on a per request basis by using off-line
computed queue backlog thresholds. The solutions work with homogeneous services
with fixed object sizes, chunk sizes, operation type (e.g., read or write) as
well as heterogeneous services with mixture of object sizes, chunk sizes, and
operation types. We also present a simple greedy solution that
opportunistically uses idle connections and picks the erasure coding rate
accordingly on the fly. Both backlog and greedy solutions support the full rate
region and provide best mean delay performance when compared to the best fixed
coding rate policy. Our evaluations show that backlog based solutions achieve
better delay performance at higher percentile values than the greedy solution.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1295</identifier>
 <datestamp>2013-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1295</id><created>2013-01-07</created><authors><author><keyname>Herrera</keyname><forenames>Roberto H.</forenames></author><author><keyname>Tary</keyname><forenames>Jean-Baptiste</forenames></author><author><keyname>van der Baan</keyname><forenames>Mirko</forenames></author></authors><title>Time-Frequency Representation of Microseismic Signals using the
  Synchrosqueezing Transform</title><categories>physics.geo-ph cs.CE cs.CV</categories><comments>4 pages, 2 figures, GeoConvention 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Resonance frequencies can provide useful information on the deformation
occurring during fracturing experiments or $CO_2$ management, complementary to
the microseismic event distribution. An accurate time-frequency representation
is of crucial importance prior to interpreting the cause of resonance
frequencies during microseismic experiments. The popular methods of Short-Time
Fourier Transform (STFT) and wavelet analysis have limitations in representing
close frequencies and dealing with fast varying instantaneous frequencies and
this is often the nature of microseismic signals. The synchrosqueezing
transform (SST) is a promising tool to track these resonant frequencies and
provide a detailed time-frequency representation. Here we apply the
synchrosqueezing transform to microseismic signals and also show its potential
to general seismic signal processing applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1299</identifier>
 <datestamp>2013-01-08</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1299</id><created>2013-01-07</created><authors><author><keyname>Wingate</keyname><forenames>David</forenames></author><author><keyname>Weber</keyname><forenames>Theophane</forenames></author></authors><title>Automated Variational Inference in Probabilistic Programming</title><categories>stat.ML cs.AI cs.LG</categories><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  We present a new algorithm for approximate inference in probabilistic
programs, based on a stochastic gradient for variational programs. This method
is efficient without restrictions on the probabilistic program; it is
particularly practical for distributions which are not analytically tractable,
including highly structured distributions that arise in probabilistic programs.
We show how to automatically derive mean-field probabilistic programs and
optimize them, and demonstrate that our perspective improves inference
efficiency over other algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1327</identifier>
 <datestamp>2015-09-07</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1327</id><created>2013-01-07</created><updated>2014-09-16</updated><authors><author><keyname>Misra</keyname><forenames>Sidhant</forenames></author><author><keyname>Parrilo</keyname><forenames>Pablo A.</forenames></author></authors><title>Weighted $\ell_1$-minimization for generalized non-uniform sparse model</title><categories>cs.IT math.IT</categories><comments>32 Pages</comments><journal-ref>IEEE Transactions on Information Theory, Vol. 61, Issue 8, pp.
  4424 - 4439, 2015</journal-ref><doi>10.1109/TIT.2015.2442922</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Model-based compressed sensing refers to compressed sensing with extra
structure about the underlying sparse signal known a priori. Recent work has
demonstrated that both for deterministic and probabilistic models imposed on
the signal, this extra information can be successfully exploited to enhance
recovery performance. In particular, weighted $\ell_1$-minimization with
suitable choice of weights has been shown to improve performance in the so
called non-uniform sparse model of signals. In this paper, we consider a full
generalization of the non-uniform sparse model with very mild assumptions. We
prove that when the measurements are obtained using a matrix with i.i.d
Gaussian entries, weighted $\ell_1$-minimization successfully recovers the
sparse signal from its measurements with overwhelming probability. We also
provide a method to choose these weights for any general signal model from the
non-uniform sparse class of signal models.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1332</identifier>
 <datestamp>2013-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1332</id><created>2013-01-07</created><updated>2013-01-08</updated><authors><author><keyname>Ritter</keyname><forenames>Daniel</forenames></author></authors><title>A Logic Programming Approach to Integration Network Inference</title><categories>cs.DB cs.AI</categories><comments>15 pages, The 26th Workshop on Logic Programming (WLP), Bonn, 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The discovery, representation and reconstruction of (technical) integration
networks from Network Mining (NM) raw data is a difficult problem for
enterprises. This is due to large and complex IT landscapes within and across
enterprise boundaries, heterogeneous technology stacks, and fragmented data. To
remain competitive, visibility into the enterprise and partner IT networks on
different, interrelated abstraction levels is desirable.
  We present an approach to represent and reconstruct the integration networks
from NM raw data using logic programming based on first-order logic. The raw
data expressed as integration network model is represented as facts, on which
rules are applied to reconstruct the network. We have built a system that is
used to apply this approach to real-world enterprise landscapes and we report
on our experience with this system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1334</identifier>
 <datestamp>2014-08-21</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1334</id><created>2013-01-07</created><updated>2013-03-19</updated><authors><author><keyname>Arabas</keyname><forenames>Sylwester</forenames></author><author><keyname>Jarecka</keyname><forenames>Dorota</forenames></author><author><keyname>Jaruga</keyname><forenames>Anna</forenames></author><author><keyname>Fija&#x142;kowski</keyname><forenames>Maciej</forenames></author></authors><title>Object-oriented implementations of the MPDATA advection equation solver
  in C++, Python and Fortran</title><categories>physics.comp-ph cs.MS physics.ao-ph</categories><journal-ref>Scientific Programming 22, 201-222 (2014)</journal-ref><doi>10.3233/SPR-140379</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Three object-oriented implementations of a prototype solver of the advection
equation are introduced. The presented programs are based on Blitz++ (C++),
NumPy (Python), and Fortran's built-in array containers. The solvers include an
implementation of the Multidimensional Positive-Definite Advective Transport
Algorithm (MPDATA). The introduced codes exemplify how the application of
object-oriented programming (OOP) techniques allows to reproduce the
mathematical notation used in the literature within the program code. A
discussion on the tradeoffs of the programming language choice is presented.
The main angles of comparison are code brevity and syntax clarity (and hence
maintainability and auditability) as well as performance. In the case of
Python, a significant performance gain is observed when switching from the
standard interpreter (CPython) to the PyPy implementation of Python. Entire
source code of all three implementations is embedded in the text and is
licensed under the terms of the GNU GPL license.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1373</identifier>
 <datestamp>2013-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1373</id><created>2013-01-07</created><authors><author><keyname>Shin</keyname><forenames>Joonwoo</forenames></author><author><keyname>Moon</keyname><forenames>Jaekyun</forenames></author></authors><title>Regularized Zero-Forcing Interference Alignment for the Two-Cell MIMO
  Interfering Broadcast Channel</title><categories>cs.IT math.IT</categories><comments>10 pages, 3 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper, we propose transceiver design strategies for the two-cell
multiple-input multiple-output (MIMO) interfering broadcast channel where
inter-cell interference (ICI) exists in addition to interuser interference
(IUI). We first formulate the generalized zero-forcing interference alignment
(ZF-IA) method based on the alignment of IUI and ICI in multi-dimensional
subspace. We then devise a minimum weighted-mean-square-error (WMSE) method
based on regularizing the precoders and decoders of the generalized ZF-IA
scheme. In contrast to the existing weighted-sum-rate-maximizing transceiver,
our method does not require an iterative calculation of the optimal weights.
Because of this, the proposed scheme, while not designed specifically to
maximize the sum rate, is computationally efficient and achieves a faster
convergence compared to the known weighted-sum-rate maximizing scheme. Through
analysis and simulation, we show the effectiveness of the proposed regularized
ZF-IA scheme.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1374</identifier>
 <datestamp>2013-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1374</id><created>2013-01-07</created><authors><author><keyname>Sarkar</keyname><forenames>R.</forenames></author><author><keyname>Das</keyname><forenames>S.</forenames></author><author><keyname>Vaswani</keyname><forenames>N.</forenames></author></authors><title>PaFiMoCS: Particle Filtered Modified-CS and Applications in Visual
  Tracking across Illumination Change</title><categories>cs.CV</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study the problem of tracking (causally estimating) a time sequence of
sparse spatial signals with changing sparsity patterns, as well as other
unknown states, from a sequence of nonlinear observations corrupted by
(possibly) non-Gaussian noise. In many applications, particularly those in
visual tracking, the unknown state can be split into a small dimensional part,
e.g. global motion, and a spatial signal, e.g. illumination or shape
deformation. The spatial signal is often well modeled as being sparse in some
domain. For a long sequence, its sparsity pattern can change over time,
although the changes are usually slow. To address the above problem, we propose
a novel solution approach called Particle Filtered Modified-CS (PaFiMoCS). The
key idea of PaFiMoCS is to importance sample for the small dimensional state
vector, while replacing importance sampling by slow sparsity pattern change
constrained posterior mode tracking for recovering the sparse spatial signal.
We show that the problem of tracking moving objects across spatially varying
illumination change is an example of the above problem and explain how to
design PaFiMoCS for it. Experiments on both simulated data as well as on real
videos with significant illumination changes demonstrate the superiority of the
proposed algorithm as compared with existing particle filter based tracking
algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1378</identifier>
 <datestamp>2015-04-20</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1378</id><created>2013-01-07</created><updated>2015-04-16</updated><authors><author><keyname>Vass</keyname><forenames>J&#xf3;zsef</forenames></author></authors><title>Apollonian Circumcircles of IFS Fractals</title><categories>cs.CG cs.GR</categories><comments>Submitted for publication. (Contains 8 pages with 4 figures.)</comments><msc-class>28A80 (Primary), 68U05, 52A27 (Secondary)</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Euclidean triangles and IFS fractals seem to be disparate geometrical
concepts, unless we consider the Sierpi\'{n}ski gasket, which is a self-similar
collection of triangles. The &quot;circumcircle&quot; hints at a direct link, as it can
be derived for three-map IFS fractals in general, defined in an Apollonian
manner. Following this path, one may discover a broader relationship between
polygons and IFS fractals.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1379</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1379</id><created>2013-01-07</created><updated>2014-08-03</updated><authors><author><keyname>Vass</keyname><forenames>J&#xf3;zsef</forenames></author></authors><title>On Intersecting IFS Fractals with Lines</title><categories>math.DS cs.GR</categories><comments>The first draft of the paper was shared on Dec. 23, 2011. The second
  draft was submitted on Dec. 25, 2012 and was accepted for publication on Jun.
  29, 2014 in the journal Fractals \copyright\ 2014 World Scientific Publishing
  Company. (Contains 12 pages with 2 figures.)</comments><msc-class>28A80 (Primary) 37F99, 52A35 (Secondary)</msc-class><doi>10.1142/S0218348X14500145</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  IFS fractals - the attractors of Iterated Function Systems - have motivated
plenty of research to date, partly due to their simplicity and applicability in
various fields, such as the modeling of plants in computer graphics, and the
design of fractal antennas. The statement and resolution of the Fractal-Line
Intersection Problem is imperative for a more efficient treatment of certain
applications. This paper intends to take further steps towards this resolution,
building on the literature. For the broad class of hyperdense fractals, a
verifiable condition guaranteeing intersection with any line passing through
the convex hull of a planar IFS fractal is shown, in general R^d for
hyperplanes. The condition also implies a constructive algorithm for finding
the points of intersection. Under certain conditions, an infinite number of
approximate intersections are guaranteed, if there is at least one.
Quantification of the intersection is done via an explicit formula for the
invariant measure of IFS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1385</identifier>
 <datestamp>2013-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1385</id><created>2013-01-07</created><authors><author><keyname>Alviano</keyname><forenames>Mario</forenames></author><author><keyname>Faber</keyname><forenames>Wolfgang</forenames></author></authors><title>Translating NP-SPEC into ASP</title><categories>cs.AI</categories><comments>Proceedings of Answer Set Programming and Other Computing Paradigms
  (ASPOCP 2012), 5th International Workshop, September 4, 2012, Budapest,
  Hungary</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  NP-SPEC is a language for specifying problems in NP in a declarative way.
Despite the fact that the semantics of the language was given by referring to
Datalog with circumscription, which is very close to ASP, so far the only
existing implementations are by means of ECLiPSe Prolog and via Boolean
satisfiability solvers. In this paper, we present translations from NP-SPEC
into various forms of ASP and analyze them. We also argue that it might be
useful to incorporate certain language constructs of NP-SPEC into mainstream
ASP.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1386</identifier>
 <datestamp>2013-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1386</id><created>2013-01-07</created><authors><author><keyname>Balai</keyname><forenames>Evgenii</forenames></author><author><keyname>Gelfond</keyname><forenames>Michael</forenames></author><author><keyname>Zhang</keyname><forenames>Yuanlin</forenames></author></authors><title>SPARC - Sorted ASP with Consistency Restoring Rules</title><categories>cs.PL cs.AI</categories><comments>Proceedings of Answer Set Programming and Other Computing Paradigms
  (ASPOCP 2012), 5th International Workshop, September 4, 2012, Budapest,
  Hungary</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This is a preliminary report on the work aimed at making CR-Prolog -- a
version of ASP with consistency restoring rules -- more suitable for use in
teaching and large applications. First we describe a sorted version of
CR-Prolog called SPARC. Second, we translate a basic version of the CR-Prolog
into the language of DLV and compare the performance with the state of the art
CR-Prolog solver. The results form the foundation for future more efficient and
user friendly implementation of SPARC and shed some light on the relationship
between two useful knowledge representation constructs: consistency restoring
rules and weak constraints of DLV.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1387</identifier>
 <datestamp>2013-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1387</id><created>2013-01-07</created><authors><author><keyname>Balduccini</keyname><forenames>Marcello</forenames></author><author><keyname>Gelfond</keyname><forenames>Michael</forenames></author></authors><title>Language ASP{f} with Arithmetic Expressions and Consistency-Restoring
  Rules</title><categories>cs.AI</categories><comments>Proceedings of Answer Set Programming and Other Computing Paradigms
  (ASPOCP 2012), 5th International Workshop, September 4, 2012, Budapest,
  Hungary</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we continue the work on our extension of Answer Set Programming
by non-Herbrand functions and add to the language support for arithmetic
expressions and various inequality relations over non-Herbrand functions, as
well as consistency-restoring rules from CR-Prolog. We demonstrate the use of
this latest version of the language in the representation of important kinds of
knowledge.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1388</identifier>
 <datestamp>2013-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1388</id><created>2013-01-07</created><authors><author><keyname>Charwat</keyname><forenames>G&#xfc;nther</forenames></author><author><keyname>Wallner</keyname><forenames>Johannes Peter</forenames></author><author><keyname>Woltran</keyname><forenames>Stefan</forenames></author></authors><title>Utilizing ASP for Generating and Visualizing Argumentation Frameworks</title><categories>cs.AI</categories><comments>Proceedings of Answer Set Programming and Other Computing Paradigms
  (ASPOCP 2012), 5th International Workshop, September 4, 2012, Budapest,
  Hungary</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Within the area of computational models of argumentation, the
instantiation-based approach is gaining more and more attention, not at least
because meaningful input for Dung's abstract frameworks is provided in that
way. In a nutshell, the aim of instantiation-based argumentation is to form,
from a given knowledge base, a set of arguments and to identify the conflicts
between them. The resulting network is then evaluated by means of
extension-based semantics on an abstract level, i.e. on the resulting graph.
While several systems are nowadays available for the latter step, the
automation of the instantiation process itself has received less attention. In
this work, we provide a novel approach to construct and visualize an
argumentation framework from a given knowledge base. The system we propose
relies on Answer-Set Programming and follows a two-step approach. A first
program yields the logic-based arguments as its answer-sets; a second program
is then used to specify the relations between arguments based on the
answer-sets of the first program. As it turns out, this approach not only
allows for a flexible and extensible tool for instantiation-based
argumentation, but also provides a new method for answer-set visualization in
general.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1389</identifier>
 <datestamp>2013-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1389</id><created>2013-01-07</created><authors><author><keyname>Chintabathina</keyname><forenames>Sandeep</forenames></author></authors><title>Planning and Scheduling in Hybrid Domains Using Answer Set Programming</title><categories>cs.AI</categories><comments>Proceedings of Answer Set Programming and Other Computing Paradigms
  (ASPOCP 2012), 5th International Workshop, September 4, 2012, Budapest,
  Hungary</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we present an Action Language-Answer Set Programming based
approach to solving planning and scheduling problems in hybrid domains -
domains that exhibit both discrete and continuous behavior. We use action
language H to represent the domain and then translate the resulting theory into
an A-Prolog program. In this way, we reduce the problem of finding solutions to
planning and scheduling problems to computing answer sets of A-Prolog programs.
We cite a planning and scheduling example from the literature and show how to
model it in H. We show how to translate the resulting H theory into an
equivalent A-Prolog program. We compute the answer sets of the resulting
program using a hybrid solver called EZCSP which loosely integrates a
constraint solver with an answer set solver. The solver allows us reason about
constraints over reals and compute solutions to complex planning and scheduling
problems. Results have shown that our approach can be applied to any planning
and scheduling problem in hybrid domains.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1390</identifier>
 <datestamp>2013-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1390</id><created>2013-01-07</created><authors><author><keyname>Eiter</keyname><forenames>Thomas</forenames></author><author><keyname>Fink</keyname><forenames>Michael</forenames></author><author><keyname>Krennwallner</keyname><forenames>Thomas</forenames></author><author><keyname>Redl</keyname><forenames>Christoph</forenames></author><author><keyname>Sch&#xfc;ller</keyname><forenames>Peter</forenames></author></authors><title>Eliminating Unfounded Set Checking for HEX-Programs</title><categories>cs.LO cs.AI</categories><comments>Proceedings of Answer Set Programming and Other Computing Paradigms
  (ASPOCP 2012), 5th International Workshop, September 4, 2012, Budapest,
  Hungary</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  HEX-programs are an extension of the Answer Set Programming (ASP) paradigm
incorporating external means of computation into the declarative programming
language through so-called external atoms. Their semantics is defined in terms
of minimal models of the Faber-Leone-Pfeifer (FLP) reduct. Developing native
solvers for HEX-programs based on an appropriate notion of unfounded sets has
been subject to recent research for reasons of efficiency. Although this has
lead to an improvement over naive minimality checking using the FLP reduct,
testing for foundedness remains a computationally expensive task. In this work
we improve on HEX-program evaluation in this respect by identifying a syntactic
class of programs, that can be efficiently recognized and allows to entirely
skip the foundedness check. Moreover, we develop criteria for decomposing a
program into components, such that the search for unfounded sets can be
restricted. Observing that our results apply to many HEX-program applications
provides analytic evidence for the significance and effectiveness of our
approach, which is complemented by a brief discussion of preliminary
experimental validation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1391</identifier>
 <datestamp>2013-05-06</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1391</id><created>2013-01-07</created><updated>2013-05-02</updated><authors><author><keyname>Fichte</keyname><forenames>Johannes Klaus</forenames></author><author><keyname>Szeider</keyname><forenames>Stefan</forenames></author></authors><title>Backdoors to Normality for Disjunctive Logic Programs</title><categories>cs.LO cs.AI cs.CC</categories><comments>A short version will appear in the Proceedings of the Proceedings of
  the 27th AAAI Conference on Artificial Intelligence (AAAI'13). A preliminary
  version of the paper was presented on the workshop Answer Set Programming and
  Other Computing Paradigms (ASPOCP 2012), 5th International Workshop,
  September 4, 2012, Budapest, Hungary</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Over the last two decades, propositional satisfiability (SAT) has become one
of the most successful and widely applied techniques for the solution of
NP-complete problems. The aim of this paper is to investigate theoretically how
Sat can be utilized for the efficient solution of problems that are harder than
NP or co-NP. In particular, we consider the fundamental reasoning problems in
propositional disjunctive answer set programming (ASP), Brave Reasoning and
Skeptical Reasoning, which ask whether a given atom is contained in at least
one or in all answer sets, respectively. Both problems are located at the
second level of the Polynomial Hierarchy and thus assumed to be harder than NP
or co-NP. One cannot transform these two reasoning problems into SAT in
polynomial time, unless the Polynomial Hierarchy collapses. We show that
certain structural aspects of disjunctive logic programs can be utilized to
break through this complexity barrier, using new techniques from Parameterized
Complexity. In particular, we exhibit transformations from Brave and Skeptical
Reasoning to SAT that run in time O(2^k n^2) where k is a structural parameter
of the instance and n the input size. In other words, the reduction is
fixed-parameter tractable for parameter k. As the parameter k we take the size
of a smallest backdoor with respect to the class of normal (i.e.,
disjunction-free) programs. Such a backdoor is a set of atoms that when deleted
makes the program normal. In consequence, the combinatorial explosion, which is
expected when transforming a problem from the second level of the Polynomial
Hierarchy to the first level, can now be confined to the parameter k, while the
running time of the reduction is polynomial in the input size n, where the
order of the polynomial is independent of k.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1392</identifier>
 <datestamp>2013-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1392</id><created>2013-01-07</created><authors><author><keyname>Gebser</keyname><forenames>Martin</forenames></author><author><keyname>Grote</keyname><forenames>Torsten</forenames></author><author><keyname>Kaminski</keyname><forenames>Roland</forenames></author><author><keyname>Obermeier</keyname><forenames>Philipp</forenames></author><author><keyname>Sabuncu</keyname><forenames>Orkunt</forenames></author><author><keyname>Schaub</keyname><forenames>Torsten</forenames></author></authors><title>Answer Set Programming for Stream Reasoning</title><categories>cs.AI</categories><comments>Proceedings of Answer Set Programming and Other Computing Paradigms
  (ASPOCP 2012), 5th International Workshop, September 4, 2012, Budapest,
  Hungary</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The advance of Internet and Sensor technology has brought about new
challenges evoked by the emergence of continuous data streams. Beyond rapid
data processing, application areas like ambient assisted living, robotics, or
dynamic scheduling involve complex reasoning tasks. We address such scenarios
and elaborate upon approaches to knowledge-intense stream reasoning, based on
Answer Set Programming (ASP). While traditional ASP methods are devised for
singular problem solving, we develop new techniques to formulate and process
problems dealing with emerging as well as expiring data in a seamless way.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1393</identifier>
 <datestamp>2013-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1393</id><created>2013-01-07</created><authors><author><keyname>Lee</keyname><forenames>Joohyung</forenames></author><author><keyname>Meng</keyname><forenames>Yunsong</forenames></author></authors><title>Two New Definitions of Stable Models of Logic Programs with Generalized
  Quantifiers</title><categories>cs.LO cs.AI</categories><comments>Proceedings of Answer Set Programming and Other Computing Paradigms
  (ASPOCP 2012), 5th International Workshop, September 4, 2012, Budapest,
  Hungary</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present alternative definitions of the first-order stable model semantics
and its extension to incorporate generalized quantifiers by referring to the
familiar notion of a reduct instead of referring to the SM operator in the
original definitions. Also, we extend the FLP stable model semantics to allow
generalized quantifiers by referring to an operator that is similar to the
$\sm$ operator. For a reasonable syntactic class of logic programs, we show
that the two stable model semantics of generalized quantifiers are
interchangeable.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1394</identifier>
 <datestamp>2013-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1394</id><created>2013-01-07</created><authors><author><keyname>Lifschitz</keyname><forenames>Vladimir</forenames></author><author><keyname>Yang</keyname><forenames>Fangkai</forenames></author></authors><title>Lloyd-Topor Completion and General Stable Models</title><categories>cs.LO cs.AI</categories><comments>Proceedings of Answer Set Programming and Other Computing Paradigms
  (ASPOCP 2012), 5th International Workshop, September 4, 2012, Budapest,
  Hungary</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate the relationship between the generalization of program
completion defined in 1984 by Lloyd and Topor and the generalization of the
stable model semantics introduced recently by Ferraris et al. The main theorem
can be used to characterize, in some cases, the general stable models of a
logic program by a first-order formula. The proof uses Truszczynski's stable
model semantics of infinitary propositional formulas.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1395</identifier>
 <datestamp>2013-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1395</id><created>2013-01-07</created><authors><author><keyname>Vennekens</keyname><forenames>Joost</forenames></author><author><keyname>Denecker</keyname><forenames>Marc</forenames></author></authors><title>Extending FO(ID) with Knowledge Producing Definitions: Preliminary
  Results</title><categories>cs.LO cs.AI</categories><comments>Proceedings of Answer Set Programming and Other Computing Paradigms
  (ASPOCP 2012), 5th International Workshop, September 4, 2012, Budapest,
  Hungary</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Previous research into the relation between ASP and classical logic has
identified at least two different ways in which the former extends the latter.
First, ASP program typically contain sets of rules that can be naturally
interpreted as inductive definitions, and the language FO(ID) has shown that
such inductive definitions can elegantly be added to classical logic in a
modular way. Second, there is of course also the well-known epistemic component
of ASP, which was mainly emphasized in the early papers on stable model
semantics. To investigate whether this kind of knowledge can also, and in a
similarly modular way, be added to classical logic, the language of Ordered
Epistemic Logic was presented in recent work. However, this logic views the
epistemic component as entirely separate from the inductive definition
component, thus ignoring any possible interplay between the two. In this paper,
we present a language that extends the inductive definition construct found in
FO(ID) with an epistemic component, making such interplay possible. The
eventual goal of this work is to discover whether it is really appropriate to
view the epistemic component and the inductive definition component of ASP as
two separate extensions of classical logic, or whether there is also something
of importance in the combination of the two.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1408</identifier>
 <datestamp>2013-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1408</id><created>2013-01-07</created><authors><author><keyname>Knill</keyname><forenames>Oliver</forenames></author></authors><title>The McKean-Singer Formula in Graph Theory</title><categories>math.CO cs.CG math-ph math.MP</categories><comments>36 pages, 25 figures</comments><msc-class>05C50, 81Q10</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For any finite simple graph G=(V,E), the discrete Dirac operator D=d+d* and
the Laplace-Beltrami operator L=d d* + d* d on the exterior algebra bundle
Omega are finite v times v matrices, where dim(Omega) = v is the sum of the
cardinalities v(k) of the set G(k) of complete subgraphs K(k) of G. We prove
the McKean-Singer formula chi(G) = str(exp(-t L)) which holds for any complex
time t, where chi(G) = str(1)= sum (-1)k v(k) is the Euler characteristic of G.
The super trace of the heat kernel interpolates so the Euler-Poincare formula
for t=0 with the Hodge theorem in the real limit t going to infinity. More
generally, for any continuous complex valued function f satisfying f(0)=0, one
has the formula chi(G) = str(exp(f(D))). This includes for example the
Schroedinger evolutions chi(G) = str(cos(t D)) on the graph. After stating some
general facts about the spectrum of D which includes statements about the
complexity, the product of the non-zero eigenvalues as well as a perturbation
result estimating the spectral difference of two graphs, we mention as a
combinatorial consequence that the spectrum of D encodes the number of closed
paths in the simplex space of a graph. McKean-Singer implies that the number of
closed paths of length n starting at an even dimensional simplex is the same
than the number of closed paths of length n starting at an odd dimensional
simplex. We give a couple of worked out examples and see that McKean-Singer
allows to find explicit pairs of non-isometric graphs which have isospectral
Dirac operators.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1409</identifier>
 <datestamp>2013-07-24</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1409</id><created>2013-01-07</created><updated>2013-07-23</updated><authors><author><keyname>Penunuri</keyname><forenames>F.</forenames></author><author><keyname>Peon-Escalante</keyname><forenames>R.</forenames></author><author><keyname>Villanueva</keyname><forenames>C.</forenames></author><author><keyname>Mendoza</keyname><forenames>O.</forenames></author><author><keyname>Cruz-Villar</keyname><forenames>Carlos A.</forenames></author></authors><title>A Dual Number Approach for Numerical Calculation of Velocity and
  Acceleration in the Spherical 4R Mechanism</title><categories>cs.CE</categories><comments>10 pages, 2 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper proposes a methodology to calculate both the first and second
derivatives of a vector function of one variable in a single computation step.
The method is based on the nested application of the dual number approach for
first order derivatives.
  It has been implemented in Fortran language, a module which contains the dual
version of elementary functions as well as more complex functions, which are
common in the field of rotational kinematics. Since we have three quantities of
interest, namely the function itself and its first and second derivative, our
basic numerical entity has three elements. Then, for a given vector function
$f:\mathbb{R}\to \mathbb{R}^m$, its dual version will have the form
$\tilde{f}:\mathbb{R}^3\to \mathbb{R}^{3m}$.
  As a study case, the proposed methodology is used to calculate the velocity
and acceleration of a point moving on the coupler-point curve generated by a
spherical four-bar mechanism.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1415</identifier>
 <datestamp>2013-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1415</id><created>2013-01-08</created><authors><author><keyname>Sakzad</keyname><forenames>Amin</forenames></author><author><keyname>Harshan</keyname><forenames>J.</forenames></author><author><keyname>Viterbo</keyname><forenames>Emanuele</forenames></author></authors><title>On Complex LLL Algorithm for Integer Forcing Linear Receivers</title><categories>cs.IT math.IT</categories><comments>To appear in the proceedings of the IEEE Australian Communication
  Theory Workshop-2013, Adelaide, Australia</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Integer-forcing (IF) linear receiver has been recently introduced for
multiple-input multiple-output (MIMO) fading channels. The receiver has to
compute an integer linear combination of the symbols as a part of the decoding
process. In particular, the integer coefficients have to be chosen based on the
channel realizations, and the choice of such coefficients is known to determine
the receiver performance. The original known solution of finding these integers
was based on exhaustive search. A practical algorithm based on
Hermite-Korkine-Zolotareff (HKZ) and Minkowski lattice reduction algorithms was
also proposed recently. In this paper, we propose a low-complexity method based
on complex LLL algorithm to obtain the integer coefficients for the IF
receiver. For the 2 X 2 MIMO channel, we study the effectiveness of the
proposed method in terms of the ergodic rate. We also compare the bit error
rate (BER) of our approach with that of other linear receivers, and show that
the suggested algorithm outperforms the minimum mean square estimator (MMSE)
and zero-forcing (ZF) linear receivers, but trades-off error performance for
complexity in comparison with the IF receiver based on exhaustive search or on
HKZ and Minkowski lattice reduction algorithms.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1420</identifier>
 <datestamp>2013-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1420</id><created>2013-01-08</created><authors><author><keyname>Slinko</keyname><forenames>Arkadii</forenames></author><author><keyname>White</keyname><forenames>Shaun</forenames></author></authors><title>Is it ever safe to vote strategically?</title><categories>cs.GT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  There are many situations in which mis-coordinated strategic voting can leave
strategic voters worse off than they would have been had they not tried to
strategize. We analyse the simplest of such scenarios, in which the set of
strategic voters all have the same sincere preferences and all cast the same
strategic vote, while all other voters vote sincerely. Most mis-coordinations
in this framework can be classified as instances of either strategic
overshooting (too many voted strategically) or strategic undershooting (too
few). If mis-coordination can result in strategic voters ending up worse off
than they would have been had they all just voted sincerely, we call the
relevant strategic vote unsafe. We show that under every onto and
non-dictatorial social choice rule there exist circumstances where a voter has
an incentive to cast a safe strategic vote. We extend the Gibbard-Satterthwaite
Theorem by proving that every onto and non-dictatorial social choice rule can
be individually manipulated by a voter casting a safe strategic vote.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1423</identifier>
 <datestamp>2014-02-18</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1423</id><created>2013-01-08</created><updated>2014-02-15</updated><authors><author><keyname>Xu</keyname><forenames>Yingying</forenames></author><author><keyname>Kabashima</keyname><forenames>Yoshiyuki</forenames></author></authors><title>Statistical mechanics approach to 1-bit compressed sensing</title><categories>cs.IT cond-mat.dis-nn math.IT</categories><comments>23 pages, 7 figures, 1 table</comments><journal-ref>Journal of Statistical Mechanics: Theory and Experiment (2013)
  P02041</journal-ref><doi>10.1088/1742-5468/2013/02/P02041</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Compressed sensing is a technique for recovering a high-dimensional signal
from lower-dimensional data, whose components represent partial information
about the signal, utilizing prior knowledge on the sparsity of the signal. For
further reducing the data size of the compressed expression, a scheme to
recover the original signal utilizing only the sign of each entry of the
linearly transformed vector was recently proposed. This approach is often
termed the 1-bit compressed sensing. Here we analyze the typical performance of
an L1-norm based signal recovery scheme for the 1-bit compressed sensing using
statistical mechanics methods. We show that the signal recovery performance
predicted by the replica method under the replica symmetric ansatz, which turns
out to be locally unstable for modes breaking the replica symmetry, is in a
good consistency with experimental results of an approximate recovery algorithm
developed earlier. This suggests that the L1-based recovery problem typically
has many local optima of a similar recovery accuracy, which can be achieved by
the approximate algorithm. We also develop another approximate recovery
algorithm inspired by the cavity method. Numerical experiments show that when
the density of nonzero entries in the original signal is relatively large the
new algorithm offers better performance than the abovementioned scheme and does
so with a lower computational cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1425</identifier>
 <datestamp>2013-11-19</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1425</id><created>2013-01-08</created><updated>2013-11-16</updated><authors><author><keyname>Komarath</keyname><forenames>Balagopal</forenames></author><author><keyname>N</keyname><forenames>Jayalal Sarma M.</forenames></author></authors><title>Pebbling, Entropy and Branching Program Size Lower Bounds</title><categories>cs.CC</categories><comments>25 Pages, Manuscript submitted to Journal in June 2013 This version
  includes a proof for tight size bounds for (syntactic) read-once NTBPs. The
  proof is in the same spirit as the proof for size bounds for bitwise
  independent NTBPs present in the earlier version of the paper and is included
  in the journal version of the paper submitted in June 2013</comments><acm-class>F.1.1</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We contribute to the program of proving lower bounds on the size of branching
programs solving the Tree Evaluation Problem introduced by Cook et. al. (2012).
Proving a super-polynomial lower bound for the size of nondeterministic thrifty
branching programs (NTBP) would separate $NL$ from $P$ for thrifty models
solving the tree evaluation problem. First, we show that {\em Read-Once NTBPs}
are equivalent to whole black-white pebbling algorithms thus showing a tight
lower bound (ignoring polynomial factors) for this model.
  We then introduce a weaker restriction of NTBPs called {\em Bitwise
Independence}. The best known NTBPs (of size $O(k^{h/2+1})$) for the tree
evaluation problem given by Cook et. al. (2012) are Bitwise Independent. As our
main result, we show that any Bitwise Independent NTBP solving $TEP_{2}^{h}(k)$
must have at least $\frac{1}{2}k^{h/2}$ states. Prior to this work, lower
bounds were known for NTBPs only for fixed heights $h=2,3,4$ (See Cook et. al.
(2012)). We prove our results by associating a fractional black-white pebbling
strategy with any bitwise independent NTBP solving the Tree Evaluation Problem.
Such a connection was not known previously even for fixed heights.
  Our main technique is the entropy method introduced by Jukna and Z{\'a}k
(2001) originally in the context of proving lower bounds for read-once
branching programs. We also show that the previous lower bounds given by Cook
et. al. (2012) for deterministic branching programs for Tree Evaluation Problem
can be obtained using this approach. Using this method, we also show tight
lower bounds for any $k$-way deterministic branching program solving Tree
Evaluation Problem when the instances are restricted to have the same group
operation in all internal nodes.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1429</identifier>
 <datestamp>2013-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1429</id><created>2013-01-08</created><authors><author><keyname>Alis</keyname><forenames>Christian M.</forenames></author><author><keyname>Lim</keyname><forenames>May T.</forenames></author></authors><title>Adaptation of fictional and online conversations to communication media</title><categories>physics.soc-ph cs.CL physics.data-an</categories><journal-ref>Eur. Phys. J. B, vol. 85, no. 12, pp. 1-7, 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Conversations allow the quick transfer of short bits of information and it is
reasonable to expect that changes in communication medium affect how we
converse. Using conversations in works of fiction and in an online social
networking platform, we show that the utterance length of conversations is
slowly shortening with time but adapts more strongly to the constraints of the
communication medium. This indicates that the introduction of any new medium of
communication can affect the way natural language evolves.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1444</identifier>
 <datestamp>2013-12-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1444</id><created>2013-01-08</created><updated>2013-12-06</updated><authors><author><keyname>Mortera</keyname><forenames>Julia</forenames></author><author><keyname>Vicard</keyname><forenames>Paola</forenames></author><author><keyname>Vergari</keyname><forenames>Cecilia</forenames></author></authors><title>Object-oriented Bayesian networks for a decision support system for
  antitrust enforcement</title><categories>cs.AI stat.AP</categories><comments>Published in at http://dx.doi.org/10.1214/12-AOAS625 the Annals of
  Applied Statistics (http://www.imstat.org/aoas/) by the Institute of
  Mathematical Statistics (http://www.imstat.org)</comments><proxy>vtex</proxy><report-no>IMS-AOAS-AOAS625</report-no><journal-ref>Annals of Applied Statistics 2013, Vol. 7, No. 2, 714-738</journal-ref><doi>10.1214/12-AOAS625</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We study an economic decision problem where the actors are two firms and the
Antitrust Authority whose main task is to monitor and prevent firms' potential
anti-competitive behaviour and its effect on the market. The Antitrust
Authority's decision process is modelled using a Bayesian network where both
the relational structure and the parameters of the model are estimated from a
data set provided by the Authority itself. A number of economic variables that
influence this decision process are also included in the model. We analyse how
monitoring by the Antitrust Authority affects firms' strategies about
cooperation. Firms' strategies are modelled as a repeated prisoner's dilemma
using object-oriented Bayesian networks. We show how the integration of firms'
decision process and external market information can be modelled in this way.
Various decision scenarios and strategies are illustrated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1465</identifier>
 <datestamp>2013-06-03</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1465</id><created>2013-01-08</created><updated>2013-05-31</updated><authors><author><keyname>Condo</keyname><forenames>Carlo</forenames></author><author><keyname>Baghdadi</keyname><forenames>Amer</forenames></author><author><keyname>Masera</keyname><forenames>Guido</forenames></author></authors><title>A joint communication and application simulator for NoC-based SoCs</title><categories>cs.AR</categories><comments>Withdrawn, due to extended and revised version being published</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  NoCs have become a widespread paradigm in the system-on-chip design world,
not only for multi-purpose SoCs, but also for application-specific ICs. The
common approach in the NoC design world is to separate the design of the
interconnection from the design of the processing elements: this is well suited
for a large number of developments, but the need for joint application and NoC
design is not uncommon, especially in the application specific case. The
correlation between processing and communication tasks can be strong, and
separate or trace-based simulations fall often short of the desired precision.
In this work, the OMNET++ based JANoCS simulator is presented: concurrent
simulation of processing and communication allow cycle-accurate evaluation of
the system. Two cases of study are presented, showing both the need for joint
simulations and the effectiveness of JANoCS.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1493</identifier>
 <datestamp>2013-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1493</id><created>2013-01-08</created><authors><author><keyname>McKay</keyname><forenames>Brendan D.</forenames></author><author><keyname>Piperno</keyname><forenames>Adolfo</forenames></author></authors><title>Practical graph isomorphism, II</title><categories>cs.DM math.CO</categories><comments>This is partially a replacement for http://arxiv.org/abs/0804.4881</comments><msc-class>05C85, 68R10, 20B40</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We report the current state of the graph isomorphism problem from the
practical point of view. After describing the general principles of the
refinement-individualization paradigm and proving its validity, we explain how
it is implemented in several of the key programs. In particular, we bring the
description of the best known program nauty up to date and describe an
innovative approach called Traces that outperforms the competitors for many
difficult graph classes. Detailed comparisons against saucy, Bliss and conauto
are presented.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1502</identifier>
 <datestamp>2013-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1502</id><created>2013-01-08</created><authors><author><keyname>Kalaiselvi</keyname><forenames>N.</forenames></author><author><keyname>Inbarani</keyname><forenames>H. Hannah</forenames></author></authors><title>Fuzzy Soft Set Based Classification for Gene Expression Data</title><categories>cs.AI cs.CE</categories><comments>7 pages, IJSER Vol.3 Issue: 10 Oct 2012</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Classification is one of the major issues in Data Mining Research fields. The
classification problems in medical area often classify medical dataset based on
the result of medical diagnosis or description of medical treatment by the
medical practitioner. This research work discusses the classification process
of Gene Expression data for three different cancers which are breast cancer,
lung cancer and leukemia cancer with two classes which are cancerous stage and
non cancerous stage. We have applied a fuzzy soft set similarity based
classifier to enhance the accuracy to predict the stages among cancer genes and
the informative genes are selected by using Entopy filtering.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1514</identifier>
 <datestamp>2013-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1514</id><created>2013-01-08</created><authors><author><keyname>F&#xfc;l&#xf6;p</keyname><forenames>Zolt&#xe1;n</forenames></author><author><keyname>Maletti</keyname><forenames>Andreas</forenames></author></authors><title>Composition Closure of Linear Extended Top-down Tree Transducers</title><categories>cs.FL</categories><comments>21 pages, 7 figures, 4 tables</comments><msc-class>68Q45, 68Q42</msc-class><acm-class>F.4.3; F.4.2</acm-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Linear extended top-down tree transducers (or synchronous tree-substitution
grammars) are popular formal models of tree transformations. The expressive
power of compositions of such transducers with and without regular look-ahead
is investigated. In particular, the restrictions of nondeletion,
epsilon-freeness, and strictness are considered. The composition hierarchy
turns out to be finite for all epsilon-free (all rules consume input) variants
of these transducers except for nondeleting epsilon-free linear extended
top-down tree transducers. The least number of transducers needed for the full
expressive power of arbitrary compositions is presented. In all remaining cases
(including nondeleting epsilon-free linear extended top-down tree transducers)
the composition hierarchy does not collapse.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1517</identifier>
 <datestamp>2013-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1517</id><created>2013-01-08</created><authors><author><keyname>Wahlstr&#xf6;m</keyname><forenames>Magnus</forenames></author></authors><title>Abusing the Tutte Matrix: An Algebraic Instance Compression for the
  K-set-cycle Problem</title><categories>cs.DS</categories><comments>12 pages</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We give an algebraic, determinant-based algorithm for the K-Cycle problem,
i.e., the problem of finding a cycle through a set of specified elements. Our
approach gives a simple FPT algorithm for the problem, matching the
$O^*(2^{|K|})$ running time of the algorithm of Bj\&quot;orklund et al. (SODA,
2012). Furthermore, our approach is open for treatment by classical algebraic
tools (e.g., Gaussian elimination), and we show that it leads to a polynomial
compression of the problem, i.e., a polynomial-time reduction of the $K$-Cycle
problem into an algebraic problem with coding size $O(|K|^3)$. This is
surprising, as several related problems (e.g., k-Cycle and the Disjoint Paths
problem) are known not to admit such a reduction unless the polynomial
hierarchy collapses. Furthermore, despite the result, we are not aware of any
witness for the K-Cycle problem of size polynomial in $|K|+\log n$, which seems
(for now) to separate the notions of polynomial compression and polynomial
kernelization (as a polynomial kernelization for a problem in NP necessarily
implies a small witness).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1534</identifier>
 <datestamp>2013-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1534</id><created>2013-01-08</created><authors><author><keyname>Saganowski</keyname><forenames>Stanis&#x142;aw</forenames></author><author><keyname>Br&#xf3;dka</keyname><forenames>Piotr</forenames></author><author><keyname>Kazienko</keyname><forenames>Przemys&#x142;aw</forenames></author></authors><title>Influence Of The User Importance Measure On The Group Evolution
  Discovery</title><categories>cs.SI physics.soc-ph</categories><comments>Creative Commons Attribution-NonCommercial-NoDerivs license.
  Presented at the Congress of Young IT Scientists, Mi{\ke}dzyzdroje, Poland,
  20-22.09.2012</comments><journal-ref>Foundations of Computing and Decision Sciences, Volume 37, Issue
  4, Pages 293-303, 2012</journal-ref><doi>10.2478/v10209-011-0017-6</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  One of the most interesting topics in social network science are social
groups. Their extraction, dynamics and evolution. One year ago the method for
group evolution discovery (GED) was introduced. The GED method during
extraction process takes into account both the group members quality and
quantity. The quality is reflected by user importance measure. In this paper
the influence of different user importance measures on the results of the GED
method is examined and presented. The results indicate that using global
measures like social position (page rank) allows to achieve more precise
results than using local measures like degree centrality or no measure at all.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1547</identifier>
 <datestamp>2013-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1547</id><created>2013-01-08</created><updated>2013-01-15</updated><authors><author><keyname>Bauwens</keyname><forenames>Bruno</forenames></author><author><keyname>Makhlin</keyname><forenames>Anton</forenames></author><author><keyname>Vereshchagin</keyname><forenames>Nikolay</forenames></author><author><keyname>Zimand</keyname><forenames>Marius</forenames></author></authors><title>Short lists with short programs in short time</title><categories>cs.CC cs.DS</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Given a machine U, a c-short program for x is a string p such that U(p)=x and
the length of p is bounded by c + (the length of a shortest program for x). We
show that for any universal machine, it is possible to compute in polynomial
time on input x a list of polynomial size guaranteed to contain a
O(log|x|)-short program for x. We also show that there exist computable
functions that map every x to a list of size O(|x|^2) containing a O(1)-short
program for x and this is essentially optimal because we prove that such a list
must have size O(|x|^2) . Finally we show that for some machines, computable
lists containing a shortest program must have length Omega(2^|x|) .
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1549</identifier>
 <datestamp>2013-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1549</id><created>2013-01-08</created><authors><author><keyname>Gast&#xf3;n</keyname><forenames>Bernat</forenames></author><author><keyname>Pujol</keyname><forenames>Jaume</forenames></author><author><keyname>Villanueva</keyname><forenames>Merc&#xe8;</forenames></author></authors><title>A realistic distributed storage system that minimizes data storage and
  repair bandwidth</title><categories>cs.IT cs.DC math.IT</categories><comments>10 pages, accepted as a poster in the Data Compression Conference
  2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In a realistic distributed storage environment, storage nodes are usually
placed in racks, a metallic support designed to accommodate electronic
equipment. It is known that the communication (bandwidth) cost between nodes
within a rack is much lower than the communication (bandwidth) cost between
nodes within different racks.
  In this paper, a new model, where the storage nodes are placed in two racks,
is proposed and analyzed. In this model, the storage nodes have different
repair costs to repair a node depending on the rack where they are placed. A
threshold function, which minimizes the amount of stored data per node and the
bandwidth needed to regenerate a failed node, is shown. This threshold function
generalizes the threshold function from previous distributed storage models.
The tradeoff curve obtained from this threshold function is compared with the
ones obtained from the previous models, and it is shown that this new model
outperforms the previous ones in terms of repair cost.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1551</identifier>
 <datestamp>2013-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1551</id><created>2013-01-08</created><authors><author><keyname>Ewerling</keyname><forenames>Philipp</forenames></author></authors><title>A novel processing pipeline for optical multi-touch surfaces</title><categories>cs.CV</categories><comments>MSc Thesis, 80 pages, a condensed version of this thesis has been
  published as &quot;Finger and hand detection for multi-touch interfaces based on
  maximally stable extremal regions&quot; in Proceedings of the 2012 ACM
  international conference on Interactive tabletops and surfaces.
  (http://doi.acm.org/10.1145/2396636.2396663)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this thesis a new approach for touch detection on optical multi-touch
devices is proposed that exploits the fact that the camera images reveal not
only the actual touch points but also objects above the screen such as the hand
or arm of a user. The touch processing relies on the Maximally Stable Extremal
Regions algorithm for finding the users' fingertips in the camera image. The
hierarchical structure of the generated extremal regions serves as a starting
point for agglomerative clustering of the fingertips into hands. Furthermore, a
heuristic is suggested that supports the identification of individual fingers
as well as the distinction between left hands and right hands if all five
fingers of a hand are in contact with the touch surface.
  The evaluation confirmed that the system is robust against detection errors
resulting from non-uniform illumination and reliably assigns touch points to
individual hands based on the implicitly tracked context information. The
efficient multi-threaded implementation handles two-handed input from multiple
users in real-time.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1555</identifier>
 <datestamp>2013-08-26</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1555</id><created>2013-01-08</created><updated>2013-08-23</updated><authors><author><keyname>Karbasi</keyname><forenames>Amin</forenames></author><author><keyname>Salavati</keyname><forenames>Amir Hesam</forenames></author><author><keyname>Shokrollahi</keyname><forenames>Amin</forenames></author></authors><title>Coupled Neural Associative Memories</title><categories>cs.NE cs.IT cs.LG math.IT</categories><comments>A shorter version of this paper is going to be submitted to
  International symposium on Information Theory (ISIT 2013) in Istanbul, Turkey</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We propose a novel architecture to design a neural associative memory that is
capable of learning a large number of patterns and recalling them later in
presence of noise. It is based on dividing the neurons into local clusters and
parallel plains, very similar to the architecture of the visual cortex of
macaque brain. The common features of our proposed architecture with those of
spatially-coupled codes enable us to show that the performance of such networks
in eliminating noise is drastically better than the previous approaches while
maintaining the ability of learning an exponentially large number of patterns.
Previous work either failed in providing good performance during the recall
phase or in offering large pattern retrieval (storage) capacities. We also
present computational experiments that lend additional support to the
theoretical analysis.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1563</identifier>
 <datestamp>2013-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1563</id><created>2012-12-26</created><authors><author><keyname>Tang</keyname><forenames>Kun</forenames></author><author><keyname>Jin</keyname><forenames>Qiwei</forenames></author><author><keyname>Zou</keyname><forenames>Xin</forenames></author><author><keyname>Yang</keyname><forenames>Jiansheng</forenames></author><author><keyname>Vannier</keyname><forenames>Michael</forenames></author><author><keyname>Wang</keyname><forenames>Ge</forenames></author></authors><title>Academic Ranking with Web Mining and Axiomatic Analysis</title><categories>cs.DL</categories><comments>11 pages, 3 figures, 3 tables, and 10 references</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Academic ranking is a public topic, such as for universities, colleges, or
departments, which has significant educational, administrative and social
effects. Popular ranking systems include the US News &amp; World Report (USNWR),
the Academic Ranking of World Universities (ARWU), and others. The most popular
observables for such ranking are academic publications and their citations.
However, a rigorous, quantitative and thorough methodology has been missing for
this purpose. With modern web technology and axiomatic bibliometric analysis,
here we perform a feasibility study on Microsoft Academic Search metadata and
obtain the first-of-its-kind ranking results for American departments of
computer science. This approach can be extended for fully automatic intuitional
and college ranking based on comprehensive data on Internet.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1575</identifier>
 <datestamp>2013-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1575</id><created>2013-01-05</created><authors><author><keyname>Pyayt</keyname><forenames>Anna</forenames></author><author><keyname>Gubanov</keyname><forenames>Michael</forenames></author></authors><title>BigDB: Automatic Machine Learning Optimizer</title><categories>cs.DB</categories><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this short vision paper, we introduce a machine learning optimizer for
data management and describe its architecture and main functionality.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1576</identifier>
 <datestamp>2013-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1576</id><created>2013-01-08</created><updated>2013-05-21</updated><authors><author><keyname>Kirisits</keyname><forenames>Clemens</forenames></author><author><keyname>Lang</keyname><forenames>Lukas F.</forenames></author><author><keyname>Scherzer</keyname><forenames>Otmar</forenames></author></authors><title>Optical Flow on Evolving Surfaces with an Application to the Analysis of
  4D Microscopy Data</title><categories>math.OC cs.CV</categories><comments>The final publication is available at link.springer.com</comments><doi>10.1007/978-3-642-38267-3_21</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We extend the concept of optical flow to a dynamic non-Euclidean setting.
Optical flow is traditionally computed from a sequence of flat images. It is
the purpose of this paper to introduce variational motion estimation for images
that are defined on an evolving surface. Volumetric microscopy images depicting
a live zebrafish embryo serve as both biological motivation and test data.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1578</identifier>
 <datestamp>2013-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1578</id><created>2012-12-20</created><authors><author><keyname>Toosarvandani</keyname><forenames>Marzieh Sameni</forenames></author><author><keyname>Modiri</keyname><forenames>Nasser</forenames></author><author><keyname>Afzali</keyname><forenames>Mahdi</forenames></author></authors><title>The risk assessment and treatment approach in order to provide lan
  security based on isms standard</title><categories>cs.CR</categories><comments>20 pages, 2 figures, 5 tables, International Journal in Foundations
  of Computer Science &amp; Technology (IJFCST), Vol. 2, No.6, November 2012</comments><msc-class>68M99</msc-class><acm-class>H.1.0</acm-class><doi>10.5121/ijfcst.2012.2602</doi><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  Local Area Networks(LAN) at present become an important instrument for
organizing of process and information communication in an organization. They
provides important purposes such as association of large amount of data,
hardware and software resources and expanding of optimum communications. Becase
these network do work with valuable information, the problem of security
providing is an important issue in organization. So, the stablishment of an
information security management system(ISMS) in organization is significant. In
this paper, we introduce ISMS and its implementation in LAN scop. The assets of
LAN and threats and vulnerabilities of these assets are identified, the risks
are evaluated and techniques to reduce them and at result security
establishment of the network is expressed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1590</identifier>
 <datestamp>2013-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1590</id><created>2013-01-08</created><authors><author><keyname>Chitsaz</keyname><forenames>Hamidreza</forenames></author><author><keyname>Forouzmand</keyname><forenames>Elmirasadat</forenames></author><author><keyname>Haffari</keyname><forenames>Gholamreza</forenames></author></authors><title>An Efficient Algorithm for Upper Bound on the Partition Function of
  Nucleic Acids</title><categories>q-bio.BM cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  It has been shown that minimum free energy structure for RNAs and RNA-RNA
interaction is often incorrect due to inaccuracies in the energy parameters and
inherent limitations of the energy model. In contrast, ensemble based
quantities such as melting temperature and equilibrium concentrations can be
more reliably predicted. Even structure prediction by sampling from the
ensemble and clustering those structures by Sfold [7] has proven to be more
reliable than minimum free energy structure prediction. The main obstacle for
ensemble based approaches is the computational complexity of the partition
function and base pairing probabilities. For instance, the space complexity of
the partition function for RNA-RNA interaction is $O(n^4)$ and the time
complexity is $O(n^6)$ which are prohibitively large [4,12]. Our goal in this
paper is to give a fast algorithm, based on sparse folding, to calculate an
upper bound on the partition function. Our work is based on the recent
algorithm of Hazan and Jaakkola [10]. The space complexity of our algorithm is
the same as that of sparse folding algorithms, and the time complexity of our
algorithm is $O(MFE(n)\ell)$ for single RNA and $O(MFE(m, n)\ell)$ for RNA-RNA
interaction in practice, in which $MFE$ is the running time of sparse folding
and $\ell \leq n$ ($\ell \leq n + m$) is a sequence dependent parameter.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1594</identifier>
 <datestamp>2014-11-25</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1594</id><created>2013-01-08</created><updated>2013-03-18</updated><authors><author><keyname>Berta</keyname><forenames>Mario</forenames></author><author><keyname>Renes</keyname><forenames>Joseph M.</forenames></author><author><keyname>Wilde</keyname><forenames>Mark M.</forenames></author></authors><title>Identifying the Information Gain of a Quantum Measurement</title><categories>quant-ph cs.IT math.IT</categories><comments>v2: new result about non-feedback measurement simulation, 45 pages, 4
  figures</comments><journal-ref>IEEE Transactions on Information Theory, vol. 60, no. 12, pages
  7987-8006, December 2014</journal-ref><doi>10.1109/TIT.2014.2365207</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that quantum-to-classical channels, i.e., quantum measurements, can
be asymptotically simulated by an amount of classical communication equal to
the quantum mutual information of the measurement, if sufficient shared
randomness is available. This result generalizes Winter's measurement
compression theorem for fixed independent and identically distributed inputs
[Winter, CMP 244 (157), 2004] to arbitrary inputs, and more importantly, it
identifies the quantum mutual information of a measurement as the information
gained by performing it, independent of the input state on which it is
performed. Our result is a generalization of the classical reverse Shannon
theorem to quantum-to-classical channels. In this sense, it can be seen as a
quantum reverse Shannon theorem for quantum-to-classical channels, but with the
entanglement assistance and quantum communication replaced by shared randomness
and classical communication, respectively. The proof is based on a novel
one-shot state merging protocol for &quot;classically coherent states&quot; as well as
the post-selection technique for quantum channels, and it uses techniques
developed for the quantum reverse Shannon theorem [Berta et al., CMP 306 (579),
2011].
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1608</identifier>
 <datestamp>2013-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1608</id><created>2013-01-08</created><authors><author><keyname>Forouzmand</keyname><forenames>Elmirasadat</forenames></author><author><keyname>Chitsaz</keyname><forenames>Hamidreza</forenames></author></authors><title>The RNA Newton Polytope and Learnability of Energy Parameters</title><categories>q-bio.BM cs.CE cs.LG</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Despite nearly two scores of research on RNA secondary structure and RNA-RNA
interaction prediction, the accuracy of the state-of-the-art algorithms are
still far from satisfactory. Researchers have proposed increasingly complex
energy models and improved parameter estimation methods in anticipation of
endowing their methods with enough power to solve the problem. The output has
disappointingly been only modest improvements, not matching the expectations.
Even recent massively featured machine learning approaches were not able to
break the barrier. In this paper, we introduce the notion of learnability of
the parameters of an energy model as a measure of its inherent capability. We
say that the parameters of an energy model are learnable iff there exists at
least one set of such parameters that renders every known RNA structure to date
the minimum free energy structure. We derive a necessary condition for the
learnability and give a dynamic programming algorithm to assess it. Our
algorithm computes the convex hull of the feature vectors of all feasible
structures in the ensemble of a given input sequence. Interestingly, that
convex hull coincides with the Newton polytope of the partition function as a
polynomial in energy parameters. We demonstrated the application of our theory
to a simple energy model consisting of a weighted count of A-U and C-G base
pairs. Our results show that this simple energy model satisfies the necessary
condition for less than one third of the input unpseudoknotted
sequence-structure pairs chosen from the RNA STRAND v2.0 database. For another
one third, the necessary condition is barely violated, which suggests that
augmenting this simple energy model with more features such as the Turner loops
may solve the problem. The necessary condition is severely violated for 8%,
which provides a small set of hard cases that require further investigation.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1609</identifier>
 <datestamp>2013-08-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1609</id><created>2013-01-08</created><updated>2013-07-30</updated><authors><author><keyname>Jin</keyname><forenames>Xin</forenames></author><author><keyname>Marzouki</keyname><forenames>Abdelwaheb</forenames></author><author><keyname>Zeghlache</keyname><forenames>Djamal</forenames></author><author><keyname>Kong</keyname><forenames>Linghe</forenames></author><author><keyname>Vasilakos</keyname><forenames>Athanasios V.</forenames></author></authors><title>Two Design Issues in Cognitive Sub-Small Cell for Sojourners</title><categories>cs.IT math.IT</categories><comments>Small cells, cognitive radio, Multiuser Multiple-Input
  Multiple-Output (MU-MIMO), Block Diagonalization (BD) precoding based on
  uncertain Channel State Information (CSI), auxiliary optimal Beamformer (BF)</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper, we propound a solution named Cognitive Sub-Small Cell for
Sojourners (CSCS) in allusion to a broadly representative small cell scenario,
where users can be categorized into two groups: sojourners and inhabitants.
CSCS contributes to save energy, enhance the number of concurrently supportable
users and enshield inhabitants. We consider two design issues in CSCS: i)
determining the number of transmit antennas on sub-small cell APs; ii)
controlling downlink inter-sub-small cell interference. For issue i), we
excogitate an algorithm helped by the probability distribution of the number of
concurrent sojourners. For issue ii), we propose an interference control scheme
named BDBF: Block Diagonalization (BD) Precoding based on uncertain channel
state information in conjunction with auxiliary optimal Beamformer (BF). In the
simulation, we delve into the issue: how the factors impact the number of
transmit antennas on sub-small cell APs. Moreover, we verify a significant
conclusion: Using BDBF gains more capacity than using optimal BF alone within a
bearably large radius of uncertainty region.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1626</identifier>
 <datestamp>2013-05-23</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1626</id><created>2013-01-08</created><authors><author><keyname>Kandiah</keyname><forenames>Vivek</forenames></author><author><keyname>Shepelyansky</keyname><forenames>Dima L.</forenames></author></authors><title>Google matrix analysis of DNA sequences</title><categories>q-bio.GN cs.IR physics.soc-ph</categories><comments>latex, 11 figs</comments><journal-ref>PLoS ONE 8(5): e61519, 2013</journal-ref><doi>10.1371/journal.pone.0061519</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  For DNA sequences of various species we construct the Google matrix G of
Markov transitions between nearby words composed of several letters. The
statistical distribution of matrix elements of this matrix is shown to be
described by a power law with the exponent being close to those of outgoing
links in such scale-free networks as the World Wide Web (WWW). At the same time
the sum of ingoing matrix elements is characterized by the exponent being
significantly larger than those typical for WWW networks. This results in a
slow algebraic decay of the PageRank probability determined by the distribution
of ingoing elements. The spectrum of G is characterized by a large gap leading
to a rapid relaxation process on the DNA sequence networks. We introduce the
PageRank proximity correlator between different species which determines their
statistical similarity from the view point of Markov chains. The properties of
other eigenstates of the Google matrix are also discussed. Our results
establish scale-free features of DNA sequence networks showing their
similarities and distinctions with the WWW and linguistic networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1629</identifier>
 <datestamp>2013-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1629</id><created>2013-01-08</created><authors><author><keyname>Alglave</keyname><forenames>Jade</forenames></author><author><keyname>Kroening</keyname><forenames>Daniel</forenames></author><author><keyname>Tautschnig</keyname><forenames>Michael</forenames></author></authors><title>Partial Orders for Efficient BMC of Concurrent Software</title><categories>cs.LO cs.PL</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The vast number of interleavings that a concurrent program can have is
typically identified as the root cause of the difficulty of automatic analysis
of concurrent software. Weak memory is generally believed to make this problem
even harder. We address both issues by modelling programs' executions with
partial orders rather than the interleaving semantics (SC). We implemented a
software analysis tool based on these ideas. It scales to programs of
sufficient size to achieve first-time formal verification of non-trivial
concurrent systems code over a wide range of models, including SC, Intel x86
and IBM Power.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1638</identifier>
 <datestamp>2013-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1638</id><created>2013-01-08</created><authors><author><keyname>C&#xe9;c&#xe9;</keyname><forenames>G&#xe9;rard</forenames><affiliation>FEMTO-ST/DISC</affiliation></author></authors><title>Three Simulation Algorithms for Labelled Transition Systems</title><categories>cs.FL</categories><proxy>ccsd</proxy><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Algorithms which compute the coarsest simulation preorder are generally
designed on Kripke structures. Only in a second time they are extended to
labelled transition systems. By doing this, the size of the alphabet appears in
general as a multiplicative factor to both time and space complexities. Let $Q$
denotes the state space, $\rightarrow$ the transition relation, $\Sigma$ the
alphabet and $P_{sim}$ the partition of $Q$ induced by the coarsest simulation
equivalence. In this paper, we propose a base algorithm which minimizes, since
the first stages of its design, the incidence of the size of the alphabet in
both time and space complexities. This base algorithm, inspired by the one of
Paige and Tarjan in 1987 for bisimulation and the one of Ranzato and Tapparo in
2010 for simulation, is then derived in three versions. One of them has the
best bit space complexity up to now,
$O(|P_{sim}|^2+|{\rightarrow}|.\log|{\rightarrow}|)$, while another one has the
best time complexity up to now, $O(|P_{sim}|.|{\rightarrow}|)$. Note the
absence of the alphabet in these complexities. A third version happens to be a
nice compromise between space and time since it runs in
$O(b.|P_{sim}|.|{\rightarrow}|)$ time, with $b$ a branching factor generally
far below $|P_{sim}|$, and uses
$O(|P_{sim}|^2.\log|P_{sim}|+|{\rightarrow}|.\log|{\rightarrow}|)$ bits.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1661</identifier>
 <datestamp>2013-07-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1661</id><created>2013-01-08</created><updated>2013-07-14</updated><authors><author><keyname>Liu</keyname><forenames>Xi</forenames></author><author><keyname>Erkip</keyname><forenames>Elza</forenames></author></authors><title>Transmission Schemes for Gaussian Interference Channels with Transmitter
  Processing Energy</title><categories>cs.IT math.IT</categories><comments>23 pages, 11 figures, to appear in journal publication</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work considers communication over Gaussian interference channels with
processing energy cost, which explicitly takes into account the energy expended
for processing when transmitters are on. In the presence of processing energy
cost, transmitting all the time as in the conventional no-cost case is no
longer optimal. For a two-user Gaussian interference channel with processing
energy cost, assuming that the on-off states of transmitters are not utilized
for signaling, several transmission schemes with varying complexities are
proposed and their sum rates are compared with an interference-free upper
bound. Moreover, the very strong interference regime, under which interference
does not incur any rate penalty, is identified and shown to be larger than the
case of no processing energy cost for certain scenarios of interest. Also,
extensions to a three-user cascade Gaussian Z interference channel with
processing energy cost are provided, where scheduling of user transmissions
based on the channel set-up is investigated.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1671</identifier>
 <datestamp>2013-01-09</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1671</id><created>2013-01-08</created><authors><author><keyname>Couprie</keyname><forenames>Camille</forenames></author><author><keyname>Farabet</keyname><forenames>Cl&#xe9;ment</forenames></author><author><keyname>LeCun</keyname><forenames>Yann</forenames></author></authors><title>Causal graph-based video segmentation</title><categories>cs.CV</categories><comments>6 pages, 5 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Numerous approaches in image processing and computer vision are making use of
super-pixels as a pre-processing step. Among the different methods producing
such over-segmentation of an image, the graph-based approach of Felzenszwalb
and Huttenlocher is broadly employed. One of its interesting properties is that
the regions are computed in a greedy manner in quasi-linear time. The algorithm
may be trivially extended to video segmentation by considering a video as a 3D
volume, however, this can not be the case for causal segmentation, when
subsequent frames are unknown. We propose an efficient video segmentation
approach that computes temporally consistent pixels in a causal manner, filling
the need for causal and real time applications.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1701</identifier>
 <datestamp>2013-01-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1701</id><created>2013-01-08</created><authors><author><keyname>Mirzaee</keyname><forenames>Meysam</forenames></author><author><keyname>Akhlaghi</keyname><forenames>Soroush</forenames></author></authors><title>Secrecy Capacity of Two-Hop Relay Assisted Wiretap Channels</title><categories>cs.IT math.IT</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Incorporating the physical layer characteristics to secure communications has
received considerable attention in recent years. Moreover, cooperation with
some nodes of network can give benefits of multiple-antenna systems, increasing
the secrecy capacity of such channels. In this paper, we consider cooperative
wiretap channel with the help of an Amplify and Forward (AF) relay to transmit
confidential messages from source to legitimate receiver in the presence of an
eavesdropper. In this regard, the secrecy capacity of AF relying is derived,
assuming the relay is subject to a peak power constraint. To this end, an
achievable secrecy rate for Gaussian input is evaluated through solving a
non-convex optimization problem. Then, it is proved that any rates greater than
this secrecy rate is not achievable. To do this, the capacity of a genie-aided
channel as an upper bound for the secrecy capacity of the underlying channel is
derived, showing this upper bound is equal to the computed achievable secrecy
rate with Gaussian input. Accordingly, the corresponding secrecy capacity is
compared to the Decode and Forward (DF) strategy which is served as the
benchmark in the current work.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1702</identifier>
 <datestamp>2013-05-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1702</id><created>2013-01-08</created><authors><author><keyname>Solovyev</keyname><forenames>Alexey</forenames></author><author><keyname>Hales</keyname><forenames>Thomas C.</forenames></author></authors><title>Formal Verification of Nonlinear Inequalities with Taylor Interval
  Approximations</title><categories>cs.LO math.LO</categories><comments>15 pages</comments><doi>10.1007/978-3-642-38088-4_26</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present a formal tool for verification of multivariate nonlinear
inequalities. Our verification method is based on interval arithmetic with
Taylor approximations. Our tool is implemented in the HOL Light proof assistant
and it is capable to verify multivariate nonlinear polynomial and
non-polynomial inequalities on rectangular domains. One of the main features of
our work is an efficient implementation of the verification procedure which can
prove non-trivial high-dimensional inequalities in several seconds. We
developed the verification tool as a part of the Flyspeck project (a formal
proof of the Kepler conjecture). The Flyspeck project includes about 1000
nonlinear inequalities. We successfully tested our method on more than 100
Flyspeck inequalities and estimated that the formal verification procedure is
about 3000 times slower than an informal verification method implemented in
C++. We also describe future work and prospective optimizations for our method.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1704</identifier>
 <datestamp>2013-01-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1704</id><created>2013-01-08</created><authors><author><keyname>Hu</keyname><forenames>Qi</forenames></author><author><keyname>Gumerov</keyname><forenames>Nail A.</forenames></author><author><keyname>Duraiswami</keyname><forenames>Ramani</forenames></author></authors><title>Parallel Algorithms for Constructing Data Structures for Fast Multipole
  Methods</title><categories>cs.MS cs.DC</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We present efficient algorithms to build data structures and the lists needed
for fast multipole methods. The algorithms are capable of being efficiently
implemented on both serial, data parallel GPU and on distributed architectures.
With these algorithms it is possible to map the FMM efficiently on to the GPU
or distributed heterogeneous CPU-GPU systems. Further, in dynamic problems, as
the distribution of the particles change, the reduced cost of building the data
structures improves performance. Using these algorithms, we demonstrate example
high fidelity simulations with large problem sizes by using FMM on both single
and multiple heterogeneous computing facilities equipped with multi-core CPU
and many-core GPUs.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1710</identifier>
 <datestamp>2013-01-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1710</id><created>2013-01-08</created><authors><author><keyname>Lindgren</keyname><forenames>Kristian</forenames></author><author><keyname>Verendel</keyname><forenames>Vilhelm</forenames></author></authors><title>Evolutionary Exploration of the Finitely Repeated Prisoners'
  Dilemma--The Effect of Out-of-Equilibrium Play</title><categories>q-bio.PE cs.GT</categories><journal-ref>Games 2013, 4(1):1-20</journal-ref><doi>10.3390/g4010001</doi><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  The finitely repeated Prisoners' Dilemma is a good illustration of the
discrepancy between the strategic behaviour suggested by a game-theoretic
analysis and the behaviour often observed among human players, where
cooperation is maintained through most of the game. A game-theoretic reasoning
based on backward induction eliminates strategies step by step until defection
from the first round is the only remaining choice, reflecting the Nash
equilibrium of the game. We investigate the Nash equilibrium solution for two
different sets of strategies in an evolutionary context, using
replicator-mutation dynamics. The first set consists of conditional
cooperators, up to a certain round, while the second set in addition to these
contains two strategy types that react differently on the first round action:
The &quot;Convincer&quot; strategies insist with two rounds of initial cooperation,
trying to establish more cooperative play in the game, while the &quot;Follower&quot;
strategies, although being first round defectors, have the capability to
respond to an invite in the first round. For both of these strategy sets,
iterated elimination of strategies shows that the only Nash equilibria are
given by defection from the first round. We show that the evolutionary dynamics
of the first set is always characterised by a stable fixed point, corresponding
to the Nash equilibrium, if the mutation rate is sufficiently small (but still
positive). The second strategy set is numerically investigated, and we find
that there are regions of parameter space where fixed points become unstable
and the dynamics exhibits cycles of different strategy compositions. The
results indicate that, even in the limit of very small mutation rate, the
replicator-mutation dynamics does not necessarily bring the system with
Convincers and Followers to the fixed point corresponding to the Nash
equilibrium of the game.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1712</identifier>
 <datestamp>2013-01-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1712</id><created>2013-01-08</created><authors><author><keyname>de Lamare</keyname><forenames>Rodrigo C.</forenames></author><author><keyname>Sampaio-Neto</keyname><forenames>Raimundo</forenames></author><author><keyname>Haardt</keyname><forenames>Martin</forenames></author></authors><title>Blind Adaptive Constrained Constant-Modulus Reduced-Rank Interference
  Suppression Algorithms Based on Interpolation, Switched Decimation and
  Filtering</title><categories>cs.IT math.IT</categories><comments>9 figures; IEEE Transactions on Signal Processing, 2011</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This work proposes a blind adaptive reduced-rank scheme and constrained
constant-modulus (CCM) adaptive algorithms for interference suppression in
wireless communications systems. The proposed scheme and algorithms are based
on a two-stage processing framework that consists of a transformation matrix
that performs dimensionality reduction followed by a reduced-rank estimator.
The complex structure of the transformation matrix of existing methods
motivates the development of a blind adaptive reduced-rank constrained (BARC)
scheme along with a low-complexity reduced-rank decomposition. The proposed
BARC scheme and a reduced-rank decomposition based on the concept of joint
interpolation, switched decimation and reduced-rank estimation subject to a set
of constraints are then detailed. The proposed set of constraints ensures that
the multi-path components of the channel are combined prior to dimensionality
reduction. In order to cost-effectively design the BARC scheme, we develop
low-complexity decimation techniques, stochastic gradient and recursive least
squares reduced-rank estimation algorithms. A model-order selection algorithm
for adjusting the length of the estimators is devised along with techniques for
determining the required number of switching branches to attain a predefined
performance. An analysis of the convergence properties and issues of the
proposed optimization and algorithms is carried out, and the key features of
the optimization problem are discussed. We consider the application of the
proposed algorithms to interference suppression in DS-CDMA systems. The results
show that the proposed algorithms outperform the best known reduced-rank
schemes, while requiring lower complexity.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1714</identifier>
 <datestamp>2013-02-01</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1714</id><created>2013-01-08</created><authors><author><keyname>Washizawa</keyname><forenames>Teruyoshi</forenames></author><author><keyname>Nakahara</keyname><forenames>Yasuhiro</forenames></author></authors><title>Parallel Computing of Discrete Element Method on GPU</title><categories>cs.CE cs.DC</categories><comments>3 tables</comments><journal-ref>Applied Mathematics, vol.4, no.1A, pp.242-247, (January 2013)</journal-ref><doi>10.4236/am.2013.41A037</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We investigate applicability of GPU to DEM. NVIDIA's code obtained superior
performance than CPU in computational time. A model of contact forces in
NVIDIA's code is too simple for practical use. We modify this model by
replacing it with the practical model. The simulation shows that the practical
model obtains the computing speed 6 times faster than the practical one on CPU
while 7 times slower than the simple one on GPU. The result are analyzed.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1722</identifier>
 <datestamp>2013-01-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1722</id><created>2013-01-08</created><authors><author><keyname>Deshpande</keyname><forenames>Yash</forenames></author><author><keyname>Montanari</keyname><forenames>Andrea</forenames></author></authors><title>Linear Bandits in High Dimension and Recommendation Systems</title><categories>cs.LG stat.ML</categories><comments>21 pages, 4 figures</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  A large number of online services provide automated recommendations to help
users to navigate through a large collection of items. New items (products,
videos, songs, advertisements) are suggested on the basis of the user's past
history and --when available-- her demographic profile. Recommendations have to
satisfy the dual goal of helping the user to explore the space of available
items, while allowing the system to probe the user's preferences.
  We model this trade-off using linearly parametrized multi-armed bandits,
propose a policy and prove upper and lower bounds on the cumulative &quot;reward&quot;
that coincide up to constants in the data poor (high-dimensional) regime. Prior
work on linear bandits has focused on the data rich (low-dimensional) regime
and used cumulative &quot;risk&quot; as the figure of merit. For this data rich regime,
we provide a simple modification for our policy that achieves near-optimal risk
performance under more restrictive assumptions on the geometry of the problem.
We test (a variation of) the scheme used for establishing achievability on the
Netflix and MovieLens datasets and obtain good agreement with the qualitative
predictions of the theory we develop.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1732</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1732</id><created>2013-01-08</created><authors><author><keyname>Gao</keyname><forenames>Jie</forenames></author><author><keyname>Vorobyov</keyname><forenames>Sergiy A.</forenames></author><author><keyname>Jiang</keyname><forenames>Hai</forenames></author><author><keyname>Zhang</keyname><forenames>Jianshu</forenames></author><author><keyname>Haardt</keyname><forenames>Martin</forenames></author></authors><title>Sum-Rate Maximization with Minimum Power Consumption for MIMO DF Two-Way
  Relaying: Part I - Relay Optimization</title><categories>cs.IT math.IT</categories><comments>24 pages, 4 figures, Submitted to the IEEE Trans. Signal Processing
  in August 2012</comments><journal-ref>J. Gao, S.A. Vorobyov, H. Jiang, J. Zhang, M. Haardt, &quot;Sum-rate
  maximization with minimum power consumption for MIMO DF TWR. Part I Relay
  optimization,&quot; IEEE Trans. Signal Processing, vol. 61, no. 14, pp. 3563-3577,
  July 2013</journal-ref><doi>10.1109/TSP.2013.2262277</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The problem of power allocation is studied for a multiple-input
multiple-output (MIMO) decode-and-forward (DF) two-way relaying system
consisting of two source nodes and one relay. It is shown that achieving
maximum sum-rate in such a system does not necessarily demand the consumption
of all available power at the relay. Instead, the maximum sum-rate can be
achieved through efficient power allocation with minimum power consumption.
Deriving such power allocation, however, is nontrivial due to the fact that it
generally leads to a nonconvex problem. In Part I of this two-part paper, a
sum-rate maximizing power allocation with minimum power consumption is found
for MIMO DF two-way relaying, in which the relay optimizes its own power
allocation strategy given the power allocation strategies of the source nodes.
An algorithm is proposed for efficiently finding the optimal power allocation
of the relay based on the proposed idea of relative water-levels. The
considered scenario features low complexity due to the fact that the relay
optimizes its power allocation without coordinating the source nodes. As a
trade-off for the low complexity, it is shown that there can be waste of power
at the source nodes because of no coordination between the relay and the source
nodes. Simulation results demonstrate the performance of the proposed algorithm
and the effect of asymmetry on the considered system.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1739</identifier>
 <datestamp>2013-01-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1739</id><created>2013-01-08</created><authors><author><keyname>Christmas</keyname><forenames>S. P.</forenames></author><author><keyname>Leidich</keyname><forenames>R. M.</forenames></author></authors><title>Driving an NP-Complete problem with Combinatorial Decomposition to
  generate a unique and irreversible bitstring from a single integer seed value</title><categories>cs.CR</categories><license>http://creativecommons.org/licenses/publicdomain/</license><abstract>  Generation of an (arbitrarily) long string of bits unique to a given
finite-length numerical seed is of great value in the field of random number
generation, computer simulations, and other areas of computer science.
Extending this idea such that the bitstring cannot be reverse-engineered to
recover the original seed value extends the value of such a system to the field
of cryptography, as the string can be used directly as an encryption mask, or
as the input to some other cryptographic function. The longer the string that
can be generated, the closer the system would come to the ideal cryptographic
case of the One Time Pad.
  In this paper we propose a scheme for taking an initial seed (nominally a
128-bit integer, but not restricted to such), and expanding this into a unique
bitstring of a length determined by a limit cycle that makes it useful in
practical applications. We utilize novel mathematical concepts such as
combinatorial decomposition to turn the seed value into unique rotations of a
pre-defined table, which are operated on via destructive functions such as
exclusive-OR (XOR) and add-with-carry (ADC) to eventually produce the unique
bitstring.
  We assert that the process of iterating the XOR or ADC operation conforms to
the known NP-complete problem known as Subset-Sum, meaning that the reversal of
the process that produced the bitstring is tantamount to solving the
NP-Complete Subset-Sum problem, which in turn is less efficient than the
brute-force method of testing every seed value until the corresponding
bitstring is found, making the system highly relevant in a cryptographic
context.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1740</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1740</id><created>2013-01-08</created><updated>2013-04-03</updated><authors><author><keyname>Schnoes</keyname><forenames>Alexandra M.</forenames></author><author><keyname>Ream</keyname><forenames>David C.</forenames></author><author><keyname>Thorman</keyname><forenames>Alexander W.</forenames></author><author><keyname>Babbitt</keyname><forenames>Patricia C.</forenames></author><author><keyname>Friedberg</keyname><forenames>Iddo</forenames></author></authors><title>Biases in the Experimental Annotations of Protein Function and their
  Effect on Our Understanding of Protein Function Space</title><categories>q-bio.GN cs.DL cs.IT math.IT</categories><comments>Accepted to PLoS Computational Biology. Press embargo applies. v4:
  text corrected for style and supplementary material inserted</comments><doi>10.1371/journal.pcbi.1003063</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  The ongoing functional annotation of proteins relies upon the work of
curators to capture experimental findings from scientific literature and apply
them to protein sequence and structure data. However, with the increasing use
of high-throughput experimental assays, a small number of experimental studies
dominate the functional protein annotations collected in databases. Here we
investigate just how prevalent is the &quot;few articles -- many proteins&quot;
phenomenon. We examine the experimentally validated annotation of proteins
provided by several groups in the GO Consortium, and show that the distribution
of proteins per published study is exponential, with 0.14% of articles
providing the source of annotations for 25% of the proteins in the UniProt-GOA
compilation. Since each of the dominant articles describes the use of an assay
that can find only one function or a small group of functions, this leads to
substantial biases in what we know about the function of many proteins.
Mass-spectrometry, microscopy and RNAi experiments dominate high throughput
experiments. Consequently, the functional information derived from these
experiments is mostly of the subcellular location of proteins, and of the
participation of proteins in embryonic developmental pathways. For some
organisms, the information provided by different studies overlap by a large
amount. We also show that the information provided by high throughput
experiments is less specific than those provided by low throughput experiments.
Given the experimental techniques available, certain biases in protein function
annotation due to high-throughput experiments are unavoidable. Knowing that
these biases exist and understanding their characteristics and extent is
important for database curators, developers of function annotation programs,
and anyone who uses protein function annotation data to plan experiments.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1746</identifier>
 <datestamp>2013-01-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1746</id><created>2013-01-08</created><authors><author><keyname>Shen</keyname><forenames>Yulong</forenames></author><author><keyname>Jiang</keyname><forenames>Xiaohong</forenames></author><author><keyname>Ma</keyname><forenames>Jianfeng</forenames></author></authors><title>Generalized Secure Transmission Protocol for Flexible Load-Balance
  Control with Cooperative Relays in Two-Hop Wireless Networks</title><categories>cs.CR cs.IT cs.NI math.IT</categories><comments>15 pages, 2 figures. arXiv admin note: substantial text overlap with
  arXiv:1212.0287, arXiv:1212.6627, arXiv:1211.7075</comments><license>http://creativecommons.org/licenses/by/3.0/</license><abstract>  This work considers secure transmission protocol for flexible load-balance
control in two-hop relay wireless networks without the information of both
eavesdropper channels and locations. The available secure transmission
protocols via relay cooperation in physical layer secrecy framework cannot
provide a flexible load-balance control, which may significantly limit their
application scopes. This paper extends the conventional works and proposes a
general transmission protocol with considering load-balance control, in which
the relay is randomly selected from the first $k$ preferable assistant relays
located in the circle area with the radius $r$ and the center at the middle
between source and destination (2HR-($r,k$) for short). This protocol covers
the available works as special cases, like ones with the optimal relay
selection ($r=\infty$, $k=1$) and with the random relay selection ($r=\infty$,
$k = n$ i.e. the number of system nodes) in the case of equal path-loss, ones
with relay selected from relay selection region ($r \in (0, \infty), k = 1$) in
the case of distance-dependent path-loss. The theoretic analysis is further
provided to determine the maximum number of eavesdroppers one network can
tolerate to ensure a desired performance in terms of the secrecy outage
probability and transmission outage probability. The analysis results also show
the proposed protocol can balance load distributed among the relays by a proper
setting of $r$ and $k$ under the premise of specified secure and reliable
requirements.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1747</identifier>
 <datestamp>2013-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1747</id><created>2013-01-08</created><updated>2013-01-09</updated><authors><author><keyname>Xu</keyname><forenames>Kui</forenames></author><author><keyname>Xu</keyname><forenames>Youyun</forenames></author><author><keyname>Zhang</keyname><forenames>Dongmei</forenames></author><author><keyname>Ma</keyname><forenames>Wenfeng</forenames></author></authors><title>On Max-SINR Receiver for HMT System over Doubly Dispersive Channel</title><categories>cs.IT math.IT</categories><comments>7 pages. The paper has been accepted by IEEE Transactions on
  Vehicular Technology 2013. Copyright transferred to IEEE. arXiv admin note:
  substantial text overlap with arXiv:1212.5792</comments><license>http://creativecommons.org/licenses/by-nc-sa/3.0/</license><abstract>  In this paper, a novel receiver for Hexagonal Multicarrier Transmission (HMT)
system based on the maximizing Signal-to-Interference-plus-Noise Ratio
(Max-SINR) criterion is proposed. Theoretical analyses show that there is a
timing offset between the prototype pulses of the proposed Max-SINR receiver
and the traditional projection receiver. Meanwhile, the timing offset should be
matched to the channel scattering factor of the doubly dispersive (DD) channel.
The closed form timing offset expressions of the prototype pulse for Max-SINR
HMT receiver over DD channel with different channel scattering functions are
derived. Simulation results show that the proposed Max-SINR receiver
outperforms traditional projection scheme and obtains an approximation to the
theoretical upper bound SINR performance. Consistent with the SINR performance
improvement, the bit error rate (BER) performance of HMT system has also been
further improved by using the proposed Max-SINR receiver. Meanwhile, the SINR
performance of the proposed Max-SINR receiver is robust to the channel delay
spread estimation errors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1748</identifier>
 <datestamp>2013-01-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1748</id><created>2013-01-08</created><authors><author><keyname>Petersen</keyname><forenames>Ian R.</forenames></author></authors><title>Quantum Robust Stability of a Small Josephson Junction in a Resonant
  Cavity</title><categories>quant-ph cs.SY math.OC</categories><comments>A version of this paper appeared in the proceedings of the 2012 IEEE
  Multi-conference on Systems and Control</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  This paper applies recent results on the robust stability of nonlinear
quantum systems to the case of a Josephson junction in a resonant cavity. The
Josephson junction is characterized by a Hamiltonian operator which contains a
non-quadratic term involving a cosine function. This leads to a sector bounded
nonlinearity which enables the previously developed theory to be applied to
this system in order to analyze its stability.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1751</identifier>
 <datestamp>2013-01-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1751</id><created>2013-01-08</created><authors><author><keyname>Liang</keyname><forenames>Hongyu</forenames></author><author><keyname>Yuan</keyname><forenames>Hao</forenames></author></authors><title>On the Complexity of $t$-Closeness Anonymization and Related Problems</title><categories>cs.DS cs.DB</categories><comments>An extended abstract to appear in DASFAA 2013</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  An important issue in releasing individual data is to protect the sensitive
information from being leaked and maliciously utilized. Famous privacy
preserving principles that aim to ensure both data privacy and data integrity,
such as $k$-anonymity and $l$-diversity, have been extensively studied both
theoretically and empirically. Nonetheless, these widely-adopted principles are
still insufficient to prevent attribute disclosure if the attacker has partial
knowledge about the overall sensitive data distribution. The $t$-closeness
principle has been proposed to fix this, which also has the benefit of
supporting numerical sensitive attributes. However, in contrast to
$k$-anonymity and $l$-diversity, the theoretical aspect of $t$-closeness has
not been well investigated.
  We initiate the first systematic theoretical study on the $t$-closeness
principle under the commonly-used attribute suppression model. We prove that
for every constant $t$ such that $0\leq t&lt;1$, it is NP-hard to find an optimal
$t$-closeness generalization of a given table. The proof consists of several
reductions each of which works for different values of $t$, which together
cover the full range. To complement this negative result, we also provide exact
and fixed-parameter algorithms. Finally, we answer some open questions
regarding the complexity of $k$-anonymity and $l$-diversity left in the
literature.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1752</identifier>
 <datestamp>2013-01-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1752</id><created>2013-01-08</created><authors><author><keyname>Bhattacharyya</keyname><forenames>Arnab</forenames></author><author><keyname>Kahn</keyname><forenames>Jeff</forenames></author></authors><title>A bipartite graph with non-unimodal independent set sequence</title><categories>math.CO cs.DM</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the independent set sequence of a bipartite graph need not be
unimodal.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1753</identifier>
 <datestamp>2013-01-17</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1753</id><created>2013-01-09</created><updated>2013-01-16</updated><authors><author><keyname>Godbole</keyname><forenames>Vaibhav</forenames></author></authors><title>FCA - An Approach On LEACH Protocol Of Wireless Sensor Networks Using
  Fuzzy Logic</title><categories>cs.NI cs.AI</categories><comments>This paper is withdrawn due to mistakes in the figure captions</comments><journal-ref>International Journal of Computer Communications and Networks
  (IJCCN),Vol.2, No.3, pp.1-13, December 2012</journal-ref><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In order to gather information more efficiently, wireless sensor networks are
partitioned into clusters. The most of the proposed clustering algorithms do
not consider the location of the base station. This situation causes hot spots
problem in multi-hop wireless sensor networks. In this paper, we propose a
fuzzy clustering algorithm (FCA) which aims to prolong the lifetime of wireless
sensor networks. FCA adjusts the cluster-head radius considering the residual
energy and the distance to the base station parameters of the sensor nodes.
This helps decreasing the intra-cluster work of the sensor nodes which are
closer to the base station or have lower battery level. We utilize fuzzy logic
for handling the uncertainties in cluster-head radius estimation. We compare
our algorithm with LEACH according to first node dies, half of the nodes alive
and energy-efficiency metrics. Our simulation results show that FCA performs
better than other algorithms in most of the cases. Therefore, our proposed
algorithm is a stable and energy-efficient clustering algorithm.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1757</identifier>
 <datestamp>2013-08-05</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1757</id><created>2013-01-09</created><updated>2013-08-02</updated><authors><author><keyname>Lengyel</keyname><forenames>Bal&#xe1;zs</forenames></author><author><keyname>Sebesty&#xe9;n</keyname><forenames>Tam&#xe1;s</forenames></author><author><keyname>Leydesdorff</keyname><forenames>Loet</forenames></author></authors><title>Challenges for regional innovation policies in CEE countries: Spatial
  concentration and foreign control of US patenting</title><categories>cs.CY</categories><comments>Science and Public Policy (forthcoming)</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Using techniques of data collection and mapping as overlays to Google
Maps--on the basis of patent information available online at the U.S. Patent
and Trademark Office (USPTO)--we point at two major and interconnected
challenges that policy-makers face in Central and Eastern Europe (CEE) when
combating the lagging innovation performance. First, we address the spatial
concentration by using a distribution analysis at the city level. The results
suggest that patenting is concentrated in post-socialist territories more than
in western nations and regions. However, there is not a single outstanding hub
in CEE when one compares USPTO patents normalized for the respective population
sizes. Secondly, we argue that dominance of foreign control over USPTO patents
is mostly embodied in international co-operations at the individual level, and
only rarely spilled-over to MNE subsidiaries. In our opinion, catching-up of
CEE in terms of patenting is unlikely, unless innovation policy measures focus
on growing hubs and target both domestic inventors and international relations
of companies.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1760</identifier>
 <datestamp>2015-06-12</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1760</id><created>2013-01-09</created><authors><author><keyname>McKilliam</keyname><forenames>Robby</forenames></author><author><keyname>Pollok</keyname><forenames>Andre</forenames></author><author><keyname>Cowley</keyname><forenames>Bill</forenames></author><author><keyname>Clarkson</keyname><forenames>I. Vaughan L.</forenames></author><author><keyname>Quinn</keyname><forenames>Barry</forenames></author></authors><title>Carrier phase and amplitude estimation for phase shift keying using
  pilots and data</title><categories>cs.IT math.IT stat.AP</categories><doi>10.1109/TSP.2014.2332976</doi><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We consider least squares estimators of carrier phase and amplitude from a
noisy communications signal that contains both pilot signals, known to the
receiver, and data signals, unknown to the receiver. We focus on signaling
constellations that have symbols evenly distributed on the complex unit circle,
i.e., M-ary phase shift keying. We show, under reasonably mild conditions on
the distribution of the noise, that the least squares estimator of carrier
phase is strongly consistent and asymptotically normally distributed. However,
the amplitude estimator is not consistent, but converges to a positive real
number that is a function of the true carrier amplitude, the noise distribution
and the size of the constellation. Our theoretical results can also be applied
to the case where no pilot symbols exist, i.e., noncoherent detection. The
results of Monte Carlo simulations are provided and these agree with the
theoretical results.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1762</identifier>
 <datestamp>2015-06-22</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1762</id><created>2013-01-09</created><updated>2015-06-18</updated><authors><author><keyname>Goldberg</keyname><forenames>David A.</forenames></author></authors><title>Second-order Markov random fields for independent sets on the infinite
  Cayley tree</title><categories>math.PR cs.CC cs.DM math-ph math.CO math.MP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Recently, there has been significant interest in understanding the properties
of Markov random fields (M.r.f.) defined on the independent sets of sparse
graphs. When these M.r.f. are restricted to pairwise interactions (i.e.
hardcore model), much progress has been made. However, considerably less is
known in the presence of higher-order interactions, which arise e.g. in the
analysis of independent sets with special properties and the study of
resource-constrained communication networks. In this paper, we further our
understanding of such models by analyzing M.r.f. with second-order interactions
on the independent sets of the infinite Cayley tree. We prove that the
associated Gibbsian specification satisfies the celebrated FKG Inequality
whenever the local potentials defining the Hamiltonian satisfy a log-convexity
condition. Under this condition, we give necessary and sufficient conditions
for the existence of a unique infinite-volume Gibbs measure in terms of an
explicit system of equations, prove the existence of a phase transition, and
give explicit bounds on the associated critical activity, which we prove to
exhibit a certain robustness. For potentials which are small perturbations of
those coinciding to the hardcore model at the critical activity, we
characterize whether the resulting specification has a unique infinite-volume
Gibbs measure in terms of whether these perturbations satisfy an explicit
linear inequality. Our analysis reveals an interesting non-monotonicity with
regards to biasing towards excluded nodes with no included neighbors.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1799</identifier>
 <datestamp>2013-01-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1799</id><created>2013-01-09</created><authors><author><keyname>Bornmann</keyname><forenames>Lutz</forenames></author><author><keyname>Williams</keyname><forenames>Richard</forenames></author></authors><title>How to calculate the practical significance of citation impact
  differences? An empirical example from evaluative institutional bibliometrics
  using adjusted predictions and marginal effects</title><categories>cs.DL stat.AP</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Evaluative bibliometrics is concerned with comparing research units by using
statistical procedures. According to Williams (2012) an empirical study should
be concerned with the substantive and practical significance of the findings as
well as the sign and statistical significance of effects. In this study we will
explain what adjusted predictions and marginal effects are and how useful they
are for institutional evaluative bibliometrics. As an illustration, we will
calculate a regression model using publications (and citation data) produced by
four universities in German-speaking countries from 1980 to 2010. We will show
how these predictions and effects can be estimated and plotted, and how this
makes it far easier to get a practical feel for the substantive meaning of
results in evaluative bibliometric studies. We will focus particularly on
Average Adjusted Predictions (AAPs), Average Marginal Effects (AMEs), Adjusted
Predictions at Representative Values (APRVs) and Marginal Effects at
Representative Values (MERVs).
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1818</identifier>
 <datestamp>2013-01-11</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1818</id><created>2013-01-09</created><updated>2013-01-10</updated><authors><author><keyname>Ummels</keyname><forenames>Michael</forenames></author><author><keyname>Baier</keyname><forenames>Christel</forenames></author></authors><title>Computing Quantiles in Markov Reward Models</title><categories>cs.LO</categories><comments>17 pages, 1 figure; typo in example corrected</comments><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  Probabilistic model checking mainly concentrates on techniques for reasoning
about the probabilities of certain path properties or expected values of
certain random variables. For the quantitative system analysis, however, there
is also another type of interesting performance measure, namely quantiles. A
typical quantile query takes as input a lower probability bound p and a
reachability property. The task is then to compute the minimal reward bound r
such that with probability at least p the target set will be reached before the
accumulated reward exceeds r. Quantiles are well-known from mathematical
statistics, but to the best of our knowledge they have not been addressed by
the model checking community so far.
  In this paper, we study the complexity of quantile queries for until
properties in discrete-time finite-state Markov decision processes with
non-negative rewards on states. We show that qualitative quantile queries can
be evaluated in polynomial time and present an exponential algorithm for the
evaluation of quantitative quantile queries. For the special case of Markov
chains, we show that quantitative quantile queries can be evaluated in time
polynomial in the size of the chain and the maximum reward.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1825</identifier>
 <datestamp>2015-03-13</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1825</id><created>2013-01-09</created><authors><author><keyname>El-atty</keyname><forenames>Saied M. Abd</forenames></author><author><keyname>Gharsseldien</keyname><forenames>Z. M.</forenames></author></authors><title>Analytical model for mobile user connectivity in coexisting
  femtocell/macrocell networks</title><categories>cs.NI</categories><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In this paper we investigate the performance of mobile user connectivity in
femtocell/macrocell networks. The femto user equipment (FUE) can connect to
femto access point (FAP) with low communication range rather than higher
communication range to macro base station (MBS). Furthermore, in such emerging
networks, the spatial reuse of resources is permissible and the transmission
range can be decreased, then the probability of connectivity is high. Thereby
in this study, we propose a tractable analytical model for the connectivity
probability based on communication range and the mobility of mobile users in
femtocell/macrocell networks. Further, we study the interplays between outage
probability and spectral efficiency in such networks. Numerical results
demonstrate the effectiveness of computing the connectivity probability in
femtocell/macrocell networks.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1848</identifier>
 <datestamp>2013-01-16</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1848</id><created>2013-01-09</created><updated>2013-01-15</updated><authors><author><keyname>Chebotarev</keyname><forenames>Pavel</forenames></author><author><keyname>Agaev</keyname><forenames>Rafig</forenames></author></authors><title>The forest consensus theorem</title><categories>cs.MA cs.DM cs.SY math.CO math.OC</categories><comments>11 pages, 2 figures, 27 references; presentation improved</comments><msc-class>93A14, 68T42, 15B51, 05C50, 05C05, 60J22</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  We show that the limiting state vector in the differential model of consensus
seeking with an arbitrary communication digraph is obtained by multiplying the
eigenprojection of the Laplacian matrix of the model by the vector of initial
states. Furthermore, the eigenprojection coincides with the stochastic matrix
of maximum out-forests of the weighted communication digraph. These statements
make the forests consensus theorem. A similar result for DeGroot's iterative
pooling model requires the Cesaro (time-average) limit in the general case. The
forests consensus theorem is useful for the analysis of consensus protocols.
</abstract></arXiv>
</metadata>
</record>
<record>
<header>
 <identifier>oai:arXiv.org:1301.1873</identifier>
 <datestamp>2013-01-10</datestamp>
 <setSpec>cs</setSpec>
</header>
<metadata>
 <arXiv xmlns="http://arxiv.org/OAI/arXiv/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://arxiv.org/OAI/arXiv/ http://arxiv.org/OAI/arXiv.xsd">
 <id>1301.1873</id><created>2013-01-09</created><authors><author><keyname>Ochem</keyname><forenames>Pascal</forenames></author><author><keyname>Pinlou</keyname><forenames>Alexandre</forenames></author></authors><title>Application of entropy compression in pattern avoidance</title><categories>cs.DM math.CO</categories><comments>11 pages</comments><msc-class>68R15</msc-class><license>http://arxiv.org/licenses/nonexclusive-distrib/1.0/</license><abstract>  In combinatorics on words, a word $w$ over an alphabet $\Sigma$ is said to
avoid a pattern $p$ over an alphabet $\Delta$ if there is no factor $f$ of $w$
such that $f= (p)$ where $h: \Delta^*\to\Sigma^*$ is a non-erasing morphism. A
pattern $p$ is said to be $k$-avoidable if there exists an infinite word over a
$k$-letter alphabet that avoids $p$. We give a positive answer to Problem 3.3.2
in Lothaire's book &quot;Algebraic combinatorics on words&quot;, that is, every pattern
with $k$ variables of length at least $2^k$ (resp. $3\times2^{k-1}$) is
3-avoidable (resp. 2-avoidable). This improves previous bounds due to Bell and
Goh, and Rampersad.
</abstract></arXiv>
</metadata>
</record>
<resumptionToken cursor="39000" completeListSize="102538">1122234|40001</resumptionToken>
</ListRecords>
</OAI-PMH>
